It's uh my pleasure to be here and have a chance to talk to you all uh and present my ideas about the mind. Today I'm really going to present sort of a vision. Um it's not a complete worked algorithm but uh and it it sort of can't be because it's um it's got some prerequisites and the prerequisites are not available yet. And so uh prerequisites are basically um a working deep learning uh algorithm an algorithm that will uh that will um be able to continually learn and and improve the way it learns over time. It's a small ask I always thought but it's something we don't strangely don't have uh yet in in our uh in our family of algorithms. Okay. So before I want to start before I start I just want to mention among my many jobs is I founded something called the open mind research institute where we do basic research on reinforcement learning related approaches to the mind and if you're a young person interested in something in that area you you might want to check us out at openmindress research.org Okay, so let's get started. Basically what I want to say talk about is my quest my quest for a simple general architecture for an AI agent and I'm going to spend some time talking about the goals and how basically want learning from experience in a very big complex unknown world and then we'll get into the oak architecture itself and I'll have several ways of talking about it explaining it pictures four pictures and go through it step by step and talk about this vision um So it's like the holy grail of of AI of AGI really is is to have a a quest for mind that's that's a little bit hard to look at. Let me just get rid of the background. Uh a a quest a vision of mind that's general. And by general I mean it contains nothing specific to any world. So there's no domain knowledge. Um yeah and secondly it's experential. We learn from runtime experience and not from a special training phase. So runtime experience is in red because in the diagram I don't know maybe you can see my cursor these things in red the observation the action and the reward those are the experience. Experience is um unlabeled information uninterpreted information because we have a general a we're seeking for a general design. And so uh there's nothing specific to the world. The observations are just bits, distinctions, and uh you have to through your experience make sense of it. And finally, perhaps most important is the idea of open-ended abstraction. We want the agent to develop uh concepts, ways of thinking uh common patterns in its behavior uh that are uh unlimited in their sophistication other than of course by their computational resources. So abstraction tends to really be of two kinds just to help you think about where I am. Abstraction is is like finding features of the world, features of your um understanding of the world uh that um are helpful to you. Features I mean cues uh ways, concepts, a way of organizing that that a signal that you form in trying to understand the world that then you use to help you make your decisions. So you can search for good features which are sort of like uh state and it's good the good adjective to add to feature is a state feature. And then the second thing we want to find is good temporal abstractions ways things that you could do um that are at a larger scale than uh a single action. So like abstractions like uh walking to work or opening the door or uh picking up the object. Okay. So we'll talk all about that. That's the quest. And and I talk about it's useful to have these two words design time and runtime. Design time is when an agent is designed and before it's sent out into the world to obtain its reward. And at design time you build in domain knowledge into the agent. So of course I'm going to I'm going to minimize that. I'm going to deprecate that. Runtime is when the agent is learning from experience and making plans that are specific to the particular part of the world that it's encountering. I'm going to emphasize that runtime learning from experience. Now in a big world in a big world a complex world where things can't be fully anticipated then uh building things in at design time alone is insufficient and in particular if you are interested in open-ended abstractions then you have to be able to discover them at runtime. They cannot have been uh only anticipated ones nonopended ones could have been built in at design time. Really we have to discover things at runtime. Everything has to be done at runtime. So if everything has to be run at done at runtime really why don't we just not do any of it at design time because it will just complicate the design. You so the design is you you might want to think of it as um a way of understanding intelligence rather than as a way of producing a product. So for un for maybe for producing a product you might want to build in a bunch of information a bunch of background knowledge but for understanding the mind you want it as simple and and pure as possible and so for understanding it you you want to you don't want to leave out all that complications okay I'll say a little bit more about that uh right here in fact um so I'm saying that this agent architecture an ideal one uh For this goal, understanding intelligence uh should make no no design time commitments to any particular world from the bitter the bitter my bitter lesson blog post. As we say there as I say there the actual contents of minds are part of the arbitrary intrinsically complex outside world. Arbitrarily complex uh their complexity is endless. They should not buil be built in. Instead, we should build in only the meta methods that can find and capture this complexity. So we want AI we want AI agents that can discover like we can't we can not which contain what we have already discovered. Well, that's the basic idea and why we deprecate um domain knowledge in the in the name of generality. Okay, so let me ask you a question. Should the agent learn from special training data or only should we restrict ourselves to only learn learn from runtime experience? And for me the answer is clear. I think this is what the power of intelligence is and particularly the power of reinforcement learning is that we can learn from unprepared runtime experience. So I let me just be clear about my commitment. My commitment is that the agent should only learn from runtime experience. And that's what I mean when I say the agent should be entirely experiential. Okay? And I mentioned the big world perspective. So here's our agent like the person and it's it's uh smaller than the world. In fact, it's much smaller than the world. The world has all those other agents. Maybe usually looks like this. Um, the world contains all the objects and places and complexities of the physical world. It also contains a a myriad of other agents and what's going on in their minds is totally important for um for for the agent. The agent agents correct behavior is interacting with people, its boss and its its its spouse and its friends. what's going on in their minds matters to them just as what going is going on in your minds now matters to me. So the implications of the world being much much more complex than the agent and inevitably so um uh yeah nothing the agent learns can be correct can be exactly correct. It can't be optimal. Uh it's just any any any theorem you have that has to do with optimality or correctness really you have to um know that it's irrelevant to the real situation. All your value functions must be approximate. Your policies must be approximate. Your state transition models particularly your model of the world has to be much simpler than the world. So it's going to be wrong. It's going to be approximate. Even a single state of the world you can't really keep in your head because the state of the world can includes what's going on in all those other agents minds. And of course what's in your mind can't be much more complex than than all those other minds. Okay. Another implications of this a little bit more advanced subtle is that the world will appear to be non-stationary because you don't know exactly what's going on in it. it's going to appear to sometimes be one way and then sometimes be another way. Just like if you're driving down the street and you see the car ahead of you, you don't know uh whether that car will turn right or turn left. I mean, it's not random really what'sever in the head of that agent who's driving the car ahead of you is is uh maybe deterministic, but it's going to appear non-stationary. And you know, sometimes the car will turn one way and sometimes it'll the car will turn the other way. it will appear to vary. So with this all these these effective uncertainties about the world, you have to learn at runtime. You have to plan at runtime. Um you have to be able to find any needed abstractions at runtime. You're going to be born and grow up and encounter the world and you'll have to figure out what what are its its objects and its people and the ways that the world works uh and the way the social institutions work. You'll have to figure all that out at runtime. It can't can't be designed in. So we deprecate that. Okay. Uh so let's get going now to the a to the oak architecture which is addressing all these things. Um let me approach it. Uh just talk about the general AI problem. It's to design an effective purposeful agent that acts in the world. And in reinforcement learning it's the same problem except we uh we we we we formulate in a particular way. We we specify the purpose as a scale in terms of a scalar signal called reward. I'll say a little bit more about that in a minute. And secondly perhaps uh u in specialty reinforcement learning we assume that the world is general and incompletely known. And so the world could be anything from a grid world to a complex human world stochastic complex non-mark nonlinear and its state space as I've said is effectively infinite and its dynamics is effectively non-stationary. We have to work at runtime. We have to uh run apply at runtime without some special uh training signal from a teacher or human. And this this is a problem is is great. I think we don't need to change it. Uh we just you know the reward signal is enough. We just have to solve it. And I also want to mention that I'm going to assume that the agent is limited by compute not data. The data is is is rich and we want streaming algorithms so that we will exclude things like replay buffers. Now I've said the purpose is specified by a scalar signal. I'll spend another slide on that. Uh this is what what's called the reward hypothesis. The reward hypothesis is that that all of what we mean by goals and purposes can be well thought of as the maximization of the expected value of the cumulative sum of a received scalar signal that we call reward. And I think this is not a limitation at all but this is in fact just a great clear way to specify the goal and adding additional things doesn't make it better would not make it better. And there's been some theoretical work on this and I encourage you to check out this paper by Michael Bowling and others on settling the reward hypothesis. Um and also we know that even a simple reward can lead to all the attributes of intelligence and maybe I shouldn't say we know that we we I I and others have argued that in a sufficiently complex world a simple reward can lead to all the attributes of intelligence. That's the reward is enough paper. Okay, now let's get started. The oak architecture and I like to start with uh a prior architecture and this is what uh I call the common model of the intelligent agent and it's common to many fields. Uh it's common to AI and psychology, control theory, economics, neuroscience, operations research. Everybody um uses this standard uses a model of the agent that has uh uh certain basic attributes. Number number one is the in is the the red things you know the exponential interface the fact that you have reward the fact that you have action fact that you have some kind of signals from the world that are your observations and the fact that the observation uh in general the observation is not uh sufficient to totally disambiguate the state. So that's the outside view, those three things, the red things. Um then the other but but really the common model much of it is about the inside things and the four uh components. Okay, the first actually first look at what's interconnects the components. The inter what interconnects the components is the blue thing. It's the state feature vector. It's how you represent um the uh state of the world and it's a feature vector. So it has has components and each component is a feature and this state feature vector is created it's basically your understanding of where you are. And so when you create this sense of where you are from your input and past actions that a good word for that is perception. It's how you perceive the world and and represent where how you how you uh and represent where you are are now. And the b it's the basis on which you for example pick your action. I'll assume you can see my cursor here. This state representation flows through and it's the input of your action. Uh your policy will reactively which means without much deliberation it will pick an action. Uh so those are the first two components the perception and the policy and they they together make up a full agent. But in order to learn that agent to improve it you want to learn a value function. The value function says oh I'm doing well now or I'm doing poorly now. Things have gotten better or worse. So it has to know the rewards because the value function is essentially a prediction of reward. And that's used to uh to do to learn the policy which we suggest by drawing this diagonal line through the policy structure. And the fourth component is the transition model which we use for planning. the transition model it basically you know takes the states and tries to predict the next states if if you act in a particular way and I can't show all of how it works but uh in one way or another by the process of planning we have a diagonal line through the policy and again we change the policy and we also change the value function in both these cases okay that's the common model where we start does learning and planning has these four components and um I think it's kind of interesting Interesting that we can take this single view to cover all these different fields like control theory will use different names. It'll it'll won't talk about action. It'll talk about controls. Won't talk about reward. It'll talk about uh uh payoffs or uh or costs. And in psychology for example, we would we might say reward, but we would instead of saying observations, we talk about the stimulus. And so anyway, all all these fields really um have the same ideas sometimes with different names. Okay. So, and we have to ask the question, what is missing from the common model? Why aren't we done? And the the key thing that we're missing is that it's kind of um um uh it's low level. The actions are the momentary actions, reward momentary, the obligations are momentary. We have to get we have to get higher level abstractions. We have to develop concepts. We have to develop uh ways of thinking that are at a higher level. And so uh basically I think what we need to add and what the oak architecture tries to add is open-ended abstraction to this view. Okay. So here's the oak architecture and I've tried to show in in purple all the things that are new. Okay. It's this oak architecture and it's adding auxiliary subpros. So subpros that are auxiliary to the main problem. The main problem is getting reward and the sub problems well we're going to see this phrase a couple times. They are reward respecting subpros of feature attainment which means they are about you know each one each sub problem takes a particular feature and tries to attain it but it respects the reward. He says, "Attain this feature while still, you know, getting a good amount of reward and not suffering too much in terms of your your reward." The picture here looks almost the same. Um the the policy now we have behind it a bunch of more things that are called options and they are are like further policies and behind the value function we have more value functions. So we have a whole bunch of value functions. Of course, we have to have one value function for each subpros because it's a separate problem and we have to know how well we're doing with respect to it. So, we need more value functions. So now we have four kinds of components because well these two are the same but these two are now multiple. We still uh have perception much the same produces a state feature vector which will be used by all the sub problems and now the policy we also have form all the options um and the they produce a way of behaving that's appropriate for the sub problem and value functions many value functions uh for evaluating how well we're doing on each sub problem I don't show the sub problems here it's interesting you don't actually need an object uh in your architecture For the sub problem, you just need to have the value function and and for it and this pulling off little tendrils of the of the um feature vector represents the idea that we are each each value function has a separate uh feature that it's trying to optimize. Okay. Now let's let's uh dwell on the just for a second on the transition model. The trans transition model uh is what I'm going to use the word knowledge for. Um anything that's all of our knowledge is in our transition model. How we believe the world would change if we did different things and in particular if we did these larger things called options. Let me talk about that now. Uh so the name oak the oak architecture comes from options and knowledge. And an option is this higher level way of behaving. But it is just a pair of a policy and a way of terminating. Just those two things grouped together. A policy just a map from the states to the probability distribution over the low-level actions. And the uh termination condition is just a map from the states to probability of terminating. Now in oak we have a lot of these options. Well, like one for each feature uh each numerical feature and then it learns the knowledge about what will happen if you followed that option until it terminated. So that's the form of knowledge you'd have, you know, things like um if I picked up the object and held it in my hand, what what would it feel like? If I walked downstairs and looked in the kitchen, who would I see there? Um if I traveled to Iceland, you know, what kind of people might I meet? If I attend the talk what will I learn something what that that is your knowledge about what will happen it becomes a high level transition model of the world that enables planning to look at larger jumps and hopefully carves the world at its joints so the oak architecture involves these uh eight steps that we'll be talking about they're all done in parallel at runtime I'll be talking about them bit by bit but you might just look at them I'm going to learn policies and value functions maximizing reward. I'm going to generate new state features going to order the features. I'm going to uh create the subpros one for each highly ranked feature. Then I'll learn solutions of the sub problems. I'll learn transition models for the solutions of those sub problems. And then I'll plan and I'll have to maintain metadata on the utility of everything. Okay. So hopefully some of those words um made sense to you. Let's go through it step by step uh and and assess are we able to do it now. So the first step of learning the policies and the value functions for maximizing reward well that is classic reinforcement learning and we have mains of algorithms for doing that. Uh but still I'm not going to give it a green check because I don't consider it totally solved. It would be solved if we could really do continual deep learning. Well uh and and as I said at the beginning, this is the major prerequisite that we don't yet have. So let's look at that. Uh so here I'm just going to focus on that that first step for a moment and noting that it would it requires reliable continual deep learning. And can we do this yet? I mean because we've needed to do this for at least 40 years. And I'm I've always been or I am rather disappointed that no one's figured out how to do reliable continual nonlinear learning. We can do it for uh linear networks. Linear networks can learn reliably and continually. But conventional deep learning, you know, fails catastrophically once we uh ask it to continue continual learning to maintain its learning ability. So most famously uh we get catastrophic forgetting. This is going back to the 90s and more recently it's come to the four the fact that we just lose the ability to keep learning. This is called loss of plasticity and it's been shown by another folks uh in my lab and in others. So and one of the solutions you know maybe we have partial recent solutions we can we can do uh uh deep learning and have learn continually uh somewhat uh continual backdrop. Um, another idea that might help is continual discovery of new features and and step sizes. And I do believe that one way or another we will get reliable uh continual uh nonlinear learning in the next few years. And so I think it's it's sort of fair to assume that it will come along and to rely on it existing as I do in in the oak architecture. Okay, going back to the second step. we need to generate new features from existing state teachers. So here I'm not talking about you know finding the best ones. I'm talking about generating a bunch of candidates. Okay. So I think um this is is is not really uh clear this this one there are lots of ideas. Let's talk about this. Um so oak will require the discovery of new state features. And it's an old problem. It's going back to the 60s. You're probably uh familiar with it if you have been around. It has many names. So the whole idea of representation learning or the new terms problem or metalarning uh is often used for for this idea and this is what back prop was supposed to solve was supposed to solve but really does not you know what was the paper of learning representations by gradient descent or something like that was the original backdrop paper. uh but and there were some papers sort of claiming that it it did it does but I think nowadays we acknowledge that it it doesn't really uh find good representations except in the desperate sense that it eventually solves the problem and that may require some representations but it doesn't uh generalize well particularly and uh it really isn't finding the representations. Other methods have been based on generate and test. Again, things growing going way back into the past. Uh like try a bunch of of of candidate features and then select the one just just see which ones generalize better and keep those. And uh the back continual backdrop again is one of these new algorithms based on generate and test. And so this I'm holding out is not is really an unsolved problem. We need to do it. we need to uh have a good way of generating new state features from existing features and I'm going to rely on it. I have my own personal favorite idea for solving this problem which is uh the IDBD algorithm. Uh I think that's going to be part of the solution. So I give you the the name here at the bottom of the paper if you want to check it out. Uh okay next we want to we've got all these features that we've created candidate features that we've created and we want to rank them and this I think is is seems easy it's easy to rank them by um uh are they're proving useful to the uh to the subpros and to the agent and to the learning of the model all you can just see are the features being used and use that to rank them. So you can rank them and then the big the big problem is or the big uh substantial step is to create the sub problems one for each highly ranked features. So I want to talk about talking about that uh because I actually think this is easy to do and and we have done it. So how do you create sub problems? Well, first I have to acknowledge there's a long history again of people working on auxiliary sub problems and um there's some issues that are largely settled and there's some issues that are still open. And so let's talk about the open questions. Open questions like what should the sub problems be? Where do they come from? Can the agent generate its own subpros? And how do the sub problems help on the main problem of getting reward? and oak the oak architecture proposes offers answers to all these questions. So uh sub problems sub problems I think are what we're looking at in play and if you just think about the life of an animal or or even a human that our life is full of sub problems but here is a young orangutang and I think he's focused on the sub problems just the sensation he gets when he swings he's trying to he's trying to uh enjoy that and reproduce it. Uh so you know can can he can he get that to happen again? What does he need to do to get it to happen? You can think of what the play that he's undergoing here as is trying to obtain that feature the feature of of swinging his vestibular system saying saying what that feels like which is something that doesn't normally happen to him. Okay. Similarly, here is actually a an orca, sometimes called a killer whale, uh who's in an aquarium, and someone's thrown this this uh this this float in with him and he's been playing with it and he's found out that he he can carry it on its back. It's just sort of an interesting thing that he noticed that he could do and then he without being trained at all, he learned how to do it uh reliably. So, this is what I think of play is, and we can think of it as being focused on obtaining a feature. Uh, here's a human example. Here's an infant playing. Um, and it behavior is not random. It's behavior is sped up a little bit. I'm sure you noticed. Um, but it interacts with the toy. it learn some things and then it moves on you know when it stops being able to learn more to look at the next opportunity for learning and um so um this is what we call curiosity of course uh but we can think about it as having subpros sub problems of feature attainment uh certain experiences it can have colors or feelings or sounds and trying to be able to reproduce them it's one way of thinking about the process of exploration and curiosity in terms of subpros. It's gaining control over its world. Okay. So, obviously an agent must create its own subpros. There's no way for all the possible subpros to be built in. Uh the infant encounters its world and and sees what what what's available to be done and and then it learns how to how to how to attain the things that are there. So the problems possible sub problems are too various and they're too world dependent. We have to give the responsibility for forming the subpros for posing the problems uh to the agent and we have a lot of the necessary algorithms for doing this. We have we have options. We have the idea of value functions and its full generality. We have off policy learning and planning methods. And so we have to we have to create subpros in a domain general way. And so how could you do that? I mean what possibly could they be uh a subpro and I think I'm going to offer the solution this reward respecting sub problems of feature attainment. And the best rationale for it is just that there's really almost nothing else one can think of other than attaining the features. You know you can't talk about states because the age doesn't have states. You can't talk about uh objects in the world uh because we don't have opriori objects in the world. All we have is um the features and concepts which are which are concepts which we have formed to understand the world. Okay. Uh so if we if we uh I haven't checked my timing too much. Uh I do wonder how I'm doing. Uh if I have time to go through the specific way we create subpros in oak. Um basically we start with a feature. >> Please do. >> So that is a an index a feature number like feature 53 and and kappa is how intensely you want the feature that you have to specify. You specify those two things, we can automatically have a sub problem. The sub problem is to drive the world to a state where the feature is high without losing too much reward. And so the solution is an option. It's a policy and a way of terminating that maximizes the value of the I feature at termination while respecting the rewards and value. So um the uh you want to maximize you want to choose the option so as to maximize the expected value of the the the total reward you'll get during the option some of the rewards some of the rewards from the current time this is all conditional I'm starting at time t in the designated state s and then we start following the option and we'll get some rewards and we'll end at time capital t so we're just in the rewards received between those times And we're also interested in the value of the resultant state. We don't want to be left in um a state where where bad things are going to happen. So this is our estimated value of being in that terminal state. And oh this is the the feature attainment. Of course we want and when we stop at time capital T we want to be in a state where the feature fi fi is high. So that value is large and it's multiplied by kappa which is how intensely you want the feature. So these are reward respecting subtasks the feature attainment and there's a paper on this in AI journal a year before last. Um so that's what I think the sub problem should be and we then have to learn the solutions and uh and learn the models of the of the solutions. The solutions are the options. So these two things I think are relatively straightforward but again they are rely rely on us being able to do continual deep learning well but if we if you grant me that that prerequisite then we can do it. Um and so this is where we really get to the heart of oak. So just so pay attention to the overall structure of the argument now and uh basically we're going to do all these all these learning processes and they're going to use features and they're also going to drive the creation of new candidate features. So basically we're going to have features that carries over here. Uh once we have features that are interesting we can create uh subpros to achieve each one. So once we have those subpros, we then solve them. If we have uh a thousand sub problems, we would solve them to produce a thousand options. Okay, each one of those options will then um uh be the foundation. You know, we ask the question if I did that option in this situation, what would happen? So that's the transition model. We're predicting the consequences of behaving in that way. Now this is different than you know solving the problem is finding a good way of behaving uh and and that would be a way which hopefully achieves the feature but many other consequence there will be many other consequences of behaving that way. You may achieve the feature you may you know other things will happen. If I go downstairs to uh the kitchen, um I may get to the kitchen, uh but I I I might also uh see my spouse or I might um I might trip and fall down the stairs. Many things could happen and I need to predict the possible consequences. Um, so that's the transition models and then I have once I have the transition models, I want I have I have to use them to plan which improves my my behavior and hopefully get me highly adapted behavior. Okay, now let's look at the same with a different picture. So we're talking about perception giving me the features. So now I've got the state features. Then I I solve those sub problems. I play with the world and I get the options. the options I predict the consequences of the options and that gives me a transition model. The transition model through planning gives me improved values and policies and then all of these uh later steps feed back and they inform perception. They we tell perception oh in order to form this model I had to use some features and and not use other features. In order to solve the problem I I used some features and didn't use other features. So this these processes inform the feature construction process and they could say, "Oh, these features that you've offered me are are useless and you should you should throw them out and get me more features and and these other features are very useful. Maybe they should be the basis of future uh uh problems, subpros that are posed and solved and thus recursively um improve in an open-ended way uh the abstractions of the agent. Okay. Okay. So, uh I need to say a little bit about planning because it's one of the key steps. So, how and planning is is a big topic. uh and I'm going to give it the green check because assuming we have the models uh I have a I have a specific plan for um for planning a plan for planning. Uh well first why do we plan and why do we want to plan with sort of these large scale steps jumpy uh steps? We want to plan because the world changes and because the world changes our values change. Another way of saying it is it's easier to get the model right than the values right. So or at any point in time if the world changes the the values change uh like finding my way to the restroom is is is really important. Uh but the value for being in the restroom changes. Sometimes I want to go to the restroom, sometimes I don't. But uh the the the model of the uh the option for getting me to the restroom um may I want to retain that even as I change in my desires about whether I want to go there right now. So, so uh it's as if yeah the values of the states change the most of the world dynamics remains the same and to prepare for this you have to get ready and this has implications um but let me just go into planning just to give you a rough idea of what I'm thinking of um uh um the the idea value of of planning is that it will be done by value iteration which in which you improve your value function so that you know which states are good which states are are you want to be in or not don't want to be in which features do you want to obtain do you want to be true now this expression in classic value iteration this and what what's the equation written here is classic value iteration classic value iteration is expressed for the case of uh discrete states where you have it could have a table where you can store the value of each state so we'll do this we need to we need I need to do this case first in order to understand the idea So in valuation continually all the time when you have spare time well time to think what you do is you select a state and you know how you do that is an important question and then you perform a backup at that state. So the back backup of the state will change improve the estimated value of that state and you take a max over all action all the things that you might do and you want to look at the reward that you expect to get. R hat is your expected reward. Okay, this is from your model. Over here in the upper right, we have the model. What is the model? The model of the world, it's it's two things, R hat and P hat. Uh, and the model takes a state and an action and guesses what will be the probability distribution over the next state and also what will be the expected reward. Those are the two this is what a primitive model does. This is not a high level model. This is a primitive model. value rations defined for primitive primitive models, low-level models. Okay, so you have the expected reward and then you have uh the ex basically the expected value of the next state. You're summing over the probabilities of each next state and the values of that next state discounting it by a little bit. You're basically sitting here at the state s and you're looking at all the actions you might do and all the states that might result. and then you sort of figure out what's likely to happen and you back it up and change the value of state and all planning methods uh have the same idea within them. Some of them, you know, this is valuation looks exactly like this. But even things like AAR and Monte Carlo research and model predictive control, they all involve the basic idea of looking ahead and seeing the consequences using your model and updating your sense of of of the value of your different states or different uh choices and then making your choice accordingly. So this is value iteration. And then when we we we we make it abstract so that we can make jumps. We can make large steps in the world. Um that's would be that's that's the next step. Okay. So life is lived one step at a time. But you plan your life at a higher level. You plan to go to the talk. You plan to stand up. You plan to go to the restroom. So this model the the knowledge your transition model this is the transition model which I'm calling knowledge it's about these largecale dynamics where your actions or your choices which are options are are purposive like so our our knowledge is not conditional on single actions. It's conditional on things like you know getting a calling an Uber to get to the airport and traveling to Iceland or going to the restroom or picking up the object. So these are larger ways based in knowledge about the options and so the option model this is this is the conventional model repeated from the last slide and we generalize that to an option model we just replace action with option so the input is all things that I might do this these extended ways of doing and I get uh uh it's not the next state but it's the state at which I will stop or terminate when the option stops And instead of the expected reward on on onestep reward, I have the expected reward that's summed over from when I started to when I terminated. Okay, but with those two changes, the valuation is is the same. We have the the same sum over possibilities and we get the reward and the uh expected value of the stated stopping now. And it's the same idea. Okay. So we can do we can do planning at the abstract level using value iteration. And the other thing I need to do is I need to pres provide some insight into how we generalize this to to handle function approximation because the world is big and we don't have a we don't have a V of S. We don't have a value for each state of the world but we have a value of each state of the world which is determined um by our weight vector by our parameters. So this we can have we have some approximation the the state of the world the world will be presented to us in our observations and our um and our uh in our rep our feature vector and then that will be involved with our our parameters and give us an approximation. Okay. And so of course the model will also become parametric. We will predict the reward along the way and also the transition probabilities to the next states of termination. Those will also be parametrically represented. And so instead of doing this conventional thing with the table, we're going to do uh well we're going to update the weights that's shown down here uh with by a gradient procedure. It's but we this thing in here the thing in in brackets uh this is uh this is replaced. So we have the reward uh expected reward is now based on a model and a transition model and a parameter of that model and the transition probabilities are based on the parameter and then we have the estimated values. So this backed up value is the is analogous to this the thing in brackets and so that becomes the target. Uh so here we move the estimate estimated value for this the state we're looking at s is moved towards the max of the backedup um value for the um uh for our choices. Okay. So we've gone back to single a primitive actions. now to do the to do the uh the the uh function approximation. This will also generalize to the full the full option case. Okay. So I I have to move on and finish up. Um and so so I can take some of your questions. The very last step is to is to acknowledge that um everything needs to be uh uh you we need to keep statistics or metadata on the the qualities of everything the quality of the transition models because the model will be approximate and we have to learn where it will give me reliable answers where it won't. We have to have keep statistics on the features so that we can inform the feature uh generation process about when it's doing which features are good and which features are bad as it generates new features. Uh so with that I'm just going to wrap up. Oh I have one more figure uh to remind you uh to give you a sense of what it is one more time. uh oak is open-ended because it has its cycle of of of discovery and this is the ambitious idea ambitious new idea of oak uh that is to be open-ended in its possible abstractions limited only by compute and the basic cycle is this cycle we have start with the state features bas from those we uh construct subpros from sub problems we solve them to produce options and the options we and uh can produce models of the options which improve the options and the policies through planning processes. And all of these steps, these three steps here will use the state features. And so they they uh uh the the so the arrows go in one direction, but really there's a influence the other way, which is we're informing the state features which ones are useful and which ones are not useful. So that involves the ranking and then so so that that will lead to oh here's a new feature that I didn't realize was was really important before but everyone's making a high use of it. a good concept and then that concept uh that feature becomes the basis for a new sub problem and that's really what makes it open-ended and possibly uh getting better and better abstractions through time through this cycle with everything is ultimately tied to reward uh uh so remember the quest the quest is to be general experential and have open-ended abstraction which I think I think maybe I hope You can see this vision gives you a way you might uh hope to achieve all of these things even though it has some missing prerequisites. Uh so Oak has a vision of how to grow super intelligence from runtime experience. It has all the most important capabilities. It it can act, learn and plan. Can learn a model. It can form subpros. These abstractions work with different things to work on. and uh the options and it can do this with function approximation, partial observability, non-stationary worlds, stochastic worlds. It can discover state features and thereby discover the subpros, options and models. And this all feeds back to motivate more abstract features. This virtuous cycle, virtuous open-ended cycle of discovery. uh while at the same time the architecture is completely general. It doesn't have any domain specific components and thus is scalable and of potentially lasting impact. Thank you very much for your attention. [Applause] Thank you very much, Rich. It's wonderful to have you here. Of course, congratulations on your recent touring award and thank you for all your contributions to the field. and of course to reinforcement learning. Hopefully you can also help us with this uh robust continual deep learning as well. And we have a few questions from the audience if you have time. So we have one over here. >> Uh yes, looking through your uh list of requirements there for oak. Uh I noticed that in the chimpenzee example that or rangutan example you use there. Um it it seems like the create subpros for highly ranked features can consume the second and third states because you already have a reward function built into the mechanism of the chimpanzeee to enjoy the the swinging let's say. So it's a some sort of chemical reaction that then says this is a highly ranked feature because of your experience. And so the generating state functions comes from the formation of the system itself in its embodiment of how it gets rewards from the environment. And then that rank ordering then becomes that like the benefit of the reward rather than necessarily a a taught K to your point of biasing features. So I guess my question would be I is there a way to maybe skip steps uh between one and four by assuming that subpros come from play based on generated you know uh fun let's say in in your terms to say well the the thing that is getting the reward is tuned to get that reward by the function of how it's built. Well, so you're you're you're uh rightly uh focusing on how we uh select um features to become the basis of subpros. Now uh some of that can come from our prior design, you know, like our design may say um bright lights are likely to be important or loud sounds. And maybe maybe our design can tell us that things that activate our vestibular system are are important. Uh but many things are not like that. Uh so you know you think of your work in science you uh you study a problem and you become aware of very subtle distinctions and and and and that you still might want to uh focus on them and work towards them. Or maybe um uh you play chess and uh you have things that are superficially subtle like you know the uh structure of your pawns and then you can make a goal. Um or maybe or the safety of your king you can make goals for that. Um so I don't think you can anticipate you can expect them to be anticipatable. Um you But it could come about by just noticing that that these subtle things are are important for answering some of your questions and they could be a useful for answering um your basic reward questions or it could be for a a answering other other subpros. So maybe a good example is to think about an infant uh and the the infant becomes very aware of its caregiver. Let's say its mother and I don't know maybe the smell of its mother is a is a subtle thing but uh it through through association with with good outcomes it becomes important. um or the sound of their voice or the sound uh yeah also I also think of the example of you know playing with a rattle maybe you you move and you make the sound of a rattle the rattle is some particular sound but it's novel and just because it's novel uh you might want to uh focus on it and and be able to reproduce it anyway it's a it's a wonderful interesting question of how you the sub problems become become chosen Thank you for your question. >> Thank you. One more here. >> Yeah. Hey, this is uh Ben. So, we've we've had a few talks at this conference on predictive coding as an alternative to back propagation for training deep neural nets. And I know there's some research going on trying to use this to to solve the catastrophic forgetting problem and and uh make neural nets able to continuously learn. So I guess my question is is your intuition that the continued learning problem can be solved using backation or do you think we'll need to go back DC or some other more perhaps more biological or else other innovative way of of doing the the training of neural nets. >> Uh thank you Ben. Um I don't think it's a problem with neural networks, the structure of neural networks. I think it's a problem with the weakness of of simple back propagation. Uh the solution and and and the solution I think will still involve gradient descent but the gradient descent alone is not enough. This is sort of the lesson of of the continual back propagation algorithm. This is the new algorithm that was proposed by Dohare and and studied in the loss of plasticity paper. Uh that it's it's a simple thing. It's just the same as back prop except instead of initializing with small random weights only at the beginning uh you do it as you go along a small fraction of the units that are not being used very much are reinitialized with random weights. So this is a a a very simple idea, a simple modification. It's actually just making uh back prop more continual, you know, making it doing doing more of what it already does. And so it's a small modification of the algorithm which makes it vastly much more uh capable of of continued learning. And so I think I think the space of algorithms is is large and we have just done so little to explore it and they'll be small uh or or maybe larger but anyway there's a there's there's a wide range of possible ideas to uh make it more effective at continual learning and they will involve still uh gradients. It's just the gradient >> predict predictive coding also involves gradient descent. You're just learning it using it for different things. I mean you're you're doing gradient descent at one neuron to predict what some other neuron is doing locally but so you're not propagating the same way as in back prop but you're still doing gradient descent locally it it's more like in the direction of Benio's difference target propagation but a little more extreme right so yeah as you say though there's there's a lot of ways to organize it >> the key thing is that you need um you need a random component you need a search component. Gradient descent is deterministic and follow follows the gradient. That's not enough. You need something that will generate variety. Uh and uh this is what this the initialization with random weights generates variety. >> That's a good bit of wisdom. I I I would agree a random component is is is going to be one of the critical elements. So yeah, thanks for that. Thanks, Ben. And we have another question at the back of the room. >> Hello. Uh, I had a question about how uh continuous observations on the world get turned into new possible actions, how that sort of uh process occurs. Like if the child's playing around and they discover this heavy thing that's a hammer that they can pound on stuff with and slowly they discover nails and boards, you know, how does using the hammer to put the boards together come into being your learning mechanism? >> Now you said continuous and continuous has two possible meanings. I just want you you to clarify which one you mean. Continuous could mean uh like it's a number that takes a a continuum of values or you might mean that it's ongoing in time. Yeah, I'm referring to the ongoing in time just creating new possible actions that you can take based on observations. You know, is there a reification that turns into an abstraction of here's a tool I can use? How does tool use get discovered is I guess what I'm asking. Well, that's a big question. >> Yeah. Yeah. >> But even even like a simple tool, I would uh offer that that this has as a a hint of its of how it's done. Um let's see. How would uh so so you would you would find maybe you find the hammer and you you realize you could use the hammer to make a noise. So you could become skilled at making a noise with this hammer and um then you might I mean what I'm pointing out is that you're hammering hammering in a nail is a pretty advanced concept. is a skill, advanced skill, and you won't expect it to be successful right away, but you could use the hammer to make noise. And then you could use the hammer to break things. And u probably in order to become skilled at at hammering a nail in, you you'll have to watch someone else do it. And if you watch someone else uh take take a a nail and a and a hammer and do this really interesting thing where the the nail went into a board, you know, that that's kind of a unusual observation and and and that could be the basis then for you know can I use the hammer to get a nail to go into a board? Okay. And then you one could work on that for a long time. And then once you had that you would you know maybe you would you would have the idea that you could uh you know assemble something by by uh by the tool. I don't know that's a bit hand wavy but I think there is a there is a progression from uh detecting interesting novel features and uh and and wondering if you can reproduce them. And so then, you know, the key is that you you you just want to be able to hammer it even though it's not getting you reward. You want to make uh you want to reproduce something that that uh you saw it being done rather than even though it doesn't get you any any reward immediately, but you you learn how to and you also learn how to hammer the nail into the board without without hurting yourself, for example, without hitting your thumb with the hammer. uh that that's the sense in which you want to be reward respecting. >> Thanks. >> Thank you. >> Thank you very much. One one last question. >> Hi. Um how do you think a new agent will go about uh discovering and understanding the boundary between itself and the world? Like what is it self and what's uh what's the environment? Uh well, I think it's kind of interesting how that blurs um how uh because when you're where is the boundary first of all, you know, where would I draw it? I would draw it around your mind, which you might think of as as around your brain. So, in particular, your your body like your hand as part of the environment. It's uh something that you can't control completely and you would you're interested in controlling and and you know we could think of how a child uh u doesn't initially know how to control its its hand or its other limbs and you could see a young infant in fact you know will often just look at their hands and figure out how they can control them and you know first you can find them like you know hey here's here's my hand and I can see that now now I can't see it and I say, "Well, hey, can I get that? Can I?" It doesn't say here's my hand. It says, "Here's an interesting sensation. Can I get it to happen again? Can I Can I bring my hand up to my field of view? Can I get my thumb back in my mouth? Uh, here's an interesting object. I wonder how it would taste if I put it in my mouth." So, there isn't um where where's the boundary? And let me There's It reminds me to say something because it's often unclear. uh to to folks who haven't been doing reinforcement learning for a long time um about the reward. The reward comes from outside the agent but it doesn't come from outside our body. Right? The reward is computed inside our head in fact inside our brain but it's a part of the brain that I don't that is not considered part of the agent because the the agent is not allowed to uh just set its reward to be high. There's a there's a there's a part of our hypothalamus I think that computes the reward signal and um that is technically outside the agent um just as the body is outside the agent. Um I have to of course refer now to the extended hypothesis. How uh as we get more familiar with controlling something it like our body it becomes almost uh almost it becomes more useful to think of it as part of the agent. you know, or is we driving a car? We start to think of the car as we're controlling the wheels and the wheels are like our choices. This is changing the abstraction. And it's it is a useful way to think of it. But formally formally there, we want to have a a clear line between the agent and the environment and a clear definition of the primitive actions that are available before we introduce higher level abstractions. Thanks again. >> That's wonderful. Thank you very much, Rich. Such a clear explanation. Great discussion. I don't know if you can hear the applause, but we really appreciate you being here and joining us today. Thank you very much. >> Thank you very