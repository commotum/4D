# 1. Basic Metadata
- Title: Training data-efficient image transformers & distillation through attention.
- Authors: Hugo Touvron; Francisco Massa; Matthieu Cord; Matthijs Douze; Alexandre Sablayrolles; Herve Jegou.
- Year: 2021.
- Venue: arXiv (arXiv:2012.12877v2 [cs.CV]).
- Evidence (Title, front matter): "Training data-efficient image transformers\n& distillation through attention" (Front matter).
- Evidence (Year/Venue, front matter): "arXiv:2012.12877v2 [cs.CV] 15 Jan 2021" (Front matter).
- Evidence (Authors, front matter): "Hugo Touvron"; "Francisco Massa"; "Matthieu Cord"; "Matthijs Douze"; "Alexandre Sablayrolles"; "Hervé Jégou" (Front matter).

# 2. One-Sentence Contribution Summary
The paper claims it can train competitive convolution-free vision transformers on ImageNet alone by introducing a transformer-specific distillation token to improve data efficiency and performance.

# 3. Tasks Evaluated

| Task name | Task type | Dataset(s) used | Domain | Evidence (quote + section) |
| --- | --- | --- | --- | --- |
| Image classification (ImageNet) | Classification | ImageNet | RGB images | "Our reference vision transformer (86M parameters) achieves top-1 accuracy of 83.1% (single-crop) on ImageNet with no external data." (Abstract) and "The fixed-size input RGB image is decomposed into a batch of N patches of a fixed size of 16 × 16 pixels (N = 14 × 14)." (Section 3, Vision transformer: overview) |
| Image classification (ImageNet-Real) | Classification | ImageNet-Real | RGB images | "Table 5 reports the numerical results in more details and additional evaluations on ImageNet V2 and ImageNet Real, that have a test set distinct from the ImageNet validation, which reduces overfitting on the validation set." (Section 5.3, Efficiency vs accuracy) and "The fixed-size input RGB image is decomposed into a batch of N patches of a fixed size of 16 × 16 pixels (N = 14 × 14)." (Section 3) |
| Image classification (ImageNet-V2) | Classification | ImageNet-V2 (matched frequency) | RGB images | "Table 5 reports the numerical results in more details and additional evaluations on ImageNet V2 and ImageNet Real, that have a test set distinct from the ImageNet validation, which reduces overfitting on the validation set." (Section 5.3, Efficiency vs accuracy) and "The fixed-size input RGB image is decomposed into a batch of N patches of a fixed size of 16 × 16 pixels (N = 14 × 14)." (Section 3) |
| Transfer learning: CIFAR-10 classification | Classification | CIFAR-10 | RGB images | "Our models pre-learned on Imagenet are competitive when transferred to different downstream tasks such as fine-grained classification, on several popular public benchmarks: CIFAR-10, CIFAR-100, Oxford-102 flowers, Stanford Cars and iNaturalist-18/19." (Introduction) and "We evaluated this on transfer learning tasks by fine-tuning on the datasets in Table 6." (Section 5.4, Transfer learning) |
| Transfer learning: CIFAR-100 classification | Classification | CIFAR-100 | RGB images | "Our models pre-learned on Imagenet are competitive when transferred to different downstream tasks such as fine-grained classification, on several popular public benchmarks: CIFAR-10, CIFAR-100, Oxford-102 flowers, Stanford Cars and iNaturalist-18/19." (Introduction) and "We evaluated this on transfer learning tasks by fine-tuning on the datasets in Table 6." (Section 5.4, Transfer learning) |
| Transfer learning: Oxford-102 Flowers classification | Classification | Flowers-102 | RGB images | "Our models pre-learned on Imagenet are competitive when transferred to different downstream tasks such as fine-grained classification, on several popular public benchmarks: CIFAR-10, CIFAR-100, Oxford-102 flowers, Stanford Cars and iNaturalist-18/19." (Introduction) and "We evaluated this on transfer learning tasks by fine-tuning on the datasets in Table 6." (Section 5.4, Transfer learning) |
| Transfer learning: Stanford Cars classification | Classification | Stanford Cars | RGB images | "Our models pre-learned on Imagenet are competitive when transferred to different downstream tasks such as fine-grained classification, on several popular public benchmarks: CIFAR-10, CIFAR-100, Oxford-102 flowers, Stanford Cars and iNaturalist-18/19." (Introduction) and "We evaluated this on transfer learning tasks by fine-tuning on the datasets in Table 6." (Section 5.4, Transfer learning) |
| Transfer learning: iNaturalist-2018 classification | Classification | iNaturalist 2018 | RGB images | "Our models pre-learned on Imagenet are competitive when transferred to different downstream tasks such as fine-grained classification, on several popular public benchmarks: CIFAR-10, CIFAR-100, Oxford-102 flowers, Stanford Cars and iNaturalist-18/19." (Introduction) and "We evaluated this on transfer learning tasks by fine-tuning on the datasets in Table 6." (Section 5.4, Transfer learning) |
| Transfer learning: iNaturalist-2019 classification | Classification | iNaturalist 2019 | RGB images | "Our models pre-learned on Imagenet are competitive when transferred to different downstream tasks such as fine-grained classification, on several popular public benchmarks: CIFAR-10, CIFAR-100, Oxford-102 flowers, Stanford Cars and iNaturalist-18/19." (Introduction) and "We evaluated this on transfer learning tasks by fine-tuning on the datasets in Table 6." (Section 5.4, Transfer learning) |

# 4. Domain and Modality Scope
- Modality: RGB images. Evidence: "The fixed-size input RGB image is decomposed into a batch of N patches of a fixed size of 16 × 16 pixels (N = 14 × 14)." (Section 3, Vision transformer: overview).
- Domain scope: Multiple datasets within the same image modality. Evidence: "We evaluated this on transfer learning tasks by fine-tuning on the datasets in Table 6." and the dataset list in Table 6 (Section 5.4, Transfer learning).
- Domain generalization / cross-domain transfer claims: The paper describes transfer learning across datasets but does not explicitly claim domain generalization or cross-domain transfer. Evidence: "Our models pre-learned on Imagenet are competitive when transferred to different downstream tasks such as fine-grained classification, on several popular public benchmarks: CIFAR-10, CIFAR-100, Oxford-102 flowers, Stanford Cars and iNaturalist-18/19." (Introduction). Not claimed beyond this statement.

# 5. Model Sharing Across Tasks

| Task | Shared Weights? | Fine-Tuned? | Separate Head? | Evidence |
| --- | --- | --- | --- | --- |
| ImageNet classification | Trained per task (ImageNet only) | Not specified in the paper | Not specified in the paper | "Our reference vision transformer (86M parameters) achieves top-1 accuracy of 83.1% (single-crop) on ImageNet with no external data." (Abstract) and "It uses Imagenet as the sole training set." (Introduction) |
| ImageNet-Real evaluation | Not specified in the paper (reported as additional evaluation) | Not specified in the paper | Not specified in the paper | "Table 5 reports the numerical results in more details and additional evaluations on ImageNet V2 and ImageNet Real, that have a test set distinct from the ImageNet validation..." (Section 5.3) |
| ImageNet-V2 evaluation | Not specified in the paper (reported as additional evaluation) | Not specified in the paper | Not specified in the paper | "Table 5 reports the numerical results in more details and additional evaluations on ImageNet V2 and ImageNet Real..." (Section 5.3) |
| CIFAR-10 (transfer learning) | ImageNet-pretrained weights then fine-tuned per task | Yes | Not specified in the paper | "We evaluated this on transfer learning tasks by fine-tuning on the datasets in Table 6." (Section 5.4) and "Table 7: We compare Transformers based models on different transfer learning task with ImageNet pre-training." (Section 5.4) |
| CIFAR-100 (transfer learning) | ImageNet-pretrained weights then fine-tuned per task | Yes | Not specified in the paper | "We evaluated this on transfer learning tasks by fine-tuning on the datasets in Table 6." (Section 5.4) and "Table 7: We compare Transformers based models on different transfer learning task with ImageNet pre-training." (Section 5.4) |
| Flowers-102 (transfer learning) | ImageNet-pretrained weights then fine-tuned per task | Yes | Not specified in the paper | "We evaluated this on transfer learning tasks by fine-tuning on the datasets in Table 6." (Section 5.4) and "Table 7: We compare Transformers based models on different transfer learning task with ImageNet pre-training." (Section 5.4) |
| Stanford Cars (transfer learning) | ImageNet-pretrained weights then fine-tuned per task | Yes | Not specified in the paper | "We evaluated this on transfer learning tasks by fine-tuning on the datasets in Table 6." (Section 5.4) and "Table 7: We compare Transformers based models on different transfer learning task with ImageNet pre-training." (Section 5.4) |
| iNaturalist 2018 (transfer learning) | ImageNet-pretrained weights then fine-tuned per task | Yes | Not specified in the paper | "We evaluated this on transfer learning tasks by fine-tuning on the datasets in Table 6." (Section 5.4) and "Table 7: We compare Transformers based models on different transfer learning task with ImageNet pre-training." (Section 5.4) |
| iNaturalist 2019 (transfer learning) | ImageNet-pretrained weights then fine-tuned per task | Yes | Not specified in the paper | "We evaluated this on transfer learning tasks by fine-tuning on the datasets in Table 6." (Section 5.4) and "Table 7: We compare Transformers based models on different transfer learning task with ImageNet pre-training." (Section 5.4) |

Additional training regime explicitly reported:
- CIFAR-10 trained from scratch (no ImageNet pretraining). Evidence: "We investigate the performance when training from scratch on a small dataset, without Imagenet pre-training. We get the following results on the small CIFAR-10..." (Section 5.4, Transfer learning).

# 6. Input and Representation Constraints
- Fixed-size RGB inputs at base resolution: "The fixed-size input RGB image is decomposed into a batch of N patches of a fixed size of 16 × 16 pixels (N = 14 × 14)." (Section 3).
- Fixed patch size: "fixed size of 16 × 16 pixels" (Section 3).
- Fixed patch embedding dimensionality: "Each patch is projected with a linear layer that conserves its overall dimension 3 × 16 × 16 = 768." (Section 3).
- Token count per image depends on resolution (variable N): "When increasing the resolution of an input image, we keep the patch size the same, therefore the number N of input patches does change." (Section 3).
- Token count for classification at a given resolution: "The transformer thus process batches of (N + 1) tokens of dimension D" (Section 3).
- Model dimension (example for DeiT-B): "The parameters of ViT-B (and therefore of DeiT-B) are fixed as D = 768, h = 12 and d = D/h = 64." (Section 5.1).
- Training/fine-tuning resolution choices: "By default and similar to ViT [15] we train DeiT models with at resolution 224 and we fine-tune at resolution 384." (Section 6, Training details & ablation).
- Explicit resizing in a transfer experiment: "we rescale images to 224 × 224 to ensure that we have the same augmentation." (Section 5.4, Transfer learning).
- Padding requirements: Not specified in the paper.

# 7. Context Window and Attention Structure
- Maximum sequence length: Not specified in the paper. The token count is tied to resolution; e.g., "The fixed-size input RGB image is decomposed into a batch of N patches of a fixed size of 16 × 16 pixels (N = 14 × 14)." and "The transformer thus process batches of (N + 1) tokens of dimension D" (Section 3).
- Sequence length fixed or variable: Variable with resolution. Evidence: "When increasing the resolution of an input image, we keep the patch size the same, therefore the number N of input patches does change." (Section 3).
- Attention type: Global self-attention over all tokens. Evidence: "with the constraint k = N , meaning that the attention is in between all the input vectors." (Section 3, Vision transformer: overview).
- Mechanisms to manage computational cost (windowing, pooling, sparse attention): The paper highlights lower-resolution training followed by higher-resolution fine-tuning to speed training: "Touvron et al. [50] show that it is desirable to use a lower training resolution and fine-tune the network at the larger resolution. This speeds up the full training and improves the accuracy under prevailing data augmentation schemes." (Section 3). Windowed/sparse attention mechanisms are not specified in the paper.

# 8. Positional Encoding (Critical Section)
- Positional encoding mechanism: Positional embeddings are used; the paper mentions fixed or trainable embeddings but does not specify which is used in all experiments. Evidence: "The positional information is incorporated as fixed [52] or trainable [18] positional embeddings." (Section 3).
- Where applied: Input-level addition before the first transformer block. Evidence: "They are added before the first transformer block to the patch tokens, which are then fed to the stack of transformer blocks." (Section 3).
- Changes across resolutions: Positional embeddings are adapted/interpolated when changing resolution. Evidence: "Dosovitskiy et al. [15] interpolate the positional encoding when changing the resolution and demonstrate that this method works with the subsequent fine-tuning stage." (Section 3) and "Therefore we adopt a bicubic interpolation that approximately preserves the norm of the vectors, before fine-tuning the network..." (Section 6).
- Fixed across all experiments vs modified per task: Modified when resolution changes (interpolation); no per-task modifications explicitly described. (See quotes above.)
- Ablations/alternatives: Not specified in the paper.

# 9. Positional Encoding as a Variable
- Role of positional encoding: Described as a standard architectural component; not presented as a core research variable. Evidence: "The positional information is incorporated as fixed [52] or trainable [18] positional embeddings." (Section 3).
- Multiple positional encodings compared: Not specified in the paper.
- Claims that PE choice is not critical/secondary: Not specified in the paper.

# 10. Evidence of Constraint Masking (Scale vs Structure)
- Model size evidence: "Our reference vision transformer (86M parameters) achieves top-1 accuracy of 83.1% (single-crop) on ImageNet with no external data." (Abstract).
- Dataset size evidence (transfer tasks): "Table 6: Datasets used for our different tasks." and the dataset list, e.g., "ImageNet [42]" and "1,281,167" (train size) and "CIFAR-10 [31]" and "50,000" (train size). (Section 5.4, Table 6).
- Scaling data vs training tricks: The paper emphasizes data-efficient training and distillation rather than scaling data. Evidence: "In this work, we produce competitive convolution-free transformers by training on Imagenet only." (Abstract) and "More importantly, we introduce a teacher-student strategy specific to transformers. It relies on a distillation token ensuring that the student learns from the teacher through attention." (Abstract).
- Training tricks as primary drivers: "Compared to models that integrate more priors (such as convolutions), transformers require a larger amount of data. Thus, in order to train with datasets of the same size, we rely on extensive data augmentation." (Section 6, Training details & ablation).
- Architectural hierarchy as primary driver: Not specified in the paper.

# 11. Architectural Workarounds
- Fixed patching/grid assumption: "The fixed-size input RGB image is decomposed into a batch of N patches of a fixed size of 16 × 16 pixels (N = 14 × 14)." (Section 3).
- Class token for classification: "The class token is a trainable vector, appended to the patch tokens before the first layer, that goes through the transformer layers, and is then projected with a linear layer to predict the class." (Section 3).
- Distillation token (new architectural element): "We add a new token, the distillation token, to the initial embeddings (patches and class token)." (Section 4, Distillation through attention) and "It interacts with the class and patch tokens through the self-attention layers." (Section 4).
- Positional embedding interpolation for resolution changes: "When increasing the resolution of an input image... one needs to adapt the positional embeddings" (Section 3) and "Therefore we adopt a bicubic interpolation that approximately preserves the norm of the vectors, before fine-tuning the network..." (Section 6).
- Separate heads for class/distillation embeddings (within a task): "At test time, both the class or the distillation embeddings produced by the transformer are associated with linear classifiers and able to infer the image label." (Section 4).

# 12. Explicit Limitations and Non-Claims
- Stated future work direction: "Therefore it is likely that research on dataaugmentation more adapted or learned for transformers will bring further gains." (Conclusion).
- Explicit non-claims about scope (e.g., open-world, multi-domain/multi-task learning): Not specified in the paper.
