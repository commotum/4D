                                   Generative Pretraining from Pixels


    Mark Chen 1 Alec Radford 1 Rewon Child 1 Jeff Wu 1 Heewoo Jun 1 Prafulla Dhariwal 1 David Luan 1
                                            Ilya Sutskever 1


                        Abstract                                 ported strong results using a single layer of learned features
     Inspired by progress in unsupervised representa-            (Coates et al., 2011), or even random features (Huang et al.,
     tion learning for natural language, we examine              2014; May et al., 2017). The approach fell out of favor as
     whether similar models can learn useful repre-              the state of the art increasingly relied on directly encoding
     sentations for images. We train a sequence Trans-           prior structure into the model and utilizing abundant su-
     former to auto-regressively predict pixels, without         pervised data to directly learn representations (Krizhevsky
     incorporating knowledge of the 2D input structure.          et al., 2012; Graves & Jaitly, 2014). Retrospective study of
     Despite training on low-resolution ImageNet with-           unsupervised pre-training demonstrated that it could even
     out labels, we find that a GPT-2 scale model learns         hurt performance in modern settings (Paine et al., 2014).
     strong image representations as measured by lin-            Instead, unsupervised pre-training flourished in a differ-
     ear probing, fine-tuning, and low-data classifica-          ent domain. After initial strong results for word vectors
     tion. On CIFAR-10, we achieve 96.3% accuracy                (Mikolov et al., 2013), it has pushed the state of the art
     with a linear probe, outperforming a supervised             forward in Natural Language Processing on most tasks (Dai
     Wide ResNet, and 99.0% accuracy with full fine-             & Le, 2015; Peters et al., 2018; Howard & Ruder, 2018;
     tuning, matching the top supervised pre-trained             Radford et al., 2018; Devlin et al., 2018). Interestingly, the
     models. An even larger model trained on a mix-              training objective of a dominant approach like BERT, the
     ture of ImageNet and web images is competitive              prediction of corrupted inputs, closely resembles that of the
     with self-supervised benchmarks on ImageNet,                Denoising Autoencoder, which was originally developed for
     achieving 72.0% top-1 accuracy on a linear probe            images.
     of our features.
                                                                 As a higher dimensional, noisier, and more redundant modal-
                                                                 ity than text, images are believed to be difficult for genera-
                                                                 tive modeling. Here, self-supervised approaches designed to
1. Introduction                                                  encourage the modeling of more global structure (Doersch
Unsupervised pre-training played a central role in the resur-    et al., 2015) have shown significant promise. A combination
gence of deep learning. Starting in the mid 2000’s, ap-          of new training objectives (Oord et al., 2018), more recent
proaches such as the Deep Belief Network (Hinton et al.,         architectures (Gomez et al., 2017), and increased model ca-
2006) and Denoising Autoencoder (Vincent et al., 2008)           pacity (Kolesnikov et al., 2019) has allowed these methods
were commonly used in neural networks for computer vi-           to achieve state of the art performance in low data settings
sion (Lee et al., 2009) and speech recognition (Mohamed          (Hénaff et al., 2019) and sometimes even outperform super-
et al., 2009). It was believed that a model which learned        vised representations in transfer learning settings (He et al.,
the data distribution P (X) would also learn beneficial fea-     2019; Misra & van der Maaten, 2019; Chen et al., 2020).
tures for the subsequent supervised modeling of P (Y |X)         Given that it has been a decade since the original wave of
(Lasserre et al., 2006; Erhan et al., 2010). However, advance-   generative pre-training methods for images and considering
ments such as piecewise linear activation functions (Nair        their substantial impact in NLP, this class of methods is due
& Hinton, 2010), improved initializations (Glorot & Ben-         for a modern re-examination and comparison with the recent
gio, 2010), and normalization strategies (Ioffe & Szegedy,       progress of self-supervised methods. We re-evaluate genera-
2015; Ba et al., 2016) removed the need for pre-training in      tive pre-training on images and demonstrate that when using
order to achieve strong results. Other research cast doubt       a flexible architecture (Vaswani et al., 2017), a tractable and
on the benefits of deep unsupervised representations and re-     efficient likelihood based training objective (Larochelle &
  1
    OpenAI, San Francisco, CA, USA. Correspondence to: Mark      Murray, 2011; Oord et al., 2016), and significant compute
Chen <mark@openai.com>.                                          resources (2048 TPU cores), generative pre-training is com-
                                                                 petitive with other self-supervised approaches and learns
                                                 Generative Pretraining from Pixels




Figure 1. An overview of our approach. First, we pre-process raw images by resizing to a low resolution and reshaping into a 1D sequence.
We then chose one of two pre-training objectives, auto-regressive next pixel prediction or masked pixel prediction. Finally, we evaluate
the representations learned by these objectives with linear probes or fine-tuning.
representations that significantly improve the state of the           for the downstream task rather than because of better pre-
art in low-resolution unsupervised representation learning            training.
settings.
                                                                      We begin this section by defining the auto-regressive and
This is especially promising as our architecture uses a dense         BERT objectives in the context of images. Next, we outline
connectivity pattern which does not encode the 2D spatial             implementation details for our transformer decoder. Finally,
structure of images yet is able to match and even outperform          we describe how the transformer is used for fine-tuning and
approaches which do. We report a set of experiments charac-           how features are extracted for linear probes.
terizing the performance of our approach on many datasets
and in several different evaluation settings (low data, linear        2.1. Pre-training
evaluation, full fine-tuning). We also conduct several exper-         Given an unlabeled dataset X consisting of high dimen-
iments designed to better understand the achieved perfor-             sional data x = (x1 , ..., xn ), we can pick a permutation π
mance of these models. We investigate how representations             of the set [1, n] and model the density p(x) auto-regressively
are computed inside our model via the performance of linear           as follows:
probes as a function of model depth as well as studying how                                 Yn
scaling the resolution and parameter count of the approach                          p(x) =     p(xπi |xπ1 , ..., xπi−1 , θ)
affects performance.                                                                        i=1
                                                                      When working with images, we pick the identity permuta-
2. Approach                                                           tion πi = i for 1 ≤ i ≤ n, also known as raster order. We
                                                                      train our model by minimizing the negative log-likelihood
Our approach consists of a pre-training stage followed by
                                                                      of the data:
a fine-tuning stage. In pre-training, we explore both the
auto-regressive and BERT objectives. We also apply the                                   LAR = E [− log p(x)]
                                                                                                   x∼X
sequence Transformer architecture to predict pixels instead
of language tokens.                                                   We also consider the BERT objective, which samples a
                                                                      sub-sequence M ⊂ [1, n] such that each index i indepen-
One way to measure representation quality is to fine-tune for         dently has probability 0.15 of appearing in M . We call M
image classification. Fine-tuning adds a small classification         the BERT mask, and we train our model by minimizing
head to the model, used to optimize a classification objective        the negative log-likelihood of the “masked” elements xM
and adapts all weights. Pre-training can be viewed as a               conditioned on the “unmasked” ones x[1,n]\M :
favorable initialization or as a regularizer when used in                                      X                         
combination with early stopping (Erhan et al., 2010).                       LBERT = E E              − log p xi |x[1,n]\M
                                                                                          x∼X M
                                                                                                  i∈M
Another approach for measuring representation quality uses
the pre-trained model as a feature extractor. In particular,          In pre-training, we pick one of LAR or LBERT and mini-
given labeled examples (X, Y ), the model is applied to X             mize the loss over our pre-training dataset.
to produce features fX . Then, a linear classifier is trained         2.2. Architecture
on (fX , Y ). Linear probing captures the intuition that good
features should linearly separate the classes of transfer tasks.      The transformer decoder takes an input sequence x1 , ..., xn
Furthermore, linear probes help disentangle feature quality           of discrete tokens and produces a d-dimensional embedding
from model architecture: in fine-tuning, one model may                for each position. The decoder is realized as a stack of
outperform another because its architecture is more suited            L blocks, the l-th of which produces an intermediate em-
                                                                      bedding hl1 , ..., hln also of dimension d. We use the GPT-2
                                                 Generative Pretraining from Pixels

(Radford et al., 2019) formulation of the transformer de-           always at the final layer:
coder block, which acts on an input tensor hl as follows:
                                                                                             f l = hnli ii
               nl = layer norm(hl )
               al = hl + multihead attention(nl )                   where 0 ≤ l ≤ L. We will show in the experiments section
             l+1      l                      l                      that the best features often lie in the middle of the network.
           h       = a + mlp(layer norm(a ))                        As in fine-tuning, we project these intermediate features
In particular, layer norms precede both the attention and           to produce class logits. Because we view the features as
mlp operations, and all operations lie strictly on residual         fixed when linear probing, this projection contains the only
paths. We find that such a formulation allows us to scale the       trainable weights, so we can only optimize LCLF .
transformer with ease.
                                                                    3. Methodology
The only mixing across sequence elements occurs in the
attention operation, and to ensure proper conditioning when         Although supervised pre-training is the dominant paradigm
training the AR objective, we apply the standard upper              for image classification, curating large labeled image
triangular mask to the n×n matrix of attention logits. When         datasets is both expensive and time consuming. Instead
using the BERT objective, no attention logit masking is             of further scaling up labeling efforts, we can instead as-
required: after applying content embeddings to the input            pire to learn general purpose representations from the much
sequence, we zero out the positions in M .                          larger set of available unlabeled images and fine-tune them
                                                                    for classification. We investigate this setting using Ima-
Additionally, since we learn independent position embed-
                                                                    geNet as a proxy for a large unlabeled corpus, and small
dings for each sequence element, our BERT model has no
                                                                    classic labeled datasets (CIFAR-10, CIFAR-100, STL-10)
positional inductive biases (i.e. it is permutation invariant).
                                                                    as proxies for downstream tasks. For our largest model, we
Put another way, any spatial relationships between posi-
                                                                    use an additional 100 million unlabeled web images, filtered
tions must be learned by the model at train time. This is
                                                                    to be similar to ImageNet.
not entirely true for the AR model, as choosing the raster
order also fixes a prespecified ordering of the condition-          Even in cases where labels are available, unsupervised or
als. Nevertheless, permutation invariance is a property in          self-supervised pre-training can still provide benefits in data
strong contrast to convolutional neural networks, which in-         efficiency or on fine-tuning speed. We investigate this set-
corporate the inductive bias that features should arise from        ting by pre-training without labels and then fine-tuning or
spatially proximate elements.                                       linear probing with labels.
Following the final transformer layer, we apply a layer norm        3.1. Dataset and Data Augmentation
nL = layer norm(hL ), and learn a projection from nL to
logits parameterizing the conditional distributions at each         We use the ImageNet ILSVRC 2012 training dataset, split-
sequence element. When training BERT, we simply ignore              ting off 4% as our experimental validation set and report
the logits at unmasked positions.                                   results on the ILSVRC 2012 validation set as our test set.
                                                                    For CIFAR-10, CIFAR-100 and STL-10, we split off 10%
2.3. Fine-tuning                                                    of the provided training set instead. We ignore the provided
                                                                    unlabeled examples in STL-10, which constitute a subset of
When fine-tuning, we average pool nL across the sequence
                                                                    ImageNet.
dimension to extract a d-dimensional vector of features per
example:                                                            No data augmentation is used when pre-training on web
                        f L = hnL
                                i ii                                images, and lightweight data augmentation is used when
We learn a projection from f L to class logits, which we use        pre-training or fine-tuning on ImageNet. Specifically, when
to minimize a cross entropy loss LCLF .                             employing data augmentation, we randomly resize an image
                                                                    such that the shorter sidelength is in the range [256, 384]
While fine-tuning on LCLF yields reasonable downstream              and then take a random 224 × 224 crop. When evaluating
performance, we find empirically that the joint objective           on ImageNet, we resize the image such that the shorter
                          LGEN + LCLF                               sidelength is 224, and use the single 224 × 224 center crop.

LGEN ∈ {LAR , LBERT } works even better. Similar find-              When full-network fine-tuning on CIFAR-10 and CIFAR-
ings were reported by Radford et al. (2018).                        100, we use the augmentation popularized by Wide Residual
                                                                    Networks: 4 pixels are reflection padded on each side, and
2.4. Linear Probing                                                 a 32 × 32 crop is randomly sampled from the padded image
                                                                    or its horizontal flip (Zagoruyko & Komodakis, 2016).
Extracting fixed features for linear probing follows a similar
procedure to fine-tuning, except that average pooling is not        Once optimal hyperparameters are found, we fold our ex-
                                                Generative Pretraining from Pixels

perimental validation set back into the training set, retrain       a cosine schedule. No dropout is used.
the model, and report numbers on the respective test set.
                                                                    When fine-tuning, we use the same batch size and Adam
3.2. Context Reduction                                              hyperparameters. Here, we do not employ a cosine sched-
                                                                    ule, and early stop once we reach the maximum validation
Because the memory requirements of the transformer de-              accuracy. Again, no dropout is used.
coder scale quadratically with context length when using
dense attention, we must employ further techniques to re-           When running a linear probe on ImageNet, we follow recent
duce context length. If we naively trained a transformer on         literature and use SGD with momentum 0.9 and a high
a sequence of length 2242 × 3, our attention logits would be        learning rate (we try the values 30, 10, 3, ... in the manner
tens of thousands of times larger than those used in language       described above) (He et al., 2019). We train for 1000000
models and even a single layer would not fit on a GPU. To           iterations with a cosine learning rate schedule. Finally, when
deal with this, we first resize our image to a lower resolution,    running a linear probe on CIFAR-10, CIFAR-100, or STL-
which we call the input resolution (IR). Our models have            10, we use the L-BFGS algorithm for consistency with prior
IRs of either 322 × 3, 482 × 3, or 642 × 3.                         results (Pedregosa et al., 2011).

An IR of 322 × 3 is still quite computationally intensive.          4. Experiments and Results
While working at even lower resolutions is tempting, prior
work has demonstrated human performance on image classi-            We begin with experiments and results from the autore-
fication begins to drop rapidly below this size (Torralba et al.,   gressive formulation of iGPT. Comparisons with the BERT
2008). Instead, motivated by early color display palettes,          formulation appear in Section 4.6.
we create our own 9-bit color palette by clustering (R, G,
                                                                    4.1. What Representation Works Best in a Generative
B) pixel values using k-means with k = 512. Using this
                                                                         Model Without Latent Variables?
palette yields an input sequence length 3 times shorter than
the standard (R, G, B) palette, while still encoding color
faithfully. A similar approach was applied to spatial patches
by Ranzato et al. (2014). We call the resulting context length
(322 or 482 or 642 ) the model resolution (MR). Note that
this reduction breaks permutation invariance of the color
channels, but keeps the model spatially invariant.
3.3. Model
Our largest model, iGPT-XL, contains L = 60 layers and
uses an embedding size of d = 3072 for a total of 6.8B pa-
rameters. Our next largest model, iGPT-L, is essentially            Figure 2. Representation quality depends on the layer from which
identical to GPT-2 with L = 48 layers, but contains a               we extract features. In contrast with supervised models, the best
slightly smaller embedding size of d = 1536 (vs 1600)               representations for these generative models lie in the middle of the
for a total of 1.4M parameters. We use the same model               network. We plot this unimodal dependence on depth by showing
code as GPT-2, except that we initialize weights in the layer-      linear probes for iGPT-L on CIFAR-10, CIFAR-100, and STL-10.
dependent fashion as in Sparse Transformer (Child et al.,
2019) and zero-initialize all projections producing logits.         In supervised pre-training, representation quality tends to
                                                                    increase monotonically with depth, such that the best rep-
We also train iGPT-M, a 455M parameter model with L =
                                                                    resentations lie at the penultimate layer (Zeiler & Fergus,
36 and d = 1024 and iGPT-S, a 76M parameter model with
                                                                    2014). Indeed, since a linear layer produces class logits
L = 24 and d = 512 to study the effect of model capacity
                                                                    from pre-logits, a good classifier necessarily achieves high
on representation quality in a generative model.
                                                                    accuracy on a linear probe of its pre-logits. If a downstream
3.4. Training                                                       task also involves classification, it is empirically validated
                                                                    that penultimate features perform well.
When pre-training iGPT-XL, we use a batch size of 64 and
train for 2M iterations, and for all other models we use            With generative pre-training, it is not obvious whether a task
a batch size of 128 and train for 1M iterations. We use             like pixel prediction is relevant to image classification. This
Adam with β1 = 0.9 and β2 = 0.95 and sequentially try the           suggests that the penultimate layer of a model trained for
learning rates 0.01, 0.003, 0.001, 0.0003, ..., stopping once       pixel prediction might not produce the most useful repre-
the final validation loss starts increasing. The learning rate      sentations for classification. Latent variable models such as
is warmed up for one epoch, and then decays to 0 following          VAEs can avoid this issue by explicitly learning a represen-
                                                                    tation of the input data, but deep autoregressive generative
                                                  Generative Pretraining from Pixels

models have the same width and connectivity pattern at
                                                                       Table 1. Comparing linear probe accuracies between our models
every layer. Our first experiment studies how representa-
                                                                       and state-of-the-art models utilizing unsupervised ImageNet trans-
tion quality varies over one set of candidate representations:         fer or supervised ImageNet transfer.
different layers of a generative model. We observe a very
different behavior from supervised learning: representations                Model           Acc    Unsup Transfer     Sup Transfer
first improve as a function of depth, and then, starting around
the middle layer, begin to deteriorate until the penultimate                CIFAR-10                                       √
                                                                            ResNet-152      94            √
layer (Figure 2).                                                           SimCLR         95.3           √
This behavior potentially suggests that these generative mod-               iGPT-L         96.3
els operate in two phases. In the first phase, each position                CIFAR-100                                      √
gathers information from its surrounding context in order                   ResNet-152     78.0           √
to build a more global image representation. In the second                  SimCLR         80.2           √
phase, this contextualized input is used to solve the condi-                iGPT-L         82.8
tional next pixel prediction task. This could resemble the
                                                                            STL-10                        √
behavior of encoder-decoder architectures common across                     AMDIM-L        94.2           √
deep learning, but learned within a monolithic architecture                 iGPT-L         95.5
via a pre-training objective.
Consequently, when evaluating a generative model with
                                                                       several model capacities, with higher capacity models
a linear probe, it is important to search for the best layer.
                                                                       achieving better validation losses. This highlights the im-
Taking the final layer on CIFAR-10 decreases performance
                                                                       portance of scale for our approach. Note that for a given
by 2.4%, the difference between a baseline and a state-of-
                                                                       validation loss value, bigger models also perform better.
the-art result. In all settings, we find that the dependence of
representation quality on depth is strongly unimodal.                  4.3. Linear Probes on CIFAR and STL-10
4.2. Better Generative Models Learn Better                             In addition to CIFAR-10, we also evaluate linear probes on
     Representations                                                   CIFAR-100 and STL-10 (Figure 2) to check whether the
                                                                       learned representations are useful across multiple datasets.
                                                                       For this evaluation setting, we achieve state-of-the-art across
                                                                       the entire spectrum of pre-training approaches (Table 1).
                                                                       For example, on CIFAR-10, our model achieves 96.3%, out-
                                                                       performing both SimCLR (pre-trained on ImageNet without
                                                                       labels) and a ResNet-152 (pre-trained on ImageNet with
                                                                       labels). In fact, on all three datasets a linear classifier fit to
                                                                       the representations of iGPT-L outperforms the end-to-end
                                                                       supervised training of a WideResNet baseline.
                                                                       Note that our model is trained at the same input resolution
Figure 3. Plot of representation quality as a function of validation
                                                                       (IR) as CIFAR, whereas models trained at the standard Im-
generative loss. Each line tracks a model throughout generative        ageNet IR may experience distribution shock upon linear
pre-training: the dotted markers denote checkpoints at steps 65K,      evaluation. As a counterpoint, though STL-10 has an IR
131K, 262K, 524K, and 1000K. The positive slope suggests a link        of 962 × 3, we still outperform AMDIM-L when we down-
between improved generative performance and improved represen-         sample to 322 × 3 before linear probing. We also note that
tation quality. Larger models produce better representations than      fine-tuning should allow models trained at high IR to adjust
smaller ones both at the end of training and at the same value of      to low resolution input.
validation loss. iGPT-XL is not shown since it was trained on a
different dataset.                                                     4.4. Linear Probes on ImageNet
                                                                       Recently, there has been a resurgence of interest in unsuper-
Using the linear probe as a tool for measuring representation
                                                                       vised and self-supervised learning on ImageNet, evaluated
quality, we investigate whether better generative models (as
                                                                       using linear probes on ImageNet. This is a particularly
measured by log-prob on held-out data) also learn better
                                                                       difficult setting for us, since we cannot efficiently train at
representations.
                                                                       the standard ImageNet input resolution (IR). Indeed, when
In Figure 3, we see that as validation loss on the auto-               training iGPT-L with a model resolution (MR) of 322 , we
regressive objective decreases throughout training, linear             achieve only 60.3% best-layer linear probe accuracy. As
probe accuracy increases as well. This trend holds across              with CIFAR-10, scale is critical to our approach: iGPT-
                                                 Generative Pretraining from Pixels


Table 2. Comparing linear probe accuracies between our models        Table 3. Comparing fine-tuning performance between our models
and state-of-the-art self-supervised models. A blank input resolu-   and state-of-the-art models utilizing supervised ImageNet transfer.
tion (IR) corresponds to a model working at standard ImageNet        We also include AutoAugment, the best performing model trained
resolution. We report the best performing configuration for each     end-to-end on CIFAR. Table results: AutoAugment (Cubuk et al.,
contrastive method, finding that our models achieve comparable       2019), SimCLR (Chen et al., 2020), GPipe (Huang et al., 2019),
performance.                                                         EfficentNet (Tan & Le, 2019)

     Method          IR      Params (M)     Features    Acc              Model             Acc     Unsup Transfer     Sup Transfer
     Rotation       orig.         86          8192      55.4             CIFAR-10
     iGPT-L        322 ·3       1362          1536      60.3             AutoAugment       98.5          √
     BigBiGAN       orig.         86          8192      61.3             SimCLR            98.6                            √
     iGPT-L        482 ·3       1362          1536      65.2             GPipe             99.0          √
     AMDIM          orig.        626          8192      68.1             iGPT-L            99.0
     MoCo           orig.        375          8192      68.6
     iGPT-XL       642 ·3       6801          3072      68.7             CIFAR-100                       √
     SimCLR         orig.         24          2048      69.3             iGPT-L            88.5          √
     CPC v2         orig.        303          8192      71.5             SimCLR            89.0
     iGPT-XL       642 ·3       6801         15360      72.0             AutoAugment       89.3                            √
     SimCLR         orig.        375          8192      76.5             EfficientNet      91.7


                                                                     on these datasets, though we do not use sophisticated data
M achieves 54.5% accuracy and iGPT-S achieves 41.9%
                                                                     augmentation techniques. In fact, 99.0% ties GPipe, the best
accuracy.
                                                                     model which pre-trains using ImageNet labels.
The first obvious optimization is to increase MR while stay-
                                                                     On ImageNet, we achieve 66.3% accuracy after fine-tuning
ing within accelerator memory limits. With a MR of 482 ,
                                                                     at MR 322 , a bump of 6% over linear probing. When fine-
iGPT-L achieves a best-layer accuracy of 65.2% using 1536
                                                                     tuning at MR 482 , we achieve 72.6% accuracy, with a simi-
features and with a MR of 642 , iGPT-XL achieves a best-
                                                                     lar 7% bump over linear probing. However, our models still
layer accuracy of 68.7% using 3072 features.
                                                                     slightly underperform Isometric Neural Nets (Sandler et al.,
Since contrastive methods report their best results on 8192          2019), which achieves 70.2% at an IR of 282 × 3.
features, we would ideally evaluate iGPT with an embed-
                                                                     Finally, as a baseline for ImageNet fine-tuning, we train
ding dimension 8192 for comparison. Training such a model
                                                                     the classification objective from a random initialization. At
is prohibitively expensive, so we instead concatenate fea-
                                                                     MR 482 , a model with tuned learning rate and dropout
tures from multiple layers as an approximation. However,
                                                                     achieves 53.2% after 18 epochs, 19.4% worse than the pre-
our features tend to be correlated across layers, so we need
                                                                     trained model. Comparatively, the pre-trained model is
more of them to be competitive. If we concatenate features
                                                                     much quicker to fine-tune, achieving the same 53.2% loss
from 5 layers centered at the best single layer of iGPT-XL,
                                                                     in roughly a single epoch.
we achieve an accuracy of 72.0% using 15360 features,
which is competitive with recent contrastive learning ap-            When fine-tuning, it is important to search over learning
proaches (Table 2). Note that we require more parameters             rates again, as the optimal learning rate on the joint training
and compute to achieve this accuracy, but we work at low             objective is often an order of magnitude smaller than that
resolution and without utilizing knowledge of the 2D input           for pre-training. We also tried regularizing with dropout,
structure.                                                           though we did not observe any clear benefits. It is easy to
                                                                     overfit the classification objective on small datasets, so we
4.5. Full Fine-tuning                                                employ early stopping based on validation accuracy.
To achieve even higher accuracy on downstream tasks, we
                                                                     4.6. BERT
adapt the entire model for classification through fine-tuning.
Building off of the previous analysis, we tried attaching the        Given the success of BERT in language, we train iGPT-L
classification head to the layer with the best representations.      at an input resolution of 322 × 3 and a model resolution
Though this setup trains faster than one with the head at-           of 322 (Figure 4). On CIFAR-10, we observe that linear
tached at the end, the latter is able to leverage greater model      probe accuracy at every layer is worse than that of the auto-
depth and eventually outperforms.                                    regressive model, with best-layer performance more than
                                                                     1% lower. Best-layer accuracy on ImageNet is 6% lower.
On CIFAR-10, iGPT-L achieves 99.0% accuracy and on
CIFAR-100, it achieves 88.5% accuracy after fine-tuning.             However, during fine-tuning, BERT makes up much of this
We outperform AutoAugment, the best supervised model                 gap. A fully fine-tuned CIFAR-10 model achieves 98.6%
                                                Generative Pretraining from Pixels


                                                                    Table 4. Comparing performance on low-data CIFAR-10. By lever-
                                                                    aging many unlabeled ImageNet images, iGPT-L is able to outper-
                                                                    form methods such as Mean Teacher (Tarvainen & Valpola, 2017)
                                                                    and MixMatch (Berthelot et al., 2019) but still underperforms the
                                                                    state of the art methods (Xie et al., 2019; Sohn et al., 2020). Our
                                                                    approach to semi-supervised learning is very simple since we only
                                                                    fit a logistic regression classifier on iGPT-L’s features without any
                                                                    data augmentation or fine-tuning - a significant difference from spe-
                                                                    cially designed semi-supervised approaches. Other results reported
                                                                    from FixMatch (Sohn et al., 2020).

                                                                       Model                40 labels     250 labels     4000 labels
                                                                       Mean Teacher                       32.3 ± 2.3      9.2 ± 0.2
Figure 4. Comparison of auto-regressive pre-training with BERT         MixMatch           47.5 ± 11.5     11.0 ± 0.9      6.4 ± 0.1
pre-training using iGPT-L at an input resolution of 322 × 3. Blue      iGPT-L              26.8 ± 1.5     12.4 ± 0.6      5.7 ± 0.1
bars display linear probe accuracy and orange bars display fine-       UDA                 29.0 ± 5.9      8.8 ± 1.1      4.9 ± 0.2
tune accuracy. Bold colors show the performance boost from             FixMatch RA         13.8 ± 3.4      5.1 ± 0.7      4.3 ± 0.1
ensembling BERT masks. We see that auto-regressive models              FixMatch CTA        11.4 ± 3.4      5.1 ± 0.3      4.3 ± 0.2
produce much better features than BERT models after pre-training,
but BERT models catch up after fine-tuning.
                                                                    As is standard in the low-data setting, we sample 5 random
                                                                    subsets and report mean and standard deviation accuracies
                                                                    (Table 4). On CIFAR-10, we find that with 4 labels per class,
accuracy, only 0.4% behind its auto-regressive counterpart,         we achieve 73.2% accuracy outperforming MixMatch with
while a fully fine-tuned ImageNet model achieves 66.5%,             much lower variance between runs and with 25 labels per
slightly surpassing auto-regressive performance.                    class, we achieve 87.6% accuracy, though still significantly
Finally, because inputs to the BERT model are masked at             lower than the state of the art, FixMatch.
training time, we must also mask them at evaluation time to         Although we have established that large models are neces-
keep inputs in-distribution. This masking corruption may            sary for producing good representations, large models are
hinder the BERT model’s ability to correctly predict image          also difficult to fine-tune in the ultra-low data regime. In-
classes. Therefore, we also try an evaluation scheme where          deed, we find that iGPT-L quickly memorizes a 40-example
we sample 5 independent masks for each input and take the           training set and fails to generalize well, achieving only
modal prediction, breaking ties at random. In this setting,         42.1% accuracy. We expect adapting recent approaches
CIFAR-10 results are largely unchanged, but on ImageNet,            to semi-supervised learning will help in this setting.
we gain almost 1% on our linear probes and fine-tunes.
                                                                    5. Related Work
4.7. Low-Data CIFAR-10 Classification
                                                                    Many generative models have been developed and evalu-
Evaluations of unsupervised representations often reuse su-
                                                                    ated for their representation learning capabilities. Notably,
pervised learning datasets which have thousands to millions
                                                                    GANs (Goodfellow et al., 2014; Radford et al., 2015; Don-
of labeled examples. However, a representation which has
                                                                    ahue et al., 2016) and VAEs (Kingma & Welling, 2013;
robustly encoded a semantic concept should be exceedingly
                                                                    Kingma et al., 2014; Higgins et al., 2017) have been well-
data efficient. As inspiration, we note that humans are able
                                                                    studied.
to reliably recognize even novel concepts with a single ex-
ample (Carey and Bartlett 1978). This motivates evaluating          As of yet, most generative model based approaches have
performance in a low-data regime as well. It is also a more         not been competitive with supervised and self-supervised
realistic evaluation setting for the potential practical use-       methods in the image domain. A notable exception is Big-
fulness of an approach since it better matches the common           BiGAN (Donahue & Simonyan, 2019) which first demon-
real-world scenario of an abundance of raw data but a lack          strated that sufficiently high fidelity generative models learn
of labels.                                                          image representations which are competitive with other self-
                                                                    supervised methods.
In contrast with recent approaches for low-data classifica-
tion, we do not make use of pseudo-labeling or data aug-            Many self-supervised approaches focus on designing aux-
mentation. Instead, we work directly on a subset of the raw         iliary objectives which support the learning of useful rep-
supervised dataset, extracting features using our pre-trained       resentations without attempting to directly model the input
model, and training a linear classifier on those features.          data. Examples include surrogate classification (Dosovit-
                                              Generative Pretraining from Pixels

skiy et al., 2015), jigsaw puzzle solving (Noroozi & Favaro,     However, our experiments also demonstrate several areas
2016), and rotation prediction (Gidaris et al., 2018). A clus-   for improvement. We currently model low resolution in-
ter of similar approaches based on contrastive losses com-       puts with self-attention. By comparison, most other self-
paring various views and transformations of input images         supervised results use CNN based encoders that easily work
have recently driven significant progress in self-supervised     with high resolution images. It is not immediately obvious
learning (Hjelm et al., 2018; Bachman et al., 2019; Tian         how to best bridge the gap between performant autoregres-
et al., 2019).                                                   sive and discriminative models. Additionally, we observed
                                                                 that our approach requires large models in order to learn
Among contrastive approaches, our work is most similar
                                                                 high quality representations. iGPT-L has 2 to 3 times as
to Contrast Predictive Coding (Oord et al., 2018) which
                                                                 many parameters as similarly performing models on Ima-
also utilizes a autoregressive prediction objective, but in a
                                                                 geNet and uses more compute.
learned latent space, and to Selfie (Trinh et al., 2019) which
trains a bidirectional self-attention architecture on top of a   Although dense self-attention was a deliberate choice for
standard convolutional network to differentiate correct vs       this work due to it being domain agnostic and widely used in
wrong patches.                                                   NLP, it becomes very memory and computationally expen-
                                                                 sive due to its quadratic scaling with sequence length. We
Our work is directly inspired by the success of generative
                                                                 mitigated this via the context reduction techniques discussed
pre-training methods developed for Natural Language Pro-
                                                                 in section 3.2 but it is still a significant limitation. Future
cessing. These methods predict some parts of a piece of text
                                                                 work could instead address this via architectural changes by
conditioned on other parts. Our work explores two training
                                                                 exploring more efficient self-attention approaches. Several
objectives in this framework, autoregressive prediction as
                                                                 promising techniques have recently been developed such as
originally explored for modern neural sequence models by
                                                                 local 2D relative attention (Bello et al., 2019; Ramachan-
Dai & Le (2015), and a denoising objective, similar to BERT
                                                                 dran et al., 2019), sparse attention patterns (Child et al.,
(Devlin et al., 2018). The context in-painting approach of
                                                                 2019), locality sensitive hashing (Kitaev et al., 2020), and
Pathak et al. (2016) also explores pre-training by predict-
                                                                 multiscale modeling (Menick & Kalchbrenner, 2018).
ing corruptions but predicts large regions of high-resolution
images.                                                          Finally, our results, considered together with Donahue &
                                                                 Simonyan (2019), suggest revisiting the representation learn-
Kolesnikov et al. (2019); Goyal et al. (2019) conducted
                                                                 ing capabilities of other families of generative models such
rigorous investigations of existing self-supervised methods.
                                                                 as flows (Dinh et al., 2014; Kingma & Dhariwal, 2018)
Several of our findings are consistent with their results, in-
                                                                 and VAEs in order to study whether they show similarly
cluding the benefits of scale and the non-monotonic perfor-
                                                                 competitive representation learning capabilities.
mance of representations with depth in certain architectures.
Expressive autoregressive models tractably optimizing like-      References
lihood were first applied to images by Uria et al. (2013)
and popularized by Oord et al. (2016) serving for the ba-        Ba, J. L., Kiros, J. R., and Hinton, G. E. Layer normalization.
sis of several papers similarly adapting transformers to the       arXiv preprint arXiv:1607.06450, 2016.
problem of generative image modeling (Parmar et al., 2018;
                                                                 Bachman, P., Hjelm, R. D., and Buchwalter, W. Learning
Child et al., 2019).
                                                                   representations by maximizing mutual information across
Ke et al. (2018) introduced the pixel-by-pixel CIFAR10 task        views. In Advances in Neural Information Processing
and first benchmarked the performance of a 1D sequence             Systems, pp. 15509–15519, 2019.
transformer on a competitive image classification dataset.
Rives et al. (2019) similarly investigates whether the recent    Bello, I., Zoph, B., Vaswani, A., Shlens, J., and Le, Q. V.
success of unsupervised pre-training in NLP applies to other       Attention augmented convolutional networks. In Proceed-
domains, observing promising results on protein sequence           ings of the IEEE International Conference on Computer
data.                                                              Vision, pp. 3286–3295, 2019.
                                                                 Berthelot, D., Carlini, N., Goodfellow, I., Papernot, N.,
6. Discussion and Conclusion                                       Oliver, A., and Raffel, C. A. Mixmatch: A holistic
Our results suggest that generative image modeling contin-         approach to semi-supervised learning. In Advances in
ues to be a promising route to learn high-quality unsuper-         Neural Information Processing Systems, pp. 5050–5060,
vised image representations. Simply predicting pixels learns       2019.
state of the art representations for low resolution datasets.    Chen, T., Kornblith, S., Norouzi, M., and Hinton, G. A
In high resolution settings, our approach is also competitive      simple framework for contrastive learning of visual rep-
with other self-supervised results on ImageNet.                    resentations. arXiv preprint arXiv:2002.05709, 2020.
                                              Generative Pretraining from Pixels

Child, R., Gray, S., Radford, A., and Sutskever, I. Gen-         Gomez, A. N., Ren, M., Urtasun, R., and Grosse, R. B. The
  erating long sequences with sparse transformers. arXiv           reversible residual network: Backpropagation without
  preprint arXiv:1904.10509, 2019.                                 storing activations. In Advances in neural information
                                                                   processing systems, pp. 2214–2224, 2017.
Coates, A., Ng, A., and Lee, H. An analysis of single-
  layer networks in unsupervised feature learning. In Pro-       Goodfellow, I., Pouget-Abadie, J., Mirza, M., Xu, B.,
  ceedings of the fourteenth international conference on          Warde-Farley, D., Ozair, S., Courville, A., and Bengio,
  artificial intelligence and statistics, pp. 215–223, 2011.      Y. Generative adversarial nets. In Advances in neural
                                                                   information processing systems, pp. 2672–2680, 2014.
Cubuk, E., Zoph, B., Mane, D., Vasudevan, V., and Le, Q. V.
  Autoaugment: Learning augmentation strategies from             Goyal, P., Mahajan, D., Gupta, A., and Misra, I. Scaling and
  data, 2019.                                                      benchmarking self-supervised visual representation learn-
                                                                   ing. In Proceedings of the IEEE International Conference
Dai, A. M. and Le, Q. V. Semi-supervised sequence learning.        on Computer Vision, pp. 6391–6400, 2019.
  In Advances in neural information processing systems,
  pp. 3079–3087, 2015.                                           Graves, A. and Jaitly, N. Towards end-to-end speech recog-
                                                                   nition with recurrent neural networks. In International
Devlin, J., Chang, M.-W., Lee, K., and Toutanova, K. Bert:         conference on machine learning, pp. 1764–1772, 2014.
  Pre-training of deep bidirectional transformers for lan-
                                                                 He, K., Fan, H., Wu, Y., Xie, S., and Girshick, R. Mo-
  guage understanding. arXiv preprint arXiv:1810.04805,
                                                                   mentum contrast for unsupervised visual representation
  2018.
                                                                   learning. arXiv preprint arXiv:1911.05722, 2019.
Dinh, L., Krueger, D., and Bengio, Y. Nice: Non-linear
                                                                 Hénaff, O. J., Razavi, A., Doersch, C., Eslami, S., and Oord,
  independent components estimation. arXiv preprint
                                                                   A. v. d. Data-efficient image recognition with contrastive
  arXiv:1410.8516, 2014.
                                                                   predictive coding. arXiv preprint arXiv:1905.09272,
Doersch, C., Gupta, A., and Efros, A. A. Unsupervised              2019.
 visual representation learning by context prediction. In        Higgins, I., Matthey, L., Pal, A., Burgess, C., Glorot, X.,
 Proceedings of the IEEE International Conference on               Botvinick, M., Mohamed, S., and Lerchner, A. beta-
 Computer Vision, pp. 1422–1430, 2015.                             vae: Learning basic visual concepts with a constrained
                                                                   variational framework. 2017.
Donahue, J. and Simonyan, K. Large scale adversarial rep-
  resentation learning. In Advances in Neural Information        Hinton, G. E., Osindero, S., and Teh, Y.-W. A fast learning
 Processing Systems, pp. 10541–10551, 2019.                        algorithm for deep belief nets. Neural computation, 18
                                                                   (7):1527–1554, 2006.
Donahue, J., Krähenbühl, P., and Darrell, T. Adversarial
  feature learning. arXiv preprint arXiv:1605.09782, 2016.       Hjelm, R. D., Fedorov, A., Lavoie-Marchildon, S., Grewal,
                                                                   K., Bachman, P., Trischler, A., and Bengio, Y. Learning
Dosovitskiy, A., Fischer, P., Springenberg, J. T., Riedmiller,     deep representations by mutual information estimation
  M., and Brox, T. Discriminative unsupervised feature             and maximization. arXiv preprint arXiv:1808.06670,
  learning with exemplar convolutional neural networks.            2018.
  IEEE transactions on pattern analysis and machine intel-
  ligence, 38(9):1734–1747, 2015.                                Howard, J. and Ruder, S. Universal language model
                                                                   fine-tuning for text classification. arXiv preprint
Erhan, D., Bengio, Y., Courville, A., Manzagol, P.-A., Vin-        arXiv:1801.06146, 2018.
  cent, P., and Bengio, S. Why does unsupervised pre-
  training help deep learning? Journal of Machine Learn-         Huang, P.-S., Avron, H., Sainath, T. N., Sindhwani, V., and
  ing Research, 11(Feb):625–660, 2010.                             Ramabhadran, B. Kernel methods match deep neural net-
                                                                  works on timit. In 2014 IEEE International Conference
Gidaris, S., Singh, P., and Komodakis, N. Unsupervised rep-        on Acoustics, Speech and Signal Processing (ICASSP),
  resentation learning by predicting image rotations. arXiv        pp. 205–209. IEEE, 2014.
  preprint arXiv:1803.07728, 2018.
                                                                 Huang, Y., Cheng, Y., Bapna, A., Firat, O., Chen, D., Chen,
Glorot, X. and Bengio, Y. Understanding the difficulty             M., Lee, H., Ngiam, J., Le, Q. V., Wu, Y., et al. Gpipe:
  of training deep feedforward neural networks. In Pro-            Efficient training of giant neural networks using pipeline
  ceedings of the thirteenth international conference on           parallelism. In Advances in Neural Information Process-
  artificial intelligence and statistics, pp. 249–256, 2010.       ing Systems, pp. 103–112, 2019.
                                              Generative Pretraining from Pixels

Ioffe, S. and Szegedy, C. Batch normalization: Accelerating      Menick, J. and Kalchbrenner, N. Generating high fidelity im-
   deep network training by reducing internal covariate shift.    ages with subscale pixel networks and multidimensional
   arXiv preprint arXiv:1502.03167, 2015.                         upscaling. arXiv preprint arXiv:1812.01608, 2018.

Ke, N. R., GOYAL, A. G. A. P., Bilaniuk, O., Binas, J.,          Mikolov, T., Sutskever, I., Chen, K., Corrado, G. S., and
  Mozer, M. C., Pal, C., and Bengio, Y. Sparse attentive          Dean, J. Distributed representations of words and phrases
  backtracking: Temporal credit assignment through re-            and their compositionality. In Advances in neural infor-
  minding. In Advances in neural information processing           mation processing systems, pp. 3111–3119, 2013.
  systems, pp. 7640–7651, 2018.
                                                                 Misra, I. and van der Maaten, L. Self-supervised learn-
Kingma, D. P. and Dhariwal, P. Glow: Generative flow              ing of pretext-invariant representations. arXiv preprint
  with invertible 1x1 convolutions. In Advances in Neural         arXiv:1912.01991, 2019.
  Information Processing Systems, pp. 10215–10224, 2018.         Mohamed, A.-r., Dahl, G., and Hinton, G. Deep belief
                                                                  networks for phone recognition. 2009.
Kingma, D. P. and Welling, M. Auto-encoding variational
  bayes. arXiv preprint arXiv:1312.6114, 2013.                   Nair, V. and Hinton, G. E. Rectified linear units improve
                                                                   restricted boltzmann machines. In Proceedings of the 27th
Kingma, D. P., Mohamed, S., Rezende, D. J., and Welling,           international conference on machine learning (ICML-10),
  M. Semi-supervised learning with deep generative mod-            pp. 807–814, 2010.
  els. In Advances in neural information processing sys-
  tems, pp. 3581–3589, 2014.                                     Noroozi, M. and Favaro, P. Unsupervised learning of visual
                                                                   representations by solving jigsaw puzzles. In European
Kitaev, N., Kaiser, Ł., and Levskaya, A. Reformer: The             Conference on Computer Vision, pp. 69–84. Springer,
  efficient transformer. arXiv preprint arXiv:2001.04451,          2016.
  2020.
                                                                 Oord, A. v. d., Kalchbrenner, N., and Kavukcuoglu,
Kolesnikov, A., Zhai, X., and Beyer, L. Revisiting self-           K. Pixel recurrent neural networks. arXiv preprint
  supervised visual representation learning. In Proceedings        arXiv:1601.06759, 2016.
  of the IEEE conference on Computer Vision and Pattern
  Recognition, pp. 1920–1929, 2019.                              Oord, A. v. d., Li, Y., and Vinyals, O. Representation learn-
                                                                   ing with contrastive predictive coding. arXiv preprint
Krizhevsky, A., Sutskever, I., and Hinton, G. E. Imagenet          arXiv:1807.03748, 2018.
  classification with deep convolutional neural networks.
  In Advances in neural information processing systems,          Paine, T. L., Khorrami, P., Han, W., and Huang, T. S. An
  pp. 1097–1105, 2012.                                             analysis of unsupervised pre-training in light of recent
                                                                   advances. arXiv preprint arXiv:1412.6597, 2014.
Larochelle, H. and Murray, I. The neural autoregressive
                                                                 Parmar, N., Vaswani, A., Uszkoreit, J., Kaiser, Ł., Shazeer,
  distribution estimator. In Proceedings of the Fourteenth
                                                                   N., Ku, A., and Tran, D. Image transformer. arXiv
  International Conference on Artificial Intelligence and
                                                                   preprint arXiv:1802.05751, 2018.
  Statistics, pp. 29–37, 2011.
                                                                 Pathak, D., Krahenbuhl, P., Donahue, J., Darrell, T., and
Lasserre, J. A., Bishop, C. M., and Minka, T. P. Principled        Efros, A. A. Context encoders: Feature learning by
  hybrids of generative and discriminative models. In 2006         inpainting. In Proceedings of the IEEE conference on
  IEEE Computer Society Conference on Computer Vision              computer vision and pattern recognition, pp. 2536–2544,
  and Pattern Recognition (CVPR’06), volume 1, pp. 87–             2016.
  94. IEEE, 2006.
                                                                 Pedregosa, F., Varoquaux, G., Gramfort, A., Michel, V.,
Lee, H., Grosse, R., Ranganath, R., and Ng, A. Y. Convo-           Thirion, B., Grisel, O., Blondel, M., Prettenhofer, P.,
  lutional deep belief networks for scalable unsupervised          Weiss, R., Dubourg, V., Vanderplas, J., Passos, A., Cour-
  learning of hierarchical representations. In Proceedings         napeau, D., Brucher, M., Perrot, M., and Duchesnay, E.
  of the 26th annual international conference on machine           Scikit-learn: Machine learning in Python. Journal of
  learning, pp. 609–616, 2009.                                     Machine Learning Research, 12:2825–2830, 2011.
May, A., Garakani, A. B., Lu, Z., Guo, D., Liu, K., Bellet,      Peters, M. E., Neumann, M., Iyyer, M., Gardner, M., Clark,
 A., Fan, L., Collins, M., Hsu, D., Kingsbury, B., et al.          C., Lee, K., and Zettlemoyer, L. Deep contextualized
 Kernel approximation methods for speech recognition.              word representations. arXiv preprint arXiv:1802.05365,
 arXiv preprint arXiv:1701.03577, 2017.                            2018.
                                              Generative Pretraining from Pixels

Radford, A., Metz, L., and Chintala, S. Unsupervised rep-        Trinh, T. H., Luong, M.-T., and Le, Q. V. Selfie: Self-
  resentation learning with deep convolutional generative          supervised pretraining for image embedding. arXiv
  adversarial networks. arXiv preprint arXiv:1511.06434,           preprint arXiv:1906.02940, 2019.
  2015.
                                                                 Uria, B., Murray, I., and Larochelle, H. Rnade: The real-
Radford, A., Narasimhan, K., Salimans, T., and Sutskever,          valued neural autoregressive density-estimator. In Ad-
  I. Improving language understanding by generative pre-           vances in Neural Information Processing Systems, pp.
  training. 2018.                                                  2175–2183, 2013.
                                                                 Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones,
Radford, A., Wu, J., Child, R., Luan, D., Amodei, D., and
                                                                   L., Gomez, A. N., Kaiser, Ł., and Polosukhin, I. Atten-
  Sutskever, I. Language models are unsupervised multitask
                                                                   tion is all you need. In Advances in neural information
  learners. 2019.
                                                                   processing systems, pp. 5998–6008, 2017.
Ramachandran, P., Parmar, N., Vaswani, A., Bello, I., Lev-       Vincent, P., Larochelle, H., Bengio, Y., and Manzagol, P.-A.
  skaya, A., and Shlens, J. Stand-alone self-attention in          Extracting and composing robust features with denoising
  vision models. arXiv preprint arXiv:1906.05909, 2019.            autoencoders. In Proceedings of the 25th international
                                                                   conference on Machine learning, pp. 1096–1103, 2008.
Ranzato, M., Szlam, A., Bruna, J., Mathieu, M., Collobert,
  R., and Chopra, S. Video (language) modeling: a baseline       Xie, Q., Dai, Z., Hovy, E., Luong, M.-T., and Le,
  for generative models of natural videos. arXiv preprint          Q. V. Unsupervised data augmentation. arXiv preprint
  arXiv:1412.6604, 2014.                                           arXiv:1904.12848, 2019.

Rives, A., Goyal, S., Meier, J., Guo, D., Ott, M., Zitnick,      Zagoruyko, S. and Komodakis, N. Wide residual networks.
  C. L., Ma, J., and Fergus, R. Biological structure and func-     arXiv preprint arXiv:1605.07146, 2016.
  tion emerge from scaling unsupervised learning to 250          Zeiler, M. D. and Fergus, R. Visualizing and understand-
  million protein sequences. bioRxiv, pp. 622803, 2019.            ing convolutional networks. In European conference on
                                                                   computer vision, pp. 818–833. Springer, 2014.
Sandler, M., Baccash, J., Zhmoginov, A., and Howard, A.
  Non-discriminative data or weak model? on the relative
  importance of data and model resolution. In Proceedings
  of the IEEE International Conference on Computer Vision
                                                                 A. Experimental details
  Workshops, pp. 0–0, 2019.                                      A.1. Hyperparameters
Sohn, K., Berthelot, D., Li, C.-L., Zhang, Z., Carlini, N.,      In Table 5, we present the learning rates used to train each
  Cubuk, E. D., Kurakin, A., Zhang, H., and Raffel, C. Fix-      model in the paper. When using too high a learning rate,
  match: Simplifying semi-supervised learning with consis-       we observe an irrecoverable loss spike early on in training.
  tency and confidence. arXiv preprint arXiv:2001.07685,         Conversely, with too low a learning rate, training is stable
  2020.                                                          but loss improves slowly and eventually underperforms. As
                                                                 we increase model size, the irrecoverable loss spike occurs
Tan, M. and Le, Q. V. Efficientnet: Rethinking model             at even lower learning rates. This motivates our procedure
  scaling for convolutional neural networks. arXiv preprint      of sequentially searching learning rates from large to small
  arXiv:1905.11946, 2019.                                        and explains why larger models use lower learning rates
                                                                 than smaller models at fixed input resolution.
Tarvainen, A. and Valpola, H. Mean teachers are better role
  models: Weight-averaged consistency targets improve            We used an Adam β2 of 0.95 instead of the default 0.999
  semi-supervised deep learning results. In Advances in          because the latter causes loss spikes during training. We
  neural information processing systems, pp. 1195–1204,          did not use weight decay because applying a small weight
  2017.                                                          decay of 0.01 did not change representation quality.
                                                                 On iGPT-S, we found small gains in representation quality
Tian, Y., Krishnan, D., and Isola, P. Contrastive multiview      from using float32 instead of float16, from untying the token
  coding. arXiv preprint arXiv:1906.05849, 2019.                 embedding matrix and the matrix producing token logits,
                                                                 and from zero initializing the matrices producing token and
Torralba, A., Fergus, R., and Freeman, W. T. 80 million tiny
                                                                 class logits. We applied these settings to all models.
  images: A large data set for nonparametric object and
  scene recognition. IEEE transactions on pattern analysis       When training BERT models, one additional hyperparameter
  and machine intelligence, 30(11):1958–1970, 2008.              is the masking probability, set to 15% in Devlin et al. (2018).
                                                Generative Pretraining from Pixels


Table 5. Learning rates used for each model, objective, and input
resolution (IR) combination.

    Model          Objective          IR      Learning Rate
                                     2
    iGPT-S      auto-regressive    32 × 3         0.003
    iGPT-M      auto-regressive    322 × 3        0.003
    iGPT-L      auto-regressive    322 × 3        0.001
    iGPT-L      auto-regressive    482 × 3         0.01
    iGPT-XL     auto-regressive    642 × 3        0.0003
    iGPT-S          BERT           322 × 3         0.01
    iGPT-M          BERT           322 × 3        0.003
    iGPT-L          BERT           322 × 3        0.001


We also tried higher masking rates of 20%, 25%, 30%, and
35%, finding that 20% matched the performance of 15%,
though higher probabilities decreased performance.
