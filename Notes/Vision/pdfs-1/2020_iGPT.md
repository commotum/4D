# Generative Pretraining from Pixels (iGPT) - Survey

## 1. Basic Metadata
- Title: Generative Pretraining from Pixels
- Authors: Mark Chen, Alec Radford, Rewon Child, Jeff Wu, Heewoo Jun, Prafulla Dhariwal, David Luan, Ilya Sutskever
- Year: Not specified in the paper.
- Venue: Not specified in the paper.

Evidence:
> "Generative Pretraining from Pixels" (Title page)
> "Mark Chen 1 Alec Radford 1 Rewon Child 1 Jeff Wu 1 Heewoo Jun 1 Prafulla Dhariwal 1 David Luan 1 Ilya Sutskever 1" (Title page)

## 2. One-Sentence Contribution Summary
The paper tests whether transformer-based pixel prediction pre-training can learn strong image representations that transfer to downstream image classification via linear probes and fine-tuning.

## 3. Tasks Evaluated
Task 1: Autoregressive next pixel prediction (pre-training)
- Task type: Generation; Reconstruction
- Dataset(s): ImageNet ILSVRC 2012 (unlabeled for pre-training), plus 100 million unlabeled web images for the largest model
- Domain: Natural images
- Evidence:
> "We then chose one of two pre-training objectives, auto-regressive next pixel prediction or masked pixel prediction." (Figure 1 caption)
> "We investigate this setting using ImageNet as a proxy for a large unlabeled corpus, and small classic labeled datasets (CIFAR-10, CIFAR-100, STL-10) as proxies for downstream tasks. For our largest model, we use an additional 100 million unlabeled web images, filtered to be similar to ImageNet." (Section 3 Methodology)

Task 2: Masked pixel prediction (BERT-style pre-training)
- Task type: Reconstruction
- Dataset(s): ImageNet ILSVRC 2012 (unlabeled), plus 100 million unlabeled web images for the largest model
- Domain: Natural images
- Evidence:
> "We then chose one of two pre-training objectives, auto-regressive next pixel prediction or masked pixel prediction." (Figure 1 caption)
> "We investigate this setting using ImageNet as a proxy for a large unlabeled corpus, and small classic labeled datasets (CIFAR-10, CIFAR-100, STL-10) as proxies for downstream tasks. For our largest model, we use an additional 100 million unlabeled web images, filtered to be similar to ImageNet." (Section 3 Methodology)

Task 3: Image classification (linear probing and fine-tuning)
- Task type: Classification
- Dataset(s): ImageNet ILSVRC 2012, CIFAR-10, CIFAR-100, STL-10
- Domain: Natural images
- Evidence:
> "One way to measure representation quality is to fine-tune for image classification. Fine-tuning adds a small classification head to the model, used to optimize a classification objective and adapts all weights." (Section 2 Approach)
> "Another approach for measuring representation quality uses the pre-trained model as a feature extractor. In particular, given labeled examples (X, Y ), the model is applied to X to produce features fX . Then, a linear classifier is trained on (fX , Y )." (Section 2 Approach)
> "We use the ImageNet ILSVRC 2012 training dataset, splitting off 4% as our experimental validation set and report results on the ILSVRC 2012 validation set as our test set. For CIFAR-10, CIFAR-100 and STL-10, we split off 10% of the provided training set instead. We ignore the provided unlabeled examples in STL-10, which constitute a subset of ImageNet." (Section 3.1 Dataset and Data Augmentation)

Task 4: Low-data CIFAR-10 classification
- Task type: Classification
- Dataset(s): CIFAR-10 (low-label subsets)
- Domain: Natural images
- Evidence:
> "In contrast with recent approaches for low-data classification, we do not make use of pseudo-labeling or data augmentation. Instead, we work directly on a subset of the raw supervised dataset, extracting features using our pre-trained model, and training a linear classifier on those features." (Section 4.7 Low-Data CIFAR-10 Classification)
> "On CIFAR-10, we find that with 4 labels per class, we achieve 73.2% accuracy..." (Section 4.7 Low-Data CIFAR-10 Classification)

## 4. Domain and Modality Scope
- Evaluation spans multiple datasets of natural images within a single modality (images).
Evidence:
> "We investigate this setting using ImageNet as a proxy for a large unlabeled corpus, and small classic labeled datasets (CIFAR-10, CIFAR-100, STL-10) as proxies for downstream tasks." (Section 3 Methodology)
- Multiple modalities: Not specified in the paper (all evidence refers to images).
- Domain generalization or cross-domain transfer: Not explicitly claimed; the paper frames ImageNet as an unlabeled corpus and CIFAR/STL as downstream tasks but does not claim domain generalization.
Evidence:
> "We investigate this setting using ImageNet as a proxy for a large unlabeled corpus, and small classic labeled datasets (CIFAR-10, CIFAR-100, STL-10) as proxies for downstream tasks." (Section 3 Methodology)

## 5. Model Sharing Across Tasks
| Task | Shared Weights? | Fine-Tuned? | Separate Head? | Evidence |
| --- | --- | --- | --- | --- |
| Autoregressive next pixel prediction (pre-training) | No, objective chosen per model (not joint) | No | Output projection to logits | "We then chose one of two pre-training objectives, auto-regressive next pixel prediction or masked pixel prediction." (Figure 1 caption); "Following the final transformer layer, we apply a layer norm nL = layer norm(hL ), and learn a projection from nL to logits parameterizing the conditional distributions at each sequence element." (Section 2.2 Architecture) |
| Masked pixel prediction (BERT-style pre-training) | No, objective chosen per model (not joint) | No | Output projection to logits | "We then chose one of two pre-training objectives, auto-regressive next pixel prediction or masked pixel prediction." (Figure 1 caption); "Following the final transformer layer, we apply a layer norm nL = layer norm(hL ), and learn a projection from nL to logits parameterizing the conditional distributions at each sequence element." (Section 2.2 Architecture) |
| Image classification (ImageNet, CIFAR-10/100, STL-10) | Yes, pretrained weights reused across downstream datasets | Yes (fine-tuning) and No (linear probe) | Yes, classification head / linear classifier | "Our approach consists of a pre-training stage followed by a fine-tuning stage." (Section 2 Approach); "Fine-tuning adds a small classification head to the model, used to optimize a classification objective and adapts all weights." (Section 2 Approach); "Then, a linear classifier is trained on (fX , Y )." (Section 2 Approach) |
| Low-data CIFAR-10 classification | Yes, uses pre-trained features | No (linear classifier on frozen features) | Yes, linear classifier | "extracting features using our pre-trained model, and training a linear classifier on those features." (Section 4.7 Low-Data CIFAR-10 Classification) |

## 6. Input and Representation Constraints
- Images are resized to a low resolution and reshaped into a 1D sequence.
Evidence:
> "First, we pre-process raw images by resizing to a low resolution and reshaping into a 1D sequence." (Figure 1 caption)
- Fixed input resolution per model (input resolution IR).
Evidence:
> "we first resize our image to a lower resolution, which we call the input resolution (IR)." (Section 3.2 Context Reduction)
- Fixed, discrete token representation and color quantization.
Evidence:
> "The transformer decoder takes an input sequence x1 , ..., xn of discrete tokens and produces a d-dimensional embedding for each position." (Section 2.2 Architecture)
> "we create our own 9-bit color palette by clustering (R, G, B) pixel values using k-means with k = 512. Using this palette yields an input sequence length 3 times shorter than the standard (R, G, B) palette, while still encoding color faithfully." (Section 3.2 Context Reduction)
- No built-in 2D spatial inductive bias.
Evidence:
> "our architecture uses a dense connectivity pattern which does not encode the 2D spatial structure of images yet is able to match and even outperform approaches which do." (Section 1 Introduction)
- Resizing and padding requirements during training/evaluation.
Evidence:
> "we randomly resize an image such that the shorter sidelength is in the range [256, 384]" (Section 3.1 Dataset and Data Augmentation)
> "4 pixels are reflection padded on each side" (Section 3.1 Dataset and Data Augmentation)

## 7. Context Window and Attention Structure
- Maximum sequence length (context length): 642 (paper notation for model resolution MR).
Evidence:
> "We call the resulting context length (322 or 482 or 642 ) the model resolution (MR)." (Section 3.2 Context Reduction)
- Sequence length is fixed per model via resizing to a set input resolution.
Evidence:
> "we first resize our image to a lower resolution, which we call the input resolution (IR)." (Section 3.2 Context Reduction)
- Attention type: dense (global) attention over the sequence.
Evidence:
> "Because the memory requirements of the transformer decoder scale quadratically with context length when using dense attention, we must employ further techniques to reduce context length." (Section 3.2 Context Reduction)
> "The only mixing across sequence elements occurs in the attention operation" (Section 2.2 Architecture)
- Cost management mechanisms: context reduction via lower resolution and color palette quantization.
Evidence:
> "we first resize our image to a lower resolution, which we call the input resolution (IR)." (Section 3.2 Context Reduction)
> "we create our own 9-bit color palette..." (Section 3.2 Context Reduction)

## 8. Positional Encoding (Critical Section)
- Mechanism: learned, independent position embeddings (absolute per position).
- Applied: to each sequence element (input embeddings); no layerwise or attention-bias variant specified.
- Fixed/ablated: Not specified; no comparisons reported.
Evidence:
> "Additionally, since we learn independent position embeddings for each sequence element, our BERT model has no positional inductive biases (i.e. it is permutation invariant)." (Section 2.2 Architecture)

## 9. Positional Encoding as a Variable
- Treated as a fixed architectural assumption; no multiple PE comparisons or ablations are reported.
Evidence:
> "Additionally, since we learn independent position embeddings for each sequence element, our BERT model has no positional inductive biases (i.e. it is permutation invariant)." (Section 2.2 Architecture)

## 10. Evidence of Constraint Masking (Scale vs Structure)
- Model size scaling:
Evidence:
> "Our largest model, iGPT-XL, contains L = 60 layers and uses an embedding size of d = 3072 for a total of 6.8B parameters." (Section 3.3 Model)
> "We also train iGPT-M, a 455M parameter model with L = 36 and d = 1024 and iGPT-S, a 76M parameter model with L = 24 and d = 512 to study the effect of model capacity on representation quality in a generative model." (Section 3.3 Model)
- Data scaling:
Evidence:
> "We use the ImageNet ILSVRC 2012 training dataset..." (Section 3.1 Dataset and Data Augmentation)
> "For our largest model, we use an additional 100 million unlabeled web images, filtered to be similar to ImageNet." (Section 3 Methodology)
- Claims that scale improves representations:
Evidence:
> "This highlights the importance of scale for our approach. Note that for a given validation loss value, bigger models also perform better." (Section 4.2 Better Generative Models Learn Better Representations)
> "Larger models produce better representations than smaller ones both at the end of training and at the same value of validation loss." (Figure 3 caption)
> "Additionally, we observed that our approach requires large models in order to learn high quality representations." (Section 6 Discussion and Conclusion)

## 11. Architectural Workarounds
- Context reduction by resizing and 1D sequence conversion.
Evidence:
> "First, we pre-process raw images by resizing to a low resolution and reshaping into a 1D sequence." (Figure 1 caption)
- Context reduction via low IR and color palette quantization to shorten sequences.
Evidence:
> "we first resize our image to a lower resolution, which we call the input resolution (IR)." (Section 3.2 Context Reduction)
> "we create our own 9-bit color palette... Using this palette yields an input sequence length 3 times shorter..." (Section 3.2 Context Reduction)
- Task-specific classification heads / linear classifiers for downstream tasks.
Evidence:
> "Fine-tuning adds a small classification head to the model, used to optimize a classification objective and adapts all weights." (Section 2 Approach)
> "Then, a linear classifier is trained on (fX , Y )." (Section 2 Approach)

## 12. Explicit Limitations and Non-Claims
- Limitations and future work:
Evidence:
> "We currently model low resolution inputs with self-attention." (Section 6 Discussion and Conclusion)
> "It is not immediately obvious how to best bridge the gap between performant autoregressive and discriminative models." (Section 6 Discussion and Conclusion)
> "Additionally, we observed that our approach requires large models in order to learn high quality representations. iGPT-L has 2 to 3 times as many parameters as similarly performing models on ImageNet and uses more compute." (Section 6 Discussion and Conclusion)
> "Although dense self-attention was a deliberate choice for this work due to it being domain agnostic and widely used in NLP, it becomes very memory and computationally expensive due to its quadratic scaling with sequence length. We mitigated this via the context reduction techniques discussed in section 3.2 but it is still a significant limitation. Future work could instead address this via architectural changes by exploring more efficient self-attention approaches." (Section 6 Discussion and Conclusion)
- Explicit non-claims about open-world or unrestrained multi-task learning: Not specified in the paper.

## 13. Constraint Profile (Synthesis)
Constraint Profile:
- Domain scope: natural image datasets (ImageNet, CIFAR-10/100, STL-10) in a single modality.
- Task structure: pretraining via pixel prediction and evaluation via classification (linear probes and fine-tuning), including low-data CIFAR-10.
- Representation rigidity: fixed low-resolution resizing, 1D sequence tokenization, and 9-bit color palette; no encoded 2D spatial inductive bias.
- Model sharing vs specialization: pretrain once then fine-tune or linear probe per dataset; separate models for AR vs BERT objectives.
- Role of positional encoding: learned independent position embeddings, treated as fixed.

## 14. Final Classification
Multi-task, single-domain.

Justification: The paper evaluates pixel prediction pretraining and downstream image classification across ImageNet, CIFAR-10/100, and STL-10, including low-data CIFAR-10 ("auto-regressive next pixel prediction or masked pixel prediction" (Figure 1 caption); "fine-tune for image classification" (Section 2 Approach); datasets listed in Section 3.1). All tasks are in a single modality (images) and within the natural-image domain, with no explicit multi-modal or cross-domain generalization claims.
