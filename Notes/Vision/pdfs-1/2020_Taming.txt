                                                          Taming Transformers for High-Resolution Image Synthesis

                                                                  Patrick Esser*    Robin Rombach*      Björn Ommer
                                                   Heidelberg Collaboratory for Image Processing, IWR, Heidelberg University, Germany
                                                                                    *Both authors contributed equally to this work
arXiv:2012.09841v3 [cs.CV] 23 Jun 2021




                                            Figure 1. Our approach enables transformers to synthesize high-resolution images like this one, which contains 1280x460 pixels.

                                                                  Abstract                                     and are increasingly adapted in other areas such as audio
                                                                                                               [12] and vision [8, 16]. In contrast to the predominant vi-
                                         Designed to learn long-range interactions on sequential               sion architecture, convolutional neural networks (CNNs),
                                         data, transformers continue to show state-of-the-art results          the transformer architecture contains no built-in inductive
                                         on a wide variety of tasks. In contrast to CNNs, they contain         prior on the locality of interactions and is therefore free
                                         no inductive bias that prioritizes local interactions. This           to learn complex relationships among its inputs. However,
                                         makes them expressive, but also computationally infeasi-              this generality also implies that it has to learn all relation-
                                         ble for long sequences, such as high-resolution images. We            ships, whereas CNNs have been designed to exploit prior
                                         demonstrate how combining the effectiveness of the induc-             knowledge about strong local correlations within images.
                                         tive bias of CNNs with the expressivity of transformers en-           Thus, the increased expressivity of transformers comes with
                                         ables them to model and thereby synthesize high-resolution            quadratically increasing computational costs, because all
                                         images. We show how to (i) use CNNs to learn a context-               pairwise interactions are taken into account. The result-
                                         rich vocabulary of image constituents, and in turn (ii) utilize       ing energy and time requirements of state-of-the-art trans-
                                         transformers to efficiently model their composition within            former models thus pose fundamental problems for scaling
                                         high-resolution images. Our approach is readily applied               them to high-resolution images with millions of pixels.
                                         to conditional synthesis tasks, where both non-spatial in-
                                         formation, such as object classes, and spatial information,              Observations that transformers tend to learn convolu-
                                         such as segmentations, can control the generated image.               tional structures [16] thus beg the question: Do we have
                                         In particular, we present the first results on semantically-          to re-learn everything we know about the local structure
                                         guided synthesis of megapixel images with transformers and            and regularity of images from scratch each time we train
                                         obtain the state of the art among autoregressive models on            a vision model, or can we efficiently encode inductive im-
                                         class-conditional ImageNet. Code and pretrained models                age biases while still retaining the flexibility of transform-
                                         can be found at https://git.io/JnyvK.                                 ers? We hypothesize that low-level image structure is well
                                                                                                               described by a local connectivity, i.e. a convolutional ar-
                                         1. Introduction                                                       chitecture, whereas this structural assumption ceases to be
                                                                                                               effective on higher semantic levels. Moreover, CNNs not
                                         Transformers are on the rise—they are now the de-facto                only exhibit a strong locality bias, but also a bias towards
                                         standard architecture for language tasks [74, 57, 58, 5]              spatial invariance through the use of shared weights across
                                                                                                           1
all positions. This makes them ineffective if a more holistic       products between all pairs of elements in the sequence, its
understanding of the input is required.                             computational complexity increases quadratically with the
    Our key insight to obtain an effective and expressive           sequence length. While the ability to consider interactions
model is that, taken together, convolutional and transformer        between all elements is the reason transformers efficiently
architectures can model the compositional nature of our vi-         learn long-range interactions, it is also the reason transform-
sual world [51]: We use a convolutional approach to effi-           ers quickly become infeasible, especially on images, where
ciently learn a codebook of context-rich visual parts and,          the sequence length itself scales quadratically with the res-
subsequently, learn a model of their global compositions.           olution. Different approaches have been proposed to reduce
The long-range interactions within these compositions re-           the computational requirements to make transformers fea-
quire an expressive transformer architecture to model distri-       sible for longer sequences. [55] and [76] restrict the recep-
butions over their consituent visual parts. Furthermore, we         tive fields of the attention modules, which reduces the ex-
utilize an adversarial approach to ensure that the dictionary       pressivity and, especially for high-resolution images, intro-
of local parts captures perceptually important local struc-         duces assumptions on the independence of pixels. [12] and
ture to alleviate the need for modeling low-level statistics        [26] retain the full receptive field but can√reduce costs for
with the transformer architecture. Allowing transformers to         a sequence of length n only from n2 to n n, which makes
concentrate on their unique strength—modeling long-range            resolutions beyond 64 pixels still prohibitively expensive.
relations—enables them to generate high-resolution images
as in Fig. 1, a feat which previously has been out of reach.        Convolutional Approaches The two-dimensional struc-
Our formulationgives control over the generated images by           ture of images suggests that local interactions are particu-
means of conditioning information regarding desired object          larly important. CNNs exploit this structure by restricting
classes or spatial layouts. Finally, experiments demonstrate        interactions between input variables to a local neighborhood
that our approach retains the advantages of transformers by         defined by the kernel size of the convolutional kernel. Ap-
outperforming previous codebook-based state-of-the-art ap-          plying a kernel thus results in costs that scale linearly with
proaches based on convolutional architectures.                      the overall sequence length (the number of pixels in the case
                                                                    of images) and quadratically in the kernel size, which, in
2. Related Work                                                     modern CNN architectures, is often fixed to a small constant
                                                                    such as 3 × 3. This inductive bias towards local interactions
The Transformer Family The defining characteristic of
                                                                    thus leads to efficient computations, but the wide range of
the transformer architecture [74] is that it models interac-
                                                                    specialized layers which are introduced into CNNs to han-
tions between its inputs solely through attention [2, 36, 52]
                                                                    dle different synthesis tasks [53, 80, 68, 85, 84] suggest that
which enables them to faithfully handle interactions be-
                                                                    this bias is often too restrictive.
tween inputs regardless of their relative position to one an-
                                                                        Convolutional architectures have been used for autore-
other. Originally applied to language tasks, inputs to the
                                                                    gressive modeling of images [70, 71, 10] but, for low-
transformer were given by tokens, but other signals, such as
                                                                    resolution images, previous works [55, 12, 26] demon-
those obtained from audio [41] or images [8], can be used.
                                                                    strated that transformers consistently outperform their con-
Each layer of the transformer then consists of an attention
                                                                    volutional counterparts. Our approach allows us to ef-
mechanism, which allows for interaction between inputs at
                                                                    ficiently model high-resolution images with transformers
different positions, followed by a position-wise fully con-
                                                                    while retaining their advantages over state-of-the-art con-
nected network, which is applied to all positions indepen-
                                                                    volutional approaches.
dently. More specifically, the (self-)attention mechanism
can be described by mapping an intermediate representa-
tion with three position-wise linear layers into three repre-       Two-Stage Approaches Closest to ours are two-stage ap-
sentations, query Q ∈ RN ×dk , key K ∈ RN ×dk and value             proaches which first learn an encoding of data and after-
V ∈ RN ×dv , to compute the output as                               wards learn, in a second stage, a probabilistic model of this
                                                                    encoding. [13] demonstrated both theoretical and empirical
                            QK t                                  evidence on the advantages of first learning a data repre-
   Attn(Q, K, V ) = softmax √       V ∈ RN ×dv .         (1)        sentation with a Variational Autoencoder (VAE) [38, 62],
                              dk
                                                                    and then again learning its distribution with a VAE. [18, 78]
   When performing autoregressive maximum-likelihood                demonstrate similar gains when using an unconditional nor-
learning, non-causal entries of QK t , i.e. all entries be-         malizing flow for the second stage, and [63, 64] when using
low its diagonal, are set to −∞ and the final output of the         a conditional normalizing flow. To improve training effi-
transformer is given after a linear, point-wise transforma-         ciency of Generative Adversarial Networks (GANs), [43]
tion to predict logits of the next sequence element. Since          learns a GAN [20] on representations of an autoencoder and
the attention mechanism relies on the computation of inner          [21] on low-resolution wavelet coefficients which are then


                                                                2
Figure 2. Our approach uses a convolutional VQGAN to learn a codebook of context-rich visual parts, whose composition is subsequently
modeled with an autoregressive transformer architecture. A discrete codebook provides the interface between these architectures and a
patch-based discriminator enables strong compression while retaining high perceptual quality. This method introduces the efficiency of
convolutional approaches to transformer based high resolution image synthesis.


decoded to images with a learned generator.                           understands the global composition of images, enabling it to
    [72] presents the Vector Quantised Variational Autoen-            generate locally realistic as well as globally consistent pat-
coder (VQVAE), an approach to learn discrete represen-                terns. Therefore, instead of representing an image with pix-
tations of images, and models their distribution autore-              els, we represent it as a composition of perceptually rich im-
gressively with a convolutional architecture. [61] extends            age constituents from a codebook. By learning an effective
this approach to use a hierarchy of learned representations.          code, as described in Sec. 3.1, we can significantly reduce
However, these methods still rely on convolutional density            the description length of compositions, which allows us to
estimation, which makes it difficult to capture long-range            efficiently model their global interrelations within images
interactions in high-resolution images. [8] models images             with a transformer architecture as described in Sec. 3.2.
autoregressively with transformers in order to evaluate the           This approach, summarized in Fig. 2, is able to generate
suitability of generative pretraining to learn image repre-           realistic and consistent high resolution images both in an
sentations for downstream tasks. Since input resolutions of           unconditional and a conditional setting.
32 × 32 pixels are still quite computationally expensive [8],
a VQVAE is used to encode images up to a resolution of                3.1. Learning an Effective Codebook of Image Con-
192 × 192. In an effort to keep the learned discrete repre-                 stituents for Use in Transformers
sentation as spatially invariant as possible with respect to
                                                                      To utilize the highly expressive transformer architecture for
the pixels, a shallow VQVAE with small receptive field is
                                                                      image synthesis, we need to express the constituents of an
employed. In contrast, we demonstrate that a powerful first
                                                                      image in the form of a sequence. Instead of building on indi-
stage, which captures as much context as possible in the
                                                                      vidual pixels, complexity necessitates an approach that uses
learned representation, is critical to enable efficient high-
                                                                      a discrete codebook of learned representations, such that
resolution image synthesis with transformers.
                                                                      any image x ∈ RH×W ×3 can be represented by a spatial
3. Approach                                                           collection of codebook entries zq ∈ Rh×w×nz , where nz is
                                                                      the dimensionality of codes. An equivalent representation
   Our goal is to exploit the highly promising learning ca-           is a sequence of h · w indices which specify the respective
pabilities of transformer models [74] and introduce them to           entries in the learned codebook. To effectively learn such
high-resolution image synthesis up to the megapixel range.            a discrete spatial codebook, we propose to directly incor-
Previous work [55, 8] which applied transformers to image             porate the inductive biases of CNNs and incorporate ideas
generation demonstrated promising results for images up to            from neural discrete representation learning [72]. First, we
a size of 64 × 64 pixels but, due to the quadratically in-            learn a convolutional model consisting of an encoder E and
creasing cost in sequence length, cannot simply be scaled             a decoder G, such that taken together, they learn to repre-
to higher resolutions.                                                sent images with codes from a learned, discrete codebook
   High-resolution image synthesis requires a model that              Z = {zk }K  k=1 ⊂ R
                                                                                           nz
                                                                                              (see Fig. 2 for an overview). More


                                                                  3
precisely, we approximate a given image x by x̂ = G(zq ).            the decoder, and δ = 10−6 is used for numerical stability.
We obtain zq using the encoding ẑ = E(x) ∈ Rh×w×nz                  To aggregate context from everywhere, we apply a single
and a subsequent element-wise quantization q(·) of each              attention layer on the lowest resolution. This training pro-
spatial code ẑij ∈ Rnz onto its closest codebook entry zk :         cedure significantly reduces the sequence length when un-
                                                                   rolling the latent code and thereby enables the application
   zq = q(ẑ) := arg minkẑij − zk k ∈ Rh×w×nz . (2)                 of powerful transformer models.
                       zk ∈Z
                                                                     3.2. Learning the Composition of Images with
The reconstruction x̂ ≈ x is then given by                                Transformers
                 x̂ = G(zq ) = G (q(E(x))) .               (3)       Latent Transformers With E and G available, we can
                                                                     now represent images in terms of the codebook-indices of
Backpropagation through the non-differentiable quantiza-             their encodings. More precisely, the quantized encoding of
tion operation in Eq. (3) is achieved by a straight-through          an image x is given by zq = q(E(x)) ∈ Rh×w×nz and
gradient estimator, which simply copies the gradients from           is equivalent to a sequence s ∈ {0, . . . , |Z|−1}h×w of in-
the decoder to the encoder [3], such that the model and              dices from the codebook, which is obtained by replacing
codebook can be trained end-to-end via the loss function             each code by its index in the codebook Z:
   LVQ (E, G, Z) = kx − x̂k2 + ksg[E(x)] − zq k22
                                                                                    sij = k such that (zq )ij = zk .            (8)
                                  + ksg[zq ] − E(x)k22 .   (4)
                                                                     By mapping indices of a sequence s back   to their corre-
Here, Lrec = kx − x̂k2 is a reconstruction loss, sg[·] denotes       sponding codebook entries, zq = zsij is readily recov-
the stop-gradient operation, and ksg[zq ] − E(x)k22 is the so-       ered and decoded to an image x̂ = G(zq ).
called “commitment loss” [72].                                           Thus, after choosing some ordering of the indices in
                                                                     s, image-generation can be formulated as autoregressive
Learning a Perceptually Rich Codebook Using trans-                   next-index prediction: Given indices s<i , the transformer
formers to represent images as a distribution over latent im-        learns to predict the distribution of possible next indices,
age constituents requires us to push the limits of compres-          i.e. p(si |s<i ) to compute
                                                                                           Q     the likelihood of the full repre-
sion and learn a rich codebook. To do so, we propose VQ-             sentation as p(s) = i p(si |s<i ). This allows us to directly
GAN, a variant of the original VQVAE, and use a discrimi-            maximize the log-likelihood of the data representations:
nator and perceptual loss [40, 30, 39, 17, 47] to keep good
perceptual quality at increased compression rate. Note that                      LTransformer = Ex∼p(x) [− log p(s)] .          (9)
this is in contrast to previous works which applied pixel-
based [71, 61] and transformer-based autoregressive mod-             Conditioned Synthesis In many image synthesis tasks a
els [8] on top of only a shallow quantization model. More            user demands control over the generation process by provid-
specifically, we replace the L2 loss used in [72] for Lrec by        ing additional information from which an example shall be
a perceptual loss and introduce an adversarial training pro-         synthesized. This information, which we will call c, could
cedure with a patch-based discriminator D [28] that aims to          be a single label describing the overall image class or even
differentiate between real and reconstructed images:                 another image itself. The task is then to learn the likelihood
                                                                     of the sequence given this information c:
 LGAN ({E, G, Z}, D) = [log D(x) + log(1 − D(x̂))] (5)
                                                                                                Y
The complete objective for finding the optimal compression                             p(s|c) =     p(si |s<i , c).             (10)
                                                                                                  i
model Q∗ = {E ∗ , G∗ , Z ∗ } then reads
                               h                                     If the conditioning information c has spatial extent, we first
 Q∗ = arg min max Ex∼p(x) LVQ (E, G, Z)                              learn another VQGAN to obtain again an index-based rep-
         E,G,Z     D
                                                    i                resentation r ∈ {0, . . . , |Zc |−1}hc ×wc with the newly ob-
                               +λLGAN ({E, G, Z}, D) , (6)           tained codebook Zc Due to the autoregressive structure of
                                                                     the transformer, we can then simply prepend r to s and
where we compute the adaptive weight λ according to                  restrict the computation of the negative log-likelihood to
                                                                     entries p(si |s<i , r). This “decoder-only” strategy has also
                            ∇GL [Lrec ]
                    λ=                                     (7)       been successfully used for text-summarization tasks [44].
                          ∇GL [LGAN ] + δ
where Lrec is the perceptual reconstruction loss [81], ∇GL [·]       Generating High-Resolution Images The attention
denotes the gradient of its input w.r.t. the last layer L of         mechanism of the transformer puts limits on the sequence


                                                                 4
                                                                                               Negative Log-Likelihood (NLL)
                                                                             Data /          Transformer    Transformer     PixelSNAIL
                                                                            # params        P-SNAIL steps   P-SNAIL time     fixed time

                                                                         RIN / 85M              4.78            4.84           4.96
              Figure 3. Sliding attention window.
                                                                       LSUN-CT / 310M           4.63            4.69           4.89
                                                                          IN / 310M             4.78            4.83           4.96
length h · w of its inputs s. While we can adapt the number             D-RIN / 180 M           4.70            4.78           4.88
of downsampling blocks m of our VQGAN to reduce                        S-FLCKR / 310 M          4.49            4.57           4.64
images of size H × W to h = H/2m × w = W/2m , we                      Table 1. Comparing Transformer and PixelSNAIL architectures
observe degradation of the reconstruction quality beyond              across different datasets and model sizes. For all settings, trans-
a critical value of m, which depends on the considered                formers outperform the state-of-the-art model from the PixelCNN
dataset. To generate images in the megapixel regime, we               family, PixelSNAIL in terms of NLL. This holds both when com-
therefore have to work patch-wise and crop images to                  paring NLL at fixed times (PixelSNAIL trains roughly 2 times
restrict the length of s to a maximally feasible size during          faster) and when trained for a fixed number of steps. See Sec. 4.1
training. To sample images, we then use the transformer               for the abbreviations.
in a sliding-window manner as illustrated in Fig. 3. Our
VQGAN ensures that the available context is still sufficient
                                                                      conditioning information, and then train both a transformer
to faithfully model images, as long as either the statistics of
                                                                      and a PixelSNAIL [10] model on the same representations,
the dataset are approximately spatially invariant or spatial
                                                                      as the latter has been used in previous state-of-the-art two-
conditioning information is available. In practice, this is
                                                                      stage approaches [61]. For a thorough comparison, we vary
not a restrictive requirement, because when it is violated,
                                                                      the model capacities between 85M and 310M parameters
i.e. unconditional image synthesis on aligned data, we can
                                                                      and adjust the number of layers in each model to match one
simply condition on image coordinates, similar to [42].
                                                                      another. We observe that PixelSNAIL trains roughly twice
4. Experiments                                                        as fast as the transformer and thus, for a fair comparison,
                                                                      report the negative log-likelihood both for the same amount
This section evaluates the ability of our approach to re-
                                                                      of training time (P-SNAIL time) and for the same amount of
tain the advantages of transformers over their convolutional
                                                                      training steps (P-SNAIL steps).
counterparts (Sec. 4.1) while integrating the effectiveness
of convolutional architectures to enable high-resolution im-          Results Tab. 1 reports results for unconditional image
age synthesis (Sec. 4.2). Furthermore, in Sec. 4.3, we in-            modeling on ImageNet (IN) [14], Restricted ImageNet
vestigate how codebook quality affects our approach. We               (RIN) [65], consisting of a subset of animal classes from
close the analysis by providing a quantitative comparison             ImageNet, LSUN Churches and Towers (LSUN-CT) [79],
to a wide range of existing approches for generative im-              and for conditional image modeling of RIN conditioned on
age synthesis in Sec. 4.4. Based on initial experiments, we           depth maps obtained with the approach of [60] (D-RIN) and
usually set |Z|= 1024 and train all subsequent transformer            of landscape images collected from Flickr conditioned on
models to predict sequences of length 16 · 16, as this is the         semantic layouts (S-FLCKR) obtained with the approach
maximum feasible length to train a GPT2-medium architec-              of [7]. Note that for the semantic layouts, we train the
ture (307 M parameters) [58] on a GPU with 12GB VRAM.                 first-stage using a cross-entropy reconstruction loss due to
More details on architectures and hyperparameters can be              their discrete nature. The results shows that the transformer
found in the appendix (Tab. 7 and Tab. 8).                            consistently outperforms PixelSNAIL across all tasks when
                                                                      trained for the same amount of time and the gap increases
4.1. Attention Is All You Need in the Latent Space                    even further when trained for the same number of steps.
Transformers show state-of-the-art results on a wide va-              These results demonstrate that gains of transformers carry
riety of tasks, including autoregressive image modeling.              over to our proposed two-stage setting.
However, evaluations of previous works were limited to
                                                                      4.2. A Unified Model for Image Synthesis Tasks
transformers working directly on (low-resolution) pixels
[55, 12, 26], or to deliberately shallow pixel encodings [8].         The versatility and generality of the transformer architec-
This raises the question if our approach retains the advan-           ture makes it a promising candidate for image synthesis. In
tages of transformers over convolutional approaches.                  the conditional case, additional information c such as class
    To answer this question, we use a variety of conditional          labels or segmentation maps are used and the goal is to learn
and unconditional tasks and compare the performance be-               the distribution of images as described in Eq. (10). Using
tween our transformer-based approach and a convolutional              the same setting as in Sec. 4.1 (i.e. image size 256 × 256,
approach. For each task, we train a VQGAN with m = 4                  latent size 16 × 16), we perform various conditional image
downsampling blocks, and, if needed, another one for the              synthesis experiments:
                                                                  5
   conditioning                        samples                             upsampled. We train our model for an upsampling factor of
                                                                           8 on ImageNet and show results in Fig. 6.
                                                                           (v): Class-conditional image synthesis: Here, the condi-
                                                                           tioning information c is a single index describing the class
                                                                           label of interest. Results for the RIN and IN dataset are
                                                                           demonstrated in Fig. 4 and Fig. 8, respectively.
                                                                           All of these examples make use of the same methodology.
                                                                           Instead of requiring task specific architectures or modules,
                                                                           the flexibility of the transformer allows us to learn appropri-
                                                                           ate interactions for each task, while the VQGAN — which
                                                                           can be reused across different tasks — leads to short se-
                                                                           quence lengths. In combination, the presented approach can
                                                                           be understood as an efficient, general purpose mechanism
                                                                           for conditional image synthesis. Note that additional results
                                                                           for each experiment can be found in the appendix, Sec. D.

                                                                           High-Resolution Synthesis The sliding window ap-
                                                                           proach introduced in Sec. 3.2 enables image synthesis be-
                                                                           yond a resolution of 256 × 256 pixels. We evaluate this
                                                                           approach on unconditional image generation on LSUN-CT
                                                                           and FacesHQ (see Sec. 4.3) and conditional synthesis on D-
                                                                           RIN, COCO-Stuff and S-FLCKR, where we show results
                                                                           in Fig. 1, 6 and the supplementary (Fig. 29-39). Note that
                                                                           this approach can in principle be used to generate images
                                                                           of arbitrary ratio and size, given that the image statistics
                                                                           of the dataset of interest are approximately spatially invari-
                                                                           ant or spatial information is available. Impressive results
                                                                           can be achieved by applying this method to image genera-
Figure 4. Transformers within our setting unify a wide range of
image synthesis tasks. We show 256 × 256 synthesis results                 tion from semantic layouts on S-FLCKR, where a strong
across different conditioning inputs and datasets, all obtained with       VQGAN can be learned with m = 5, so that its code-
the same approach to exploit inductive biases of effective CNN             book together with the conditioning information provides
based VQGAN architectures in combination with the expressiv-               the transformer with enough context for image generation
ity of transformer architectures. Top row: Completions from un-            in the megapixel regime.
conditional training on ImageNet. 2nd row: Depth-to-Image on
RIN. 3rd row: Semantically guided synthesis on ADE20K. 4th                 4.3. Building Context-Rich Vocabularies
row: Pose-guided person generation on DeepFashion. Bottom
                                                                           How important are context-rich vocabularies? To inves-
row: Class-conditional samples on RIN.
                                                                           tigate this question, we ran experiments where the trans-
                                                                           former architecture is kept fixed while the amount of con-
                                                                           text encoded into the representation of the first stage is var-
(i): Semantic image synthesis, where we condition on
                                                                           ied through the number of downsampling blocks of our VQ-
semantic segmentation masks of ADE20K [83], a web-
                                                                           GAN. We specify the amount of context encoded in terms
scraped landscapes dataset (S-FLCKR) and COCO-Stuff
                                                                           of reduction factor in the side-length between image in-
[6]. Results are depicted in Figure 4, 5 and Fig. 6.
                                                                           puts and the resulting representations, i.e. a first stage en-
(ii): Structure-to-image, where we use either depth or edge                coding images of size H × W into discrete codes of size
information to synthesize images from both RIN and IN                      H/f × W/f is denoted by a factor f . For f = 1, we re-
(see Sec. 4.1). The resulting depth-to-image and edge-to-                  produce the approach of [8] and replace our VQGAN by a
image translations are visualized in Fig. 4 and Fig. 6.                    k-means clustering of RGB values with k = 512.
(iii): Pose-guided synthesis: Instead of using the semanti-                During training, we always crop images to obtain inputs of
cally rich information of either segmentation or depth maps,               size 16 × 16 for the transformer, i.e. when modeling im-
Fig. 4 shows that the same approach as for the previous ex-                ages with a factor f in the first stage, we use crops of size
periments can be used to build a shape-conditional genera-                 16f × 16f . To sample from the models, we always apply
tive model on the DeepFashion [45] dataset.                                them in a sliding window manner as described in Sec. 3.
(iv): Stochastic superresolution, where low-resolution im-                 Results Fig. 7 shows results for unconditional synthesis of
ages serve as the conditioning information and are thereby                 faces on FacesHQ, the combination of CelebA-HQ [31] and
                                                                       6
Figure 5. Samples generated from semantic layouts on S-FLCKR.          Figure 6. Applying the sliding attention window approach (Fig. 3)
Sizes from top-to-bottom: 1280 × 832, 1024 × 416 and 1280 ×            to various conditional image synthesis tasks. Top: Depth-to-image
240 pixels. Best viewed zoomed in. A larger visualization can be       on RIN, 2nd row: Stochastic superresolution on IN, 3rd and 4th
found in the appendix, see Fig 29.                                     row: Semantic synthesis on S-FLCKR, bottom: Edge-guided syn-
                                                                       thesis on IN. The resulting images vary between 368 × 496 and
                                                                       1024 × 576, hence they are best viewed zoomed in.

                                                                          Dataset    ours    SPADE [53]     Pix2PixHD (+aug) [75]   CRN [9]
FFHQ [33]. It clearly demonstrates the benefits of power-
                                                                        COCO-Stuff    22.4   22.6/23.9(*)        111.5 (54.2)        70.4
ful VQGANs by increasing the effective receptive field of                ADE20K       35.5   33.9/35.7(*)        81.8 (41.5)         73.3
the transformer. For small receptive fields, or equivalently
                                                                       Table 2. FID score comparison for semantic image synthesis
small f , the model cannot capture coherent structures. For
                                                                       (256 × 256 pixels). (*): Recalculated with our evaluation protocol
an intermediate value of f = 8, the overall structure of               based on [50] on the validation splits of each dataset.
images can be approximated, but inconsistencies of facial
features such as a half-bearded face and of viewpoints in              4.4. Benchmarking Image Synthesis Results
different parts of the image arise. Only our full setting of
f = 16 can synthesize high-fidelity samples. For analogous             In this section we investigate how our approach quantita-
results in the conditional setting on S-FLCKR, we refer to             tively compares to existing models for generative image
the appendix (Fig. 13 and Sec. C).                                     synthesis. In particular, we assess the performance of our
                                                                       model in terms of FID and compare to a variety of estab-
To assess the effectiveness of our approach quantitatively,            lished models (GANs, VAEs, Flows, AR, Hybrid). The
we compare results between training a transformer directly             results on semantic synthesis are shown in Tab. 2, where
on pixels, and training it on top of a VQGAN’s latent code             we compare to [53, 75, 35, 9], and the results on uncon-
with f = 2, given a fixed computational budget. Again, we              ditional face synthesis are shown in Tab. 3. While some
follow [8] and learn a dictionary of 512 RGB values on CI-             task-specialized GAN models report better FID scores, our
FAR10 to operate directly on pixel space and train the same            approach provides a unified model that works well across
transformer architecture on top of our VQGAN with a latent             a wide range of tasks while retaining the ability to encode
code of size 16 × 16 = 256. We observe improvements of                 and reconstruct images. It thereby bridges the gap between
18.63% for FIDs and 14.08× faster sampling of images.                  purely adversarial and likelihood-based approaches.


                                                                   7
      f1                       f2                                  f8                                  f16                downsampling factor




      1.0                   3.86                                  65.81                               280.68                      speed-up
Figure 7. Evaluating the importance of effective codebook for HQ-Faces (CelebA-HQ and FFHQ) for a fixed sequence length |s|= 16·16 =
256. Globally consistent structures can only be modeled with a context-rich vocabulary (right). All samples are generated with temperature
t = 1.0 and top-k sampling with k = 100. Last row reports the speedup over the f1 baseline which operates directly on pixels and takes
7258 seconds to produce a sample on a NVIDIA GeForce GTX Titan X.

     CelebA-HQ 256 × 256                     FFHQ 256 × 256                            Model            acceptance rate   FID           IS
      Method           FID ↓               Method             FID ↓               mixed k, p = 1.0              1.0       17.04     70.6 ± 1.8
                                                                                 k = 973, p = 1.0               1.0       29.20     47.3 ± 1.3
    GLOW [37]           69.0        VDVAE (t = 0.7) [11]      38.8               k = 250, p = 1.0               1.0       15.98     78.6 ± 1.1
     NVAE [69]          40.3          VDVAE (t = 1.0)         33.5
                                                                                 k = 973, p = 0.88              1.0       15.78     74.3 ± 1.8
 PIONEER (B.) [23]   39.2 (25.3)      VDVAE (t = 0.8)         29.8
                                                                                 k = 600, p = 1.0              0.05        5.20     280.3 ± 5.5
    NCPVAE [1]          24.8          VDVAE (t = 0.9)         28.5
    VAEBM [77]          20.4          VQGAN+P.SNAIL           21.9               mixed k, p = 1.0               0.5       10.26     125.5 ± 2.4
  Style ALAE [56]       19.2               BigGAN             12.4               mixed k, p = 1.0              0.25        7.35     188.6 ± 3.3
   DC-VAE [54]          15.8             ours (k=300)          9.6               mixed k, p = 1.0              0.05        5.88     304.8 ± 3.6
    ours (k=400)        10.2        U-Net GAN (+aug) [66]   10.9 (7.6)
                                                                                 mixed k, p = 1.0              0.005       6.59     402.7 ± 2.9
    PGGAN [31]           8.0        StyleGAN2 (+aug) [34]   3.8 (3.6)
                                                                                DCTransformer [48]              1.0        36.5         n/a
Table 3. FID score comparison for face image synthesis. CelebA-                   VQVAE-2 [61]                  1.0       ∼31          ∼45
HQ results reproduced from [1, 54, 77, 24], FFHQ from [66, 32].                     VQVAE-2                     n/a       ∼10          ∼330
                                                                                   BigGAN [4]                   1.0       7.53      168.6 ± 2.5
                                                                                  BigGAN-deep                   1.0       6.84      203.6 ± 2.6
Autoregressive models are typically sampled with a decod-                          IDDPM [49]                   1.0       12.3          n/a
                                                                               ADM-G, no guid. [15]             1.0       10.94       100.98
ing strategy [27] such as beam-search, top-k or nucleus                          ADM-G, 1.0 guid.               1.0       4.59         186.7
sampling. For most of our results, including those in Tab. 2,                   ADM-G, 10.0 guid.               1.0       9.11        283.92
we use top-k sampling with k = 100 unless stated other-                               val. data                 1.0       1.62      234.0 ± 3.9
wise. For the results on face synthesis in Tab. 3, we com-                    Table 4. FID score comparison for class-conditional synthesis
puted scores for k ∈ {100, 200, 300, 400, 500} and report                     on 256 × 256 ImageNet, evaluated between 50k samples and the
the best results, obtained with k = 400 for CelebA-HQ and                     training split. Classifier-based rejection sampling as in VQVAE-2
k = 300 for FFHQ. Fig. 10 in the supplementary shows                          uses a ResNet-101 [22] classifier. BigGAN(-deep) evaluated via
FID and Inception scores as a function of k.                                  https://tfhub.dev/deepmind truncated at 1.0. “Mixed”
                                                                              k refers to samples generated with different top-k values, here k ∈
Class-Conditional Synthesis on ImageNet To address a                          {100, 200, 250, 300, 350, 400, 500, 600, 800, 973}.
direct comparison with the previous state-of-the-art for au-                  Quantitative results are summarized in Tab. 4. We report
toregressive modeling of class-conditional image synthesis                    FID and Inception Scores for the best k/p in top-k/top-p
on ImageNet, VQVAE-2 [61], we train a class-conditional                       sampling. Following [61], we can further increase quality
ImageNet transformer on 256 × 256 images, using a VQ-                         via classifier-rejection, which keeps only the best m-out-
GAN with dim Z = 16384 and f = 16, and addition-                              of-n samples in terms of the classifier’s score, i.e. with an
ally compare to BigGAN [4], IDDPM [49], DCTransformer                         acceptance rate of m/n. We use a ResNet-101 classifier [22].
[48] and ADM [15] in Tab. 4. Note that our model uses
                                                                                 We observe that our model outperforms other autoregres-
≃ 10× less parameters than VQVAE-2, which has an esti-
                                                                              sive approaches (VQVAE-2, DCTransformer) in terms of
mated parameter count of 13.5B (estimate based on [67]).
                                                                              FID and IS, surpasses BigGAN and IDDPM even for low
   Samples of this model for different ImageNet classes are                   rejection rates and yields scores close to the state of the art
shown in Fig. 8. We observe that the adversarial training                     for higher rejection rates, see also Fig. 9.
of the corresponding VQGAN enables sampling of high-
quality images with realistic textures, of comparable or                      How good is the VQGAN? Reconstruction FIDs obtained
higher quality than existing approaches such as BigGAN                        via the codebook provide an estimate on the achievable FID
and VQVAE-2, see also Fig. 14-17 in the supplementary.                        of the generative model trained on it. To quantify the per-


                                                                          8
                      Figure 8. Samples from our class-conditional ImageNet model trained on 256 × 256 images.




                        Figure 9. FID and Inception Score as a function of top-k, nucleus and rejection filtering.


    Model          Codebook Size      dim Z     FID/val   FID/train       5. Conclusion
  VQVAE-2        64 × 64 & 32 × 32      512        n/a      ∼ 10
 DALL-E [59]          32 × 32          8192      32.01      33.88         This paper adressed the fundamental challenges that previ-
   VQGAN              16 × 16          1024       7.94      10.54         ously confined transformers to low-resolution images. We
   VQGAN              16 × 16         16384       4.98       7.41         proposed an approach which represents images as a compo-
  VQGAN∗              32 × 32          8192       1.49       3.24
   VQGAN         64 × 64 & 32 × 32      512       1.45       2.78         sition of perceptually rich image constituents and thereby
                                                                          overcomes the infeasible quadratic complexity when mod-
Table 5. FID on ImageNet between reconstructed validation split           eling images directly in pixel space. Modeling constituents
and original validation (FID/val) and training (FID/train) splits.        with a CNN architecture and their compositions with a
∗
  trained with Gumbel-Softmax reparameterization as in [59, 29].
                                                                          transformer architecture taps into the full potential of their
                                                                          complementary strengths and thereby allowed us to rep-
formance gains of our VQGAN over discrete VAEs trained
                                                                          resent the first results on high-resolution image synthesis
without perceptual and adversarial losses (e.g. VQVAE-2,
                                                                          with a transformer-based architecture. In experiments, our
DALL-E [59]), we evaluate this metric on ImageNet and
                                                                          approach demonstrates the efficiency of convolutional in-
report results in Tab. 5. Our VQGAN outperforms non-
                                                                          ductive biases and the expressivity of transformers by syn-
adversarial models while providing significantly more com-
                                                                          thesizing images in the megapixel range and outperforming
pression (seq. length of 256 vs. 5120 = 322 + 642 for
                                                                          state-of-the-art convolutional approaches. Equipped with a
VQVAE-2, 256 vs 1024 for DALL-E). As expected, larger
                                                                          general mechanism for conditional synthesis, it offers many
versions of VQGAN (either in terms of larger codebook
                                                                          opportunities for novel neural rendering approaches.
sizes or increased code lengths) further improve perfor-
mance. Using the same hierarchical codebook setting as in
VQVAE-2 with our model provides the best reconstruction
FID, albeit at the cost of a very long and thus impractical                  This work has been supported by the German Research Foundation
sequence. The qualitative comparison corresponding to the                 (DFG) projects 371923335, 421703927 and a hardware donation from
results in Tab. 5 can be found in Fig. 12.                                NVIDIA corporation.


                                                                      9
                              Taming Transformers for High-Resolution
                                         Image Synthesis
                                                –
                                               Supplementary Material


The supplementary material for our work Taming Transformers for High-Resolution Image Synthesis is structured as follows:
First, Sec. A summarizes changes to a previous version of this paper. In Sec. B, we present hyperparameters and architectures
which were used to train our models. Next, extending the discussion of Sec. 4.3, Sec. C presents additional evidence for the
importance of perceptually rich codebooks and its interpretation as a trade-off between reconstruction fidelity and sampling
capability. Additional results on high-resolution image synthesis for a wide range of tasks are then presented in Sec. D, and
Sec. E shows nearest neighbors of samples. Finally, Sec. F contains results regarding the ordering of image representations.

A. Changelog
We summarize changes between this version 1 of the paper and its previous version 2 .
    In the previous version, Eq. (4) had a weighting term β on the commitment loss, and Tab. 8 reported a value of β = 0.25
for all models. However, due to a bug in the implementation, β was never used and all models have been trained with β = 1.0.
Thus, we removed β in Eq. (4).
    We updated class-conditional synthesis results on ImageNet in Sec. 4.4. The previous results, included here in Tab. 6
for completeness, were based on a slightly different implementation where the transformer did not predict the distribution
of the first token but used a histogram for it. The new model has been trained for 2.4 million steps with a batch size of
16 accumulated over 8 batches, which took 45.8 days on a single A100 GPU. The previous model had been trained for
1.0 million steps. Furthermore, the FID values were based on 50k (18k) samples against 50k (18k) training examples (to
compare with MSP). For better comparison with other works, the current version reports FIDs based on 50k samples against
all training examples of ImageNet using torch-fidelity [50]. We updated all qualitative figures showing samples from
this model and added visualizations of the effect of tuning top-k/p or rejection rate in Fig. 14-26.
    To provide a better overview, we also include results from works that became available after the previous version of our
work. Specifically, we include results on reconstruction quality of the VQVAE from [59] in Tab. 5 and Fig. 12 (which replaces
the previous qualitative comparison), and results on class-conditional ImageNet sampling from [49, 48, 15] in Tab. 4. Note
that with the exception of BigGAN and BigGAN-deep [4], no models or sampling results are available for the methods we
compare to in Tab. 4. Thus, we can only report the numbers from the respective papers but cannot re-evaluate them with the
same code. We follow the common evaluation protocol for class-conditional ImageNet synthesis from [4] and evaluate 50k
samples from the model against the whole training split of ImageNet. However, it is not clear how different implementations
resize the training images. In our code, we use the largest center-crop and resize it bilinearly with anti-aliasing to 256 × 256
using Pillow [73]. FID and Inception Scores are then computed with torch-fidelity [50].
    We updated face-synthesis results in Tab. 3 based on a slightly different implementation as in the case of class-conditional
ImageNet results and improve the previous results slightly. In addition, we evaluate the ability of our NLL-based training to
detect overfitting. We train larger models (FFHQ (big) and CelebA-HQ (big) in Tab. 8) on the face datasets, and show nearest
neighbors of samples obtained from checkpoints with the best NLL on the validation split and the training split in Sec. E. We
also added Fig. 10, which visualizes the effect of tuning k in top-k sampling on FID and IS.

B. Implementation Details
The hyperparameters for all experiments presented in the main paper and supplementary material can be found in Tab. 8.
Except for the c-IN (big), COCO-Stuff and ADE20K models, these hyperparameters are set such that each transformer model
can be trained with a batch-size of at least 2 on a GPU with 12GB VRAM, but we generally train on 2-4 GPUs with an
accumulated VRAM of 48 GB. If hardware permits, 16-bit precision training is enabled.

  1 https://arxiv.org/abs/2012.09841v3
  2 https://arxiv.org/abs/2012.09841v2




                                                              10
                 Dataset     ours-previous (+R)   BigGAN (-deep)    MSP               Dataset       ours-previous   ours-new
              IN 256, 50K        19.8 (11.2)         7.1 (7.3)       n.a.          CelebA-HQ 256        10.7          10.2
              IN 256, 18K           23.5             9.6 (9.7)       50.4            FFHQ 256           11.4          9.6
Table 6. Results from a previous version of this paper, see also Sec. A. Left: Previous results on class-conditional ImageNet synthesis
with a slightly different implementation and evaluated against 50k and 18k training examples instead of the whole training split. See Tab. 4
for new, improved results evaluated against the whole training split. Right: Previous results on face-synthesis with a slightly different
implementation compared to the new implementation. See also Tab. 3 for comparison with other methods.




                   Figure 10. FID and Inception Score as a function of top-k for CelebA-HQ (left) and FFHQ (right).

                               Encoder                                                             Decoder
                                   H×W ×C
                      x∈R                                                                     zq ∈ Rh×w×nz
                                      0                                                                      00
                  Conv2D → RH×W ×C                                                        Conv2D → Rh×w×C
                                                   00                                                            00
   m× { Residual Block, Downsample Block} → Rh×w×C                                     Residual Block → Rh×w×C
                                        00                                                                         00
               Residual Block → Rh×w×C                                                Non-Local Block → Rh×w×C
                                          00                                                                     00
              Non-Local Block → Rh×w×C                                                 Residual Block → Rh×w×C
                                        00                                                                                 0
               Residual Block → Rh×w×C                                      m× { Residual Block, Upsample Block} → RH×W ×C
                                        h×w×nz                                                                  H×W ×C
         GroupNorm, Swish, Conv2D → R                                            GroupNorm, Swish, Conv2D → R
Table 7. High-level architecture of the encoder and decoder of our VQGAN. The design of the networks follows the architecture presented
in [25] with no skip-connections. For the discriminator, we use a patch-based model as in [28]. Note that h = 2Hm , w = 2Wm and f = 2m .


VQGAN Architecture The architecture of our convolutional encoder and decoder models used in the VQGAN experiments
is described in Tab. 7. Note that we adopt the compression rate by tuning the number of downsampling steps m. Further note
that λ in Eq. 5 is set to zero in an initial warm-up phase. Empirically, we found that longer warm-ups generally lead to better
reconstructions. As a rule of thumb, we recommend setting λ = 0 for at least one epoch.

Transformer Architecture Our transformer model is identical to the GPT2 architecture [58] and we vary its capacity
mainly through varying the amount of layers (see Tab. 8). Furthermore, we generally produce samples with a temperature
t = 1.0 and a top-k cutoff at k = 100 (with higher top-k values for larger codebooks).

C. On Context-Rich Vocabularies
Sec. 4.3 investigated the effect of the downsampling factor f used for encoding images. As demonstrated in Fig. 7, large
factors are crucial for our approach, since they enable the transformer to model long-range interactions efficiently. However,
since larger f correspond to larger compression rates, the reconstruction quality of the VQGAN starts to decrease after a
certain point, which is analyzed in Fig. 11. The left part shows the reconstruction error (measured by LPIPS [81]) versus the
negative log-likelihood obtained by the transformer for values of f ranging from 1 to 64. The latter provides a measure of the
ability to model the distribution of the image representation, which increases with f . The reconstruction error on the other
hand decreases with f and the qualitative results on the right part show that beyond a critical value of f , in this case f = 16,
reconstruction errors become severe. At this point, even when the image representations are modeled faithfully, as suggested
by a low negative log-likelihood, sampled images are of low-fidelity, because the reconstruction capabilities provide an upper
bound on the quality that can be achieved.
   Hence, Fig. 11 shows that we must learn perceptually rich encodings, i.e. encodings with a large f and perceptually faithful
reconstructions. This is the goal of our VQGAN and Fig. 12 compares its reconstruction capabilities against the VQVAE [72]


                                                                    11
                  Experiment            nlayer     # params [M ]        nz     |Z|    dropout     length(s)      ne      m
                     RIN                  12            85               64    768       0.0         512        1024     4
                    c-RIN                 18            128              64    768       0.0         257         768     4
                  D-RINv1                 14            180             256   1024       0.0         512         768     4
                  D-RINv2                 24            307             256   1024       0.0         512        1024     4
                      IN                  24            307             256   1024       0.0         256        1024     4
                     c-IN                 24            307             256    1024      0.0         257        1024     4
                  c-IN (big)              48           1400             256   16384      0.0         257        1536     4
                  IN-Edges                24           307              256   1024       0.0         512        1024     3
                    IN-SR                 12            153             256   1024       0.0         512        1024     3
              S-FLCKR, f = 4              24           307              256   1024       0.0         512        1024     2
              S-FLCKR, f = 16             24           307              256   1024       0.0         512        1024     4
              S-FLCKR, f = 32             24           307              256   1024       0.0         512        1024     5
             (FacesHQ, f = 1)∗            24           307                –    512       0.0         512        1024     –
               FacesHQ, f = 2             24           307              256   1024       0.0         512        1024     1
               FacesHQ, f = 4             24           307              256   1024       0.0         512        1024     2
               FacesHQ, f = 8             24           307              256   1024       0.0         512        1024     3
             FacesHQ∗∗ , f = 16           24           307              256   1024       0.0         512        1024     4
               FFHQ∗∗ , f = 16            28           355              256   1024       0.0         256        1024     4
            CelebA-HQ∗∗ , f = 16          28           355              256   1024       0.0         256        1024     4
                 FFHQ (big)               24           801              256   1024       0.0         256        1664     4
              CelebA-HQ (big)             24           801              256   1024       0.0         256        1664     4
                 COCO-Stuff               32           651              256   8192       0.0         512        1280     4
                  ADE20K                  28            405             256   4096       0.1         512        1024     4
                DeepFashion               18           129              256   1024       0.0         340         768     4
                  LSUN-CT                 24           307              256   1024       0.0         256        1024     4
                  CIFAR-10                24           307              256   1024       0.0         256        1024     1
Table 8. Hyperparameters. For every experiment, we set the number of attention heads in the transformer to nh = 16. nlayer denotes the
number of transformer blocks, # params the number of transformer parameters, nz the dimensionality of codebook entries, |Z| the number
of codebook entries, dropout the dropout rate for training the transformer, length(s) the total length of the sequence, ne the embedding
dimensionality and m the number of downsampling steps in the VQGAN. D-RINv1 is the experiment which compares to Pixel-SNAIL in
Sec. 4.1. Note that the experiment (FacesHQ, f = 1)∗ does not use a learned VQGAN but a fixed k-means clustering algorithm as in [8]
with K = 512 centroids. A prefix “c” refers to a class-conditional model. The models marked with a ‘∗∗‘ are trained on the same VQGAN.


used in DALL-E [59]. We observe that for f = 8 and 8192 codebook entries, both the VQVAE and VQGAN capture the
global structure faithfully. However, the textures produced by the VQVAE are blurry, whereas those of the VQGAN are crisp
and realistic looking (e.g. the stone texture and the fur and tail of the squirrel). When we increase the compression rate of the
VQGAN further to f = 16, we see that some reconstructed parts are not perfectly aligned with the input anymore (e.g. the
paw of the squirrel), but, especially with slightly larger codebooks, the reconstructions still look realistic. This demonstrates
how the VQGAN provides high-fidelity reconstructions at large factors, and thereby enables efficient high-resolution image
synthesis with transformers.
   To illustrate how the choice of f depends on the dataset, Fig. 13 presents results on S-FLCKR. In the left part, it shows,
analogous to Fig. 7, how the quality of samples increases with increasing f . However, in the right part, it shows that
reconstructions remain faithful perceptually faithful even for f 32, which is in contrast to the corresponding results on faces
in Fig. 11. These results might be explained by a higher perceptual sensitivity to facial features as compared to textures, and
allow us to generate high-resolution landscapes even more efficiently with f = 32.

D. Additional Results
Qualitative Comparisons The qualitative comparison corresponding to Tab. 4 and Tab. 6 can be found in Fig. 14, 15, 16
and 17. Since no models are available for VQVAE-2 and MSP, we extracted results directly from the supplementary3 and
  3 https://drive.google.com/file/d/1H2nr_Cu7OK18tRemsWn_6o5DGMNYentM/view?usp=sharing




                                                                   12
from the provided samples4 , respectively. For BigGAN, we produced the samples via the provided model5 . Similarly, the
qualitative comparison with the best competitor model (SPADE) for semantic synthesis on standard benchmarks (see Tab. 2)
can be found in Fig. 40 (ADE20K) and Fig. 41 (COCO-Stuff)6 .

Comparison to Image-GPT To further evaluate the effectiveness of our approach, we compare to the state-of-the-art
generative transformer model on images, ImageGPT [8]. By using immense amounts of compute the authors demonstrated
that transformer models can be applied to the pixel-representation of images and thereby achieved impressive results both in
representation learning and image synthesis. However, as their approach is confined to pixel-space, it does not scale beyond
a resolution of 192 × 192. As our approach leverages a strong compression method to obtain context-rich representations
of images and then learns a transformer model, we can synthesize images of much higher resolution. We compare both
approaches in Fig. 27 and Fig. 28, where completions of images are depicted. Both plots show that our approach is able
to synthesize consistent completions of dramatically increased fidelity. The results of [8] are obtained from https://
openai.com/blog/image-gpt/.

Additional High-Resolution Results Fig. 29, 30, 31 and Fig. 32 contain additional HR results on the S-FLCKR dataset
for both f = 16 (m = 4) and f = 32 (m = 5) (semantically guided). In particular, we provide an enlarged version of Fig. 5
from the main text, which had to be scaled down due to space constraints. Additionally, we use our sliding window approach
(see Sec. 3) to produce high-resolution samples for the depth-to-image setting on RIN in Fig. 33 and Fig. 34, edge-to-image
on IN in Fig. 35, stochastic superresolution on IN in Fig. 36, more examples on semantically guided landscape synthesis
on S-FLCKR in Fig. 37 with f = 16 and in Fig. 38 with f = 32, and unconditional image generation on LSUN-CT (see
Sec. 4.1) in Fig. 39. Moreover, for images of size 256 × 256, we provide results for generation from semantic layout on
(i) ADE20K in Fig. 40 and (ii) COCO-Stuff in Fig. 41, depth-to-image on IN in Fig. 42, pose-guided person generation in
Fig. 43 and class-conditional synthesis on RIN in Fig. 44.

E. Nearest Neighbors of Samples
One advantage of likelihood-based generative models over, e.g., GANs is the ability to evaluate NLL on training data and
validation data to detect overfitting. To test this, we trained large models for face synthesis, which can easily overfit them,
and retained two checkpoints on each dataset: One for the best validation NLL (at the 10th and 13th epoch for FFHQ and
CelebA-HQ, respectively), and another for the best training NLL (at epoch 1000). We then produced samples from both
checkpoints and retrieved nearest neighbors from the training data based on the LPIPS similarity metric [81]. The results
are shown in Fig. 45, where it can be observed that the checkpoints with best training NLL (best train NLL) reproduce the
training examples, whereas samples from the checkpoints with best validation NLL (best val. NLL) depict new faces which
are not found in the training data.
    Based on these results, we can conclude that early-stopping based on validation NLL can prevent overfitting. Furthermore,
the bottleneck for our approach on face synthesis is given by the dataset size since it has the capacity to almost perfectly fit
the training data. Unfortunately, FID scores cannot detect such an overfitting. Indeed, the best train NLL checkpoints achieve
FID scores of 3.86 on CelebA-HQ and 2.68 on FFHQ, compared to 10.2 and 9.6 for the best val. NLL checkpoints. While
validation NLL provides a way to detect overfitting for likelihood-based models, it is not clear if early-stopping based on it
is optimal if one is mainly interested in the quality of samples. To address this and the evaluation of GANs, new metrics will
be required which can differentiate between models that produce new, high-quality samples and those that simply reproduce
the training data.
    Our class-conditional ImageNet model does not display overfitting according to validation NLL, and the nearest neighbors
shown in Fig. 46 also provide evidence that the model produces new, high-quality samples.

F. On the Ordering of Image Representations
For the “classical” domain of transformer models, NLP, the order of tokens is defined by the language at hand. For images
and their discrete representations, in contrast, it is not clear which linear ordering to use. In particular, our sliding-window
approach depends on a row-major ordering and we thus investigate the performance of the following five different permu-
tations of the input sequence of codebook indices: (i) row major, or raster scan order, where the image representation is
  4 https://bit.ly/2FJkvhJ
  5 https://tfhub.dev/deepmind/biggan-deep-256/1
  6 samples were reproduced with the authors’ official implementation available at https://github.com/nvlabs/spade/




                                                                 13
unrolled from top left to bottom right. (ii) spiral out, which incorporates the prior assumption that most images show a
centered object. (iii) z-curve, also known as z-order or morton curve, which introduces the prior of preserved locality when
mapping a 2D image representation onto a 1D sequence. (iv) subsample, where prefixes correspond to subsampled repre-
sentations, see also [46]. (v) alternate, which is related to row major, but alternates the direction of unrolling every row. (vi)
spiral in, a reversed version of spiral out which provides the most context for predicting the center of the image. A graphical
visualization of these permutation variants is shown in Fig. 47. Given a VQGAN trained on ImageNet, we train a transformer
for each permutation in a controlled setting, i.e. we fix initialization and computational budget.

Results Fig.47 depicts the evolution of negative log-likelihood for each variant as a function of training iterations, with
final values given by (i) 4.767, (ii) 4.889, (iii) 4.810, (iv) 5.015, (v) 4.812, (vi) 4.901. Interestingly, row major performs best
in terms of this metric, whereas the more hierarchical subsample prior does not induce any helpful bias. We also include
qualitative samples in Fig. 48 and observe that the two worst performing models in terms of NLL (subsample and spiral in)
tend to produce more textural samples, while the other variants synthesize samples with much more recognizable structures.
Overall, we can conclude that the autoregressive codebook modeling is not permutation-invariant, but the common row major
ordering [71, 8] outperforms other orderings.




                                                                14
                                                                                                                 f2            f4               f8         f16           f32           f64

                           105                                                       f1
                                                                                            recon-
                                                                                     f2    struction
 negative log-likelihood




                                                                  area of            f4
                           104                                  low-fidelity         f8
                                                                                     f16
                                                                                     f32    sample
                                                                                     f64
                           103
                                           area of
                                        high-fidelity                                      samples
                           102

                             0.0         0.2            0.4         0.6        0.8         rec. error        0.11 ± 0.02   0.20 ± 0.03    0.23 ± 0.04   0.38 ± 0.07   0.63 ± 0.08   0.66 ± 0.11

                                                  reconstruction error                        nll            5.66 · 104    1.29 · 104      4.10 · 103   2.32 · 103    2.28 · 102    6.75 · 101

Figure 11. Trade-off between negative log-likelihood (nll) and reconstruction error. While context-rich encodings obtained with large
factors f allow the transformer to effectively model long-range interactions, the reconstructions capabilities and hence quality of samples
suffer after a critical value (here, f = 16). For more details, see Sec. C.




Figure 12. Comparing reconstruction capabilities between VQVAEs and VQGANs. Numbers in parentheses denote compression factor and
codebook size. With the same compression factor and codebook size, VQGANs produce more realistic reconstructions compared to blurry
reconstructions of VQVAEs. This enables increased compression rates for VQGAN while retaining realistic reconstructions. See Sec. C.

                                   f4                         f16                                   f32                                  factor              reconstructions


                                                                                                                                          f4



                                                                                                                                          f16



                                                                                                                                          f32

Figure 13. Samples on landscape dataset (left) obtained with different factors f , analogous to Fig. 7. In contrast to faces, a factor of f = 32
still allows for faithful reconstructions (right). See also Sec. C.


                                                                                                        15
              ours                       VQVAE-2 [61]                      BigGAN [4]                        MSP [19]




Figure 14. Qualitative assessment of various models for class-conditional image synthesis on ImageNet. Depicted classes: 28: spotted
salamander (top) and 97: drake (bottom). We report class labels as in VQVAE-2 [61].




                                                                16
              ours                      VQVAE-2 [61]                     BigGAN [4]                       MSP [19]




Figure 15. Qualitative assessment of various models for class-conditional image synthesis on ImageNet. Depicted classes: 108: sea
anemone (top) and 141: redshank (bottom). We report class labels as in VQVAE-2 [61].




                                                               17
               ours                       VQVAE-2 [61]                      BigGAN [4]                        MSP [19]




Figure 16. Qualitative assessment of various models for class-conditional image synthesis on ImageNet. Depicted classes: 11: goldfinch
(top) and 22: bald eagle (bottom).




                                                                 18
               ours                       VQVAE-2 [61]                      BigGAN [4]                         MSP [19]




Figure 17. Qualitative assessment of various models for class-conditional image synthesis on ImageNet. Depicted classes: 0: tench (top)
and 9: ostrich (bottom).




                                                                  19
                                                         933: cheeseburger
                    acc. rate 1.0                           acc. rate 0.5                            acc. rate 0.1




                                                            992: agaric
                    acc. rate 1.0                           acc. rate 0.5                            acc. rate 0.1




                                                        200: tibetian terrier
                    acc. rate 1.0                          acc. rate 0.5                             acc. rate 0.1




Figure 18. Visualizing the effect of increased rejection rate (i.e. lower acceptance rate) by using a ResNet-101 classifier trained on
ImageNet and samples from our class-conditional ImageNet model. Higher rejection rates tend to produce images showing more central,
recognizable objects compared to the unguided samples. Here, k = 973, p = 1.0 are fixed for all samples. Note that k = 973 is the
effective size of the VQGAN’s codebook, i.e. it describes how many entries of the codebook with dim Z = 16384 are actually used.




                                                                 20
                                                          933: cheeseburger
                      k = 973                                  k = 300                                  k = 100




                                                             992: agaric
                      k = 973                                 k = 300                                   k = 100




                                                         200: tibetian terrier
                      k = 973                                  k = 300                                  k = 100




Figure 19. Visualizing the effect of varying k in top-k sampling (i.e. truncating the probability distribution per image token) by using
a ResNet-101 classifier trained on ImageNet and samples from our class-conditional ImageNet model. Lower values of k produce more
uniform, low-entropic images compared to samples obtained with full k. Here, an acceptance rate of 1.0 and p = 1.0 are fixed for all
samples. Note that k = 973 is the effective size of the VQGAN’s codebook, i.e. it describes how many entries of the codebook with
dim Z = 16384 are actually used.




                                                                  21
                                                          933: cheeseburger
                      p = 1.0                                 p = 0.96                                 p = 0.84




                                                             992: agaric
                      p = 1.0                                 p = 0.96                                 p = 0.84




                                                         200: tibetian terrier
                      p = 1.0                                 p = 0.96                                 p = 0.84




Figure 20. Visualizing the effect of varying p in top-p sampling (or nucleus sampling [27]) by using a ResNet-101 classifier trained on
ImageNet and samples from our class-conditional ImageNet model. Lowering p has similar effects as decreasing k, see Fig. 19. Here, an
acceptance rate of 1.0 and k = 973 are fixed for all samples.




                                                                  22
Figure 21. Random samples on 256 × 256 class-conditional ImageNet with k ∈ [100, 200, 250, 300, 350, 400, 500, 600, 800, 973] ,
p = 1.0, acceptance rate 1.0. FID: 17.04, IS: 70.6 ± 1.8. Please see https://git.io/JLlvY for an uncompressed version.
                                                              23
Figure 22. Random samples on 256 × 256 class-conditional ImageNet with k = 600, p = 1.0, acceptance rate 0.05. FID: 5.20, IS:
280.3 ± 5.5. Please see https://git.io/JLlvY for an uncompressed version.
                                                             24
Figure 23. Random samples on 256 × 256 class-conditional ImageNet with k = 250, p = 1.0, acceptance rate 1.0. FID: 15.98, IS:
78.6 ± 1.1. Please see https://git.io/JLlvY for an uncompressed version.
                                                             25
Figure 24. Random samples on 256 × 256 class-conditional ImageNet with k = 973, p = 0.88, acceptance rate 1.0. FID: 15.78, IS:
74.3 ± 1.8. Please see https://git.io/JLlvY for an uncompressed version.
                                                             26
Figure 25. Random samples on 256 × 256 class-conditional ImageNet with k ∈ [100, 200, 250, 300, 350, 400, 500, 600, 800, 973] ,
p = 1.0, acceptance rate 0.005. FID: 6.59, IS: 402.7 ± 2.9. Please see https://git.io/JLlvY for an uncompressed version.
                                                              27
Figure 26. Random samples on 256 × 256 class-conditional ImageNet with k ∈ [100, 200, 250, 300, 350, 400, 500, 600, 800, 973] ,
p = 1.0, acceptance rate 0.05. FID: 5.88, IS: 304.8 ± 3.6. Please see https://git.io/JLlvY for an uncompressed version.
                                                              28
           conditioning                                           ours (top) vs iGPT [8] (bottom)




Figure 27. Comparing our approach with the pixel-based approach of [8]. Here, we use our f = 16 S-FLCKR model to obtain high-fidelity
image completions of the inputs depicted on the left (half completions). For each conditioning, we show three of our samples (top) and
three of [8] (bottom).



                                                                 29
           conditioning                                           ours (top) vs iGPT [8] (bottom)




Figure 28. Comparing our approach with the pixel-based approach of [8]. Here, we use our f = 16 S-FLCKR model to obtain high-fidelity
image completions of the inputs depicted on the left (half completions). For each conditioning, we show three of our samples (top) and
three of [8] (bottom).



                                                                 30
Figure 29. Samples generated from semantic layouts on S-FLCKR. Sizes from top-to-bottom: 1280 × 832, 1024 × 416 and 1280 × 240
pixels.


                                                             31
Figure 30. Samples generated from semantic layouts on S-FLCKR. Sizes from top-to-bottom: 1536 × 512, 1840 × 1024, and 1536 × 620
pixels.
                                                              32
Figure 31. Samples generated from semantic layouts on S-FLCKR. Sizes from top-to-bottom: 2048 × 512, 1460 × 440, 2032 × 448 and
2016 × 672 pixels.




                                                              33
Figure 32. Samples generated from semantic layouts on S-FLCKR. Sizes from top-to-bottom: 1280 × 832, 1024 × 416 and 1280 × 240
pixels.
                                                             34
conditioning                                                        samples




       Figure 33. Depth-guided neural rendering on RIN with f = 16 using the sliding attention window.




                                                     35
conditioning                                                        samples




       Figure 34. Depth-guided neural rendering on RIN with f = 16 using the sliding attention window.




                                                     36
                 conditioning                                                             samples




Figure 35. Intentionally limiting the receptive field can lead to interesting creative applications like this one: Edge-to-Image synthesis on
IN with f = 8, using the sliding attention window.


                                                                     37
    conditioning                                                           samples




Figure 36. Additional results for stochastic superresolution with an f = 16 model on IN, using the sliding attention window.




                                                            38
conditioning                                                         samples




Figure 37. Samples generated from semantic layouts on S-FLCKR with f = 16, using the sliding attention window.




                                                     39
conditioning                                                         samples




Figure 38. Samples generated from semantic layouts on S-FLCKR with f = 32, using the sliding attention window.




                                                     40
Figure 39. Unconditional samples from a model trained on LSUN Churches & Towers, using the sliding attention window.




                                                        41
conditioning               ground truth                      ours                     SPADE [53]




                                               42
      Figure 40. Qualitative comparison to [53] on 256 × 256 images from the ADE20K dataset.
conditioning                ground truth                      ours                     SPADE [53]




                                                43
     Figure 41. Qualitative comparison to [53] on 256 × 256 images from the COCO-Stuff dataset.
conditioning                       samples                   conditioning                   samples




                            Figure 42. Conditional samples for the depth-to-image model on IN.



conditioning                       samples                   conditioning                   samples




             Figure 43. Conditional samples for the pose-guided synthesis model via keypoints on DeepFashion.



  class exemplar                   samples                      class exemplar              samples




                       Figure 44. Samples produced by the class-conditional model trained on RIN.



                                                           44
                                          sample                  nearest neighbors

             CelebA-HQ (best val. NLL)
             CelebA-HQ (best train NLL)
             FFHQ (best val. NLL)
             FFHQ (best train NLL)




Figure 45. Nearest neighbors for our face-models trained on FFHQ and CelebA-HQ (256 × 256 pix), based on the LPIPS [82] distance.
The left column shows a sample from our model, while the 10 examples to the right show the nearest neighbors from the corresponding
class (increasing distance) in the training dataset. We evaluate two different model checkpoints for each dataset: Best val. NLL denotes
the minimal NLL over the course of training, evaluated on unseen testdata. For this checkpoint, both models generate crisp, high-quality
samples not present in the training data. However, when drastically overfitting the model, it reproduces samples from the training data
(best train NLL). Although not an ideal measure of image quality, NLL thus provides a proxy on model selection, whereas FID does not.
See also Sec. E.

                                                                  45
            k = 250, p = 1.0, a = 1.0
            k = 973, p = 0.88, a = 1.0
            mixed k, p = 1.0, a = 0.05
            mixed k, p = 1.0, a = 0.005




Figure 46. Nearest neighbors for our class-conditional ImageNet model (256 × 256 pix), based on the LPIPS [82] distance. The left
column shows a sample from our model, while the 10 examples to the right show the nearest neighbors from the corresponding class
(increasing distance) in the training dataset. Our model produces new, unseen high-quality images, not present in the training data.

                                                                46
                                 row major                 spiral in           spiral out                                           z-curve                       subsample                       alternate

                            0     1    2     3       0 11 10 9              15 4      5     6                               0       1     4   5             0      4     1    5               0   1     2     3
                            4     5    6     7       1 12 15 8              14 3      0     7                               2       3     6   7             8 12 9 13                         7   6     5     4
                            8     9 10 11            2 13 14 7              13 2      1     8                               8       9 12 13                 2      6     3    7               8   9 10 11
                           12 13 14 15               3     4     5     6    12 11 10 9                                     10 11 14 15                      10 14 11 15                       15 14 13 12

                                                                                                                           5.2
                                                 Batch-wise Training Loss          subsample                                                                       Validation Loss                      subsample
                           5.6                                                     spiralin                                                                                                             spiralin
 negative log-likelihood




                                                                                                 negative log-likelihood




                                                                                   spiralout                               5.1                                                                          spiralout
                           5.4                                                     alternate                                                                                                            alternate
                                                                                   zcurve                                  5.0                                                                          zcurve
                           5.2                                                     rowmajor                                                                                                             rowmajor

                           5.0                                                                                             4.9


                           4.8                                                                                             4.8


                                 0     100000 200000 300000 400000 500000 600000 700000 800000                                   100000   200000   300000       400000   500000      600000    700000   800000
                                                         training step                                                                                             training step
Figure 47. Top: All sequence permutations we investigate, illustrated on a 4 × 4 grid. Bottom: The transformer architecture is permutation
invariant but next-token prediction is not: The average loss on the validation split of ImageNet, corresponding to the negative log-likelihood,
differs significantly between different prediction orderings. Among our choices, the commonly used row-major order performs best.




                                                                                                47
                                   Row Major                                            Subsample




                                     Z-Curve                                            Spiral Out




                                   Alternating                                           Spiral In




Figure 48. Random samples from transformer models trained with different orderings for autoregressive prediction as described in Sec. F.



                                                                  48
References
 [1] Jyoti Aneja, Alexander G. Schwing, Jan Kautz, and Arash Vahdat. NCP-VAE: variational autoencoders with noise contrastive priors.
     CoRR, abs/2010.02917, 2020. 8
 [2] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly learning to align and translate, 2016.
     2
 [3] Yoshua Bengio, Nicholas Léonard, and Aaron C. Courville. Estimating or propagating gradients through stochastic neurons for
     conditional computation. CoRR, abs/1308.3432, 2013. 4
 [4] Andrew Brock, Jeff Donahue, and Karen Simonyan. Large Scale GAN Training for High Fidelity Natural Image Synthesis. In 7th
     International Conference on Learning Representations, ICLR, 2019. 8, 10, 16, 17, 18, 19
 [5] Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav
     Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child,
     Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott
     Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. Language
     Models are Few-Shot Learners. arXiv preprint arXiv:2005.14165, 2020. 1
 [6] Holger Caesar, Jasper Uijlings, and Vittorio Ferrari. COCO-Stuff: Thing and stuff classes in context. In Computer vision and pattern
     recognition (CVPR), 2018 IEEE conference on. IEEE, 2018. 6
 [7] Liang-Chieh Chen, G. Papandreou, I. Kokkinos, Kevin Murphy, and A. Yuille. DeepLab: Semantic Image Segmentation with
     Deep Convolutional Nets, Atrous Convolution, and Fully Connected CRFs. IEEE Transactions on Pattern Analysis and Machine
     Intelligence, 2018. 5
 [8] Mark Chen, Alec Radford, Rewon Child, Jeff Wu, Heewoo Jun, Prafulla Dhariwal, David Luan, and Ilya Sutskever. Generative
     pretraining from pixels. 2020. 1, 2, 3, 4, 5, 6, 7, 12, 13, 14, 29, 30
 [9] Qifeng Chen and Vladlen Koltun. Photographic image synthesis with cascaded refinement networks. In IEEE International Con-
     ference on Computer Vision, ICCV 2017, Venice, Italy, October 22-29, 2017, pages 1520–1529. IEEE Computer Society, 2017.
     7
[10] Xi Chen, Nikhil Mishra, Mostafa Rohaninejad, and Pieter Abbeel. Pixelsnail: An improved autoregressive generative model. In
     ICML, volume 80 of Proceedings of Machine Learning Research, pages 863–871. PMLR, 2018. 2, 5
[11] Rewon Child. Very deep vaes generalize autoregressive models and can outperform them on images. CoRR, abs/2011.10650, 2020.
     8
[12] Rewon Child, Scott Gray, Alec Radford, and Ilya Sutskever. Generating long sequences with sparse transformers, 2019. 1, 2, 5
[13] Bin Dai and David P. Wipf. Diagnosing and enhancing VAE models. In 7th International Conference on Learning Representations,
     ICLR, 2019. 2
[14] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical image database. In 2009
     IEEE Computer Society Conference on Computer Vision and Pattern Recognition CVPR, 2009. 5
[15] Prafulla Dhariwal and Alex Nichol. Diffusion models beat gans on image synthesis, 2021. 8, 10
[16] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani,
     Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image is worth 16x16 words: Transformers for image recognition at
     scale. 2020. 1
[17] Alexey Dosovitskiy and Thomas Brox. Generating Images with Perceptual Similarity Metrics based on Deep Networks. In Advances
     in Neural Information Processing Systems 29: Annual Conference on Neural Information Processing Systems, NeurIPS, 2016. 4
[18] Patrick Esser, Robin Rombach, and Björn Ommer. A Disentangling Invertible Interpretation Network for Explaining Latent Repre-
     sentations. In 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR, 2020. 2
[19] Jeffrey De Fauw, Sander Dieleman, and Karen Simonyan. Hierarchical autoregressive image models with auxiliary decoders. CoRR,
     abs/1903.04933, 2019. 16, 17, 18, 19
[20] Ian J. Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron C. Courville, and Yoshua
     Bengio. Generative Adversarial Nets. In Advances in Neural Information Processing Systems 27: Annual Conference on Neural
     Information Processing Systems, NeurIPS, 2014. 2
[21] Seungwook Han, Akash Srivastava, Cole L. Hurwitz, Prasanna Sattigeri, and David D. Cox. not-so-biggan: Generating high-fidelity
     images on a small compute budget. CoRR, abs/2009.04433, 2020. 2
[22] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. CoRR, abs/1512.03385,
     2015. 8
[23] Ari Heljakka, Arno Solin, and Juho Kannala. Pioneer networks: Progressively growing generative autoencoder. In C. V. Jawahar,
     Hongdong Li, Greg Mori, and Konrad Schindler, editors, Computer Vision - ACCV 2018 - 14th Asian Conference on Computer Vision,
     Perth, Australia, December 2-6, 2018, Revised Selected Papers, Part I, 2018. 8
[24] Ari Heljakka, Arno Solin, and Juho Kannala. Towards photographic image manipulation with balanced growing of generative
     autoencoders. In IEEE Winter Conference on Applications of Computer Vision, WACV 2020, Snowmass Village, CO, USA, March
     1-5, 2020, pages 3109–3118. IEEE, 2020. 8


                                                                   49
[25] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models, 2020. 11
[26] Jonathan Ho, Nal Kalchbrenner, Dirk Weissenborn, and Tim Salimans. Axial attention in multidimensional transformers. CoRR,
     abs/1912.12180, 2019. 2, 5
[27] Ari Holtzman, Jan Buys, Li Du, Maxwell Forbes, and Yejin Choi. The curious case of neural text degeneration. In ICLR. OpenRe-
     view.net, 2020. 8, 22
[28] Phillip Isola, Jun-Yan Zhu, Tinghui Zhou, and Alexei A. Efros. Image-to-Image Translation with Conditional Adversarial Networks.
     In 2017 IEEE Conference on Computer Vision and Pattern Recognition, CVPR, 2017. 4, 11
[29] Eric Jang, Shixiang Gu, and Ben Poole. Categorical reparameterization with gumbel-softmax. arXiv preprint arXiv:1611.01144,
     2016. 9
[30] Justin Johnson, Alexandre Alahi, and Li Fei-Fei. Perceptual losses for real-time style transfer and super-resolution. In ECCV (2),
     volume 9906 of Lecture Notes in Computer Science, pages 694–711. Springer, 2016. 4
[31] Tero Karras, Timo Aila, Samuli Laine, and Jaakko Lehtinen. Progressive growing of gans for improved quality, stability, and variation.
     CoRR, abs/1710.10196, 2017. 6, 8
[32] Tero Karras, Miika Aittala, Janne Hellsten, Samuli Laine, Jaakko Lehtinen, and Timo Aila. Training generative adversarial networks
     with limited data. In Hugo Larochelle, Marc’Aurelio Ranzato, Raia Hadsell, Maria-Florina Balcan, and Hsuan-Tien Lin, editors, Ad-
     vances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS
     2020, December 6-12, 2020, virtual, 2020. 8
[33] Tero Karras, Samuli Laine, and Timo Aila. A style-based generator architecture for generative adversarial networks. In IEEE
     Conference on Computer Vision and Pattern Recognition, (CVPR) 2019, Long Beach, CA, USA, June 16-20, 2019, pages 4401–4410.
     Computer Vision Foundation / IEEE, 2019. 7
[34] Tero Karras, Samuli Laine, Miika Aittala, Janne Hellsten, Jaakko Lehtinen, and Timo Aila. Analyzing and improving the image
     quality of stylegan. In 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR 2020, Seattle, WA, USA,
     June 13-19, 2020, pages 8107–8116. IEEE, 2020. 8
[35] Prateek Katiyar and Anna Khoreva. Improving augmentation and evaluation schemes for semantic image synthesis, 2021. 7
[36] Yoon Kim, Carl Denton, Luong Hoang, and Alexander M. Rush. Structured attention networks, 2017. 2
[37] Diederik P. Kingma and Prafulla Dhariwal. Glow: Generative Flow with Invertible 1x1 Convolutions. In Advances in Neural
     Information Processing Systems 31: Annual Conference on Neural Information Processing Systems 2018, NeurIPS, 2018. 8
[38] Diederik P. Kingma and Max Welling. Auto-Encoding Variational Bayes. In 2nd International Conference on Learning Representa-
     tions, ICLR, 2014. 2
[39] Alex Lamb, Vincent Dumoulin, and Aaron C. Courville. Discriminative regularization for generative models. CoRR, abs/1602.03220,
     2016. 4
[40] Anders Boesen Lindbo Larsen, Søren Kaae Sønderby, Hugo Larochelle, and Ole Winther. Autoencoding beyond pixels using a
     learned similarity metric, 2015. 4
[41] Naihan Li, Shujie Liu, Yanqing Liu, Sheng Zhao, and Ming Liu. Neural speech synthesis with transformer network. In AAAI, pages
     6706–6713. AAAI Press, 2019. 2
[42] Chieh Hubert Lin, Chia-Che Chang, Yu-Sheng Chen, Da-Cheng Juan, Wei Wei, and Hwann-Tzong Chen. COCO-GAN: generation
     by parts via conditional coordinating. In ICCV, pages 4511–4520. IEEE, 2019. 5
[43] Jinlin Liu, Yuan Yao, and Jianqiang Ren. An acceleration framework for high resolution image synthesis. CoRR, abs/1909.03611,
     2019. 2
[44] Peter J. Liu, Mohammad Saleh, Etienne Pot, Ben Goodrich, Ryan Sepassi, Lukasz Kaiser, and Noam Shazeer. Generating wikipedia
     by summarizing long sequences. In ICLR (Poster). OpenReview.net, 2018. 4
[45] Ziwei Liu, Ping Luo, Shi Qiu, Xiaogang Wang, and Xiaoou Tang. Deepfashion: Powering robust clothes recognition and retrieval
     with rich annotations. In Proceedings of IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2016. 6
[46] Jacob Menick and Nal Kalchbrenner. Generating high fidelity images with subscale pixel networks and multidimensional upscaling.
     In 7th International Conference on Learning Representations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019. OpenReview.net,
     2019. 14
[47] Fabian Mentzer, George Toderici, Michael Tschannen, and Eirikur Agustsson. High-fidelity generative image compression, 2020. 4
[48] Charlie Nash, Jacob Menick, Sander Dieleman, and Peter W. Battaglia. Generating images with sparse representations, 2021. 8, 10
[49] Alex Nichol and Prafulla Dhariwal. Improved denoising diffusion probabilistic models, 2021. 8, 10
[50] Anton Obukhov, Maximilian Seitzer, Po-Wei Wu, Semen Zhydenko, Jonathan Kyl, and Elvis Yu-Jing Lin. High-fidelity performance
     metrics for generative models in pytorch, 2020. Version: 0.3.0, DOI: 10.5281/zenodo.4957738. 7, 10
[51] B. Ommer and J. M. Buhmann. Learning the compositional nature of visual objects. In 2007 IEEE Conference on Computer Vision
     and Pattern Recognition, pages 1–8, 2007. 2
[52] Ankur P. Parikh, Oscar Täckström, Dipanjan Das, and Jakob Uszkoreit. A decomposable attention model for natural language
     inference, 2016. 2


                                                                    50
[53] Taesung Park, Ming-Yu Liu, Ting-Chun Wang, and Jun-Yan Zhu. Semantic Image Synthesis with Spatially-Adaptive Normalization.
     In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, CVPR, 2019. 2, 7, 42, 43
[54] Gaurav Parmar, Dacheng Li, Kwonjoon Lee, and Zhuowen Tu. Dual contradistinctive generative autoencoder, 2020. 8
[55] Niki Parmar, Ashish Vaswani, Jakob Uszkoreit, Lukasz Kaiser, Noam Shazeer, Alexander Ku, and Dustin Tran. Image transformer.
     In ICML, volume 80 of Proceedings of Machine Learning Research, pages 4052–4061. PMLR, 2018. 2, 3, 5
[56] Stanislav Pidhorskyi, Donald A. Adjeroh, and Gianfranco Doretto. Adversarial latent autoencoders. In 2020 IEEE/CVF Conference
     on Computer Vision and Pattern Recognition, CVPR 2020, Seattle, WA, USA, June 13-19, 2020, pages 14092–14101. IEEE, 2020. 8
[57] A. Radford. Improving language understanding by generative pre-training. 2018. 1
[58] A. Radford, Jeffrey Wu, R. Child, David Luan, Dario Amodei, and Ilya Sutskever. Language models are unsupervised multitask
     learners. 2019. 1, 5, 11
[59] Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec Radford, Mark Chen, and Ilya Sutskever. Zero-shot
     text-to-image generation, 2021. 9, 10, 12
[60] René Ranftl, Katrin Lasinger, David Hafner, Konrad Schindler, and Vladlen Koltun. Towards robust monocular depth estimation:
     Mixing datasets for zero-shot cross-dataset transfer. IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI), 2020.
     5
[61] Ali Razavi, Aaron van den Oord, and Oriol Vinyals. Generating diverse high-fidelity images with vq-vae-2, 2019. 3, 4, 5, 8, 16, 17,
     18, 19
[62] Danilo Jimenez Rezende, Shakir Mohamed, and Daan Wierstra. Stochastic backpropagation and approximate inference in deep
     generative models. In Proceedings of the 31st International Conference on International Conference on Machine Learning, ICML,
     2014. 2
[63] Robin Rombach, Patrick Esser, and Björn Ommer. Making sense of cnns: Interpreting deep representations and their invariances
     with inns. In Andrea Vedaldi, Horst Bischof, Thomas Brox, and Jan-Michael Frahm, editors, Computer Vision - ECCV 2020 - 16th
     European Conference, Glasgow, UK, August 23-28, 2020, Proceedings, Part XVII, volume 12362 of Lecture Notes in Computer
     Science, pages 647–664. Springer, 2020. 2
[64] Robin Rombach, Patrick Esser, and Bjorn Ommer. Network-to-network translation with conditional invertible neural networks. In
     H. Larochelle, M. Ranzato, R. Hadsell, M. F. Balcan, and H. Lin, editors, Advances in Neural Information Processing Systems,
     volume 33, pages 2784–2797. Curran Associates, Inc., 2020. 2
[65] Shibani Santurkar, Dimitris Tsipras, Brandon Tran, Andrew Ilyas, Logan Engstrom, and Aleksander Madry. Computer vision with a
     single (robust) classifier. In ArXiv preprint arXiv:1906.09453, 2019. 5
[66] Edgar Schönfeld, Bernt Schiele, and Anna Khoreva. A u-net based discriminator for generative adversarial networks. In 2020
     IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR 2020, Seattle, WA, USA, June 13-19, 2020, pages 8204–
     8213. IEEE, 2020. 8
[67] Kim Seonghyeon. Implementation of generating diverse high-fidelity images with vq-vae-2 in pytorch, 2020. 8
[68] Aliaksandr Siarohin, Stéphane Lathuilière, Sergey Tulyakov, Elisa Ricci, and Nicu Sebe. First order motion model for image anima-
     tion. In Conference on Neural Information Processing Systems (NeurIPS), December 2019. 2
[69] Arash Vahdat and Jan Kautz. NVAE: A deep hierarchical variational autoencoder. In Hugo Larochelle, Marc’Aurelio Ranzato,
     Raia Hadsell, Maria-Florina Balcan, and Hsuan-Tien Lin, editors, Advances in Neural Information Processing Systems 33: Annual
     Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual, 2020. 8
[70] Aäron van den Oord, Nal Kalchbrenner, and Koray Kavukcuoglu. Pixel recurrent neural networks. In ICML, volume 48 of JMLR
     Workshop and Conference Proceedings, pages 1747–1756. JMLR.org, 2016. 2
[71] Aaron van den Oord, Nal Kalchbrenner, Oriol Vinyals, Lasse Espeholt, Alex Graves, and Koray Kavukcuoglu. Conditional image
     generation with pixelcnn decoders, 2016. 2, 4, 14
[72] Aaron van den Oord, Oriol Vinyals, and Koray Kavukcuoglu. Neural discrete representation learning, 2018. 3, 4, 11
[73] Hugo van Kemenade, wiredfool, Andrew Murray, Alex Clark, Alexander Karpinsky, Ondrej Baranovič, Christoph Gohlke, Jon
     Dufresne, Brian Crowell, David Schmidt, Konstantin Kopachev, Alastair Houghton, Sandro Mani, Steve Landey, vashek, Josh Ware,
     Jason Douglas, David Caro, Uriel Martinez, Steve Kossouho, Riley Lahd, Stanislau T., Antony Lee, Eric W. Brown, Oliver Tonnhofer,
     Mickael Bonfill, Peter Rowlands, Fahad Al-Saidi, German Novikov, and Michał Górny. python-pillow/pillow: 8.2.0, Apr. 2021. 10
[74] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin.
     Attention is All you Need. In Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information
     Processing Systems, NeurIPS, 2017. 1, 2, 3
[75] Ting-Chun Wang, Ming-Yu Liu, Jun-Yan Zhu, Andrew Tao, Jan Kautz, and Bryan Catanzaro. High-resolution image synthesis and
     semantic manipulation with conditional gans. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,
     2018. 7
[76] Dirk Weissenborn, Oscar Täckström, and Jakob Uszkoreit. Scaling autoregressive video models. In ICLR. OpenReview.net, 2020. 2
[77] Zhisheng Xiao, Karsten Kreis, Jan Kautz, and Arash Vahdat. Vaebm: A symbiosis between variational autoencoders and energy-based
     models, 2021. 8


                                                                  51
[78] Zhisheng Xiao, Qing Yan, Yi-an Chen, and Yali Amit. Generative latent flow: A framework for non-adversarial image generation.
     CoRR, abs/1905.10485, 2019. 2
[79] Fisher Yu, Yinda Zhang, Shuran Song, Ari Seff, and Jianxiong Xiao. Lsun: Construction of a large-scale image dataset using deep
     learning with humans in the loop. arXiv preprint arXiv:1506.03365, 2015. 5
[80] Pan Zhang, Bo Zhang, Dong Chen, Lu Yuan, and Fang Wen. Cross-Domain Correspondence Learning for Exemplar-Based Image
     Translation. In 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR, 2020. 2
[81] Richard Zhang, Phillip Isola, Alexei A Efros, Eli Shechtman, and Oliver Wang. The Unreasonable Effectiveness of Deep Features as
     a Perceptual Metric. In CVPR, 2018. 4, 11, 13
[82] Richard Zhang, Phillip Isola, Alexei A Efros, Eli Shechtman, and Oliver Wang. The unreasonable effectiveness of deep features as a
     perceptual metric. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, CVPR, 2018. 45, 46
[83] Bolei Zhou, Hang Zhao, Xavier Puig, Sanja Fidler, Adela Barriuso, and Antonio Torralba. Semantic understanding of scenes through
     the ade20k dataset. arXiv preprint arXiv:1608.05442, 2016. 6
[84] Tinghui Zhou, Shubham Tulsiani, Weilun Sun, Jitendra Malik, and Alexei A. Efros. View synthesis by appearance flow, 2017. 2
[85] Peihao Zhu, Rameen Abdal, Yipeng Qin, and Peter Wonka. Sean: Image synthesis with semantic region-adaptive normalization,
     2019. 2




                                                                  52
