# 2020 DETR Survey Answers

## 1. Basic Metadata
- Title: "End-to-End Object Detection with Transformers" (Page 1, title block)
- Authors: "Nicolas Carion? , Francisco Massa? , Gabriel Synnaeve, Nicolas Usunier, Alexander Kirillov, and Sergey Zagoruyko" (Page 1, title block)
- Year: 2020 (evidence: "arXiv:2005.12872v3 [cs.CV] 28 May 2020" - Page 1, arXiv header)
- Venue: arXiv (evidence: "arXiv:2005.12872v3 [cs.CV]" - Page 1, arXiv header)

## 2. One-Sentence Contribution Summary
DETR proposes an end-to-end object detector that frames detection as direct set prediction using a transformer encoder-decoder and bipartite matching loss to remove hand-designed detection components. (Evidence: "We present a new method that views object detection as a direct set prediction problem." - Abstract; "The main ingredients of the new framework, called DEtection TRansformer or DETR, are a set-based global loss that forces unique predictions via bipartite matching, and a transformer encoder-decoder architecture." - Abstract)

## 3. Tasks Evaluated
Task 1: Object detection
- Task type: Detection
- Dataset(s): COCO 2017 detection
- Domain: Images (COCO)
- Evidence: "The goal of object detection is to predict a set of bounding boxes and category labels for each object of interest." (Section 1 Introduction)
- Evidence: "We evaluate DETR on one of the most popular object detection datasets, COCO [24], against a very competitive Faster R-CNN baseline [37]." (Section 1 Introduction)
- Evidence: "We perform experiments on COCO 2017 detection and panoptic segmentation datasets [24,18], containing 118k training images and 5k validation images." (Section 4 Experiments)

Task 2: Panoptic segmentation
- Task type: Segmentation (panoptic)
- Dataset(s): COCO 2017 panoptic segmentation
- Domain: Images (COCO)
- Evidence: "In our experiments, we show that a simple segmentation head trained on top of a pre-trained DETR outperfoms competitive baselines on Panoptic Segmentation [19], a challenging pixel-level recognition task that has recently gained popularity." (Section 1 Introduction)
- Evidence: "Panoptic segmentation [19] has recently attracted a lot of attention from the computer vision community." (Section 4.4 DETR for panoptic segmentation)
- Evidence: "We perform experiments on COCO 2017 detection and panoptic segmentation datasets [24,18], containing 118k training images and 5k validation images." (Section 4 Experiments)

## 4. Domain and Modality Scope
- Single domain or multiple domains within same modality: Evaluation is on COCO detection and panoptic segmentation, which are both from the COCO dataset. (Evidence: "We perform experiments on COCO 2017 detection and panoptic segmentation datasets [24,18]" - Section 4 Experiments)
- Multiple modalities: Not specified in the paper.
- Domain generalization or cross-domain transfer: Not claimed.

## 5. Model Sharing Across Tasks
| Task | Shared Weights? | Fine-Tuned? | Separate Head? | Evidence |
| --- | --- | --- | --- | --- |
| Object detection | Yes, DETR weights are reused for panoptic segmentation | Not specified | No (base detection head) | "a simple segmentation head trained on top of a pre-trained DETR" (Section 1 Introduction) |
| Panoptic segmentation | Yes, DETR is used as a base model | Base DETR is frozen in reported setup; mask head trained | Yes, mask head | "a simple segmentation head trained on top of a pre-trained DETR" (Section 1 Introduction); "The mask head can be trained either jointly, or in a two steps process, where we train DETR for boxes only, then freeze all the weights and train only the mask head for 25 epochs." (Section 4.4 DETR for panoptic segmentation) |

## 6. Input and Representation Constraints
- Fixed or variable input resolution: Variable, with explicit resizing constraints. (Evidence: "We use scale augmentation, resizing the input images such that the shortest side is at least 480 and at most 800 pixels while the longest at most 1333 [50]." - Section 4 Experiments)
- Fixed patch size: Not specified in the paper.
- Fixed number of tokens: Decoder uses a fixed number of predictions (queries). (Evidence: "DETR infers a fixed-size set of N predictions, in a single pass through the decoder, where N is set to be significantly larger than the typical number of objects in an image." - Section 3.1; "All models were trained with N = 100 decoder query slots." - Appendix A.4 Training hyperparameters)
- Fixed dimensionality (e.g., strictly 2D): The paper is explicit about images, but does not discuss other modalities. (Evidence: "containing 118k training images and 5k validation images. Each image is annotated with bounding boxes and panoptic segmentation." - Section 4 Experiments)
- Padding or resizing requirements: Yes, explicit padding and resizing. (Evidence: "The input images are batched together, applying 0-padding adequately to ensure they all have the same dimensions (H0 , W0) as the largest image of the batch." - Section 3.2 Backbone footnote; "We use scale augmentation, resizing the input images such that the shortest side is at least 480 and at most 800 pixels while the longest at most 1333 [50]." - Section 4 Experiments)

## 7. Context Window and Attention Structure
- Maximum sequence length: Not specified in the paper.
- Fixed or variable sequence length: Encoder sequence length varies with input spatial size; decoder length is fixed. (Evidence: "The encoder expects a sequence as input, hence we collapse the spatial dimensions of z0 into one dimension" - Section 3.2 Transformer encoder; "DETR infers a fixed-size set of N predictions" - Section 3.1)
- Attention type: Global self-attention over the full sequence. (Evidence: "The self-attention mechanisms of transformers, which explicitly model all pairwise interactions between elements in a sequence" - Section 1 Introduction; "Attention mechanisms [2] are neural network layers that aggregate information from the entire input sequence." - Section 2.2)
- Mechanisms to manage computational cost: Use lower-resolution CNN features and a fixed number of decoder queries; resolution increases are acknowledged as costly. (Evidence: "a conventional CNN backbone generates a lower-resolution activation map" - Section 3.2 Backbone; "DETR infers a fixed-size set of N predictions" - Section 3.1; "This modification increases the resolution by a factor of two, thus improving performance for small objects, at the cost of a 16x higher cost in the self-attentions of the encoder, leading to an overall 2x increase in computational cost." - Section 4 Experiments)

## 8. Positional Encoding (Critical Section)
- Mechanism used:
  - Spatial positions: Fixed absolute 2D sine/cosine encoding. (Evidence: "In our model we use a fixed absolute encoding to represent these spatial positions. We adopt a generalization of the original Transformer [47] encoding to the 2D case [31]. Specifically, for both spatial coordinates of each embedding we independently use d2 sine and cosine functions with different frequencies. We then concatenate them to get the final d channel positional encoding." - Appendix A.4 Training hyperparameters)
  - Output queries: Learned positional encodings. (Evidence: "These input embeddings are learnt positional encodings that we refer to as object queries" - Section 3.2 Transformer decoder)
- Where applied:
  - Spatial positional encodings are added at each attention layer. (Evidence: "Since the transformer architecture is permutation-invariant, we supplement it with fixed positional encodings [31,3] that are added to the input of each attention layer." - Section 3.2 Transformer encoder; "spatial positional encoding that are added to queries and keys at every multi-head self-attention layer." - Appendix A.3 Detailed architecture)
  - Output positional encodings (object queries) are added to the input of each decoder attention layer. (Evidence: "These input embeddings are learnt positional encodings that we refer to as object queries, and similarly to the encoder, we add them to the input of each attention layer." - Section 3.2 Transformer decoder)
- Fixed across experiments or modified: The paper explicitly varies positional encodings in ablations. (Evidence: "We experiment with various combinations of fixed and learned encodings, results can be found in table 3." - Section 4.2 Ablations)

## 9. Positional Encoding as a Variable
- Treated as a core research variable vs fixed assumption: It is an explicit ablation variable. (Evidence: "We experiment with various combinations of fixed and learned encodings, results can be found in table 3." - Section 4.2 Ablations)
- Multiple positional encodings compared: Yes (fixed vs learned; different insertion points). (Evidence: "We experiment with various combinations of fixed and learned encodings" - Section 4.2 Ablations)
- Claim that PE choice is not critical or secondary: Not explicitly claimed; they note sensitivity. (Evidence: "Surprisingly, we find that not passing any spatial encodings in the encoder only leads to a minor AP drop of 1.3 AP." - Section 4.2 Ablations)

## 10. Evidence of Constraint Masking
- Model size(s): "The model has 41.3M parameters" (Section 4.2 Ablations)
- Dataset size(s): "containing 118k training images and 5k validation images." (Section 4 Experiments)
- Scaling model size/backbone: "We report results with two different backbones: a ResNet-50 and a ResNet-101." (Section 4 Experiments)
- Training tricks and schedule: "Training settings for DETR differ from standard object detectors in multiple ways. The new model requires extra-long training schedule and benefits from auxiliary decoding losses in the transformer." (Section 1 Introduction); "For the longer schedule used to compare with Faster R-CNN we train for 500 epochs with learning rate drop after 400 epochs. This schedule adds 1.5 AP compared to the shorter schedule." (Section 4 Experiments)
- Data augmentation: "We use scale augmentation, resizing the input images such that the shortest side is at least 480 and at most 800 pixels while the longest at most 1333 [50]." and "we also apply random crop augmentations during training, improving the performance by approximately 1 AP." (Section 4 Experiments)
- Architectural contribution to performance: "DETR demonstrates significantly better performance on large objects, a result likely enabled by the non-local computations of the transformer." (Section 1 Introduction)

## 11. Architectural Workarounds
- Fixed-size output set to avoid duplicate predictions and control output size: "DETR infers a fixed-size set of N predictions, in a single pass through the decoder" (Section 3.1)
- CNN backbone to produce compact features: "a conventional CNN backbone generates a lower-resolution activation map" (Section 3.2 Backbone)
- Channel reduction for transformer input: "First, a 1x1 convolution reduces the channel dimension of the high-level activation map f from C to a smaller dimension d." (Section 3.2 Transformer encoder)
- Learned object queries to differentiate outputs: "A transformer decoder then takes as input a small fixed number of learned positional embeddings, which we call object queries" (Figure 2 caption)
- Task-specific head for panoptic segmentation: "DETR can be naturally extended by adding a mask head on top of the decoder outputs." (Section 4.4)
- FPN-like mask refinement: "To make the final prediction and increase the resolution, an FPN-like architecture is used." (Section 4.4)

## 12. Explicit Limitations and Non-Claims
- Limitations and future work: "It obtains, however, lower performances on small objects. We expect that future work will improve this aspect in the same way the development of FPN [22] did for Faster R-CNN." (Section 1 Introduction)
- Limitations and challenges: "This new design for detectors also comes with new challenges, in particular regarding training, optimization and performances on small objects." (Section 5 Conclusion)
- Explicit non-claims (e.g., open-world learning, unrestrained multi-task learning, cross-domain transfer): Not specified in the paper.
