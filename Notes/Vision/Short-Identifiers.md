| Short Identifier        | Full Title                                                                                      |
| ----------------------- | ----------------------------------------------------------------------------------------------- |
| 2000_FFRec              | Feedforward and Recurrent Processing in Vision                                                  |
| 2007_SudokuLogic        | The Hidden Logic of Sudoku (Second Edition)                                                     |
| 2007_CoreKnow           | Core Knowledge                                                                                  |
| 2012_CMPC               | Canonical Microcircuits for Predictive Coding                                                   |
| 2014_ITC                | Hierarchy of Intrinsic Timescales in Cortex                                                     |
| 2014_NTM                | Neural Turing Machines                                                                          |
| 2016_ACT                | Adaptive Computation Time                                                                       |
| 2017_AIAYN              | Attention Is All You Need                                                                       |
| 2018_RRN                | Recurrent Relational Networks                                                                   |
| 2018_UT                 | Universal Transformers                                                                          |
| 2018_CC3M               | Conceptual Captions: A Cleaned, Hypernymed, Image Alt-text Dataset                              |
| 2018_BERT               | BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding                |
| 2019_LXMERT             | LXMERT: Learning Cross-Modality Encoder Representations                                         |
| 2019_ViLBERT            | ViLBERT: Pretraining Task-Agnostic V-L Representations                                          |
| 2019_VisualBERT         | VisualBERT: A Simple and Performant Baseline for Vision and Language                            |
| 2019_DEQ                | Deep Equilibrium Models                                                                         |
| 2019_VideoBERT          | VideoBERT: A Joint Model for Video and Language Representation Learning                         |
| 2019_HowTo100M          | HowTo100M: Learning a Text-Video Embedding by Watching Narrated Videos                          |
| 2019_MOI                | On the Measure of Intelligence                                                                  |
| 2020_UNITER             | UNITER: Universal Image-Text Representation Learning                                            |
| 2020_OSCAR              | OSCAR: Object-Semantics Aligned Pre-training for Vision-Language Tasks                          |
| 2020_DETR               | End-to-End Object Detection with Transformers (DETR)                                            |
| 2020_ViT                | An Image is Worth 16×16 Words: Vision Transformer (ViT)                                         |
| 2021_ConViT             | ConViT: Improving Vision Transformers with Soft Convolutional Inductive Biases                  |
| 2021_TimeSformer        | Is Space-Time Attention All You Need for Video Understanding? (TimeSformer)                     |
| 2021_ALIGN              | Scaling Up Vision-Language Learning With Noisy Text Supervision (ALIGN)                         |
| 2021_CLIP               | Learning Transferable Visual Models From Natural Language Supervision (CLIP)                    |
| 2021_Swin               | Swin Transformer                                                                                |
| 2021_RoFormer           | RoFormer: Enhanced Transformer with Rotary Position Embedding (RoPE)                            |
| 2021_VATT               | VATT: Transformers for Multimodal Self-Supervised Learning                                      |
| 2021_ViLT               | ViLT: Vision-and-Language Transformer Without Convolution or Region Supervision                 |
| 2021_VideoCLIP          | VideoCLIP: Contrastive Pretraining for Zero-Shot Video-Text Understanding                       |
| 2021_ALBEF              | ALBEF: Align Before Fuse                                                                        |
| 2021_ALiBi              | Train Short, Test Long: Attention with Linear Biases (ALiBi)                                    |
| 2021_LAION400M          | LAION-400M: Open Dataset for CLIP Training                                                      |
| 2021_MAE                | Masked Autoencoders Are Scalable Vision Learners (MAE)                                          |
| 2022_BLIP               | BLIP: Bootstrapping Language-Image Pre-training                                                 |
| 2022_NoPELMs            | Transformer Language Models without Positional Encodings Still Learn Positional Information     |
| 2022_CoCa               | CoCa: Contrastive Captioners                                                                    |
| 2022_Flamingo           | Flamingo: a Visual Language Model for Few-Shot Learning                                         |
| 2022_Winoground         | Winoground: Probing Vision-Language Models for Compositionality                                 |
| 2022_LAION5B            | LAION-5B: An Open Large-Scale Dataset for Training Next Generation Image-Text Models            |
| 2022_ScienceQA          | ScienceQA: Benchmark for Multimodal Reasoning                                                   |
| 2023_BLIP2              | BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Models                            |
| 2023_GPT4               | GPT-4 Technical Report                                                                          |
| 2023_Kosmos1            | Kosmos-1: Language Is Not All You Need                                                          |
| 2023_PaLME              | PaLM-E: An Embodied Multimodal Language Model                                                   |
| 2023_LLaVA              | LLaVA: Large Language-and-Vision Assistant                                                      |
| 2023_MiniGPT4           | MiniGPT-4                                                                                       |
| 2023_ConceptARC         | ConceptARC                                                                                      |
| 2023_MMBench            | MMBench: Evaluating Multimodal LLMs                                                             |
| 2023_XPOS               | A Length-Extrapolatable Transformer (XPOS / LeX)                                                |
| 2023_AHAT               | Average-Hard Attention Transformers Are Threshold Circuits                                      |
| 2023_YaRN               | YaRN: Efficient Context Window Extension of Large Language Models                               |
| 2023_SphericalPE        | Spherical Position Encoding for Transformers                                                    |
| 2023_MMMU               | MMMU: A Massive Multidiscipline Multimodal Benchmark                                            |
| 2023_Gemini             | Gemini: A Family of Highly Capable Multimodal Models                                            |
| 2023_Uni3DL             | Uni3DL: Unified Model for 3D and Language Understanding                                         |
| 2024_EvoTTC             | Evolutionary Test-Time Compute (write-up)                                                       |
| 2024_FPD                | Fixed Point Diffusion Models                                                                    |
| 2024_AlphaGeometry      | Solving olympiad geometry without human demonstrations (AlphaGeometry)                          |
| 2024_GAA                | Gaussian Adaptive Attention Is All You Need                                                     |
| 2024_BeyondAStar        | Beyond A*: Planning with Transformers                                                           |
| 2024_RoPEViT            | Rotary Position Embedding for Vision Transformer (RoPE-ViT)                                     |
| 2024_VG4D               | VG4D: Vision-Language Model Goes 4D Video Recognition                                           |
| 2024_DAPE               | DAPE: Data-Adaptive Positional Encoding for Length Extrapolation                                |
| 2024_Idefics2           | What matters when building vision-language models? (Idefics2)                                   |
| 2024_RoPEBase           | Base of RoPE Bounds Context Length                                                              |
| 2024_RoTHP              | RoTHP: Rotary Position Embedding-based Transformer Hawkes Process                               |
| 2024_LieRE              | LieRE: Lie Rotational Positional Encodings                                                      |
| 2024_EDiffusion         | Learning Iterative Reasoning through Energy Diffusion                                           |
| 2024_SIPBS              | Simultaneous Instance Pooling & Bag Selection for MIL using ViTs                                |
| 2024_TTCScale           | Scaling LLM Test-Time Compute Optimally can be More Effective than Scaling Model Parameters     |
| 2024_NoPE               | Length Extrapolation of Causal Transformers without Position Encoding (NoPE)                    |
| 2024_HARC               | H-ARC: A Robust Estimate of Human Performance on the Abstraction and Reasoning Corpus Benchmark |
| 2024_PESurvey           | Length Extrapolation of Transformers: A Survey from the Perspective of Positional Encoding      |
| 2024_WaveletTFs         | Beyond Position: The Emergence of Wavelet-like Properties in Transformers                       |
| 2024_ARCHeavy           | ARC-Heavy / ARC-Potpourri (dataset description embedded in Cornell report)                      |
| 2024_IndTrans           | Combining Induction and Transduction for Abstract Reasoning                                     |
| 2024_LPS                | Searching Latent Program Spaces                                                                 |
| 2024_TTT                | The Surprising Effectiveness of Test-Time Training for Few-Shot Learning                        |
| 2024_NGPI               | Towards Efficient Neurally-Guided Program Induction for ARC-AGI                                 |
| 2024_MesaExtrap         | Mesa-Extrapolation: A Weave Position Encoding Method for Enhanced Extrapolation in LLMs         |
| 2025_APSRL              | Adaptive Patch Selection for ViTs via Reinforcement Learning                                    |
| 2025_AlphaProof         | Olympiad-level formal mathematical reasoning with large language models (AlphaProof)            |
| 2025_RoPEDI             | The Rotary Position Embedding May Cause Dimension Inefficiency                                  |
| 2025_VRoPE              | VRoPE: Rotary Position Embedding for Video Large Language Models                                |
| 2025_MPVG               | Maximizing the Position Embedding for Vision Transformers (MPVG)                                |
| 2025_SmolVLM            | SmolVLM: Redefining small and efficient multimodal models                                       |
| 2025_LOOPE              | LOOPE: Learnable Optimal Patch Order in Vision Transformers                                     |
| 2025_GatedAttention     | Gated Attention for LLMs: Non-linearity, Sparsity, Sink-Free                                    |
| 2025_CircleRoPE         | Circle-RoPE: Cone-like Decoupled Rotary Positional Embedding for Large Vision-Language Models   |
| 2025_RotaryMAE          | Rotary Masked Autoencoders Are Versatile Learners                                               |
| 2025_LLaVA4D            | LLaVA-4D: Embedding Spatiotemporal Prompt into LMMs                                             |
| 2025_ComRoPE            | ComRoPE: Scalable and Robust Rotary Position Embedding                                          |
| 2025_HRM                | Hierarchical Reasoning Model (HRM)                                                              |
| 2025_EVA02AT            | EVA02-AT: Egocentric Video-Language with Spatial-Temporal RoPE                                  |
| 2025_TransXSSM          | TransXSSM: Hybrid Transformer–SSM with Unified RoPE                                             |
| 2025_CARoPE             | Context-aware Rotary Position Embedding (CARoPE)                                                |
| 2025_PoPE               | Decoupling the "What" and "Where" With Polar Coordinate Positional Embeddings (PoPE)            |
| 2025_LessIsMore         | Less is More: Recursive Reasoning with Tiny Networks                                            |
| 2025_HARoPE             | Head-Wise Adaptive Rotary Positional Encoding (HARoPE)                                          |
| 2025_NestedLearning     | Nested Learning: The Illusion of Deep Learning Architecture                                     |
| 2025_DoPE               | DoPE: Denoising Rotary Position Embedding                                                       |
| 2025_WALRUS             | WALRUS: A Cross-Domain Foundation Model for Continuum Dynamics                                  |
| 2025_SelectiveRoPE      | Selective Rotary Position Embedding                                                             |
