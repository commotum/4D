| Title                                                                                                    | Lead Author | Published   | PDF URL                                                                   | Citations | Relevance |
| -------------------------------------------------------------------------------------------------------- | ----------- | ----------- | ------------------------------------------------------------------------- | --------: | --------: |
| End-to-End Object Detection with Transformers (DETR)                                                     | Carion      | 2020-05     | `https://arxiv.org/pdf/2005.12872.pdf`                                    |     21960 |       9.9 |
| Generative Pretraining From Pixels (iGPT)                                                                | Chen        | 2020 (ICML) | `https://cdn.openai.com/papers/Generative_Pretraining_from_Pixels_V2.pdf` |      2246 |       6.5 |
| An Image is Worth 16×16 Words: Transformers for Image Recognition at Scale (ViT)                         | Dosovitskiy | 2020-10     | `https://arxiv.org/pdf/2010.11929.pdf`                                    |     80578 |      10.0 |
| Deformable DETR: Deformable Transformers for End-to-End Object Detection                                 | Zhu         | 2020-10     | `https://arxiv.org/pdf/2010.04159.pdf`                                    |      8849 |       8.7 |
| Taming Transformers for High-Resolution Image Synthesis                                                  | Esser       | 2020-12     | `https://arxiv.org/pdf/2012.09841.pdf`                                    |      4397 |       7.8 |
| Training data-efficient image transformers & distillation through attention (DeiT)                       | Touvron     | 2020-12     | `https://arxiv.org/pdf/2012.12877.pdf`                                    |     10469 |       9.0 |
| Is Space-Time Attention All You Need for Video Understanding? (TimeSformer)                              | Bertasius   | 2021-02     | `https://arxiv.org/pdf/2102.05095.pdf`                                    |      3373 |       7.4 |
| Scaling Up Visual and Vision-Language Representation Learning With Noisy Text Supervision (ALIGN)        | Jia         | 2021-02     | `https://arxiv.org/pdf/2102.05918.pdf`                                    |      5674 |       8.1 |
| Zero-Shot Text-to-Image Generation (DALL·E)                                                              | Ramesh      | 2021-02     | `https://arxiv.org/pdf/2102.12092.pdf`                                    |      7807 |       8.6 |
| Pyramid Vision Transformer: A Versatile Backbone for Dense Prediction without Convolutions (PVT)         | Wang        | 2021-02     | `https://arxiv.org/pdf/2102.12122.pdf`                                    |      6196 |       8.3 |
| Learning Transferable Visual Models From Natural Language Supervision (CLIP)                             | Radford     | 2021-02     | `https://arxiv.org/pdf/2103.00020.pdf`                                    |     50075 |      10.0 |
| Transformer in Transformer (TNT)                                                                         | Han         | 2021-03     | `https://arxiv.org/pdf/2103.00112.pdf`                                    |      2604 |       7.0 |
| Swin Transformer: Hierarchical Vision Transformer using Shifted Windows                                  | Liu         | 2021-03     | `https://arxiv.org/pdf/2103.14030.pdf`                                    |     39387 |      10.0 |
| CrossViT: Cross-Attention Multi-Scale Vision Transformer for Image Classification                        | Chen        | 2021-03     | `https://arxiv.org/pdf/2103.14899.pdf`                                    |      2582 |       7.0 |
| CvT: Introducing Convolutions to Vision Transformers                                                     | Wu          | 2021-03     | `https://arxiv.org/pdf/2103.15808.pdf`                                    |      3090 |       7.5 |
| Twins: Revisiting the Design of Spatial Attention in Vision Transformers                                 | Chu         | 2021-04     | `https://arxiv.org/pdf/2104.13840.pdf`                                    |      1448 |       6.5 |
| SegFormer: Simple and Efficient Design for Semantic Segmentation with Transformers                       | Xie         | 2021-05     | `https://arxiv.org/pdf/2105.15203.pdf`                                    |      8502 |       8.7 |
| CoAtNet: Marrying Convolution and Attention for All Data Sizes                                           | Dai         | 2021-06     | `https://arxiv.org/pdf/2106.04803.pdf`                                    |      1915 |       7.1 |
| BEiT: BERT Pre-Training of Image Transformers                                                            | Bao         | 2021-06     | `https://arxiv.org/pdf/2106.08254.pdf`                                    |      4190 |       7.7 |
| CSWin Transformer: A General Vision Transformer Backbone with Cross-Shaped Windows                       | Dong        | 2021-07     | `https://arxiv.org/pdf/2107.00652.pdf`                                    |      1942 |       7.1 |
| Masked Autoencoders Are Scalable Vision Learners (MAE)                                                   | He          | 2021-11     | `https://arxiv.org/pdf/2111.06377.pdf`                                    |     12870 |       9.5 |
| Swin Transformer V2: Scaling Up Capacity and Resolution                                                  | Liu         | 2021-11     | `https://arxiv.org/pdf/2111.09883.pdf`                                    |      3194 |       7.5 |
| BLIP: Bootstrapping Language-Image Pre-training for Unified Vision-Language Understanding and Generation | Li          | 2022-01     | `https://arxiv.org/pdf/2201.12086.pdf`                                    |      7420 |       8.6 |
| MaxViT: Multi-Axis Vision Transformer                                                                    | Tu          | 2022-04     | `https://arxiv.org/pdf/2204.01697.pdf`                                    |      1243 |       6.8 |
| Flamingo: a Visual Language Model for Few-Shot Learning                                                  | Alayrac     | 2022-04     | `https://arxiv.org/pdf/2204.14198.pdf`                                    |      6582 |       8.9 |
| Image as a Foreign Language: BEiT Pretraining for All Vision and Vision-Language Tasks (BEiT-3)          | Wang        | 2022-08     | `https://arxiv.org/pdf/2208.10442.pdf`                                    |       796 |       6.4 |
| Segment Anything (SAM)                                                                                   | Kirillov    | 2023-04     | `https://arxiv.org/pdf/2304.02643.pdf`                                    |     16423 |      10.0 |
