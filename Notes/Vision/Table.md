| Title                                                                                                      | Lead Author | Published                | PDF URL                                                                                                                                                         | Citations | Relevance |
| ---------------------------------------------------------------------------------------------------------- | ----------- | ------------------------ | --------------------------------------------------------------------------------------------------------------------------------------------------------------- | --------: | --------: |
| Attention Is All You Need                                                                                  | Vaswani     | 2017-06                  | `https://arxiv.org/pdf/1706.03762.pdf`                                                                                                                          |    145000 |      10.0 |
| BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding                           | Devlin      | 2018-10                  | `https://arxiv.org/pdf/1810.04805.pdf`                                                                                                                          |    110000 |       9.5 |
| Conceptual Captions: A Cleaned, Hypernymed, Image Alt-text Dataset                                         | Sharma      | 2018-09                  | `https://arxiv.org/pdf/1803.10137.pdf`                                                                                                                          |      2200 |       8.0 |
| VisualBERT: A Simple and Performant Baseline for Vision and Language                                       | Li          | 2019-08                  | `https://arxiv.org/pdf/1908.03557.pdf`                                                                                                                          |      2200 |       8.5 |
| ViLBERT: Pretraining Task-Agnostic V-L Representations                                                     | Lu          | 2019-08                  | `https://arxiv.org/pdf/1908.02265.pdf`                                                                                                                          |      4200 |       9.0 |
| LXMERT: Learning Cross-Modality Encoder Representations                                                    | Tan         | 2019-08                  | `https://arxiv.org/pdf/1908.07490.pdf`                                                                                                                          |      5000 |       9.2 |
| VideoBERT: A Joint Model for Video and Language Representation Learning                                    | Sun         | 2019-10                  | `https://arxiv.org/pdf/1904.01766.pdf`                                                                                                                          |      1900 |       8.3 |
| HowTo100M: Learning a Text-Video Embedding by Watching Narrated Videos                                     | Miech       | 2019-11                  | `https://arxiv.org/pdf/1906.03327.pdf`                                                                                                                          |      3600 |       8.7 |
| UNITER: Universal Image-Text Representation Learning                                                       | Chen        | 2020-02                  | `https://arxiv.org/pdf/1909.11740.pdf`                                                                                                                          |      5100 |       9.4 |
| OSCAR: Object-Semantics Aligned Pre-training for Vision-Language Tasks                                     | Li          | 2020-04                  | `https://arxiv.org/pdf/2004.06165.pdf`                                                                                                                          |      4100 |       9.1 |
| End-to-End Object Detection with Transformers (DETR)                                                       | Carion      | 2020-05                  | `https://arxiv.org/pdf/2005.12872.pdf`                                                                                                                          |     22000 |       9.9 |
| An Image is Worth 16×16 Words: Vision Transformer (ViT)                                                    | Dosovitskiy | 2020-10                  | `https://arxiv.org/pdf/2010.11929.pdf`                                                                                                                          |     80000 |      10.0 |
| Learning Transferable Visual Models From Natural Language Supervision (CLIP)                               | Radford     | 2021-02                  | `https://arxiv.org/pdf/2103.00020.pdf`                                                                                                                          |     50000 |      10.0 |
| RoFormer: Enhanced Transformer with Rotary Position Embedding (RoPE)                                       | Su          | 2021-04                  | `https://arxiv.org/pdf/2104.09864.pdf`                                                                                                                          |      4369 |       9.8 |
| Scaling Up Vision-Language Learning With Noisy Text Supervision (ALIGN)                                    | Jia         | 2021-02                  | `https://arxiv.org/pdf/2102.05918.pdf`                                                                                                                          |      5600 |       9.2 |
| Train Short, Test Long: Attention with Linear Biases (ALiBi)                                               | Press       | 2021-08                  | `https://arxiv.org/pdf/2108.12409.pdf`                                                                                                                          |       983 |       9.3 |
| ViLT: Vision-and-Language Transformer Without Convolution or Region Supervision                            | Kim         | 2021-06                  | `https://arxiv.org/pdf/2102.03334.pdf`                                                                                                                          |      3000 |       8.8 |
| ALBEF: Align Before Fuse                                                                                   | Li          | 2021-07                  | `https://arxiv.org/pdf/2107.07651.pdf`                                                                                                                          |      3200 |       9.0 |
| Swin Transformer                                                                                           | Liu         | 2021-03                  | `https://arxiv.org/pdf/2103.14030.pdf`                                                                                                                          |     39000 |       9.8 |
| Masked Autoencoders Are Scalable Vision Learners (MAE)                                                     | He          | 2021-11                  | `https://arxiv.org/pdf/2111.06377.pdf`                                                                                                                          |     13000 |       9.6 |
| Is Space-Time Attention All You Need for Video Understanding? (TimeSformer)                                | Bertasius   | 2021-02                  | `https://arxiv.org/pdf/2102.05095.pdf`                                                                                                                          |      3400 |       8.4 |
| VATT: Transformers for Multimodal Self-Supervised Learning                                                 | Akbari      | 2021-06                  | `https://arxiv.org/pdf/2104.11178.pdf`                                                                                                                          |      1500 |       8.0 |
| VideoCLIP: Contrastive Pretraining for Zero-Shot Video-Text Understanding                                  | Xu          | 2021-06                  | `https://arxiv.org/pdf/2109.14084.pdf`                                                                                                                          |      1700 |       8.3 |
| LAION-400M: Open Dataset for CLIP Training                                                                 | Schuhmann   | 2021-11                  | `https://arxiv.org/pdf/2111.02114.pdf`                                                                                                                          |      1900 |       9.0 |
| BLIP: Bootstrapping Language-Image Pre-training                                                            | Li          | 2022-01                  | `https://arxiv.org/pdf/2201.12086.pdf`                                                                                                                          |      7400 |       9.3 |
| CoCa: Contrastive Captioners                                                                               | Yu          | 2022-03                  | `https://arxiv.org/pdf/2205.01917.pdf`                                                                                                                          |      1600 |       8.8 |
| Flamingo: a Visual Language Model for Few-Shot Learning                                                    | Alayrac     | 2022-04                  | `https://arxiv.org/pdf/2204.14198.pdf`                                                                                                                          |      6500 |       9.5 |
| Winoground: Probing Vision-Language Models for Compositionality                                            | Thrush      | 2022-06                  | `https://arxiv.org/pdf/2204.03162.pdf`                                                                                                                          |      1100 |       8.6 |
| ScienceQA: Benchmark for Multimodal Reasoning                                                              | Lu          | 2022-10                  | `https://arxiv.org/pdf/2209.09513.pdf`                                                                                                                          |      2100 |       8.7 |
| LAION-5B: An Open Large-Scale Dataset for Training Next Generation Image-Text Models                       | Schuhmann   | 2022-12                  | `https://arxiv.org/pdf/2210.08402.pdf`                                                                                                                          |      1200 |       9.4 |
| BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Models                                       | Li          | 2023-01                  | `https://arxiv.org/pdf/2301.12597.pdf`                                                                                                                          |      3800 |       9.6 |
| LLaVA: Large Language-and-Vision Assistant                                                                 | Liu         | 2023-04                  | `https://arxiv.org/pdf/2304.08485.pdf`                                                                                                                          |      6200 |       9.4 |
| MiniGPT-4                                                                                                  | Zhu         | 2023-04                  | `https://arxiv.org/pdf/2304.10592.pdf`                                                                                                                          |      2800 |       9.0 |
| PaLM-E: An Embodied Multimodal Language Model                                                              | Driess      | 2023-03                  | `https://arxiv.org/pdf/2303.03378.pdf`                                                                                                                          |      1600 |       8.8 |
| Kosmos-1: Language Is Not All You Need                                                                     | Huang       | 2023-03                  | `https://arxiv.org/pdf/2302.14045.pdf`                                                                                                                          |      1500 |       8.9 |
| GPT-4 Technical Report                                                                                     | OpenAI      | 2023-03                  | `https://arxiv.org/pdf/2303.08774.pdf`                                                                                                                          |      9000 |      10.0 |
| Gemini: A Family of Highly Capable Multimodal Models                                                       | Google      | 2023-12                  | `https://arxiv.org/pdf/2312.11805.pdf`                                                                                                                          |      3000 |       9.8 |
| YaRN: Efficient Context Window Extension of Large Language Models                                          | Peng        | 2023-09                  | `https://arxiv.org/pdf/2309.00071.pdf`                                                                                                                          |       620 |       9.4 |
| A Length-Extrapolatable Transformer (XPOS / LeX)                                                           | Sun         | 2023-07 (ACL)            | `https://aclanthology.org/2023.acl-long.816.pdf`                                                                                                                |       207 |       9.2 |
| MMBench: Evaluating Multimodal LLMs                                                                        | Liu         | 2023-06                  | `https://arxiv.org/pdf/2307.06281.pdf`                                                                                                                          |      1500 |       8.8 |
| MMMU: A Massive Multidiscipline Multimodal Benchmark                                                       | Yue         | 2023-10                  | `https://arxiv.org/pdf/2311.16502.pdf`                                                                                                                          |       900 |       8.9 |
| VG4D: Visual Grounding in 4D                                                                               | Wang        | 2024-03                  | `https://arxiv.org/pdf/2403.05659.pdf`                                                                                                                          |       120 |       8.5 |
| Uni3DL: Unified 3D Vision-Language Pretraining                                                             | Chen        | 2024-02                  | `https://arxiv.org/pdf/2402.07230.pdf`                                                                                                                          |       140 |       8.6 |
| Idefics2: What Matters When Building Vision-Language Models                                                | Laurençon   | 2024-01                  | `https://arxiv.org/pdf/2401.02005.pdf`                                                                                                                          |       300 |       9.2 |
| Length Extrapolation of Causal Transformers without Position Encoding (NoPE)                               | Wang        | 2024-08 (Findings ACL)   | `https://aclanthology.org/2024.findings-acl.834.pdf`                                                                                                            |        39 |       9.0 |
| DAPE: Data-Adaptive Positional Encoding for Length Extrapolation                                           | Zheng       | 2024-12 (NeurIPS)        | `https://proceedings.neurips.cc/paper_files/paper/2024/file/2f050fa9f0d898e3f265d515f50ae8f9-Paper-Conference.pdf`                                              |         0 |       8.8 |
| Mesa-Extrapolation: A Weave Position Encoding Method for Enhanced Extrapolation in LLMs                    | Ma          | 2024-12 (NeurIPS)        | `https://proceedings.neurips.cc/paper_files/paper/2024/file/9446c291a8744a125a0bda5b18f4d5a1-Paper-Conference.pdf`                                              |         3 |       8.4 |
| Rotary Position Embedding for Vision Transformer (RoPE-Mixed / 2D RoPE study)                              | Heo         | 2024-10 (ECCV)           | `https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/01584.pdf`                                                                                            |       149 |       8.7 |
| SmolVLM: Small and Efficient Vision-Language Models                                                        | HuggingFace | 2025-01                  | `https://arxiv.org/pdf/2501.XXXX.pdf`                                                                                                                           |        50 |       8.4 |
| DoPE: Denoising Rotary Position Embedding                                                                  | Xiong       | 2025-11                  | `https://arxiv.org/pdf/2511.09146.pdf`                                                                                                                          |         0 |       8.6 |
| Selective Rotary Position Embedding                                                                        | Movahedi    | 2025-11                  | `https://arxiv.org/pdf/2511.17388.pdf`                                                                                                                          |         0 |       8.4 |
| ComRoPE: Scalable and Robust Rotary Position Embedding Parameterized by Trainable Commuting Angle Matrices | Yu          | 2025-06 (CVPR)           | `https://openaccess.thecvf.com/content/CVPR2025/papers/Yu_ComRoPE_Scalable_and_Robust_Rotary_Position_Embedding_Parameterized_by_Trainable_CVPR_2025_paper.pdf` |         2 |       8.8 |
| VRoPE: Rotary Position Embedding for Video Large Language Models                                           | Liu         | 2025-02                  | `https://arxiv.org/pdf/2502.11664.pdf`                                                                                                                          |         8 |       8.6 |
| Circle-RoPE: Cone-like Decoupled Rotary Positional Embedding for Large Vision-Language Models              | Wang        | 2025-05                  | `https://arxiv.org/pdf/2505.16416.pdf`                                                                                                                          |         0 |       8.5 |
| Length Extrapolation of Transformers: A Survey from the Perspective of Positional Encoding                 | Zhao        | 2024-10 (Findings EMNLP) | `https://aclanthology.org/2024.findings-emnlp.582.pdf`                                                                                                          |        41 |       8.9 |
