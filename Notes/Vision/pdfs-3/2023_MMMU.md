## 1. Basic Metadata
Title: MMMU: A Massive Multi-discipline Multimodal Understanding and Reasoning Benchmark for Expert AGI.
Evidence (p.1):
"MMMU: A Massive Multi-discipline Multimodal
Understanding and Reasoning Benchmark for Expert AGI"

Authors: Xiang Yue; Yuansheng Ni; Kai Zhang; Tianyu Zheng; Ruoqi Liu; Ge Zhang; Samuel Stevens; Dongfu Jiang; Weiming Ren; Yuxuan Sun; Cong Wei; Botao Yu; Ruibin Yuan; Renliang Sun; Ming Yin; Boyuan Zheng; Zhenzhu Yang; Yibo Liu; Wenhao Huang; Huan Sun; Yu Su; Wenhu Chen.
Evidence (p.1):
"Xiang Yue*†
, 2
Yuansheng Ni*
, 3
Kai Zhang*
, 4
Tianyu Zheng*
,
3
Ruoqi Liu, 2
Ge Zhang, 3
Samuel Stevens, 2
Dongfu Jiang, 2
Weiming Ren, 4
Yuxuan Sun,
2
Cong Wei, 3
Botao Yu, 5
Ruibin Yuan, 2
Renliang Sun, 7
Ming Yin,
3
Boyuan Zheng, 4
Zhenzhu Yang, 6
Yibo Liu, 4
Wenhao Huang,
3
Huan Sun*, 3
Yu Su*†
, 2
Wenhu Chen*†"

Year: 2024.
Evidence (p.1):
"arXiv:2311.16502v4 [cs.CL] 13 Jun 2024"

Venue: arXiv (cs.CL).
Evidence (p.1):
"arXiv:2311.16502v4 [cs.CL] 13 Jun 2024"

## 2. One-Sentence Contribution Summary
MMMU introduces a benchmark to evaluate multimodal models on college-level, multi-discipline tasks that require expert-level perception and reasoning.
Evidence (Abstract, p.1):
"We introduce MMMU: a new benchmark designed to eval-
uate multimodal models on massive multi-discipline tasks
demanding college-level subject knowledge and deliberate
reasoning."

## 3. Tasks Evaluated
Task 1: Multimodal question answering (multiple-choice)
- Task type: Reasoning / relational; Other (multimodal question answering, multiple-choice).
- Dataset(s): MMMU.
- Domain: Multimodal questions across six disciplines with heterogeneous image types (charts, diagrams, maps, tables, music sheets, chemical structures).
- Evidence (Table 1, p.4):
  "Multiple-choice Questions              10861 (94.03%)"
- Evidence (Abstract, p.1):
  "MMMU includes 11.5K meticulously collected
  multimodal questions from college exams, quizzes, and text-
  books, covering six core disciplines: Art & Design, Busi-
  ness, Science, Health & Medicine, Humanities & Social
  Science, and Tech & Engineering."
- Evidence (Abstract, p.1):
  "These questions span
  30 subjects and 183 subfields, comprising 30 highly het-
  erogeneous image types, such as charts, diagrams, maps,
  tables, music sheets, and chemical structures."
- Evidence (Section 3, p.4):
  "MMMU is designed to mea-
  sure three essential skills in LMMs: perception, knowledge,
  and reasoning."

Task 2: Multimodal question answering (open-ended)
- Task type: Reasoning / relational; Other (multimodal question answering, open-ended).
- Dataset(s): MMMU.
- Domain: Multimodal questions across six disciplines with heterogeneous image types (charts, diagrams, maps, tables, music sheets, chemical structures).
- Evidence (Table 1, p.4):
  "Open Questions                          689 (5.97%)"
- Evidence (Abstract, p.1):
  "MMMU includes 11.5K meticulously collected
  multimodal questions from college exams, quizzes, and text-
  books, covering six core disciplines: Art & Design, Busi-
  ness, Science, Health & Medicine, Humanities & Social
  Science, and Tech & Engineering."
- Evidence (Abstract, p.1):
  "These questions span
  30 subjects and 183 subfields, comprising 30 highly het-
  erogeneous image types, such as charts, diagrams, maps,
  tables, music sheets, and chemical structures."
- Evidence (Section 3, p.4):
  "MMMU is designed to mea-
  sure three essential skills in LMMs: perception, knowledge,
  and reasoning."

## 4. Domain and Modality Scope
- Single domain? No. Multiple domains within the same modality (image+text) across six disciplines.
  Evidence (Abstract, p.1):
  "MMMU includes 11.5K meticulously collected
  multimodal questions from college exams, quizzes, and text-
  books, covering six core disciplines: Art & Design, Busi-
  ness, Science, Health & Medicine, Humanities & Social
  Science, and Tech & Engineering."
- Multiple modalities? Yes (interleaved text and images).
  Evidence (Figure 1 caption, p.1):
  "MMMU presents four challenges: 1) comprehensiveness: 11.5K college-level problems across six
  broad disciplines and 30 college subjects; 2) highly heterogeneous image types; 3) interleaved text and images; 4) expert-level perception
  and reasoning rooted in deep subject knowledge."
- Domain generalization or cross-domain transfer? Not claimed.

## 5. Model Sharing Across Tasks
| Task | Shared Weights? | Fine-Tuned? | Separate Head? | Evidence |
| --- | --- | --- | --- | --- |
| Multimodal QA (multiple-choice) | Yes (same checkpoint, zero-shot evaluation) | No | Not specified | Zero-shot evaluation without fine-tuning (Section 4, p.5) |
| Multimodal QA (open-ended) | Yes (same checkpoint, zero-shot evaluation) | No | Not specified | Zero-shot evaluation without fine-tuning (Section 4, p.5) |

Evidence (Section 4, p.5):
"Our evaluation is conducted under a zero-shot set-
ting to assess the capability of models to generate accurate
answers without fine-tuning or few-shot demonstrations on
our benchmark."

Prompting note:
"If models do not provide prompts for task types
in MMMU, we conduct prompt engineering on the validation
set and use the most effective prompt for the zero-shot setup
in the main experiments." (Section 4, p.5)

## 6. Input and Representation Constraints
- Fixed or variable input resolution? Not specified in the paper.
- Fixed patch size? Not specified in the paper.
- Fixed number of tokens? Not specified in the paper.
- Fixed dimensionality (e.g., strictly 2D)? Not specified in the paper.
- Any padding or resizing requirements? Not specified in the paper.

## 7. Context Window and Attention Structure
- Maximum sequence length: Not specified in the paper.
- Fixed or variable sequence length: Not specified in the paper.
- Attention type (global/windowed/hierarchical/sparse): Not specified in the paper.
- Mechanisms to manage computational cost (windowing/pooling/token pruning): Not specified in the paper.

## 8. Positional Encoding (Critical Section)
- Positional encoding mechanism used: Not specified in the paper.
- Where applied (input only / every layer / attention bias): Not specified in the paper.
- Fixed/modified/ablated across experiments: Not specified in the paper.

## 9. Positional Encoding as a Variable
- Core research variable vs fixed assumption: Not specified in the paper.
- Multiple positional encodings compared: Not specified in the paper.
- PE claimed “not critical” or secondary: Not specified in the paper.

## 10. Evidence of Constraint Masking (Scale vs Structure)
- Model size(s) reported:
  "With only 1.6B
  model size, Kosmos2 is able to achieve comparable or better
  performance with Flamingo-9B [2] on VQA and caption-
  ing tasks." (Section 4.1, p.5)
- Larger model results (not explicitly attributed to scaling):
  "For example, LLaVA-1.6-34B and InternVL-
  Chat-V1.2 achieve test accuracies of 44.7% and 46.2%, re-
  spectively, narrowing the gap with proprietary models." (Section 4.2, p.7)
- Dataset size(s):
  "MMMU includes 11.5K meticulously collected
  multimodal questions from college exams, quizzes, and text-
  books, covering six core disciplines: Art & Design, Busi-
  ness, Science, Health & Medicine, Humanities & Social
  Science, and Tech & Engineering." (Abstract, p.1)
  "Total Questions                            11550" (Table 1, p.4)
- Training tricks / tooling effects:
  "The application of OCR and captioning technologies does
  not yield a significant improvement in the performance of
  text-only LMMs." (Section 4.2, p.7)
- Attribution of gains to scaling model size, scaling data, architectural hierarchy, or training tricks: Not explicitly stated in the paper. The paper reports comparative results but does not claim that scaling (model/data) is the primary driver of performance gains.

## 11. Architectural Workarounds
Not specified in the paper. The paper introduces a benchmark and reports results; it does not propose new architectural mechanisms (e.g., windowed attention, hierarchical stages, token pooling).

## 12. Explicit Limitations and Non-Claims
- Benchmark not sufficient for Expert AGI:
  "MMMU is not a sufficient test for Expert AGI,
  as per the definition [57], because there lacks a direct map-
  ping between performance on MMMU and “90th percentile of
  skilled adults,” nor are college exams the only tasks an AGI
  shall tackle." (Section 2 / Related Work, p.3)
- Benchmark limitations and bias risk:
  "MMMU, like any benchmark, has limitations despite its
  comprehensive nature. The manual curation process may
  carry biases, and the focus on college-level subjects might
  not be sufficient for testing Expert AGI [57]." (Conclusion, p.8)
