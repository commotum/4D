# Survey Answers for 2024_NoPE

## 1. Basic Metadata
- Title: "Length Generalization of Causal Transformers without Position Encoding" (p. 1).
- Authors: "Jie Wang1 * , Tao Ji2 * , Yuanbin Wu1 , Hang Yan5 , Tao Gui3 , Qi Zhang2 , Xuanjing Huang2,4 , Xiaoling Wang1" (p. 1).
- Year: "August 11-16, 2024 ©2024 Association for Computational Linguistics" (p. 1).
- Venue: "Linguistics: ACL 2024, pages 14024–14040" and "Findings of the Association for Computational Linguistics" (p. 1).

## 2. One-Sentence Contribution Summary
- The paper studies length generalization in causal Transformers without explicit position encoding (NoPE) and proposes parameter-efficient attention temperature scaling (uniform and head-based) to extend context length, as stated: "In this paper, we study the length generalization property of NoPE" and "We propose a parameter-efficient tuning for searching attention heads’ best temperature hyper-parameters, which substantially expands NoPE’s context size" (p. 1).

## 3. Tasks Evaluated

### 3.1 Long sequence language modeling
- Task name: Long sequence language modeling.
- Task type: Generation (language modeling / perplexity).
- Dataset(s): PG19 and Proof-pile.
- Domain: Natural language text.
- Evidence: "To evaluate the long sequence language modeling performances, we test our NoPE-based methods and RoPE-based baselines on PG19 (Rae et al., 2020) and proof-pile (Azerbayev et al., 2022) datasets... We report the perplexity (PPL) of the models in Table 2." (p. 6) and "Experiments on long sequence language modeling" (p. 1).

### 3.2 Commonsense reasoning benchmarks
- Task name: Commonsense reasoning.
- Task type: Reasoning / relational (multiple-choice QA).
- Dataset(s): arc_challenge, arc_easy, boolq, hellaswag, openbookqa, piqa, winogrande (Table 1).
- Domain: Natural language text.
- Evidence: "Following TinyLlama, we evaluate the common-sense reasoning ability of the NoPE model and report acc_norm in Table 1." (p. 6) and the Table 1 header listing "arc_challenge", "arc_easy", "boolq", "hellaswag", "openbookqa", "piqa", "winogrande" (p. 5-6).

### 3.3 Synthetic passkey retrieval
- Task name: Passkey Retrieval (synthetic long-context task).
- Task type: Other (retrieval from long context).
- Dataset(s): Synthetic "Passkey Retrieval" task (Landmark Attention).
- Domain: Synthetic text tokens / long sequences.
- Evidence: "A synthetic task is constructed in Landmark Attention (Mohtashami and Jaggi, 2023b) called \"Passkey Retrieval\". It aims to test the effective context window size of the model. The task is to retrieve a randomly placed passkey from a long sequence of tokens, where the passkey is a randomly sampled number of 5 digits and the sequence is built by concatenating irrelevant sentences." (p. 7).

### 3.4 Real-world long-context understanding (LongBench)
- Task name: LongBench (multi-task benchmark).
- Task type: Other (long-context QA, summarization, few-shot learning, synthetic, code).
- Dataset(s): Listed in Table 3: "Singl-Doc QA" (NQA, Qsp, MulF), "Multi-Doc QA" (HpQA, 2WQA, Musq.), "Summarization" (GRpt, QSum, MulN), "Few-shot Learning" (TREC, TrQA, SSum), "Synthetic" (PsgC, PsgR), "Code" (Lcc, Re-P).
- Domain: Natural language text and code (as indicated by the "Code" category).
- Evidence: "LongBench (Bai et al., 2023) is a comprehensive assessment of the long context understanding capabilities of large language models." (p. 8) and Table 3 categories/datasets: "Singl-Doc QA", "Multi-Doc QA", "Summarization", "Few-shot Learning", "Synthetic", "Code" with dataset abbreviations "NQA", "Qsp", "MulF", "HpQA", "2WQA", "Musq.", "GRpt", "QSum", "MulN", "TREC", "TrQA", "SSum", "PsgC", "PsgR", "Lcc", "Re-P" (p. 7).

## 4. Domain and Modality Scope
- Modality/domain: Single modality (text) across multiple datasets. Evidence includes repeated references to language modeling and tokens: "long sequence language modeling" (p. 6) and "retrieve a randomly placed passkey from a long sequence of tokens" (p. 7) and "long context understanding capabilities of large language models" (p. 8).
- Single domain or multiple domains: Multiple datasets within the same modality (text). Evidence: evaluation on "PG19" and "proof-pile" (p. 6), plus LongBench (p. 8).
- Domain generalization or cross-domain transfer: Not claimed.

## 5. Model Sharing Across Tasks
| Task | Shared Weights? | Fine-Tuned? | Separate Head? | Evidence |
| --- | --- | --- | --- | --- |
| Long sequence language modeling (PG19, Proof-pile) | Yes (same NoPE base model / length-extension variants evaluated) | Length-extension fine-tuning only for head-based scale; no task-specific fine-tuning described | Not specified in the paper | "We train a NoPE base model from scratch and investigate its capability in length generalization. We conduct length generalization experiments on long sequence language modeling, synthetic tasks (passkey retrieval), and real-world long context tasks (LongBench)." (p. 5). Also, "We model the scales’ optimal search as a parameter-efficient fine-tuning task... optimize the language modeling loss function LLM on the pre-training dataset D" (p. 4). |
| Commonsense reasoning benchmarks | Yes (same base NoPE model evaluation) | Not specified in the paper | Not specified in the paper | "Following TinyLlama, we evaluate the common-sense reasoning ability of the NoPE model and report acc_norm in Table 1." (p. 6). |
| Passkey Retrieval | Yes (same NoPE base model / length-extension variants evaluated) | Length-extension fine-tuning only for head-based scale; no task-specific fine-tuning described | Not specified in the paper | Same evidence as above for shared model across tasks (p. 5), plus passkey evaluation settings (p. 7-8). |
| LongBench | Yes (same NoPE base model / length-extension variants evaluated) | Length-extension fine-tuning only for head-based scale; no task-specific fine-tuning described | Not specified in the paper | Same evidence as above for shared model across tasks (p. 5), plus LongBench evaluation context (p. 8). |

## 6. Input and Representation Constraints
- Fixed training sequence length: "training sequence length L = 2048" (p. 2).
- Variable evaluation length for generalization: "Given a language model (LM) with pre-trained maximal sequence length L, the goal of length generalization is to expand it to length L′ > L." (p. 2).
- Model dimensionality details: "The NoPE model has 22 layers of Transformer blocks, 32 attention heads per layer, 2048 embedding size." (p. 5).
- Sliding window evaluation setting: "evaluate on 2M tokens using sliding window evaluation (S = 256)" (p. 6).
- Fixed or variable input resolution: Not specified in the paper.
- Fixed patch size / fixed number of tokens (as an image or patch grid): Not specified in the paper.
- Padding or resizing requirements: Not specified in the paper.

## 7. Context Window and Attention Structure
- Maximum sequence length reported: "We simply expanded the length L′ to 18K and then solved it." (Appendix A, p. 11) and Table 2/3 show results at "18K" (p. 6-7).
- Fixed vs variable sequence length: Training length is fixed at L = 2048, while evaluation length varies (L′ > L) for length generalization (p. 2).
- Attention type: Standard causal multi-head self-attention. Evidence: "the casual Transformer block is only left with three core modules, the embedding layer, feed-forward layers, and self-attention layers" (p. 3) and "We write the general scaled dot-product attention as" (p. 3).
- Mechanisms to manage computational cost: No windowed/hierarchical/sparse attention is described in the model. The only efficiency mechanism described is parameter-efficient tuning: "The search process is highly efficient. (1) The number of tunable parameters is extremely small, only 704 delta parameters over 1B model parameters" (p. 5).

## 8. Positional Encoding (Critical Section)
- Positional encoding mechanism used: None for NoPE. Evidence: "Transformers without position encodings (NoPE)" (p. 1) and "casually masked Transformers are shown to be able to successfully model languages without any prior position encoding (NoPE)" (p. 1).
- Where it is applied: Not applied (NoPE). Evidence: same as above (p. 1).
- Fixed or modified per task / ablated: The NoPE setting is fixed, and RoPE is used for comparison: "We also include the original TinyLlama model which uses rotary position encoding (RoPE) for comparison" (p. 2) and "the rotary position embedding (RoPE) in TinyLlama is removed, making it a NoPE model" (p. 6).

## 9. Positional Encoding as a Variable
- Core research variable vs fixed assumption: Core research variable. Evidence: "In this paper, we study the length generalization property of NoPE" (p. 1).
- Multiple positional encodings compared: Yes, NoPE is compared against RoPE and RoPE extension methods. Evidence: "We also include the original TinyLlama model which uses rotary position encoding (RoPE) for comparison" (p. 2) and "successful RoPE extension algorithms (e.g., RoPE-NTK in Figure 1) can control the distraction of entropy by explicitly manipulate position encodings." (p. 3).
- Claim that PE choice is “not critical” or secondary: Not specified in the paper.

## 10. Evidence of Constraint Masking
- Model size(s): "Our default NoPE has 1.1B parameters." (p. 2) and "The NoPE model has 22 layers of Transformer blocks, 32 attention heads per layer, 2048 embedding size." (p. 5).
- Dataset size(s): "training sequence length L = 2048 and 50K steps (≈ 100B tokens)" (p. 2) and "The amount of training tokens for fine-tuning is extremely small too, only 0.03% of the pre-training data." (p. 5).
- Performance gains attributed to scaling/training tricks: The paper attributes gains to attention temperature scaling: "by scaling the attention score by a factor of 1.2, NoPE can immediately generalize to over 4K tokens" (p. 2) and "with all NoPE’s parameters frozen and only uniformly increasing the softmax’s temperature, NoPE can successfully generalize to unseen lengths" (p. 3).
- Gains attributed to scaling model size or scaling data: Not claimed.

## 11. Architectural Workarounds
- Uniform attention scale (temperature scaling): "We write the general scaled dot-product attention as... where the scaling factor λ is the temperature hyper-parameter of the SoftMax operator." (p. 3) and "we can try to gradually increase the scale factor λ to reconcentrate attention" (p. 3).
- Head-based attention scale: "We reformulate the uniform attention scale as head-base attention scales... where λ(h) is a unique attention scaling factor for each head, totaling 704." (p. 4).
- Focus constraint during tuning: "we apply a focus constraint during the optimization of Equation 5" (p. 5).
- Parameter-efficient tuning: "The search process is highly efficient. (1) The number of tunable parameters is extremely small, only 704 delta parameters over 1B model parameters" (p. 5).
- Windowed attention / hierarchical stages / token pooling / task-specific heads / fixed grid assumptions: Not specified in the paper.

## 12. Explicit Limitations and Non-Claims
- Limitations: "the NoPE model itself still underperforms with state-of-the-art RoPE models, which makes the results over long sequence language modeling tasks and LongBench tasks are less competitive." (p. 9). Also, "NoPE still faces the challenges of considerable memory usage and computational complexity due to the quadratic nature of attention computation when processing extremely long contexts." (p. 9).
- Future work / explicit limitations: "We plan to further improve the NoPE’s performances for a fairer comparison. This paper is also most an empirical one, which requires a deeper theoretical understanding of NoPE’s length generalization in the future." (p. 9).
- Explicit statements about what the model does not attempt to do (open-world learning, unrestrained multi-task learning, meta-learning, etc.): Not specified in the paper.
