                       Length Extrapolation of Transformers:
                 A Survey from the Perspective of Positional Encoding
             Liang Zhao1 , Xiachong Feng2 , Xiaocheng Feng1,3 * , Weihong Zhong1 ,
              Dongliang Xu4 , Qing Yang4 , Hongtao Liu4 , Bing Qin1,3 , Ting Liu1
                1
                  Harbin Institute of Technology 2 The University of Hong Kong
                   3
                     Peng Cheng Laboratory 4 Du Xiaoman Financial (Beijing)
          {lzhao, xcfeng, whzhong, qinb, tliu}@ir.hit.edu.cn fengxc@hku.hk
                 {xudongliang, yangqing, liuhongtao01}@duxiaoman.com

                        Abstract                              Transformer architecture (Vaswani et al., 2017),
                                                              though Transformer-based large language models
     Built upon the Transformer, large language
                                                              (LLMs) (Touvron et al., 2023a; OpenAI, 2023)
     models (LLMs) have captured worldwide at-
     tention due to their remarkable abilities. Never-        have drastically advanced the NLP field.
     theless, all Transformer-based models includ-               Transformer-based models are trained on se-
     ing LLMs suffer from a preset length limit and           quences with a maximum length (Raffel et al.,
     can hardly generalize from short training se-            2020; Zhang et al., 2020; Brown et al., 2020), as a
     quences to longer inference ones, namely, they           result of the quadratic memory and computational
     cannot perform length extrapolation to handle            complexity with regard to input length. To make
     long sequences, which severely hinders their             matters worse, some research reveals that Trans-
     application in scenarios demanding long input
                                                              formers might have gained their performance from
     sequences such as legal or scientific documents.
     Thus, numerous methods have emerged to en-               surface-level memorization instead of abstract, gen-
     hance the length extrapolation of Transformers.          eralizable skills (Razeghi et al., 2022; Wu et al.,
     Despite the great research efforts, a systematic         2024), which means they can hardly break through
     survey is still lacking. To fill this gap, we delve      the maximum training length and perform poorly
     into these advances in a unified notation from           on sequences with length beyond it (Dai et al.,
     the perspective of positional encoding (PE), as          2019; Neishi and Yoshinaga, 2019), i.e., they can-
     it has been considered the primary factor on
                                                              not perform length extrapolation (Mitchell et al.,
     length extrapolation. Specifically, we begin
     with extrapolatable PEs that have dominated              2018; Press et al., 2021). To offer a more compre-
     this research field. Then, we dive into extrap-          hensive understanding of the challenges in length
     olation methods based on them, covering po-              extrapolation, we present comparison results of
     sition interpolation and randomized position             three state-of-the-art models with different context
     methods. Finally, several challenges and future          sizes on several generation tasks in Appendix A.1.
     directions in this area are highlighted. Through            The length limit together with poor length ex-
     this survey, we aim to enable the reader to gain         trapolation prevents LLMs from handling long
     a deep understanding of existing methods and
                                                              sequences, such as DNA and protein sequences
     provide stimuli for future research.
                                                              (Abramson et al., 2024), high-resolution images
1    Introduction                                             (Liu et al., 2023a), and even videos (Lin et al.,
                                                              2023). Moreover, existing approaches for harness-
It has been suggested that with limited learning re-          ing the full potential of LLMs also demand a larger
sources, humans can potentially comprehend utter-             context window, to incorporate elaborate prompts
ances of infinite length by understanding their com-          (Liu et al., 2023c), sufficient in-context demonstra-
ponents and structures (Chomsky, 1957; MON-                   tions (Brown et al., 2020) and long-term mem-
TAGUE, 1970). In natural language processing                  ory of agents (Park et al., 2023). Hence, there is
(NLP), given the limited training data (Kazem-                a growing body of research trying to strengthen
nejad et al., 2023) and compute, models cannot                length extrapolation of LLMs (Press et al., 2021;
learn from large-scale long sequences and thus are            Ontanon et al., 2022; Anil et al., 2022; Chi et al.,
also expected to possess such generalization ability          2023b; Sun et al., 2023), mostly from the perspec-
to process long sequences (Shaham et al., 2023).              tive of positional encoding (PE).
However, it is a challenging task for the de facto               Despite the prosperity in this area, a systematic
    * Corresponding Author                                    survey is still lacking. We aim to fill this blank
                                                           9959
                 Findings of the Association for Computational Linguistics: EMNLP 2024, pages 9959‚Äì9977
                          November 12-16, 2024 ¬©2024 Association for Computational Linguistics
                                                                                             Integrating
                                                                                                                                    SHAPE (Kiyono et al., 2021); CAPE (Likhomanenko et al., 2021)
                                                                                       Shift Invariance ¬ß3.1.1
                                                 APEs (Vaswani et al., 2017) ¬ß3.1
                                                                                    Enhancing Smoothness ¬ß3.1.2                         Complex (Wang et al., 2019); FLOATER (Liu et al., 2020)
                         Extrapolatable PEs ¬ß3
 Length Extrapolation



                                                                                        RoPE Family ¬ß3.2.1                                    RoPE (Su et al., 2024); xPos (Sun et al., 2023)
                                                  RPEs (Shaw et al., 2018) ¬ß3.2
                                                                                                                                   T5-Bias (Raffel et al., 2020); TISA (Wennberg and Henter, 2021);
                                                                                        T5-bias Family ¬ß3.2.2                           ALiBi (Press et al., 2021); KERPLE (Chi et al., 2022);
                                                                                                                             Sandwich (Chi et al., 2023b); FIRE (Li et al., 2023b); CAPE (Zheng et al., 2024)

                                                                                                Linear Positional Interpolation (Chen et al., 2023b); NTK-Aware Interpolation (bloc97, 2023b);
                                                   Position Interpolation ¬ß4.1
                                                                                    Dynamic-NTK Interpolation (emozilla, 2023); NTK-by-parts Interpolation (bloc97, 2023a); Truncated Basis (Pal et al., 2023)
                         PE-based Methods ¬ß4
                                                      Randomized PE ¬ß4.2                                             Randomized PE (Ruoss et al., 2023); PoSE (Zhu et al., 2023)



                                                        Figure 1: Taxonomy for length extrapolation of Transformers.

                                                                                                                                       ‚àö
by investigating existing approaches that enable                                                                 a scaling factor1 1/ d (Equation 1). Then, the
and enhance length extrapolation of Transform-                                                                   row-wise softmax function converts compatibility
ers. Specifically, a brief formal introduction to                                                                scores into weights, and the weighted sum of the
Transformer is given in ¬ß2 as a solid foundation for                                                             values is the output of the attention layer (Equa-
further discussion. Then, we comprehensively sum-                                                                tion 2). The fully connected feed-forward net-
marize extrapolatable PEs proposed from the birth                                                                work consists of two linear transformations with a
of Transformer to the prevalence of LLMs in ¬ß3.                                                                  ReLU activation between (Equation 4), with param-
Note that we focus exclusively on PEs proposed                                                                   eters W (f1 ) ‚àà Rd√ódf , W (f2 ) ‚àà Rdf √ód , bf (1) ‚àà
for better extrapolation and omit others, since there                                                            Rdf , b(f2 ) ‚àà Rd , where df is the intermediate di-
is already an insightful survey on PEs of Trans-                                                                 mension. Besides, residual connection (He et al.,
former (Dufter et al., 2022). Based on these PEs,                                                                2016) and layer normalization (Ba et al., 2016) are
many novel methods emerge in the era of LLMs                                                                     leveraged (Equation 3 and 5) to enhance scalability.
to further enhance extrapolation, which we inten-                                                                   Note that in the above descriptions, we have not
tionally centralize in ¬ß4, covering popular position                                                             imposed any limit on input length n, which means
interpolation methods and randomized methods.                                                                    the Transformer is naturally equipped with a notion
These advancements demonstrate the vibrancy and                                                                  of length extrapolation. Theoretically, a fixed set-
vastness of this area, from which we distill future                                                              ting of Transformer weights defines a sequence-to-
directions and insights, represented in ¬ß5 and ¬ß6.                                                               sequence function on sequences of arbitrary length
                                                                                                                 (Yun et al., 2019). If the function applies the cor-
2                       Preliminary                                                                              rect transformation for inputs of any length, it is
                                                                                                                 expected to length extrapolate (Zhou et al., 2023).
In this section, we follow Dufter et al. (2022) to                                                                  However, we have to break this nature by in-
present a formal description of the encoder layer                                                                tegrating PE with Transformers to inject position
of the Transformer, as the decoder layer is almost                                                               information into them. Otherwise, they are per-
the same except for the cross-attention mechanism.                                                               mutation equivalent or order invariant2 . Thus,
Given an input matrix X ‚àà Rn√ód as a sequence of                                                                  PEs are central to length extrapolation and form
n embeddings with dimension d, an encoder layer                                                                  the core focus of this survey.
f : Rn√ód ‚Üí‚àí Rn√ód with f (X) = Z is defined by:
                                                                                                                 3      Extrapolatable Positional Encodings
      QK T
  C= ‚àö                                                                                             (1)           Sinusoidal position embeddings are proposed with
         d
                                                                                                                 Transformer as it may help extrapolate to longer
  A = Softmax(C)V                                                                                  (2)
                                                                                                                 sequences beyond training (Vaswani et al., 2017).
  O = LayerNorm1 (A + X)                                                                           (3)           The idea behind this claim, that length extrapola-
    F = ReLU(OW (f1 ) + b(f1 ) )W (f2 ) + b(f2 ) (4)                                                             tion can be enabled by simply changing PE, has
    Z = LayerNorm2 (O + F )                                                                        (5)           been widely supported and demonstrated (Neishi
                                                                                                                 and Yoshinaga, 2019; Press et al., 2021; Ruoss
                                                                                                                 et al., 2023). Hence, developing better PEs has
where Q = XW q , K = XW k , V = XW v are
queries, keys and values, with W q , W k , W v ‚àà                                                                     1
                                                                                                                       We will omit this scaling factor in the following for sim-
Rd√ód being the projection matrices.                                                                              plicity and clarity.
                                                                                                                     2
                                                                                                                       Note that some existing research suggests causal language
   Firstly, the compatibility scores C are computed                                                              models can learn position information without PE (Tsai et al.,
as the dot product between queries and keys with                                                                 2019; Haviv et al., 2022; Chi et al., 2023a).

                                                                                                        9960
      PE                                  Manifestation Learnable Integration
                                                                                Injection
                                                                                  Layer
                                                                                               is fed into Transformer, so the compatibility score
      Sinusoidal (Vaswani et al., 2017)    Embedding       ‚úï         Add         Initial       between query q i and key kj can be formalized as
      with Shift Invariance
      SHAPE (Kiyono et al., 2021)          Embedding       ‚úï         Add         Initial           q i kTj = ((xi + pi )W q )((xj + pj )W k )T . (7)
APE




      CAPE (Likhomanenko et al., 2021)     Embedding       ‚úï         Add         Initial
      with Smoothness                                                                          This equation is the basis of many other PEs.
      Complex (Wang et al., 2020)          Embedding       ‚úì       Multiply      Initial
      FLOATER (Liu et al., 2020)           Embedding       ‚úì        Add          Initial          However, researchers subsequently found that
      Shaw et al. (2018)                   Embedding       ‚úì        Add          Every
                                                                                               sinusoidal APE is hard to extrapolate (Dai et al.,
      T5 Family
      T5 Bias (Raffel et al., 2020)          Bias          ‚úì         Add         Every         2019; Neishi and Yoshinaga, 2019). Hence, a wide
      ALiBi (Press et al., 2021)
      KERPLE (Chi et al., 2022)
                                             Bias
                                             Bias
                                                           ‚úï
                                                           ‚úì
                                                                     Add
                                                                     Add
                                                                                 Every
                                                                                 Every
                                                                                               variety of APEs have been proposed to enhance
RPE




      SANDWICH (Chi et al., 2023b)         Embedding       ‚úï         Add         Every         sinusoidal APE and extrapolation of Transformers
      FIRE (Li et al., 2023b)                Bias          ‚úì         Add         Every
      CAPE (Zheng et al., 2024)              Bias          ‚úì         Add         Every         from different perspectives, either trying to inte-
      RoPE Family                                                                              grate shift invariance in sinusoidal APE (¬ß3.1.1) or
      RoPE (Su et al., 2024)               Embedding       ‚úï       Multiply      Every
      xPOS (Sun et al., 2023)              Embedding       ‚úï       Multiply      Every         aiming to generate position embeddings varying
                                                                                               smoothly with position indices (¬ß3.1.2).
Table 1: A list of extrapolatable PEs. Bolded methods
are proposed or widely adopted for LLMs. Manifesta-                                            3.1.1 Integrating Shift Invariance
tion shows how the position infomation is introduced.                                          Taking inspiration from the three properties of PEs
Learnable shows whether it can adjust based on the                                             proposed by Wang et al. (2020), Kiyono et al.
input. Integration shows how the position representa-
                                                                                               (2021) speculated superior extrapolation perfor-
tions are integrated with token representations. Injection
Layer shows the injecting position PE.                                                         mance comes from shift invariance, the property
                                                                                               of a function to not change its output even if its
                                                                                               input is shifted. Aiming to incorporate the benefit
been the predominant avenue to enhance length                                                  of shift invariance in sinusoidal APE, they simply
extrapolation of Transformers. Table 1 presents a                                              shift every position index of a sequence by a ran-
characterization of these extrapolatable PEs.                                                  dom offset k during training, which prevents the
   Basically, absolute positional encodings (APEs)                                             model from using absolute positions and instead
map each position to a unique representation and                                               encourages the use of relative positions.
integrate it with corresponding word embedding,                                                   Following a similar idea, Likhomanenko et al.
while relative positional encodings (RPEs) encode                                              (2021) took it a step further by leveraging continu-
the relative distance between tokens and directly                                              ous signals. In addition to shifting every position
inject it into the attention module. Besides, RPEs                                             index of APE by an identical random offset, which
usually keep modifications independent of value                                                they call global shift, they also introduced local
vectors and leaves them not entangled with posi-                                               shift, i.e., shifting each position index by a differ-
tion information. Hence, position information of                                               ent random shift, and global scaling, i.e., scaling
RPEs can be scalars and usually recurs at each                                                 every position index by an identical random scalar,
layer. Figure 2 illustrates these general differ-                                              to further prevent capturing spontaneous correla-
ences. We divide Table 1 and this section based on                                             tions and memorizing distances.
whether the PE is absolute or relative, as existing
                                                                                               3.1.2 Enhancing Smoothness
research suggests this distinction significantly im-
pacts length extrapolation (Neishi and Yoshinaga,                                              Apart from above relatively straightforward meth-
2019; Likhomanenko et al., 2021; Chi et al., 2022).                                            ods based on sinusoidal APE, there are several
                                                                                               APEs taking quite different theoretical avenues to
3.1 Absolute Positional Encodings                                                              enhance length extrapolation, aiming to improve
Specifically, for a token in position pos, the sinu-                                           the smoothness of the position representations.
soidal position embedding is defined as:                                                           Wang et al. (2019) proposed to extend each
                                                                                               word embedding as a continuous function over an
                           pos                pos                                              independent variable, i.e., position, so that word
  [. . . , sin(                    ), cos(           ), . . . ], (6)
                        10000 2i/d         100002i/d                                           representations vary smoothly with increasing po-
                                                                                               sitions. Through mathematically sound derivation,
where i ‚àà [0, d/2 ‚àí 1] is the dimension of the                                                 their general complex-valued embedding f (j, pos)
position embedding and d denotes model dimen-                                                  of a word wj in position pos is
sion. Then, each position embedding is added to
the corresponding token embedding and the sum                                                      [rj,1 ei(œâj,1 pos+Œ∏j,1 ) , ¬∑ ¬∑ ¬∑ , rj,d ei(œâj,d pos+Œ∏j,d ) ], (8)
                                                                                            9961
                                     ùë®                                  ùë®
                                                                                         3.2   Relative Positional Encodings
             Attention                             Modified
                                                   Attention                             Albeit for the efforts in extrapolatable APEs, it
                         ùë™                                     ùë™                         is believed that RPEs are theoretically capable of
                                                                                         running on unseen lengths and are more robust to
                   ùë∏             ùë≤        ùëΩ            ùë∏            ùë≤        ùëΩ           input length change (Neishi and Yoshinaga, 2019;
                       Input Projection                    Input Projection              Likhomanenko et al., 2021; Chi et al., 2022), as
  Position    ùíë!         ùíë"          ùíë#   ùíë$
                                                                                         RPEs only rely on relative position information,
Embeddings
              +          +           +    +
                                                     ùíô!        ùíô"       ùíô#       ùíô$
                                                                                         which means they encode the idea of shift invari-
              ùíô!         ùíô"          ùíô#   ùíô$
                                                                                         ance naturally and are not subject to a maximum
               Word Embedding Layer                  Word Embedding Layer
                                                                                         position value. Besides, there is a consensus that
  Words       ùë§!         ùë§"          ùë§#       ùë§$    ùë§!         ùë§"       ùë§#       ùë§$      in natural language, it is not absolute but relative
                                                                                         position that matters (Huang et al., 2020; Sinha
Figure 2: General differences between APE (left part)
and RPE (right part), where orange denotes elements
                                                                                         et al., 2022). Thus, RPEs become the dominant
holding position information.                                                            way to encode positions, which we detail in this
                                                                                         section. Before that, we reformulate Equation 7 as
                                                                                         follows to clarify the perspective of RPEs:
where amplitude r = [rj,1 , . . . , rj,d ], frequency                                        q i kTj = (xi W q )(xj W k )T ‚äï p(j ‚àí i),      (10)
œâ = [œâj,1 , . . . , œâj,d ] and initial phrase Œ∏ =
                                                                                         where p(j ‚àí i) encodes the relative position infor-
[Œ∏j,1 , . . . , Œ∏j,d ] are all trainable. In addition to rep-
                                                                                         mation, ‚äï denotes any approach of integrating the
resenting positions in complex plane for the first
                                                                                         position information into the compatibility score.
time, multiplying position embeddings with word
                                                                                           Among the first, Shaw et al. (2018) intro-
embeddings is another of their innovations.
                                                                                         duced the idea of RPE based on above formulation.
   An alternative approach is to directly capture the                                    Specifically, they concretized Equation 10 as
dynamics between position representations. Liu
et al. (2020) introduced a dynamical system to                                                    q i kTj = (xi W q )(xj W k + pr )T ,      (11)
model position representations {pi ‚àà Rd : i =                                            where pr ‚àà Rd is a trainable relative position em-
1, . . . , n}, which can be characterized as                                             bedding and r = clip(j ‚àí i, rmin , rmax ) denotes
                                                                                         the clipped relative position. By clipping the rela-
                          Z t                                                            tive positions to a determined range, the number of
 p(t) = p(s) +                   h(œÑ, p(œÑ ); Œ∏ h )dœÑ, 0 ‚â§ s ‚â§ t < ‚àû                      position embeddings to be learned is reduced and
                             s
                                                  (9)                                    length extrapolation is enhanced as unseen position
with an initial vector p(0), where p(t) : R+ 7‚Üí Rd                                       embeddings are avoided. This RPE can also be
is the continuous version of the discrete sequences                                      regarded as a derivation of sinusoidal APE. Fol-
{pi }. h(œÑ, p(œÑ ); Œ∏ h ), which is the "latent force"                                    lowing this line, more RPEs have been proposed
that drives the changes from pi to pi+1 , is actually                                    to better model position information, such as Dai
a neural network parameterized by Œ∏ h and takes in                                       et al. (2019), Huang et al. (2020) and TUPE (Ke
the previous state (œÑ, p(œÑ )).                                                           et al., 2020). We omit them here since they are not
                                                                                         proposed for stronger length extrapolation.
   Highlights: As the first PE for Transformer, si-
nusoidal APE has a significant impact on PEs there-                                      3.2.1 RoPE Family
after, despite its poor extrapolation. To improve                                        Also inspired by sinusoidal APE, Su et al. (2024)
this, researchers either leverage random shift to                                        proposed to multiply keys and queries by rotation
incorporate shift invariance in sinusoidal APE or                                        matrices, leaving compatibility scores as
generate position embeddings varying smoothly
with position. Among them, simple random shift-                                                q Ti kj = (RdŒò,i xi W q )T (RdŒò,j xj W k )
ing is like a small patch for sinusoidal APE and                                                      = W Tq xTi RdŒò,j‚àíi xj W k ,           (12)
has limited benefits for extrapolation, at the cost of
possible semantic confusion in position encoding,                                        where RdŒò,j‚àíi = (RdŒò,i )T RdŒò,j with RdŒò,i being a
while the latter can hopefully lead to better extrapo-                                   block-diagonal matrix with rotation matrices
                                                                                                                           
lation, coming with a much higher parameter- and                                                       cos iŒ∏m ‚àí sin iŒ∏m
computation-complexity.                                                                                                                (13)
                                                                                                        sin iŒ∏m cos iŒ∏m
                                                                                      9962
on its diagonal, given the parameters Œò =                  where scalar m is a head-specific slope fixed be-
(Œ∏m )m=1,2,...,d/2 where Œ∏m = 10000‚àí2(m‚àí1)/d .             fore training. It is worth noting that there is no
Here the base is 10000, and Œªm = 2œÄ/Œ∏m is wave-            additional learnable parameter, which leads to su-
length. This method is called Rotary Position Em-          perior efficiency and may also contribute to better
bedding (RoPE) as intuitively it rotates key/value         extrapolation of ALiBi. Empirical experiments on
embeddings according to their position index:              language modeling demonstrated its superiority.
                                                              From the perspective of kernel methods, Chi
          f{q,k} (xi , i) = RdŒò,i xi W {q,k} .     (14)    et al. (2022) considered ALiBi as a triangle ker-
It is noteworthy that despite the absolute nature of       nel and extended it to KERPLE, a framework that
this rotary process, the compatibility score and thus      generalizes relative position embeddings for ex-
attention depend only on relative distance. This           trapolation by kernelizing positional differences
property together with long-term decay for inter-          using conditionally positive definite kernels. In this
token product benefit length extrapolation.                framework, various RPEs can be derived from dif-
    As RoPE has been widely used in popular LLMs           ferent conditionally positive definite kernels in a
(Touvron et al., 2023a; Jiang et al., 2023; Anil et al.,   principled way, among which the logarithmic vari-
2023), there are some variants proposed to improve         ant achieves preferred extrapolation performance,
it. Sun et al. (2023) defined attention score expecta-     by calculating the compatibility score as follows:
tion between two tokens at a specific distance and
                                                           q Ti kj = (xi W q )T (xj W k )‚àír1 ¬∑log(1+r2 |i‚àíj|),
further attributed the poor extrapolation of RoPE
                                                                                                             (18)
to the dramatic oscillation of their attention expec-
                                                           where r1 , r2 are positive scalar parameters.
tations. They proposed to fix this issue by incorpo-
                                                               Aware of the overfitting issue of sinusoidal APE,
rating a balancing term to punish the oscillation of
                                                           Chi et al. (2023b) proposed to overcome it by sim-
unstable dimensions and keep the distribution of
                                                           plifying sinusoidal APE to a new RPE, Sandwich.
stable ones, which can be simplified to:
                                                           Specifically, they dropped the cross terms in Equa-
     q Ti kj = Œ≥ i‚àíj W Tq xTi RdŒò,j‚àíi xj W k ,     (15)    tion 7 and kept the inner product of two position
                                                           embeddings as position information:
where Œ≥ ‚àà (0, 1) is a scalar hyperparameter.
                                                                q Ti kj = (xi wq )T (xj W k ) + pTi pj .    (19)
3.2.2 T5-Bias Family
Different from complex embedding form, some re-            It is worth noting that in this formula, pTi pj be-
searchers reduce position information p(j ‚àí i) to a        comes a temporal bias term with the same decay-
simpler form. Raffel et al. (2020) utilized learnable      with-distance pattern as ALiBi, which is exactly
scalars to represent relative position information:        what the authors want to achieve as they suggested
      q i kTj = (xi W q )(xj W k )T + Œ≤i,j .       (16)    this pattern is likely to be the secret to success-
                                                           ful length extrapolation. Besides, since position
In addition, they extended the clipping mechanism          embeddings here only need to interact with them-
by a logarithmic bucket assignment to achieve pre-         selves, the authors make the dimension of them a
cise discrimination of nearby positions and less           hyperparameter to further improve performance.
precise discrimination of further positions (e.g.,            FIRE (Li et al., 2023b) integrates positional
mapping the position indices 1-4 to themselves, 5-         information into Transformers following T5 bias:
6 to 5, 7-8 to 6, 9-12 to 7, and so forth.), which
further reduces the parameters to be learned and               q i kTj = (xi W q )(xj W k )T + b(i, j),     (20)
is beneficial for extrapolation (Chi et al., 2022).
Moreover, Wennberg and Henter (2021) introduced            where the bias b(i, j) is mapped from positions us-
TISE, which leverages a radial-basis function of           ing a learnable continuous function fŒ∏ : R ‚Üí  ‚àí R,
relative distance with multiple trainable parameters       e.g., MLP. To avoid the generalization issue when
to add a bias to attention scores.                         the inputs are outside the training domain of the
   As the first PE aiming mainly for length extrap-        function, they proposed progressive interpolation
olation, ALiBi (Press et al., 2021) takes an even          by normalizing the distance by query position in-
simpler way to represent relative position:                dex, namely b(i, j) = fŒ∏ ( i‚àíj i ). Note that in
                                                           causal attention, the normalized distance is always
   q i kTj = (xi W q )(xj W k )T + m(j ‚àí i),       (17)    bounded between [0, 1], which aligns the inference
                                                       9963
domain with the training domain for any sequence        distance-attention functions based on Fourier basis
lengths, leading to better length extrapolation.        like RoPE. Therefore, RoPE become the de facto
   However, the above methods separate positional       PE of recent LLMs due to its advanced general
bias from semantics completely, which may cause         performance, in spite of its poor extrapolation.
semantic similarity to be overshadowed by position
information. Hence, Zheng et al. (2024) proposed        4       Extrapolation Methods in LLMs Era
Context-Adaptive Positional Encoding (CAPE) to          Based on PEs in ¬ß3, various methods have been
integrate both semantic and positional information:     developed to further enhance length extrapolation
                                                        of LLMs. This section is separated in response to
   q i kj T = (xi W q )(xj W k )T
                                                        this wave, focusing on interpolation methods and
          + f ((xi W q )(xj W k )T , b(i, j)).   (21)   randomized PEs, as illustrated in Figure 3.

Here f : R √ó R ‚Üí R is parameterized by a two-           4.1      Position Interpolation
layer LeakyReLU neural network and b(i, j) come         Despite the large quantity of PEs with better ex-
from other RPEs(e.g., ALiBi and FIRE).                  trapolation, RoPE has been most widely adopted
   In addition to RPEs introduced previously, there     in recent LLMs due to its superior in-distribution
are some methods cannot be categorized into RoPE        performance. Hence, loads of methods have been
or T5-bias family. He et al. (2024) introduce           proposed to enhance the extrapolation of RoPE, the
bilevel PE that employs two distinct PE for each        most prevalent of which is position interpolation.
position: an APE for intra-segment position to               Chen et al. (2023b) firstly 3 introduced posi-
help model capture the semantics contained therein,     tion interpolation for RoPE to extrapolate LLMs
while an RPE for inter-segment position to capture      to longer sequences by applying linear scaling
relationships between segments and exhibits extrap-     to down-scale position indices so that the maxi-
olation. This decoupling offers greater flexibility     mum position index matches the previous length
in addressing the length extrapolation problem.         limit during pre-training. Formally, this method
   Based on the observation that existing PEs use       replaces RoPE f (Equation 14) by f ‚Ä≤ defined as
token as the unit of measurement, Golovneva             f ‚Ä≤ (x, i) = f (x, iL
                                                                           L‚Ä≤ ), where L is the length limit
et al. (2024) claimed that this feature prevents PEs    during pre-training and L‚Ä≤ is the longer sequence
from generalizing to higher levels of abstraction       length at inference. The scale ratio Œ∫ = L‚Ä≤ /L
such as sentences and paragraphs. Therefore, they       transforms position n to n/Œ∫. This method reduces
proposed Contextual Positional Encoding (CoPE),         absolute position indices from [0, L‚Ä≤ ) to [0, L) and
which allows the model to determine semantic unit       maximum relative distance from L‚Ä≤ to L, aligning
(e.g., word and sentence) and assign tokens therein     the ranges of position indices and relative distances
a same position index. Since CoPE can distribute        to mitigate effects on attention score computation.
positions to a much larger number of tokens and             However, from the perspective of Neural Tan-
focus attention on semantic units at a higher level     gent Kernel (NTK) theory (Jacot et al., 2018),
of abstraction, it exhibits stronger extrapolation.     simply interpolating RoPE‚Äôs Fourier space linearly
   Highlights: Earlier RPEs had been greatly in-        will cause the loss of high-frequency information
fluenced by sinusoidal APEs by modifying terms          and prevent models from distinguishing nearby po-
in Equation 7 and replacing absolute embeddings         sitions.Hence, NTK-Aware Scaled RoPE (NTK-
with relative embeddings. These methods usually         aware interpolation) (bloc97, 2023b) has been pro-
leverage clipping or binning strategy to avoid out-     posed by modifying the base of RoPE:
of-distribution position embeddings and enhance
                                                                                   d
extrapolation. Since RPEs decouple the one-to-one                      ‚àó
                                                                      Œ∏m = (b ¬∑ Œ∫ d‚àí2 )‚àí2(m‚àí1)/d ,      (22)
correspondence between position and position rep-
resentation, incorporating bias term directly into      where b is the original base and Œ∫ is still the scale
compatibility score (Equation 10) becomes a fea-        ratio. The core idea here is to scale high frequen-
sible and even better way to encode positional in-      cies less and low frequencies more to reduce infor-
formation, which is much simpler and naturally          mation loss of high frequencies. As NTK-aware
disentangles value vectors and position informa-        interpolation does not scale the Fourier features
tion. However, despite the strong extrapolation of          3
                                                            There is a concurrent work: https://kaiokendev.
these bias methods, they cannot represent complex       github.io/til#extending-context-to-8k

                                                    9964
 Randomized Positional Encoding
                                                                                                late dimensions of small wavelengths at all while
                  0        1                          ...                                M      always interpolating those of big ones.
                                         Ordered Subsampling
  Training
                                                                                                   Similar observations with NTK-by-parts have
              0   0               ...                 L
               00 03                ......             LL+2
                                                                                                been made by Pal et al. (2023), based on which
                               L tokens                                                         they proposed to use the truncated basis:
 Evaluation       0        1       ...                L     L+1         ...         E
                                                                                                               Ô£±
                                                                  Beyond L but                                 Ô£¥
                                                                                                               Ô£≤Œ∏i       for Œ∏i ‚â• b,
                                                                  in-distribution
                                                                                                            ‚àó
                                                                                                           Œ∏i = œÅ        for a < Œ∏i < b,         (24)
 Position Interpolation                                                                                        Ô£¥
                                                                                                               Ô£≥
  Training    0   0
               00 01
                                  ...
                                    ......
                                                      L
                                                       LL
                                                                                                                0        for Œ∏i < a.

                  0        1                    ...                                 L‚Äô
                                                                                                where œÅ is a fixed value that is relatively small, and
 Evaluation                       Position Interpolation                                        a and b are chosen cutoff values. This way, mod-
              0
                      L
                      L‚Äô
                                 ...                   L                                        els will experience all values of the basis in the
                                                                                                context length used during fine-tuning by choos-
Figure 3: Essentials of position interpolation and ran-
                                                                                                ing appropriate cutoff values, and are supposed to
domized PE. Randomized PE aims to ensure that posi-
tions falling outside the context window at inference re-
                                                                                                extrapolate better during inference.
main in distribution through advanced exposure in train-                                           Additionally, Peng et al. (2023b) observed that
ing. Position interpolation, on the other hand, works                                           by introducing a temperature t into compatibility
during the inference stage by scaling a longer position                                         score before Softmax, perplexity decreases consis-
range into the original context window.                                                         tently. Combining this finding with NTK-by-parts
                                                                                                interpolation, they subsequently proposed YaRN
directly, all positions are distinguishable from each                                           that surpasses previous interpolation methods in
other. Moreover, this method does not require any                                               both fine-tuned and non-fine-tuned scenarios.
fine-tuning to extend the context window.                                                          The interpolation methods reflect the critical im-
   Further, Dynamic-NTK interpolation (emozilla,                                                pact of the rotary base of RoPE on length extrapo-
2023) combined NTK-aware interpolation with dy-                                                 lation, prompting efforts to enhance extrapolation
namic scaling, using exact positions for tokens                                                 of RoPE-based LLM by fine-tuning it with a scaled
within pre-trained context window to prevent per-                                               base (Xiong et al., 2023; Rozi√®re et al., 2023; Liu
formance degradation and dynamically increases                                                  et al., 2023d). However, fixed scaling factors over-
scale ratio Œ∫ as current sequence length increases                                              look the gradual length-extension process and im-
to adjust positions beyond the window:                                                          pair performance at shorter lengths, leading to the
                  (                                                                             proposal of dynamic scaling methods (Chen et al.,
                    L‚Ä≤ /L, if L‚Ä≤ /L > 1,                                                        2023a; Zhang et al., 2024b; Ding et al., 2024). Inno-
             Œ∫=                                  (23)                                           vatively, Wang et al. (2024) scale each dimension‚Äôs
                    1,       otherwise,
                                                                                                base by rounding its wavelength to the nearest inte-
where L‚Ä≤ is the sequence length of the current se-                                              ger, avoiding phase shifts after each full rotation.
quence, which will increase after each step.                                                       Highlights: Recently, position interpolation
   Either scaling position indices or modifying                                                 methods have raised widespread interest in the re-
bases, all position representations become closer to                                            search community, as a natural result of their supe-
each other, impairing LLM‚Äôs ability to distinguish                                              rior extrapolation performance and extremely low
the positional order of close-by tokens. Besides,                                               overhead. Current interpolation methods either in-
bloc97 (2023a) observed that some RoPE dimen-                                                   terpolate position indices or RoPE‚Äôs base, guided
sions have wavelengths longer than the pre-trained                                              by sound theoretical intuition. Besides, different
context window, where they presume absolute po-                                                 from other extrapolation methods, position interpo-
sitional information remains intact4 . Hence, they                                              lation methods have already seen their presence in
proposed NTK-by-parts, which does not interpo-                                                  the open-source models (Bai et al., 2023a; Touvron
    4
                                                                                                et al., 2023b; AI et al., 2024).
     From the perspective of frequency, the full range of high-
frequency components have been seen by the model during
training, while low-frequency components have not. Thus,                                        4.2   Randomized Positional Encoding
every position within the context window leads to a unique
value in these low-frequency components, based on which                                         For PEs without clipping mechanism, length ex-
models can determine the absolute position of each token.                                       trapolation means positions beyond those that have
                                                                                             9965
been observed during training, leading to out-of-        empirical results of trending PEs on language mod-
distribution position representations and thus per-      eling in Appendix A.2. However, it has become
formance degradation. To address this, an intu-          clear that perplexity alone does not adequately re-
itive way is enabling models to observe all possible     flect downstream task performance and is insuffi-
position representations during training, which is       cient (Tay et al., 2021; Kazemnejad et al., 2023; Pal
exactly the core idea behind randomized PEs.             et al., 2023; Hu et al., 2024). Therefore, dedicated
   As a realization of this idea, Ruoss et al. (2023)    benchmarks and evaluation methods are needed to
proposed to simulate a much longer range of posi-        further advance the field of length extrapolation.
tions (M ) and randomly selects an ordered subset           To stimulate subsequent research, we present
to fit the training context window for each iteration.   several preliminary thoughts on the construction of
Thus, through adequate training, we can ensure that      a standardized benchmark in Appendix A.3.
the model encounters enough unique positions and            Explainability and Principle. Despite the re-
all M positions have been fully trained, leading to      markable progress, our understanding of length
consistent extrapolation performance.                    extrapolation remains limited, lacking a general
   Different from Ruoss et al. (2023), PoSE (Zhu         and solid theoretical foundation. The decaying-
et al., 2023) partitions a sequence into chunks          with-distance pattern was initially thought to be
and adjusts the position indices by adding dis-          crucial for extrapolatable PEs (Press et al., 2021;
tinct skipping bias terms between chunks. Hence,         Su et al., 2024), but it was later shown to merely ac-
PoSE keeps the positions continuous in each chunk,       commodate the recency bias of language modeling
which bears a close resemblance to pre-training,         (Chi et al., 2023c). Although Qin et al. (2024) fur-
while simultaneously help the model adapt to all         ther provided a theoretical analysis and elaborated
positions within a longer context window.                that exponential convergence is a sufficient condi-
   Highlights: Essentially, randomized PEs sim-          tion for RPEs to length extrapolate, their definition
ply decouple the trained context window with the         of length extrapolation is also based on language
longer inference one by introducing randomized           modeling and perplexity, which may limit the ap-
positions during training or fine-tuning, boosting       plicability of their theorem. Besides, extrapolation
exposure of all possible positions in advance. This      methods tend to avoid out-of-distribution positions
idea is quite different from that of position interpo-   via interpolation or advanced exposure. Thus, it
lation methods, where the latter tries to interpolate    remains unclear when or if Transformers length
positions during inference to make them fall into        extrapolate in real-world scenarios and whether or
the trained range. For the same reason, position         how existing methods help with it.
interpolation methods are mostly plug-and-play              Long Context Utilization. Existing length ex-
while randomized PEs usually need further fine-          trapolation methods mostly focus on expanding
tuning, which makes position interpolation much          context window of Transformers, while much less
more appealing due to its low overhead.                  attention has been paid to the investigation and
                                                         optimization of the utilization of long context. In
5   Future Directions                                    fact, as a result of recent advances, state-of-the-art
                                                         LLMs are claimed to be capable of processing se-
Evaluation and Benchmark. Initially, researchers         quences with up to 128k tokens (Abdin et al., 2024;
evaluated length extrapolation by training models        AI, 2024). Given such a long context, the extent to
on sequences with a length limit and testing them        which the models can effectively utilize it becomes
on slightly longer sequences (Liu et al., 2020;          a critical question. Previous study has revealed
Likhomanenko et al., 2021). During this phase,           that LLMs tend to "lost in the middle" (Liu et al.,
evaluation samples and metrics came from vari-           2023b), i.e., they cannot effectively leverage infor-
ous downstream tasks such as machine translation         mation in the middle of a long context. Despite
and question answering. Given the demonstrated           a few preliminary explorations trying to improve
versatility of pre-trained language models in vari-      long context utilization (Staniszewski et al., 2023;
ous downstream tasks (Raffel et al., 2020; Brown         Ravaut et al., 2024), recent long-context bench-
et al., 2020), language modeling and perplexity          marks (Li et al., 2023a; An et al., 2024; Bai et al.,
have emerged as the standard metrics for evaluat-        2024; Zhang et al., 2024a) suggest that trending
ing length extrapolation (Press et al., 2021; Haviv      long-context LLMs still struggle on long sequences,
et al., 2022). Thus, we statistically present some       and significant advancements are required.
                                                     9966
6   Discussions                                        6.2    Length Extrapolation and Generalization
6.1 Length-Extrapolated and Long-Context               In parallel to research efforts that deem length ex-
    Transformers                                       trapolation as a promising approach to extend con-
Throughout this survey, we position length extrapo-    text window of LLMs, another line of research
lation as a promising avenue towards long-context      treats it as a generalization problem and analyzes
transformers. However, as stated in ¬ß1, it‚Äôs the       the length generalization behavior of Transform-
length limit and poor length extrapolation together    ers within small context window on synthetic tasks
that prevents transformers from processing long        such as arithmetic and deductive reasoning in a con-
sequences, thus the more direct way to extend the      trolled setup (Lake and Baroni, 2018; Dubois et al.,
context window is to simply relax the length limit.    2020; Abbe et al., 2023), where some intriguing
   The most intuitive way to achieve large con-        observations and insights have been discovered.
text window is directly pre-training the model or         One common observation is that Transformers
fine-tuning (continual pre-training) a pre-trained     often struggle with length generalization, whether
model on long sequences. Xiong et al. (2023) em-       they are trained from scratch on synthetic tasks
pirically demonstrated that long context continual     (Lee et al., 2023; Kazemnejad et al., 2023), fine-
pre-training is more efficient and similarly effec-    tuned from pre-trained LLMs (Anil et al., 2022) or
tive compared to pre-training from scratch with        tested in in-context learning (Saparov et al., 2023).
long sequences. However, both pre-training and            As explanations, Dziri et al. (2023) hypothesize
fine-tuning (continual pre-training) are costly and    certain tasks may not possess the inherent com-
demand large-scale high-quality long data, which       positionality and allow for shortcut pattern match-
is scarce (Kazemnejad et al., 2023). To reduce         ing. On the other side, Transformers are proven
memory and computational overhead during train-        to length generalize on specific tasks (Zhou et al.,
ing, recurrent Transformer variances integrate re-     2023; Xiao and Liu, 2024) or with the right combi-
currence with attention (Dai et al., 2019; Bulatov     nation of data format and PE (Zhou et al., 2024).
et al., 2022) while efficient Transformer variants     Meanwhile, some studies show other factors in
(Tay et al., 2022; Fournier et al., 2023) mainly aim   length generalization. Anil et al. (2022) find that
at improving the quadratic complexity of attention     fine-tuning regime, scaling data, model sizes, and
mechanism, but both usually compromise some of         compute does not improve length generalization,
the modeling capability and still need large-scale     while scratchpad (Nye et al., 2022) or chain-of-
long sequence data. Flash Attention (Dao et al.,       thought (Wei et al., 2022) in the in-context learning
2022; Dao, 2023) greatly improves both training        regime do. In addition, Kazemnejad et al. (2023)
and inference efficiency of Transformers with little   show that explicit PE is not essential for decoder-
to no overhead, leading to models with much larger     only Transformer to length generalize on small-
context window (Jiang et al., 2023; Gunasekar          scale synthetic tasks. These studies have deep-
et al., 2023; Li et al., 2023a).                       ened our understanding of length extrapolation in a
   On the other side, there are more radical re-       mechanistic way and broadened our perspectives
search efforts that attempt to abandon attention and   to go beyond PE, demonstrating that the extrapola-
its quadratic complexity with regard to sequence       tion ability needs a systematic design where PE is
length completely, such as S4 (Gu et al., 2022),       crucial but by no means the sole component.
RWKV (Peng et al., 2023a), and Hyena (Poli
et al., 2023). Further, some recent studies have       7     Conclusion
attempted to scale these novel architectures to bil-
lions of parameters, leading to the emergence of       Through this survey, we systematically summa-
Mamba (Gu and Dao, 2023) and RWKV-5/6 (Peng            rized existing methods and recent advances in
et al., 2024). However, it has been demonstrated       length extrapolation from the perspective of PE.
that Transformer models perform dramatically bet-      Specifically, we meticulously categorize extrapo-
ter than state space models like S4 at copying and     latable PEs and further dive into methods based on
retrieving information from context (Jelassi et al.,   these PEs in LLMs era. In addition, we highlight
2024). Thus, whether these novel architectures are     existing challenges and identify new trends in this
better than Transformer and how they perform on        research field, hoping to facilitate researchers and
real-world scenarios remains to be evaluated.          provide stimuli for future research.
                                                   9967
Limitation                                                 Guanhua Wang, Philipp Witte, Michael Wyatt, Can
                                                           Xu, Jiahang Xu, Sonali Yadav, Fan Yang, Ziyi Yang,
This survey presented a systematic review of exist-        Donghan Yu, Chengruidong Zhang, Cyril Zhang,
ing methods and recent trends in length extrapola-         Jianwen Zhang, Li Lyna Zhang, Yi Zhang, Yunan
tion of Transformers. However, due to the lack of          Zhang, and Xiren Zhou. 2024. Phi-3 Technical Re-
                                                           port: A Highly Capable Language Model Locally on
standardized benchmark and evaluation methods,             Your Phone. Preprint, arXiv:2404.14219.
we primarily focus on high-level comparisons and
distinctions in principle of different approaches,      Josh Abramson, Jonas Adler, Jack Dunger, Richard
                                                          Evans, Tim Green, Alexander Pritzel, Olaf Ron-
rather than fine-grained empirical analysis. Further-     neberger, Lindsay Willmore, Andrew J. Ballard,
more, in this work, we focus on length extrapola-         Joshua Bambrick, Sebastian W. Bodenstein, David A.
tion studies aimed at extending the context window        Evans, Chia-Chun Hung, Michael O‚ÄôNeill, David
of LLMs in real-world scenarios. Although we              Reiman, Kathryn Tunyasuvunakool, Zachary Wu,
acknowledge the importance of studies analyzing           AkvileÃá ≈ΩemgulyteÃá, Eirini Arvaniti, Charles Beattie,
                                                          Ottavia Bertolli, Alex Bridgland, Alexey Cherepanov,
length generalization in synthetic tasks within a         Miles Congreve, Alexander I. Cowen-Rivers, An-
small context window as well, we provide only a           drew Cowie, Michael Figurnov, Fabian B. Fuchs,
brief discussion on them due to the page limitation.      Hannah Gladman, Rishub Jain, Yousuf A. Khan, Car-
                                                          oline M. R. Low, Kuba Perlin, Anna Potapenko, Pas-
Acknowledgements                                          cal Savy, Sukhdeep Singh, Adrian Stecula, Ashok
                                                          Thillaisundaram, Catherine Tong, Sergei Yakneen,
Xiaocheng Feng is the corresponding author of             Ellen D. Zhong, Michal Zielinski, Augustin ≈Ω√≠dek,
this work, We thank the anonymous review-                 Victor Bapst, Pushmeet Kohli, Max Jaderberg, Demis
                                                          Hassabis, and John M. Jumper. 2024. Accurate struc-
ers for their insightful comments. This work              ture prediction of biomolecular interactions with Al-
was supported by the National Natural Science             phaFold 3. Nature, pages 1‚Äì3. Publisher: Nature
Foundation of China (NSFC) (U22B2059, grant               Publishing Group.
62276078), the Key R&D Program of Heilongjiang
                                                        01 AI, Alex Young, Bei Chen, Chao Li, Chengen Huang,
via grant 2022ZX01A32, the International Cooper-           Ge Zhang, Guanwei Zhang, Heng Li, Jiangcheng
ation Project of PCL, PCL2022D01and the Funda-             Zhu, Jianqun Chen, Jing Chang, Kaidong Yu, Peng
mental Research Funds for the Central Universities         Liu, Qiang Liu, Shawn Yue, Senbin Yang, Shiming
(Grant No.HIT.OCEF.2023018).                              Yang, Tao Yu, Wen Xie, Wenhao Huang, Xiaohui
                                                           Hu, Xiaoyi Ren, Xinyao Niu, Pengcheng Nie, Yuchi
                                                          Xu, Yudong Liu, Yue Wang, Yuxuan Cai, Zhenyu
                                                           Gu, Zhiyuan Liu, and Zonghong Dai. 2024. Yi:
References                                                 Open Foundation Models by 01.AI. arXiv preprint.
Emmanuel Abbe, Samy Bengio, Aryo Lotfi, and Kevin         ArXiv:2403.04652 [cs].
  Rizk. 2023. Generalization on the Unseen, Logic       Mistral    AI.      2024.           Mistral    NeMo.
  Reasoning and Degree Curriculum. arXiv preprint.        https://mistral.ai/news/mistral-nemo/.
  ArXiv:2301.13105 [cs, stat].
                                                        Chenxin An, Shansan Gong, Ming Zhong, Xingjian
Marah Abdin, Sam Ade Jacobs, Ammar Ahmad Awan,            Zhao, Mukai Li, Jun Zhang, Lingpeng Kong, and
 Jyoti Aneja, Ahmed Awadallah, Hany Awadalla,             Xipeng Qiu. 2024. L-Eval: Instituting Standardized
 Nguyen Bach, Amit Bahree, Arash Bakhtiari,               Evaluation for Long Context Language Models. In
 Harkirat Behl, Alon Benhaim, Misha Bilenko, Jo-          Proceedings of the 62nd Annual Meeting of the As-
 han Bjorck, S√©bastien Bubeck, Martin Cai, Caio           sociation for Computational Linguistics (Volume 1:
 C√©sar Teodoro Mendes, Weizhu Chen, Vishrav               Long Papers), pages 14388‚Äì14411, Bangkok, Thai-
 Chaudhary, Parul Chopra, Allie Del Giorno, Gustavo       land. Association for Computational Linguistics.
 de Rosa, Matthew Dixon, Ronen Eldan, Dan Iter, Ab-
 hishek Goswami, Suriya Gunasekar, Emman Haider,        Cem Anil, Yuhuai Wu, Anders Andreassen, Aitor
 Junheng Hao, Russell J. Hewett, Jamie Huynh, Mo-         Lewkowycz, Vedant Misra, Vinay Ramasesh, Am-
 jan Javaheripi, Xin Jin, Piero Kauffmann, Nikos          brose Slone, Guy Gur-Ari, Ethan Dyer, and Behnam
 Karampatziakis, Dongwoo Kim, Mahoud Khademi,             Neyshabur. 2022. Exploring Length Generalization
 Lev Kurilenko, James R. Lee, Yin Tat Lee, Yuanzhi        in Large Language Models. Advances in Neural In-
 Li, Chen Liang, Weishung Liu, Eric Lin, Zeqi Lin,        formation Processing Systems, 35:38546‚Äì38556.
 Piyush Madan, Arindam Mitra, Hardik Modi, Anh
 Nguyen, Brandon Norick, Barun Patra, Daniel Perez-     Rohan Anil, Andrew M. Dai, Orhan Firat, Melvin John-
 Becker, Thomas Portet, Reid Pryzant, Heyang Qin,         son, Dmitry Lepikhin, Alexandre Passos, Siamak
 Marko Radmilac, Corby Rosset, Sambudha Roy, Olli         Shakeri, Emanuel Taropa, Paige Bailey, Zhifeng
 Saarikivi, Amin Saied, Adil Salim, Michael San-          Chen, Eric Chu, Jonathan H. Clark, Laurent El
 tacroce, Shital Shah, Ning Shang, Hiteshi Sharma,        Shafey, Yanping Huang, Kathy Meier-Hellstern, Gau-
 Xia Song, Olatunji Ruwase, Xin Wang, Rachel Ward,        rav Mishra, Erica Moreira, Mark Omernick, Kevin
                                                    9968
  Robinson, Sebastian Ruder, Yi Tay, Kefan Xiao,             and Juanzi Li. 2024. LongBench: A Bilingual, Mul-
  Yuanzhong Xu, Yujing Zhang, Gustavo Hernandez              titask Benchmark for Long Context Understanding.
  Abrego, Junwhan Ahn, Jacob Austin, Paul Barham,            In Proceedings of the 62nd Annual Meeting of the
  Jan Botha, James Bradbury, Siddhartha Brahma,              Association for Computational Linguistics (Volume 1:
  Kevin Brooks, Michele Catasta, Yong Cheng, Colin           Long Papers), pages 3119‚Äì3137, Bangkok, Thailand.
  Cherry, Christopher A. Choquette-Choo, Aakanksha           Association for Computational Linguistics.
  Chowdhery, Cl√©ment Crepy, Shachi Dave, Mostafa
  Dehghani, Sunipa Dev, Jacob Devlin, Mark D√≠az,          bloc97. 2023a. Add NTK-Aware interpolation "by
  Nan Du, Ethan Dyer, Vlad Feinberg, Fangxiaoyu              parts" correction.
  Feng, Vlad Fienber, Markus Freitag, Xavier Gar-
  cia, Sebastian Gehrmann, Lucas Gonzalez, Guy Gur-       bloc97. 2023b. NTK-Aware Scaled RoPE allows
  Ari, Steven Hand, Hadi Hashemi, Le Hou, Joshua             LLaMA models to have extended (8k+) context size
  Howland, Andrea Hu, Jeffrey Hui, Jeremy Hur-               without any fine-tuning and minimal perplexity degra-
  witz, Michael Isard, Abe Ittycheriah, Matthew Jagiel-      dation.
  ski, Wenhao Jia, Kathleen Kenealy, Maxim Krikun,        Tom Brown, Benjamin Mann, Nick Ryder, Melanie
  Sneha Kudugunta, Chang Lan, Katherine Lee, Ben-           Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind
  jamin Lee, Eric Li, Music Li, Wei Li, YaGuang Li,         Neelakantan, Pranav Shyam, Girish Sastry, Amanda
  Jian Li, Hyeontaek Lim, Hanzhao Lin, Zhongtao Liu,        Askell, Sandhini Agarwal, Ariel Herbert-Voss,
  Frederick Liu, Marcello Maggioni, Aroma Mahendru,         Gretchen Krueger, Tom Henighan, Rewon Child,
  Joshua Maynez, Vedant Misra, Maysam Moussalem,            Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens
  Zachary Nado, John Nham, Eric Ni, Andrew Nys-             Winter, Chris Hesse, Mark Chen, Eric Sigler, Ma-
  trom, Alicia Parrish, Marie Pellat, Martin Polacek,       teusz Litwin, Scott Gray, Benjamin Chess, Jack
  Alex Polozov, Reiner Pope, Siyuan Qiao, Emily Reif,       Clark, Christopher Berner, Sam McCandlish, Alec
  Bryan Richter, Parker Riley, Alex Castro Ros, Au-         Radford, Ilya Sutskever, and Dario Amodei. 2020.
  rko Roy, Brennan Saeta, Rajkumar Samuel, Renee            Language Models are Few-Shot Learners. In Ad-
  Shelby, Ambrose Slone, Daniel Smilkov, David R.           vances in Neural Information Processing Systems,
  So, Daniel Sohn, Simon Tokumine, Dasha Valter,            volume 33, pages 1877‚Äì1901. Curran Associates,
  Vijay Vasudevan, Kiran Vodrahalli, Xuezhi Wang,           Inc.
  Pidong Wang, Zirui Wang, Tao Wang, John Wiet-
  ing, Yuhuai Wu, Kelvin Xu, Yunhan Xu, Linting           Aydar Bulatov, Yury Kuratov, and Mikhail Burtsev.
  Xue, Pengcheng Yin, Jiahui Yu, Qiao Zhang, Steven         2022. Recurrent Memory Transformer. Advances in
  Zheng, Ce Zheng, Weikang Zhou, Denny Zhou, Slav           Neural Information Processing Systems, 35:11079‚Äì
  Petrov, and Yonghui Wu. 2023. PaLM 2 Technical            11091.
  Report. arXiv preprint. ArXiv:2305.10403 [cs].
                                                          Guanzheng Chen, Xin Li, Zaiqiao Meng, Shangsong
Lei Jimmy Ba, Jamie Ryan Kiros, and Geoffrey E.             Liang, and Lidong Bing. 2023a. CLEX: Continuous
  Hinton. 2016.    Layer Normalization.  CoRR,              Length Extrapolation for Large Language Models.
  abs/1607.06450. ArXiv: 1607.06450.                        arXiv preprint. ArXiv:2310.16450 [cs].

Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang,     Shouyuan Chen, Sherman Wong, Liangjian Chen, and
   Xiaodong Deng, Yang Fan, Wenbin Ge, Yu Han, Fei          Yuandong Tian. 2023b. Extending Context Window
   Huang, Binyuan Hui, Luo Ji, Mei Li, Junyang Lin,         of Large Language Models via Positional Interpola-
   Runji Lin, Dayiheng Liu, Gao Liu, Chengqiang Lu,         tion. arXiv preprint. ArXiv:2306.15595 [cs].
   Keming Lu, Jianxin Ma, Rui Men, Xingzhang Ren,
   Xuancheng Ren, Chuanqi Tan, Sinan Tan, Jianhong        Ta-Chung Chi. 2024. Toward Length-Extrapolatable
   Tu, Peng Wang, Shijie Wang, Wei Wang, Sheng-             Transformers. Thesis, Carnegie Mellon University.
   guang Wu, Benfeng Xu, Jin Xu, An Yang, Hao Yang,       Ta-Chung Chi, Ting-Han Fan, Li-Wei Chen, Alexander
   Jian Yang, Shusheng Yang, Yang Yao, Bowen Yu,            Rudnicky, and Peter Ramadge. 2023a. Latent Posi-
   Hongyi Yuan, Zheng Yuan, Jianwei Zhang, Xingx-           tional Information is in the Self-Attention Variance
   uan Zhang, Yichang Zhang, Zhenru Zhang, Chang            of Transformer Language Models Without Positional
   Zhou, Jingren Zhou, Xiaohuan Zhou, and Tianhang          Embeddings. In Proceedings of the 61st Annual
   Zhu. 2023a. Qwen Technical Report. arXiv preprint.       Meeting of the Association for Computational Lin-
   ArXiv:2309.16609 [cs].                                   guistics (Volume 2: Short Papers), pages 1183‚Äì1193,
                                                            Toronto, Canada. Association for Computational Lin-
Yushi Bai, Xin Lv, Jiajie Zhang, Hongchang Lyu,             guistics.
  Jiankai Tang, Zhidian Huang, Zhengxiao Du, Xiao
  Liu, Aohan Zeng, Lei Hou, Yuxiao Dong, Jie Tang,        Ta-Chung Chi, Ting-Han Fan, Peter J. Ramadge, and
  and Juanzi Li. 2023b. LongBench: A Bilingual, Mul-        Alexander Rudnicky. 2022. KERPLE: Kernelized
  titask Benchmark for Long Context Understanding.          Relative Positional Embedding for Length Extrapo-
  arXiv preprint. ArXiv:2308.14508 [cs].                    lation. Advances in Neural Information Processing
                                                            Systems, 35:8386‚Äì8399.
Yushi Bai, Xin Lv, Jiajie Zhang, Hongchang Lyu,
  Jiankai Tang, Zhidian Huang, Zhengxiao Du, Xiao         Ta-Chung Chi, Ting-Han Fan, Alexander Rudnicky,
  Liu, Aohan Zeng, Lei Hou, Yuxiao Dong, Jie Tang,          and Peter Ramadge. 2023b. Dissecting Transformer
                                                      9969
  Length Extrapolation via the Lens of Receptive Field     gkamradt.      2024.                           Gkam-
  Analysis. In Proceedings of the 61st Annual Meeting        radt/LLMTest_NeedleInAHaystack.
  of the Association for Computational Linguistics (Vol-
  ume 1: Long Papers), pages 13522‚Äì13537, Toronto,         Team GLM, Aohan Zeng, Bin Xu, Bowen Wang, Chen-
  Canada. Association for Computational Linguistics.         hui Zhang, Da Yin, Dan Zhang, Diego Rojas, Guanyu
                                                             Feng, Hanlin Zhao, Hanyu Lai, Hao Yu, Hongning
Ta-Chung Chi, Ting-Han Fan, and Alexander I. Rud-            Wang, Jiadai Sun, Jiajie Zhang, Jiale Cheng, Jiayi
  nicky. 2023c. Attention Alignment and Flexible Po-         Gui, Jie Tang, Jing Zhang, Jingyu Sun, Juanzi Li,
  sitional Embeddings Improve Transformer Length             Lei Zhao, Lindong Wu, Lucen Zhong, Mingdao Liu,
  Extrapolation. arXiv preprint. ArXiv:2311.00684            Minlie Huang, Peng Zhang, Qinkai Zheng, Rui Lu,
  [cs].                                                      Shuaiqi Duan, Shudan Zhang, Shulin Cao, Shuxun
                                                             Yang, Weng Lam Tam, Wenyi Zhao, Xiao Liu, Xiao
Noam Chomsky. 1957. Syntactic structures.                    Xia, Xiaohan Zhang, Xiaotao Gu, Xin Lv, Xinghan
                                                             Liu, Xinyi Liu, Xinyue Yang, Xixuan Song, Xunkai
Zihang Dai, Zhilin Yang, Yiming Yang, Jaime Car-
                                                             Zhang, Yifan An, Yifan Xu, Yilin Niu, Yuantao
  bonell, Quoc Le, and Ruslan Salakhutdinov. 2019.
                                                             Yang, Yueyan Li, Yushi Bai, Yuxiao Dong, Zehan Qi,
  Transformer-XL: Attentive Language Models beyond
                                                             Zhaoyu Wang, Zhen Yang, Zhengxiao Du, Zhenyu
  a Fixed-Length Context. In Proceedings of the 57th
                                                             Hou, and Zihan Wang. 2024. ChatGLM: A Family of
  Annual Meeting of the Association for Computational
                                                             Large Language Models from GLM-130B to GLM-4
  Linguistics, pages 2978‚Äì2988, Florence, Italy. Asso-
                                                             All Tools. Preprint, arXiv:2406.12793.
  ciation for Computational Linguistics.
                                                           Olga Golovneva, Tianlu Wang, Jason Weston, and Sain-
Tri Dao. 2023. FlashAttention-2: Faster Attention            bayar Sukhbaatar. 2024. Contextual Position Encod-
  with Better Parallelism and Work Partitioning. arXiv       ing: Learning to Count What‚Äôs Important. arXiv
   preprint. ArXiv:2307.08691 [cs].                          preprint. ArXiv:2405.18719 [cs].
Tri Dao, Dan Fu, Stefano Ermon, Atri Rudra, and            Albert Gu and Tri Dao. 2023. Mamba: Linear-Time Se-
   Christopher R√©. 2022.      FlashAttention: Fast           quence Modeling with Selective State Spaces. arXiv
   and Memory-Efficient Exact Attention with IO-             preprint. ArXiv:2312.00752 [cs].
  Awareness. Advances in Neural Information Pro-
   cessing Systems, 35:16344‚Äì16359.                        Albert Gu, Karan Goel, and Christopher R√©. 2022. Ef-
                                                             ficiently Modeling Long Sequences with Structured
Yiran Ding, Li Lyna Zhang, Chengruidong Zhang,               State Spaces. arXiv preprint. ArXiv:2111.00396
  Yuanyuan Xu, Ning Shang, Jiahang Xu, Fan Yang,             [cs].
  and Mao Yang. 2024. LongRoPE: Extending LLM
  Context Window Beyond 2 Million Tokens. arXiv            Suriya Gunasekar, Yi Zhang, Jyoti Aneja, Caio
  preprint. ArXiv:2402.13753 [cs].                           C√©sar Teodoro Mendes, Allie Del Giorno, Sivakanth
                                                             Gopi, Mojan Javaheripi, Piero Kauffmann, Gustavo
Yann Dubois, Gautier Dagan, Dieuwke Hupkes, and              de Rosa, Olli Saarikivi, Adil Salim, Shital Shah,
  Elia Bruni. 2020. Location Attention for Extrapola-        Harkirat Singh Behl, Xin Wang, S√©bastien Bubeck,
  tion to Longer Sequences. In Proceedings of the 58th       Ronen Eldan, Adam Tauman Kalai, Yin Tat Lee, and
  Annual Meeting of the Association for Computational        Yuanzhi Li. 2023. Textbooks Are All You Need.
  Linguistics, pages 403‚Äì413, Online. Association for        Preprint, arXiv:2306.11644.
  Computational Linguistics.
                                                           Adi Haviv, Ori Ram, Ofir Press, Peter Izsak, and Omer
Philipp Dufter, Martin Schmitt, and Hinrich Sch√ºtze.         Levy. 2022. Transformer Language Models without
  2022. Position Information in Transformers: An             Positional Encodings Still Learn Positional Informa-
  Overview. Computational Linguistics, 48(3):733‚Äì            tion. In Findings of the Association for Computa-
  763. Place: Cambridge, MA Publisher: MIT Press.            tional Linguistics: EMNLP 2022, pages 1382‚Äì1390,
                                                             Abu Dhabi, United Arab Emirates. Association for
Nouha Dziri, Ximing Lu, Melanie Sclar, Xiang (Lor-           Computational Linguistics.
  raine) Li, Liwei Jiang, Bill Yuchen Lin, Sean Welleck,
  Peter West, Chandra Bhagavatula, Ronan Le Bras,          Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian
  Jena Hwang, Soumya Sanyal, Xiang Ren, Allyson Et-          Sun. 2016. Deep Residual Learning for Image Recog-
  tinger, Zaid Harchaoui, and Yejin Choi. 2023. Faith        nition. pages 770‚Äì778.
  and Fate: Limits of Transformers on Composition-
  ality. Advances in Neural Information Processing         Zhenyu He, Guhao Feng, Shengjie Luo, Kai Yang,
  Systems, 36:70293‚Äì70332.                                   Di He, Jingjing Xu, Zhi Zhang, Hongxia Yang, and
                                                             Liwei Wang. 2024. Two Stones Hit One Bird: Bilevel
emozilla. 2023. Dynamically Scaled RoPE further in-          Positional Encoding for Better Length Extrapolation.
  creases performance of long context LLaMA with             arXiv preprint. ArXiv:2401.16421 [cs, stat].
  zero fine-tuning.
                                                           Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou,
Quentin Fournier, Ga√©tan Marceau Caron, and Daniel           Mantas Mazeika, Dawn Song, and Jacob Steinhardt.
  Aloise. 2023. A Practical Survey on Faster and             2020. Measuring Massive Multitask Language Un-
  Lighter Transformers. ACM Computing Surveys,               derstanding. In International Conference on Learn-
  55(14s):304:1‚Äì304:40.                                      ing Representations.
                                                       9970
Yutong Hu, Quzhe Huang, Mingxu Tao, Chen Zhang,           Dacheng Li, Rulin Shao, Anze Xie, Ying Sheng, Lian-
  and Yansong Feng. 2024. Can Perplexity Reflect            min Zheng, Joseph Gonzalez, Ion Stoica, Xuezhe
  Large Language Model‚Äôs Ability in Long Text Un-           Ma, and Hao Zhang. 2023a. How Long Can Context
  derstanding? arXiv preprint. ArXiv:2405.06105             Length of Open-Source LLMs truly Promise? In
  [cs].                                                     NeurIPS 2023 Workshop on Instruction Tuning and
                                                            Instruction Following.
Zhiheng Huang, Davis Liang, Peng Xu, and Bing Xiang.
  2020. Improve Transformer Models with Better Rel-       Shanda Li, Chong You, Guru Guruganesh, Joshua
  ative Position Embeddings. In Findings of the Associ-     Ainslie, Santiago Ontanon, Manzil Zaheer, Sumit
  ation for Computational Linguistics: EMNLP 2020,          Sanghai, Yiming Yang, Sanjiv Kumar, and Srinadh
  pages 3327‚Äì3335, Online. Association for Computa-         Bhojanapalli. 2023b. Functional Interpolation for
  tional Linguistics.                                       Relative Positions Improves Long Context Trans-
                                                            formers. arXiv preprint. ArXiv:2310.04418 [cs].
Arthur Jacot, Franck Gabriel, and Clement Hongler.
  2018. Neural Tangent Kernel: Convergence and            Tatiana Likhomanenko, Qiantong Xu, Gabriel Syn-
  Generalization in Neural Networks. In Advances in         naeve, Ronan Collobert, and Alex Rogozhnikov.
  Neural Information Processing Systems, volume 31.         2021. CAPE: Encoding Relative Positions with Con-
  Curran Associates, Inc.                                   tinuous Augmented Positional Embeddings. In Ad-
Samy Jelassi, David Brandfonbrener, Sham M. Kakade,         vances in Neural Information Processing Systems,
  and Eran Malach. 2024. Repeat After Me: Trans-            volume 34, pages 16079‚Äì16092. Curran Associates,
  formers are Better than State Space Models at Copy-       Inc.
  ing. arXiv preprint. ArXiv:2402.01032 [cs].
                                                          Bin Lin, Yang Ye, Bin Zhu, Jiaxi Cui, Munan Ning,
Albert Q. Jiang, Alexandre Sablayrolles, Arthur Men-        Peng Jin, and Li Yuan. 2023. Video-LLaVA: Learn-
  sch, Chris Bamford, Devendra Singh Chaplot, Diego         ing United Visual Representation by Alignment Be-
  de las Casas, Florian Bressand, Gianna Lengyel, Guil-     fore Projection. arXiv preprint. ArXiv:2311.10122
  laume Lample, Lucile Saulnier, L√©lio Renard Lavaud,       [cs].
  Marie-Anne Lachaux, Pierre Stock, Teven Le Scao,
  Thibaut Lavril, Thomas Wang, Timoth√©e Lacroix,          Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae
  and William El Sayed. 2023. Mistral 7B. arXiv             Lee. 2023a. Visual instruction tuning. In NeurIPS.
  preprint. ArXiv:2310.06825 [cs].
                                                          Nelson F. Liu, Kevin Lin, John Hewitt, Ashwin Paran-
Amirhossein      Kazemnejad,      Inkit      Padhi,         jape, Michele Bevilacqua, Fabio Petroni, and Percy
 Karthikeyan Natesan Ramamurthy, Payel Das,                 Liang. 2023b. Lost in the Middle: How Lan-
 and Siva Reddy. 2023. The Impact of Positional             guage Models Use Long Contexts. arXiv preprint.
 Encoding on Length Generalization in Transformers.         ArXiv:2307.03172 [cs] rate: 0.
 arXiv preprint. ArXiv:2305.19466 [cs].
                                                          Pengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao Jiang,
Guolin Ke, Di He, and Tie-Yan Liu. 2020. Rethinking         Hiroaki Hayashi, and Graham Neubig. 2023c. Pre-
  Positional Encoding in Language Pre-training.             train, Prompt, and Predict: A Systematic Survey of
                                                            Prompting Methods in Natural Language Processing.
Shun Kiyono, Sosuke Kobayashi, Jun Suzuki, and Ken-         ACM Computing Surveys, 55(9):195:1‚Äì195:35.
  taro Inui. 2021. SHAPE: Shifted Absolute Position
  Embedding for Transformers. In Proceedings of the       Xiaoran Liu, Hang Yan, Shuo Zhang, Chenxin An,
  2021 Conference on Empirical Methods in Natural           Xipeng Qiu, and Dahua Lin. 2023d. Scaling
  Language Processing, pages 3309‚Äì3321, Online and          Laws of RoPE-based Extrapolation. arXiv preprint.
  Punta Cana, Dominican Republic. Association for           ArXiv:2310.05209 [cs].
  Computational Linguistics.
Brenden Lake and Marco Baroni. 2018. Generalization       Xuanqing Liu, Hsiang-Fu Yu, Inderjit Dhillon, and Cho-
  without Systematicity: On the Compositional Skills        Jui Hsieh. 2020. Learning to Encode Position for
  of Sequence-to-Sequence Recurrent Networks. In            Transformer with Continuous Dynamical Model. In
  Proceedings of the 35th International Conference on       Proceedings of the 37th International Conference on
  Machine Learning, pages 2873‚Äì2882. PMLR. ISSN:            Machine Learning, pages 6327‚Äì6335. PMLR. ISSN:
  2640-3498.                                                2640-3498.

Nayoung Lee, Kartik Sreenivasan, Jason D. Lee, Kang-      Jeff Mitchell, Pontus Stenetorp, Pasquale Minervini,
  wook Lee, and Dimitris Papailiopoulos. 2023. Teach-        and Sebastian Riedel. 2018. Extrapolation in NLP.
  ing Arithmetic to Small Transformers.                      In Proceedings of the Workshop on Generalization
                                                             in the Age of Deep Learning, pages 28‚Äì33, New
Chen Li, Weiqi Wang, Jingcheng Hu, Yixuan Wei,               Orleans, Louisiana. Association for Computational
  Nanning Zheng, Han Hu, Zheng Zhang, and                    Linguistics.
  Houwen Peng. 2024. Common 7B Language
  Models Already Possess Strong Math Capabilities.        RICHARD MONTAGUE. 1970. Universal grammar.
  https://arxiv.org/abs/2403.04706v1.                       Theoria, 36(3):373‚Äì398.
                                                      9971
Masato Neishi and Naoki Yoshinaga. 2019. On the Re-      Bowen Peng, Jeffrey Quesnelle, Honglu Fan, and Enrico
 lation between Position Information and Sentence          Shippole. 2023b. YaRN: Efficient Context Window
 Length in Neural Machine Translation. In Proceed-         Extension of Large Language Models. arXiv preprint.
 ings of the 23rd Conference on Computational Nat-         ArXiv:2309.00071 [cs].
 ural Language Learning (CoNLL), pages 328‚Äì338,
 Hong Kong, China. Association for Computational         Michael Poli, Stefano Massaroli, Eric Nguyen, Daniel Y.
 Linguistics.                                              Fu, Tri Dao, Stephen Baccus, Yoshua Bengio, Ste-
                                                           fano Ermon, and Christopher Re. 2023. Hyena Hi-
Maxwell Nye, Anders Johan Andreassen, Guy Gur-Ari,         erarchy: Towards Larger Convolutional Language
 Henryk Michalewski, Jacob Austin, David Bieber,           Models. In Proceedings of the 40th International
 David Dohan, Aitor Lewkowycz, Maarten Bosma,              Conference on Machine Learning, pages 28043‚Äì
 David Luan, Charles Sutton, and Augustus Odena.           28078. PMLR. ISSN: 2640-3498.
 2022. Show Your Work: Scratchpads for Intermedi-
 ate Computation with Language Models.                   Ofir Press, Noah Smith, and Mike Lewis. 2021. Train
                                                           Short, Test Long: Attention with Linear Biases En-
Santiago Ontanon, Joshua Ainslie, Zachary Fisher, and      ables Input Length Extrapolation.
  Vaclav Cvicek. 2022. Making Transformers Solve
  Compositional Tasks. In Proceedings of the 60th        Zhen Qin, Yiran Zhong, and Hui Deng. 2024. Exploring
  Annual Meeting of the Association for Computational      Transformer Extrapolation. Proceedings of the AAAI
  Linguistics (Volume 1: Long Papers), pages 3591‚Äì         Conference on Artificial Intelligence, 38(17):18897‚Äì
  3607, Dublin, Ireland. Association for Computational     18905.
  Linguistics.
                                                         Colin Raffel, Noam Shazeer, Adam Roberts, Katherine
OpenAI. 2023. GPT-4 Technical Report. arXiv preprint.      Lee, Sharan Narang, Michael Matena, Yanqi Zhou,
  ArXiv:2303.08774 [cs].                                   Wei Li, and Peter J. Liu. 2020. Exploring the limits
Arka Pal, Deep Karkhanis, Manley Roberts, Samuel           of transfer learning with a unified text-to-text trans-
  Dooley, Arvind Sundararajan, and Siddartha               former. The Journal of Machine Learning Research,
  Naidu. 2023. Giraffe: Adventures in Expand-              21(1):140:5485‚Äì140:5551.
  ing Context Lengths in LLMs. arXiv preprint.
  ArXiv:2308.10882 [cs].                                 Mathieu Ravaut, Aixin Sun, Nancy Chen, and Shafiq
                                                          Joty. 2024. On Context Utilization in Summariza-
Joon Sung Park, Joseph C. O‚ÄôBrien, Carrie J. Cai,         tion with Large Language Models. In Proceedings
  Meredith Ringel Morris, Percy Liang, and Michael S.     of the 62nd Annual Meeting of the Association for
  Bernstein. 2023. Generative Agents: Interactive         Computational Linguistics (Volume 1: Long Papers),
  Simulacra of Human Behavior. arXiv preprint.            pages 2764‚Äì2781, Bangkok, Thailand. Association
  ArXiv:2304.03442 [cs].                                  for Computational Linguistics.

Bo Peng, Eric Alcaide, Quentin Anthony, Alon Al-         Yasaman Razeghi, Robert L Logan IV, Matt Gardner,
  balak, Samuel Arcadinho, Stella Biderman, Huanqi         and Sameer Singh. 2022. Impact of Pretraining Term
  Cao, Xin Cheng, Michael Chung, Leon Derczynski,          Frequencies on Few-Shot Numerical Reasoning. In
  Xingjian Du, Matteo Grella, Kranthi Gv, Xuzheng          Findings of the Association for Computational Lin-
  He, Haowen Hou, Przemyslaw Kazienko, Jan Ko-             guistics: EMNLP 2022, pages 840‚Äì854, Abu Dhabi,
  con, Jiaming Kong, Bart≈Çomiej Koptyra, Hayden            United Arab Emirates. Association for Computa-
  Lau, Jiaju Lin, Krishna Sri Ipsit Mantri, Ferdinand      tional Linguistics.
  Mom, Atsushi Saito, Guangyu Song, Xiangru Tang,
  Johan Wind, Stanis≈Çaw WozÃÅniak, Zhenyuan Zhang,        Baptiste Rozi√®re, Jonas Gehring, Fabian Gloeckle,
  Qinghua Zhou, Jian Zhu, and Rui-Jie Zhu. 2023a.          Sten Sootla, Itai Gat, Xiaoqing Ellen Tan, Yossi
  RWKV: Reinventing RNNs for the Transformer Era.          Adi, Jingyu Liu, Tal Remez, J√©r√©my Rapin, Artyom
  In Findings of the Association for Computational         Kozhevnikov, Ivan Evtimov, Joanna Bitton, Manish
  Linguistics: EMNLP 2023, pages 14048‚Äì14077, Sin-         Bhatt, Cristian Canton Ferrer, Aaron Grattafiori, Wen-
  gapore. Association for Computational Linguistics.       han Xiong, Alexandre D√©fossez, Jade Copet, Faisal
                                                           Azhar, Hugo Touvron, Louis Martin, Nicolas Usunier,
Bo Peng, Daniel Goldstein, Quentin Anthony, Alon           Thomas Scialom, and Gabriel Synnaeve. 2023. Code
  Albalak, Eric Alcaide, Stella Biderman, Eugene           Llama: Open Foundation Models for Code. arXiv
  Cheah, Xingjian Du, Teddy Ferdinan, Haowen               preprint. ArXiv:2308.12950 [cs].
  Hou, Przemys≈Çaw Kazienko, Kranthi Kiran Gv, Jan
  KoconÃÅ, Bart≈Çomiej Koptyra, Satyapriya Krishna,        Anian Ruoss, Gr√©goire Del√©tang, Tim Genewein, Jordi
  Ronald McClelland Jr., Niklas Muennighoff, Fares         Grau-Moya, R√≥bert Csord√°s, Mehdi Bennani, Shane
  Obeid, Atsushi Saito, Guangyu Song, Haoqin Tu,           Legg, and Joel Veness. 2023. Randomized Positional
  Stanis≈Çaw WozÃÅniak, Ruichong Zhang, Bingchen             Encodings Boost Length Generalization of Trans-
  Zhao, Qihang Zhao, Peng Zhou, Jian Zhu, and              formers. In Proceedings of the 61st Annual Meet-
  Rui-Jie Zhu. 2024. Eagle and Finch: RWKV                 ing of the Association for Computational Linguistics
  with Matrix-Valued States and Dynamic Recurrence.        (Volume 2: Short Papers), pages 1889‚Äì1903, Toronto,
  https://arxiv.org/abs/2404.05892v3.                      Canada. Association for Computational Linguistics.
                                                     9972
Abulhair Saparov, Richard Yuanzhe Pang, Vishakh Pad-        Hugo Touvron, Louis Martin, Kevin Stone, Peter Al-
  makumar, Nitish Joshi, Mehran Kazemi, Najoung               bert, Amjad Almahairi, Yasmine Babaei, Nikolay
  Kim, and He He. 2023. Testing the General Deduc-            Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti
  tive Reasoning Capacity of Large Language Models            Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton
  Using OOD Examples. Advances in Neural Informa-             Ferrer, Moya Chen, Guillem Cucurull, David Esiobu,
  tion Processing Systems, 36:3083‚Äì3105.                      Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller,
                                                              Cynthia Gao, Vedanuj Goswami, Naman Goyal, An-
Uri Shaham, Maor Ivgi, Avia Efrat, Jonathan Berant,           thony Hartshorn, Saghar Hosseini, Rui Hou, Hakan
  and Omer Levy. 2023. ZeroSCROLLS: A Zero-Shot               Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa,
  Benchmark for Long Text Understanding. In Find-             Isabel Kloumann, Artem Korenev, Punit Singh Koura,
  ings of the Association for Computational Linguis-          Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Di-
  tics: EMNLP 2023, pages 7977‚Äì7989, Singapore.               ana Liskovich, Yinghai Lu, Yuning Mao, Xavier Mar-
  Association for Computational Linguistics.                  tinet, Todor Mihaylov, Pushkar Mishra, Igor Moly-
                                                              bog, Yixin Nie, Andrew Poulton, Jeremy Reizen-
Peter Shaw, Jakob Uszkoreit, and Ashish Vaswani. 2018.        stein, Rashi Rungta, Kalyan Saladi, Alan Schel-
  Self-Attention with Relative Position Representa-           ten, Ruan Silva, Eric Michael Smith, Ranjan Sub-
  tions. In Proceedings of the 2018 Conference of             ramanian, Xiaoqing Ellen Tan, Binh Tang, Ross
  the North American Chapter of the Association for           Taylor, Adina Williams, Jian Xiang Kuan, Puxin
  Computational Linguistics: Human Language Tech-             Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang, An-
  nologies, Volume 2 (Short Papers), pages 464‚Äì468,           gela Fan, Melanie Kambadur, Sharan Narang, Aure-
  New Orleans, Louisiana. Association for Computa-            lien Rodriguez, Robert Stojnic, Sergey Edunov, and
  tional Linguistics.                                         Thomas Scialom. 2023b. Llama 2: Open Founda-
                                                              tion and Fine-Tuned Chat Models. arXiv preprint.
Koustuv Sinha, Amirhossein Kazemnejad, Siva Reddy,            ArXiv:2307.09288 [cs].
  Joelle Pineau, Dieuwke Hupkes, and Adina Williams.
  2022. The Curious Case of Absolute Position Em-           Yao-Hung Hubert Tsai, Shaojie Bai, Makoto Yamada,
  beddings. arXiv preprint. ArXiv:2210.12574 [cs].            Louis-Philippe Morency, and Ruslan Salakhutdinov.
                                                              2019. Transformer Dissection: An Unified Under-
Konrad Staniszewski, Szymon Tworkowski, Sebastian             standing for Transformer‚Äôs Attention via the Lens of
  Jaszczur, Henryk Michalewski, ≈Åukasz KucinÃÅski, and         Kernel. In Proceedings of the 2019 Conference on
  Piotr Mi≈ÇosÃÅ. 2023. Structured Packing in LLM Train-        Empirical Methods in Natural Language Processing
  ing Improves Long Context Utilization.                      and the 9th International Joint Conference on Natu-
                                                              ral Language Processing (EMNLP-IJCNLP), pages
Jianlin Su, Murtadha Ahmed, Yu Lu, Shengfeng Pan,             4344‚Äì4353, Hong Kong, China. Association for Com-
   Wen Bo, and Yunfeng Liu. 2024. RoFormer: En-               putational Linguistics. Rate: 3.
   hanced transformer with Rotary Position Embedding.
   Neurocomputing, 568:127063.                              Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob
                                                              Uszkoreit, Llion Jones, Aidan N Gomez, ≈Åukasz
Yutao Sun, Li Dong, Barun Patra, Shuming Ma, Shao-            Kaiser, and Illia Polosukhin. 2017. Attention is All
  han Huang, Alon Benhaim, Vishrav Chaudhary, Xia             you Need. In Advances in Neural Information Pro-
  Song, and Furu Wei. 2023. A Length-Extrapolatable           cessing Systems, volume 30. Curran Associates, Inc.
  Transformer. In Proceedings of the 61st Annual Meet-
  ing of the Association for Computational Linguis-         Benyou Wang, Lifeng Shang, Christina Lioma, Xin
  tics (Volume 1: Long Papers), pages 14590‚Äì14604,            Jiang, Hao Yang, Qun Liu, and Jakob Grue Simonsen.
  Toronto, Canada. Association for Computational Lin-         2020. On Position Embeddings in BERT.
  guistics.                                                 Benyou Wang, Donghao Zhao, Christina Lioma, Qiuchi
                                                              Li, Peng Zhang, and Jakob Grue Simonsen. 2019.
Yi Tay, Mostafa Dehghani, Dara Bahri, and Donald              Encoding word order in complex embeddings.
  Metzler. 2022. Efficient Transformers: A Survey.
  ACM Computing Surveys, 55(6):109:1‚Äì109:28.                Suyuchen Wang, Ivan Kobyzev, Peng Lu, Mehdi
                                                              Rezagholizadeh, and Bang Liu. 2024.        Reso-
Yi Tay, Mostafa Dehghani, Jinfeng Rao, William Fedus,         nance RoPE: Improving Context Length General-
   Samira Abnar, Hyung Won Chung, Sharan Narang,              ization of Large Language Models. arXiv preprint.
   Dani Yogatama, Ashish Vaswani, and Donald Met-             ArXiv:2403.00071 [cs].
   zler. 2021. Scale Efficiently: Insights from Pretrain-
   ing and Finetuning Transformers.                         Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten
                                                               Bosma, Brian Ichter, Fei Xia, Ed Chi, Quoc V. Le,
Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier          and Denny Zhou. 2022. Chain-of-Thought Prompt-
  Martinet, Marie-Anne Lachaux, Timoth√©e Lacroix,              ing Elicits Reasoning in Large Language Models.
  Baptiste Rozi√®re, Naman Goyal, Eric Hambro, Faisal          Advances in Neural Information Processing Systems,
  Azhar, Aurelien Rodriguez, Armand Joulin, Edouard            35:24824‚Äì24837.
  Grave, and Guillaume Lample. 2023a. LLaMA:
  Open and Efficient Foundation Language Models.            Ulme Wennberg and Gustav Eje Henter. 2021. The
  arXiv preprint. ArXiv:2302.13971 [cs].                      Case for Translation-Invariant Self-Attention in
                                                        9973
  Transformer-Based Language Models. In Proceed-           Hattie Zhou, Arwen Bradley, Etai Littwin, Noam Razin,
  ings of the 59th Annual Meeting of the Association for     Omid Saremi, Josh Susskind, Samy Bengio, and Pree-
  Computational Linguistics and the 11th International       tum Nakkiran. 2023. What Algorithms can Trans-
  Joint Conference on Natural Language Processing            formers Learn? A Study in Length Generalization.
  (Volume 2: Short Papers), pages 130‚Äì140, Online.           arXiv preprint. ArXiv:2310.16028 [cs, stat].
  Association for Computational Linguistics.
                                                           Yongchao Zhou, Uri Alon, Xinyun Chen, Xuezhi Wang,
Zhaofeng Wu, Linlu Qiu, Alexis Ross, Ekin Aky√ºrek,           Rishabh Agarwal, and Denny Zhou. 2024. Trans-
  Boyuan Chen, Bailin Wang, Najoung Kim, Jacob An-           formers Can Achieve Length Generalization But Not
  dreas, and Yoon Kim. 2024. Reasoning or Reciting?          Robustly. arXiv preprint. ArXiv:2402.09371 [cs].
  Exploring the Capabilities and Limitations of Lan-
  guage Models Through Counterfactual Tasks. arXiv         Dawei Zhu, Nan Yang, Liang Wang, Yifan Song, Wen-
  preprint. ArXiv:2307.02477 [cs].                           hao Wu, Furu Wei, and Sujian Li. 2023. PoSE:
                                                             Efficient Context Window Extension of LLMs via
Changnan Xiao and Bing Liu. 2024. A Theory for               Positional Skip-wise Training. arXiv preprint.
  Length Generalization in Learning to Reason. arXiv         ArXiv:2309.10400 [cs].
  preprint. ArXiv:2404.00560 [cs].
Wenhan Xiong, Jingyu Liu, Igor Molybog, Hejia Zhang,
  Prajjwal Bhargava, Rui Hou, Louis Martin, Rashi
  Rungta, Karthik Abinav Sankararaman, Barlas Oguz,
  Madian Khabsa, Han Fang, Yashar Mehdad, Sharan
  Narang, Kshitiz Malik, Angela Fan, Shruti Bhosale,
  Sergey Edunov, Mike Lewis, Sinong Wang, and Hao
  Ma. 2023. Effective Long-Context Scaling of Foun-
  dation Models. arXiv preprint. ArXiv:2309.16039
  [cs].
Chulhee Yun, Srinadh Bhojanapalli, Ankit Singh Rawat,
  Sashank Reddi, and Sanjiv Kumar. 2019. Are
  Transformers universal approximators of sequence-
  to-sequence functions?
Jingqing Zhang, Yao Zhao, Mohammad Saleh, and Peter
   Liu. 2020. PEGASUS: Pre-training with Extracted
   Gap-sentences for Abstractive Summarization. In
   Proceedings of the 37th International Conference
   on Machine Learning, pages 11328‚Äì11339. PMLR.
   ISSN: 2640-3498.
Xinrong Zhang, Yingfa Chen, Shengding Hu, Zihang
  Xu, Junhao Chen, Moo Hao, Xu Han, Zhen Thai,
  Shuo Wang, Zhiyuan Liu, and Maosong Sun. 2024a.
  ƒ±nftyBench: Extending Long Context Evaluation Be-
  yond 100K Tokens. In Proceedings of the 62nd An-
  nual Meeting of the Association for Computational
  Linguistics (Volume 1: Long Papers), pages 15262‚Äì
  15277, Bangkok, Thailand. Association for Compu-
  tational Linguistics.
Yikai Zhang, Junlong Li, and Pengfei Liu. 2024b. Ex-
  tending LLMs‚Äô Context Window with 100 Samples.
  arXiv preprint. ArXiv:2401.07004 [cs].
Chuanyang Zheng, Yihang Gao, Han Shi, Minbin
  Huang, Jingyao Li, Jing Xiong, Xiaozhe Ren,
  Michael Ng, Xin Jiang, Zhenguo Li, and Yu Li. 2024.
  CAPE: Context-Adaptive Positional Encoding for
  Length Extrapolation.
Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan
  Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin,
  Zhuohan Li, Dacheng Li, Eric Xing, Hao Zhang,
  Joseph E. Gonzalez, and Ion Stoica. 2023. Judging
  LLM-as-a-Judge with MT-Bench and Chatbot Arena.
  Advances in Neural Information Processing Systems,
  36:46595‚Äì46623.
                                                       9974
A     Appendix                                                   position matters rather than absolute posi-
                                                                 tions).
A.1 Length Extrapolation on Generation
    Tasks                                                     ‚Ä¢ RPEs demonstrate better extrapolation ca-
To help readers gain a deeper understanding of                  pability. In the length extrapolation setting
the challenges of length extrapolation, we leverage             that this survey concerns most, RPEs also out-
LongBench-E (Bai et al., 2023b) as our testbed                  perform APEs due to intrinsic shift-invariance
and choose three trending LLMs with different con-              and binning strategy (for T5 bias) or expo-
text window sizes to evaluate their performance on              nentially decay with distance (for ALiBi and
various generation tasks and different evaluation               RoPE).
length ranges. The results are shown in Table 2.
                                                              ‚Ä¢ RPEs seek a balance between expressive-
   From the results, some intriguing conclusions
                                                                ness (embedding-based RPE) and extrapo-
can be drawn:
                                                                lation (bias-based RPE) and perplexity is in-
    1. When evaluating models on sequences beyond               sufficient. As in comparisons between RPEs,
       the original context window, a consistent per-           we can see that bias methods (T5 bias and
       formance degradation can be observed across              ALiBi) lead to lower perplexity on sequences
       models and tasks, which strongly supports the            with length both within and beyond the con-
       necessity of studying length extrapolation.              text window, which indicates bias methods
                                                                are better at language modeling by explicitly
    2. Thanks to the shift-invariance and decay-with-           pandering recency bias. Note that it does
       distance property of RPE these LLMs use,                 not mean our claim that embedding-based
       they can maintain a reasonable performance               methods like RoPE are more expressive is
       when dealing with sequences beyond the con-              wrong, considering that models with ALiBi
       text window, i.e., the performance will grad-            have worse performance than RoPE-based
       ually decline rather than immediately crush              models on current trending benchmarks (Pal
       after length exceeding the context window.               et al., 2023) like MMLU (Hendrycks et al.,
                                                                2020) and LMSys arena (Zheng et al., 2023).
    3. Even evaluating on sequences within the con-
                                                                This further shows that perplexity is insuffi-
       text window, the increase in sequence length
                                                                cient to reflect performance in these down-
       still leads to degraded performance. This may
                                                                stream tasks.
       be as a result of the increasing difficulty with
       increasing length or due to the sparsity of long-
       range dependencies in concatenated training         A.3    Thoughts on Standardized Benchmark
       long sequences, meaning length extrapolation        Realizing the difficulty and complexity of con-
       as a problem even exists within training con-       structing a standardized benchmark for length ex-
       text window and long-context transformers           trapolation, we present some preliminary thoughts
       trained on long sequences do not necessarily        on it as follows:
       possess strong length extrapolation capability.
                                                              ‚Ä¢ The benchmark should have no position bias.
                                                                This means the model cannot consistently rely
A.2 Results on Language Modeling                                on tokens at specific locations to reach the
To offer an empirical comparison between popular                correct answer. Thus, language modeling is
PEs, we statistically collect results from published            not an ideal task due to its recency bias, which
literatures and form Table 3.                                   makes it possible for the model to generate the
   We highlight several important conclusions from              correct token based solely on nearby tokens.
these results:
                                                              ‚Ä¢ The benchmark should require modeling the
     ‚Ä¢ RPEs demonstrate better in-distribution                  full range. This indicates the model cannot
       performance. On sequences with length                    depend on a small portion of the input but
       within context window, RPEs already demon-               needs to attend and model the full range of
       strate better performance, compared to APEs.             context to give correct responses. Thus, the
       We explain the results as RPE is consistent              popular Needle In A Haystack test (gkam-
       with the nature of natural language (relative            radt, 2024) is not an ideal benchmark, as it
                                                       9975
 Task               Evaluation Window Llama2-7B-Chat (4K) ChatGLM3-6B (8K) Vicuna-v1.5-7b-16k
 QA
                            0-4K                  34.56                   21.86                 31.19
 2WikiMQA                   4-8K                  23.95                   21.85                 17.71
                            8K+                   23.12                   13,72                 12.33
                            0-4K                  37.59                   25.92                 37.35
 HotpotQA                   4-8K                  27.84                   19.63                 24.09
                            8K+                   23.17                   15.96                 21.91
                            0-4K                  41.42                   44.04                 47.1
 MultiFieldQA-en            4-8K                  34.29                   29.31                 33.83
                            8K+                   21.21                   28.45                 28.29
 Summarization
                            0-4K                  26.67                   25.71                 27.96
 MultiNews                  4-8K                  22.33                   21.37                 23.62
                            8K+                   22.46                    20.4                 21.22
                            0-4K                  30.66                    30.7                 33.95
 GovReport                  4-8K                  27.39                   23.39                 29.91
                            8K+                   25.6                    22.2                  24.89
 Code Completion
                            0-4K                  63.73                   52.18                 56.14
 LCC                        4-8K                  61.59                   43.63                 57.69
                            8K+                   56.83                   40.37                 43.25

Table 2: Performance of Llama2-7B-Chat (Touvron et al., 2023b), ChatGLM3-6B (GLM et al., 2024) and
Vicuna-v1.5-7b (Zheng et al., 2023) on LongBench-E, where the context window of each model is indicated in
parentheses.




 Dataset                                WikiText-103                  OpenWebText2              ArXiv
 Context Window       512      1024                                                  512
 Evaluation Window 512 1012 1024 2024 512                                         1024   512 1024
 APE
 Sinusoidal                    20.05 43.54 19.34 51.09 26                         14168       5.8 1070
 RPE
 T5 Bias                       19.65 18.79 18.8 18.34 22.6                        22.2       5.16 4.91
 ALiBi                         19.73 18.73 18.66 18.05 22.8                       23.3       5.25 5.41
 RoPE                          20.07 21.37 19.33 31.17 23                          61        5.25 16.02
Table 3: Empirical comparisons of different PEs on language modeling. The results on WikiText-103 are obtained
from Sun et al. (2023) and the results on OpenWebText2 and ArXiv are obtained from Chi (2024). Note that the
results may not be fairly comparable across dataset due to differences in model and training.



                                                    9976
     only requires the model to search and retrieve
     only a small portion of the input that is signif-
     icantly different from other content, which is
     quite different from understanding and use of
     context (Liu et al., 2023b).

   ‚Ä¢ This benchmark should offer flexibility in se-
     quence length with relatively stable diffi-
     culty. This means the benchmark should con-
     sist of enough sequences at increasing lengths
     but not increasing difficulty. Thus, the bench-
     mark can directly help with the fine-grained
     evaluation of the length extrapolation capabil-
     ity of Transformers without the need to crop a
     complete sequence, where the consistency of
     difficulty ensures the evaluation is only rele-
     vant to the increasing length.

  As for a concrete example, calculating long se-
quences containing only addition and subtraction
within ten (and keeping the intermediate results in a
small range) might be a promising evaluation task,
considering that the task itself is simple enough for
common LLMs (Li et al., 2024) and we can thus
focus on the impact of increasing length.




                                                     9977
