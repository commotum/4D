                                                                         ConViT: Improving Vision Transformers
                                                                         with Soft Convolutional Inductive Biases


                                           SteÃÅphane d‚ÄôAscoli 1 2 Hugo Touvron 2 Matthew L. Leavitt 2 Ari S. Morcos 2 Giulio Biroli 1 2 Levent Sagun 2


                                                                  Abstract                                  1. Introduction
                                                                                                            The success of deep learning over the last decade has largely
arXiv:2103.10697v2 [cs.CV] 10 Jun 2021




                                                                                                            been fueled by models with strong inductive biases, al-
                                              Convolutional architectures have proven ex-
                                                                                                            lowing efficient training across domains (Mitchell, 1980;
                                              tremely successful for vision tasks. Their hard
                                                                                                            Goodfellow et al., 2016). The use of Convolutional Neural
                                              inductive biases enable sample-efficient learning,
                                                                                                            Networks (CNNs) (LeCun et al., 1998; 1989), which have
                                              but come at the cost of a potentially lower perfor-
                                                                                                            become ubiquitous in computer vision since the success of
                                              mance ceiling. Vision Transformers (ViTs) rely
                                                                                                            AlexNet in 2012 (Krizhevsky et al., 2017), epitomizes this
                                              on more flexible self-attention layers, and have
                                                                                                            trend. Inductive biases are hard-coded into the architectural
                                              recently outperformed CNNs for image classifi-
                                                                                                            structure of CNNs in the form of two strong constraints
                                              cation. However, they require costly pre-training
                                                                                                            on the weights: locality and weight sharing. By encourag-
                                              on large external datasets or distillation from pre-
                                                                                                            ing translation equivariance (without pooling layers) and
                                              trained convolutional networks. In this paper, we
                                                                                                            translation invariance (with pooling layers) (Scherer et al.,
                                              ask the following question: is it possible to com-
                                                                                                            2010; Schmidhuber, 2015; Goodfellow et al., 2016), the
                                              bine the strengths of these two architectures while
                                                                                                            convolutional inductive bias makes models more sample-
                                              avoiding their respective limitations? To this
                                                                                                            efficient and parameter-efficient (Simoncelli & Olshausen,
                                              end, we introduce gated positional self-attention
                                                                                                            2001; Ruderman & Bialek, 1994). Similarly, for sequence-
                                              (GPSA), a form of positional self-attention which
                                                                                                            based tasks, recurrent networks with hard-coded memory
                                              can be equipped with a ‚Äúsoft‚Äù convolutional in-
                                                                                                            cells have been shown to simplify the learning of long-range
                                              ductive bias. We initialize the GPSA layers to
                                                                                                            dependencies (LSTMs) and outperform vanilla recurrent
                                              mimic the locality of convolutional layers, then
                                                                                                            neural networks in a variety of settings (Gers et al., 1999;
                                              give each attention head the freedom to escape
                                                                                                            Sundermeyer et al., 2012; Greff et al., 2017).
                                              locality by adjusting a gating parameter regu-
                                              lating the attention paid to position versus con-             However, the rise of models based purely on attention in
                                              tent information. The resulting convolutional-                recent years calls into question the necessity of hard-coded
                                              like ViT architecture, ConViT, outperforms the                inductive biases. First introduced as an add-on to recurrent
                                              DeiT (Touvron et al., 2020) on ImageNet, while                neural networks for Sequence-to-Sequence models (Bah-
                                              offering a much improved sample efficiency. We                danau et al., 2014), attention has led to a breakthrough in
                                              further investigate the role of locality in learn-            Natural Language Processing through the emergence of
                                              ing by first quantifying how it is encouraged in              Transformer models, which rely solely on a particular kind
                                              vanilla self-attention layers, then analyzing how it          of attention: Self-Attention (SA) (Vaswani et al., 2017).
                                              is escaped in GPSA layers. We conclude by pre-                The strong performance of these models when pre-trained
                                              senting various ablations to better understand the            on large datasets has quickly led to Transformer-based ap-
                                              success of the ConViT. Our code and models are                proaches becoming the default choice over recurrent models
                                              released publicly at https://github.com/                      like LSTMs (Devlin et al., 2018).
                                              facebookresearch/convit.
                                                                                                            In vision tasks, the locality of CNNs impairs the ability to
                                                                                                            capture long-range dependencies, whereas attention does
                                                                                                            not suffer from this limitation. Chen et al. (2018) and Bello
                                             1
                                               Department of Physics, Ecole Normale SupeÃÅrieure, Paris,     et al. (2019) leveraged this complementarity by augmenting
                                         France 2 Facebook AI Research, Paris, France. Correspondence to:   convolutional layers with attention. More recently, Ra-
                                         SteÃÅphane d‚ÄôAscoli <stephane.dascoli@ens.fr>.                      machandran et al. (2019) ran a series of experiments replac-
                                                                                                            ing some or all convolutional layers in ResNets with atten-
                                         Proceedings of the 38 th International Conference on Machine
                                                                                                            tion, and found the best performing models used convolu-
                                         Learning, PMLR 139, 2021. Copyright 2021 by the author(s).
                        ConViT: Improving Vision Transformers with Soft Convolutional Inductive Biases

                  Hard inductive bias    Soft inductive bias          et al., 2015), which has recently been applied to transfer
                        (CNN)                  (ConViT)               the inductive bias of a convolutional teacher to a student
                   # parameters            # parameters               transformer (Touvron et al., 2020). While these two meth-
  Helpful                                                             ods offer an interesting compromise, they forcefully induce
                                                                      convolutional inductive biases into the Transformers, poten-
  Neutral                                                             tially affecting the Transformer with their limitations.

 Harmful
                            # samples               # samples
                                                                      Contribution In this paper, we take a new step towards
                                                                      bridging the gap between CNNs and Transformers, by pre-
                                                                      senting a new method to ‚Äúsoftly‚Äù introduce a convolutional
                                                                      inductive bias into the ViT. The idea is to let each SA layer
Figure 1. Soft inductive biases can help models learn without
                                                                      decide whether to behave as a convolutional layer or not,
being restrictive. Hard inductive biases, such as the architectural
                                                                      depending on the context. We make the following contribu-
constraints of CNNs, can greatly improve the sample-efficiency of
learning, but can become constraining when the size of the dataset    tions:
is not an issue. The soft inductive biases introduced by the ConViT
avoid this limitation by vanishing away when not required.              1. We present a new form of SA layer, named gated posi-
                                                                           tional self-attention (GPSA), which one can initialize
                                                                           as a convolutional layer. Each attention head then has
tions in early layers and attention in later layers. The Vision            the freedom to recover expressivity by adjusting a gat-
Transformer (ViT), introduced by Dosovitskiy et al. (2020),                ing parameter.
entirely dispenses with the convolutional inductive bias by             2. We then perform experiments based on the DeiT (Tou-
performing SA across embeddings of patches of pixels. The                  vron et al., 2020), with a certain number of SA layers
ViT is able to match or exceed the performance of CNNs                     replaced by GPSA layers. The resulting Convolutional
but requires pre-training on vast amounts of data. More                    Vision Transformer (ConViT) outperforms the DeiT
recently, the Data-efficient Vision Transformer (DeiT) (Tou-               while boasting a much improved sample-efficiency
vron et al., 2020) was able to reach similar performances                  (Fig. 2).
without any pre-training on supplementary data, instead re-             3. We analyze quantitatively how local attention is natu-
lying on Knowledge Distillation (Hinton et al., 2015) from                 rally encouraged in vanilla ViTs, then investigate the
a convolutional teacher.                                                   inner workings of the ConViT and perform ablations
                                                                           to investigate how it benefits from the convolution ini-
                                                                           tialization.
Soft inductive biases The recent success of the ViT
demonstrates that while convolutional constraints can enable
strongly sample-efficient training in the small-data regime,          Overall, our work demonstrates the effectiveness of ‚Äùsoft‚Äù
they can also become limiting as the dataset size is not              inductive biases, especially in the low-data regime where
an issue. In data-plentiful regimes, hard inductive biases            the learning model is highly underspecified (see Fig. 1), and
can be overly restrictive and learning the most appropriate           motivates the exploration of further methods to induce them.
inductive bias can prove more effective. The practitioner
is therefore confronted with a dilemma between using a                Related work Our work is motivated by combining the
convolutional model, which has a high performance floor               recent success of pure Transformer models (Dosovitskiy
but a potentially lower performance ceiling due to the hard           et al., 2020) with the formalized relationship between SA
inductive biases, or a self-attention based model, which has          and convolution. Indeed, Cordonnier et al. (2019) showed
a lower floor but a higher ceiling. This dilemma leads to                         ‚àö with Nh heads can express a convolution of
                                                                      that a SA layer
the following question: can one get the best of both worlds,          kernel size N h , if each head focuses on one of the pixels
and obtain the benefits of the convolutional inductive biases         in the kernel patch. By investigating the qualitative aspect of
without suffering from its limitations (see Fig. 1)?                  attention maps of models trained on CIFAR-10, it is shown
                                                                      that SA layers with relative positional encodings naturally
In this direction, one successful approach is the combina-
                                                                      converge towards convolutional-like configurations, sug-
tion of the two architectures in ‚Äúhybrid‚Äù models. These
                                                                      gesting that some degree of convolutional inductive bias is
models, which interleave or combine convolutional and self-
                                                                      desirable.
attention layers, have fueled successful results in a variety
of tasks (Carion et al., 2020; Hu et al., 2018a; Ramachan-            Conversely, the restrictiveness of hard locality constraints
dran et al., 2019; Chen et al., 2020; Locatello et al., 2020;         has been proven by Elsayed et al. (2020). A breadth of
Sun et al., 2019; Srinivas et al., 2021; Wu et al., 2020). An-        approaches have been taken to imbue CNN architectures
other approach is that of Knowledge Distillation (Hinton              with nonlocality (Hu et al., 2018b;c; Wang et al., 2018; Wu
                                               ConViT: Improving Vision Transformers with Soft Convolutional Inductive Biases


                  80                                                                       84                               Multi-head self-attention The attention mechanism is
                                                  35                                                                        based on a trainable associative memory with (key, query)
                                                                                           82
                  70                              30
                                                                                           80                               vector pairs. A sequence of L1 ‚Äúquery‚Äù embeddings Q ‚àà




                                                      Relative gain (%)

                                                                          Top-1 accuracy
 Top-1 accuracy




                                                  25
                  60                     ConViT-S                                          78                               RL1 √óDh is matched against another sequence of L2 ‚Äúkey‚Äù
                                         DeiT-S   20
                                                                                           76                               embeddings K ‚àà RL2 √óDh using inner products. The re-
                  50                              15
                                                                                           74                               sult is an attention matrix whose entry (ij) quantifies how
                                                  10                                                    ConViT    ViT
                  40                                                                       72           DeiT      T2T-ViT   semantically ‚Äúrelevant‚Äù Qi is to Kj :
                                                  5                                                     EffNet    VT
                                                                                           70           ResNet
                            10                  100                                             101           102                                    QK >
                                                                                                                                                         
                        Images in training set (%)                                              Number of params [M]                  A = softmax ‚àö         ‚àà RL1 √óL2 ,               (1)
                                                                                                                                                      Dh
                       (a) Sample efficiency                                               (b) Parameter efficiency
                                                                                                                            where (softmax [X])ij = eXij / k eXik .
                                                                                                                                                          P
Figure 2. The ConViT outperforms the DeiT both in sample
and parameter efficiency. Left: we compare the sample effi-
                                                                                                                            Self-attention is a special case of attention where a sequence
ciency of our ConViT-S (see Tab. 1) with that of the DeiT-S by                                                              is matched to itself, to extract the semantic dependencies
training them on restricted portions of ImageNet-1k, where we                                                               between its parts. In the ViT, the queries and keys are linear
only keep a certain fraction of the images of each class. Both mod-                                                         projections of the embeddings of 16 √ó 16 pixel patches
els are trained with the hyperparameters reported in (Touvron et al.,                                                       X ‚àà RL√óDemb . Hence, we have Q = Wqry X and K =
2020). We display the the relative improvement of the ConViT                                                                Wkey X, where Wkey , Wqry ‚àà RDemb √óDh .
over the DeiT in green. Right: we compare the top-1 accuracies
of our ConViT models with those of other ViTs (diamonds) and                                                                Multi-head SA layers use several self-attention heads in
CNNs (squares) on ImageNet-1k. The performance of other mod-                                                                parallel to allow the learning of different kinds of interde-
els on ImageNet are taken from (Touvron et al., 2020; He et al.,                                                            pendencies. They take as input a sequence of L embeddings
2016; Tan & Le, 2019; Wu et al., 2020; Yuan et al., 2021).                                                                  of dimension Demb = Nh Dh , and output a sequence of L
                                                                                                                            embeddings of the same dimension through the following
                                                                                                                            mechanism:

et al., 2020). Another line of research is to induce a convolu-                                                                   MSA(X) := concat [SAh (X)] Wout + bout ,            (2)
                                                                                                                                                 h‚àà[Nh ]
tional inductive bias is different architectures. For example,
Neyshabur (2020) uses a regularization method to encour-
                                                                                                                            where Wout ‚àà RDemb √óDemb , bout ‚àà RDemb . Each self-
age fully-connected networks (FCNs) to learn convolutions
                                                                                                                            attention head h performs the following operation:
from scratch throughout training.
Most related to our approach, d‚ÄôAscoli et al. (2019) explored                                                                                SAh (X) := Ah XWval
                                                                                                                                                             h
                                                                                                                                                                 ,                    (3)
a method to initialize FCNs networks as CNNs. This en-
ables the resulting FCN to reach much higher performance
                                                                                                                                   h
                                                                                                                            where Wval ‚àà RDemb √óDh is the value matrix.
than achievable with standard initialization. Moreover, if                                                                  However, in the vanilla form of Eq. 1, SA layers are position-
the FCN is initialized from a partially trained CNN, the                                                                    agnostic: they do not know how the patches are located ac-
recovered degrees of freedom allow the FCN to outperform                                                                    cording to each other. To incorporate positional information,
the CNN it stems from. This method relates more generally                                                                   there are several options. One is to add some positional
to ‚Äúwarm start‚Äù approaches such as those used in spiked                                                                     information to the input at embedding time, before propa-
tensor models (Anandkumar et al., 2016), where a smart                                                                      gating it through the SA layers: (Dosovitskiy et al., 2020)
initialization, containing prior information on the problem,                                                                use this approach in their ViT. Another possibility is to re-
is used to ease the learning task.                                                                                          place the vanilla SA with positional self-attention (PSA),
                                                                                                                            using encodings rij of the relative position of patches i and
                                                                                                                            j (Ramachandran et al., 2019):
Reproducibility We provide an open-source implemen-
                                                                                                                                       Ahij := softmax Qhi Kjh> + vpos h>
                                                                                                                                                                              
tation of our method as well as pretrained models                                                                                                                         rij          (4)
at the following address: https://github.com/
                                                                                                                                                                                  h
facebookresearch/convit.                                                                                                    Each attention head uses a trainable embedding vpos       ‚àà
                                                                                                                              Dpos
                                                                                                                            R      , and the relative positional encodings rij ‚àà RDpos
                                                                                                                            only depend on the distance between pixels i and j, denoted
2. Background
                                                                                                                            denoted by a two-dimensional vector Œ¥ij .
We begin by introducing the basics of SA layers, and show
how positional attention can allow SA layers to express                                                                     Self-attention as a generalized convolution Cordonnier
convolutional layers.                                                                                                       et al. (2019) show that a multi-head PSA layer with Nh
                         ConViT: Improving Vision Transformers with Soft Convolutional Inductive Biases

                      Head 1             Head 2              Head 3
                                                                                     Image
                                                                                                                       normalize              ùê¥ùëñùëó
                                                                                   embedding


                                                                                         FFN                 1 ‚àí ùúé(ùúÜ)                  ùúé(ùúÜ)




                                                                        Nonlocal
                                                                                          SA                                +




   (a) Input                   (b) Standard initialization                               FFN
     Head 1           Head 2             Head 3              Head 4                       SA                 softmax               softmax


                                                                                      FFN
                                                                                      GPSA




                                                                        Local
                                                                                                                 ‚àó                      ‚àó



                                                                                      FFN
                                                                                      GPSA
                                                                                                          ùëäùëûùëüùë¶       ùëäùëòùëíùë¶       ùë£ùëùùëúùë†          ùëüùëñùëó

        (c) Convolutional initialization, strength Œ± = 0.5
     Head 1           Head 2             Head 3              Head 4
                                                                                               Class       ùëãùëñ        ùëãùëó
                                                                                    Patches    token



                                                                                    ConViT                   Gated Positional
                                                                                                              Self-Attention
         (d) Convolutional initialization, strength Œ± = 2
                                                                       Figure 4. Architecture of the ConViT. The ConViT (left) is a ver-
Figure 3. Positional self-attention layers can be initialized as       sion of the ViT in which some of the self-attention (SA) layers
convolutional layers. (a): Input image from ImageNet, where the        are replaced with gated positional self-attention layers (GPSA;
query patch is highlighted by a red box. (b),(c),(d): attention maps   right). Because GPSA layers involve positional information, the
of an untrained SA layer (b) and those of a PSA layer using the        class token is concatenated with hidden representation after the
convolutional-like initialization scheme of Eq. 5 with two different   last GPSA layer. In this paper, we typically take 10 GPSA lay-
values of the locality strength parameter, Œ± (c, d). Note that the     ers followed by 2 vanilla SA layers. FFN: feedforward network
shapes of the image can easily be distinguished in (b), but not in     (2 linear layers separated by a GeLU activation); Wqry : query
(c) or (d), when the attention is purely positional.                   weights; Wkey : key weights; vpos : attention center and span em-
                                                                       beddings (learned); rqk : relative position encodings (fixed); Œª:
                                                                       gating parameter (learned); œÉ: sigmoid function.
heads and learnable relative positional encodings (Eq. 4) of
dimension‚àöDpos ‚â•‚àö  3 can express any convolutional layer of                                                         ‚àö        ‚àö
filter size N h √ó N h , by setting the following:                      each of the possible positional offsets of a Nh √ó Nh
        Ô£±                                                             convolutional kernel, and sending the locality strengths Œ±h
             h          h            h          h                      to some large value.
        Ô£≤ vpos := ‚àíŒ± 1, ‚àí2‚àÜ1 , ‚àí2‚àÜ
        Ô£¥
                                               2 , 0, . . . 0
            rŒ¥ := kŒ¥k2 , Œ¥1 , Œ¥2 , 0, . . . 0                    (5)
                                                                       3. Approach
        Ô£¥
            Wqry = Wkey := 0, Wval := I
        Ô£≥

                                                                       Building on the insight of (Cordonnier et al., 2019), we in-
In the above,
                                                                       troduce the ConVit, a variant of the ViT (Dosovitskiy et al.,
                                                                       2020) obtained by replacing some of the SA layers by a new
   ‚Ä¢ The center of attention ‚àÜh ‚àà R2 is the position to
                                                                       type of layer which we call gated positional self-attention
     which head h pays most attention to, relative to the
                                                                       (GPSA) layers. The core idea is to enforce the ‚Äúinformed‚Äù
     query patch. For example, in Fig. 3(c), the four heads
                                                                       convolutional configuration of Eqs. 5 in the GPSA layers at
     correspond, from left to right, to ‚àÜ1 = (‚àí1, 1), ‚àÜ2 =
                                                                       initialization, then let them decide whether to stay convo-
     (‚àí1, ‚àí1), ‚àÜ3 = (1, 1), ‚àÜ4 = (1, ‚àí1).
                                                                       lutional or not. However, the standard parameterization of
   ‚Ä¢ The locality strength Œ±h > 0 determines how focused               PSA layers (Eq. 4) suffers from two limitations, which lead
     the attention is around its center ‚àÜh (it can also by un-         us two introduce two modifications.
     derstood as the ‚Äútemperature‚Äù of the softmax in Eq. 1).
     When Œ±h is large, the attention is focused only on the            Adaptive attention span The first caveat in PSA is the
     patch(es) located at ‚àÜh , as in Fig. 3(d); when Œ±h is             vast number of trainable parameters involved, since the num-
     small, the attention is spread out into a larger area, as         ber of relative positional encodings rŒ¥ is quadratic in the
     in Fig. 3(c).                                                     number of patches. This led some authors to restrict the
                                                                       attention to a subset of patches around the query patch (Ra-
Thus, the PSA layer can achieve a strictly convolutional               machandran et al., 2019), at the cost of losing long-range
attention map by setting the centers of attention ‚àÜh to                information.
                       ConViT: Improving Vision Transformers with Soft Convolutional Inductive Biases

To avoid this, we leave the relative positional encodings         equipped with residual connections. The ConViT is simply
                                                h
rŒ¥ fixed, and train only the embeddings vpos        which de-     a ViT where the first 10 blocks replace the SA layers by
termine the center and span of the attention heads; this          GPSA layers with a convolutional initialization.
approach relates to the adaptive attention span introduced
                                                                  Similar to language Transformers like BERT (Devlin et al.,
in Sukhbaatar et al. (2019) for Language Transformers. The
                            h                                     2018), the ViT uses an extra ‚Äúclass token‚Äù, appended to the
initial values of rŒ¥ and vpos   are given by Eq. 5, where we
                                                                  sequence of patches to predict the class of the input. Since
take Dpos = 3 to get rid of the useless zero components.
                                                                  this class token does not carry any positional information,
Thanks to Dpos  Dh , the number of parameters involved
                                                                  the SA layers of the ViT do not use positional attention:
in the positional attention is negligible compared to the num-
                                                                  the positional information is instead injected to each patch
ber of parameters involved in the content attention. This
                                                                  before the first layer, by adding a learnable positional em-
makes sense, as content interactions are inherently much
                                                                  bedding of dimension Demb . As GPSA layers involve posi-
simpler to model than positional interactions.
                                                                  tional attention, they are not well suited for the class token
                                                                  approach. We solve this problem by appending the class
Positional gating The second issue with standard PSA is           token to the patches after the last GPSA layer, similarly to
the fact that the content and positional terms in Eq. 4 are po-   what is done in (Touvron et al., 2021b) (see Fig. 4)1 .
tentially of different magnitudes, in which case the softmax
will ignore the smallest of the two. In particular, the con-      For fairness, and since they are computationally cheap, we
volutional initialization scheme discussed above involves         keep the absolute positional embeddings of the ViT active
highly concentrated attention scores, i.e. high-magnitude         in the ConViT. However, as shown in SM. F, the ConViT
values in the softmax. In practice, we observed that using a      relies much less on them, since the GPSA layers already use
convolutional initialization scheme on vanilla PSA layers         relative positional encodings. Hence, the absolute positional
gives a boost in early epochs, but degrades late-time perfor-     embeddings could easily be removed, dispensing with the
mance as the attention mechanism lazily ignores the content       need to interpolate the embeddings when changing the input
information (see SM. A).                                          resolution (the relative positional encodings simply need to
                                                                  be resampled according to Eq. 5, as performed automatically
To avoid this, GPSA layers sum the content and positional         in our open-source implementation).
terms after the softmax, with their relative importances gov-
erned by a learnable gating parameter Œªh (one for each
attention head). Finally, we normalize the resulting sum of       Training details We based our ConVit on the DeiT (Tou-
matrices (whose terms are positive) to ensure that the result-    vron et al., 2020), a hyperparameter-optimized version of
ing attention scores define a probability distribution. The       the ViT which has been open-sourced2 . Thanks to its ability
resulting GPSA layer is therefore parametrized as follows         to achieve competitive results without using any external
(see also Fig. 4):                                                data, the DeiT both an excellent baseline and relatively easy
                                                                  to train: the largest model (DeiT-B) only requires a few days
     GPSAh (X) := normalize Ah XWval          h
                                  
                                                          (6)     of training on 8 GPUs.
                h                                h   h>
                                                        
              Aij := (1 ‚àí œÉ(Œªh )) softmax Qi Kj                   To mimic 2 √ó 2, 3 √ó 3 and 4 √ó 4 convolutional filters, we
                                            h>
                                                                 consider three different ConViT models with 4, 9 and 16
                     + œÉ(Œªh ) softmax vpos      rij ,     (7)
                                                                  attention heads (see Tab. 1). Their number of heads are
                                    P
where (normalize [A])ij = Aij / k Aik and œÉ : x 7‚Üí                slightly larger than the DeiT-Ti, ConViT-S and ConViT-B
1/(1+e‚àíx ) is the sigmoid function. By setting the gating         of Touvron et al. (2020), which respectively use 3, 6 and 12
parameter Œªh to a large positive value at initialization, one     attention heads. To obtain models of similar sizes, we use
has œÉ(Œªh ) ‚âÉ 1 : the GPSA bases its attention purely on           two methods of comparison.
position, dispensing with the need of setting Wqry and
Wkey to zero as in Eq. 5. However, to avoid the ConViT               ‚Ä¢ To establish a direct comparison with Touvron et al.
staying stuck at Œªh  1, we initialize Œªh = 1 for all layers           (2020), we lower the embedding dimension of the Con-
and all heads.                                                         ViTs to Demb /Nh = 48 instead of 64 used for the
                                                                       DeiTs. Importantly, we leave all hyperparameters
Architectural details The ViT slices input images of size              (scheduling, data-augmentation, regularization) pre-
224 into 16 √ó 16 non-overlapping patches of 14 √ó 14 pixels             sented in (Touvron et al., 2020) unchanged in order to
and embeds them into vectors of dimension Demb = 64Nh                1
                                                                        We also experimented incorporating the class token as an
using a convolutional stem. It then propagates the patches
                                                                  extra patch of the image to which all heads pay attention to at
through 12 blocks which keep their dimensionality constant.       initialization, but results were worse than concatenating the class
Each block consists in a SA layer followed by a 2-layer           token after the GPSA layers (not shown).
                                                                      2
Feed-Forward Network (FFN) with GeLU activation, both                   https://github.com/facebookresearch/deit
                       ConViT: Improving Vision Transformers with Soft Convolutional Inductive Biases

                        Name           Model    Nh      Demb     Size      Flops    Speed     Top-1    Top-5
                                    DeiT         3       192      6M         1G      1442      72.2      -
                           Ti
                                   ConViT        4       192      6M         1G       734      73.1     91.7
                                    DeiT         4       256      10M        2G      1036      75.9     93.2
                           Ti+
                                   ConViT        4       256      10M        2G       625      76.7     93.6
                                    DeiT         6       384      22M       4.3G      587      79.8      -
                           S
                                   ConViT        9       432      27M       5.4G      305      81.3     95.7
                                    DeiT         9       576      48M       10G       480      79.0     94.4
                           S+
                                   ConViT        9       576      48M       10G       382      82.2     95.9
                                    DeiT        12       768      86M       17G       187      81.8      -
                           B
                                   ConViT       16       768      86M       17G       141      82.4     95.9
                                    DeiT        16      1024     152M       30G       114      77.5     93.5
                           B+
                                   ConViT       16      1024     152M       30G        96      82.5     95.9

Table 1. Performance of the models considered, trained from scratch on ImageNet. Speed is the number of images processed per
second on a Nvidia Quadro GP100 GPU at batch size 128. Top-1 accuracy is measured on ImageNet-1k test set without distillation (see
SM. B for distillation). The results for DeiT-Ti, DeiT-S and DeiT-B are reported from (Touvron et al., 2020).


 Train             Top-1                        Top-5                   their number of parameters, number of flops and throughput.
  size    DeiT     ConViT        Gap     DeiT   ConViT     Gap          Each ConViT outperforms its DeiT of same size and same
  5%       34.8     47.8         37%     57.8    70.7      22%          number of flops by a margin. Importantly, although the
  10%      48.0     59.6         24%     71.5    80.3      12%          positional self-attention does slow down the throughput
  30%      66.1     73.7         12%     86.0    90.7       5%          of the ConViTs, they also outperform the DeiTs at equal
  50%      74.6     78.2          5%     91.8    93.8       2%          throughput. For example, The ConViT-S+ reaches a top-
 100%      79.9     81.4          2%     95.0    95.8       1%
                                                                        1 of 82.2%, outperforming the original DeiT-B with less
Table 2. The convolutional inductive bias strongly improves             parameters and higher throughput. Without any tuning, the
sample efficiency. We compare the top-1 and top-5 accuracy              ConViT also reaches high performance on CIFAR100, see
of our ConViT-S with that of the DeiT-S, both trained using the         SM. C where we also report learning curves.
original hyperparameters of the DeiT (Touvron et al., 2020), as
                                                                        Note that our ConViT is compatible with the distillation
well as the relative improvement of the ConViT over the DeiT.
                                                                        methods introduced in Touvron et al. (2020) at no extra cost.
Both models are trained on a subsampled version of ImageNet-1k,
where we only keep a variable fraction (leftmost column) of the         As shown in SM. B, hard distillation improves performance,
images of each class for training.                                      enabling the hard-distilled ConViT-S+ to reach 82.9% top-1
                                                                        accuracy, on the same footing as the hard-distilled DeiT-
                                                                        B with half the number of parameters. However, while
     achieve a fair comparison. The resulting models are                distillation requires an additional forward pass through a
     named ConViT-Ti, ConViT-S and ConViT-B.                            pre-trained CNN at each step of training, ConViT has no
                                                                        such requirement, providing similar benefits to distillation
   ‚Ä¢ We also trained DeiTs and ConViTs using the same
                                                                        without additonal computational requirements.
     number of heads and Demb /Nh = 64, to ensure that
     the improvement due to ConViT is not simply due to
     the larger number of heads (Touvron et al., 2021b).                Sample efficiency of the ConViT In Tab. 2, we investi-
     This leads to slightly larger models denoted with a ‚Äú+‚Äù            gate the sample-efficiency of the ConViT in a systematic
     in Tab. 1. To maintain stable training while fitting these         way, by subsampling each class of the ImageNet-1k dataset
     models on 8 GPUs, we lowered the learning rate from                by a fraction f = {0.05, 0.1, 0.3, 0.5, 1} while multiply-
     0.0005 to 0.0004 and the batch size from 1024 to 512.              ing the number of epochs by 1/f so that the total number
     These minimal hyperparameter changes lead the DeiT-                images presented to the model remains constant. As one
     B+ to perform less well than the DeiT-S+, which is not             might expect, the top-1 accuracy of both the DeiT-S and its
     the case for the ConViT, suggesting a higher stability             ConViT-S counterpart drops as f decreases. However, the
     to hyperparameter changes.                                         ConViT suffers much less: while training on only 10% of
                                                                        the data, the ConVit reaches 59.5% top-1 accuracy, com-
                                                                        pared to 46.5% for its DeiT counterpart.
Performance of the ConViT In Tab. 1, we display the
top-1 accuracy achieved by these models evaluated on the                This result can be directly compared to (Zhai et al., 2019),
ImageNet test set after 300 epochs of training, alongside               which after testing several thousand convolutional models
                       ConViT: Improving Vision Transformers with Soft Convolutional Inductive Biases

reaches a top-1 accuracy of 56.4%; the ConViT is therefore                                          DeiT                                   ConViT
                                                                                                                                                                            Layer 1
highly competitive in terms of sample efficiency. These find-                       7                                       7                                               Layer 2
ings confirm our hypothesis that the convolutional inductive                        6                                       6                                               Layer 3




                                                                  Non-locality
                                                                                                                                                                            Layer 4
bias is most helpful on small datasets, as depicted in Fig. 1.                      5                                       5                                               Layer 5
                                                                                                                                                                            Layer 6
                                                                                    4                                       4                                               Layer 7
4. Investigating the role of locality                                                                                                                                       Layer 8
                                                                                    3                                       3                                               Layer 9
                                                                                        0        100 200             300        0         100 200             300           Layer 10
In this section, we demonstrate that locality is naturally                                         Epochs                                  Epochs                           Layer 11
                                                                                                                                                                            Layer 12
encouraged in standard SA layers, and examine how the
ConViT benefits from locality being imposed at initializa-
                                                                  Figure 5. SA layers try to become local, GPSA layers escape
tion.
                                                                  locality. We plot the nonlocality metric defined in Eq. 8, averaged
                                                                  over a batch of 1024 images: the higher, the further the attention
                                                                  heads look from the query pixel. We trained the DeiT-S and
SA layers are pulled towards locality We begin by in-
                                                                  ConViT-S for 300 epochs on ImageNet. Similar results for DeiT-
vestigating whether the hypothesis that PSA layers are nat-
                                                                  Ti/ConViT-Ti and DeiT-B/ConViT-B are shown in SM. D.
urally encouraged to become ‚Äúlocal‚Äù over the course of
training (Cordonnier et al., 2019) holds for the vanilla SA
                                                                                    1.0 Layer 1         1.0
                                                                                                                  Layer 2               Layer 3
                                                                                                                                                  1.0
                                                                                                                                                            Layer 4             Layer 5




                                                                  Positional attn
layers used in ViTs, which do not benefit from positional
attention. To quantify this, we define a measure of ‚Äúnonlo-                         0.5                 0.5                 0.5                   0.5                 0.5
cality‚Äù by summing, for each query patch i, the distances
kŒ¥ij k to all the key patches j weighted by their attention                         0.0                 0.0                 0.0                   0.0                 0.0
                                                                                          0       250         0       250           0       250         0       250         0       250
score Aij . We average the number obtained over the query                                     Layer 6             Layer 7               Layer 8             Layer 9         Layer 10
                                                                                    1.0
                                                                  Positional attn

patch to obtain the nonlocality metric of head h, which
can then be averaged over the attention heads to obtain the                         0.5                 0.5                 0.5                   0.5                 0.5
nonlocality of the whole layer `:
                                                                                    0.0                 0.0                 0.0                   0.0                 0.0
                                                                                          0 250               0 250                 0 250               0 250               0 250
                                                                                           Epochs              Epochs                Epochs              Epochs              Epochs
                  `,h    1 X h,`
                 Dloc :=     A kŒ¥ij k,
                         L ij ij
                                                                  Figure 6. The gating parameters reveal the inner workings of
                  `         1 X `,h                               the ConViT. For each layer, the colored lines (one for each of the
                 Dloc :=       Dloc                        (8)
                           Nh                                     9 attention heads) quantify how much attention head h pays to
                                h
                                                                  positional information versus content, i.e. the value of œÉ(Œªh ), see
                                                                  Eq. 7. The black line represents the value averaged over all heads.
Intuitively, Dloc is the number of patches between the center     We trained the ConViT-S for 300 epochs on ImageNet. Similar
of attention and the query patch: the further the attention       results for ConViT-Ti and ConViT-B are shown in SM D.
heads look from the query patch, the higher the nonlocality.
In Fig. 5 (left panel), we show how the nonlocality metric        GPSA layers escape locality In the ConViT, strong lo-
evolves during training across the 12 layers of a DeiT-S          cality is imposed at the beginning of training in the GPSA
trained for 300 epochs on ImageNet. During the first few          layers thanks to the convolutional initialization. In Fig. 5
epochs, the nonlocality falls from its initial value in all       (right panel), we see that this local configuration is escaped
layers, confirming that the DeiT becomes more ‚Äúconvolu-           throughout training, as the nonlocality metric grows in all
tional‚Äù. During the later stages of training, the nonlocality     the GPSA layers. However, the nonlocality at the end of
metric stays low for lower layers, and gradually climbs back      training is lower than that reached by the DeiT, showing
up for upper layers, revealing that the latter capture long       that some information about the initialization is preserved
range dependencies, as observed for language Transform-           throughout training. Interestingly, the final nonlocality does
ers (Sukhbaatar et al., 2019).                                    not increase monotonically throughout the layers as for the
                                                                  DeiT. The first layer and the final layers strongly escape
These observations are particularly clear when examining
                                                                  locality, whereas the intermediate layers (particularly the
the attention maps (Fig. 15 of the SM), and point to the
                                                                  second layer) stay more local.
beneficial effect of locality in lower layers. In Fig. 10 of
the SM., we also show that the nonlocality metric is lower        To gain more understanding, we examine the dynamics of
when training with distillation from a convolutional network      the gating parameters in Fig. 6. We see that in all layers,
as in Touvron et al. (2020), suggesting that the locality of      the average gating parameter Eh œÉ(Œªh ) (in black), which
the teacher is partly transferred to the student (Abnar et al.,   reflects the average amount of attention paid to positional
2020).                                                            information versus content, decreases throughout training.
                        ConViT: Improving Vision Transformers with Soft Convolutional Inductive Biases

                                                                                                            Head 1        Head 2     Head 3
This quantity reaches 0 in layers 6-10, meaning that posi-
tional information is practically ignored. However, in layers




                                                                                              Layer 2
1-5, some of the attention heads keep a high value of œÉ(Œªh ),
hence take advantage of positional information. Interest-
ingly, the ConViT-Ti only uses positional information up
to layer 4, whereas the ConViT-B uses it up to layer 6 (see




                                                                                              Layer 10
App. D), suggesting that larger models - which are more
under-specified - benefit more from the convolutional prior.
These observations highlight the usefulness of the gating
parameter in terms of interpretability.                                          (a) Input                            (b) DeiT
The inner workings of the ConViT are further revealed by                           Head 1                 Head 2       Head 3       Head 4
the attention maps of Fig. 7, which are obtained by prop-                        ( ) = 0.00              ( ) = 0.09   ( ) = 0.51   ( ) = 0.73
agating an embedded input image through the layers and




                                                                      Layer 2
selecting a query patch at the center of the image3 . In layer
10, (bottom row), the attention maps of DeiT and ConViT
look qualitatively similar: they both perform content-based
attention. In layer 2 however (top row), the attention maps                      ( ) = 0.00              ( ) = 0.00   ( ) = 0.00   ( ) = 0.00


                                                                      Layer 10
of the ConViT are more varied: some heads pay attention
to content (heads 1 and 2) whereas other focus mainly on
position (heads 3 and 4). Among the heads which focus on
position, some stay highly localized (head 4) whereas others                                                 (c) ConViT
broaden their attention span (head 3). The interested reader
can find more attention maps in SM. E.                                Figure 7. The ConViT learns more diverse attention maps.
                                                                      Left: input image which is embedded then fed into the models. The
              Train    Conv     Train      Use      Full    10%       query patch is highlighted by a red box and the colormap is loga-
   Ref.                                                               rithmic to better reveal details. Center: attention maps obtained
              gating    init    GPSA      GPSA      data    data
                                                                      by a DeiT-Ti after 300 epochs of training on ImageNet. Right:
 a (ConViT)     3        3        3         3       82.2    59.7      Same for ConViT-Ti. In each map, we indicated the value of the
     b          7        3        3         3       82.0    57.4
                                                                      gating parameter in a color varying from white (for heads paying
     c          3        7        3         3       81.4    56.9
     d          7        7        3         3       81.6    54.6      attention to content) to red (for heads paying attention to position).
  e (DeiT)      7        7        7         7       79.1    47.8      Attention maps for more images and heads are shown in SM. E.
     f          7        3        7         3       78.6    54.3
     g          7        7        7         3       73.7    44.8
                                                                      In Fig. 8(b), we show how performance at various stages of
Table 3. Gating and convolutional initialization play nicely to-      training is impacted by the presence of GPSA layers. We
gether. We ran an ablation study on the ConViT-S+ trained for 300     see that the boost due to GPSA is particularly strong during
epochs on the full ImageNet training set and on 10% of the train-     the early stages of training: after 20 epochs, using 9 GPSA
ing data. From the left column to right column, we experimented       layers leads the test-accuracy to almost double, suggesting
freezing the gating parameters to 0, removing the convolutional
                                                                      that the convolution initialization gives the model a substan-
initialization, freezing the GPSA layers and removing them alto-
                                                                      tial ‚Äúhead start‚Äù. This speedup is of practical interest in
gether.
                                                                      itself, on top of the boost in final performance.

Strong locality is desirable We next investigate how the
                                                                      Ablation study In Tab. 3, we present an ablation on the
performance of the ConViT is affected by two important hy-
                                                                      ConViT, denoted as [a]. We experiment removing the posi-
perparameters of the ConViT: the locality strength, Œ±, which
                                                                      tional gating [b]4 , the convolutional initialization [c], both
determines how focused the heads are around their center of
                                                                      gating and the convolutional initialization [d], and the GPSA
attention, and the number of SA layers replaced by GPSA
                                                                      altogether ([e], which leaves us with a plain DeiT).
layers. We examined the effects of these hyperparameters
on ConViT-S, trained on the first 100 classes of ImageNet.            Surprisingly, on full ImageNet, GPSA without gating [d]
As shown in Fig. 8(a), final test accuracy increases both with        already brings a substantial benefit over the DeiT (+2.5),
the locality strength and with the number of GPSA layers;             which is mildly increased by the convolutional initializa-
in other words, the more convolutional, the better.                   tion ([b], +2.9). As for gating, it helps a little in presence
   3                                                                      4
     We do not show the attention paid to the class token in the SA         To remove gating, we freeze all gating parameters to Œª = 0 so
layers                                                                that the same amount of attention is paid to content and position.
                                                      ConViT: Improving Vision Transformers with Soft Convolutional Inductive Biases

                                                 81                    80                                              need for greedy architectural search while offering higher
                                                                                                          20 epochs
 Number of GPSA layers
                         0
                                                 80                                                       75 epochs    interpretability.




                                                      Top-1 accuracy
                         3                                             60                                 130 epochs
                                                 79                                                       185 epochs
                                                 78                                                       240 epochs   Another direction which will be explored in future work
                         6                                                                                295 epochs
                                                                       40                                              is the following: if SA layers benefit from being initial-
                                                 77
                         9                                                                                             ized as random convolutions, could one reduce even more
                                                                       20
                             0.01 0.1 1                                     0        3       6        9                drastically their sample complexity by initializing them as
                             Locality strength                                  Number of GPSA layers
                                                                                                                       pre-trained convolutions?
Figure 8. The beneficial effect of locality. Left: As we increase
the locality strength (i.e. how focused each attention head is its                                                     Acknowledgements We thank HerveÃÅ JeÃÅgou and Fran-
associated patch) and the number of GPSA layers of a ConViT-S+,                                                        cisco Massa for helpful discussions. SD and GB acknowl-
the final top-1 accuracy increases significantly. Right: The benefi-                                                   edge funding from the French government under manage-
cial effect of locality is particularly strong in the early epochs.                                                    ment of Agence Nationale de la Recherche as part of the ‚ÄúIn-
                                                                                                                       vestissements d‚Äôavenir‚Äù program, reference ANR-19-P3IA-
                                                                                                                       0001 (PRAIRIE 3IA Institute).
of the convolutional initialization ([a], +3.1), and is un-
helpful otherwise. These mild improvements due to gating
and convolutional initialization (likely due to performance                                                            References
saturation above 80% top-1) become much clearer in the                                                                 Abnar, S., Dehghani, M., and Zuidema, W. Transferring
low data regime. Here, GPSA alone brings +6.8, with an                                                                   inductive biases through knowledge distillation. arXiv
extra +2.3 coming from gating, +2.8 from convolution ini-                                                                preprint arXiv:2006.00555, 2020.
tialization and +5.1 with the two together, illustrating their
complementarity.                                                                                                       Anandkumar, A., Deng, Y., Ge, R., and Mobahi, H.
                                                                                                                         Homotopy analysis for tensor pca. arXiv preprint
We also investigated the performance of the ConViT with
                                                                                                                         arXiv:1610.09322, 2016.
all GPSA layers frozen, leaving only the FFNs to be trained
in the first 10 layers. As one could expect, performance                                                               Bahdanau, D., Cho, K., and Bengio, Y. Neural machine
is strongly degraded in the full data regime if we initial-                                                              translation by jointly learning to align and translate. arXiv
ize the GPSA layers randomly ([f], -5.4 compared to the                                                                  preprint arXiv:1409.0473, 2014.
DeiT). However, the convolutional initialization remarkably
enables the frozen ConViT to reach a very decent perfor-                                                               Bello, I., Zoph, B., Vaswani, A., Shlens, J., and Le, Q. V.
mance, almost equalling that of the DeiT ([e], -0.5). In                                                                 Attention augmented convolutional networks. In Proceed-
other words, replacing SA layers by random ‚Äúconvolutions‚Äù                                                                ings of the IEEE International Conference on Computer
hardly impacts performance. In the low data regime, the                                                                  Vision, pp. 3286‚Äì3295, 2019.
frozen ConViT even outperforms the DeiT by a margin
(+6.5). This naturally begs the question: is attention really                                                          Carion, N., Massa, F., Synnaeve, G., Usunier, N., Kirillov,
key to the success of ViTs (Dong et al., 2021; Tolstikhin                                                                A., and Zagoruyko, S. End-to-end object detection with
et al., 2021; Touvron et al., 2021a)?                                                                                    transformers. arXiv preprint arXiv:2005.12872, 2020.

                                                                                                                       Chen, Y., Kalantidis, Y., Li, J., Yan, S., and Feng, J.
5. Conclusion and perspectives
                                                                                                                         A2-nets: Double attention networks. arXiv preprint
The present work investigates the importance of initializa-                                                              arXiv:1810.11579, 2018.
tion and inductive biases in learning with vision transform-
ers. By showing that one can take advantage of convolu-                                                                Chen, Y.-C., Li, L., Yu, L., El Kholy, A., Ahmed, F., Gan, Z.,
tional constraints in a soft way, we merge the benefits of                                                               Cheng, Y., and Liu, J. Uniter: Universal image-text repre-
architectural priors and expressive power. The result is a sim-                                                          sentation learning. In European Conference on Computer
ple recipe that improves trainability and sample efficiency,                                                             Vision, pp. 104‚Äì120. Springer, 2020.
without increasing model size or requiring any tuning.
                                                                                                                       Cordonnier, J.-B., Loukas, A., and Jaggi, M. On the rela-
Our approach can be summarized as follows: instead of                                                                    tionship between self-attention and convolutional layers.
interleaving convolutional layers with SA layers as done                                                                 arXiv preprint arXiv:1911.03584, 2019.
in hybrid models, let the layers decide whether to be con-
volutional or not by adjusting a set of gating parameters.                                                             d‚ÄôAscoli, S., Sagun, L., Biroli, G., and Bruna, J. Finding the
More generally, combining the biases of varied architec-                                                                 needle in the haystack with convolutions: on the benefits
tures and letting the model choose which ones are best for a                                                             of architectural bias. In Advances in Neural Information
given task could become a promising direction, reducing the                                                              Processing Systems, pp. 9334‚Äì9345, 2019.
                      ConViT: Improving Vision Transformers with Soft Convolutional Inductive Biases

Devlin, J., Chang, M.-W., Lee, K., and Toutanova, K. Bert:        Hu, J., Shen, L., and Sun, G. Squeeze-and-Excitation Net-
  Pre-training of deep bidirectional transformers for lan-         works. In 2018 IEEE/CVF Conference on Computer Vi-
  guage understanding. arXiv preprint arXiv:1810.04805,             sion and Pattern Recognition, pp. 7132‚Äì7141, June 2018c.
  2018.                                                             doi: 10.1109/CVPR.2018.00745. ISSN: 2575-7075.

Dong, Y., Cordonnier, J.-B., and Loukas, A. Attention is          Krizhevsky, A., Sutskever, I., and Hinton, G. E. Imagenet
  not all you need: Pure attention loses rank doubly expo-          classification with deep convolutional neural networks.
  nentially with depth. arXiv preprint arXiv:2103.03404,            Communications of the ACM, 60(6):84‚Äì90, 2017.
  2021.
                                                                  LeCun, Y., Boser, B., Denker, J. S., Henderson, D., Howard,
Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn,            R. E., Hubbard, W., and Jackel, L. D. Backpropaga-
  D., Zhai, X., Unterthiner, T., Dehghani, M., Minderer, M.,        tion applied to handwritten zip code recognition. Neural
  Heigold, G., Gelly, S., et al. An image is worth 16x16            computation, 1(4):541‚Äì551, 1989.
 words: Transformers for image recognition at scale. arXiv
  preprint arXiv:2010.11929, 2020.                                LeCun, Y., Bottou, L., Bengio, Y., and Haffner, P. Gradient-
                                                                    based learning applied to document recognition. Proceed-
Elsayed, G., Ramachandran, P., Shlens, J., and Kornblith, S.        ings of the IEEE, 86(11):2278‚Äì2324, 1998.
  Revisiting spatial invariance with low-rank local connec-
                                                                  Locatello, F., Weissenborn, D., Unterthiner, T., Mahendran,
  tivity. In International Conference on Machine Learning,
                                                                    A., Heigold, G., Uszkoreit, J., Dosovitskiy, A., and Kipf,
  pp. 2868‚Äì2879. PMLR, 2020.
                                                                    T. Object-centric learning with slot attention. arXiv
Gers, F. A., Schmidhuber, J., and Cummins, F. Learning to           preprint arXiv:2006.15055, 2020.
  forget: Continual prediction with lstm. 1999.
                                                                  Mitchell, T. M. The need for biases in learning generaliza-
Goodfellow, I., Bengio, Y., and Courville, A. Deep Learning.       tions. Department of Computer Science, Laboratory for
  MIT Press, 2016.                                                 Computer Science Research . . . , 1980.

                                                                  Neyshabur, B. Towards learning convolutions from scratch.
Greff, K., Srivastava, R. K., Koutnƒ±ÃÅk, J., Steunebrink, B. R.,
                                                                    Advances in Neural Information Processing Systems, 33,
  and Schmidhuber, J. LSTM: A Search Space Odyssey.
                                                                    2020.
  IEEE Transactions on Neural Networks and Learning Sys-
  tems, 28(10):2222‚Äì2232, October 2017. ISSN 2162-2388.           Radosavovic, I., Kosaraju, R. P., Girshick, R., He, K., and
  doi: 10.1109/TNNLS.2016.2582924. Conference Name:                 DollaÃÅr, P. Designing network design spaces. In Proceed-
  IEEE Transactions on Neural Networks and Learning                 ings of the IEEE/CVF Conference on Computer Vision
  Systems.                                                          and Pattern Recognition, pp. 10428‚Äì10436, 2020.
He, K., Zhang, X., Ren, S., and Sun, J. Deep residual learn-      Ramachandran, P., Parmar, N., Vaswani, A., Bello, I., Lev-
  ing for image recognition. In Proceedings of the IEEE             skaya, A., and Shlens, J. Stand-alone self-attention in
  conference on computer vision and pattern recognition,            vision models. arXiv preprint arXiv:1906.05909, 2019.
  pp. 770‚Äì778, 2016.
                                                                  Ruderman, D. L. and Bialek, W. Statistics of natural images:
Hinton, G., Vinyals, O., and Dean, J.     Distilling                Scaling in the woods. Physical review letters, 73(6):814,
  the knowledge in a neural network. arXiv preprint                 1994.
  arXiv:1503.02531, 2015.
                                                                  Scherer, D., MuÃàller, A., and Behnke, S. Evaluation of
Hu, H., Gu, J., Zhang, Z., Dai, J., and Wei, Y. Relation            Pooling Operations in Convolutional Architectures for
  networks for object detection. In Proceedings of the IEEE         Object Recognition. In Diamantaras, K., Duch, W., and
 Conference on Computer Vision and Pattern Recognition,             Iliadis, L. S. (eds.), Artificial Neural Networks ‚Äì ICANN
  pp. 3588‚Äì3597, 2018a.                                             2010, Lecture Notes in Computer Science, pp. 92‚Äì101,
                                                                    Berlin, Heidelberg, 2010. Springer. ISBN 978-3-642-
Hu, J., Shen, L., Albanie, S., Sun, G., and Vedaldi, A.             15825-4. doi: 10.1007/978-3-642-15825-4 10.
  Gather-Excite: Exploiting Feature Context in Convo-
  lutional Neural Networks. In Bengio, S., Wallach, H.,           Schmidhuber, J. Deep learning in neural networks:
  Larochelle, H., Grauman, K., Cesa-Bianchi, N., and Gar-           An overview. Neural Networks, 61:85‚Äì117, January
  nett, R. (eds.), Advances in Neural Information Process-          2015. ISSN 0893-6080. doi: 10.1016/j.neunet.2014.09.
  ing Systems 31, pp. 9401‚Äì9411. Curran Associates, Inc.,           003. URL http://www.sciencedirect.com/
  2018b.                                                            science/article/pii/S0893608014002135.
                      ConViT: Improving Vision Transformers with Soft Convolutional Inductive Biases

Simoncelli, E. P. and Olshausen, B. A. Natural image statis-      00813. URL https://ieeexplore.ieee.org/
  tics and neural representation. Annual review of neuro-         document/8578911/.
  science, 24(1):1193‚Äì1216, 2001.
                                                                Wu, B., Xu, C., Dai, X., Wan, A., Zhang, P., Tomizuka, M.,
Srinivas, A., Lin, T.-Y., Parmar, N., Shlens, J., Abbeel, P.,    Keutzer, K., and Vajda, P. Visual Transformers: Token-
  and Vaswani, A. Bottleneck Transformers for Visual             based Image Representation and Processing for Computer
  Recognition. arXiv e-prints, art. arXiv:2101.11605, Jan-       Vision. arXiv:2006.03677 [cs, eess], July 2020. URL
  uary 2021.                                                     http://arxiv.org/abs/2006.03677. arXiv:
                                                                 2006.03677.
Sukhbaatar, S., Grave, E., Bojanowski, P., and Joulin, A.
  Adaptive attention span in transformers. arXiv preprint       Yuan, L., Chen, Y., Wang, T., Yu, W., Shi, Y., Tay, F. E.,
  arXiv:1905.07799, 2019.                                         Feng, J., and Yan, S. Tokens-to-token vit: Training vision
                                                                  transformers from scratch on imagenet. arXiv preprint
Sun, C., Myers, A., Vondrick, C., Murphy, K., and Schmid,         arXiv:2101.11986, 2021.
  C. Videobert: A joint model for video and language
  representation learning. In Proceedings of the IEEE Inter-    Zhai, X., Oliver, A., Kolesnikov, A., and Beyer, L. S4l: Self-
  national Conference on Computer Vision, pp. 7464‚Äì7473,          supervised semi-supervised learning. In Proceedings of
  2019.                                                           the IEEE/CVF International Conference on Computer
                                                                  Vision, pp. 1476‚Äì1485, 2019.
Sundermeyer, M., SchluÃàter, R., and Ney, H. LSTM neural
  networks for language modeling. In Thirteenth annual          Zhao, S., Zhou, L., Wang, W., Cai, D., Lam, T. L., and
  conference of the international speech communication            Xu, Y. Splitnet: Divide and co-training. arXiv preprint
  association, 2012.                                              arXiv:2011.14660, 2020.

Tan, M. and Le, Q. Efficientnet: Rethinking model scaling
  for convolutional neural networks. In International Con-
  ference on Machine Learning, pp. 6105‚Äì6114. PMLR,
  2019.

Tolstikhin, I., Houlsby, N., Kolesnikov, A., Beyer, L., Zhai,
  X., Unterthiner, T., Yung, J., Keysers, D., Uszkoreit, J.,
  Lucic, M., et al. Mlp-mixer: An all-mlp architecture for
  vision. arXiv preprint arXiv:2105.01601, 2021.

Touvron, H., Cord, M., Douze, M., Massa, F., Sablayrolles,
  A., and JeÃÅgou, H. Training data-efficient image trans-
  formers & distillation through attention. arXiv preprint
  arXiv:2012.12877, 2020.

Touvron, H., Bojanowski, P., Caron, M., Cord, M., El-
  Nouby, A., Grave, E., Joulin, A., Synnaeve, G., Verbeek,
  J., and JeÃÅgou, H. Resmlp: Feedforward networks for
  image classification with data-efficient training. arXiv
  preprint arXiv:2105.03404, 2021a.

Touvron, H., Cord, M., Sablayrolles, A., Synnaeve, G., and
  JeÃÅgou, H. Going deeper with image transformers, 2021b.

Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones,
  L., Gomez, A. N., Kaiser, ≈Å., and Polosukhin, I. Atten-
  tion is all you need. In Advances in neural information
  processing systems, pp. 5998‚Äì6008, 2017.

Wang, X., Girshick, R., Gupta, A., and He, K. Non-
 local Neural Networks. In 2018 IEEE/CVF Confer-
 ence on Computer Vision and Pattern Recognition, pp.
 7794‚Äì7803, Salt Lake City, UT, USA, June 2018. IEEE.
 ISBN 978-1-5386-6420-9. doi: 10.1109/CVPR.2018.
                       ConViT: Improving Vision Transformers with Soft Convolutional Inductive Biases

A. The importance of positional gating
In the main text, we discussed the importance of using GPSA layers instead of the standard PSA layers, where content
and positional information are summed before the softmax and lead the attention heads to focus only on the positional
information. We give evidence for this claim in Fig. 9, where we train a ConViT-B for 300 epochs on ImageNet, but
replace the GPSA by standard PSA. The convolutional initialization of the PSA still gives the ConViT a large advantage
over the DeiT baseline early in training. However, the ConViT stays in the convolutional configuration and ignores the
content information, as can be seen by looking at the attention maps (not shown). Later in training, the DeiT catches up and
surpasses the performance of the ConViT by utilizing content information.


                                                         80

                                                         60
                                        Top-1 accuracy


                                                         40

                                                         20
                                                                              ConViT - vanilla PSA
                                                                              DeiT
                                                          0
                                                              0    50    100 150 200 250 300
                                                                            Epochs

Figure 9. Convolutional initialization without GPSA is helfpul during early training but deteriorates final performance. We
trained the ConViT-B along with its DeiT-B counterpart for 300 epochs on ImageNet, replacing the GPSA layers of the ConViT-B by
vanilla PSA layers.


B. The effect of distillation
Nonlocality In Fig. 10, we compare the nonlocality curves of Fig. 5 of the main text with those obtained when the DeiT is
trained via hard distillation from a RegNetY-16GF (84M parameters) (Radosavovic et al., 2020), as in Touvron et al. (2020).
In the distillation setup, the nonlocality still drops in the early epochs of training, but increases less at late times compared to
without distillation. Hence, the final internal states of the DeiT are more ‚Äúlocal‚Äù due to the distillation. This suggests that
knowledge distillation transfers the locality of the convolutional teacher to the student, in line with the results of (Abnar
et al., 2020).

Performance The hard distillation introduced in Touvron et al. (2020) greatly improves the performance of the DeiT. We
have verified the complementarity of their distillation methods with our ConViT. In the same way as in the DeiT paper, we
used a RegNet-16GF teacher and experimented hard distillation during 300 epochs on ImageNet. The results we obtain are
summarized in Tab. 4.

                               Method                         DeiT-S (22M)   DeiT-B (86M)    ConViT-S+ (48M)
                           No distillation                        79.8            81.8               82.2
                          Hard distillation                       80.9            83.0               82.9

Table 4. Top-1 accuracies of the ConViT-S+ compared to the DeiT-S and DeiT-B, both trained for 300 epochs on ImageNet.

Just like the DeiT, the ConViT benefits from distillation, albeit somewhat less than the DeiT, as can be seen from the DeiT-B
performing less well than the ConViT-S+ without distillation but better with distillation. This hints to the fact that the
convolutional inductive bias transferred from the teacher is redundant with its own convolutional prior.
Nevertheless, the performance improvement obtained by the ConViT with hard distillation demonstrates that instantiating
soft inductive biases directly in a model can yield benefits on top of those obtained by instantiating such biases indirectly, in
                                                 ConViT: Improving Vision Transformers with Soft Convolutional Inductive Biases


                                              7.50                                                                                   7.50
                                                                                                                                                                                                                                         Layer 1
                                              7.25                                                                                   7.25                                                                                                Layer 2
                                              7.00                                                                                   7.00                                                                                                Layer 3
                                                                                                                                                                                                                                         Layer 4
                               Non-locality
                                              6.75                                                                                   6.75                                                                                                Layer 5
                                                                                                                                                                                                                                         Layer 6
                                              6.50                                                                                   6.50                                                                                                Layer 7
                                              6.25                                                                                   6.25                                                                                                Layer 8
                                                                                                                                                                                                                                         Layer 9
                                              6.00                                                                                   6.00                                                                                                Layer 10
                                                                                                                                                                                                                                         Layer 11
                                              5.75                                                                                   5.75                                                                                                Layer 12
                                                     0                                100 200                                 300            0                                   100 200                              300
                                                                                       Epochs                                                                                      Epochs

Figure 10. Distillation pulls the DeiT towards a more local configuration. We plotted the nonlocality metric defined in Eq. 8 throughout
training, for the DeiT-S trained on ImageNet. Left: regular training. Right: training with hard distillation from a RegNet teacher, by means
of the distillation introduced in (Touvron et al., 2020).


this case via distillation.

C. Further performance results
In Fig. 11, we display the time evolution of the top-1 accuracy of our ConViT+ models on CIFAR100, ImageNet and
subsampled ImageNet, along with a comparison with the corresponding DeiT+ models.
For CIFAR100, we kept all hyperparameters unchanged, but rescaled the images to 224 √ó 224 and increased the number
of epochs (adapting the learning rate schedule correspondingly) to mimic the ImageNet scenario. After 1000 epochs, the
ConViTs shows clear signs of overfitting, but reach impressive performances (82.1% top-1 accuracy with 10M parameters,
which is better than the EfficientNets reported in (Zhao et al., 2020)).

                   85                                                           12                                   85
                                                                                                                                                                                 6                                   80                                                         35
                                                     Relative improvement (%)




                                                                                                                                                      Relative improvement (%)




                   80                                                                                                80
                                                                                                                                                                                                                                                     Relative improvement (%)

                                                                                10                                                                                                                                   70
                                                                                                                                                                                 5                                                                                              30
  Top-1 accuracy




                                                                                                    Top-1 accuracy




                                                                                 8
                                                                                                                                                                                                    Top-1 accuracy




                   75                                                                                                75                                                          4                                   60                                                         25
                                                                                                                                                                                                                                        f = 5%
                                     base+                                       6                                                       base+                                   3                                                      f = 10%                                 20
                   70                small+                                                                          70                  small+                                                                      50                 f = 30%
                                     tiny+                                       4                                                       tiny+                                   2                                                                                              15
                                                                                                                                                                                                                     40                 f = 50%
                   65                Baseline                                    2                                   65                  Baseline                                                                                       f = 100%                                10
                                                                                                                                                                                 1
                                     ConViT                                                                                              ConViT                                                                      30                 DeiT-S
                   60                                                            0                                   60                                                          0                                                      ConViT-S                                 5
                        0   1000 2000 3000                                              e+ ll+ y+                         0     100    200      300                                     e+ ll+ y+
                              Epochs                                                 bas sma tin                                  Epochs                                             bas sma tin                     20
                                                                                                                                                                                                                          0   100     200      300
                                                                                                                                                                                                                                                                                 0
                                                                                                                                                                                                                                                                                     0.050.1 0.3 0.5 1
                                                                                         Model                                                                                           Model                                 Epochs * f                                               Subsampling f

                                (a) CIFAR100                                                                                        (b) ImageNet-1k                                                                           (c) Subsampled ImageNet

Figure 11. The convolutional inductive bias is particularly useful for large models applied to small datasets. Each of the three
panels displays the top-1 accuracy of the ConViT+ model and their corresponding DeiT+ throughout training, as well as the relative
improvement between the best top-1 accuracy reached by the DeiT+ and that reached by the ConViT+. Left: tiny, small and base models
trained for 3000 epochs on CIFAR100. Middle: tiny, small and base models trained for 300 epochs on ImageNet-1k. The relative
improvement of the ConViT over the DeiT increases with model size. Right: small model trained on a subsampled version of ImageNet-1k,
where we only keep a fraction f ‚àà {0.05, 0.1, 0.3, 0.5, 1} of the images of each class. The relative improvement of the ConViT over the
DeiT increases as the dataset becomes smaller.

In Fig. 12, we study the impact of the various ingredients of the ConViT (presence and number of GPSA layers, gating
parameters, convolutional initialization) on the dynamics of learning.
                              ConViT: Improving Vision Transformers with Soft Convolutional Inductive Biases



                   80                                                 80                                     From best to worst
                                                                                                                GPSA+conv 82.2
  Top-1 accuracy




                                                     Top-1 accuracy
                   70                                                 75                                        PSA+conv 82.0
                                   0 GPSA layers                                                                PSA 81.6
                   60                                                 70                                        GPSA 81.4
                                   3 GPSA layers                                                                Baseline 79.1
                   50              6 GPSA layers                      65                                        Frozen conv 78.6
                                   9 GPSA layers                                                                Frozen 73.7
                   40                                                 60
                        0   100      200      300                          100         200       300
                               Epochs                                            Epochs

Figure 12. Impact of various ingredients of the ConViT on the dynamics of learning. In both cases, we train the ConViT-S+ for 300
epochs on first 100 classes of ImageNet. Left: ablation on number of GPSA layers, as in Fig. 8. Right: ablation on various ingredients
of the ConViT, as in Tab. 3. The baseline is the DeiT-S+ (pink). We experimented (i) replacing the 10 first SA layers by GPSA layers
(‚ÄúGPSA‚Äù) (ii) freezing the gating parameter of the GPSA layers (‚Äúfrozen gate‚Äù); (iii) removing the convolutional initialization (‚Äúconv‚Äù);
(iv) freezing all attention modules in the GPSA layers (‚Äúfrozen‚Äù). The final top-1 accuracy of the various models trained is reported in the
legend.


D. Effect of model size
In Fig. 13, we show the analog of Fig. 5 of the main text for the tiny and base models. Results are qualitatively similar
to those observed for the small model. Interestingly, the first layers of DeiT-B and ConViT-B reach significantly higher
nonlocality than those of the DeiT-Ti and ConViT-Ti.
In Fig. 14, we show the analog of Fig. 6 of the main text for the tiny and base models. Again, results are qualitatively
similar: the average weight of the positional attention, Eh œÉ(Œªh ), decreases over time, so that more attention goes to the
content of the image. Note that in the ConViT-Ti, only the first 4 layers still pay attention to position at the end of training
(average gating parameter smaller than one), whereas for ConViT-S, the 5 first layers still do, and for the ConViT-B, the 6
first layers still do. This suggests that the larger (i.e. the more underspecified) the model is, the more layers make use of the
convolutional prior.
                            ConViT: Improving Vision Transformers with Soft Convolutional Inductive Biases




                                             DeiT                      ConViT
                                                                                                     Layer 1
                                    7                       7                                        Layer 2
                                    6                       6                                        Layer 3
                     Non-locality




                                                                                                     Layer 4
                                    5                       5                                        Layer 5
                                                                                                     Layer 6
                                    4                       4                                        Layer 7
                                                                                                     Layer 8
                                    3                       3                                        Layer 9
                                        0   100 200   300       0     100 200          300           Layer 10
                                             Epochs                    Epochs                        Layer 11
                                                                                                     Layer 12
                                                      (a) DeiT-Ti and ConViT-Ti

                                             DeiT                      ConViT
                                                                                                     Layer 1
                                    7                       7                                        Layer 2
                                    6                       6                                        Layer 3
                     Non-locality




                                                                                                     Layer 4
                                    5                       5                                        Layer 5
                                                                                                     Layer 6
                                    4                       4                                        Layer 7
                                                                                                     Layer 8
                                    3                       3                                        Layer 9
                                        0   100 200   300       0     100 200          300           Layer 10
                                             Epochs                    Epochs                        Layer 11
                                                                                                     Layer 12
                                                       (b) DeiT-B and ConViT-B

Figure 13. The bigger the model, the more non-local the attention. We plotted the nonlocality metric defined in Eq. 8 of the main text
(the higher, the further the attention heads look from the query pixel) throughout 300 epochs of training on ImageNet-1k.
                              ConViT: Improving Vision Transformers with Soft Convolutional Inductive Biases




                                       1.0 Layer 1               Layer 2             Layer 3             Layer 4             Layer 5
                     Positional attn



                                       0.5             0.5                 0.5                 0.5                 0.5

                                       0.0             0.0                 0.0                 0.0                 0.0
                                             0   250         0       250         0       250         0       250         0       250
                                       1.0 Layer 6               Layer 7             Layer 8             Layer 9         Layer 10
                     Positional attn




                                                       0.5                 0.5                 0.5                 0.5
                                       0.5

                                       0.0             0.0                 0.0                 0.0                 0.0
                                             0 250           0 250               0 250               0 250               0 250
                                              Epochs          Epochs              Epochs              Epochs              Epochs
                                                                           (a) ConViT-Ti

                                       1.0 Layer 1     1.0
                                                                 Layer 2
                                                                           1.0
                                                                                     Layer 3
                                                                                               1.0
                                                                                                         Layer 4
                                                                                                                   1.0
                                                                                                                             Layer 5
                     Positional attn




                                       0.5             0.5                 0.5                 0.5                 0.5

                                       0.0             0.0                 0.0                 0.0                 0.0
                                             0   250         0       250         0       250         0       250         0       250
                                       1.0 Layer 6     1.0
                                                                 Layer 7
                                                                           1.0
                                                                                     Layer 8
                                                                                               1.0
                                                                                                         Layer 9
                                                                                                                   1.0 Layer 10
                     Positional attn




                                       0.5             0.5                 0.5                 0.5                 0.5

                                       0.0             0.0                 0.0                 0.0                 0.0
                                             0 250           0 250               0 250               0 250               0 250
                                              Epochs          Epochs              Epochs              Epochs              Epochs
                                                                           (b) ConViT-B

Figure 14. The bigger the model, the more layers pay attention to position. We plotted the gating parameters of various heads and
various layers, as in Fig. 6 of the main text (the lower, the less attention is paid to positional information) throughout 300 epochs of
training on ImageNet-1k. Note that the ConViT-Ti only has 4 attention heads whereas the ConViT-B has 16, hence the different number of
curves.
                       ConViT: Improving Vision Transformers with Soft Convolutional Inductive Biases

E. Attention maps
Attention maps of the DeiT reveal locality In Fig. 15, we give some visual evidence for the fact that vanilla SA layers
extract local information by averaging the attention map of the first and tenth layer of the DeiT over 100 images. Before
training, the maps look essentially random. After training, however, most of the attention heads of the first layer focus on the
query pixel and its immediate surroundings, whereas the attention heads of the tenth layer capture long-range dependencies.

           Head 1              Head 2               Head 3                   Head 1               Head 2               Head 3
Layer 1




                                                                  Layer 1
Layer 10




                                                                  Layer 10
                        (a) Before training                                                 (b) After training

Figure 15. The averaged attention maps of the DeiT reveal locality at the end of training. To better visualise the center of attention,
we averaged the attention maps over 100 images. Top: before training, the attention patterns exhibit a random structure. Bottom: after
training, most of the attention is devoted to the query pixel, and the rest is focused on its immediate surroundings.


Attention maps of the ConViT reveal the diversity of the attention heads In Fig. 16, we show a comparison of the
attention maps of Deit-Ti and ConViT-Ti for different images of the ImageNet validation set. In Fig. 17, we compare the
attention maps of DeiT-S and ConViT-S.
In all cases, results are qualitatively similar: the DeiT attention maps look similar across different heads and different layers,
whereas those of the ConViT perform very different operations. Notice that in the second layer, the third and forth head
focus stay local whereas the first two heads focus on content. In the last layer, all the heads ignore positional information,
focusing only on content.
                        ConViT: Improving Vision Transformers with Soft Convolutional Inductive Biases




                                             Head 1     Head 2    Head 3                    Head 1       Head 2       Head 3       Head 4
                                                                                           ( ) = 0.00   ( ) = 0.09   ( ) = 0.51   ( ) = 0.73




                                                                                Layer 2
                                  Layer 2




                                                                                           ( ) = 0.00   ( ) = 0.00   ( ) = 0.00   ( ) = 0.00




                                                                                Layer 10
                                  Layer 10




                                             Head 1     Head 2    Head 3                    Head 1       Head 2       Head 3       Head 4
                                                                                           ( ) = 0.00   ( ) = 0.09   ( ) = 0.51   ( ) = 0.73

                                                                                Layer 2
                                  Layer 2




                                                                                           ( ) = 0.00   ( ) = 0.00   ( ) = 0.00   ( ) = 0.00
                                                                                Layer 10
                                  Layer 10




                                             Head 1     Head 2    Head 3                    Head 1       Head 2       Head 3       Head 4
                                                                                           ( ) = 0.00   ( ) = 0.09   ( ) = 0.51   ( ) = 0.73
                                                                                Layer 2
                                  Layer 2




                                                                                           ( ) = 0.00   ( ) = 0.00   ( ) = 0.00   ( ) = 0.00
                                                                                Layer 10
                                  Layer 10




        (a) Input images                              (b) DeiT                                              (c) ConViT

Figure 16. Left: input image which is embedded then fed into the models. The query patch is highlighted by a red box and the colormap is
logarithmic to better reveal details. Center: attention maps obtained by a DeiT-Ti after 300 epochs of training on ImageNet. Right: Same
for ConViT-Ti. In each map, we indicated the value of the gating parameter in a color varying from white (for heads paying attention to
content) to red (for heads paying attention to position).
                                 ConViT: Improving Vision Transformers with Soft Convolutional Inductive Biases




                                                       Head 1            Head 2           Head 3          Head 4         Head 5         Head 6




                                            Layer 1
                                            Layer 4
                                            Layer 7
                                            Layer 10




                     (a) Input image                                                          (b) DeiT
            Head 1             Head 2      Head 3               Head 4        Head 5          Head 6          Head 7        Head 8         Head 9
           ( ) = 0.01       ( ) = 0.52    ( ) = 0.01       ( ) = 0.00        ( ) = 0.04      ( ) = 0.92     ( ) = 0.07     ( ) = 0.01    ( ) = 0.07
Layer 1




           ( ) = 0.00       ( ) = 0.82    ( ) = 0.03       ( ) = 0.82        ( ) = 0.14      ( ) = 0.84     ( ) = 0.75     ( ) = 0.00    ( ) = 0.34
Layer 4




           ( ) = 0.15       ( ) = 0.00    ( ) = 0.59       ( ) = 0.00        ( ) = 0.04      ( ) = 0.02     ( ) = 0.04     ( ) = 0.00    ( ) = 0.00
Layer 7




           ( ) = 0.00       ( ) = 0.00    ( ) = 0.00       ( ) = 0.00        ( ) = 0.00      ( ) = 0.00     ( ) = 0.00     ( ) = 0.00    ( ) = 0.00
Layer 10




                                                                          (c) ConViT

 Figure 17. Attention maps obtained by a DeiT-S and ConViT-S after 300 epochs of training on ImageNet. In each map, we indicated the
 value of the gating parameter in a color varying from white (for heads paying attention to content) to red (for heads paying attention to
 position).
                      ConViT: Improving Vision Transformers with Soft Convolutional Inductive Biases

F. Further ablations
In this section, we explore masking off various parts of the network to understand which are most crucial.
In Tab. 5, we explore the importance of the absolute positional embeddings injected to the input in both the DeiT and
ConViT. We see that masking them off at test time a mild impact on accuracy for the ConViT, but a significant impact for
the DeiT, which is expected as the ConViT already has relative positional information in each of the GPSA layers. This also
shows that the absolute positional information contained in the embeddings is not very useful.
In Tab. 6, we explore the relative importance of the positional and content information by masking them off at test time. To
do so, we manually set the gating parameter œÉ(Œª) to 1 (no content attention) or 0 (no positional attention). In the first GPSA
layers, both procedures affect performance similarly, signalling that positional and content information are both useful.
However in the last GPSA layers, masking the content information kills performance, whereas masking the positional
information does not, confirming that content information is more crucial.

                                               Model      Mask pos embed        No mask
                                               DeiT-Ti            38.3              72.2
                                              ConViT-Ti           67.1              73.1

                    Table 5. Performance on ImageNet with the positional embeddings masked off at test time.


                                  # layers masked      Mask content      Mask position     No mask
                                          3                62.3              63.5           73.1
                                          5                35.0              53.1           73.1
                                         10                 1.3              46.8           73.1

            Table 6. Performance of ConViT-Ti on ImageNet with positional or content attention masked off at test time.
