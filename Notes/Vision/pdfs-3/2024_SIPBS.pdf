<!DOCTYPE html>
<html lang="en" class="no-js">
    <head>
        <meta charset="UTF-8">
        <meta http-equiv="X-UA-Compatible" content="IE=edge">
        <meta name="applicable-device" content="pc,mobile">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        
        
        
            <meta name="robots" content="max-image-preview:large">
            <meta name="access" content="Yes">

        
        <meta name="360-site-verification" content="1268d79b5e96aecf3ff2a7dac04ad990" />

        <title>Simultaneous instance pooling and bag representation selection approach for multiple-instance learning (MIL) using vision transformer | Neural Computing and Applications</title>

        
            
    
    <meta name="twitter:site" content="@SpringerLink"/>
    <meta name="twitter:card" content="summary_large_image"/>
    <meta name="twitter:image:alt" content="Content cover image"/>
    <meta name="twitter:title" content="Simultaneous instance pooling and bag representation selection approach for multiple-instance learning (MIL) using vision transformer"/>
    <meta name="twitter:description" content="Neural Computing and Applications - In multiple-instance learning (MIL), the existing bag encoding and attention-based pooling approaches assume that the instances in the bag have no relationship..."/>
    <meta name="twitter:image" content="https://static-content.springer.com/image/art%3A10.1007%2Fs00521-024-09417-3/MediaObjects/521_2024_9417_Fig1_HTML.png"/>
    <meta name="journal_id" content="521"/>
    <meta name="dc.title" content="Simultaneous instance pooling and bag representation selection approach for multiple-instance learning (MIL) using vision transformer"/>
    <meta name="dc.source" content="Neural Computing and Applications 2024 36:12"/>
    <meta name="dc.format" content="text/html"/>
    <meta name="dc.publisher" content="Springer"/>
    <meta name="dc.date" content="2024-02-16"/>
    <meta name="dc.type" content="OriginalPaper"/>
    <meta name="dc.language" content="En"/>
    <meta name="dc.copyright" content="2024 The Author(s)"/>
    <meta name="dc.rights" content="2024 The Author(s)"/>
    <meta name="dc.rightsAgent" content="journalpermissions@springernature.com"/>
    <meta name="dc.description" content="In multiple-instance learning (MIL), the existing bag encoding and attention-based pooling approaches assume that the instances in the bag have no relationship among them. This assumption is unsuited, as the instances in the bags are rarely independent in diverse MIL applications. In contrast, the instance relationship assumption-based techniques incorporate the instance relationship information in the classification process. However, in MIL, the bag composition process is complicated, and it may be possible that instances in one bag are related and instances in another bag are not. In present MIL algorithms, this relationship assumption is not explicitly modeled. The learning algorithm is trained based on one of two relationship assumptions (whether instances in all bags have a relationship or not). Hence, it is essential to model the assumption of instance relationships in the bag classification process. This paper proposes a robust approach that generates vector representation for the bag for both assumptions and the representation selection process to determine whether to consider the instances related or unrelated in the bag classification process. This process helps to determine the essential bag representation vector for every individual bag. The proposed method utilizes attention pooling and vision transformer approaches to generate bag representation vectors. Later, the representation selection subnetwork determines the vector representation essential for bag classification in an end-to-end trainable manner. The generalization abilities of the proposed framework are demonstrated through extensive experiments on several benchmark datasets. The experiments demonstrate that the proposed approach outperforms other state-of-the-art MIL approaches in bag classification."/>
    <meta name="prism.issn" content="1433-3058"/>
    <meta name="prism.publicationName" content="Neural Computing and Applications"/>
    <meta name="prism.publicationDate" content="2024-02-16"/>
    <meta name="prism.volume" content="36"/>
    <meta name="prism.number" content="12"/>
    <meta name="prism.section" content="OriginalPaper"/>
    <meta name="prism.startingPage" content="6659"/>
    <meta name="prism.endingPage" content="6680"/>
    <meta name="prism.copyright" content="2024 The Author(s)"/>
    <meta name="prism.rightsAgent" content="journalpermissions@springernature.com"/>
    <meta name="prism.url" content="https://link.springer.com/article/10.1007/s00521-024-09417-3"/>
    <meta name="prism.doi" content="doi:10.1007/s00521-024-09417-3"/>
    <meta name="citation_pdf_url" content="https://link.springer.com/content/pdf/10.1007/s00521-024-09417-3.pdf"/>
    <meta name="citation_fulltext_html_url" content="https://link.springer.com/article/10.1007/s00521-024-09417-3"/>
    <meta name="citation_journal_title" content="Neural Computing and Applications"/>
    <meta name="citation_journal_abbrev" content="Neural Comput &amp; Applic"/>
    <meta name="citation_publisher" content="Springer London"/>
    <meta name="citation_issn" content="1433-3058"/>
    <meta name="citation_title" content="Simultaneous instance pooling and bag representation selection approach for multiple-instance learning (MIL) using vision transformer"/>
    <meta name="citation_volume" content="36"/>
    <meta name="citation_issue" content="12"/>
    <meta name="citation_publication_date" content="2024/04"/>
    <meta name="citation_online_date" content="2024/02/16"/>
    <meta name="citation_firstpage" content="6659"/>
    <meta name="citation_lastpage" content="6680"/>
    <meta name="citation_article_type" content="Original Article"/>
    <meta name="citation_fulltext_world_readable" content=""/>
    <meta name="citation_language" content="en"/>
    <meta name="dc.identifier" content="doi:10.1007/s00521-024-09417-3"/>
    <meta name="DOI" content="10.1007/s00521-024-09417-3"/>
    <meta name="size" content="445680"/>
    <meta name="citation_doi" content="10.1007/s00521-024-09417-3"/>
    <meta name="citation_springer_api_url" content="http://api.springer.com/xmldata/jats?q=doi:10.1007/s00521-024-09417-3&amp;api_key="/>
    <meta name="description" content="In multiple-instance learning (MIL), the existing bag encoding and attention-based pooling approaches assume that the instances in the bag have no relation"/>
    <meta name="dc.creator" content="Waqas, Muhammad"/>
    <meta name="dc.creator" content="Tahir, Muhammad Atif"/>
    <meta name="dc.creator" content="Author, Muhammad Danish"/>
    <meta name="dc.creator" content="Al-Maadeed, Sumaya"/>
    <meta name="dc.creator" content="Bouridane, Ahmed"/>
    <meta name="dc.creator" content="Wu, Jia"/>
    <meta name="dc.subject" content="Artificial Intelligence"/>
    <meta name="dc.subject" content="Data Mining and Knowledge Discovery"/>
    <meta name="dc.subject" content="Probability and Statistics in Computer Science"/>
    <meta name="dc.subject" content="Computational Science and Engineering"/>
    <meta name="dc.subject" content="Image Processing and Computer Vision"/>
    <meta name="dc.subject" content="Computational Biology/Bioinformatics"/>
    <meta name="citation_reference" content="citation_journal_title=Natl Sci Rev; citation_title=A brief introduction to weakly supervised learning; citation_author=Z-H Zhou; citation_volume=5; citation_issue=1; citation_publication_date=2018; citation_pages=44-53; citation_doi=10.1093/nsr/nwx106; citation_id=CR1"/>
    <meta name="citation_reference" content="citation_journal_title=Knowl-Based Syst; citation_title=Explainable multi-instance and multi-task learning for COVID-19 diagnosis and lesion segmentation in CT images; citation_author=M Li, X Li, Y Jiang, J Zhang, H Luo, S Yin; citation_volume=252; citation_publication_date=2022; citation_pages=109278; citation_doi=10.1016/j.knosys.2022.109278; citation_id=CR2"/>
    <meta name="citation_reference" content="citation_journal_title=IEEE Trans. Pattern Anal. Mach. Intell.; citation_title=Leveraging instance-, image-and dataset-level information for weakly supervised instance segmentation; citation_author=Y Liu, YH Wu, P Wen, Y Shi, Y Qiu, MM Cheng; citation_volume=44; citation_issue=3; citation_publication_date=2020; citation_pages=1415-1428; citation_doi=10.1109/TPAMI.2020.3023152; citation_id=CR3"/>
    <meta name="citation_reference" content="citation_journal_title=Neural Comput Appl; citation_title=Multi-instance discriminative contrastive learning for brain image representation; citation_author=Y Zhang, S Liu, X Qu, X Shang; citation_publication_date=2022; citation_doi=10.1007/s00521-022-07524-7; citation_id=CR4"/>
    <meta name="citation_reference" content="citation_journal_title=Neural Comput Appl; citation_title=A deep learning approach for insulator instance segmentation and defect detection; citation_author=E Antwi-Bekoe, G Liu, J-P Ainam, G Sun, X Xie; citation_volume=34; citation_issue=9; citation_publication_date=2022; citation_pages=7253-7269; citation_doi=10.1007/s00521-021-06792-z; citation_id=CR5"/>
    <meta name="citation_reference" content="citation_journal_title=Neural Comput Appl; citation_title=Domain transfer multi-instance dictionary learning; citation_author=K Wang, J Liu, D Gonz&#225;lez; citation_volume=28; citation_publication_date=2017; citation_pages=983-992; citation_doi=10.1007/s00521-016-2406-5; citation_id=CR6"/>
    <meta name="citation_reference" content="citation_journal_title=Pattern Recogn; citation_title=Multiple instance learning: a survey of problem characteristics and applications; citation_author=M-A Carbonneau, V Cheplygina, E Granger, G Gagnon; citation_volume=77; citation_publication_date=2018; citation_pages=329-353; citation_doi=10.1016/j.patcog.2017.10.009; citation_id=CR7"/>
    <meta name="citation_reference" content="citation_journal_title=IEEE Trans Neural Netw Learn Syst; citation_title=Dissimilarity-based ensembles for multiple instance learning; citation_author=V Cheplygina, DM Tax, M Loog; citation_volume=27; citation_issue=6; citation_publication_date=2015; citation_pages=1379-1391; citation_doi=10.1109/TNNLS.2015.2424254; citation_id=CR8"/>
    <meta name="citation_reference" content="citation_journal_title=IEEE Trans Neural Netw Learn Syst; citation_title=Scalable algorithms for multi-instance learning; citation_author=X-S Wei, J Wu, Z-H Zhou; citation_volume=28; citation_issue=4; citation_publication_date=2016; citation_pages=975-987; citation_doi=10.1109/TNNLS.2016.2519102; citation_id=CR9"/>
    <meta name="citation_reference" content="Perronnin F, S&#225;nchez J, Mensink T (2010) Improving the fisher kernel for large-scale image classification. In: European Conference on Computer Vision, pp. 143&#8211;156. Springer"/>
    <meta name="citation_reference" content="Ramon J, De&#160;Raedt L (2000) Multi instance neural networks. In: Proceedings of the ICML-2000 Workshop on Attribute-value and Relational Learning, pp. 53&#8211;60"/>
    <meta name="citation_reference" content="citation_journal_title=Comput Med Imaging Graph; citation_title=Computer-aided diagnosis from weak supervision: a benchmarking study; citation_author=M Kandemir, FA Hamprecht; citation_volume=42; citation_publication_date=2015; citation_pages=44-50; citation_doi=10.1016/j.compmedimag.2014.11.010; citation_id=CR12"/>
    <meta name="citation_reference" content="Ilse M, Tomczak J, Welling M (2018) Attention-based deep multiple instance learning. In: International conference on machine learning, pp. 2127&#8211;2136. PMLR"/>
    <meta name="citation_reference" content="Zhang W-J, Zhou Z-H (2014) Multi-instance learning with distribution change. In: Proceedings of the AAAI conference on artificial intelligence, vol. 28"/>
    <meta name="citation_reference" content="Shi X, Xing F, Xie Y, Zhang Z, Cui L, Yang L (2020) Loss-based attention for deep multiple instance learning. In: Proceedings of the AAAI conference on artificial intelligence, vol. 34, pp. 5742&#8211;5749"/>
    <meta name="citation_reference" content="Zhou Z-H, Sun Y-Y, Li Y-F (2009) Multi-instance learning by treating instances as non-IID samples. In: Proceedings of the 26th annual international conference on machine learning, pp. 1249&#8211;1256"/>
    <meta name="citation_reference" content="Waqas M, Tahir MA, Qureshi R (2021) Ensemble-based instance relevance estimation in multiple-instance learning. In: 2021 9th European workshop on visual information processing (EUVIP), pp. 1&#8211;6. IEEE"/>
    <meta name="citation_reference" content="citation_journal_title=Appl Intell; citation_title=Deep Gaussian mixture model based instance relevance estimation for multiple instance learning applications; citation_author=M Waqas, MA Tahir, R Qureshi; citation_volume=53; citation_issue=9; citation_publication_date=2023; citation_pages=10310-10325; citation_doi=10.1007/s10489-022-04045-7; citation_id=CR18"/>
    <meta name="citation_reference" content="citation_journal_title=Expert Syst Appl; citation_title=Robust bag classification approach for multi-instance learning via subspace fuzzy clustering; citation_author=M Waqas, MA Tahir, SA Khan; citation_volume=214; citation_publication_date=2023; citation_pages=119113; citation_doi=10.1016/j.eswa.2022.119113; citation_id=CR19"/>
    <meta name="citation_reference" content="citation_journal_title=Adv Neural Inf Process Syst; citation_title=Transmil: transformer based correlated multiple instance learning for whole slide image classification; citation_author=Z Shao, H Bian, Y Chen, Y Wang, J Zhang, X Ji; citation_volume=34; citation_publication_date=2021; citation_pages=2136; citation_id=CR20"/>
    <meta name="citation_reference" content="Waqas M, Khan Z, Ahmed SU, Raza A (2023) MIL-Mixer: a robust bag encoding strategy for Multiple Instance Learning (mil) using MLP-Mixer. In 2023 18th IEEE International Conference on Emerging Technologies (ICET) 22&#8211;26"/>
    <meta name="citation_reference" content="citation_journal_title=Mach Learn; citation_title=An empirical study on image bag generators for multi-instance learning; citation_author=X-S Wei, Z-H Zhou; citation_volume=105; citation_issue=2; citation_publication_date=2016; citation_pages=155-198; citation_doi=10.1007/s10994-016-5560-1; citation_id=CR21"/>
    <meta name="citation_reference" content="citation_journal_title=Artif Intell; citation_title=Solving the multiple instance problem with axis-parallel rectangles; citation_author=TG Dietterich, RH Lathrop, T Lozano-P&#233;rez; citation_volume=89; citation_issue=1&#8211;2; citation_publication_date=1997; citation_pages=31-71; citation_doi=10.1016/S0004-3702(96)00034-3; citation_id=CR22"/>
    <meta name="citation_reference" content="citation_journal_title=IEEE Trans Med Imaging; citation_title=Locality sensitive deep learning for detection and classification of nuclei in routine colon cancer histology images; citation_author=K Sirinukunwattana, SEA Raza, Y-W Tsang, DR Snead, IA Cree, NM Rajpoot; citation_volume=35; citation_issue=5; citation_publication_date=2016; citation_pages=1196-1206; citation_doi=10.1109/TMI.2016.2525803; citation_id=CR23"/>
    <meta name="citation_reference" content="Raykar VC, Krishnapuram B, Bi J, Dundar M, Rao RB (2008) Bayesian multiple instance learning: automatic feature selection and inductive transfer. In: Proceedings of the 25th international conference on machine learning, pp. 808&#8211;815"/>
    <meta name="citation_reference" content="Andrews S, Tsochantaridis I, Hofmann T (2002) Support vector machines for multiple-instance learning. In: NIPS, vol. 2, p. 7"/>
    <meta name="citation_reference" content="Amar RA, Dooly DR, Goldman SA, Zhang Q (2001) Multiple-instance learning of real-valued data. In: ICML, pp. 3&#8211;10. Citeseer"/>
    <meta name="citation_reference" content="Zhang Q, Goldman S (2001) EM-DD: An improved multiple-instance learning technique. In: Dietterich T, Becker S, Ghahramani Z(ed) Advances in neural information processing systems. MIT Press, 14. 
                  https://proceedings.neurips.cc/paper_files/paper/2001/file/e4dd5528f7596dcdf871aa55cfccc53c-Paper.pdf
                  
                "/>
    <meta name="citation_reference" content="citation_journal_title=Pattern Recogn; citation_title=Robust multiple-instance learning ensembles using random subspace instance selection; citation_author=M-A Carbonneau, E Granger, AJ Raymond, G Gagnon; citation_volume=58; citation_publication_date=2016; citation_pages=83-99; citation_doi=10.1016/j.patcog.2016.03.035; citation_id=CR28"/>
    <meta name="citation_reference" content="citation_journal_title=Knowl Inf Syst; citation_title=Solving multi-instance problems with classifier ensemble based on constructive clustering; citation_author=Z-H Zhou, M-L Zhang; citation_volume=11; citation_issue=2; citation_publication_date=2007; citation_pages=155-170; citation_doi=10.1007/s10115-006-0029-3; citation_id=CR29"/>
    <meta name="citation_reference" content="Zhou Z-H, Xu J-M (2007) On the relation between multi-instance learning and semi-supervised learning. In: Proceedings of the 24th international conference on machine learning, pp. 1167&#8211;1174"/>
    <meta name="citation_reference" content="Leistner C, Saffari A, Bischof H (2010) Miforests: Multiple-instance learning with randomized trees. In: European conference on computer vision, pp. 29&#8211;42. Springer"/>
    <meta name="citation_reference" content="citation_journal_title=J Supercomput; citation_title=An efficient parallel neural network-based multi-instance learning algorithm; citation_author=CH Li, I Gondra, L Liu; citation_volume=62; citation_issue=2; citation_publication_date=2012; citation_pages=724-740; citation_doi=10.1007/s11227-012-0746-1; citation_id=CR32"/>
    <meta name="citation_reference" content="Waqas M, Khan Z, Anjum S, Tahir MA (2020) Lung-wise tuberculosis analysis and automatic CT report generation with hybrid feature and ensemble learning. In: CLEF (Working Notes)"/>
    <meta name="citation_reference" content="citation_journal_title=Knowl-Based Syst; citation_title=Natural language understanding for argumentative dialogue systems in the opinion building domain; citation_author=WA Abro, A Aicher, N Rach, S Ultes, W Minker, G Qi; citation_volume=242; citation_publication_date=2022; citation_pages=108318; citation_doi=10.1016/j.knosys.2022.108318; citation_id=CR34"/>
    <meta name="citation_reference" content="citation_journal_title=Sustainability; citation_title=Deepsdc: deep ensemble learner for the classification of social-media flooding events; citation_author=M Hanif, M Waqas, A Muneer, A Alwadain, MA Tahir, M Rafi; citation_volume=15; citation_issue=7; citation_publication_date=2023; citation_pages=6049; citation_doi=10.3390/su15076049; citation_id=CR35"/>
    <meta name="citation_reference" content="Hoffman J, Pathak D, Darrell T, Saenko K (2015) Detector discovery in the wild: joint multiple instance and representation learning. In: Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 2883&#8211;2891"/>
    <meta name="citation_reference" content="Zhang C, Platt J, Viola P (2005) Multiple instance boosting for object detection. In: Weiss J, Sch\&quot;{o}lkopf B, Platt J(ed) Advances in neural information processing systems. MIT Press, 18"/>
    <meta name="citation_reference" content="citation_journal_title=Med Image Anal; citation_title=Supervised graph hashing for histopathology image retrieval and classification; citation_author=X Shi, F Xing, K Xu, Y Xie, H Su, L Yang; citation_volume=42; citation_publication_date=2017; citation_pages=117-128; citation_doi=10.1016/j.media.2017.07.009; citation_id=CR38"/>
    <meta name="citation_reference" content="Liu Y, Chen H, Wang Y, Zhang P (2021) Power pooling: an adaptive pooling function for weakly labelled sound event detection. In: 2021 International joint conference on neural networks (IJCNN), pp. 1&#8211;7. IEEE"/>
    <meta name="citation_reference" content="citation_journal_title=Pattern Recogn; citation_title=Revisiting multiple instance neural networks; citation_author=X Wang, Y Yan, P Tang, X Bai, W Liu; citation_volume=74; citation_publication_date=2018; citation_pages=15-24; citation_doi=10.1016/j.patcog.2017.08.026; citation_id=CR40"/>
    <meta name="citation_reference" content="citation_journal_title=IEEE Access; citation_title=Multi-view attention-guided multiple instance detection network for interpretable breast cancer histopathological image diagnosis; citation_author=G Li, C Li, G Wu, D Ji, H Zhang; citation_volume=9; citation_publication_date=2021; citation_pages=79671-79684; citation_doi=10.1109/ACCESS.2021.3084360; citation_id=CR41"/>
    <meta name="citation_reference" content="Wang F, Jiang M, Qian C, Yang S, Li C, Zhang H, Wang X, Tang X (2017) Residual attention network for image classification. In: Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 3156&#8211;3164"/>
    <meta name="citation_reference" content="Dosovitskiy A, Beyer L, Kolesnikov A, Weissenborn D, Zhai X, Unterthiner T, Dehghani M, Minderer M, Heigold G, Gelly S et al.(2020) An image is worth 16x16 words: transformers for image recognition at scale. arXiv preprint 
                  arXiv:2010.11929
                  
                "/>
    <meta name="citation_reference" content="Vaswani A, Shazeer N, Parmar N, Uszkoreit J, Jones L, Gomez AN, Kaiser &#321;, Polosukhin I (2017) Attention is all you need. Advances in neural information processing systems. Curran Associates, Inc., 30"/>
    <meta name="citation_reference" content="Jang E, Gu S, Poole B (2017) Categorical Reparametrization with Gumbel-Softmax. In: Proceedings international conference on learning representations (ICLR). 
                  https://openreview.net/pdf?id=rkE3y85ee
                  
                "/>
    <meta name="citation_reference" content="citation_journal_title=Sci China Inf Sci; citation_title=Deep multiple instance selection; citation_author=X-C Li, D-C Zhan, J-Q Yang, Y Shi; citation_volume=64; citation_issue=3; citation_publication_date=2021; citation_pages=1-15; citation_doi=10.1007/s11432-020-3117-3; citation_id=CR46"/>
    <meta name="citation_reference" content="LeCun Y, Cortes C, Burges C (2010) Mnist handwritten digit database. ATT Labs [Online]. Available: 
                  http://yann.lecun.com/exdb/mnist
                  
                2"/>
    <meta name="citation_reference" content="Krizhevsky A, Hinton G (2009) Learning multiple layers of features from tiny images. Technical report, University of Toronto. 
                  https://www.cs.toronto.edu/~kriz/learning-features-2009-TR.pdf
                  
                "/>
    <meta name="citation_reference" content="citation_journal_title=Annu Rev Pathol; citation_title=Digital imaging in pathology: whole-slide imaging and beyond; citation_author=F Ghaznavi, A Evans, A Madabhushi, M Feldman; citation_volume=8; citation_publication_date=2013; citation_pages=331-359; citation_doi=10.1146/annurev-pathol-011811-120902; citation_id=CR49"/>
    <meta name="citation_reference" content="citation_journal_title=Front Med; citation_title=Deep learning for whole slide image analysis: an overview; citation_author=N Dimitriou, O Arandjelovi&#263;, PD Caie; citation_volume=6; citation_publication_date=2019; citation_pages=264; citation_doi=10.3389/fmed.2019.00264; citation_id=CR50"/>
    <meta name="citation_reference" content="citation_journal_title=Pattern Recogn Lett; citation_title=An embarrassingly simple approach to neural multiple instance classification; citation_author=A Asif; citation_volume=128; citation_publication_date=2019; citation_pages=474-479; citation_doi=10.1016/j.patrec.2019.10.022; citation_id=CR51"/>
    <meta name="citation_reference" content="citation_journal_title=Trans Assoc Comput Linguist; citation_title=Theoretical limitations of self-attention in neural sequence models; citation_author=M Hahn; citation_volume=8; citation_publication_date=2020; citation_pages=156-171; citation_doi=10.1162/tacl_a_00306; citation_id=CR52"/>
    <meta name="citation_reference" content="Frank E, Xu X (2008) Applying propositional learning algorithms to multi-instance data. Working paper series, Department of computer science, The University of Waikato. 
                  https://books.google.com/books?id=5eaGzgEACAAJ
                  
                "/>
    <meta name="citation_reference" content="Wang J, Zucker J-D (2000) Solving multiple-instance problem: a lazy learning approach. International Conference on Machine Learning. 1:1119&#8211;1126. 
                  https://api.semanticscholar.org/CorpusID:13896348
                  
                "/>
    <meta name="citation_reference" content="Wei X-S, Wu J, Zhou Z-H (2014) Scalable multi-instance learning. In: 2014 IEEE international conference on data mining, pp. 1037&#8211;1042. IEEE"/>
    <meta name="citation_reference" content="citation_title=Individual comparisons by ranking methods; citation_inbook_title=Breakthroughs in statistics: methodology and distribution; citation_publication_date=1992; citation_pages=196-202; citation_id=CR56; citation_author=F Wilcoxon; citation_publisher=Springer"/>
    <meta name="citation_reference" content="citation_title=Practical nonparametric statistics; citation_publication_date=1999; citation_id=CR57; citation_author=WJ Conover; citation_publisher=Wiley"/>
    <meta name="citation_reference" content="citation_journal_title=J Mach Learn Res; citation_title=Statistical comparisons of classifiers over multiple data sets; citation_author=J Dem&#353;ar; citation_volume=7; citation_publication_date=2006; citation_pages=1-30; citation_id=CR58"/>
    <meta name="citation_reference" content="citation_journal_title=J Am Stat Assoc; citation_title=The use of ranks to avoid the assumption of normality implicit in the analysis of variance; citation_author=M Friedman; citation_volume=32; citation_issue=200; citation_publication_date=1937; citation_pages=675-701; citation_doi=10.1080/01621459.1937.10503522; citation_id=CR59"/>
    <meta name="citation_reference" content="citation_journal_title=Proc IEEE; citation_title=Gradient-based learning applied to document recognition; citation_author=Y LeCun, L Bottou, Y Bengio, P Haffner; citation_volume=86; citation_issue=11; citation_publication_date=1998; citation_pages=2278-2324; citation_doi=10.1109/5.726791; citation_id=CR60"/>
    <meta name="citation_author" content="Waqas, Muhammad"/>
    <meta name="citation_author_email" content="waqas.sheikh@nu.edu.pk"/>
    <meta name="citation_author_institution" content="FAST School of Computing, National University of Computer Emerging Science (FAST-NUCES), Karachi, Pakistan"/>
    <meta name="citation_author_institution" content="Department of Imaging Physics, The University of Texas MD Anderson Cancer Center, Houston, USA"/>
    <meta name="citation_author" content="Tahir, Muhammad Atif"/>
    <meta name="citation_author_email" content="atif.tahir@nu.edu.pk"/>
    <meta name="citation_author_institution" content="FAST School of Computing, National University of Computer Emerging Science (FAST-NUCES), Karachi, Pakistan"/>
    <meta name="citation_author" content="Author, Muhammad Danish"/>
    <meta name="citation_author_email" content="k190887@nu.edu.pk"/>
    <meta name="citation_author_institution" content="College of information technology, United Arab Emirates University, Abu Dhabi, United Arab Emirates"/>
    <meta name="citation_author" content="Al-Maadeed, Sumaya"/>
    <meta name="citation_author_email" content="Salali@qu.edu.qa"/>
    <meta name="citation_author_institution" content="Department of Computer Science and Engineering, Qatar University, Doha, Qatar"/>
    <meta name="citation_author" content="Bouridane, Ahmed"/>
    <meta name="citation_author_email" content="abouridane@sharjah.ac.ae"/>
    <meta name="citation_author_institution" content="Cybersecurity and Data Analytics Research Center, University of Sharjah, Sharjah, UAE"/>
    <meta name="citation_author" content="Wu, Jia"/>
    <meta name="citation_author_email" content="jiawu11@mdanderson.com"/>
    <meta name="citation_author_institution" content="Department of Imaging Physics, The University of Texas MD Anderson Cancer Center, Houston, USA"/>
    <meta name="format-detection" content="telephone=no"/>
    <meta name="citation_cover_date" content="2024/04/01"/>
    

            
    
    <meta property="og:url" content="https://link.springer.com/article/10.1007/s00521-024-09417-3"/>
    <meta property="og:type" content="article"/>
    <meta property="og:site_name" content="SpringerLink"/>
    <meta property="og:title" content="Simultaneous instance pooling and bag representation selection approach for multiple-instance learning (MIL) using vision transformer - Neural Computing and Applications"/>
    <meta property="og:description" content="In multiple-instance learning (MIL), the existing bag encoding and attention-based pooling approaches assume that the instances in the bag have no relationship among them. This assumption is unsuited, as the instances in the bags are rarely independent in diverse MIL applications. In contrast, the instance relationship assumption-based techniques incorporate the instance relationship information in the classification process. However, in MIL, the bag composition process is complicated, and it may be possible that instances in one bag are related and instances in another bag are not. In present MIL algorithms, this relationship assumption is not explicitly modeled. The learning algorithm is trained based on one of two relationship assumptions (whether instances in all bags have a relationship or not). Hence, it is essential to model the assumption of instance relationships in the bag classification process. This paper proposes a robust approach that generates vector representation for the bag for both assumptions and the representation selection process to determine whether to consider the instances related or unrelated in the bag classification process. This process helps to determine the essential bag representation vector for every individual bag. The proposed method utilizes attention pooling and vision transformer approaches to generate bag representation vectors. Later, the representation selection subnetwork determines the vector representation essential for bag classification in an end-to-end trainable manner. The generalization abilities of the proposed framework are demonstrated through extensive experiments on several benchmark datasets. The experiments demonstrate that the proposed approach outperforms other state-of-the-art MIL approaches in bag classification."/>
    <meta property="og:image" content="https://static-content.springer.com/image/art%3A10.1007%2Fs00521-024-09417-3/MediaObjects/521_2024_9417_Fig1_HTML.png"/>
    

        

        <meta name="format-detection" content="telephone=no">

        
    
        
    
    
    

    


        <link rel="apple-touch-icon" sizes="180x180" href=/oscar-static/img/favicons/darwin/apple-touch-icon-6ef0829b9c.png>
<link rel="icon" type="image/png" sizes="192x192" href=/oscar-static/img/favicons/darwin/android-chrome-192x192.png>
<link rel="icon" type="image/png" sizes="32x32" href=/oscar-static/img/favicons/darwin/favicon-32x32.png>
<link rel="icon" type="image/png" sizes="16x16" href=/oscar-static/img/favicons/darwin/favicon-16x16.png>
<link rel="shortcut icon" data-test="shortcut-icon" href=/oscar-static/img/favicons/darwin/favicon-de0c289efe.ico>

<meta name="theme-color" content="#e6e6e6">


        



<link rel="stylesheet" media="print" href=/oscar-static/app-springerlink/css/print-b8af42253b.css>



    
        
            
    <style> html{line-height:1.15;text-size-adjust:100%}body{font-family:Merriweather Sans,Helvetica Neue,Helvetica,Arial,sans-serif;line-height:1.8;margin:0}details,main{display:block}h1{font-size:2em;margin:.67em 0}a{background-color:transparent;color:#025e8d}b{font-weight:bolder}sub{bottom:-.25em;font-size:75%;line-height:0;position:relative;vertical-align:baseline}img{border:0;height:auto;max-width:100%;vertical-align:middle}button,input{font-family:inherit;font-size:100%;line-height:1.15;margin:0;overflow:visible}button{text-transform:none}[type=button],[type=submit],button{-webkit-appearance:button}[type=search]{-webkit-appearance:textfield;outline-offset:-2px}summary{display:list-item}[hidden]{display:none}button{cursor:pointer}svg{height:1rem;width:1rem}.eds-c-header__brand img{max-width:100%}@media only screen and (min-width:768px){.eds-c-header__brand img{max-width:340px}} </style>
    <style>@media only print, only all and (prefers-color-scheme: no-preference), only all and (prefers-color-scheme: light), only all and (prefers-color-scheme: dark) {  body{background:#fff;color:#222;font-family:Merriweather Sans,Helvetica Neue,Helvetica,Arial,sans-serif;line-height:1.8;min-height:100%}a{color:#025e8d;padding:initial;text-decoration:underline;text-decoration-skip-ink:auto}button{cursor:pointer}img{border:0;height:auto;max-width:100%;vertical-align:middle}html{box-sizing:border-box;font-size:100%;height:100%;overflow-y:scroll}h1{font-size:2.25rem}h2{font-size:1.75rem}h1,h2,h3{font-weight:700;line-height:1.2}h3{font-size:1.5rem}body{font-size:1.125rem}*{box-sizing:inherit}p{margin-bottom:2rem;margin-top:0}p:last-of-type{margin-bottom:0}.c-ad{text-align:center}@media only screen and (min-width:480px){.c-ad{padding:8px}}.c-ad--728x90{display:none}.c-ad--728x90 .c-ad__inner{min-height:calc(1.5em + 94px)}@media only screen and (min-width:876px){.js .c-ad--728x90{display:none}}.c-ad__label{color:#333;font-family:Merriweather Sans,Helvetica Neue,Helvetica,Arial,sans-serif;font-size:.875rem;font-weight:400;line-height:1.5;margin-bottom:4px}.c-status-message{align-items:center;box-sizing:border-box;display:flex;width:100%}.c-status-message__heading{font-family:Merriweather Sans,Helvetica Neue,Helvetica,Arial,sans-serif;font-size:1rem;font-weight:700}.c-status-message__icon{fill:currentcolor;display:inline-block;flex:0 0 auto;transform:translate(0);vertical-align:text-top}.c-status-message--boxed.c-status-message--info{border-bottom:4px solid #003f8d}.c-status-message--boxed.c-status-message--error{border-bottom:4px solid #c40606}.c-status-message--boxed.c-status-message--success{border-bottom:4px solid #00b8b0}.c-status-message--boxed.c-status-message--warning{border-bottom:4px solid #edbc53}.eds-c-button{border-radius:32px;cursor:pointer;display:inline-block;font-family:Merriweather Sans,Helvetica Neue,Helvetica,Arial,sans-serif;font-size:1rem;font-weight:700;line-height:1.5;margin:0;padding:.5rem 1.5rem;position:relative;text-align:center;text-decoration:none;transition:all .2s ease 0s;width:100%}.eds-c-button span,.eds-c-button svg{vertical-align:middle}.eds-c-button svg{height:1.5rem;width:1.5rem}.eds-c-button svg:last-child{margin-left:8px}@media only screen and (min-width:480px){.eds-c-button{width:auto}}.eds-c-button--primary{background-color:#025e8d;background-image:none;border:2px solid transparent;box-shadow:none;color:#fff;text-decoration:none}.eds-c-button--primary svg,.eds-c-button--secondary svg{fill:currentcolor}.eds-c-button--secondary{background-color:#fff;background-image:none;border:2px solid #025e8d;box-shadow:none;color:#025e8d;text-decoration:none}.eds-c-header{background-color:#fff;border-bottom:2px solid #01324b;font-family:Merriweather Sans,Helvetica Neue,Helvetica,Arial,sans-serif;font-size:1rem;line-height:1.5;padding:8px 0 0}.eds-c-header__container{align-items:center;display:flex;flex-wrap:nowrap;gap:8px 16px;justify-content:space-between;margin:0 auto 8px;max-width:1280px;padding:0 8px;position:relative}.eds-c-header__nav{border-top:2px solid #c5e0f4;padding-top:4px;position:relative}.eds-c-header__nav-container{align-items:center;display:flex;flex-wrap:wrap;margin:0 auto 4px;max-width:1280px;padding:0 8px;position:relative}.eds-c-header__nav-container>:not(:last-child){margin-right:32px}.eds-c-header__link-container{align-items:center;display:flex;flex:1 0 auto;gap:8px 16px;justify-content:space-between}.eds-c-header__list{list-style:none;margin:0;padding:0}.eds-c-header__list-item{font-weight:700;margin:0 auto;max-width:1280px;padding:8px}.eds-c-header__list-item:not(:last-child){border-bottom:2px solid #c5e0f4}.eds-c-header__item{color:inherit}@media only screen and (min-width:768px){.eds-c-header__item--menu{display:none;visibility:hidden}.eds-c-header__item--menu:first-child+*{margin-block-start:0}}.eds-c-header__item--inline-links{display:none;visibility:hidden}@media only screen and (min-width:768px){.eds-c-header__item--inline-links{display:flex;gap:16px 16px;visibility:visible}}.eds-c-header__item--divider:before{border-left:2px solid #c5e0f4;content:"";height:calc(100% - 16px);margin-left:-15px;position:absolute;top:8px}.eds-c-header__brand{padding:16px 8px}.eds-c-header__brand a{display:block;line-height:1;text-decoration:none}.eds-c-header__brand img{height:1.5rem;width:auto}.eds-c-header__link{color:inherit;display:inline-block;font-weight:700;padding:16px 8px;position:relative;text-decoration-color:transparent;white-space:nowrap;word-break:normal}.eds-c-header__icon{fill:currentcolor;display:inline-block;font-size:1.5rem;height:1em;transform:translate(0);vertical-align:bottom;width:1em}.eds-c-header__icon+*{margin-left:8px}.eds-c-header__expander{background-color:#f0f7fc}.eds-c-header__search{display:block;padding:24px 0}@media only screen and (min-width:768px){.eds-c-header__search{max-width:70%}}.eds-c-header__search-container{position:relative}.eds-c-header__search-label{color:inherit;display:inline-block;font-weight:700;margin-bottom:8px}.eds-c-header__search-input{background-color:#fff;border:1px solid #000;padding:8px 48px 8px 8px;width:100%}.eds-c-header__search-button{background-color:transparent;border:0;color:inherit;height:100%;padding:0 8px;position:absolute;right:0}.has-tethered.eds-c-header__expander{border-bottom:2px solid #01324b;left:0;margin-top:-2px;top:100%;width:100%;z-index:10}@media only screen and (min-width:768px){.has-tethered.eds-c-header__expander--menu{display:none;visibility:hidden}}.has-tethered .eds-c-header__heading{display:none;visibility:hidden}.has-tethered .eds-c-header__heading:first-child+*{margin-block-start:0}.has-tethered .eds-c-header__search{margin:auto}.eds-c-header__heading{margin:0 auto;max-width:1280px;padding:16px 16px 0}.eds-c-pagination,.js .eds-c-modal--open{align-items:center;display:flex;justify-content:center}.eds-c-pagination{flex-wrap:wrap;font-family:Merriweather Sans,Helvetica Neue,Helvetica,Arial,sans-serif;font-size:.875rem;gap:16px 0;line-height:1.4;list-style:none;margin:0;padding:32px 0}@media only screen and (min-width:480px){.eds-c-pagination{padding:32px 16px}}.eds-c-pagination__item{margin-right:8px}.eds-c-pagination__item--prev{margin-right:16px}.eds-c-pagination__item--next .eds-c-pagination__link,.eds-c-pagination__item--prev .eds-c-pagination__link{padding:16px 8px}.eds-c-pagination__item--next{margin-left:8px}.eds-c-pagination__item:last-child{margin-right:0}.eds-c-pagination__link{align-items:center;color:#222;cursor:pointer;display:inline-block;font-size:1rem;margin:0;padding:16px 24px;position:relative;text-align:center;transition:all .2s ease 0s}.eds-c-pagination__link:visited{color:#222}.eds-c-pagination__link--disabled{border-color:#555;color:#555;cursor:default}.eds-c-pagination__link--active{background-color:#01324b;background-image:none;border-radius:8px;color:#fff}.eds-c-pagination__link--active:focus,.eds-c-pagination__link--active:hover,.eds-c-pagination__link--active:visited{color:#fff}.eds-c-pagination__link-container{align-items:center;display:flex}.eds-c-pagination__icon{fill:#222;height:1.5rem;width:1.5rem}.eds-c-pagination__icon--disabled{fill:#555}.eds-c-pagination__visually-hidden{border:0;clip:rect(0,0,0,0);clip-path:inset(50%);height:1px;overflow:hidden;padding:0;position:absolute!important;white-space:nowrap;width:1px}.c-breadcrumbs{color:#333;font-family:Merriweather Sans,Helvetica Neue,Helvetica,Arial,sans-serif;font-size:1rem;list-style:none;margin:0;padding:0}.c-breadcrumbs>li{display:inline}svg.c-breadcrumbs__chevron{margin:0 .25rem;fill:#333;height:10px;width:10px}.c-breadcrumbs--contrast,.c-breadcrumbs--contrast .c-breadcrumbs__link,.c-breadcrumbs--contrast .c-breadcrumbs__link.hover,.c-breadcrumbs--contrast .c-breadcrumbs__link.visited,.c-breadcrumbs--contrast .c-breadcrumbs__link:hover,.c-breadcrumbs--contrast .c-breadcrumbs__link:visited{color:#fff}.c-breadcrumbs--contrast svg.c-breadcrumbs__chevron{fill:#fff}@media only screen and (max-width:479px){.c-breadcrumbs .c-breadcrumbs__item{display:none}.c-breadcrumbs .c-breadcrumbs__item:last-child,.c-breadcrumbs .c-breadcrumbs__item:nth-last-child(2){display:inline}}.c-header__link{color:inherit;display:inline-block;font-weight:700;padding:16px 8px;position:relative;text-decoration-color:transparent;white-space:nowrap;word-break:normal}.l-with-sidebar{display:flex;flex-wrap:wrap}.l-with-sidebar>*{margin:0}.l-with-sidebar__sidebar{flex-basis:var(--with-sidebar--basis,400px);flex-grow:1}.l-with-sidebar>:not(.l-with-sidebar__sidebar){flex-basis:0px;flex-grow:999;min-width:var(--with-sidebar--min,52%)}.l-with-sidebar>:first-child{padding-right:4rem}@supports (gap:1em){.l-with-sidebar>:first-child{padding-right:0}.l-with-sidebar{gap:var(--with-sidebar--gap,4rem)}}.c-meta__link:visited,.c-status-message a{color:#000}.c-skip-link,.js .c-popup{font-family:Merriweather Sans,Helvetica Neue,Helvetica,Arial,sans-serif;position:absolute}.c-skip-link{background:#01324b;bottom:auto;color:#fff;font-size:1rem;padding:8px;text-align:center;transform:translateY(-100%);width:100%;z-index:9999}@media (prefers-reduced-motion:reduce){.c-skip-link{transition:top .3s ease-in-out 0s}}@media print{.c-skip-link{display:none}}.c-skip-link:active,.c-skip-link:hover,.c-skip-link:link,.c-skip-link:visited{color:#fff}.c-skip-link:focus{transform:translateY(0)}.c-status-message{font-family:Merriweather Sans,Helvetica Neue,Helvetica,Arial,sans-serif;font-weight:400;position:relative}.c-status-message :last-child{margin-bottom:0}.c-status-message--bold{font-weight:700}.c-status-message--boxed{background-color:#fff;border:1px solid #dadada;box-shadow:0 0 5px 0 rgba(51,51,51,.1);line-height:1.4;overflow:hidden;padding:16px}.c-status-message--banner{background-color:#fff;line-height:1.4;padding:16px 0}.c-status-message--banner .c-status-message__container{justify-content:center;padding:0 16px}.c-status-message--sticky{position:sticky;top:0;z-index:999}.c-status-message__container{align-items:center;display:flex;justify-content:flex-start}.c-status-message__icon{align-self:flex-start;flex-shrink:0;height:21px;margin-right:8px;width:21px}.c-status-message__message :first-child,.c-status-message__message :last-child{margin-top:0}.c-status-message__icon--top{align-self:flex-start}.c-status-message--info .c-status-message__icon{color:#0070a8}.c-status-message--banner.c-status-message--info{border-bottom:4px solid #0070a8}.c-status-message--boxed.c-status-message--info .c-status-message__bottom-border{background:#0070a8;bottom:0;content:"";height:4px;left:0;position:absolute;width:100%}.c-status-message--info .c-status-message__icon{fill:#0070a8}.c-status-message--error .c-status-message__icon{color:#be1818}.c-status-message--banner.c-status-message--error{border-bottom:4px solid #be1818}.c-status-message--boxed.c-status-message--error .c-status-message__bottom-border{background:#be1818;bottom:0;content:"";height:4px;left:0;position:absolute;width:100%}.c-status-message--error .c-status-message__icon{fill:#be1818}.c-status-message--success .c-status-message__icon{color:#00a69d}.c-status-message--banner.c-status-message--success{border-bottom:4px solid #00a69d}.c-status-message--boxed.c-status-message--success .c-status-message__bottom-border{background:#00a69d;bottom:0;content:"";height:4px;left:0;position:absolute;width:100%}.c-status-message--success .c-status-message__icon{fill:#00a69d}.c-status-message--warning .c-status-message__icon{color:#f58220}.c-status-message--banner.c-status-message--warning{border-bottom:4px solid #f58220}.c-status-message--boxed.c-status-message--warning .c-status-message__bottom-border{background:#f58220;bottom:0;content:"";height:4px;left:0;position:absolute;width:100%}.c-status-message--warning .c-status-message__icon{fill:#f58220}:root{--header-height:58px}.app-masthead__colour-4{--background-color:#ff9500;--gradient-light:rgba(0,0,0,.5);--gradient-dark:rgba(0,0,0,.8)}.app-masthead--pastel{--gradient-light:hsla(0,0%,100%,.9);--gradient-dark:hsla(0,0%,100%,.75);--masthead-color:#000}.app-masthead{background:var(--background-color,#0070a8);position:relative}.app-masthead:after{background:radial-gradient(circle at top right,var(--gradient-light,rgba(0,0,0,.4)),var(--gradient-dark,rgba(0,0,0,.7)));bottom:0;content:"";left:0;position:absolute;right:0;top:0}.app-masthead--pastel .app-masthead{background:var(--background-color,#6ac)}@media only screen and (max-width:479px){.app-masthead:after{background:linear-gradient(225deg,var(--gradient-light,rgba(0,0,0,.4)),var(--gradient-dark,rgba(0,0,0,.7)))}}.app-masthead__container{color:var(--masthead-color,#fff);margin:0 auto;max-width:1280px;padding:0 16px;position:relative;z-index:1}.u-clear-both{clear:both}.u-container{margin:0 auto;max-width:1280px;padding:0 16px}.u-justify-content-space-between{justify-content:space-between}.u-display-none{display:none}.js .u-js-hide,.u-hide{display:none;visibility:hidden}.u-visually-hidden{border:0;clip:rect(0,0,0,0);clip-path:inset(50%);height:1px;overflow:hidden;padding:0;position:absolute!important;white-space:nowrap;width:1px}.u-icon{fill:currentcolor;display:inline-block;height:1em;transform:translate(0);vertical-align:text-top;width:1em}.u-list-reset{list-style:none;margin:0;padding:0}.u-ma-16{margin:16px}.u-mt-0{margin-top:0}.u-mt-24{margin-top:24px}.u-mt-32{margin-top:32px}.u-mb-8{margin-bottom:8px}.u-mb-32{margin-bottom:32px}.c-article-extras .c-pdf-container .c-pdf-download+.c-pdf-download,.u-ml-0{margin-left:0}.u-sans-serif{font-family:Merriweather Sans,Helvetica Neue,Helvetica,Arial,sans-serif}.u-serif{font-family:Merriweather,serif}h1,h2,h3{-webkit-font-smoothing:antialiased}p{overflow-wrap:break-word;word-break:break-word}.u-h4{font-size:1.25rem;font-weight:700;line-height:1.2}.u-mbs-0{margin-block-start:0!important}.c-article-header{font-family:Merriweather Sans,Helvetica Neue,Helvetica,Arial,sans-serif}.c-article-identifiers{color:#6f6f6f;display:flex;flex-wrap:wrap;font-size:1rem;line-height:1.3;list-style:none;padding:0}.c-article-identifiers__item{list-style:none;margin-right:8px;padding-right:8px}.c-article-identifiers__item:last-child{margin-right:0;padding-right:0}@media only screen and (min-width:876px){.c-article-title{font-size:1.875rem;line-height:1.2}}.c-article-author-list{display:inline;font-size:1rem;list-style:none;margin:0 8px 0 0;padding:0;width:100%}.c-article-author-list__item{display:inline;padding-right:0}.c-article-author-list__show-more{display:none;margin-right:4px}.c-article-author-list__button,.js .c-article-author-list__item--hide,.js .c-article-author-list__show-more{display:none}.js .c-article-author-list--long .c-article-author-list__show-more,.js .c-article-author-list--long+.c-article-author-list__button{display:inline}@media only screen and (max-width:767px){.js .c-article-author-list__item--hide-small-screen{display:none}.js .c-article-author-list--short .c-article-author-list__show-more,.js .c-article-author-list--short+.c-article-author-list__button{display:inline}}#uptodate-client,.js .c-article-author-list--expanded .c-article-author-list__show-more{display:none!important}.js .c-article-author-list--expanded .c-article-author-list__item--hide-small-screen{display:inline!important}.c-article-author-list__button,.c-button-author-list{background:#ebf1f5;border:4px solid #ebf1f5;border-radius:20px;color:#666;font-size:.875rem;line-height:1.4;padding:2px 11px 2px 8px;text-decoration:none}.c-article-author-list__button svg,.c-button-author-list svg{margin:1px 4px 0 0}.c-article-author-list__button:hover,.c-button-author-list:hover{background:#025e8d;border-color:transparent;color:#fff}.c-article-body .c-article-access-provider{padding:8px 16px}.c-article-body .c-article-access-provider,.c-notes{border:1px solid #d5d5d5;border-image:initial;border-left:none;border-right:none;margin:24px 0}.c-article-body .c-article-access-provider__text{color:#555}.c-article-body .c-article-access-provider__text,.c-notes__text{font-size:1rem;margin-bottom:0;padding-bottom:2px;padding-top:2px;text-align:center}.c-article-body .c-article-author-affiliation__address{color:inherit;font-weight:700;margin:0}.c-article-body .c-article-author-affiliation__authors-list{list-style:none;margin:0;padding:0}.c-article-body .c-article-author-affiliation__authors-item{display:inline;margin-left:0}.c-article-authors-search{margin-bottom:24px;margin-top:0}.c-article-authors-search__item,.c-article-authors-search__title{font-family:Merriweather Sans,Helvetica Neue,Helvetica,Arial,sans-serif}.c-article-authors-search__title{color:#626262;font-size:1.05rem;font-weight:700;margin:0;padding:0}.c-article-authors-search__item{font-size:1rem}.c-article-authors-search__text{margin:0}.c-article-body .c-article-subject-list--no-mb{margin-bottom:0}.c-code-block{border:1px solid #fff;font-family:monospace;margin:0 0 24px;padding:20px}.c-code-block__heading{font-weight:400;margin-bottom:16px}.c-code-block__line{display:block;overflow-wrap:break-word;white-space:pre-wrap}.c-article-share-box{font-family:Merriweather Sans,Helvetica Neue,Helvetica,Arial,sans-serif;margin-bottom:24px}.c-article-share-box__description{font-size:1rem;margin-bottom:8px}.c-article-share-box__no-sharelink-info{font-size:.813rem;font-weight:700;margin-bottom:24px;padding-top:4px}.c-article-share-box__only-read-input{border:1px solid #d5d5d5;box-sizing:content-box;display:inline-block;font-size:.875rem;font-weight:700;height:24px;margin-bottom:8px;padding:8px 10px}.c-article-share-box__additional-info{color:#626262;font-size:.813rem}.c-article-share-box__button{background:#fff;box-sizing:content-box;text-align:center}.c-article-share-box__button--link-like{background-color:transparent;border:0;color:#025e8d;cursor:pointer;font-size:.875rem;margin-bottom:8px;margin-left:10px}.c-article-associated-content__container .c-article-associated-content__collection-label{font-size:.875rem;line-height:1.4}.c-article-associated-content__container .c-article-associated-content__collection-title{line-height:1.3}.c-reading-companion{clear:both;min-height:389px}.c-reading-companion__figures-list,.c-reading-companion__references-list{list-style:none;min-height:389px;padding:0}.c-reading-companion__references-list--numeric{list-style:decimal inside}.c-reading-companion__figure-item{border-top:1px solid #d5d5d5;font-size:1rem;padding:16px 8px 16px 0}.c-reading-companion__figure-item:first-child{border-top:none;padding-top:8px}.c-reading-companion__reference-item{font-size:1rem}.c-reading-companion__reference-item:first-child{border-top:none}.c-reading-companion__reference-item a{word-break:break-word}.c-reading-companion__reference-citation{display:inline}.c-reading-companion__reference-links{font-size:.813rem;font-weight:700;list-style:none;margin:8px 0 0;padding:0;text-align:right}.c-reading-companion__reference-links>a{display:inline-block;padding-left:8px}.c-reading-companion__reference-links>a:first-child{display:inline-block;padding-left:0}.c-reading-companion__figure-title{display:block;font-size:1.25rem;font-weight:700;line-height:1.2;margin:0 0 8px}.c-reading-companion__figure-links{display:flex;justify-content:space-between;margin:8px 0 0}.c-reading-companion__figure-links>a{align-items:center;display:flex}.c-article-section__figure-caption{display:block;margin-bottom:8px;word-break:break-word}.c-article-section__figure .video,p.app-article-masthead__access--above-download{margin:0 0 16px}.c-cod{display:block;font-size:1rem;width:100%}.c-cod__form{background:#ebf0f3}.c-cod__prompt{font-size:1.125rem;line-height:1.3;margin:0 0 24px}.c-cod__label{display:block;margin:0 0 4px}.c-cod__row{display:flex;margin:0 0 16px}.c-cod__row:last-child{margin:0}.c-cod__input{border:1px solid #d5d5d5;border-radius:2px;flex:1 1 auto;margin:0;padding:13px}.c-cod__input--submit{background-color:#025e8d;border:1px solid #025e8d;color:#fff;flex-shrink:1;margin-left:8px;transition:background-color .2s ease-out 0s,color .2s ease-out 0s}.c-cod__input--submit-single{flex-basis:100%;flex-shrink:0;margin:0}.c-cod__input--submit:focus,.c-cod__input--submit:hover{background-color:#fff;color:#025e8d}.save-data .c-article-author-institutional-author__sub-division,.save-data .c-article-equation__number,.save-data .c-article-figure-description,.save-data .c-article-fullwidth-content,.save-data .c-article-main-column,.save-data .c-article-satellite-article-link,.save-data .c-article-satellite-subtitle,.save-data .c-article-table-container,.save-data .c-blockquote__body,.save-data .c-code-block__heading,.save-data .c-reading-companion__figure-title,.save-data .c-reading-companion__reference-citation,.save-data .c-site-messages--nature-briefing-email-variant .serif,.save-data .c-site-messages--nature-briefing-email-variant.serif,.save-data .serif,.save-data .u-serif,.save-data h1,.save-data h2,.save-data h3{font-family:Merriweather Sans,Helvetica Neue,Helvetica,Arial,sans-serif}.c-pdf-download__link{display:flex;flex:1 1 0%;padding:13px 24px}.c-pdf-download__link:hover{text-decoration:none}.c-pdf-container{display:flex;justify-content:flex-end}@media only screen and (max-width:767px){.c-pdf-container .c-pdf-download{display:flex;flex-basis:100%}}.c-pdf-container--flex-start{justify-content:flex-start}.c-pdf-container .c-pdf-download+.c-pdf-download{margin-left:16px}.c-article-extras .c-pdf-container{flex-wrap:wrap;width:100%}.c-article-extras .c-pdf-container .c-pdf-download{width:100%}@media only screen and (min-width:768px){.c-context-bar--sticky .c-pdf-download__link{align-items:center;flex:1 1 183px}}@media only screen and (max-width:320px){.c-context-bar--sticky .c-pdf-download__link{padding:16px}}.article-page--commercial .c-pdf-container{display:block}.c-article-body .c-article-recommendations-list,.c-book-body .c-article-recommendations-list{display:flex;flex-direction:row;gap:16px 16px;margin:0;max-width:100%;padding:16px 0 0}.c-article-body .c-article-recommendations-list__item,.c-book-body .c-article-recommendations-list__item{flex:1 1 0%}@media only screen and (max-width:767px){.c-article-body .c-article-recommendations-list,.c-book-body .c-article-recommendations-list{flex-direction:column}}.c-article-body .c-article-recommendations-card__authors{display:none;font-family:Merriweather Sans,Helvetica Neue,Helvetica,Arial,sans-serif;font-size:.875rem;line-height:1.5;margin:0 0 8px}@media only screen and (max-width:767px){.c-article-body .c-article-recommendations-card__authors{display:block;margin:0}}.c-article-body .c-article-history{margin-top:24px}.app-article-metrics-bar p{margin:0}.app-article-masthead{display:flex;flex-direction:column;gap:16px 16px;padding:16px 0 24px}.app-article-masthead__info{display:flex;flex-direction:column;flex-grow:1}.app-article-masthead__brand{border-top:1px solid hsla(0,0%,100%,.8);display:flex;flex-direction:column;flex-shrink:0;gap:8px 8px;min-height:96px;padding:16px 0 0}.app-article-masthead__brand img{border:1px solid #fff;border-radius:8px;box-shadow:0 4px 15px 0 hsla(0,0%,50%,.25);height:auto;left:0;position:absolute;width:72px}.app-article-masthead__journal-link{display:block;font-size:1.125rem;font-weight:700;margin:0 0 8px;max-width:400px;padding:0 0 0 88px;position:relative}.app-article-masthead__journal-title{display:-webkit-box;overflow:hidden;-webkit-box-orient:vertical;-webkit-line-clamp:3}.app-article-masthead__submission-link{align-items:center;display:flex;font-size:1rem;gap:4px 4px;margin:0 0 0 88px}.app-article-masthead__access{align-items:center;display:flex;flex-wrap:wrap;font-size:.875rem;font-weight:300;gap:4px 4px;line-height:1.4;margin:0}.app-article-masthead__buttons{display:flex;flex-flow:column wrap;gap:16px 16px}.app-article-masthead__access svg{fill:currentcolor}.app-article-masthead a{color:#fff}.app-article-masthead a.c-pdf-download__link,.app-article-masthead__syndicated-card a,.app-article-masthead__syndicated-card a:visited,.app-masthead--pastel .app-article-masthead .c-article-identifiers *,.app-masthead--pastel .app-article-masthead .c-article-identifiers a:focus,.app-masthead--pastel .app-article-masthead .c-article-identifiers a:hover,.app-masthead--pastel .app-article-masthead a,.app-masthead--pastel .app-article-masthead a:visited{color:#000}.app-masthead--pastel .app-article-masthead .c-article-identifiers__item{border-left:1px solid #000}.app-masthead--pastel .c-pdf-download a.u-button--primary,.c-context-bar--sticky .c-context-bar__container .c-pdf-download a.u-button--primary{background-color:#025e8d;border:2px solid transparent;box-shadow:none;color:#fff;font-weight:700}.app-masthead--pastel .c-pdf-download a.u-button--primary:focus,.app-masthead--pastel .c-pdf-download a.u-button--primary:hover,.c-context-bar--sticky .c-context-bar__container .c-pdf-download a.u-button--primary:focus,.c-context-bar--sticky .c-context-bar__container .c-pdf-download a.u-button--primary:hover{background:0 0;border:2px solid #025e8d;box-shadow:none;color:#025e8d}.app-masthead--pastel .c-pdf-download a.u-button--secondary,.c-context-bar--sticky .c-context-bar__container .c-pdf-download a.u-button--secondary{background:0 0;border:2px solid #025e8d;color:#025e8d}.app-masthead--pastel .c-pdf-download a.u-button--secondary:focus,.app-masthead--pastel .c-pdf-download a.u-button--secondary:hover,.c-context-bar--sticky .c-context-bar__container .c-pdf-download a.u-button--secondary:focus,.c-context-bar--sticky .c-context-bar__container .c-pdf-download a.u-button--secondary:hover{background-color:#025e8d;border:2px solid transparent;color:#fff}@media only screen and (min-width:768px){.app-article-masthead{flex-direction:row;gap:64px 64px;padding:24px 0}.app-article-masthead__brand{border:0;padding:0}.app-article-masthead__brand img{height:auto;position:static;width:auto}.app-article-masthead__buttons{align-items:normal;margin-top:auto}.app-article-masthead__journal-link{display:flex;flex-direction:column;gap:24px 24px;margin:0 0 8px;padding:0}.app-article-masthead__submission-link{margin:0}.app-article-masthead .c-pdf-container{flex-grow:0}}@media only screen and (min-width:1024px){.app-article-masthead__brand{flex-basis:400px}}.app-article-masthead__buttons .c-pdf-container{justify-content:flex-start}.app-article-masthead .c-article-identifiers{font-size:.875rem;font-weight:300;line-height:1;margin:0 0 8px;overflow:hidden;padding:0}.app-article-masthead .c-article-identifiers--cite-list{margin:0 0 16px}.app-article-masthead .c-article-identifiers *{color:#fff}.app-article-masthead .c-cod{display:none}.app-article-masthead .c-article-identifiers__item{border-left:1px solid #fff;border-right:0;margin:0 17px 8px -9px;padding:0 0 0 8px}.app-article-masthead .c-article-identifiers__item--cite{border-left:0}.app-article-masthead__access-container{align-items:center;display:flex;flex-wrap:wrap;gap:16px 36px;justify-content:center}@media only screen and (min-width:480px){.app-article-masthead__access-container{justify-content:normal}}.app-article-masthead__access-container>*{flex:1 1 auto}@media only screen and (min-width:480px){.app-article-masthead__access-container>*{flex:0 1 auto}}.app-article-metrics-bar{display:flex;flex-wrap:wrap;font-size:1rem;padding:16px 0 0;row-gap:24px}.app-article-metrics-bar__item{padding:0 16px 0 0}.app-article-metrics-bar__count{font-weight:700}.app-article-metrics-bar__label{font-weight:400;padding-left:4px}.app-article-metrics-bar__icon{height:auto;margin-right:4px;margin-top:-4px;width:auto}.app-article-metrics-bar__arrow-icon{margin:4px 0 0 4px}.app-article-metrics-bar a{color:#000}.app-article-metrics-bar .app-article-metrics-bar__item--metrics{padding-right:0}.app-overview-section .c-article-author-list,.app-overview-section__authors{line-height:2}.app-article-metrics-bar{margin-top:8px}.c-book-toc-pagination+.c-book-section__back-to-top{margin-top:0}.c-article-body .c-article-access-provider__text--chapter{color:#222;font-family:Merriweather Sans,Helvetica Neue,Helvetica,Arial,sans-serif;padding:20px 0}.c-article-body .c-article-access-provider__text--chapter svg.c-status-message__icon{fill:#003f8d;vertical-align:middle}.c-article-body-section__content--separator{padding-top:40px}.c-pdf-download__link{max-height:44px}@media only screen and (max-width:767px){.app-article-masthead--book__info .c-pdf-container,.app-article-masthead__info .c-pdf-container{flex-direction:column;gap:12px 12px}.app-article-masthead--book__info .c-pdf-container .c-pdf-download+.c-pdf-download,.app-article-masthead__info .c-pdf-container .c-pdf-download+.c-pdf-download{margin:0}}.app-article-access .u-button--primary,.app-article-access .u-button--primary:visited{color:#fff}.c-article-authors-search__list{align-items:center;display:flex;flex-wrap:wrap;gap:16px 16px;justify-content:center}@media only screen and (min-width:480px){.c-article-authors-search__list{justify-content:normal}}.c-article-authors-search__text{align-items:center;display:flex;flex-flow:column wrap;font-size:14px;justify-content:center}@media only screen and (min-width:480px){.c-article-authors-search__text{flex-direction:row;font-size:16px}}.c-article-authors-search__links-text{font-weight:700;margin-right:8px;text-align:center}@media only screen and (min-width:480px){.c-article-authors-search__links-text{text-align:left}}.c-article-authors-search__list-item--left{flex:1 1 100%}@media only screen and (min-width:480px){.c-article-authors-search__list-item--left{flex-basis:auto}}.c-article-authors-search__list-item--right{flex:1 1 auto}.c-article-identifiers{margin:0}.c-article-identifiers__item{border-right:2px solid #cedbe0;color:#222;font-size:14px}@media only screen and (min-width:480px){.c-article-identifiers__item{font-size:16px}}.c-article-identifiers__item:last-child{border-right:none}.c-article-body .app-article-access p,.c-article-body .app-explore-related-subjects__list--no-mb{margin-bottom:0}.c-spp-access-message .c-status-message__icon{color:#00a69d;margin-top:8px}.c-article-sidebar{display:none}@media only screen and (min-width:1024px){.c-article-sidebar{display:block}}.c-cod__form{border-radius:12px}.c-cod__label{font-size:.875rem}.c-cod .c-status-message{align-items:center;justify-content:center;margin-bottom:16px;padding-bottom:16px}@media only screen and (min-width:1024px){.c-cod .c-status-message{align-items:inherit}}.c-cod .c-status-message__icon{margin-top:4px}.c-cod .c-cod__prompt{font-size:1rem;margin-bottom:16px}.c-article-body .app-article-access,.c-book-body .app-article-access{display:block}@media only screen and (min-width:1024px){.c-article-body .app-article-access,.c-book-body .app-article-access{display:none}}.c-article-body .app-card-service{margin-bottom:32px}@media only screen and (min-width:1024px){.c-article-body .app-card-service{display:none}}.app-article-access .buybox__buy .u-button--secondary,.app-article-access .u-button--primary,.c-cod__row .u-button--primary{background-color:#025e8d;border:2px solid #025e8d;box-shadow:none;font-size:1rem;font-weight:700;gap:8px 8px;justify-content:center;line-height:1.4;padding:8px 24px}.app-article-access .buybox__buy .u-button--secondary,.app-article-access .u-button--primary:hover,.c-cod__row .u-button--primary:hover{background-color:#fff;color:#025e8d}.app-article-access .buybox__buy .u-button--secondary:hover{background-color:#025e8d;color:#fff}.buybox__buy .c-notes__text{color:#666;font-size:.875rem;padding:0 16px 8px}.c-cod__input{flex-basis:auto;width:100%}.c-article-title{font-family:Merriweather Sans,Helvetica Neue,Helvetica,Arial,sans-serif;font-size:2.25rem;font-weight:700;line-height:1.2;margin:12px 0}.c-reading-companion__figure-item figure{margin:0}@media only screen and (min-width:768px){.c-article-title{margin:16px 0}}.app-article-access{border:1px solid #cedbe0;border-radius:12px;margin:0 0 32px}.app-article-access__heading{border-bottom:1px solid #cedbe0;font-family:Merriweather Sans,Helvetica Neue,Helvetica,Arial,sans-serif;font-size:1.125rem;font-weight:700;margin:0;padding:16px;text-align:center}@media only screen and (min-width:1024px){.app-article-access{margin:0 0 24px}}.c-status-message{font-size:1rem}.c-article-body{font-size:1.125rem}.c-article-body dl,.c-article-body ol,.c-article-body p,.c-article-body ul{margin-bottom:32px;margin-top:0}.c-article-access-provider__text:last-of-type,.c-article-body .c-notes__text:last-of-type{margin-bottom:0}.c-article-body ol p,.c-article-body ul p{margin-bottom:16px}.c-article-section__figure-caption{font-family:Merriweather Sans,Helvetica Neue,Helvetica,Arial,sans-serif}.c-reading-companion__figure-item{border-top-color:#cedbe0}.c-reading-companion__sticky{max-width:400px}.c-reading-companion__reference-item{border-top:1px solid #d5d5d5;padding:16px 0}.c-reading-companion__reference-item:first-child{padding-top:0}.c-article-share-box__button,.js .c-article-authors-search__item .c-article-button{background:0 0;border:2px solid #025e8d;border-radius:32px;box-shadow:none;color:#025e8d;font-size:1rem;font-weight:700;line-height:1.4;margin:0;padding:8px 24px;transition:all .2s ease 0s}.c-article-authors-search__item .c-article-button{width:100%}.c-pdf-download .c-pdf-download__link{align-items:center;background-color:#fff;border:2px solid #fff;border-radius:32px;box-shadow:none;color:#01324b;cursor:pointer;font-family:Merriweather Sans,Helvetica Neue,Helvetica,Arial,sans-serif;font-size:1rem;font-weight:700;justify-content:center;line-height:1.4;padding:8px 16px;text-decoration:none}.c-context-bar__container .c-pdf-download .c-pdf-download__link{background-color:#025e8d;background-image:none;border:2px solid #025e8d;box-shadow:none;color:#fff;font-size:1rem;font-weight:700;line-height:1.4;padding:8px 16px}@media only screen and (min-width:768px){.c-context-bar__container .c-pdf-download .c-pdf-download__link,.c-pdf-download .c-pdf-download__link{padding:8px 24px}}.c-pdf-download .c-pdf-download__link:hover{background:0 0;border:2px solid #fff;box-shadow:none;color:#fff}.c-pdf-download .c-pdf-download__link:focus{background:0 0;box-shadow:none;color:#fff}.c-context-bar__container .c-pdf-download .c-pdf-download__link:hover{border:2px solid #025e8d;box-shadow:none;color:#025e8d}.c-context-bar__container .c-pdf-download .c-pdf-download__link:focus,.c-pdf-download .c-pdf-download__link:focus{border:2px solid #025e8d}.c-article-share-box__button:focus:focus,.c-article__pill-button:focus:focus,.c-context-bar__container .c-pdf-download .c-pdf-download__link:focus:focus,.c-pdf-download .c-pdf-download__link:focus:focus{outline:3px solid #08c;will-change:transform}.c-pdf-download__link .u-icon{padding-top:0}.c-bibliographic-information__column button{margin-bottom:16px}.c-article-body .c-article-author-affiliation__list p,.c-article-body .c-article-author-information__list p,figure{margin:0}.c-article-share-box__button{margin-right:16px}.c-status-message--boxed{border-radius:12px}.c-article-associated-content__collection-title{font-size:1rem}.app-card-service__description,.c-article-body .app-card-service__description{color:#222;margin-bottom:0;margin-top:8px}.app-article-access__subscriptions a,.app-article-access__subscriptions a:visited,.app-book-series-listing__item a,.app-book-series-listing__item a:hover,.app-book-series-listing__item a:visited,.c-article-author-list a,.c-article-author-list a:visited,.c-article-buy-box a,.c-article-buy-box a:visited,.c-article-peer-review a,.c-article-peer-review a:visited,.c-article-satellite-subtitle a,.c-article-satellite-subtitle a:visited,.c-breadcrumbs__link,.c-breadcrumbs__link:hover,.c-breadcrumbs__link:visited{color:#000}.c-article-author-list svg{height:24px;margin:0 0 0 6px;width:24px}.c-article-header{margin-bottom:32px}@media only screen and (min-width:876px){.js .c-ad--conditional{display:block}}.u-lazy-ad-wrapper{background-color:#fff;display:none;min-height:149px}@media only screen and (min-width:876px){.u-lazy-ad-wrapper{display:block}}p.c-ad__label{margin-bottom:4px}.c-ad--728x90{background-color:#fff;border-bottom:2px solid #cedbe0} } </style>
    <style>@media only print, only all and (prefers-color-scheme: no-preference), only all and (prefers-color-scheme: light), only all and (prefers-color-scheme: dark) {  .eds-c-header__brand img{height:24px;width:203px}.app-article-masthead__journal-link img{height:93px;width:72px}@media only screen and (min-width:769px){.app-article-masthead__journal-link img{height:161px;width:122px}} } </style>

        
        <link rel="stylesheet" data-test="critical-css-handler" data-inline-css-source="critical-css" href=/oscar-static/app-springerlink/css/core-darwin-9fe647df8f.css media="print" onload="this.media='all';this.onload=null">
        <link rel="stylesheet" data-test="critical-css-handler" data-inline-css-source="critical-css"
              href="/oscar-static/app-springerlink/css/enhanced-darwin-article-02295df069.css" media="print"
              onload="this.media='only print, only all and (prefers-color-scheme: no-preference), only all and (prefers-color-scheme: light), only all and (prefers-color-scheme: dark)';this.onload=null">
    

        

        
        
        
    <script type="text/javascript">
        config = {
            env: 'live',
            site: '521.springer.com',
            siteWithPath: '521.springer.com' + window.location.pathname,
            twitterHashtag: '521',
            cmsPrefix: 'https://studio-cms.springernature.com/studio/',
            
            
            
            
            publisherBrand: 'Springer',
            mustardcut: false
        };
    </script>

        




    <script>
        window.dataLayer = [{"GA Key":"UA-26408784-1","DOI":"10.1007/s00521-024-09417-3","Page":"article","springerJournal":true,"Publishing Model":"Hybrid Access","Country":"US","japan":false,"doi":"10.1007-s00521-024-09417-3","Journal Id":521,"Journal Title":"Neural Computing and Applications","imprint":"Springer","Keywords":"Multiple-instance learning (MIL), Vision transformers, Attention-based pooling, Bag representation selection","kwrd":["Multiple-instance_learning_(MIL)","Vision_transformers","Attention-based_pooling","Bag_representation_selection"],"Labs":"Y","ksg":"Krux.segments","kuid":"Krux.uid","Has Body":"Y","Features":[],"Open Access":"Y","hasAccess":"Y","bypassPaywall":"N","user":{"license":{"businessPartnerID":[],"businessPartnerIDString":""}},"Access Type":"open","Bpids":"","Bpnames":"","BPID":["1"],"VG Wort Identifier":"vgzm.415900-10.1007-s00521-024-09417-3","Full HTML":"Y","Subject Codes":["SCI","SCI21000","SCI18030","SCI17036","SCM14026","SCI22021","SCI23050"],"pmc":["I","I21000","I18030","I17036","M14026","I22021","I23050"],"session":{"authentication":{"loginStatus":"N"},"attributes":{"edition":"academic"}},"entitlement":{"accessDecision":"OpenAccess"},"content":{"serial":{"eissn":"1433-3058","pissn":"0941-0643"},"type":"Article","category":{"pmc":{"primarySubject":"Computer Science","primarySubjectCode":"I","secondarySubjects":{"1":"Artificial Intelligence","2":"Data Mining and Knowledge Discovery","3":"Probability and Statistics in Computer Science","4":"Computational Science and Engineering","5":"Image Processing and Computer Vision","6":"Computational Biology/Bioinformatics"},"secondarySubjectCodes":{"1":"I21000","2":"I18030","3":"I17036","4":"M14026","5":"I22021","6":"I23050"}},"sucode":"SC6","articleType":"Original Article","snt":["Categorization","Machine Learning","Pattern vision","Computer Vision","Object vision","Object Recognition"]},"attributes":{"deliveryPlatform":"oscar"}},"page":{"attributes":{"environment":"live"},"category":{"pageType":"article"}},"Event Category":"Article"}];
    </script>











    <script data-test="springer-link-article-datalayer">
        window.dataLayer = window.dataLayer || [];
        window.dataLayer.push({
            ga4MeasurementId: 'G-B3E4QL2TPR',
            ga360TrackingId: 'UA-26408784-1',
            twitterId: 'o47a7',
            baiduId: 'aef3043f025ccf2305af8a194652d70b',
            ga4ServerUrl: 'https://collect.springer.com',
            imprint: 'springerlink',
                page: {
                    attributes:{
                        featureFlags: [
                            
                                { name: 'darwin-orion', active: true },
                            
                                { name: 'show-profile-page-links', active: true },
                            
                                { name: 'download-collection-test', active: false },
                            
                                { name: 'download-issue-test', active: false },
                            
                        ],
                        darwinAvailable: true
                    }
                }
            
        });
    </script>



        <script>
    (function(w, d) {
        w.config = w.config || {};
        w.config.mustardcut = false;

        
        if (w.matchMedia && w.matchMedia('only print, only all and (prefers-color-scheme: no-preference), only all and (prefers-color-scheme: light), only all and (prefers-color-scheme: dark)').matches) {
            w.config.mustardcut = true;
            d.classList.add('js');
            d.classList.remove('grade-c');
            d.classList.remove('no-js');
        }
    })(window, document.documentElement);
</script>


        <script class="js-entry">
    if (window.config.mustardcut) {
        (function(w, d) {
            
            
            
                window.Component = {};
                window.suppressShareButton = false;
                window.onArticlePage = true;
            

            var currentScript = d.currentScript || d.head.querySelector('script.js-entry');

            
            function catchNoModuleSupport() {
                var scriptEl = d.createElement('script');
                return (!('noModule' in scriptEl) && 'onbeforeload' in scriptEl)
            }

            var headScripts = [
                {'src': '/oscar-static/js/polyfill-es5-bundle-b4356fa7f5.js', 'async': false}
            ];

            var bodyScripts = [
                
                    
                    {'src': '/oscar-static/js/global-article-es5-bundle-f45c6eaf2d.js', 'async': false, 'module': false},
                    {'src': '/oscar-static/js/global-article-es6-bundle-09cde44cd7.js', 'async': false, 'module': true}
                    
                
                
                    
                
                
                
            ];

            function createScript(script) {
                var scriptEl = d.createElement('script');
                scriptEl.src = script.src;
                scriptEl.async = script.async;
                if (script.module === true) {
                    scriptEl.type = "module";
                    if (catchNoModuleSupport()) {
                        scriptEl.src = '';
                    }
                } else if (script.module === false) {
                    scriptEl.setAttribute('nomodule', true)
                }
                if (script.charset) {
                    scriptEl.setAttribute('charset', script.charset);
                }

                return scriptEl;
            }

            for (var i = 0; i < headScripts.length; ++i) {
                var scriptEl = createScript(headScripts[i]);
                currentScript.parentNode.insertBefore(scriptEl, currentScript.nextSibling);
            }

            d.addEventListener('DOMContentLoaded', function() {
                for (var i = 0; i < bodyScripts.length; ++i) {
                    var scriptEl = createScript(bodyScripts[i]);
                    d.body.appendChild(scriptEl);
                }
            });

            // Webfont repeat view
            var config = w.config;
            if (config && config.publisherBrand && sessionStorage.fontsLoaded === 'true') {
                d.documentElement.className += ' webfonts-loaded';
            }
        })(window, document);
    }
</script>



        
        
            <script>
    (function (w, d, s) {
        var urlParams = new URLSearchParams(w.location.search);
        if (urlParams.get('gptAdsTest') !== null) {
            d.addEventListener('sncc:initialise', function (e) {
                var t = d.createElement(s);
                var h = d.getElementsByTagName(s)[0];
                t.src = 'https://' + (e.detail.C03 ? 'securepubads.g.doubleclick' : 'pagead2.googlesyndication') + '.net/tag/js/gpt.js';
                t.async = false;
                t.onload = function () {
                    var n = d.createElement(s);
                    n.src = 'https://fed-libs.springer.com/production/gpt-ads-gtm.min.js';
                    n.async = false;
                    h.insertAdjacentElement('afterend', n);
                };
                h.insertAdjacentElement('afterend', t);
            })
        }
    })(window, document, 'script');
</script>
        
        
        

        
            
            
                
    <script data-test="gtm-head">
        window.initGTM = function () {
            if (window.config.mustardcut) {
                (function (w, d, s, l, i) {
                    w[l] = w[l] || [];
                    w[l].push({'gtm.start': new Date().getTime(), event: 'gtm.js'});
                    var f = d.getElementsByTagName(s)[0],
                        j = d.createElement(s),
                        dl = l != 'dataLayer' ? '&l=' + l : '';
                    j.async = true;
                    j.src = 'https://sgtm.springer.com/gtm.js?id=' + i + dl;
                    f.parentNode.insertBefore(j, f);
                })(window, document, 'script', 'dataLayer', 'GTM-MRVXSHQ');
            }
        }
    </script>

            
            
            
        

        <script>
(function (w, d, t) {
    function cc() {
        var h = w.location.hostname;
        var e = d.createElement(t),
        s = d.getElementsByTagName(t)[0];

        
        if (h.indexOf('springer.com') > -1 && h.indexOf('biomedcentral.com') === -1 && h.indexOf('springeropen.com') === -1) {
            e.src = 'https://cmp.springer.com/production_live/en/consent-bundle-17-71.js';
            e.setAttribute('onload', "initGTM(window,document,'script','dataLayer','GTM-MRVXSHQ')");
        } else if (h.indexOf('biomedcentral.com') > -1) {
            e.src = 'https://cmp.biomedcentral.com/production_live/en/consent-bundle-15-46.js';
            e.setAttribute('onload', "initGTM(window,document,'script','dataLayer','GTM-MRVXSHQ')");
        } else if (h.indexOf('springeropen.com') > -1) {
            e.src = 'https://cmp.springernature.com/production_live/en/consent-bundle-16-42.js';
            e.setAttribute('onload', "initGTM(window,document,'script','dataLayer','GTM-MRVXSHQ')");
        } else if (h.indexOf('springernature.com') > -1) {
            e.src = 'https://cmp.springernature.com/production_live/en/consent-bundle-49-65.js';
            e.setAttribute('onload', "initGTM(window,document,'script','dataLayer','GTM-NK22KLS')");
        } else {
            e.src = '/oscar-static/js/cookie-consent-es5-bundle-8d962b73c2.js';
            e.setAttribute('data-consent', h);
        }
        s.insertAdjacentElement('afterend', e);
    }

    cc();
})(window, document, 'script');
</script>


        
        
        
    
        
    

        
    
    <link rel="canonical" href="https://link.springer.com/article/10.1007/s00521-024-09417-3"/>
    

        
        
        
        
        
    <script type="application/ld+json">{"mainEntity":{"headline":"Simultaneous instance pooling and bag representation selection approach for multiple-instance learning (MIL) using vision transformer","description":"In multiple-instance learning (MIL), the existing bag encoding and attention-based pooling approaches assume that the instances in the bag have no relationship among them. This assumption is unsuited, as the instances in the bags are rarely independent in diverse MIL applications. In contrast, the instance relationship assumption-based techniques incorporate the instance relationship information in the classification process. However, in MIL, the bag composition process is complicated, and it may be possible that instances in one bag are related and instances in another bag are not. In present MIL algorithms, this relationship assumption is not explicitly modeled. The learning algorithm is trained based on one of two relationship assumptions (whether instances in all bags have a relationship or not). Hence, it is essential to model the assumption of instance relationships in the bag classification process. This paper proposes a robust approach that generates vector representation for the bag for both assumptions and the representation selection process to determine whether to consider the instances related or unrelated in the bag classification process. This process helps to determine the essential bag representation vector for every individual bag. The proposed method utilizes attention pooling and vision transformer approaches to generate bag representation vectors. Later, the representation selection subnetwork determines the vector representation essential for bag classification in an end-to-end trainable manner. The generalization abilities of the proposed framework are demonstrated through extensive experiments on several benchmark datasets. The experiments demonstrate that the proposed approach outperforms other state-of-the-art MIL approaches in bag classification.","datePublished":"2024-02-16T00:00:00Z","dateModified":"2024-02-16T00:00:00Z","pageStart":"6659","pageEnd":"6680","license":"http://creativecommons.org/licenses/by/4.0/","sameAs":"https://doi.org/10.1007/s00521-024-09417-3","keywords":["Multiple-instance learning (MIL)","Vision transformers","Attention-based pooling","Bag representation selection","Artificial Intelligence","Data Mining and Knowledge Discovery","Probability and Statistics in Computer Science","Computational Science and Engineering","Image Processing and Computer Vision","Computational Biology/Bioinformatics"],"image":["https://media.springernature.com/lw1200/springer-static/image/art%3A10.1007%2Fs00521-024-09417-3/MediaObjects/521_2024_9417_Fig1_HTML.png","https://media.springernature.com/lw1200/springer-static/image/art%3A10.1007%2Fs00521-024-09417-3/MediaObjects/521_2024_9417_Fig2_HTML.png","https://media.springernature.com/lw1200/springer-static/image/art%3A10.1007%2Fs00521-024-09417-3/MediaObjects/521_2024_9417_Fig3_HTML.png","https://media.springernature.com/lw1200/springer-static/image/art%3A10.1007%2Fs00521-024-09417-3/MediaObjects/521_2024_9417_Fig4_HTML.png","https://media.springernature.com/lw1200/springer-static/image/art%3A10.1007%2Fs00521-024-09417-3/MediaObjects/521_2024_9417_Fig5_HTML.png","https://media.springernature.com/lw1200/springer-static/image/art%3A10.1007%2Fs00521-024-09417-3/MediaObjects/521_2024_9417_Fig6_HTML.png","https://media.springernature.com/lw1200/springer-static/image/art%3A10.1007%2Fs00521-024-09417-3/MediaObjects/521_2024_9417_Fig7_HTML.png","https://media.springernature.com/lw1200/springer-static/image/art%3A10.1007%2Fs00521-024-09417-3/MediaObjects/521_2024_9417_Fig8_HTML.png","https://media.springernature.com/lw1200/springer-static/image/art%3A10.1007%2Fs00521-024-09417-3/MediaObjects/521_2024_9417_Fig9_HTML.png","https://media.springernature.com/lw1200/springer-static/image/art%3A10.1007%2Fs00521-024-09417-3/MediaObjects/521_2024_9417_Fig10_HTML.png","https://media.springernature.com/lw1200/springer-static/image/art%3A10.1007%2Fs00521-024-09417-3/MediaObjects/521_2024_9417_Fig11_HTML.png"],"isPartOf":{"name":"Neural Computing and Applications","issn":["1433-3058","0941-0643"],"volumeNumber":"36","@type":["Periodical","PublicationVolume"]},"publisher":{"name":"Springer London","logo":{"url":"https://www.springernature.com/app-sn/public/images/logo-springernature.png","@type":"ImageObject"},"@type":"Organization"},"author":[{"name":"Muhammad Waqas","url":"http://orcid.org/0000-0002-4659-783X","affiliation":[{"name":"National University of Computer Emerging Science (FAST-NUCES)","address":{"name":"FAST School of Computing, National University of Computer Emerging Science (FAST-NUCES), Karachi, Pakistan","@type":"PostalAddress"},"@type":"Organization"},{"name":"The University of Texas MD Anderson Cancer Center","address":{"name":"Department of Imaging Physics, The University of Texas MD Anderson Cancer Center, Houston, USA","@type":"PostalAddress"},"@type":"Organization"}],"email":"waqas.sheikh@nu.edu.pk","@type":"Person"},{"name":"Muhammad Atif Tahir","affiliation":[{"name":"National University of Computer Emerging Science (FAST-NUCES)","address":{"name":"FAST School of Computing, National University of Computer Emerging Science (FAST-NUCES), Karachi, Pakistan","@type":"PostalAddress"},"@type":"Organization"}],"@type":"Person"},{"name":"Muhammad Danish Author","affiliation":[{"name":"United Arab Emirates University","address":{"name":"College of information technology, United Arab Emirates University, Abu Dhabi, United Arab Emirates","@type":"PostalAddress"},"@type":"Organization"}],"@type":"Person"},{"name":"Sumaya Al-Maadeed","affiliation":[{"name":"Qatar University","address":{"name":"Department of Computer Science and Engineering, Qatar University, Doha, Qatar","@type":"PostalAddress"},"@type":"Organization"}],"@type":"Person"},{"name":"Ahmed Bouridane","affiliation":[{"name":"University of Sharjah","address":{"name":"Cybersecurity and Data Analytics Research Center, University of Sharjah, Sharjah, UAE","@type":"PostalAddress"},"@type":"Organization"}],"@type":"Person"},{"name":"Jia Wu","affiliation":[{"name":"The University of Texas MD Anderson Cancer Center","address":{"name":"Department of Imaging Physics, The University of Texas MD Anderson Cancer Center, Houston, USA","@type":"PostalAddress"},"@type":"Organization"}],"@type":"Person"}],"isAccessibleForFree":true,"@type":"ScholarlyArticle"},"@context":"https://schema.org","@type":"WebPage"}</script>

        
        
    </head>

    <body class=""
    
          >
        <div class="u-visually-hidden" aria-hidden="true" data-test="darwin-icons">
    <svg xmlns="http://www.w3.org/2000/svg"><defs><clipPath id="a"><path d="M.5 0h24v24H.5z"/></clipPath><clipPath id="youtube-icon"><rect width="24" height="24"/></clipPath></defs><symbol id="icon-eds-i-accesses-medium" viewBox="0 0 24 24"><path d="M15.59 1a1 1 0 0 1 .706.291l5.41 5.385a1 1 0 0 1 .294.709v13.077c0 .674-.269 1.32-.747 1.796a2.549 2.549 0 0 1-1.798.742H15a1 1 0 0 1 0-2h4.455a.549.549 0 0 0 .387-.16.535.535 0 0 0 .158-.378V7.8L15.178 3H5.545a.543.543 0 0 0-.538.451L5 3.538v8.607a1 1 0 0 1-2 0V3.538A2.542 2.542 0 0 1 5.545 1h10.046ZM8 13c2.052 0 4.66 1.61 6.36 3.4l.124.141c.333.41.516.925.516 1.459 0 .6-.232 1.178-.64 1.599C12.666 21.388 10.054 23 8 23c-2.052 0-4.66-1.61-6.353-3.393A2.31 2.31 0 0 1 1 18c0-.6.232-1.178.64-1.6C3.34 14.61 5.948 13 8 13Zm0 2c-1.369 0-3.552 1.348-4.917 2.785A.31.31 0 0 0 3 18c0 .083.031.161.09.222C4.447 19.652 6.631 21 8 21c1.37 0 3.556-1.35 4.917-2.785A.31.31 0 0 0 13 18a.32.32 0 0 0-.048-.17l-.042-.052C11.553 16.348 9.369 15 8 15Zm0 1a2 2 0 1 1 0 4 2 2 0 0 1 0-4Z"/></symbol><symbol id="icon-eds-i-altmetric-medium" viewBox="0 0 24 24"><path d="M12 1c5.978 0 10.843 4.77 10.996 10.712l.004.306-.002.022-.002.248C22.843 18.23 17.978 23 12 23 5.925 23 1 18.075 1 12S5.925 1 12 1Zm-1.726 9.246L8.848 12.53a1 1 0 0 1-.718.461L8.003 13l-4.947.014a9.001 9.001 0 0 0 17.887-.001L16.553 13l-2.205 3.53a1 1 0 0 1-1.735-.068l-.05-.11-2.289-6.106ZM12 3a9.001 9.001 0 0 0-8.947 8.013l4.391-.012L9.652 7.47a1 1 0 0 1 1.784.179l2.288 6.104 1.428-2.283a1 1 0 0 1 .722-.462l.129-.008 4.943.012A9.001 9.001 0 0 0 12 3Z"/></symbol><symbol id="icon-eds-i-arrow-bend-down-medium" viewBox="0 0 24 24"><path d="m11.852 20.989.058.007L12 21l.075-.003.126-.017.111-.03.111-.044.098-.052.104-.074.082-.073 6-6a1 1 0 0 0-1.414-1.414L13 17.585v-12.2C13 4.075 11.964 3 10.667 3H4a1 1 0 1 0 0 2h6.667c.175 0 .333.164.333.385v12.2l-4.293-4.292a1 1 0 0 0-1.32-.083l-.094.083a1 1 0 0 0 0 1.414l6 6c.035.036.073.068.112.097l.11.071.114.054.105.035.118.025Z"/></symbol><symbol id="icon-eds-i-arrow-bend-down-small" viewBox="0 0 16 16"><path d="M1 2a1 1 0 0 0 1 1h5v8.585L3.707 8.293a1 1 0 0 0-1.32-.083l-.094.083a1 1 0 0 0 0 1.414l5 5 .063.059.093.069.081.048.105.048.104.035.105.022.096.01h.136l.122-.018.113-.03.103-.04.1-.053.102-.07.052-.043 5.04-5.037a1 1 0 1 0-1.415-1.414L9 11.583V3a2 2 0 0 0-2-2H2a1 1 0 0 0-1 1Z"/></symbol><symbol id="icon-eds-i-arrow-bend-up-medium" viewBox="0 0 24 24"><path d="m11.852 3.011.058-.007L12 3l.075.003.126.017.111.03.111.044.098.052.104.074.082.073 6 6a1 1 0 1 1-1.414 1.414L13 6.415v12.2C13 19.925 11.964 21 10.667 21H4a1 1 0 0 1 0-2h6.667c.175 0 .333-.164.333-.385v-12.2l-4.293 4.292a1 1 0 0 1-1.32.083l-.094-.083a1 1 0 0 1 0-1.414l6-6c.035-.036.073-.068.112-.097l.11-.071.114-.054.105-.035.118-.025Z"/></symbol><symbol id="icon-eds-i-arrow-bend-up-small" viewBox="0 0 16 16"><path d="M1 13.998a1 1 0 0 1 1-1h5V4.413L3.707 7.705a1 1 0 0 1-1.32.084l-.094-.084a1 1 0 0 1 0-1.414l5-5 .063-.059.093-.068.081-.05.105-.047.104-.035.105-.022L7.94 1l.136.001.122.017.113.03.103.04.1.053.102.07.052.043 5.04 5.037a1 1 0 1 1-1.415 1.414L9 4.415v8.583a2 2 0 0 1-2 2H2a1 1 0 0 1-1-1Z"/></symbol><symbol id="icon-eds-i-arrow-diagonal-medium" viewBox="0 0 24 24"><path d="M14 3h6l.075.003.126.017.111.03.111.044.098.052.096.067.09.08c.036.035.068.073.097.112l.071.11.054.114.035.105.03.148L21 4v6a1 1 0 0 1-2 0V6.414l-4.293 4.293a1 1 0 0 1-1.414-1.414L17.584 5H14a1 1 0 0 1-.993-.883L13 4a1 1 0 0 1 1-1ZM4 13a1 1 0 0 1 1 1v3.584l4.293-4.291a1 1 0 1 1 1.414 1.414L6.414 19H10a1 1 0 0 1 .993.883L11 20a1 1 0 0 1-1 1l-6.075-.003-.126-.017-.111-.03-.111-.044-.098-.052-.096-.067-.09-.08a1.01 1.01 0 0 1-.097-.112l-.071-.11-.054-.114-.035-.105-.025-.118-.007-.058L3 20v-6a1 1 0 0 1 1-1Z"/></symbol><symbol id="icon-eds-i-arrow-diagonal-small" viewBox="0 0 16 16"><path d="m2 15-.082-.004-.119-.016-.111-.03-.111-.044-.098-.052-.096-.067-.09-.08a1.008 1.008 0 0 1-.097-.112l-.071-.11-.031-.062-.034-.081-.024-.076-.025-.118-.007-.058L1 14.02V9a1 1 0 1 1 2 0v2.584l2.793-2.791a1 1 0 1 1 1.414 1.414L4.414 13H7a1 1 0 0 1 .993.883L8 14a1 1 0 0 1-1 1H2ZM14 1l.081.003.12.017.111.03.111.044.098.052.096.067.09.08c.036.035.068.073.097.112l.071.11.031.062.034.081.024.076.03.148L15 2v5a1 1 0 0 1-2 0V4.414l-2.96 2.96A1 1 0 1 1 8.626 5.96L11.584 3H9a1 1 0 0 1-.993-.883L8 2a1 1 0 0 1 1-1h5Z"/></symbol><symbol id="icon-eds-i-arrow-down-medium" viewBox="0 0 24 24"><path d="m20.707 12.728-7.99 7.98a.996.996 0 0 1-.561.281l-.157.011a.998.998 0 0 1-.788-.384l-7.918-7.908a1 1 0 0 1 1.414-1.416L11 17.576V4a1 1 0 0 1 2 0v13.598l6.293-6.285a1 1 0 0 1 1.32-.082l.095.083a1 1 0 0 1-.001 1.414Z"/></symbol><symbol id="icon-eds-i-arrow-down-small" viewBox="0 0 16 16"><path d="m1.293 8.707 6 6 .063.059.093.069.081.048.105.049.104.034.056.013.118.017L8 15l.076-.003.122-.017.113-.03.085-.032.063-.03.098-.058.06-.043.05-.043 6.04-6.037a1 1 0 0 0-1.414-1.414L9 11.583V2a1 1 0 1 0-2 0v9.585L2.707 7.293a1 1 0 0 0-1.32-.083l-.094.083a1 1 0 0 0 0 1.414Z"/></symbol><symbol id="icon-eds-i-arrow-left-medium" viewBox="0 0 24 24"><path d="m11.272 3.293-7.98 7.99a.996.996 0 0 0-.281.561L3 12.001c0 .32.15.605.384.788l7.908 7.918a1 1 0 0 0 1.416-1.414L6.424 13H20a1 1 0 0 0 0-2H6.402l6.285-6.293a1 1 0 0 0 .082-1.32l-.083-.095a1 1 0 0 0-1.414.001Z"/></symbol><symbol id="icon-eds-i-arrow-left-small" viewBox="0 0 16 16"><path d="m7.293 1.293-6 6-.059.063-.069.093-.048.081-.049.105-.034.104-.013.056-.017.118L1 8l.003.076.017.122.03.113.032.085.03.063.058.098.043.06.043.05 6.037 6.04a1 1 0 0 0 1.414-1.414L4.417 9H14a1 1 0 0 0 0-2H4.415l4.292-4.293a1 1 0 0 0 .083-1.32l-.083-.094a1 1 0 0 0-1.414 0Z"/></symbol><symbol id="icon-eds-i-arrow-right-medium" viewBox="0 0 24 24"><path d="m12.728 3.293 7.98 7.99a.996.996 0 0 1 .281.561l.011.157c0 .32-.15.605-.384.788l-7.908 7.918a1 1 0 0 1-1.416-1.414L17.576 13H4a1 1 0 0 1 0-2h13.598l-6.285-6.293a1 1 0 0 1-.082-1.32l.083-.095a1 1 0 0 1 1.414.001Z"/></symbol><symbol id="icon-eds-i-arrow-right-small" viewBox="0 0 16 16"><path d="m8.707 1.293 6 6 .059.063.069.093.048.081.049.105.034.104.013.056.017.118L15 8l-.003.076-.017.122-.03.113-.032.085-.03.063-.058.098-.043.06-.043.05-6.037 6.04a1 1 0 0 1-1.414-1.414L11.583 9H2a1 1 0 1 1 0-2h9.585L7.293 2.707a1 1 0 0 1-.083-1.32l.083-.094a1 1 0 0 1 1.414 0Z"/></symbol><symbol id="icon-eds-i-arrow-up-medium" viewBox="0 0 24 24"><path d="m3.293 11.272 7.99-7.98a.996.996 0 0 1 .561-.281L12.001 3c.32 0 .605.15.788.384l7.918 7.908a1 1 0 0 1-1.414 1.416L13 6.424V20a1 1 0 0 1-2 0V6.402l-6.293 6.285a1 1 0 0 1-1.32.082l-.095-.083a1 1 0 0 1 .001-1.414Z"/></symbol><symbol id="icon-eds-i-arrow-up-small" viewBox="0 0 16 16"><path d="m1.293 7.293 6-6 .063-.059.093-.069.081-.048.105-.049.104-.034.056-.013.118-.017L8 1l.076.003.122.017.113.03.085.032.063.03.098.058.06.043.05.043 6.04 6.037a1 1 0 0 1-1.414 1.414L9 4.417V14a1 1 0 0 1-2 0V4.415L2.707 8.707a1 1 0 0 1-1.32.083l-.094-.083a1 1 0 0 1 0-1.414Z"/></symbol><symbol id="icon-eds-i-article-medium" viewBox="0 0 24 24"><path d="M8 7a1 1 0 0 0 0 2h4a1 1 0 1 0 0-2H8ZM8 11a1 1 0 1 0 0 2h8a1 1 0 1 0 0-2H8ZM7 16a1 1 0 0 1 1-1h8a1 1 0 1 1 0 2H8a1 1 0 0 1-1-1Z"/><path d="M5.545 1A2.542 2.542 0 0 0 3 3.538v16.924A2.542 2.542 0 0 0 5.545 23h12.91A2.542 2.542 0 0 0 21 20.462V3.5A2.5 2.5 0 0 0 18.5 1H5.545ZM5 3.538C5 3.245 5.24 3 5.545 3H18.5a.5.5 0 0 1 .5.5v16.962c0 .293-.24.538-.546.538H5.545A.542.542 0 0 1 5 20.462V3.538Z" clip-rule="evenodd"/></symbol><symbol id="icon-eds-i-book-medium" viewBox="0 0 24 24"><path d="M18.5 1A2.5 2.5 0 0 1 21 3.5v12c0 1.16-.79 2.135-1.86 2.418l-.14.031V21h1a1 1 0 0 1 .993.883L21 22a1 1 0 0 1-1 1H6.5A3.5 3.5 0 0 1 3 19.5v-15A3.5 3.5 0 0 1 6.5 1h12ZM17 18H6.5a1.5 1.5 0 0 0-1.493 1.356L5 19.5A1.5 1.5 0 0 0 6.5 21H17v-3Zm1.5-15h-12A1.5 1.5 0 0 0 5 4.5v11.837l.054-.025a3.481 3.481 0 0 1 1.254-.307L6.5 16h12a.5.5 0 0 0 .492-.41L19 15.5v-12a.5.5 0 0 0-.5-.5ZM15 6a1 1 0 0 1 0 2H9a1 1 0 1 1 0-2h6Z"/></symbol><symbol id="icon-eds-i-book-series-medium" viewBox="0 0 24 24"><path fill-rule="evenodd" d="M1 3.786C1 2.759 1.857 2 2.82 2H6.18c.964 0 1.82.759 1.82 1.786V4h3.168c.668 0 1.298.364 1.616.938.158-.109.333-.195.523-.252l3.216-.965c.923-.277 1.962.204 2.257 1.187l4.146 13.82c.296.984-.307 1.957-1.23 2.234l-3.217.965c-.923.277-1.962-.203-2.257-1.187L13 10.005v10.21c0 1.04-.878 1.785-1.834 1.785H7.833c-.291 0-.575-.07-.83-.195A1.849 1.849 0 0 1 6.18 22H2.821C1.857 22 1 21.241 1 20.214V3.786ZM3 4v11h3V4H3Zm0 16v-3h3v3H3Zm15.075-.04-.814-2.712 2.874-.862.813 2.712-2.873.862Zm1.485-5.49-2.874.862-2.634-8.782 2.873-.862 2.635 8.782ZM8 20V6h3v14H8Z" clip-rule="evenodd"/></symbol><symbol id="icon-eds-i-calendar-acceptance-medium" viewBox="0 0 24 24"><path d="M17 2a1 1 0 0 1 1 1v1h1.5C20.817 4 22 5.183 22 6.5v13c0 1.317-1.183 2.5-2.5 2.5h-15C3.183 22 2 20.817 2 19.5v-13C2 5.183 3.183 4 4.5 4a1 1 0 1 1 0 2c-.212 0-.5.288-.5.5v13c0 .212.288.5.5.5h15c.212 0 .5-.288.5-.5v-13c0-.212-.288-.5-.5-.5H18v1a1 1 0 0 1-2 0V3a1 1 0 0 1 1-1Zm-.534 7.747a1 1 0 0 1 .094 1.412l-4.846 5.538a1 1 0 0 1-1.352.141l-2.77-2.076a1 1 0 0 1 1.2-1.6l2.027 1.519 4.236-4.84a1 1 0 0 1 1.411-.094ZM7.5 2a1 1 0 0 1 1 1v1H14a1 1 0 0 1 0 2H8.5v1a1 1 0 1 1-2 0V3a1 1 0 0 1 1-1Z"/></symbol><symbol id="icon-eds-i-calendar-date-medium" viewBox="0 0 24 24"><path d="M17 2a1 1 0 0 1 1 1v1h1.5C20.817 4 22 5.183 22 6.5v13c0 1.317-1.183 2.5-2.5 2.5h-15C3.183 22 2 20.817 2 19.5v-13C2 5.183 3.183 4 4.5 4a1 1 0 1 1 0 2c-.212 0-.5.288-.5.5v13c0 .212.288.5.5.5h15c.212 0 .5-.288.5-.5v-13c0-.212-.288-.5-.5-.5H18v1a1 1 0 0 1-2 0V3a1 1 0 0 1 1-1ZM8 15a1 1 0 1 1 0 2 1 1 0 0 1 0-2Zm4 0a1 1 0 1 1 0 2 1 1 0 0 1 0-2Zm-4-4a1 1 0 1 1 0 2 1 1 0 0 1 0-2Zm4 0a1 1 0 1 1 0 2 1 1 0 0 1 0-2Zm4 0a1 1 0 1 1 0 2 1 1 0 0 1 0-2ZM7.5 2a1 1 0 0 1 1 1v1H14a1 1 0 0 1 0 2H8.5v1a1 1 0 1 1-2 0V3a1 1 0 0 1 1-1Z"/></symbol><symbol id="icon-eds-i-calendar-decision-medium" viewBox="0 0 24 24"><path d="M17 2a1 1 0 0 1 1 1v1h1.5C20.817 4 22 5.183 22 6.5v13c0 1.317-1.183 2.5-2.5 2.5h-15C3.183 22 2 20.817 2 19.5v-13C2 5.183 3.183 4 4.5 4a1 1 0 1 1 0 2c-.212 0-.5.288-.5.5v13c0 .212.288.5.5.5h15c.212 0 .5-.288.5-.5v-13c0-.212-.288-.5-.5-.5H18v1a1 1 0 0 1-2 0V3a1 1 0 0 1 1-1Zm-2.935 8.246 2.686 2.645c.34.335.34.883 0 1.218l-2.686 2.645a.858.858 0 0 1-1.213-.009.854.854 0 0 1 .009-1.21l1.05-1.035H7.984a.992.992 0 0 1-.984-1c0-.552.44-1 .984-1h5.928l-1.051-1.036a.854.854 0 0 1-.085-1.121l.076-.088a.858.858 0 0 1 1.213-.009ZM7.5 2a1 1 0 0 1 1 1v1H14a1 1 0 0 1 0 2H8.5v1a1 1 0 1 1-2 0V3a1 1 0 0 1 1-1Z"/></symbol><symbol id="icon-eds-i-calendar-impact-factor-medium" viewBox="0 0 24 24"><path d="M17 2a1 1 0 0 1 1 1v1h1.5C20.817 4 22 5.183 22 6.5v13c0 1.317-1.183 2.5-2.5 2.5h-15C3.183 22 2 20.817 2 19.5v-13C2 5.183 3.183 4 4.5 4a1 1 0 1 1 0 2c-.212 0-.5.288-.5.5v13c0 .212.288.5.5.5h15c.212 0 .5-.288.5-.5v-13c0-.212-.288-.5-.5-.5H18v1a1 1 0 0 1-2 0V3a1 1 0 0 1 1-1Zm-3.2 6.924a.48.48 0 0 1 .125.544l-1.52 3.283h2.304c.27 0 .491.215.491.483a.477.477 0 0 1-.13.327l-4.18 4.484a.498.498 0 0 1-.69.031.48.48 0 0 1-.125-.544l1.52-3.284H9.291a.487.487 0 0 1-.491-.482c0-.121.047-.238.13-.327l4.18-4.484a.498.498 0 0 1 .69-.031ZM7.5 2a1 1 0 0 1 1 1v1H14a1 1 0 0 1 0 2H8.5v1a1 1 0 1 1-2 0V3a1 1 0 0 1 1-1Z"/></symbol><symbol id="icon-eds-i-call-papers-medium" viewBox="0 0 24 24"><g><path d="m20.707 2.883-1.414 1.414a1 1 0 0 0 1.414 1.414l1.414-1.414a1 1 0 0 0-1.414-1.414Z"/><path d="M6 16.054c0 2.026 1.052 2.943 3 2.943a1 1 0 1 1 0 2c-2.996 0-5-1.746-5-4.943v-1.227a4.068 4.068 0 0 1-1.83-1.189 4.553 4.553 0 0 1-.87-1.455 4.868 4.868 0 0 1-.3-1.686c0-1.17.417-2.298 1.17-3.14.38-.426.834-.767 1.338-1 .51-.237 1.06-.36 1.617-.36L6.632 6H7l7.932-2.895A2.363 2.363 0 0 1 18 5.36v9.28a2.36 2.36 0 0 1-3.069 2.25l.084.03L7 14.997H6v1.057Zm9.637-11.057a.415.415 0 0 0-.083.008L8 7.638v5.536l7.424 1.786.104.02c.035.01.072.02.109.02.2 0 .363-.16.363-.36V5.36c0-.2-.163-.363-.363-.363Zm-9.638 3h-.874a1.82 1.82 0 0 0-.625.111l-.15.063a2.128 2.128 0 0 0-.689.517c-.42.47-.661 1.123-.661 1.81 0 .34.06.678.176.992.114.308.28.585.485.816.4.447.925.691 1.464.691h.874v-5Z" clip-rule="evenodd"/><path d="M20 8.997h2a1 1 0 1 1 0 2h-2a1 1 0 1 1 0-2ZM20.707 14.293l1.414 1.414a1 1 0 0 1-1.414 1.414l-1.414-1.414a1 1 0 0 1 1.414-1.414Z"/></g></symbol><symbol id="icon-eds-i-card-medium" viewBox="0 0 24 24"><path d="M19.615 2c.315 0 .716.067 1.14.279.76.38 1.245 1.107 1.245 2.106v15.23c0 .315-.067.716-.279 1.14-.38.76-1.107 1.245-2.106 1.245H4.385a2.56 2.56 0 0 1-1.14-.279C2.485 21.341 2 20.614 2 19.615V4.385c0-.315.067-.716.279-1.14C2.659 2.485 3.386 2 4.385 2h15.23Zm0 2H4.385c-.213 0-.265.034-.317.14A.71.71 0 0 0 4 4.385v15.23c0 .213.034.265.14.317a.71.71 0 0 0 .245.068h15.23c.213 0 .265-.034.317-.14a.71.71 0 0 0 .068-.245V4.385c0-.213-.034-.265-.14-.317A.71.71 0 0 0 19.615 4ZM17 16a1 1 0 0 1 0 2H7a1 1 0 0 1 0-2h10Zm0-3a1 1 0 0 1 0 2H7a1 1 0 0 1 0-2h10Zm-.5-7A1.5 1.5 0 0 1 18 7.5v3a1.5 1.5 0 0 1-1.5 1.5h-9A1.5 1.5 0 0 1 6 10.5v-3A1.5 1.5 0 0 1 7.5 6h9ZM16 8H8v2h8V8Z"/></symbol><symbol id="icon-eds-i-cart-medium" viewBox="0 0 24 24"><path d="M5.76 1a1 1 0 0 1 .994.902L7.155 6h13.34c.18 0 .358.02.532.057l.174.045a2.5 2.5 0 0 1 1.693 3.103l-2.069 7.03c-.36 1.099-1.398 1.823-2.49 1.763H8.65c-1.272.015-2.352-.927-2.546-2.244L4.852 3H2a1 1 0 0 1-.993-.883L1 2a1 1 0 0 1 1-1h3.76Zm2.328 14.51a.555.555 0 0 0 .55.488l9.751.001a.533.533 0 0 0 .527-.357l2.059-7a.5.5 0 0 0-.48-.642H7.351l.737 7.51ZM18 19a2 2 0 1 1 0 4 2 2 0 0 1 0-4ZM8 19a2 2 0 1 1 0 4 2 2 0 0 1 0-4Z"/></symbol><symbol id="icon-eds-i-check-circle-medium" viewBox="0 0 24 24"><path d="M12 1c6.075 0 11 4.925 11 11s-4.925 11-11 11S1 18.075 1 12 5.925 1 12 1Zm0 2a9 9 0 1 0 0 18 9 9 0 0 0 0-18Zm5.125 4.72a1 1 0 0 1 .156 1.405l-6 7.5a1 1 0 0 1-1.421.143l-3-2.5a1 1 0 0 1 1.28-1.536l2.217 1.846 5.362-6.703a1 1 0 0 1 1.406-.156Z"/></symbol><symbol id="icon-eds-i-check-filled-medium" viewBox="0 0 24 24"><path d="M12 1c6.075 0 11 4.925 11 11s-4.925 11-11 11S1 18.075 1 12 5.925 1 12 1Zm5.125 6.72a1 1 0 0 0-1.406.155l-5.362 6.703-2.217-1.846a1 1 0 1 0-1.28 1.536l3 2.5a1 1 0 0 0 1.42-.143l6-7.5a1 1 0 0 0-.155-1.406Z"/></symbol><symbol id="icon-eds-i-chevron-down-medium" viewBox="0 0 24 24"><path d="M3.305 8.28a1 1 0 0 0-.024 1.415l7.495 7.762c.314.345.757.543 1.224.543.467 0 .91-.198 1.204-.522l7.515-7.783a1 1 0 1 0-1.438-1.39L12 15.845l-7.28-7.54A1 1 0 0 0 3.4 8.2l-.096.082Z"/></symbol><symbol id="icon-eds-i-chevron-down-small" viewBox="0 0 16 16"><path d="M13.692 5.278a1 1 0 0 1 .03 1.414L9.103 11.51a1.491 1.491 0 0 1-2.188.019L2.278 6.692a1 1 0 0 1 1.444-1.384L8 9.771l4.278-4.463a1 1 0 0 1 1.318-.111l.096.081Z"/></symbol><symbol id="icon-eds-i-chevron-left-medium" viewBox="0 0 24 24"><path d="M15.72 3.305a1 1 0 0 0-1.415-.024l-7.762 7.495A1.655 1.655 0 0 0 6 12c0 .467.198.91.522 1.204l7.783 7.515a1 1 0 1 0 1.39-1.438L8.155 12l7.54-7.28A1 1 0 0 0 15.8 3.4l-.082-.096Z"/></symbol><symbol id="icon-eds-i-chevron-left-small" viewBox="0 0 16 16"><path d="M10.722 2.308a1 1 0 0 0-1.414-.03L4.49 6.897a1.491 1.491 0 0 0-.019 2.188l4.838 4.637a1 1 0 1 0 1.384-1.444L6.229 8l4.463-4.278a1 1 0 0 0 .111-1.318l-.081-.096Z"/></symbol><symbol id="icon-eds-i-chevron-right-medium" viewBox="0 0 24 24"><path d="M8.28 3.305a1 1 0 0 1 1.415-.024l7.762 7.495c.345.314.543.757.543 1.224 0 .467-.198.91-.522 1.204l-7.783 7.515a1 1 0 1 1-1.39-1.438L15.845 12l-7.54-7.28A1 1 0 0 1 8.2 3.4l.082-.096Z"/></symbol><symbol id="icon-eds-i-chevron-right-small" viewBox="0 0 16 16"><path d="M5.278 2.308a1 1 0 0 1 1.414-.03l4.819 4.619a1.491 1.491 0 0 1 .019 2.188l-4.838 4.637a1 1 0 1 1-1.384-1.444L9.771 8 5.308 3.722a1 1 0 0 1-.111-1.318l.081-.096Z"/></symbol><symbol id="icon-eds-i-chevron-up-medium" viewBox="0 0 24 24"><path d="M20.695 15.72a1 1 0 0 0 .024-1.415l-7.495-7.762A1.655 1.655 0 0 0 12 6c-.467 0-.91.198-1.204.522l-7.515 7.783a1 1 0 1 0 1.438 1.39L12 8.155l7.28 7.54a1 1 0 0 0 1.319.106l.096-.082Z"/></symbol><symbol id="icon-eds-i-chevron-up-small" viewBox="0 0 16 16"><path d="M13.692 10.722a1 1 0 0 0 .03-1.414L9.103 4.49a1.491 1.491 0 0 0-2.188-.019L2.278 9.308a1 1 0 0 0 1.444 1.384L8 6.229l4.278 4.463a1 1 0 0 0 1.318.111l.096-.081Z"/></symbol><symbol id="icon-eds-i-citations-medium" viewBox="0 0 24 24"><path d="M15.59 1a1 1 0 0 1 .706.291l5.41 5.385a1 1 0 0 1 .294.709v13.077c0 .674-.269 1.32-.747 1.796a2.549 2.549 0 0 1-1.798.742h-5.843a1 1 0 1 1 0-2h5.843a.549.549 0 0 0 .387-.16.535.535 0 0 0 .158-.378V7.8L15.178 3H5.545a.543.543 0 0 0-.538.451L5 3.538v8.607a1 1 0 0 1-2 0V3.538A2.542 2.542 0 0 1 5.545 1h10.046ZM5.483 14.35c.197.26.17.62-.049.848l-.095.083-.016.011c-.36.24-.628.45-.804.634-.393.409-.59.93-.59 1.562.077-.019.192-.028.345-.028.442 0 .84.158 1.195.474.355.316.532.716.532 1.2 0 .501-.173.9-.518 1.198-.345.298-.767.446-1.266.446-.672 0-1.209-.195-1.612-.585-.403-.39-.604-.976-.604-1.757 0-.744.11-1.39.33-1.938.222-.549.49-1.009.807-1.38a4.28 4.28 0 0 1 .992-.88c.07-.043.148-.087.232-.133a.881.881 0 0 1 1.121.245Zm5 0c.197.26.17.62-.049.848l-.095.083-.016.011c-.36.24-.628.45-.804.634-.393.409-.59.93-.59 1.562.077-.019.192-.028.345-.028.442 0 .84.158 1.195.474.355.316.532.716.532 1.2 0 .501-.173.9-.518 1.198-.345.298-.767.446-1.266.446-.672 0-1.209-.195-1.612-.585-.403-.39-.604-.976-.604-1.757 0-.744.11-1.39.33-1.938.222-.549.49-1.009.807-1.38a4.28 4.28 0 0 1 .992-.88c.07-.043.148-.087.232-.133a.881.881 0 0 1 1.121.245Z"/></symbol><symbol id="icon-eds-i-clipboard-check-medium" viewBox="0 0 24 24"><path d="M14.4 1c1.238 0 2.274.865 2.536 2.024L18.5 3C19.886 3 21 4.14 21 5.535v14.93C21 21.86 19.886 23 18.5 23h-13C4.114 23 3 21.86 3 20.465V5.535C3 4.14 4.114 3 5.5 3h1.57c.27-1.147 1.3-2 2.53-2h4.8Zm4.115 4-1.59.024A2.601 2.601 0 0 1 14.4 7H9.6c-1.23 0-2.26-.853-2.53-2H5.5c-.27 0-.5.234-.5.535v14.93c0 .3.23.535.5.535h13c.27 0 .5-.234.5-.535V5.535c0-.3-.23-.535-.485-.535Zm-1.909 4.205a1 1 0 0 1 .19 1.401l-5.334 7a1 1 0 0 1-1.344.23l-2.667-1.75a1 1 0 1 1 1.098-1.672l1.887 1.238 4.769-6.258a1 1 0 0 1 1.401-.19ZM14.4 3H9.6a.6.6 0 0 0-.6.6v.8a.6.6 0 0 0 .6.6h4.8a.6.6 0 0 0 .6-.6v-.8a.6.6 0 0 0-.6-.6Z"/></symbol><symbol id="icon-eds-i-clipboard-report-medium" viewBox="0 0 24 24"><path d="M14.4 1c1.238 0 2.274.865 2.536 2.024L18.5 3C19.886 3 21 4.14 21 5.535v14.93C21 21.86 19.886 23 18.5 23h-13C4.114 23 3 21.86 3 20.465V5.535C3 4.14 4.114 3 5.5 3h1.57c.27-1.147 1.3-2 2.53-2h4.8Zm4.115 4-1.59.024A2.601 2.601 0 0 1 14.4 7H9.6c-1.23 0-2.26-.853-2.53-2H5.5c-.27 0-.5.234-.5.535v14.93c0 .3.23.535.5.535h13c.27 0 .5-.234.5-.535V5.535c0-.3-.23-.535-.485-.535Zm-2.658 10.929a1 1 0 0 1 0 2H8a1 1 0 0 1 0-2h7.857Zm0-3.929a1 1 0 0 1 0 2H8a1 1 0 0 1 0-2h7.857ZM14.4 3H9.6a.6.6 0 0 0-.6.6v.8a.6.6 0 0 0 .6.6h4.8a.6.6 0 0 0 .6-.6v-.8a.6.6 0 0 0-.6-.6Z"/></symbol><symbol id="icon-eds-i-close-medium" viewBox="0 0 24 24"><path d="M12 1c6.075 0 11 4.925 11 11s-4.925 11-11 11S1 18.075 1 12 5.925 1 12 1Zm0 2a9 9 0 1 0 0 18 9 9 0 0 0 0-18ZM8.707 7.293 12 10.585l3.293-3.292a1 1 0 0 1 1.414 1.414L13.415 12l3.292 3.293a1 1 0 0 1-1.414 1.414L12 13.415l-3.293 3.292a1 1 0 1 1-1.414-1.414L10.585 12 7.293 8.707a1 1 0 0 1 1.414-1.414Z"/></symbol><symbol id="icon-eds-i-cloud-upload-medium" viewBox="0 0 24 24"><path d="m12.852 10.011.028-.004L13 10l.075.003.126.017.086.022.136.052.098.052.104.074.082.073 3 3a1 1 0 0 1 0 1.414l-.094.083a1 1 0 0 1-1.32-.083L14 13.416V20a1 1 0 0 1-2 0v-6.586l-1.293 1.293a1 1 0 0 1-1.32.083l-.094-.083a1 1 0 0 1 0-1.414l3-3 .112-.097.11-.071.114-.054.105-.035.118-.025Zm.587-7.962c3.065.362 5.497 2.662 5.992 5.562l.013.085.207.073c2.117.782 3.496 2.845 3.337 5.097l-.022.226c-.297 2.561-2.503 4.491-5.124 4.502a1 1 0 1 1-.009-2c1.619-.007 2.967-1.186 3.147-2.733.179-1.542-.86-2.979-2.487-3.353-.512-.149-.894-.579-.981-1.165-.21-2.237-2-4.035-4.308-4.308-2.31-.273-4.497 1.06-5.25 3.19l-.049.113c-.234.468-.718.756-1.176.743-1.418.057-2.689.857-3.32 2.084a3.668 3.668 0 0 0 .262 3.798c.796 1.136 2.169 1.764 3.583 1.635a1 1 0 1 1 .182 1.992c-2.125.194-4.193-.753-5.403-2.48a5.668 5.668 0 0 1-.403-5.86c.85-1.652 2.449-2.79 4.323-3.092l.287-.039.013-.028c1.207-2.741 4.125-4.404 7.186-4.042Z"/></symbol><symbol id="icon-eds-i-collection-medium" viewBox="0 0 24 24"><path d="M21 7a1 1 0 0 1 1 1v12.5a2.5 2.5 0 0 1-2.5 2.5H8a1 1 0 0 1 0-2h11.5a.5.5 0 0 0 .5-.5V8a1 1 0 0 1 1-1Zm-5.5-5A2.5 2.5 0 0 1 18 4.5v12a2.5 2.5 0 0 1-2.5 2.5h-11A2.5 2.5 0 0 1 2 16.5v-12A2.5 2.5 0 0 1 4.5 2h11Zm0 2h-11a.5.5 0 0 0-.5.5v12a.5.5 0 0 0 .5.5h11a.5.5 0 0 0 .5-.5v-12a.5.5 0 0 0-.5-.5ZM13 13a1 1 0 0 1 0 2H7a1 1 0 0 1 0-2h6Zm0-3.5a1 1 0 0 1 0 2H7a1 1 0 0 1 0-2h6ZM13 6a1 1 0 0 1 0 2H7a1 1 0 1 1 0-2h6Z"/></symbol><symbol id="icon-eds-i-conference-series-medium" viewBox="0 0 24 24"><path fill-rule="evenodd" d="M4.5 2A2.5 2.5 0 0 0 2 4.5v11A2.5 2.5 0 0 0 4.5 18h2.37l-2.534 2.253a1 1 0 0 0 1.328 1.494L9.88 18H11v3a1 1 0 1 0 2 0v-3h1.12l4.216 3.747a1 1 0 0 0 1.328-1.494L17.13 18h2.37a2.5 2.5 0 0 0 2.5-2.5v-11A2.5 2.5 0 0 0 19.5 2h-15ZM20 6V4.5a.5.5 0 0 0-.5-.5h-15a.5.5 0 0 0-.5.5V6h16ZM4 8v7.5a.5.5 0 0 0 .5.5h15a.5.5 0 0 0 .5-.5V8H4Z" clip-rule="evenodd"/></symbol><symbol id="icon-eds-i-delivery-medium" viewBox="0 0 24 24"><path d="M8.51 20.598a3.037 3.037 0 0 1-3.02 0A2.968 2.968 0 0 1 4.161 19L3.5 19A2.5 2.5 0 0 1 1 16.5v-11A2.5 2.5 0 0 1 3.5 3h10a2.5 2.5 0 0 1 2.45 2.004L16 5h2.527c.976 0 1.855.585 2.27 1.49l2.112 4.62a1 1 0 0 1 .091.416v4.856C23 17.814 21.889 19 20.484 19h-.523a1.01 1.01 0 0 1-.121-.007 2.96 2.96 0 0 1-1.33 1.605 3.037 3.037 0 0 1-3.02 0A2.968 2.968 0 0 1 14.161 19H9.838a2.968 2.968 0 0 1-1.327 1.597Zm-2.024-3.462a.955.955 0 0 0-.481.73L5.999 18l.001.022a.944.944 0 0 0 .388.777l.098.065c.316.181.712.181 1.028 0A.97.97 0 0 0 8 17.978a.95.95 0 0 0-.486-.842 1.037 1.037 0 0 0-1.028 0Zm10 0a.955.955 0 0 0-.481.73l-.005.156a.944.944 0 0 0 .388.777l.098.065c.316.181.712.181 1.028 0a.97.97 0 0 0 .486-.886.95.95 0 0 0-.486-.842 1.037 1.037 0 0 0-1.028 0ZM21 12h-5v3.17a3.038 3.038 0 0 1 2.51.232 2.993 2.993 0 0 1 1.277 1.45l.058.155.058-.005.581-.002c.27 0 .516-.263.516-.618V12Zm-7.5-7h-10a.5.5 0 0 0-.5.5v11a.5.5 0 0 0 .5.5h.662a2.964 2.964 0 0 1 1.155-1.491l.172-.107a3.037 3.037 0 0 1 3.022 0A2.987 2.987 0 0 1 9.843 17H13.5a.5.5 0 0 0 .5-.5v-11a.5.5 0 0 0-.5-.5Zm5.027 2H16v3h4.203l-1.224-2.677a.532.532 0 0 0-.375-.316L18.527 7Z"/></symbol><symbol id="icon-eds-i-download-medium" viewBox="0 0 24 24"><path d="M22 18.5a3.5 3.5 0 0 1-3.5 3.5h-13A3.5 3.5 0 0 1 2 18.5V18a1 1 0 0 1 2 0v.5A1.5 1.5 0 0 0 5.5 20h13a1.5 1.5 0 0 0 1.5-1.5V18a1 1 0 0 1 2 0v.5Zm-3.293-7.793-6 6-.063.059-.093.069-.081.048-.105.049-.104.034-.056.013-.118.017L12 17l-.076-.003-.122-.017-.113-.03-.085-.032-.063-.03-.098-.058-.06-.043-.05-.043-6.04-6.037a1 1 0 0 1 1.414-1.414l4.294 4.29L11 3a1 1 0 0 1 2 0l.001 10.585 4.292-4.292a1 1 0 0 1 1.32-.083l.094.083a1 1 0 0 1 0 1.414Z"/></symbol><symbol id="icon-eds-i-edit-medium" viewBox="0 0 24 24"><path d="M17.149 2a2.38 2.38 0 0 1 1.699.711l2.446 2.46a2.384 2.384 0 0 1 .005 3.38L10.01 19.906a1 1 0 0 1-.434.257l-6.3 1.8a1 1 0 0 1-1.237-1.237l1.8-6.3a1 1 0 0 1 .257-.434L15.443 2.718A2.385 2.385 0 0 1 17.15 2Zm-3.874 5.689-7.586 7.536-1.234 4.319 4.318-1.234 7.54-7.582-3.038-3.039ZM17.149 4a.395.395 0 0 0-.286.126L14.695 6.28l3.029 3.029 2.162-2.173a.384.384 0 0 0 .106-.197L20 6.864c0-.103-.04-.2-.119-.278l-2.457-2.47A.385.385 0 0 0 17.149 4Z"/></symbol><symbol id="icon-eds-i-education-medium" viewBox="0 0 24 24"><path fill-rule="evenodd" d="M12.41 2.088a1 1 0 0 0-.82 0l-10 4.5a1 1 0 0 0 0 1.824L3 9.047v7.124A3.001 3.001 0 0 0 4 22a3 3 0 0 0 1-5.83V9.948l1 .45V14.5a1 1 0 0 0 .087.408L7 14.5c-.913.408-.912.41-.912.41l.001.003.003.006.007.015a1.988 1.988 0 0 0 .083.16c.054.097.131.225.236.373.21.297.53.68.993 1.057C8.351 17.292 9.824 18 12 18c2.176 0 3.65-.707 4.589-1.476.463-.378.783-.76.993-1.057a4.162 4.162 0 0 0 .319-.533l.007-.015.003-.006v-.003h.002s0-.002-.913-.41l.913.408A1 1 0 0 0 18 14.5v-4.103l4.41-1.985a1 1 0 0 0 0-1.824l-10-4.5ZM16 11.297l-3.59 1.615a1 1 0 0 1-.82 0L8 11.297v2.94a3.388 3.388 0 0 0 .677.739C9.267 15.457 10.294 16 12 16s2.734-.543 3.323-1.024a3.388 3.388 0 0 0 .677-.739v-2.94ZM4.437 7.5 12 4.097 19.563 7.5 12 10.903 4.437 7.5ZM3 19a1 1 0 1 1 2 0 1 1 0 0 1-2 0Z" clip-rule="evenodd"/></symbol><symbol id="icon-eds-i-error-diamond-medium" viewBox="0 0 24 24"><path d="M12.002 1c.702 0 1.375.279 1.871.775l8.35 8.353a2.646 2.646 0 0 1 .001 3.744l-8.353 8.353a2.646 2.646 0 0 1-3.742 0l-8.353-8.353a2.646 2.646 0 0 1 0-3.744l8.353-8.353.156-.142c.424-.362.952-.58 1.507-.625l.21-.008Zm0 2a.646.646 0 0 0-.38.123l-.093.08-8.34 8.34a.646.646 0 0 0-.18.355L3 12c0 .171.068.336.19.457l8.353 8.354a.646.646 0 0 0 .914 0l8.354-8.354a.646.646 0 0 0-.001-.914l-8.351-8.354A.646.646 0 0 0 12.002 3ZM12 14.5a1.5 1.5 0 0 1 .144 2.993L12 17.5a1.5 1.5 0 0 1 0-3ZM12 6a1 1 0 0 1 1 1v5a1 1 0 0 1-2 0V7a1 1 0 0 1 1-1Z"/></symbol><symbol id="icon-eds-i-error-filled-medium" viewBox="0 0 24 24"><path d="M12.002 1c.702 0 1.375.279 1.871.775l8.35 8.353a2.646 2.646 0 0 1 .001 3.744l-8.353 8.353a2.646 2.646 0 0 1-3.742 0l-8.353-8.353a2.646 2.646 0 0 1 0-3.744l8.353-8.353.156-.142c.424-.362.952-.58 1.507-.625l.21-.008ZM12 14.5a1.5 1.5 0 0 0 0 3l.144-.007A1.5 1.5 0 0 0 12 14.5ZM12 6a1 1 0 0 0-1 1v5a1 1 0 0 0 2 0V7a1 1 0 0 0-1-1Z"/></symbol><symbol id="icon-eds-i-external-link-medium" viewBox="0 0 24 24"><path d="M9 2a1 1 0 1 1 0 2H4.6c-.371 0-.6.209-.6.5v15c0 .291.229.5.6.5h14.8c.371 0 .6-.209.6-.5V15a1 1 0 0 1 2 0v4.5c0 1.438-1.162 2.5-2.6 2.5H4.6C3.162 22 2 20.938 2 19.5v-15C2 3.062 3.162 2 4.6 2H9Zm6 0h6l.075.003.126.017.111.03.111.044.098.052.096.067.09.08c.036.035.068.073.097.112l.071.11.054.114.035.105.03.148L22 3v6a1 1 0 0 1-2 0V5.414l-6.693 6.693a1 1 0 0 1-1.414-1.414L18.584 4H15a1 1 0 0 1-.993-.883L14 3a1 1 0 0 1 1-1Z"/></symbol><symbol id="icon-eds-i-external-link-small" viewBox="0 0 16 16"><path d="M5 1a1 1 0 1 1 0 2l-2-.001V13L13 13v-2a1 1 0 0 1 2 0v2c0 1.15-.93 2-2.067 2H3.067C1.93 15 1 14.15 1 13V3c0-1.15.93-2 2.067-2H5Zm4 0h5l.075.003.126.017.111.03.111.044.098.052.096.067.09.08.044.047.073.093.051.083.054.113.035.105.03.148L15 2v5a1 1 0 0 1-2 0V4.414L9.107 8.307a1 1 0 0 1-1.414-1.414L11.584 3H9a1 1 0 0 1-.993-.883L8 2a1 1 0 0 1 1-1Z"/></symbol><symbol id="icon-eds-i-file-download-medium" viewBox="0 0 24 24"><path d="M14.5 1a1 1 0 0 1 .707.293l5.5 5.5A1 1 0 0 1 21 7.5v12.962A2.542 2.542 0 0 1 18.455 23H5.545A2.542 2.542 0 0 1 3 20.462V3.538A2.542 2.542 0 0 1 5.545 1H14.5Zm-.415 2h-8.54A.542.542 0 0 0 5 3.538v16.924c0 .296.243.538.545.538h12.91a.542.542 0 0 0 .545-.538V7.915L14.085 3ZM12 7a1 1 0 0 1 1 1v6.585l2.293-2.292a1 1 0 0 1 1.32-.083l.094.083a1 1 0 0 1 0 1.414l-4 4a1.008 1.008 0 0 1-.112.097l-.11.071-.114.054-.105.035-.149.03L12 18l-.075-.003-.126-.017-.111-.03-.111-.044-.098-.052-.096-.067-.09-.08-4-4a1 1 0 0 1 1.414-1.414L11 14.585V8a1 1 0 0 1 1-1Z"/></symbol><symbol id="icon-eds-i-file-report-medium" viewBox="0 0 24 24"><path d="M14.5 1a1 1 0 0 1 .707.293l5.5 5.5A1 1 0 0 1 21 7.5v12.962c0 .674-.269 1.32-.747 1.796a2.549 2.549 0 0 1-1.798.742H5.545c-.674 0-1.32-.267-1.798-.742A2.535 2.535 0 0 1 3 20.462V3.538A2.542 2.542 0 0 1 5.545 1H14.5Zm-.415 2h-8.54A.542.542 0 0 0 5 3.538v16.924c0 .142.057.278.158.379.102.102.242.159.387.159h12.91a.549.549 0 0 0 .387-.16.535.535 0 0 0 .158-.378V7.915L14.085 3ZM16 17a1 1 0 0 1 0 2H8a1 1 0 0 1 0-2h8Zm0-3a1 1 0 0 1 0 2H8a1 1 0 0 1 0-2h8Zm-4.793-6.207L13 9.585l1.793-1.792a1 1 0 0 1 1.32-.083l.094.083a1 1 0 0 1 0 1.414l-2.5 2.5a1 1 0 0 1-1.414 0L10.5 9.915l-1.793 1.792a1 1 0 0 1-1.32.083l-.094-.083a1 1 0 0 1 0-1.414l2.5-2.5a1 1 0 0 1 1.414 0Z"/></symbol><symbol id="icon-eds-i-file-text-medium" viewBox="0 0 24 24"><path d="M14.5 1a1 1 0 0 1 .707.293l5.5 5.5A1 1 0 0 1 21 7.5v12.962A2.542 2.542 0 0 1 18.455 23H5.545A2.542 2.542 0 0 1 3 20.462V3.538A2.542 2.542 0 0 1 5.545 1H14.5Zm-.415 2h-8.54A.542.542 0 0 0 5 3.538v16.924c0 .296.243.538.545.538h12.91a.542.542 0 0 0 .545-.538V7.915L14.085 3ZM16 15a1 1 0 0 1 0 2H8a1 1 0 0 1 0-2h8Zm0-4a1 1 0 0 1 0 2H8a1 1 0 0 1 0-2h8Zm-5-4a1 1 0 0 1 0 2H8a1 1 0 1 1 0-2h3Z"/></symbol><symbol id="icon-eds-i-file-upload-medium" viewBox="0 0 24 24"><path d="M14.5 1a1 1 0 0 1 .707.293l5.5 5.5A1 1 0 0 1 21 7.5v12.962A2.542 2.542 0 0 1 18.455 23H5.545A2.542 2.542 0 0 1 3 20.462V3.538A2.542 2.542 0 0 1 5.545 1H14.5Zm-.415 2h-8.54A.542.542 0 0 0 5 3.538v16.924c0 .296.243.538.545.538h12.91a.542.542 0 0 0 .545-.538V7.915L14.085 3Zm-2.233 4.011.058-.007L12 7l.075.003.126.017.111.03.111.044.098.052.104.074.082.073 4 4a1 1 0 0 1 0 1.414l-.094.083a1 1 0 0 1-1.32-.083L13 10.415V17a1 1 0 0 1-2 0v-6.585l-2.293 2.292a1 1 0 0 1-1.32.083l-.094-.083a1 1 0 0 1 0-1.414l4-4 .112-.097.11-.071.114-.054.105-.035.118-.025Z"/></symbol><symbol id="icon-eds-i-filter-medium" viewBox="0 0 24 24"><path d="M21 2a1 1 0 0 1 .82 1.573L15 13.314V18a1 1 0 0 1-.31.724l-.09.076-4 3A1 1 0 0 1 9 21v-7.684L2.18 3.573a1 1 0 0 1 .707-1.567L3 2h18Zm-1.921 2H4.92l5.9 8.427a1 1 0 0 1 .172.45L11 13v6l2-1.5V13a1 1 0 0 1 .117-.469l.064-.104L19.079 4Z"/></symbol><symbol id="icon-eds-i-funding-medium" viewBox="0 0 24 24"><path fill-rule="evenodd" d="M23 8A7 7 0 1 0 9 8a7 7 0 0 0 14 0ZM9.006 12.225A4.07 4.07 0 0 0 6.12 11.02H2a.979.979 0 1 0 0 1.958h4.12c.558 0 1.094.222 1.489.617l2.207 2.288c.27.27.27.687.012.944a.656.656 0 0 1-.928 0L7.744 15.67a.98.98 0 0 0-1.386 1.384l1.157 1.158c.535.536 1.244.791 1.946.765l.041.002h6.922c.874 0 1.597.748 1.597 1.688 0 .203-.146.354-.309.354H7.755c-.487 0-.96-.178-1.339-.504L2.64 17.259a.979.979 0 0 0-1.28 1.482L5.137 22c.733.631 1.66.979 2.618.979h9.957c1.26 0 2.267-1.043 2.267-2.312 0-2.006-1.584-3.646-3.555-3.646h-4.529a2.617 2.617 0 0 0-.681-2.509l-2.208-2.287ZM16 3a5 5 0 1 0 0 10 5 5 0 0 0 0-10Zm.979 3.5a.979.979 0 1 0-1.958 0v3a.979.979 0 1 0 1.958 0v-3Z" clip-rule="evenodd"/></symbol><symbol id="icon-eds-i-hashtag-medium" viewBox="0 0 24 24"><path d="M12 1c6.075 0 11 4.925 11 11s-4.925 11-11 11S1 18.075 1 12 5.925 1 12 1Zm0 2a9 9 0 1 0 0 18 9 9 0 0 0 0-18ZM9.52 18.189a1 1 0 1 1-1.964-.378l.437-2.274H6a1 1 0 1 1 0-2h2.378l.592-3.076H6a1 1 0 0 1 0-2h3.354l.51-2.65a1 1 0 1 1 1.964.378l-.437 2.272h3.04l.51-2.65a1 1 0 1 1 1.964.378l-.438 2.272H18a1 1 0 0 1 0 2h-1.917l-.592 3.076H18a1 1 0 0 1 0 2h-2.893l-.51 2.652a1 1 0 1 1-1.964-.378l.437-2.274h-3.04l-.51 2.652Zm.895-4.652h3.04l.591-3.076h-3.04l-.591 3.076Z"/></symbol><symbol id="icon-eds-i-home-medium" viewBox="0 0 24 24"><path d="M5 22a1 1 0 0 1-1-1v-8.586l-1.293 1.293a1 1 0 0 1-1.32.083l-.094-.083a1 1 0 0 1 0-1.414l10-10a1 1 0 0 1 1.414 0l10 10a1 1 0 0 1-1.414 1.414L20 12.415V21a1 1 0 0 1-1 1H5Zm7-17.585-6 5.999V20h5v-4a1 1 0 0 1 2 0v4h5v-9.585l-6-6Z"/></symbol><symbol id="icon-eds-i-image-medium" viewBox="0 0 24 24"><path d="M19.615 2A2.385 2.385 0 0 1 22 4.385v15.23A2.385 2.385 0 0 1 19.615 22H4.385A2.385 2.385 0 0 1 2 19.615V4.385A2.385 2.385 0 0 1 4.385 2h15.23Zm0 2H4.385A.385.385 0 0 0 4 4.385v15.23c0 .213.172.385.385.385h1.244l10.228-8.76a1 1 0 0 1 1.254-.037L20 13.392V4.385A.385.385 0 0 0 19.615 4Zm-3.07 9.283L8.703 20h10.912a.385.385 0 0 0 .385-.385v-3.713l-3.455-2.619ZM9.5 6a3.5 3.5 0 1 1 0 7 3.5 3.5 0 0 1 0-7Zm0 2a1.5 1.5 0 1 0 0 3 1.5 1.5 0 0 0 0-3Z"/></symbol><symbol id="icon-eds-i-impact-factor-medium" viewBox="0 0 24 24"><path d="M16.49 2.672c.74.694.986 1.765.632 2.712l-.04.1-1.549 3.54h1.477a2.496 2.496 0 0 1 2.485 2.34l.005.163c0 .618-.23 1.21-.642 1.675l-7.147 7.961a2.48 2.48 0 0 1-3.554.165 2.512 2.512 0 0 1-.633-2.712l.042-.103L9.108 15H7.46c-1.393 0-2.379-1.11-2.455-2.369L5 12.473c0-.593.142-1.145.628-1.692l7.307-7.944a2.48 2.48 0 0 1 3.555-.165ZM14.43 4.164l-7.33 7.97c-.083.093-.101.214-.101.34 0 .277.19.526.46.526h4.163l.097-.009c.015 0 .03.003.046.009.181.078.264.32.186.5l-2.554 5.817a.512.512 0 0 0 .127.552.48.48 0 0 0 .69-.033l7.155-7.97a.513.513 0 0 0 .13-.34.497.497 0 0 0-.49-.502h-3.988a.355.355 0 0 1-.328-.497l2.555-5.844a.512.512 0 0 0-.127-.552.48.48 0 0 0-.69.033Z"/></symbol><symbol id="icon-eds-i-info-circle-medium" viewBox="0 0 24 24"><path d="M12 1c6.075 0 11 4.925 11 11s-4.925 11-11 11S1 18.075 1 12 5.925 1 12 1Zm0 2a9 9 0 1 0 0 18 9 9 0 0 0 0-18Zm0 7a1 1 0 0 1 1 1v5h1.5a1 1 0 0 1 0 2h-5a1 1 0 0 1 0-2H11v-4h-.5a1 1 0 0 1-.993-.883L9.5 11a1 1 0 0 1 1-1H12Zm0-4.5a1.5 1.5 0 0 1 .144 2.993L12 8.5a1.5 1.5 0 0 1 0-3Z"/></symbol><symbol id="icon-eds-i-info-filled-medium" viewBox="0 0 24 24"><path d="M12 1c6.075 0 11 4.925 11 11s-4.925 11-11 11S1 18.075 1 12 5.925 1 12 1Zm0 9h-1.5a1 1 0 0 0-1 1l.007.117A1 1 0 0 0 10.5 12h.5v4H9.5a1 1 0 0 0 0 2h5a1 1 0 0 0 0-2H13v-5a1 1 0 0 0-1-1Zm0-4.5a1.5 1.5 0 0 0 0 3l.144-.007A1.5 1.5 0 0 0 12 5.5Z"/></symbol><symbol id="icon-eds-i-journal-medium" viewBox="0 0 24 24"><path d="M18.5 1A2.5 2.5 0 0 1 21 3.5v14a2.5 2.5 0 0 1-2.5 2.5h-13a.5.5 0 1 0 0 1H20a1 1 0 0 1 0 2H5.5A2.5 2.5 0 0 1 3 20.5v-17A2.5 2.5 0 0 1 5.5 1h13ZM7 3H5.5a.5.5 0 0 0-.5.5v14.549l.016-.002c.104-.02.211-.035.32-.042L5.5 18H7V3Zm11.5 0H9v15h9.5a.5.5 0 0 0 .5-.5v-14a.5.5 0 0 0-.5-.5ZM16 5a1 1 0 0 1 1 1v4a1 1 0 0 1-1 1h-5a1 1 0 0 1-1-1V6a1 1 0 0 1 1-1h5Zm-1 2h-3v2h3V7Z"/></symbol><symbol id="icon-eds-i-mail-medium" viewBox="0 0 24 24"><path d="M20.462 3C21.875 3 23 4.184 23 5.619v12.762C23 19.816 21.875 21 20.462 21H3.538C2.125 21 1 19.816 1 18.381V5.619C1 4.184 2.125 3 3.538 3h16.924ZM21 8.158l-7.378 6.258a2.549 2.549 0 0 1-3.253-.008L3 8.16v10.222c0 .353.253.619.538.619h16.924c.285 0 .538-.266.538-.619V8.158ZM20.462 5H3.538c-.264 0-.5.228-.534.542l8.65 7.334c.2.165.492.165.684.007l8.656-7.342-.001-.025c-.044-.3-.274-.516-.531-.516Z"/></symbol><symbol id="icon-eds-i-mail-send-medium" viewBox="0 0 24 24"><path d="M20.444 5a2.562 2.562 0 0 1 2.548 2.37l.007.078.001.123v7.858A2.564 2.564 0 0 1 20.444 18H9.556A2.564 2.564 0 0 1 7 15.429l.001-7.977.007-.082A2.561 2.561 0 0 1 9.556 5h10.888ZM21 9.331l-5.46 3.51a1 1 0 0 1-1.08 0L9 9.332v6.097c0 .317.251.571.556.571h10.888a.564.564 0 0 0 .556-.571V9.33ZM20.444 7H9.556a.543.543 0 0 0-.32.105l5.763 3.706 5.766-3.706a.543.543 0 0 0-.32-.105ZM4.308 5a1 1 0 1 1 0 2H2a1 1 0 1 1 0-2h2.308Zm0 5.5a1 1 0 0 1 0 2H2a1 1 0 0 1 0-2h2.308Zm0 5.5a1 1 0 0 1 0 2H2a1 1 0 0 1 0-2h2.308Z"/></symbol><symbol id="icon-eds-i-mentions-medium" viewBox="0 0 24 24"><path d="m9.452 1.293 5.92 5.92 2.92-2.92a1 1 0 0 1 1.415 1.414l-2.92 2.92 5.92 5.92a1 1 0 0 1 0 1.415 10.371 10.371 0 0 1-10.378 2.584l.652 3.258A1 1 0 0 1 12 23H2a1 1 0 0 1-.874-1.486l4.789-8.62C4.194 9.074 4.9 4.43 8.038 1.292a1 1 0 0 1 1.414 0Zm-2.355 13.59L3.699 21h7.081l-.689-3.442a10.392 10.392 0 0 1-2.775-2.396l-.22-.28Zm1.69-11.427-.07.09a8.374 8.374 0 0 0 11.737 11.737l.089-.071L8.787 3.456Z"/></symbol><symbol id="icon-eds-i-menu-medium" viewBox="0 0 24 24"><path d="M21 4a1 1 0 0 1 0 2H3a1 1 0 1 1 0-2h18Zm-4 7a1 1 0 0 1 0 2H3a1 1 0 0 1 0-2h14Zm4 7a1 1 0 0 1 0 2H3a1 1 0 0 1 0-2h18Z"/></symbol><symbol id="icon-eds-i-metrics-medium" viewBox="0 0 24 24"><path d="M3 22a1 1 0 0 1-1-1V3a1 1 0 0 1 1-1h6a1 1 0 0 1 1 1v7h4V8a1 1 0 0 1 1-1h6a1 1 0 0 1 1 1v13a1 1 0 0 1-.883.993L21 22H3Zm17-2V9h-4v11h4Zm-6-8h-4v8h4v-8ZM8 4H4v16h4V4Z"/></symbol><symbol id="icon-eds-i-news-medium" viewBox="0 0 24 24"><path d="M17.384 3c.975 0 1.77.787 1.77 1.762v13.333c0 .462.354.846.815.899l.107.006.109-.006a.915.915 0 0 0 .809-.794l.006-.105V8.19a1 1 0 0 1 2 0v9.905A2.914 2.914 0 0 1 20.077 21H3.538a2.547 2.547 0 0 1-1.644-.601l-.147-.135A2.516 2.516 0 0 1 1 18.476V4.762C1 3.787 1.794 3 2.77 3h14.614Zm-.231 2H3v13.476c0 .11.035.216.1.304l.054.063c.101.1.24.157.384.157l13.761-.001-.026-.078a2.88 2.88 0 0 1-.115-.655l-.004-.17L17.153 5ZM14 15.021a.979.979 0 1 1 0 1.958H6a.979.979 0 1 1 0-1.958h8Zm0-8c.54 0 .979.438.979.979v4c0 .54-.438.979-.979.979H6A.979.979 0 0 1 5.021 12V8c0-.54.438-.979.979-.979h8Zm-.98 1.958H6.979v2.041h6.041V8.979Z"/></symbol><symbol id="icon-eds-i-newsletter-medium" viewBox="0 0 24 24"><path d="M21 10a1 1 0 0 1 1 1v9.5a2.5 2.5 0 0 1-2.5 2.5h-15A2.5 2.5 0 0 1 2 20.5V11a1 1 0 0 1 2 0v.439l8 4.888 8-4.889V11a1 1 0 0 1 1-1Zm-1 3.783-7.479 4.57a1 1 0 0 1-1.042 0l-7.48-4.57V20.5a.5.5 0 0 0 .501.5h15a.5.5 0 0 0 .5-.5v-6.717ZM15 9a1 1 0 0 1 0 2H9a1 1 0 0 1 0-2h6Zm2.5-8A2.5 2.5 0 0 1 20 3.5V9a1 1 0 0 1-2 0V3.5a.5.5 0 0 0-.5-.5h-11a.5.5 0 0 0-.5.5V9a1 1 0 1 1-2 0V3.5A2.5 2.5 0 0 1 6.5 1h11ZM15 5a1 1 0 0 1 0 2H9a1 1 0 1 1 0-2h6Z"/></symbol><symbol id="icon-eds-i-notifcation-medium" viewBox="0 0 24 24"><path d="M14 20a1 1 0 0 1 0 2h-4a1 1 0 0 1 0-2h4ZM3 18l-.133-.007c-1.156-.124-1.156-1.862 0-1.986l.3-.012C4.32 15.923 5 15.107 5 14V9.5C5 5.368 8.014 2 12 2s7 3.368 7 7.5V14c0 1.107.68 1.923 1.832 1.995l.301.012c1.156.124 1.156 1.862 0 1.986L21 18H3Zm9-14C9.17 4 7 6.426 7 9.5V14c0 .671-.146 1.303-.416 1.858L6.51 16h10.979l-.073-.142a4.192 4.192 0 0 1-.412-1.658L17 14V9.5C17 6.426 14.83 4 12 4Z"/></symbol><symbol id="icon-eds-i-publish-medium" viewBox="0 0 24 24"><g><path d="M16.296 1.291A1 1 0 0 0 15.591 1H5.545A2.542 2.542 0 0 0 3 3.538V13a1 1 0 1 0 2 0V3.538l.007-.087A.543.543 0 0 1 5.545 3h9.633L20 7.8v12.662a.534.534 0 0 1-.158.379.548.548 0 0 1-.387.159H11a1 1 0 1 0 0 2h8.455c.674 0 1.32-.267 1.798-.742A2.534 2.534 0 0 0 22 20.462V7.385a1 1 0 0 0-.294-.709l-5.41-5.385Z"/><path d="M10.762 16.647a1 1 0 0 0-1.525-1.294l-4.472 5.271-2.153-1.665a1 1 0 1 0-1.224 1.582l2.91 2.25a1 1 0 0 0 1.374-.144l5.09-6ZM16 10a1 1 0 1 1 0 2H8a1 1 0 1 1 0-2h8ZM12 7a1 1 0 0 0-1-1H8a1 1 0 1 0 0 2h3a1 1 0 0 0 1-1Z"/></g></symbol><symbol id="icon-eds-i-refresh-medium" viewBox="0 0 24 24"><g><path d="M7.831 5.636H6.032A8.76 8.76 0 0 1 9 3.631 8.549 8.549 0 0 1 12.232 3c.603 0 1.192.063 1.76.182C17.979 4.017 21 7.632 21 12a1 1 0 1 0 2 0c0-5.296-3.674-9.746-8.591-10.776A10.61 10.61 0 0 0 5 3.851V2.805a1 1 0 0 0-.987-1H4a1 1 0 0 0-1 1v3.831a1 1 0 0 0 1 1h3.831a1 1 0 0 0 .013-2h-.013ZM17.968 18.364c-1.59 1.632-3.784 2.636-6.2 2.636C6.948 21 3 16.993 3 12a1 1 0 1 0-2 0c0 6.053 4.799 11 10.768 11 2.788 0 5.324-1.082 7.232-2.85v1.045a1 1 0 1 0 2 0v-3.831a1 1 0 0 0-1-1h-3.831a1 1 0 0 0 0 2h1.799Z"/></g></symbol><symbol id="icon-eds-i-search-medium" viewBox="0 0 24 24"><path d="M11 1c5.523 0 10 4.477 10 10 0 2.4-.846 4.604-2.256 6.328l3.963 3.965a1 1 0 0 1-1.414 1.414l-3.965-3.963A9.959 9.959 0 0 1 11 21C5.477 21 1 16.523 1 11S5.477 1 11 1Zm0 2a8 8 0 1 0 0 16 8 8 0 0 0 0-16Z"/></symbol><symbol id="icon-eds-i-settings-medium" viewBox="0 0 24 24"><path d="M11.382 1h1.24a2.508 2.508 0 0 1 2.334 1.63l.523 1.378 1.59.933 1.444-.224c.954-.132 1.89.3 2.422 1.101l.095.155.598 1.066a2.56 2.56 0 0 1-.195 2.848l-.894 1.161v1.896l.92 1.163c.6.768.707 1.812.295 2.674l-.09.17-.606 1.08a2.504 2.504 0 0 1-2.531 1.25l-1.428-.223-1.589.932-.523 1.378a2.512 2.512 0 0 1-2.155 1.625L12.65 23h-1.27a2.508 2.508 0 0 1-2.334-1.63l-.524-1.379-1.59-.933-1.443.225c-.954.132-1.89-.3-2.422-1.101l-.095-.155-.598-1.066a2.56 2.56 0 0 1 .195-2.847l.891-1.161v-1.898l-.919-1.162a2.562 2.562 0 0 1-.295-2.674l.09-.17.606-1.08a2.504 2.504 0 0 1 2.531-1.25l1.43.223 1.618-.938.524-1.375.07-.167A2.507 2.507 0 0 1 11.382 1Zm.003 2a.509.509 0 0 0-.47.338l-.65 1.71a1 1 0 0 1-.434.51L7.6 6.85a1 1 0 0 1-.655.123l-1.762-.275a.497.497 0 0 0-.498.252l-.61 1.088a.562.562 0 0 0 .04.619l1.13 1.43a1 1 0 0 1 .216.62v2.585a1 1 0 0 1-.207.61L4.15 15.339a.568.568 0 0 0-.036.634l.601 1.072a.494.494 0 0 0 .484.26l1.78-.278a1 1 0 0 1 .66.126l2.2 1.292a1 1 0 0 1 .43.507l.648 1.71a.508.508 0 0 0 .467.338h1.263a.51.51 0 0 0 .47-.34l.65-1.708a1 1 0 0 1 .428-.507l2.201-1.292a1 1 0 0 1 .66-.126l1.763.275a.497.497 0 0 0 .498-.252l.61-1.088a.562.562 0 0 0-.04-.619l-1.13-1.43a1 1 0 0 1-.216-.62v-2.585a1 1 0 0 1 .207-.61l1.105-1.437a.568.568 0 0 0 .037-.634l-.601-1.072a.494.494 0 0 0-.484-.26l-1.78.278a1 1 0 0 1-.66-.126l-2.2-1.292a1 1 0 0 1-.43-.507l-.649-1.71A.508.508 0 0 0 12.62 3h-1.234ZM12 8a4 4 0 1 1 0 8 4 4 0 0 1 0-8Zm0 2a2 2 0 1 0 0 4 2 2 0 0 0 0-4Z"/></symbol><symbol id="icon-eds-i-shipping-medium" viewBox="0 0 24 24"><path d="M16.515 2c1.406 0 2.706.728 3.352 1.902l2.02 3.635.02.042.036.089.031.105.012.058.01.073.004.075v11.577c0 .64-.244 1.255-.683 1.713a2.356 2.356 0 0 1-1.701.731H4.386a2.356 2.356 0 0 1-1.702-.731 2.476 2.476 0 0 1-.683-1.713V7.948c.01-.217.083-.43.22-.6L4.2 3.905C4.833 2.755 6.089 2.032 7.486 2h9.029ZM20 9H4v10.556a.49.49 0 0 0 .075.26l.053.07a.356.356 0 0 0 .257.114h15.23c.094 0 .186-.04.258-.115a.477.477 0 0 0 .127-.33V9Zm-2 7.5a1 1 0 0 1 0 2h-4a1 1 0 0 1 0-2h4ZM16.514 4H13v3h6.3l-1.183-2.13c-.288-.522-.908-.87-1.603-.87ZM11 3.999H7.51c-.679.017-1.277.36-1.566.887L4.728 7H11V3.999Z"/></symbol><symbol id="icon-eds-i-step-guide-medium" viewBox="0 0 24 24"><path d="M11.394 9.447a1 1 0 1 0-1.788-.894l-.88 1.759-.019-.02a1 1 0 1 0-1.414 1.415l1 1a1 1 0 0 0 1.601-.26l1.5-3ZM12 11a1 1 0 0 1 1-1h3a1 1 0 1 1 0 2h-3a1 1 0 0 1-1-1ZM12 17a1 1 0 0 1 1-1h3a1 1 0 1 1 0 2h-3a1 1 0 0 1-1-1ZM10.947 14.105a1 1 0 0 1 .447 1.342l-1.5 3a1 1 0 0 1-1.601.26l-1-1a1 1 0 1 1 1.414-1.414l.02.019.879-1.76a1 1 0 0 1 1.341-.447Z"/><path d="M5.545 1A2.542 2.542 0 0 0 3 3.538v16.924A2.542 2.542 0 0 0 5.545 23h12.91A2.542 2.542 0 0 0 21 20.462V7.5a1 1 0 0 0-.293-.707l-5.5-5.5A1 1 0 0 0 14.5 1H5.545ZM5 3.538C5 3.245 5.24 3 5.545 3h8.54L19 7.914v12.547c0 .294-.24.539-.546.539H5.545A.542.542 0 0 1 5 20.462V3.538Z" clip-rule="evenodd"/></symbol><symbol id="icon-eds-i-submission-medium" viewBox="0 0 24 24"><g><path d="M5 3.538C5 3.245 5.24 3 5.545 3h9.633L20 7.8v12.662a.535.535 0 0 1-.158.379.549.549 0 0 1-.387.159H6a1 1 0 0 1-1-1v-2.5a1 1 0 1 0-2 0V20a3 3 0 0 0 3 3h13.455c.673 0 1.32-.266 1.798-.742A2.535 2.535 0 0 0 22 20.462V7.385a1 1 0 0 0-.294-.709l-5.41-5.385A1 1 0 0 0 15.591 1H5.545A2.542 2.542 0 0 0 3 3.538V7a1 1 0 0 0 2 0V3.538Z"/><path d="m13.707 13.707-4 4a1 1 0 0 1-1.414 0l-.083-.094a1 1 0 0 1 .083-1.32L10.585 14 2 14a1 1 0 1 1 0-2l8.583.001-2.29-2.294a1 1 0 0 1 1.414-1.414l4.037 4.04.043.05.043.06.059.098.03.063.031.085.03.113.017.122L14 13l-.004.087-.017.118-.013.056-.034.104-.049.105-.048.081-.07.093-.058.063Z"/></g></symbol><symbol id="icon-eds-i-table-1-medium" viewBox="0 0 24 24"><path d="M4.385 22a2.56 2.56 0 0 1-1.14-.279C2.485 21.341 2 20.614 2 19.615V4.385c0-.315.067-.716.279-1.14C2.659 2.485 3.386 2 4.385 2h15.23c.315 0 .716.067 1.14.279.76.38 1.245 1.107 1.245 2.106v15.23c0 .315-.067.716-.279 1.14-.38.76-1.107 1.245-2.106 1.245H4.385ZM4 19.615c0 .213.034.265.14.317a.71.71 0 0 0 .245.068H8v-4H4v3.615ZM20 16H10v4h9.615c.213 0 .265-.034.317-.14a.71.71 0 0 0 .068-.245V16Zm0-2v-4H10v4h10ZM4 14h4v-4H4v4ZM19.615 4H10v4h10V4.385c0-.213-.034-.265-.14-.317A.71.71 0 0 0 19.615 4ZM8 4H4.385l-.082.002c-.146.01-.19.047-.235.138A.71.71 0 0 0 4 4.385V8h4V4Z"/></symbol><symbol id="icon-eds-i-table-2-medium" viewBox="0 0 24 24"><path d="M4.384 22A2.384 2.384 0 0 1 2 19.616V4.384A2.384 2.384 0 0 1 4.384 2h15.232A2.384 2.384 0 0 1 22 4.384v15.232A2.384 2.384 0 0 1 19.616 22H4.384ZM10 15H4v4.616c0 .212.172.384.384.384H10v-5Zm5 0h-3v5h3v-5Zm5 0h-3v5h2.616a.384.384 0 0 0 .384-.384V15ZM10 9H4v4h6V9Zm5 0h-3v4h3V9Zm5 0h-3v4h3V9Zm-.384-5H4.384A.384.384 0 0 0 4 4.384V7h16V4.384A.384.384 0 0 0 19.616 4Z"/></symbol><symbol id="icon-eds-i-tag-medium" viewBox="0 0 24 24"><path d="m12.621 1.998.127.004L20.496 2a1.5 1.5 0 0 1 1.497 1.355L22 3.5l-.005 7.669c.038.456-.133.905-.447 1.206l-9.02 9.018a2.075 2.075 0 0 1-2.932 0l-6.99-6.99a2.075 2.075 0 0 1 .001-2.933L11.61 2.47c.246-.258.573-.418.881-.46l.131-.011Zm.286 2-8.885 8.886a.075.075 0 0 0 0 .106l6.987 6.988c.03.03.077.03.106 0l8.883-8.883L19.999 4l-7.092-.002ZM16 6.5a1.5 1.5 0 0 1 .144 2.993L16 9.5a1.5 1.5 0 0 1 0-3Z"/></symbol><symbol id="icon-eds-i-trash-medium" viewBox="0 0 24 24"><path d="M12 1c2.717 0 4.913 2.232 4.997 5H21a1 1 0 0 1 0 2h-1v12.5c0 1.389-1.152 2.5-2.556 2.5H6.556C5.152 23 4 21.889 4 20.5V8H3a1 1 0 1 1 0-2h4.003l.001-.051C7.114 3.205 9.3 1 12 1Zm6 7H6v12.5c0 .238.19.448.454.492l.102.008h10.888c.315 0 .556-.232.556-.5V8Zm-4 3a1 1 0 0 1 1 1v6.005a1 1 0 0 1-2 0V12a1 1 0 0 1 1-1Zm-4 0a1 1 0 0 1 1 1v6a1 1 0 0 1-2 0v-6a1 1 0 0 1 1-1Zm2-8c-1.595 0-2.914 1.32-2.996 3h5.991v-.02C14.903 4.31 13.589 3 12 3Z"/></symbol><symbol id="icon-eds-i-user-account-medium" viewBox="0 0 24 24"><path d="M12 1c6.075 0 11 4.925 11 11s-4.925 11-11 11S1 18.075 1 12 5.925 1 12 1Zm0 16c-1.806 0-3.52.994-4.664 2.698A8.947 8.947 0 0 0 12 21a8.958 8.958 0 0 0 4.664-1.301C15.52 17.994 13.806 17 12 17Zm0-14a9 9 0 0 0-6.25 15.476C7.253 16.304 9.54 15 12 15s4.747 1.304 6.25 3.475A9 9 0 0 0 12 3Zm0 3a4 4 0 1 1 0 8 4 4 0 0 1 0-8Zm0 2a2 2 0 1 0 0 4 2 2 0 0 0 0-4Z"/></symbol><symbol id="icon-eds-i-user-add-medium" viewBox="0 0 24 24"><path d="M9 1a5 5 0 1 1 0 10A5 5 0 0 1 9 1Zm0 2a3 3 0 1 0 0 6 3 3 0 0 0 0-6Zm9 10a1 1 0 0 1 1 1v3h3a1 1 0 0 1 0 2h-3v3a1 1 0 0 1-2 0v-3h-3a1 1 0 0 1 0-2h3v-3a1 1 0 0 1 1-1Zm-5.545-.15a1 1 0 1 1-.91 1.78 5.713 5.713 0 0 0-5.705.282c-1.67 1.068-2.728 2.927-2.832 4.956L3.004 20 11.5 20a1 1 0 0 1 .993.883L12.5 21a1 1 0 0 1-1 1H2a1 1 0 0 1-1-1v-.876c.028-2.812 1.446-5.416 3.763-6.897a7.713 7.713 0 0 1 7.692-.378Z"/></symbol><symbol id="icon-eds-i-user-assign-medium" viewBox="0 0 24 24"><path d="M16.226 13.298a1 1 0 0 1 1.414-.01l.084.093a1 1 0 0 1-.073 1.32L15.39 17H22a1 1 0 0 1 0 2h-6.611l2.262 2.298a1 1 0 0 1-1.425 1.404l-3.939-4a1 1 0 0 1 0-1.404l3.94-4Zm-3.771-.449a1 1 0 1 1-.91 1.781 5.713 5.713 0 0 0-5.705.282c-1.67 1.068-2.728 2.927-2.832 4.956L3.004 20 10.5 20a1 1 0 0 1 .993.883L11.5 21a1 1 0 0 1-1 1H2a1 1 0 0 1-1-1v-.876c.028-2.812 1.446-5.416 3.763-6.897a7.713 7.713 0 0 1 7.692-.378ZM9 1a5 5 0 1 1 0 10A5 5 0 0 1 9 1Zm0 2a3 3 0 1 0 0 6 3 3 0 0 0 0-6Z"/></symbol><symbol id="icon-eds-i-user-block-medium" viewBox="0 0 24 24"><path d="M9 1a5 5 0 1 1 0 10A5 5 0 0 1 9 1Zm0 2a3 3 0 1 0 0 6 3 3 0 0 0 0-6Zm9 10a5 5 0 1 1 0 10 5 5 0 0 1 0-10Zm-5.545-.15a1 1 0 1 1-.91 1.78 5.713 5.713 0 0 0-5.705.282c-1.67 1.068-2.728 2.927-2.832 4.956L3.004 20 11.5 20a1 1 0 0 1 .993.883L12.5 21a1 1 0 0 1-1 1H2a1 1 0 0 1-1-1v-.876c.028-2.812 1.446-5.416 3.763-6.897a7.713 7.713 0 0 1 7.692-.378ZM15 18a3 3 0 0 0 4.294 2.707l-4.001-4c-.188.391-.293.83-.293 1.293Zm3-3c-.463 0-.902.105-1.294.293l4.001 4A3 3 0 0 0 18 15Z"/></symbol><symbol id="icon-eds-i-user-check-medium" viewBox="0 0 24 24"><path d="M9 1a5 5 0 1 1 0 10A5 5 0 0 1 9 1Zm0 2a3 3 0 1 0 0 6 3 3 0 0 0 0-6Zm13.647 12.237a1 1 0 0 1 .116 1.41l-5.091 6a1 1 0 0 1-1.375.144l-2.909-2.25a1 1 0 1 1 1.224-1.582l2.153 1.665 4.472-5.271a1 1 0 0 1 1.41-.116Zm-8.139-.977c.22.214.428.44.622.678a1 1 0 1 1-1.548 1.266 6.025 6.025 0 0 0-1.795-1.49.86.86 0 0 1-.163-.048l-.079-.036a5.721 5.721 0 0 0-2.62-.63l-.194.006c-2.76.134-5.022 2.177-5.592 4.864l-.035.175-.035.213c-.03.201-.05.405-.06.61L3.003 20 10 20a1 1 0 0 1 .993.883L11 21a1 1 0 0 1-1 1H2a1 1 0 0 1-1-1v-.876l.005-.223.02-.356.02-.222.03-.248.022-.15c.02-.133.044-.265.071-.397.44-2.178 1.725-4.105 3.595-5.301a7.75 7.75 0 0 1 3.755-1.215l.12-.004a7.908 7.908 0 0 1 5.87 2.252Z"/></symbol><symbol id="icon-eds-i-user-delete-medium" viewBox="0 0 24 24"><path d="M9 1a5 5 0 1 1 0 10A5 5 0 0 1 9 1Zm0 2a3 3 0 1 0 0 6 3 3 0 0 0 0-6ZM4.763 13.227a7.713 7.713 0 0 1 7.692-.378 1 1 0 1 1-.91 1.781 5.713 5.713 0 0 0-5.705.282c-1.67 1.068-2.728 2.927-2.832 4.956L3.004 20H11.5a1 1 0 0 1 .993.883L12.5 21a1 1 0 0 1-1 1H2a1 1 0 0 1-1-1v-.876c.028-2.812 1.446-5.416 3.763-6.897Zm11.421 1.543 2.554 2.553 2.555-2.553a1 1 0 0 1 1.414 1.414l-2.554 2.554 2.554 2.555a1 1 0 0 1-1.414 1.414l-2.555-2.554-2.554 2.554a1 1 0 0 1-1.414-1.414l2.553-2.555-2.553-2.554a1 1 0 0 1 1.414-1.414Z"/></symbol><symbol id="icon-eds-i-user-edit-medium" viewBox="0 0 24 24"><path d="m19.876 10.77 2.831 2.83a1 1 0 0 1 0 1.415l-7.246 7.246a1 1 0 0 1-.572.284l-3.277.446a1 1 0 0 1-1.125-1.13l.461-3.277a1 1 0 0 1 .283-.567l7.23-7.246a1 1 0 0 1 1.415-.001Zm-7.421 2.08a1 1 0 1 1-.91 1.78 5.713 5.713 0 0 0-5.705.282c-1.67 1.068-2.728 2.927-2.832 4.956L3.004 20 7.5 20a1 1 0 0 1 .993.883L8.5 21a1 1 0 0 1-1 1H2a1 1 0 0 1-1-1v-.876c.028-2.812 1.446-5.416 3.763-6.897a7.713 7.713 0 0 1 7.692-.378Zm6.715.042-6.29 6.3-.23 1.639 1.633-.222 6.302-6.302-1.415-1.415ZM9 1a5 5 0 1 1 0 10A5 5 0 0 1 9 1Zm0 2a3 3 0 1 0 0 6 3 3 0 0 0 0-6Z"/></symbol><symbol id="icon-eds-i-user-linked-medium" viewBox="0 0 24 24"><path d="M15.65 6c.31 0 .706.066 1.122.274C17.522 6.65 18 7.366 18 8.35v12.3c0 .31-.066.706-.274 1.122-.375.75-1.092 1.228-2.076 1.228H3.35a2.52 2.52 0 0 1-1.122-.274C1.478 22.35 1 21.634 1 20.65V8.35c0-.31.066-.706.274-1.122C1.65 6.478 2.366 6 3.35 6h12.3Zm0 2-12.376.002c-.134.007-.17.04-.21.12A.672.672 0 0 0 3 8.35v12.3c0 .198.028.24.122.287.09.044.2.063.228.063h.887c.788-2.269 2.814-3.5 5.263-3.5 2.45 0 4.475 1.231 5.263 3.5h.887c.198 0 .24-.028.287-.122.044-.09.063-.2.063-.228V8.35c0-.198-.028-.24-.122-.287A.672.672 0 0 0 15.65 8ZM9.5 19.5c-1.36 0-2.447.51-3.06 1.5h6.12c-.613-.99-1.7-1.5-3.06-1.5ZM20.65 1A2.35 2.35 0 0 1 23 3.348V15.65A2.35 2.35 0 0 1 20.65 18H20a1 1 0 0 1 0-2h.65a.35.35 0 0 0 .35-.35V3.348A.35.35 0 0 0 20.65 3H8.35a.35.35 0 0 0-.35.348V4a1 1 0 1 1-2 0v-.652A2.35 2.35 0 0 1 8.35 1h12.3ZM9.5 10a3.5 3.5 0 1 1 0 7 3.5 3.5 0 0 1 0-7Zm0 2a1.5 1.5 0 1 0 0 3 1.5 1.5 0 0 0 0-3Z"/></symbol><symbol id="icon-eds-i-user-multiple-medium" viewBox="0 0 24 24"><path d="M9 1a5 5 0 1 1 0 10A5 5 0 0 1 9 1Zm6 0a5 5 0 0 1 0 10 1 1 0 0 1-.117-1.993L15 9a3 3 0 0 0 0-6 1 1 0 0 1 0-2ZM9 3a3 3 0 1 0 0 6 3 3 0 0 0 0-6Zm8.857 9.545a7.99 7.99 0 0 1 2.651 1.715A8.31 8.31 0 0 1 23 20.134V21a1 1 0 0 1-1 1h-3a1 1 0 0 1 0-2h1.995l-.005-.153a6.307 6.307 0 0 0-1.673-3.945l-.204-.209a5.99 5.99 0 0 0-1.988-1.287 1 1 0 1 1 .732-1.861Zm-3.349 1.715A8.31 8.31 0 0 1 17 20.134V21a1 1 0 0 1-1 1H2a1 1 0 0 1-1-1v-.877c.044-4.343 3.387-7.908 7.638-8.115a7.908 7.908 0 0 1 5.87 2.252ZM9.016 14l-.285.006c-3.104.15-5.58 2.718-5.725 5.9L3.004 20h11.991l-.005-.153a6.307 6.307 0 0 0-1.673-3.945l-.204-.209A5.924 5.924 0 0 0 9.3 14.008L9.016 14Z"/></symbol><symbol id="icon-eds-i-user-notify-medium" viewBox="0 0 24 24"><path d="M9 1a5 5 0 1 1 0 10A5 5 0 0 1 9 1Zm0 2a3 3 0 1 0 0 6 3 3 0 0 0 0-6Zm10 18v1a1 1 0 0 1-2 0v-1h-3a1 1 0 0 1 0-2v-2.818C14 13.885 15.777 12 18 12s4 1.885 4 4.182V19a1 1 0 0 1 0 2h-3Zm-6.545-8.15a1 1 0 1 1-.91 1.78 5.713 5.713 0 0 0-5.705.282c-1.67 1.068-2.728 2.927-2.832 4.956L3.004 20 11.5 20a1 1 0 0 1 .993.883L12.5 21a1 1 0 0 1-1 1H2a1 1 0 0 1-1-1v-.876c.028-2.812 1.446-5.416 3.763-6.897a7.713 7.713 0 0 1 7.692-.378ZM18 14c-1.091 0-2 .964-2 2.182V19h4v-2.818c0-1.165-.832-2.098-1.859-2.177L18 14Z"/></symbol><symbol id="icon-eds-i-user-remove-medium" viewBox="0 0 24 24"><path d="M9 1a5 5 0 1 1 0 10A5 5 0 0 1 9 1Zm0 2a3 3 0 1 0 0 6 3 3 0 0 0 0-6Zm3.455 9.85a1 1 0 1 1-.91 1.78 5.713 5.713 0 0 0-5.705.282c-1.67 1.068-2.728 2.927-2.832 4.956L3.004 20 11.5 20a1 1 0 0 1 .993.883L12.5 21a1 1 0 0 1-1 1H2a1 1 0 0 1-1-1v-.876c.028-2.812 1.446-5.416 3.763-6.897a7.713 7.713 0 0 1 7.692-.378ZM22 17a1 1 0 0 1 0 2h-8a1 1 0 0 1 0-2h8Z"/></symbol><symbol id="icon-eds-i-user-single-medium" viewBox="0 0 24 24"><path d="M12 1a5 5 0 1 1 0 10 5 5 0 0 1 0-10Zm0 2a3 3 0 1 0 0 6 3 3 0 0 0 0-6Zm-.406 9.008a8.965 8.965 0 0 1 6.596 2.494A9.161 9.161 0 0 1 21 21.025V22a1 1 0 0 1-1 1H4a1 1 0 0 1-1-1v-.985c.05-4.825 3.815-8.777 8.594-9.007Zm.39 1.992-.299.006c-3.63.175-6.518 3.127-6.678 6.775L5 21h13.998l-.009-.268a7.157 7.157 0 0 0-1.97-4.573l-.214-.213A6.967 6.967 0 0 0 11.984 14Z"/></symbol><symbol id="icon-eds-i-warning-circle-medium" viewBox="0 0 24 24"><path d="M12 1c6.075 0 11 4.925 11 11s-4.925 11-11 11S1 18.075 1 12 5.925 1 12 1Zm0 2a9 9 0 1 0 0 18 9 9 0 0 0 0-18Zm0 11.5a1.5 1.5 0 0 1 .144 2.993L12 17.5a1.5 1.5 0 0 1 0-3ZM12 6a1 1 0 0 1 1 1v5a1 1 0 0 1-2 0V7a1 1 0 0 1 1-1Z"/></symbol><symbol id="icon-eds-i-warning-filled-medium" viewBox="0 0 24 24"><path d="M12 1c6.075 0 11 4.925 11 11s-4.925 11-11 11S1 18.075 1 12 5.925 1 12 1Zm0 13.5a1.5 1.5 0 0 0 0 3l.144-.007A1.5 1.5 0 0 0 12 14.5ZM12 6a1 1 0 0 0-1 1v5a1 1 0 0 0 2 0V7a1 1 0 0 0-1-1Z"/></symbol><symbol id="icon-eds-i-work-medium" viewBox="0 0 24 24"><path d="M4 3.53808C4 3.24519 4.23975 3 4.5451 3H14.1778L19 7.80031V8C19 8.55228 19.4477 9 20 9C20.5523 9 21 8.55228 21 8V7.38477C21 7.11876 20.894 6.86372 20.7055 6.67605L15.2962 1.29129C15.1088 1.10473 14.8551 1 14.5907 1H4.5451C3.14377 1 2 2.13206 2 3.53808V20.5007C2 21.882 3.11988 23 4.5 23H8C8.55228 23 9 22.5523 9 22C9 21.4477 8.55228 21 8 21H4.5C4.22327 21 4 20.7762 4 20.5007V3.53808Z"/><path fill-rule="evenodd" clip-rule="evenodd" d="M19.8764 10.7698C19.6887 10.5822 19.4341 10.4768 19.1687 10.4769C18.9033 10.4771 18.6489 10.5827 18.4614 10.7706L11.2306 18.0167C11.0776 18.1701 10.9785 18.3691 10.9483 18.5836L10.4867 21.8605C10.443 22.1707 10.5472 22.4835 10.7682 22.7055C10.9892 22.9275 11.3015 23.0331 11.6118 22.9909L14.8888 22.5447C15.1054 22.5152 15.3064 22.4155 15.461 22.261L22.7071 15.0148C22.8947 14.8273 23 14.5729 23 14.3077C23 14.0425 22.8947 13.7881 22.7071 13.6006L19.8764 10.7698ZM12.8821 19.1931L19.17 12.8919L20.5858 14.3077L14.285 20.6085L12.6515 20.8309L12.8821 19.1931Z"/><path d="M11.0812 4.68628C11.5307 5.00729 11.6347 5.63184 11.3137 6.08125L8.81373 9.58125C8.64288 9.82045 8.37543 9.97236 8.08248 9.99661C7.78953 10.0209 7.50075 9.91498 7.29289 9.70712L5.79289 8.20712C5.40237 7.8166 5.40237 7.18343 5.79289 6.79291C6.18342 6.40239 6.81658 6.40239 7.20711 6.79291L7.8724 7.4582L9.68627 4.91878C10.0073 4.46937 10.6318 4.36527 11.0812 4.68628Z"/><path d="M11.3137 12.0813C11.6347 11.6318 11.5307 11.0073 11.0812 10.6863C10.6318 10.3653 10.0073 10.4694 9.68627 10.9188L7.8724 13.4582L7.20711 12.7929C6.81658 12.4024 6.18342 12.4024 5.79289 12.7929C5.40237 13.1834 5.40237 13.8166 5.79289 14.2071L7.29289 15.7071C7.50075 15.915 7.78953 16.0209 8.08248 15.9966C8.37543 15.9724 8.64288 15.8205 8.81373 15.5813L11.3137 12.0813Z"/></symbol><symbol id="icon-ai-stars"><path d="M22.294 13.39c.941.536.941 1.945 0 2.482l-3.613 2.061c-.228.13-.415.325-.54.563l-1.976 3.768a1.33 1.33 0 0 1-2.38 0l-1.977-3.768a1.4 1.4 0 0 0-.539-.563l-3.614-2.061c-.94-.537-.94-1.946 0-2.482l3.614-2.061c.228-.13.415-.325.54-.563l1.976-3.768a1.33 1.33 0 0 1 2.38 0l1.977 3.768c.124.238.311.433.539.563zM10.08 4.861c1.044.508 1.044 2.056 0 2.564l-1.543.751c-.29.14-.521.383-.656.684l-.72 1.61a1.334 1.334 0 0 1-2.459 0l-.72-1.61a1.4 1.4 0 0 0-.656-.684l-1.543-.751c-1.044-.508-1.044-2.056 0-2.564l1.543-.751c.29-.14.521-.383.656-.684l.72-1.61a1.334 1.334 0 0 1 2.459 0l.72 1.61c.135.301.367.543.656.684z"/></symbol><symbol id="icon-chevron-left-medium" viewBox="0 0 24 24"><path d="M15.7194 3.3054C15.3358 2.90809 14.7027 2.89699 14.3054 3.28061L6.54342 10.7757C6.19804 11.09 6 11.5335 6 12C6 12.4665 6.19804 12.91 6.5218 13.204L14.3054 20.7194C14.7027 21.103 15.3358 21.0919 15.7194 20.6946C16.103 20.2973 16.0919 19.6642 15.6946 19.2806L8.155 12L15.6946 4.71939C16.0614 4.36528 16.099 3.79863 15.8009 3.40105L15.7194 3.3054Z"/></symbol><symbol id="icon-chevron-right-medium" viewBox="0 0 24 24"><path d="M8.28061 3.3054C8.66423 2.90809 9.29729 2.89699 9.6946 3.28061L17.4566 10.7757C17.802 11.09 18 11.5335 18 12C18 12.4665 17.802 12.91 17.4782 13.204L9.6946 20.7194C9.29729 21.103 8.66423 21.0919 8.28061 20.6946C7.89699 20.2973 7.90809 19.6642 8.3054 19.2806L15.845 12L8.3054 4.71939C7.93865 4.36528 7.90098 3.79863 8.19908 3.40105L8.28061 3.3054Z"/></symbol><symbol id="icon-citations-medium-green-dot" viewBox="0 0 32 32"><path fill-rule="evenodd" clip-rule="evenodd" d="M19.7876 1.33301C20.1402 1.33301 20.4784 1.47265 20.7282 1.72139L27.9407 8.90108C28.192 9.1513 28.3333 9.49136 28.3333 9.84603V26.2822C28.3333 27.181 27.9748 28.0426 27.3373 28.6773C26.7009 29.3108 25.8386 29.6663 24.9399 29.6663H17.1491C16.4128 29.6663 15.8158 29.0694 15.8158 28.333C15.8158 27.5966 16.4128 26.9997 17.1491 26.9997H24.9399C25.1339 26.9997 25.3196 26.9231 25.4559 26.7874C25.5911 26.6529 25.6667 26.4712 25.6667 26.2822V10.3997L19.2373 3.99967H8.39346C8.03099 3.99967 7.73226 4.26096 7.67614 4.60108L7.66667 4.71712V14.1929C7.66667 14.9293 7.06971 15.5262 6.33333 15.5262C5.59695 15.5262 5 14.9293 5 14.1929V4.71712C5 2.84677 6.52066 1.33301 8.39346 1.33301H19.7876Z"/><path style="fill:#00A69D" d="M7 19C3.68629 19 1 21.6863 1 25C1 28.3137 3.68629 31 7 31C10.3137 31 13 28.3137 13 25C13 21.6863 10.3137 19 7 19Z"/></symbol><symbol id="icon-eds-alerts" viewBox="0 0 32 32"><path d="M28 12.667c.736 0 1.333.597 1.333 1.333v13.333A3.333 3.333 0 0 1 26 30.667H6a3.333 3.333 0 0 1-3.333-3.334V14a1.333 1.333 0 1 1 2.666 0v1.252L16 21.769l10.667-6.518V14c0-.736.597-1.333 1.333-1.333Zm-1.333 5.71-9.972 6.094c-.427.26-.963.26-1.39 0l-9.972-6.094v8.956c0 .368.299.667.667.667h20a.667.667 0 0 0 .667-.667v-8.956ZM19.333 12a1.333 1.333 0 1 1 0 2.667h-6.666a1.333 1.333 0 1 1 0-2.667h6.666Zm4-10.667a3.333 3.333 0 0 1 3.334 3.334v6.666a1.333 1.333 0 1 1-2.667 0V4.667A.667.667 0 0 0 23.333 4H8.667A.667.667 0 0 0 8 4.667v6.666a1.333 1.333 0 1 1-2.667 0V4.667a3.333 3.333 0 0 1 3.334-3.334h14.666Zm-4 5.334a1.333 1.333 0 0 1 0 2.666h-6.666a1.333 1.333 0 1 1 0-2.666h6.666Z"/></symbol><symbol id="icon-eds-arrow-up" viewBox="0 0 24 24"><path fill-rule="evenodd" d="m13.002 7.408 4.88 4.88a.99.99 0 0 0 1.32.08l.09-.08c.39-.39.39-1.03 0-1.42l-6.58-6.58a1.01 1.01 0 0 0-1.42 0l-6.58 6.58a1 1 0 0 0-.09 1.32l.08.1a1 1 0 0 0 1.42-.01l4.88-4.87v11.59a.99.99 0 0 0 .88.99l.12.01c.55 0 1-.45 1-1V7.408z" class="layer"/></symbol><symbol id="icon-eds-checklist" viewBox="0 0 32 32"><path d="M19.2 1.333a3.468 3.468 0 0 1 3.381 2.699L24.667 4C26.515 4 28 5.52 28 7.38v19.906c0 1.86-1.485 3.38-3.333 3.38H7.333c-1.848 0-3.333-1.52-3.333-3.38V7.38C4 5.52 5.485 4 7.333 4h2.093A3.468 3.468 0 0 1 12.8 1.333h6.4ZM9.426 6.667H7.333c-.36 0-.666.312-.666.713v19.906c0 .401.305.714.666.714h17.334c.36 0 .666-.313.666-.714V7.38c0-.4-.305-.713-.646-.714l-2.121.033A3.468 3.468 0 0 1 19.2 9.333h-6.4a3.468 3.468 0 0 1-3.374-2.666Zm12.715 5.606c.586.446.7 1.283.253 1.868l-7.111 9.334a1.333 1.333 0 0 1-1.792.306l-3.556-2.333a1.333 1.333 0 1 1 1.463-2.23l2.517 1.651 6.358-8.344a1.333 1.333 0 0 1 1.868-.252ZM19.2 4h-6.4a.8.8 0 0 0-.8.8v1.067a.8.8 0 0 0 .8.8h6.4a.8.8 0 0 0 .8-.8V4.8a.8.8 0 0 0-.8-.8Z"/></symbol><symbol id="icon-eds-citation" viewBox="0 0 36 36"><path d="M23.25 1.5a1.5 1.5 0 0 1 1.06.44l8.25 8.25a1.5 1.5 0 0 1 .44 1.06v19.5c0 2.105-1.645 3.75-3.75 3.75H18a1.5 1.5 0 0 1 0-3h11.25c.448 0 .75-.302.75-.75V11.873L22.628 4.5H8.31a.811.811 0 0 0-.8.68l-.011.13V16.5a1.5 1.5 0 0 1-3 0V5.31A3.81 3.81 0 0 1 8.31 1.5h14.94ZM8.223 20.358a.984.984 0 0 1-.192 1.378l-.048.034c-.54.36-.942.676-1.206.951-.59.614-.885 1.395-.885 2.343.115-.028.288-.042.518-.042.662 0 1.26.237 1.791.711.533.474.799 1.074.799 1.799 0 .753-.259 1.352-.777 1.799-.518.446-1.151.669-1.9.669-1.006 0-1.812-.293-2.417-.878C3.302 28.536 3 27.657 3 26.486c0-1.115.165-2.085.496-2.907.331-.823.734-1.513 1.209-2.071.475-.558.971-.997 1.49-1.318a6.01 6.01 0 0 1 .347-.2 1.321 1.321 0 0 1 1.681.368Zm7.5 0a.984.984 0 0 1-.192 1.378l-.048.034c-.54.36-.942.676-1.206.951-.59.614-.885 1.395-.885 2.343.115-.028.288-.042.518-.042.662 0 1.26.237 1.791.711.533.474.799 1.074.799 1.799 0 .753-.259 1.352-.777 1.799-.518.446-1.151.669-1.9.669-1.006 0-1.812-.293-2.417-.878-.604-.586-.906-1.465-.906-2.636 0-1.115.165-2.085.496-2.907.331-.823.734-1.513 1.209-2.071.475-.558.971-.997 1.49-1.318a6.01 6.01 0 0 1 .347-.2 1.321 1.321 0 0 1 1.681.368Z"/></symbol><symbol id="icon-eds-i-access-indicator" viewBox="0 0 16 16"><circle cx="4.5" cy="11.5" r="3.5" style="fill:currentColor"/><path fill-rule="evenodd" d="M4 3v3a1 1 0 0 1-2 0V2.923C2 1.875 2.84 1 3.909 1h5.909a1 1 0 0 1 .713.298l3.181 3.231a1 1 0 0 1 .288.702v7.846c0 .505-.197.993-.554 1.354a1.902 1.902 0 0 1-1.355.569H10a1 1 0 1 1 0-2h2V5.64L9.4 3H4Z" clip-rule="evenodd" style="fill:#222"/></symbol><symbol id="icon-eds-i-accessibility-medium" viewBox="0 0 24 24"><path d="M17 10.5C17.5523 10.5 18 10.9477 18 11.5C18 12.0523 17.5523 12.5 17 12.5H13V13C13 13.4952 13.2735 14.3106 13.7695 15.3027C14.1249 16.0135 14.551 16.7321 14.9483 17.3564L15.332 17.9453L15.3848 18.0332C15.6218 18.4812 15.4855 19.0448 15.0547 19.332C14.6238 19.6193 14.0508 19.5282 13.7285 19.1367L13.668 19.0547L13.2627 18.4326C12.8412 17.7703 12.3781 16.9924 11.9844 16.2061C11.4619 17.2978 10.8292 18.309 10.332 19.0547C10.0257 19.5142 9.40486 19.6384 8.94533 19.332C8.4858 19.0257 8.36163 18.4048 8.66798 17.9453C9.15666 17.2123 9.75032 16.2592 10.2197 15.2617C10.6385 14.372 10.9221 13.5218 10.9863 12.8008L11 12.5H7.00002C6.44774 12.5 6.00002 12.0523 6.00002 11.5C6.00002 10.9477 6.44774 10.5 7.00002 10.5H17Z"/><path fill-rule="evenodd" clip-rule="evenodd" d="M12 4C12.7957 4 13.5585 4.3163 14.1211 4.87891C14.6837 5.44152 15 6.20435 15 7C15 7.79565 14.6837 8.55848 14.1211 9.12109C13.5585 9.6837 12.7957 10 12 10C11.2044 10 10.4415 9.6837 9.87892 9.12109C9.31631 8.55848 9.00002 7.79565 9.00002 7C9.00002 6.20435 9.31631 5.44152 9.87892 4.87891C10.4415 4.3163 11.2044 4 12 4ZM12 6C11.7348 6 11.4805 6.10543 11.293 6.29297C11.1054 6.4805 11 6.73478 11 7C11 7.26522 11.1054 7.5195 11.293 7.70703C11.4805 7.89457 11.7348 8 12 8C12.2652 8 12.5195 7.89457 12.707 7.70703C12.8946 7.5195 13 7.26522 13 7C13 6.73478 12.8946 6.48051 12.707 6.29297C12.5195 6.10543 12.2652 6 12 6Z"/><path fill-rule="evenodd" clip-rule="evenodd" d="M12 1C18.0751 1 23 5.92487 23 12C23 18.0751 18.0751 23 12 23C5.92488 23 1.00002 18.0751 1.00002 12C1.00002 5.92487 5.92488 1 12 1ZM12 3C7.02945 3 3.00002 7.02944 3.00002 12C3.00002 16.9706 7.02945 21 12 21C16.9706 21 21 16.9706 21 12C21 7.02944 16.9706 3 12 3Z"/></symbol><symbol id="icon-eds-i-book-research-medium"><path fill-rule="evenodd" d="M9.99.952c.922 0 1.822.273 2.589.784l2.42 1.614 2.421-1.614a4.667 4.667 0 0 1 2.283-.774l.306-.01h6.324a3.333 3.333 0 0 1 3.333 3.334v8.666a1.333 1.333 0 0 1-2.666 0V4.286a.667.667 0 0 0-.667-.667h-6.324a2 2 0 0 0-1.11.336l-2.566 1.71v5.954a1.333 1.333 0 1 1-2.667 0V5.666L11.1 3.955a2 2 0 0 0-1.11-.336H3.666A.667.667 0 0 0 3 4.286v17.333c0 .368.298.667.666.667h10a1.333 1.333 0 1 1 0 2.666h-10A3.333 3.333 0 0 1 .333 21.62V4.286A3.333 3.333 0 0 1 3.666.952H9.99Zm12.343 13.334a6 6 0 0 1 5.08 9.193l1.863 1.864a1.333 1.333 0 1 1-1.886 1.886l-1.864-1.863a6 6 0 1 1-3.193-11.08Zm-3.333 6a3.333 3.333 0 1 1 6.666 0 3.333 3.333 0 0 1-6.666 0Z" clip-rule="evenodd"/></symbol><symbol id="icon-eds-i-circle-bluesky" viewBox="0 0 25 24"><path d="M12.5 0c6.627 0 12 5.373 12 12s-5.373 12-12 12-12-5.373-12-12 5.373-12 12-12m7 7.44c0-2.158-1.877-1.48-3.035-.604-1.605 1.214-3.331 3.676-3.965 4.997-.634-1.321-2.36-3.783-3.965-4.997C7.377 5.96 5.5 5.282 5.5 7.439c0 .432.245 3.62.389 4.137.5 1.8 2.32 2.258 3.94 1.98-2.831.486-3.551 2.095-1.996 3.703 2.954 3.054 4.246-.766 4.577-1.745.061-.181.09-.265.09-.19 0-.075.029.009.09.19.33.98 1.623 4.8 4.577 1.745 1.555-1.608.835-3.217-1.996-3.702 1.62.277 3.44-.182 3.94-1.98.144-.518.389-3.706.389-4.138"/></symbol><symbol id="icon-eds-i-circle-facebook" viewBox="0 0 25 24"><path d="M12.5 0C5.872 0 .5 5.372.5 12s5.372 12 12 12 12-5.372 12-12-5.372-12-12-12m3.356 7.417h-1.62s-.858-.023-.93 1v1.836h2.186l-.017 2.454h-2.168l-.003 6.409-2.571-.004v-6.405H8.612V10.23h2.12V8.012s.096-2.574 2.598-2.884h2.526z"/></symbol><symbol id="icon-eds-i-circle-github" viewBox="0 0 24 24"><path fill-rule="evenodd" d="M11.964 0C5.348 0 0 5.5 0 12.304c0 5.44 3.427 10.043 8.18 11.673.595.122.813-.265.813-.59 0-.286-.02-1.264-.02-2.282-3.328.733-4.02-1.467-4.02-1.467-.536-1.426-1.328-1.793-1.328-1.793-1.09-.753.08-.753.08-.753 1.207.081 1.841 1.263 1.841 1.263 1.07 1.874 2.793 1.344 3.487 1.018.099-.795.416-1.344.752-1.65-2.654-.285-5.447-1.344-5.447-6.07 0-1.345.475-2.445 1.228-3.3-.119-.306-.535-1.57.12-3.26 0 0 1.01-.326 3.287 1.263.975-.27 1.981-.407 2.991-.408 1.01 0 2.04.143 2.991.408 2.278-1.59 3.288-1.263 3.288-1.263.654 1.69.238 2.954.12 3.26.772.855 1.227 1.955 1.227 3.3 0 4.725-2.792 5.764-5.467 6.07.436.387.812 1.12.812 2.282 0 1.65-.02 2.974-.02 3.381 0 .326.219.713.813.591 4.754-1.63 8.18-6.234 8.18-11.673C23.929 5.5 18.56 0 11.965 0" clip-rule="evenodd"/></symbol><symbol id="icon-eds-i-circle-instagram" viewBox="0 0 25 24"><g clip-path="url(#a)"><path d="M12.5 0C5.872 0 .5 5.372.5 12s5.372 12 12 12 12-5.372 12-12-5.372-12-12-12m6.687 15.313a3.377 3.377 0 0 1-3.374 3.373H9.188a3.377 3.377 0 0 1-3.373-3.373V8.687a3.377 3.377 0 0 1 3.373-3.374h6.627a3.377 3.377 0 0 1 3.373 3.374zm-3.374-8.544H9.188A1.92 1.92 0 0 0 7.27 8.687v6.627a1.92 1.92 0 0 0 1.918 1.918h6.627a1.92 1.92 0 0 0 1.918-1.918V8.686a1.92 1.92 0 0 0-1.918-1.918M12.5 15.444A3.45 3.45 0 0 1 9.056 12c0-1.9 1.545-3.444 3.444-3.444S15.944 10.1 15.944 12 14.4 15.444 12.5 15.444m3.444-6.073a.816.816 0 1 1 .002-1.632.816.816 0 0 1-.002 1.632m-3.444.64c-1.096 0-1.99.893-1.99 1.989s.894 1.99 1.99 1.99A1.99 1.99 0 0 0 14.489 12a1.99 1.99 0 0 0-1.989-1.989"/></g></symbol><symbol id="icon-eds-i-circle-linkedin" viewBox="0 0 25 24"><path d="M12.5 0C5.872 0 .5 5.373.5 12s5.372 12 12 12 12-5.373 12-12-5.373-12-12-12m-2.288 16.662h-2.24V9.765h2.24zM9.056 8.844a1.253 1.253 0 1 1 0-2.507 1.253 1.253 0 0 1 0 2.507m9.03 7.81h-2.119v-3.998s-.034-1.321-1.22-1.17c0 0-1.016-.05-1.135 1.237v3.914h-2.151v-6.86h2.032v.914s.576-1.135 2.067-1.101c0 0 2.405-.254 2.525 2.49z"/></symbol><symbol id="icon-eds-i-circle-x" viewBox="0 0 25 24"><path d="m12.59 11.233-2.397-3.427H8.915l2.97 4.247.373.534 2.542 3.636h1.278l-3.115-4.455zM12.5 0C5.872 0 .5 5.372.5 12s5.372 12 12 12 12-5.372 12-12-5.372-12-12-12m1.908 16.82-2.572-3.743-3.221 3.744h-.832l3.683-4.281-3.683-5.36h2.809l2.435 3.544 3.05-3.545h.833l-3.513 4.083 3.82 5.56z"/></symbol><symbol id="icon-eds-i-circle-youtube" viewBox="0 0 24 24"><g clip-path="url(#youtube-icon)"><path d="M10.568 9.79053V13.9189L12.0275 13.1403L14.4376 11.855L12.0275 10.5686L10.568 9.79053Z"/><path d="M12 0C5.37302 0 0 5.37247 0 12C0 18.6275 5.37302 24 12 24C18.627 24 24 18.6275 24 12C24 5.37247 18.6275 0 12 0ZM18.2984 14.5486C18.2539 14.9382 18.1418 15.286 17.9637 15.5855C17.7483 15.9471 17.4071 16.1702 16.9768 16.2323C16.6015 16.2856 12.0275 16.4301 12.0275 16.4301H12.0258C12.0258 16.4301 9.15523 16.363 8.22658 16.3317C7.84222 16.3197 7.45882 16.2865 7.07812 16.2323C6.8686 16.2052 6.66815 16.1302 6.49235 16.013L6.48961 16.0114C6.34964 15.9169 6.22913 15.7963 6.13463 15.6564L6.13298 15.6536C6.11833 15.6317 6.10386 15.6088 6.08957 15.5849C5.90594 15.267 5.79223 14.9135 5.75602 14.548C5.66366 13.7017 5.62054 12.8508 5.62689 11.9995C5.62054 11.1481 5.66366 10.2972 5.75602 9.45087C5.79943 9.06127 5.91208 8.71289 6.08957 8.41451C6.30497 8.05348 6.64731 7.82984 7.07757 7.76774C7.45123 7.71499 7.83863 7.68147 8.22603 7.66828C9.13106 7.63861 12.0269 7.57047 12.0269 7.57047C12.0269 7.57047 14.9228 7.63861 15.8273 7.66828C16.2152 7.68092 16.6015 7.71499 16.9763 7.76774C17.2554 7.80786 17.4972 7.91611 17.6906 8.08645C17.734 8.12382 17.9181 8.34032 17.9632 8.41506C18.1418 8.71399 18.2539 9.06182 18.2978 9.45142C18.3874 10.2614 18.4292 11.0944 18.4264 12.0011C18.4317 12.852 18.3893 13.7025 18.2984 14.5486Z"/></g></symbol><symbol id="icon-eds-i-copy-link" viewBox="0 0 24 24"><path fill-rule="evenodd" clip-rule="evenodd" d="M19.4594 8.57015C19.0689 8.17963 19.0689 7.54646 19.4594 7.15594L20.2927 6.32261C20.2927 6.32261 20.2927 6.32261 20.2927 6.32261C21.0528 5.56252 21.0528 4.33019 20.2928 3.57014C19.5327 2.81007 18.3004 2.81007 17.5404 3.57014L16.7071 4.40347C16.3165 4.794 15.6834 4.794 15.2928 4.40348C14.9023 4.01296 14.9023 3.3798 15.2928 2.98927L16.1262 2.15594C17.6673 0.614803 20.1659 0.614803 21.707 2.15593C23.2481 3.69705 23.248 6.19569 21.707 7.7368L20.8737 8.57014C20.4831 8.96067 19.85 8.96067 19.4594 8.57015Z"/><path fill-rule="evenodd" clip-rule="evenodd" d="M18.0944 5.90592C18.4849 6.29643 18.4849 6.9296 18.0944 7.32013L16.4278 8.9868C16.0373 9.37733 15.4041 9.37734 15.0136 8.98682C14.6231 8.59631 14.6231 7.96314 15.0136 7.57261L16.6802 5.90594C17.0707 5.51541 17.7039 5.5154 18.0944 5.90592Z"/><path fill-rule="evenodd" clip-rule="evenodd" d="M13.5113 6.32243C13.9018 6.71295 13.9018 7.34611 13.5113 7.73664L12.678 8.56997C12.678 8.56997 12.678 8.56997 12.678 8.56997C11.9179 9.33006 11.9179 10.5624 12.6779 11.3224C13.438 12.0825 14.6703 12.0825 15.4303 11.3224L16.2636 10.4891C16.6542 10.0986 17.2873 10.0986 17.6779 10.4891C18.0684 10.8796 18.0684 11.5128 17.6779 11.9033L16.8445 12.7366C15.3034 14.2778 12.8048 14.2778 11.2637 12.7366C9.72262 11.1955 9.72266 8.69689 11.2637 7.15578L12.097 6.32244C12.4876 5.93191 13.1207 5.93191 13.5113 6.32243Z"/><path d="M8 20V22H19.4619C20.136 22 20.7822 21.7311 21.2582 21.2529C21.7333 20.7757 22 20.1289 22 19.4549V15C22 14.4477 21.5523 14 21 14C20.4477 14 20 14.4477 20 15V19.4549C20 19.6004 19.9426 19.7397 19.8408 19.842C19.7399 19.9433 19.6037 20 19.4619 20H8Z"/><path d="M4 13H2V19.4619C2 20.136 2.26889 20.7822 2.74705 21.2582C3.22434 21.7333 3.87105 22 4.5451 22H9C9.55228 22 10 21.5523 10 21C10 20.4477 9.55228 20 9 20H4.5451C4.39957 20 4.26028 19.9426 4.15804 19.8408C4.05668 19.7399 4 19.6037 4 19.4619V13Z"/><path d="M4 13H2V4.53808C2 3.86398 2.26889 3.21777 2.74705 2.74178C3.22434 2.26666 3.87105 2 4.5451 2H9C9.55228 2 10 2.44772 10 3C10 3.55228 9.55228 4 9 4H4.5451C4.39957 4 4.26028 4.05743 4.15804 4.15921C4.05668 4.26011 4 4.39633 4 4.53808V13Z"/></symbol><symbol id="icon-eds-i-funding-dollar" viewBox="0 0 32 32"><path d="M17.333 7.79549V9.21808C18.3681 9.32469 19.2889 9.82002 19.9444 10.5523C20.2938 10.9427 20.5697 11.4022 20.7488 11.9089C20.9942 12.6031 20.6303 13.3649 19.936 13.6103C19.2418 13.8558 18.48 13.4919 18.2346 12.7976C18.1735 12.6249 18.0788 12.4665 17.9574 12.3308C17.6988 12.0419 17.3272 11.8632 16.9122 11.8632H16.042C16.028 11.8636 16.0139 11.8639 15.9997 11.8639C15.9907 11.8639 15.9817 11.8638 15.9727 11.8636C15.9676 11.8635 15.9624 11.8634 15.9573 11.8632H14.7952C14.1833 11.8632 13.6872 12.3593 13.6872 12.9713C13.6872 13.492 14.0498 13.9424 14.5584 14.0537L17.7816 14.7588C19.6498 15.1675 20.9806 16.8226 20.9806 18.734C20.9806 20.8383 19.3827 22.5712 17.333 22.7819V24.2051C17.333 24.9415 16.7361 25.5384 15.9997 25.5384C15.2633 25.5384 14.6663 24.9415 14.6663 24.2051V22.7817C13.0793 22.618 11.7653 21.5424 11.2524 20.091C11.007 19.3967 11.3709 18.635 12.0651 18.3896C12.7594 18.1442 13.5212 18.5081 13.7666 19.2024C13.9597 19.7486 14.4807 20.1367 15.0889 20.1367H15.9849C15.9898 20.1367 15.9947 20.1366 15.9997 20.1366C16.0046 20.1366 16.0095 20.1367 16.0144 20.1367H16.9122C17.6857 20.1367 18.3139 19.5088 18.3139 18.734C18.3139 18.0748 17.8548 17.5045 17.2118 17.3639L13.9886 16.6588C12.2557 16.2797 11.0205 14.7451 11.0205 12.9713C11.0205 10.9297 12.6413 9.26664 14.6663 9.19869V7.79549C14.6663 7.05911 15.2633 6.46216 15.9997 6.46216C16.7361 6.46216 17.333 7.05911 17.333 7.79549Z"/><path fill-rule="evenodd" clip-rule="evenodd" d="M15.9997 1.33325C7.8995 1.33325 1.33301 7.89974 1.33301 15.9999C1.33301 24.1002 7.89951 30.6666 15.9997 30.6666C24.1 30.6666 30.6663 24.1002 30.6663 15.9999C30.6663 7.89975 24.1 1.33325 15.9997 1.33325ZM3.99967 15.9999C3.99967 9.3725 9.37226 3.99992 15.9997 3.99992C22.6272 3.99992 27.9997 9.3725 27.9997 15.9999C27.9997 22.6274 22.6272 27.9999 15.9997 27.9999C9.37225 27.9999 3.99967 22.6274 3.99967 15.9999Z"/></symbol><symbol id="icon-eds-i-github-medium" viewBox="0 0 24 24"><path d="M 11.964844 0 C 5.347656 0 0 5.269531 0 11.792969 C 0 17.003906 3.425781 21.417969 8.179688 22.976562 C 8.773438 23.09375 8.992188 22.722656 8.992188 22.410156 C 8.992188 22.136719 8.972656 21.203125 8.972656 20.226562 C 5.644531 20.929688 4.953125 18.820312 4.953125 18.820312 C 4.417969 17.453125 3.625 17.101562 3.625 17.101562 C 2.535156 16.378906 3.703125 16.378906 3.703125 16.378906 C 4.914062 16.457031 5.546875 17.589844 5.546875 17.589844 C 6.617188 19.386719 8.339844 18.878906 9.03125 18.566406 C 9.132812 17.804688 9.449219 17.277344 9.785156 16.984375 C 7.132812 16.710938 4.339844 15.695312 4.339844 11.167969 C 4.339844 9.878906 4.8125 8.824219 5.566406 8.003906 C 5.445312 7.710938 5.03125 6.5 5.683594 4.878906 C 5.683594 4.878906 6.695312 4.566406 8.972656 6.089844 C 9.949219 5.832031 10.953125 5.703125 11.964844 5.699219 C 12.972656 5.699219 14.003906 5.835938 14.957031 6.089844 C 17.234375 4.566406 18.242188 4.878906 18.242188 4.878906 C 18.898438 6.5 18.480469 7.710938 18.363281 8.003906 C 19.136719 8.824219 19.589844 9.878906 19.589844 11.167969 C 19.589844 15.695312 16.796875 16.691406 14.125 16.984375 C 14.558594 17.355469 14.933594 18.058594 14.933594 19.171875 C 14.933594 20.753906 14.914062 22.019531 14.914062 22.410156 C 14.914062 22.722656 15.132812 23.09375 15.726562 22.976562 C 20.480469 21.414062 23.910156 17.003906 23.910156 11.792969 C 23.929688 5.269531 18.558594 0 11.964844 0 Z M 11.964844 0 "/></symbol><symbol id="icon-eds-i-institution-medium" viewBox="0 0 24 24"><g><path fill-rule="evenodd" clip-rule="evenodd" d="M11.9967 1C11.6364 1 11.279 1.0898 10.961 1.2646C10.9318 1.28061 10.9035 1.29806 10.8761 1.31689L2.79765 6.87C2.46776 7.08001 2.20618 7.38466 2.07836 7.76668C1.94823 8.15561 1.98027 8.55648 2.12665 8.90067C2.42086 9.59246 3.12798 10 3.90107 10H4.99994V16H4.49994C3.11923 16 1.99994 17.1193 1.99994 18.5V19.5C1.99994 20.8807 3.11923 22 4.49994 22H19.4999C20.8807 22 21.9999 20.8807 21.9999 19.5V18.5C21.9999 17.1193 20.8807 16 19.4999 16H18.9999V10H20.0922C20.8653 10 21.5725 9.59252 21.8667 8.90065C22.0131 8.55642 22.0451 8.15553 21.9149 7.7666C21.7871 7.38459 21.5255 7.07997 21.1956 6.86998L13.1172 1.31689C13.0898 1.29806 13.0615 1.28061 13.0324 1.2646C12.7143 1.0898 12.357 1 11.9967 1ZM4.6844 8L11.9472 3.00755C11.9616 3.00295 11.9783 3 11.9967 3C12.015 3 12.0318 3.00295 12.0461 3.00755L19.3089 8H4.6844ZM16.9999 16V10H14.9999V16H16.9999ZM12.9999 16V10H10.9999V16H12.9999ZM8.99994 16V10H6.99994V16H8.99994ZM3.99994 18.5C3.99994 18.2239 4.2238 18 4.49994 18H19.4999C19.7761 18 19.9999 18.2239 19.9999 18.5V19.5C19.9999 19.7761 19.7761 20 19.4999 20H4.49994C4.2238 20 3.99994 19.7761 3.99994 19.5V18.5Z"/></g></symbol><symbol id="icon-eds-i-limited-access" viewBox="0 0 16 16"><path fill-rule="evenodd" d="M4 3v3a1 1 0 0 1-2 0V2.923C2 1.875 2.84 1 3.909 1h5.909a1 1 0 0 1 .713.298l3.181 3.231a1 1 0 0 1 .288.702V6a1 1 0 1 1-2 0v-.36L9.4 3H4ZM3 8a1 1 0 0 1 1 1v1a1 1 0 1 1-2 0V9a1 1 0 0 1 1-1Zm10 0a1 1 0 0 1 1 1v1a1 1 0 1 1-2 0V9a1 1 0 0 1 1-1Zm-3.5 6a1 1 0 0 1-1 1h-1a1 1 0 1 1 0-2h1a1 1 0 0 1 1 1Zm2.441-1a1 1 0 0 1 2 0c0 .73-.246 1.306-.706 1.664a1.61 1.61 0 0 1-.876.334l-.032.002H11.5a1 1 0 1 1 0-2h.441ZM4 13a1 1 0 0 0-2 0c0 .73.247 1.306.706 1.664a1.609 1.609 0 0 0 .876.334l.032.002H4.5a1 1 0 1 0 0-2H4Z" clip-rule="evenodd"/></symbol><symbol id="icon-eds-i-marker-filled"><path d="M19.7998 21.1049C19.7998 21.7661 19.4529 22.3363 18.9053 22.6263C18.3697 22.9099 17.7222 22.8847 17.1855 22.5629C17.1651 22.5506 17.1445 22.5375 17.125 22.5238L11.999 18.9261L6.87402 22.5238C6.85458 22.5374 6.83481 22.5506 6.81445 22.5629C6.25918 22.896 5.60347 22.8929 5.08398 22.6469C4.54665 22.3923 4.09961 21.8431 4.09961 21.1049V3.57751C4.09961 2.22526 5.14769 1.17712 6.5 1.17712H17.3994C18.7517 1.17712 19.7998 2.22526 19.7998 3.57751V21.1049Z"/></symbol><symbol id="icon-eds-i-marker-unfilled"><path d="M17.7998 3.57751C17.7998 3.32977 17.6471 3.17712 17.3994 3.17712H6.5C6.25231 3.17712 6.09961 3.32977 6.09961 3.57751V20.6244L11.4248 16.8871L11.5596 16.807C11.8831 16.6484 12.2726 16.6755 12.5742 16.8871L17.7998 20.5531V3.57751ZM19.7998 21.1049C19.7998 21.7661 19.4529 22.3363 18.9053 22.6263C18.3697 22.9099 17.7222 22.8847 17.1855 22.5629C17.1651 22.5506 17.1445 22.5375 17.125 22.5238L11.999 18.9261L6.87402 22.5238C6.85458 22.5374 6.83481 22.5506 6.81445 22.5629C6.25918 22.896 5.60347 22.8929 5.08398 22.6469C4.54665 22.3923 4.09961 21.8431 4.09961 21.1049V3.57751C4.09961 2.22526 5.14769 1.17712 6.5 1.17712H17.3994C18.7517 1.17712 19.7998 2.22526 19.7998 3.57751V21.1049Z"/></symbol><symbol id="icon-eds-i-quotation-mark-large" viewBox="0 0 48 48"><path d="M25.749 28.9958C25.749 19.5707 32.5601 9.711 43.2492 7.5L44.9712 11.3977C41.0949 12.9285 37.6859 18.258 37.3061 21.932C41.6654 22.6507 44.9992 26.6015 44.9992 31.3718C44.9992 37.2055 40.4772 40.5 35.9009 40.5C30.6246 40.5 25.749 36.2742 25.749 28.9958Z"/><path d="M3 28.9958C3 19.5707 9.81107 9.711 20.5002 7.5L22.2222 11.3977C18.3459 12.9285 14.9369 18.258 14.5571 21.932C18.9164 22.6507 22.2502 26.6015 22.2502 31.3718C22.2502 37.2055 17.7282 40.5 13.1519 40.5C7.87555 40.5 3 36.2742 3 28.9958Z"/></symbol><symbol id="icon-eds-i-rss" viewBox="0 0 22 22"><path d="M1.96094 1C1.96094 0.447715 2.40865 0 2.96094 0C5.46109 0 7.93678 0.492038 10.2467 1.44806C12.5565 2.40407 14.6554 3.80534 16.4234 5.57189C18.1913 7.33843 19.5939 9.4357 20.5508 11.744C21.5077 14.0522 22.0001 16.5263 22.0001 19.0247C22.0001 19.577 21.5524 20.0247 21.0001 20.0247C20.4478 20.0247 20.0001 19.577 20.0001 19.0247C20.0001 16.7891 19.5595 14.5753 18.7033 12.5098C17.8471 10.4444 16.5919 8.56762 15.0097 6.98666C13.4275 5.40575 11.5492 4.15167 9.48182 3.29604C7.41447 2.4404 5.19868 2 2.96094 2C2.40865 2 1.96094 1.55228 1.96094 1Z"/><path fill-rule="evenodd" clip-rule="evenodd" d="M0 18.649C0 16.7974 1.50196 15.298 3.35294 15.298C5.20392 15.298 6.70588 16.7974 6.70588 18.649C6.70588 20.5003 5.20397 22 3.35294 22C1.50191 22 0 20.5003 0 18.649ZM3.35294 17.298C2.60493 17.298 2 17.9036 2 18.649C2 19.3943 2.60498 20 3.35294 20C4.1009 20 4.70588 19.3943 4.70588 18.649C4.70588 17.9036 4.10095 17.298 3.35294 17.298Z"/><path d="M3.3374 7.46115C2.78512 7.46115 2.3374 7.90887 2.3374 8.46115C2.3374 9.01344 2.78512 9.46115 3.3374 9.46115C4.54515 9.46115 5.74107 9.69885 6.85684 10.1606C7.97262 10.6224 8.98639 11.2993 9.84028 12.1525C10.6942 13.0057 11.3715 14.0185 11.8336 15.1332C12.2956 16.2478 12.5335 17.4424 12.5335 18.649C12.5335 19.2013 12.9812 19.649 13.5335 19.649C14.0858 19.649 14.5335 19.2013 14.5335 18.649C14.5335 17.1796 14.2438 15.7247 13.6811 14.3673C13.1184 13.0099 12.2936 11.7765 11.2539 10.7377C10.2142 9.69885 8.97999 8.87484 7.62168 8.31266C6.26337 7.75049 4.80757 7.46115 3.3374 7.46115Z"/></symbol><symbol id="icon-eds-i-search-category-medium" viewBox="0 0 32 32"><path fill-rule="evenodd" d="M2 5.306A3.306 3.306 0 0 1 5.306 2h5.833a3.306 3.306 0 0 1 3.306 3.306v5.833a3.306 3.306 0 0 1-3.306 3.305H5.306A3.306 3.306 0 0 1 2 11.14V5.306Zm3.306-.584a.583.583 0 0 0-.584.584v5.833c0 .322.261.583.584.583h5.833a.583.583 0 0 0 .583-.583V5.306a.583.583 0 0 0-.583-.584H5.306Zm15.555 8.945a7.194 7.194 0 1 0 4.034 13.153l2.781 2.781a1.361 1.361 0 1 0 1.925-1.925l-2.781-2.781a7.194 7.194 0 0 0-5.958-11.228Zm3.173 10.346a4.472 4.472 0 1 0-.021.021l.01-.01.011-.011Zm-5.117-19.29a.583.583 0 0 0-.584.583v5.833a1.361 1.361 0 0 1-2.722 0V5.306A3.306 3.306 0 0 1 18.917 2h5.833a3.306 3.306 0 0 1 3.306 3.306v5.833c0 .6-.161 1.166-.443 1.654a1.361 1.361 0 1 1-2.357-1.363.575.575 0 0 0 .078-.291V5.306a.583.583 0 0 0-.584-.584h-5.833ZM2 18.916a3.306 3.306 0 0 1 3.306-3.306h5.833a1.361 1.361 0 1 1 0 2.722H5.306a.583.583 0 0 0-.584.584v5.833c0 .322.261.583.584.583h5.833a.574.574 0 0 0 .29-.077 1.361 1.361 0 1 1 1.364 2.356 3.296 3.296 0 0 1-1.654.444H5.306A3.306 3.306 0 0 1 2 24.75v-5.833Z" clip-rule="evenodd"/></symbol><symbol id="icon-eds-i-search-magic" viewBox="0 0 20 20"><path d="M8.695 1.667a9.1 9.1 0 0 1 1.756.17A3.098 3.098 0 0 0 9.436 3.37a7.333 7.333 0 0 0-.738-.038c-3.841 0-6.956 2.986-6.956 6.668 0 3.681 3.115 6.665 6.956 6.665 3.642 0 6.627-2.681 6.928-6.096a3.19 3.19 0 0 0 1.763-.548 8.091 8.091 0 0 1-1.961 5.25l3.446 3.306a.81.81 0 0 1 0 1.178.897.897 0 0 1-1.23 0l-3.447-3.303a8.892 8.892 0 0 1-5.502 1.88C3.893 18.334 0 14.603 0 10c0-4.603 3.893-8.333 8.695-8.334Z"/><path d="M20 4.166a.663.663 0 0 1-.128.395.709.709 0 0 1-.342.251l-2.341.827-.863 2.244a.693.693 0 0 1-.263.326.74.74 0 0 1-.821 0 .693.693 0 0 1-.264-.326l-.862-2.244-2.341-.827a.715.715 0 0 1-.341-.252.669.669 0 0 1 0-.787.715.715 0 0 1 .34-.252l2.342-.827.862-2.244a.693.693 0 0 1 .264-.327.74.74 0 0 1 .821 0 .7.7 0 0 1 .263.327l.863 2.244 2.34.827a.709.709 0 0 1 .343.251.663.663 0 0 1 .128.394Z"/></symbol><symbol id="icon-eds-i-subjects-medium" viewBox="0 0 24 24"><g id="icon-subjects-copy" stroke="none" stroke-width="1" fill-rule="evenodd"><path d="M13.3846154,2 C14.7015971,2 15.7692308,3.06762994 15.7692308,4.38461538 L15.7692308,7.15384615 C15.7692308,8.47082629 14.7015955,9.53846154 13.3846154,9.53846154 L13.1038388,9.53925278 C13.2061091,9.85347965 13.3815528,10.1423885 13.6195822,10.3804178 C13.9722182,10.7330539 14.436524,10.9483278 14.9293854,10.9918129 L15.1153846,11 C16.2068332,11 17.2535347,11.433562 18.0254647,12.2054189 C18.6411944,12.8212361 19.0416785,13.6120766 19.1784166,14.4609738 L19.6153846,14.4615385 C20.932386,14.4615385 22,15.5291672 22,16.8461538 L22,19.6153846 C22,20.9323924 20.9323924,22 19.6153846,22 L16.8461538,22 C15.5291672,22 14.4615385,20.932386 14.4615385,19.6153846 L14.4615385,16.8461538 C14.4615385,15.5291737 15.5291737,14.4615385 16.8461538,14.4615385 L17.126925,14.460779 C17.0246537,14.1465537 16.8492179,13.857633 16.6112344,13.6196157 C16.2144418,13.2228606 15.6764136,13 15.1153846,13 C14.0239122,13 12.9771569,12.5664197 12.2053686,11.7946314 C12.1335167,11.7227795 12.0645962,11.6485444 11.9986839,11.5721119 C11.9354038,11.6485444 11.8664833,11.7227795 11.7946314,11.7946314 C11.0228431,12.5664197 9.97608778,13 8.88461538,13 C8.323576,13 7.78552852,13.2228666 7.38881294,13.6195822 C7.15078359,13.8576115 6.97533988,14.1465203 6.8730696,14.4607472 L7.15384615,14.4615385 C8.47082629,14.4615385 9.53846154,15.5291737 9.53846154,16.8461538 L9.53846154,19.6153846 C9.53846154,20.932386 8.47083276,22 7.15384615,22 L4.38461538,22 C3.06762347,22 2,20.9323876 2,19.6153846 L2,16.8461538 C2,15.5291721 3.06762994,14.4615385 4.38461538,14.4615385 L4.8215823,14.4609378 C4.95831893,13.6120029 5.3588057,12.8211623 5.97459937,12.2053686 C6.69125996,11.488708 7.64500941,11.0636656 8.6514968,11.0066017 L8.88461538,11 C9.44565477,11 9.98370225,10.7771334 10.3804178,10.3804178 C10.6184472,10.1423885 10.7938909,9.85347965 10.8961612,9.53925278 L10.6153846,9.53846154 C9.29840448,9.53846154 8.23076923,8.47082629 8.23076923,7.15384615 L8.23076923,4.38461538 C8.23076923,3.06762994 9.29840286,2 10.6153846,2 L13.3846154,2 Z M7.15384615,16.4615385 L4.38461538,16.4615385 C4.17220099,16.4615385 4,16.63374 4,16.8461538 L4,19.6153846 C4,19.8278134 4.17218833,20 4.38461538,20 L7.15384615,20 C7.36626945,20 7.53846154,19.8278103 7.53846154,19.6153846 L7.53846154,16.8461538 C7.53846154,16.6337432 7.36625679,16.4615385 7.15384615,16.4615385 Z M19.6153846,16.4615385 L16.8461538,16.4615385 C16.6337432,16.4615385 16.4615385,16.6337432 16.4615385,16.8461538 L16.4615385,19.6153846 C16.4615385,19.8278103 16.6337306,20 16.8461538,20 L19.6153846,20 C19.8278229,20 20,19.8278229 20,19.6153846 L20,16.8461538 C20,16.6337306 19.8278103,16.4615385 19.6153846,16.4615385 Z M13.3846154,4 L10.6153846,4 C10.4029708,4 10.2307692,4.17220099 10.2307692,4.38461538 L10.2307692,7.15384615 C10.2307692,7.36625679 10.402974,7.53846154 10.6153846,7.53846154 L13.3846154,7.53846154 C13.597026,7.53846154 13.7692308,7.36625679 13.7692308,7.15384615 L13.7692308,4.38461538 C13.7692308,4.17220099 13.5970292,4 13.3846154,4 Z" id="Shape" fill-rule="nonzero"/></g></symbol><symbol id="icon-eds-small-arrow-left" viewBox="0 0 16 17"><path stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M14 8.092H2m0 0L8 2M2 8.092l6 6.035"/></symbol><symbol id="icon-eds-small-arrow-right" viewBox="0 0 16 16"><g fill-rule="evenodd" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2"><path d="M2 8.092h12M8 2l6 6.092M8 14.127l6-6.035"/></g></symbol><symbol id="icon-funding-dollar-green-dot" viewBox="0 0 33 33"><path d="M16.25 0C25.2246 0 32.5 7.27537 32.5 16.25C32.5 25.2246 25.2246 32.5 16.25 32.5C15.5596 32.5 15 31.9404 15 31.25C15 30.5596 15.5596 30 16.25 30C23.8439 30 30 23.8439 30 16.25C30 8.65608 23.8439 2.5 16.25 2.5C8.65608 2.5 2.5 8.65608 2.5 16.25C2.5 16.9404 1.94036 17.5 1.25 17.5C0.559644 17.5 0 16.9404 0 16.25C0 7.27537 7.27537 0 16.25 0ZM16.3428 6.25C17.0956 6.25024 17.7059 6.86048 17.7061 7.61328V9.06836C18.7647 9.1774 19.7066 9.68466 20.377 10.4336C20.7341 10.8327 21.0161 11.3024 21.1992 11.8203C21.4502 12.5304 21.0782 13.3095 20.3682 13.5605C19.6581 13.8115 18.8789 13.4395 18.6279 12.7295C18.5655 12.5529 18.4689 12.3907 18.3447 12.252C18.0802 11.9565 17.6998 11.7734 17.2754 11.7734H16.3857C16.3715 11.7739 16.3571 11.7744 16.3428 11.7744C16.3336 11.7744 16.3236 11.7746 16.3145 11.7744C16.3093 11.7743 16.304 11.7736 16.2988 11.7734H15.1104C14.4846 11.7735 13.9775 12.2814 13.9775 12.9072C13.9776 13.4396 14.3481 13.8998 14.8682 14.0137L18.165 14.7354C20.0755 15.1534 21.4365 16.846 21.4365 18.8008C21.4365 20.9528 19.8023 22.7249 17.7061 22.9404V24.3965C17.7059 25.1493 17.0956 25.7595 16.3428 25.7598C15.5898 25.7598 14.9787 25.1495 14.9785 24.3965V22.9404C13.3555 22.7729 12.0119 21.6727 11.4873 20.1885C11.2364 19.4784 11.6083 18.6992 12.3184 18.4482C13.0283 18.1974 13.8075 18.5695 14.0586 19.2793C14.256 19.8379 14.7891 20.2354 15.4111 20.2354H17.2754C18.0665 20.2354 18.709 19.5932 18.709 18.8008C18.709 18.1266 18.2397 17.5433 17.582 17.3994L14.2852 16.6787C12.5131 16.2909 11.2501 14.7212 11.25 12.9072C11.25 10.8193 12.9075 9.11841 14.9785 9.04883V7.61328C14.9787 6.86033 15.5898 6.25 16.3428 6.25Z"/><path style="fill:#00A69D" d="M6.25 20.25C2.93629 20.25 0.25 22.9363 0.25 26.25C0.25 29.5637 2.93629 32.25 6.25 32.25C9.56371 32.25 12.25 29.5637 12.25 26.25C12.25 22.9363 9.56371 20.25 6.25 20.25Z"/></symbol><symbol id="icon-globe-with-star" viewBox="0 0 32 32"><path style="fill:none;stroke:#fff" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M27.282 17.026c0 6.797-5.51 12.307-12.307 12.307-6.798 0-12.308-5.51-12.308-12.307 0-6.798 5.51-12.308 12.308-12.308M2.667 17.026h14.201"/><path style="fill:none;stroke:#fff" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M14.975 4.718a21.244 21.244 0 0 0-4.734 12.308c.233 4.5 1.89 8.81 4.734 12.307a21.245 21.245 0 0 0 4.394-9.467M17.045 9.72c-.709-.126-.709-1.16 0-1.286 2.568-.454 4.61-2.443 5.168-5.032l.043-.199c.153-.712 1.15-.716 1.31-.006l.052.232c.578 2.577 2.621 4.549 5.182 5.002.712.126.712 1.166 0 1.292-2.561.453-4.604 2.425-5.182 5.002l-.052.231c-.16.711-1.157.707-1.31-.005l-.043-.199c-.557-2.59-2.6-4.578-5.168-5.032Z"/></symbol><symbol id="icon-journal-medium-green-dot" viewBox="0 0 32 32"><path style="stroke:#222222;stroke-width:2.5;stroke-linecap:round;fill:transparent" d="M11 2L8.51877 2.00121C7.56562 2.00168 7.08905 2.00191 6.73603 2.21084C6.51866 2.33949 6.3373 2.52093 6.20876 2.73836C6 3.09149 6 3.56806 6 4.52121L6 13.2518V15.5M11 2H24.6133C25.5669 2 26.0437 2 26.3969 2.20889C26.6143 2.3375 26.7958 2.51896 26.9244 2.73644C27.1333 3.08965 27.1333 3.56547 27.1333 4.51712C27.1333 10.3583 27.1333 19.2745 27.1333 20.9995C27.1333 22.0923 27.0835 22.9368 26.984 23.5324C26.9756 23.5832 26.9713 23.6085 26.7521 23.9008C26.6766 24.0015 26.3534 24.2752 26.2416 24.3331C25.9172 24.501 25.8079 24.501 25.5893 24.501L18 24.5M11 2V12V16M27.5 28.5011H17.5M18.6271 6.50113H19.6071C20.5607 6.50113 21.0375 6.50113 21.3907 6.71001C21.6082 6.83863 21.7896 7.02009 21.9183 7.23757C22.1271 7.59077 22.1271 8.06756 22.1271 9.02113V10.4811C22.1271 11.4347 22.1271 11.9115 21.9183 12.2647C21.7896 12.4822 21.6082 12.6636 21.3907 12.7922C21.0375 13.0011 20.5607 13.0011 19.6071 13.0011H17.6471C16.6936 13.0011 16.2168 13.0011 15.8636 12.7922C15.6461 12.6636 15.4646 12.4822 15.336 12.2647C15.1271 11.9115 15.1271 11.4347 15.1271 10.4811V9.02113C15.1271 8.06756 15.1271 7.59077 15.336 7.23757C15.4646 7.02009 15.6461 6.83863 15.8636 6.71001C16.2168 6.50113 16.6936 6.50113 17.6471 6.50113H18.6271Z"/><path style="fill:#00A69D" d="M8 19C4.68629 19 2 21.6863 2 25C2 28.3137 4.68629 31 8 31C11.3137 31 14 28.3137 14 25C14 21.6863 11.3137 19 8 19Z"/></symbol><symbol id="icon-orcid-logo" viewBox="0 0 40 40"><path fill-rule="evenodd" d="M12.281 10.453c.875 0 1.578-.719 1.578-1.578 0-.86-.703-1.578-1.578-1.578-.875 0-1.578.703-1.578 1.578 0 .86.703 1.578 1.578 1.578Zm-1.203 18.641h2.406V12.359h-2.406v16.735Z"/><path fill-rule="evenodd" d="M17.016 12.36h6.5c6.187 0 8.906 4.421 8.906 8.374 0 4.297-3.36 8.375-8.875 8.375h-6.531V12.36Zm6.234 14.578h-3.828V14.53h3.703c4.688 0 6.828 2.844 6.828 6.203 0 2.063-1.25 6.203-6.703 6.203Z" clip-rule="evenodd"/></symbol><symbol id="icon-thumbs-down" viewBox="41 0 33 33"><path stroke="currentcolor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M61 17.611h-1l-2.085 4.436a1 1 0 0 1-.366.332 1.04 1.04 0 0 1-1.197-.159.95.95 0 0 1-.298-.678v-3.32h-3.93c-.61 0-.674-1.066-.599-1.55l.726-4.301a.98.98 0 0 1 .341-.622 1.06 1.06 0 0 1 .685-.249H61M63.5 11.5v6"/></symbol><symbol id="icon-thumbs-up-medium" viewBox="0 0 24 24"><path d="M6.75 9.33333H8.25L11.3778 2.67913C11.5138 2.47128 11.7025 2.29994 11.9263 2.18116C12.1502 2.06239 12.4018 2.00005 12.6576 2C13.056 1.99999 13.4384 2.15092 13.7214 2.41998C14.0044 2.68903 14.1652 3.05442 14.1688 3.43663V8.41667C16.1342 8.41667 18.1116 8.41667 20.0648 8.41667C20.9795 8.41667 21.0744 10.0164 20.9625 10.7422L19.8733 17.194C19.8269 17.5542 19.6449 17.8856 19.3616 18.1261C19.0783 18.3667 18.7132 18.4996 18.3349 18.5H6.75" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"/><path d="M3 18.5V9.5" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"/></symbol><symbol id="icon-thumbs-up"><path stroke="currentcolor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M13 15.389h1l2.085-4.436a1 1 0 0 1 .366-.332 1.04 1.04 0 0 1 1.197.159.95.95 0 0 1 .298.678v3.32h3.93c.61 0 .674 1.066.599 1.55l-.726 4.301a.98.98 0 0 1-.341.622 1.06 1.06 0 0 1-.685.249H13m-2.5 0v-6"/></symbol><symbol id="icon-tick-with-curly-circle" viewBox="0 0 40 40"><path d="M17.923.757a3.227 3.227 0 0 1 4.401.231l2.168 2.25 3.001-.866.33-.076a3.23 3.23 0 0 1 3.598 2.076l.099.325.75 3.03 3.033.753a3.229 3.229 0 0 1 2.4 3.697l-.075.33-.865 3.001 2.249 2.168.231.247a3.227 3.227 0 0 1-.231 4.401l-2.25 2.166.866 3.003.076.33a3.228 3.228 0 0 1-2.401 3.697l-3.033.75-.75 3.033a3.229 3.229 0 0 1-4.027 2.325l-3.001-.867-2.168 2.25a3.227 3.227 0 0 1-4.648 0l-2.17-2.25-2.999.867a3.229 3.229 0 0 1-4.027-2.325l-.752-3.033-3.03-.75a3.229 3.229 0 0 1-2.326-4.027l.865-3.001-2.249-2.168a3.227 3.227 0 0 1 0-4.648l2.25-2.17-.866-2.999A3.229 3.229 0 0 1 4.697 8.48l3.031-.752.752-3.03a3.229 3.229 0 0 1 4.027-2.326l3 .865L17.675.988l.247-.231Zm.027 5.156a3.23 3.23 0 0 1-3.219.864l-2.838-.82-.712 2.87a3.227 3.227 0 0 1-2.355 2.354l-2.868.712.819 2.838a3.23 3.23 0 0 1-.864 3.219L3.786 20l2.127 2.05a3.23 3.23 0 0 1 .864 3.219l-.82 2.837 2.87.713.215.062a3.228 3.228 0 0 1 2.14 2.293l.71 2.867 2.84-.818a3.229 3.229 0 0 1 3.218.864L20 36.214l2.05-2.127.16-.156a3.228 3.228 0 0 1 2.841-.76l.218.052 2.837.818.713-2.867.062-.214a3.227 3.227 0 0 1 2.293-2.141l2.867-.713-.816-2.837c-.331-1.15 0-2.389.861-3.219L36.212 20l-2.126-2.05a3.229 3.229 0 0 1-.861-3.219l.816-2.838-2.867-.712a3.228 3.228 0 0 1-2.355-2.355l-.713-2.868-2.837.819a3.23 3.23 0 0 1-3.219-.864L20 3.786l-2.05 2.127Z" stroke="transparent" stroke-width=".9"/><path d="M25.723 13.406a1.807 1.807 0 0 1 2.699 2.397l-9.658 12.074a1.808 1.808 0 0 1-2.497.316L11.44 24.57a1.806 1.806 0 1 1 2.168-2.892l3.427 2.57 8.565-10.704.124-.138Z" stroke="transparent" stroke-width=".9"/></symbol></svg>
</div>


        

        
        
    <a class="c-skip-link" href="#main">Skip to main content</a>

    
        
    <aside class="u-lazy-ad-wrapper u-mbs-0" aria-label="Advertisement">
        <div class="c-ad c-ad--728x90 c-ad--conditional" data-test="springer-doubleclick-ad">
            <div class="c-ad c-ad__inner" >
                <p class="c-ad__label">Advertisement</p>
                <div id="div-gpt-ad-LB1"
                     class="div-gpt-ad grade-c-hide"
                     data-gpt
                     data-gpt-unitpath="/270604982/springerlink/521/article"
                     data-gpt-sizes="728x90"
                     data-gpt-targeting="pos=top;articleid=s00521-024-09417-3;"
                     data-ad-type="top"
                     style="min-width:728px;min-height:90px">
                    
                    <script>
                        window.SN = window.SN || {};
                        window.SN.libs = window.SN.libs || {};
                        window.SN.libs.ads = window.SN.libs.ads || {};
                        window.SN.libs.ads.slotConfig = window.SN.libs.ads.slotConfig || {};
                        window.SN.libs.ads.slotConfig['LB1'] = {
                            'pos': 'top',
                            'type': 'top',
                        };
                        window.SN.libs.ads.slotConfig['unitPath'] = '/270604982/springerlink/521/article';
                    </script>
                    <noscript>
                        <a href="//pubads.g.doubleclick.net/gampad/jump?iu=/270604982/springerlink/521/article&amp;sz=728x90&amp;pos=top&amp;articleid=s00521-024-09417-3">
                            <img data-test="gpt-advert-fallback-img"
                                 src="//pubads.g.doubleclick.net/gampad/ad?iu=/270604982/springerlink/521/article&amp;sz=728x90&amp;pos=top&amp;articleid=s00521-024-09417-3"
                                 alt="Advertisement"
                                 width="728"
                                 height="90">
                        </a>
                    </noscript>
                </div>
            </div>
        </div>
    </aside>

    

    <header class="eds-c-header" data-eds-c-header>
    <div class="eds-c-header__container" data-eds-c-header-expander-anchor>
        <div class="eds-c-header__brand">
            
                
                    <a href="https://link.springer.com"
                    	 data-test=springerlink-logo
                        
                            data-track="click_imprint_logo"
                        
                            data-track-context="unified header"
                        
                            data-track-action="click logo link"
                        
                            data-track-category="unified header"
                        
                            data-track-label="link"
                        
					>
                        <img src="/oscar-static/images/darwin/header/img/logo-springer-nature-link-3149409f62.svg" alt="Springer Nature Link">
                    </a>
                
            
        </div>

        
            
                
    
        <a class="c-header__link eds-c-header__link" id="identity-account-widget" data-track="click_login" data-track-context="header" href='https://idp.springer.com/auth/personal/springernature?redirect_uri=https://link.springer.com/article/10.1007/s00521-024-09417-3?'><span class="eds-c-header__widget-fragment-title">Log in</span></a>
    


            
        
    </div>

    
        <nav class="eds-c-header__nav" aria-label="header navigation">
            <div class="eds-c-header__nav-container">
                <div class="eds-c-header__item eds-c-header__item--menu">
                   <a href="#eds-c-header-nav" class="eds-c-header__link" data-eds-c-header-expander>
                        <svg class="eds-c-header__icon" width="24" height="24" aria-hidden="true" focusable="false">
                            <use xlink:href="#icon-eds-i-menu-medium"></use>
                        </svg><span>Menu</span>
                    </a>
                </div>

                <div class="eds-c-header__item eds-c-header__item--inline-links">
                    
                        <a class="eds-c-header__link" href="https://link.springer.com/journals/"
                            
                                data-track="nav_find_a_journal"
                            
                                data-track-context="unified header"
                            
                                data-track-action="click find a journal"
                            
                                data-track-category="unified header"
                            
                                data-track-label="link"
                            
						>
                            Find a journal
                        </a>
                    
                        <a class="eds-c-header__link" href="https://www.springernature.com/gp/authors"
                            
                                data-track="nav_how_to_publish"
                            
                                data-track-context="unified header"
                            
                                data-track-action="click publish with us link"
                            
                                data-track-category="unified header"
                            
                                data-track-label="link"
                            
						>
                            Publish with us
                        </a>
                    
                        <a class="eds-c-header__link" href="https://link.springernature.com/home/"
                            
                                data-track="nav_track_your_research"
                            
                                data-track-context="unified header"
                            
                                data-track-action="click track your research"
                            
                                data-track-category="unified header"
                            
                                data-track-label="link"
                            
						>
                            Track your research
                        </a>
                    
                </div>

                <div class="eds-c-header__link-container">
                    
                        <div class="eds-c-header__item eds-c-header__item--divider">
                            <a href="#eds-c-header-popup-search" class="eds-c-header__link" data-eds-c-header-expander data-eds-c-header-test-search-btn>
                                <svg class="eds-c-header__icon" width="24" height="24" aria-hidden="true" focusable="false">
                                    <use xlink:href="#icon-eds-i-search-medium"></use>
                                </svg><span>Search</span>
                            </a>
                        </div>
                    
                    
                        
                            <div id="ecommerce-header-cart-icon-link" class="eds-c-header__item ecommerce-cart" style="display:inline-block">
 <a class="eds-c-header__link" href="https://order.springer.com/public/cart" style="appearance:none;border:none;background:none;color:inherit;position:relative">
  <svg id="eds-i-cart" class="eds-c-header__icon" xmlns="http://www.w3.org/2000/svg" height="24" width="24" viewBox="0 0 24 24" aria-hidden="true" focusable="false">
   <path fill="currentColor" fill-rule="nonzero" d="M2 1a1 1 0 0 0 0 2l1.659.001 2.257 12.808a2.599 2.599 0 0 0 2.435 2.185l.167.004 9.976-.001a2.613 2.613 0 0 0 2.61-1.748l.03-.106 1.755-7.82.032-.107a2.546 2.546 0 0 0-.311-1.986l-.108-.157a2.604 2.604 0 0 0-2.197-1.076L6.042 5l-.56-3.17a1 1 0 0 0-.864-.82l-.12-.007L2.001 1ZM20.35 6.996a.63.63 0 0 1 .54.26.55.55 0 0 1 .082.505l-.028.1L19.2 15.63l-.022.05c-.094.177-.282.299-.526.317l-10.145.002a.61.61 0 0 1-.618-.515L6.394 6.999l13.955-.003ZM18 19a2 2 0 1 0 0 4 2 2 0 0 0 0-4ZM8 19a2 2 0 1 0 0 4 2 2 0 0 0 0-4Z"></path>
  </svg>
  <span>Cart</span><span class="cart-info" style="display:none;position:absolute;top:10px;right:45px;background-color:#C65301;color:#fff;width:18px;height:18px;font-size:11px;border-radius:50%;line-height:17.5px;text-align:center"></span>
 </a>
 <script>(function () { var exports = {}; if (window.fetch) {
            
            "use strict";
Object.defineProperty(exports, "__esModule", { value: true });
exports.headerWidgetClientInit = void 0;
var headerWidgetClientInit = function (getCartInfo) {
    document.body.addEventListener("updatedCart", function () {
        updateCartIcon();
    }, false);
    return updateCartIcon();
    function updateCartIcon() {
        return getCartInfo()
            .then(function (res) { return res.json(); })
            .then(refreshCartState)
            .catch(function (_) { });
    }
    function refreshCartState(json) {
        var indicator = document.querySelector("#ecommerce-header-cart-icon-link .cart-info");
        /* istanbul ignore else */
        if (indicator && json.itemCount) {
            indicator.style.display = 'block';
            indicator.textContent = json.itemCount > 9 ? '9+' : json.itemCount.toString();
            var moreThanOneItem = json.itemCount > 1;
            indicator.setAttribute('title', "there ".concat(moreThanOneItem ? "are" : "is", " ").concat(json.itemCount, " item").concat(moreThanOneItem ? "s" : "", " in your cart"));
        }
        return json;
    }
};
exports.headerWidgetClientInit = headerWidgetClientInit;

            
            headerWidgetClientInit(
              function () {
                return window.fetch("https://cart.springer.com/cart-info", {
                  credentials: "include",
                  headers: { Accept: "application/json" }
                })
              }
            )
        }})()</script>
</div>
                        
                    
                </div>
            </div>
        </nav>
    
</header>



    <article lang="en" id="main" class="app-masthead__colour-21">
        <section class="app-masthead " aria-label="article masthead">
    <div class="app-masthead__container">
        
            <div class="app-article-masthead u-sans-serif js-context-bar-sticky-point-masthead" data-track-component="article" data-test="masthead-component">
                <div class="app-article-masthead__info">
                    
    
        <nav aria-label="breadcrumbs" data-test="breadcrumbs">
            <ol class="c-breadcrumbs c-breadcrumbs--contrast" itemscope itemtype="https://schema.org/BreadcrumbList">
                
                    <li class="c-breadcrumbs__item" id="breadcrumb0" itemprop="itemListElement" itemscope="" itemtype="https://schema.org/ListItem">
                        <a href="/" class="c-breadcrumbs__link" itemprop="item" data-track="click_breadcrumb" data-track-context="article page"  data-track-category="article" data-track-action="breadcrumbs" data-track-label="breadcrumb1"><span itemprop="name">Home</span></a><meta itemprop="position" content="1">
                            <svg class="c-breadcrumbs__chevron" role="img" aria-hidden="true" focusable="false" width="10" height="10" viewBox="0 0 10 10">
                                <path d="m5.96738168 4.70639573 2.39518594-2.41447274c.37913917-.38219212.98637524-.38972225 1.35419292-.01894278.37750606.38054586.37784436.99719163-.00013556 1.37821513l-4.03074001 4.06319683c-.37758093.38062133-.98937525.38100976-1.367372-.00003075l-4.03091981-4.06337806c-.37759778-.38063832-.38381821-.99150444-.01600053-1.3622839.37750607-.38054587.98772445-.38240057 1.37006824.00302197l2.39538588 2.4146743.96295325.98624457z" fill-rule="evenodd" transform="matrix(0 -1 1 0 0 10)"/>
                            </svg>
                    </li>
                
                    <li class="c-breadcrumbs__item" id="breadcrumb1" itemprop="itemListElement" itemscope="" itemtype="https://schema.org/ListItem">
                        <a href="/journal/521" class="c-breadcrumbs__link" itemprop="item" data-track="click_breadcrumb" data-track-context="article page"  data-track-category="article" data-track-action="breadcrumbs" data-track-label="breadcrumb2"><span itemprop="name">Neural Computing and Applications</span></a><meta itemprop="position" content="2">
                            <svg class="c-breadcrumbs__chevron" role="img" aria-hidden="true" focusable="false" width="10" height="10" viewBox="0 0 10 10">
                                <path d="m5.96738168 4.70639573 2.39518594-2.41447274c.37913917-.38219212.98637524-.38972225 1.35419292-.01894278.37750606.38054586.37784436.99719163-.00013556 1.37821513l-4.03074001 4.06319683c-.37758093.38062133-.98937525.38100976-1.367372-.00003075l-4.03091981-4.06337806c-.37759778-.38063832-.38381821-.99150444-.01600053-1.3622839.37750607-.38054587.98772445-.38240057 1.37006824.00302197l2.39538588 2.4146743.96295325.98624457z" fill-rule="evenodd" transform="matrix(0 -1 1 0 0 10)"/>
                            </svg>
                    </li>
                
                    <li class="c-breadcrumbs__item" id="breadcrumb2" itemprop="itemListElement" itemscope="" itemtype="https://schema.org/ListItem">
                        <span itemprop="name">Article</span><meta itemprop="position" content="3">
                    </li>
                
            </ol>
        </nav>
    

                    <h1 class="c-article-title" data-test="article-title" data-article-title="">Simultaneous instance pooling and bag representation selection approach for multiple-instance learning (MIL) using vision transformer</h1>

                    <ul class="c-article-identifiers">
                        
        <li class="c-article-identifiers__item" data-test="article-category">Original Article</li>
    
        <li class="c-article-identifiers__item">
            <a href="https://www.springernature.com/gp/open-science/about/the-fundamentals-of-open-access-and-open-research" data-track="click" data-track-action="open access" data-track-label="link" class="u-color-open-access" data-test="open-access">Open access</a>
        </li>
    
    

                        <li class="c-article-identifiers__item">
                            Published: <time datetime="2024-02-16">16 February 2024</time>
                        </li>
                    </ul>
                    <ul class="c-article-identifiers c-article-identifiers--cite-list">
                        <li class="c-article-identifiers__item">
                            <span data-test="journal-volume">Volume36</span>,pages 66596680, (<span data-test="article-publication-year">2024</span>)
                        </li>
                        <li class="c-article-identifiers__item c-article-identifiers__item--cite">
                            <a href="#citeas" data-track="click" data-track-action="cite this article" data-track-category="article body" data-track-label="link">Cite this article</a>
                        </li>
                    </ul>

                    <div class="app-article-masthead__buttons" data-test="download-article-link-wrapper" data-track-context="masthead">
                        <p class="app-article-masthead__access">
                            <svg width="16" height="16" focusable="false" role="img" aria-hidden="true"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#icon-eds-i-check-filled-medium"></use></svg>
                            You have full access to this <a href="https://www.springernature.com/gp/open-science/about/the-fundamentals-of-open-access-and-open-research" data-track="click" data-track-action="open access" data-track-label="link">open access</a> article</p>
                        
                        <div class="app-article-masthead__access-container">
                            
        <div class="c-pdf-container">
            <div class="c-pdf-download u-clear-both u-mb-16">
                <a href="/content/pdf/10.1007/s00521-024-09417-3.pdf" class="c-pdf-download__link" data-article-pdf="true" data-readcube-pdf-url="true" data-test="pdf-link" data-draft-ignore="true" data-track="content_download" data-track-type="article pdf download" data-track-action="download pdf" data-track-label="button" data-track-external download>
                    
                        <span class="c-pdf-download__text">Download PDF</span>
                        <svg aria-hidden="true" focusable="false" width="16" height="16" class="u-icon"><use xlink:href="#icon-eds-i-download-medium"/></svg>
                    
                </a>
            </div>
        </div>
    

                            
                            
                        </div>
                    </div>
                </div>
                <div class="app-article-masthead__brand">
                    
                        
                            <a href="/journal/521"
                        
                           class="app-article-masthead__journal-link"
                           data-track="click_journal_home"
                           data-track-action="journal homepage"
                           data-track-context="article page"
                           data-track-label="link">
                            <picture>
                                <source type="image/webp" media="(min-width: 768px)" width="120" height="159"
                                        srcset="https://media.springernature.com/w120/springer-static/cover-hires/journal/521?as=webp,
                                                    https://media.springernature.com/w316/springer-static/cover-hires/journal/521?as=webp 2x">
                                <img width="72" height="95"
                                     src="https://media.springernature.com/w72/springer-static/cover-hires/journal/521?as=webp"
                                     srcset="https://media.springernature.com/w144/springer-static/cover-hires/journal/521?as=webp 2x" alt="">
                            </picture>
                            <span class="app-article-masthead__journal-title">Neural Computing and Applications</span>
                        </a>
                        
                            <a href="/journal/521/aims-and-scope" class="app-article-masthead__submission-link"
                               data-track="click_aims_and_scope"
                               data-track-action="aims and scope"
                               data-track-context="article page"
                               data-track-label="link">
                                Aims and scope
                                <svg width="16" height="16" focusable="false" role="img" aria-hidden="true" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#icon-eds-i-arrow-right-medium"></use></svg>
                            </a>
                        
                        
                            <a href="https://www.editorialmanager.com/ncaa" class="app-article-masthead__submission-link"
                               data-track="click_submit_manuscript"
                               data-track-context="article masthead on springerlink article page"
                               data-track-action="submit manuscript"
                               data-track-label="link">
                                Submit manuscript
                                <svg width="16" height="16" focusable="false" role="img" aria-hidden="true" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#icon-eds-i-arrow-right-medium"></use></svg>
                            </a>
                        
                    
                </div>
            </div>
        
    </div>
</section>

        <div class="c-article-main u-container u-mt-24 u-mb-32 l-with-sidebar" id="main-content"
             data-component="article-container">
            <main class="u-serif js-main-column" data-track-component="article body">
                
                
                    <div class="c-context-bar u-hide"
                         data-test="context-bar"
                         data-context-bar
                         aria-hidden="true">
                        <div class="c-context-bar__container">
                            <div class="c-context-bar__title">
                                Simultaneous instance pooling and bag representation selection approach for multiple-instance learning (MIL) using vision transformer
                            </div>
                            
                                <div class="c-context-bar__cta-container" data-test="inCoD" data-track-context="sticky banner">
                                    
        <div class="c-pdf-container">
            <div class="c-pdf-download u-clear-both u-mb-16">
                <a href="/content/pdf/10.1007/s00521-024-09417-3.pdf" class="c-pdf-download__link" data-article-pdf="true" data-readcube-pdf-url="true" data-test="pdf-link" data-draft-ignore="true" data-track="content_download" data-track-type="article pdf download" data-track-action="download pdf" data-track-label="button" data-track-external download>
                    
                        <span class="c-pdf-download__text">Download PDF</span>
                        <svg aria-hidden="true" focusable="false" width="16" height="16" class="u-icon"><use xlink:href="#icon-eds-i-download-medium"/></svg>
                    
                </a>
            </div>
        </div>
    


                                    
                                </div>
                            
                        </div>
                    </div>
                

                <div class="c-article-header">
                    <header>
                        <ul class="c-article-author-list c-article-author-list--short" data-test="authors-list" data-component-authors-activator="authors-list"><li class="c-article-author-list__item"><a data-test="author-name" data-track="click" data-track-action="open author" data-track-label="link" data-track-index="1_6" data-track-context="researcher popup with no profile" href="#auth-Muhammad-Waqas-Aff1-Aff2" data-author-popup="auth-Muhammad-Waqas-Aff1-Aff2" data-author-search="Waqas, Muhammad" data-corresp-id="c1">Muhammad Waqas<svg width="16" height="16" focusable="false" role="img" aria-hidden="true" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#icon-eds-i-mail-medium"></use></svg></a><span class="u-js-hide">
            <a class="js-orcid" href="https://orcid.org/0000-0002-4659-783X"><span class="u-visually-hidden">ORCID: </span>orcid.org/0000-0002-4659-783X</a></span><sup class="u-js-hide"><a href="#Aff1">1</a>,<a href="#Aff2">2</a></sup>, </li><li class="c-article-author-list__item"><a data-test="author-name" data-track="click" data-track-action="open author" data-track-label="link" data-track-index="2_6" data-track-context="researcher popup with no profile" href="#auth-Muhammad_Atif-Tahir-Aff1" data-author-popup="auth-Muhammad_Atif-Tahir-Aff1" data-author-search="Tahir, Muhammad Atif">Muhammad Atif Tahir</a><sup class="u-js-hide"><a href="#Aff1">1</a></sup>, </li><li class="c-article-author-list__item c-article-author-list__item--hide-small-screen"><a data-test="author-name" data-track="click" data-track-action="open author" data-track-label="link" data-track-index="3_6" data-track-context="researcher popup with no profile" href="#auth-Muhammad_Danish-Author-Aff3" data-author-popup="auth-Muhammad_Danish-Author-Aff3" data-author-search="Author, Muhammad Danish">Muhammad Danish Author</a><sup class="u-js-hide"><a href="#Aff3">3</a></sup>, </li><li class="c-article-author-list__item c-article-author-list__item--hide-small-screen"><a data-test="author-name" data-track="click" data-track-action="open author" data-track-label="link" data-track-index="4_6" data-track-context="researcher popup with no profile" href="#auth-Sumaya-Al_Maadeed-Aff4" data-author-popup="auth-Sumaya-Al_Maadeed-Aff4" data-author-search="Al-Maadeed, Sumaya">Sumaya Al-Maadeed</a><sup class="u-js-hide"><a href="#Aff4">4</a></sup>, </li><li class="c-article-author-list__item c-article-author-list__item--hide-small-screen"><a data-test="author-name" data-track="click" data-track-action="open author" data-track-label="link" data-track-index="5_6" data-track-context="researcher popup with no profile" href="#auth-Ahmed-Bouridane-Aff5" data-author-popup="auth-Ahmed-Bouridane-Aff5" data-author-search="Bouridane, Ahmed">Ahmed Bouridane</a><sup class="u-js-hide"><a href="#Aff5">5</a></sup> &amp; </li><li class="c-article-author-list__show-more" aria-label="Show all 6 authors for this article" title="Show all 6 authors for this article"></li><li class="c-article-author-list__item"><a data-test="author-name" data-track="click" data-track-action="open author" data-track-label="link" data-track-index="6_6" data-track-context="researcher popup with no profile" href="#auth-Jia-Wu-Aff2" data-author-popup="auth-Jia-Wu-Aff2" data-author-search="Wu, Jia">Jia Wu</a><sup class="u-js-hide"><a href="#Aff2">2</a></sup></li></ul><button aria-expanded="false" class="c-article-author-list__button"><svg width="16" height="16" focusable="false" role="img" aria-hidden="true" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#icon-eds-i-chevron-down-medium"></use></svg><span>Show authors</span></button>
                        
    


                        <div data-test="article-metrics">
                            
        <ul class="app-article-metrics-bar u-list-reset">
            
                <li class="app-article-metrics-bar__item" data-test="access-count">
                    <p class="app-article-metrics-bar__count"><svg class="u-icon app-article-metrics-bar__icon" width="24" height="24" aria-hidden="true" focusable="false">
                        <use xlink:href="#icon-eds-i-accesses-medium"></use>
                    </svg>3452 <span class="app-article-metrics-bar__label">Accesses</span></p>
                </li>
            
            
                <li class="app-article-metrics-bar__item" data-test=citation-count>
                    <p class="app-article-metrics-bar__count"><svg class="u-icon app-article-metrics-bar__icon" width="24" height="24" aria-hidden="true" focusable="false">
                        <use xlink:href="#icon-eds-i-citations-medium"></use>
                    </svg>15 <span class="app-article-metrics-bar__label">Citations</span></p>
                </li>
            
            
                
            
            
            
                
                    <li class="app-article-metrics-bar__item app-article-metrics-bar__item--metrics">
                        <p class="app-article-metrics-bar__details"><a href="/article/10.1007/s00521-024-09417-3/metrics" data-track="click" data-track-action="view metrics" data-track-label="link" rel="nofollow">Explore all metrics <svg class="u-icon app-article-metrics-bar__arrow-icon" width="24" height="24" aria-hidden="true" focusable="false">
                            <use xlink:href="#icon-eds-i-arrow-right-medium"></use>
                        </svg></a></p>
                    </li>
                
            
        </ul>
    
                        </div>
                        
                        
    <div class="u-mt-32">
    

    
    </div>

                        
                    </header>
                </div>

                <div data-article-body="true" data-track-component="article body" class="c-article-body">

                    
                    <section aria-labelledby="Abs1" data-title="Abstract" lang="en"><div class="c-article-section" id="Abs1-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Abs1">Abstract</h2><div class="c-article-section__content" id="Abs1-content"><p>In multiple-instance learning (MIL), the existing bag encoding and attention-based pooling approaches assume that the instances in the bag have no relationship among them. This assumption is unsuited, as the instances in the bags are rarely independent in diverse MIL applications. In contrast, the instance relationship assumption-based techniques incorporate the instance relationship information in the classification process. However, in MIL, the bag composition process is complicated, and it may be possible that instances in one bag are related and instances in another bag are not. In present MIL algorithms, this relationship assumption is not explicitly modeled. The learning algorithm is trained based on one of two relationship assumptions (whether instances in all bags have a relationship or not). Hence, it is essential to model the assumption of instance relationships in the bag classification process. This paper proposes a robust approach that generates vector representation for the bag for both assumptions and the representation selection process to determine whether to consider the instances related or unrelated in the bag classification process. This process helps to determine the essential bag representation vector for every individual bag. The proposed method utilizes attention pooling and vision transformer approaches to generate bag representation vectors. Later, the representation selection subnetwork determines the vector representation essential for bag classification in an end-to-end trainable manner. The generalization abilities of the proposed framework are demonstrated through extensive experiments on several benchmark datasets. The experiments demonstrate that the proposed approach outperforms other state-of-the-art MIL approaches in bag classification.</p></div></div></section>

                    

                    
    


                    

                    <div data-test="cobranding-download">
                        
                    </div>

                    
                        
        
            <section aria-labelledby="inline-recommendations" data-title="Inline Recommendations" class="c-article-recommendations" data-track-component="inline-recommendations">
                <h3 class="c-article-recommendations-title" id="inline-recommendations">Similar content being viewed by others</h3>
                <div class="c-article-recommendations-list">
                    
                        <div class="c-article-recommendations-list__item">
                            <article class="c-article-recommendations-card" itemscope itemtype="http://schema.org/ScholarlyArticle">
                                
                                    <div class="c-article-recommendations-card__img"><img src="https://media.springernature.com/w92h120/springer-static/cover-hires/book/978-3-030-86520-7?as&#x3D;webp" loading="lazy" alt=""></div>
                                
                                <div class="c-article-recommendations-card__main">
                                    <h3 class="c-article-recommendations-card__heading" itemprop="name headline">
                                        <a class="c-article-recommendations-card__link"
                                           itemprop="url"
                                           href="https://link.springer.com/10.1007/978-3-030-86520-7_44?fromPaywallRec=false"
                                           data-track="select_recommendations_1"
                                           data-track-context="inline recommendations"
                                           data-track-action="click recommendations inline - 1"
                                           data-track-label="10.1007/978-3-030-86520-7_44">Explainable Multiple Instance Learning with Instance Selection Randomized Trees
                                        </a>
                                    </h3>
                                    <div class="c-article-meta-recommendations" data-test="recommendation-info">
                                        <span class="c-article-meta-recommendations__item-type">Chapter</span>
                                        
                                         <span class="c-article-meta-recommendations__date"> 2021</span>
                                    </div>
                                </div>
                            </article>
                        </div>
                    
                        <div class="c-article-recommendations-list__item">
                            <article class="c-article-recommendations-card" itemscope itemtype="http://schema.org/ScholarlyArticle">
                                
                                    <div class="c-article-recommendations-card__img"><img src="https://media.springernature.com/w215h120/springer-static/image/art%3A10.1007%2Fs10489-022-04045-7/MediaObjects/10489_2022_4045_Fig1_HTML.png" loading="lazy" alt=""></div>
                                
                                <div class="c-article-recommendations-card__main">
                                    <h3 class="c-article-recommendations-card__heading" itemprop="name headline">
                                        <a class="c-article-recommendations-card__link"
                                           itemprop="url"
                                           href="https://link.springer.com/10.1007/s10489-022-04045-7?fromPaywallRec=false"
                                           data-track="select_recommendations_2"
                                           data-track-context="inline recommendations"
                                           data-track-action="click recommendations inline - 2"
                                           data-track-label="10.1007/s10489-022-04045-7">Deep Gaussian mixture model based instance relevance estimation for multiple instance learning applications
                                        </a>
                                    </h3>
                                    <div class="c-article-meta-recommendations" data-test="recommendation-info">
                                        <span class="c-article-meta-recommendations__item-type">Article</span>
                                        
                                         <span class="c-article-meta-recommendations__date">17 August 2022</span>
                                    </div>
                                </div>
                            </article>
                        </div>
                    
                        <div class="c-article-recommendations-list__item">
                            <article class="c-article-recommendations-card" itemscope itemtype="http://schema.org/ScholarlyArticle">
                                
                                    <div class="c-article-recommendations-card__img"><img src="https://media.springernature.com/w92h120/springer-static/cover-hires/book/978-3-319-47759-6?as&#x3D;webp" loading="lazy" alt=""></div>
                                
                                <div class="c-article-recommendations-card__main">
                                    <h3 class="c-article-recommendations-card__heading" itemprop="name headline">
                                        <a class="c-article-recommendations-card__link"
                                           itemprop="url"
                                           href="https://link.springer.com/10.1007/978-3-319-47759-6_5?fromPaywallRec=false"
                                           data-track="select_recommendations_3"
                                           data-track-context="inline recommendations"
                                           data-track-action="click recommendations inline - 3"
                                           data-track-label="10.1007/978-3-319-47759-6_5">Bag-Based Classification Methods
                                        </a>
                                    </h3>
                                    <div class="c-article-meta-recommendations" data-test="recommendation-info">
                                        <span class="c-article-meta-recommendations__item-type">Chapter</span>
                                        
                                         <span class="c-article-meta-recommendations__date"> 2016</span>
                                    </div>
                                </div>
                            </article>
                        </div>
                    
                </div>
            </section>
        
            <script>
                window.dataLayer = window.dataLayer || [];
                window.dataLayer.push({
                    recommendations: {
                        recommender: 'semantic',
                        model: 'specter',
                        policy_id: 'NA',
                        timestamp: 1766874504,
                        embedded_user: 'null'
                    }
                });
            </script>
        
    
                    

                    
                        
    <section class="app-explore-related-subjects" aria-labelledby="content-related-subjects" data-test="subject-content">
        <h3 id="content-related-subjects" class="app-explore-related-subjects__title">Explore related subjects</h3>
        <span class="u-sans-serif u-text-xs u-display-block u-mb-16">Discover the latest articles, books and news in related subjects, suggested using machine learning.</span>
        <ul class="app-explore-related-subjects__list app-explore-related-subjects__list--no-mb" role="list">
        
            <li class="app-explore-related-subjects__items" data-test="related-subject-item">
                <a href="/subjects/categorization"  data-track="select_related_subject_1" data-track-context="related subjects from content page" data-track-label="Categorization">Categorization</a>
            </li>
        
            <li class="app-explore-related-subjects__items" data-test="related-subject-item">
                <a href="/subjects/computer-vision"  data-track="select_related_subject_2" data-track-context="related subjects from content page" data-track-label="Computer Vision">Computer Vision</a>
            </li>
        
            <li class="app-explore-related-subjects__items" data-test="related-subject-item">
                <a href="/subjects/machine-learning"  data-track="select_related_subject_3" data-track-context="related subjects from content page" data-track-label="Machine Learning">Machine Learning</a>
            </li>
        
            <li class="app-explore-related-subjects__items" data-test="related-subject-item">
                <a href="/subjects/object-recognition"  data-track="select_related_subject_4" data-track-context="related subjects from content page" data-track-label="Object Recognition">Object Recognition</a>
            </li>
        
            <li class="app-explore-related-subjects__items" data-test="related-subject-item">
                <a href="/subjects/object-vision"  data-track="select_related_subject_5" data-track-context="related subjects from content page" data-track-label="Object vision">Object vision</a>
            </li>
        
            <li class="app-explore-related-subjects__items" data-test="related-subject-item">
                <a href="/subjects/pattern-vision"  data-track="select_related_subject_6" data-track-context="related subjects from content page" data-track-label="Pattern vision">Pattern vision</a>
            </li>
        
        </ul>
    </section>

                    

                    
                        
    <div class="app-card-service" data-test="article-checklist-banner">
        <div>
            <a class="app-card-service__link" data-track="click_presubmission_checklist" data-track-context="article page top of reading companion" data-track-category="pre-submission-checklist" data-track-action="clicked article page checklist banner test 2 old version" data-track-label="link" href="https://beta.springernature.com/pre-submission?journalId=521"
            data-test="article-checklist-banner-link">
            <span class="app-card-service__link-text">Use our pre-submission checklist</span>
            <svg class="app-card-service__link-icon" aria-hidden="true" focusable="false"><use xlink:href="#icon-eds-i-arrow-right-small"></use></svg>
            </a>
            <p class="app-card-service__description">Avoid common mistakes on your manuscript.</p>
        </div>
        <div class="app-card-service__icon-container">
            <svg class="app-card-service__icon" aria-hidden="true" focusable="false">
                <use xlink:href="#icon-eds-i-clipboard-check-medium"></use>
            </svg>
        </div>
    </div>

                    

                    
                        
                                <div class="main-content">
                                    <section data-title="Introduction"><div class="c-article-section" id="Sec1-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec1"><span class="c-article-section__title-number">1 </span>Introduction</h2><div class="c-article-section__content" id="Sec1-content"><p>The multiple-instance learning (MIL) approach is a case of weakly supervised learning [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1" title="Zhou Z-H (2018) A brief introduction to weakly supervised learning. Natl Sci Rev 5(1):4453" href="/article/10.1007/s00521-024-09417-3#ref-CR1" id="ref-link-section-d590711142e505">1</a>]. This learning approach is used where labeling cost is a major restriction for annotating every data instance [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2" title="Li M, Li X, Jiang Y, Zhang J, Luo H, Yin S (2022) Explainable multi-instance and multi-task learning for COVID-19 diagnosis and lesion segmentation in CT images. Knowl-Based Syst 252:109278" href="/article/10.1007/s00521-024-09417-3#ref-CR2" id="ref-link-section-d590711142e508">2</a>]. In MIL, the data are represented as bags with multiple instances, with only one label for each bag. Unlike supervised learning, the labels of the instances are not available in the training process. The model in MIL is trained using weak bag-wise labels rather than instance-wise labels. The case of supervised learning and MIL is shown in Fig.<a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s00521-024-09417-3#Fig1">1</a>a and b, respectively. In MIL, the primary objective is to develop a model that predicts the label of the test bag using training bags and corresponding labels. The application of MIL is common in image segmentation [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 3" title="Liu Y, Wu YH, Wen P, Shi Y, Qiu Y, Cheng MM (2020) Leveraging instance-, image-and dataset-level information for weakly supervised instance segmentation. IEEE Trans. Pattern Anal. Mach. Intell. 44(3):14151428" href="/article/10.1007/s00521-024-09417-3#ref-CR3" id="ref-link-section-d590711142e514">3</a>], medical image classification [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 4" title="Zhang Y, Liu S, Qu X, Shang X (2022) Multi-instance discriminative contrastive learning for brain image representation. Neural Comput Appl. &#xA;                  https://doi.org/10.1007/s00521-022-07524-7&#xA;                  &#xA;                " href="/article/10.1007/s00521-024-09417-3#ref-CR4" id="ref-link-section-d590711142e517">4</a>], and others [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" title="Antwi-Bekoe E, Liu G, Ainam J-P, Sun G, Xie X (2022) A deep learning approach for insulator instance segmentation and defect detection. Neural Comput Appl 34(9):72537269" href="#ref-CR5" id="ref-link-section-d590711142e521">5</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" title="Wang K, Liu J, Gonzlez D (2017) Domain transfer multi-instance dictionary learning. Neural Comput Appl 28:983992" href="#ref-CR6" id="ref-link-section-d590711142e521_1">6</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 7" title="Carbonneau M-A, Cheplygina V, Granger E, Gagnon G (2018) Multiple instance learning: a survey of problem characteristics and applications. Pattern Recogn 77:329353" href="/article/10.1007/s00521-024-09417-3#ref-CR7" id="ref-link-section-d590711142e524">7</a>].</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-1" data-title="Fig. 1"><figure><figcaption><b id="Fig1" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 1</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s00521-024-09417-3/figures/1" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs00521-024-09417-3/MediaObjects/521_2024_9417_Fig1_HTML.png?as=webp"><img aria-describedby="Fig1" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs00521-024-09417-3/MediaObjects/521_2024_9417_Fig1_HTML.png" alt="figure 1" loading="lazy" width="685" height="352"></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-1-desc"><p>Supervised learning (SL) vs Multiple-Instance learning (MIL), <b>a</b> shows the example of instance classification setup followed in SL, where every data instance is labeled. The MIL bag classification approach is shown in <b>b</b> where the instances are grouped in bags, and the labels are provided at bag level</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s00521-024-09417-3/figures/1" data-track-dest="link:Figure1 Full size image" aria-label="Full size image figure 1" rel="nofollow"><span>Full size image</span><svg width="16" height="16" focusable="false" role="img" aria-hidden="true" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#icon-eds-i-chevron-right-small"></use></svg></a></div></figure></div><p>The MIL approaches can be categorized based on the classification granularity: the bag-space level classification approaches [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 8" title="Cheplygina V, Tax DM, Loog M (2015) Dissimilarity-based ensembles for multiple instance learning. IEEE Trans Neural Netw Learn Syst 27(6):13791391" href="/article/10.1007/s00521-024-09417-3#ref-CR8" id="ref-link-section-d590711142e554">8</a>], which compute the distance between the bags or apply maximum margin approach to train the classifiers; embedding-space classification [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 9" title="Wei X-S, Wu J, Zhou Z-H (2016) Scalable algorithms for multi-instance learning. IEEE Trans Neural Netw Learn Syst 28(4):975987" href="/article/10.1007/s00521-024-09417-3#ref-CR9" id="ref-link-section-d590711142e557">9</a>, <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 10" title="Perronnin F, Snchez J, Mensink T (2010) Improving the fisher kernel for large-scale image classification. In: European Conference on Computer Vision, pp. 143156. Springer" href="/article/10.1007/s00521-024-09417-3#ref-CR10" id="ref-link-section-d590711142e560">10</a>], where an entire bag is transformed into a fixed-size vector representation and applies a simple single instance classification algorithms; instance-space classification [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 11" title="Ramon J, DeRaedt L (2000) Multi instance neural networks. In: Proceedings of the ICML-2000 Workshop on Attribute-value and Relational Learning, pp. 5360" href="/article/10.1007/s00521-024-09417-3#ref-CR11" id="ref-link-section-d590711142e563">11</a>], where the score for each instance is computed, and the bag label is obtained based on the instance scores. The studies in [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 12" title="Kandemir M, Hamprecht FA (2015) Computer-aided diagnosis from weak supervision: a benchmarking study. Comput Med Imaging Graph 42:4450" href="/article/10.1007/s00521-024-09417-3#ref-CR12" id="ref-link-section-d590711142e566">12</a>, <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 13" title="Ilse M, Tomczak J, Welling M (2018) Attention-based deep multiple instance learning. In: International conference on machine learning, pp. 21272136. PMLR" href="/article/10.1007/s00521-024-09417-3#ref-CR13" id="ref-link-section-d590711142e570">13</a>] show that the first two categories are robust in bag classification compared to the last category. However, the bag-space and embedding-space classification approaches cannot identify the key instances (the instances that trigger the bag label) [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 13" title="Ilse M, Tomczak J, Welling M (2018) Attention-based deep multiple instance learning. In: International conference on machine learning, pp. 21272136. PMLR" href="/article/10.1007/s00521-024-09417-3#ref-CR13" id="ref-link-section-d590711142e573">13</a>]. Identifying key instances in the bag is essential as these instances play a vital role in the bag classification process and model interpretability.</p><p>Furthermore, in the context of MIL, the bags consist of multiple instances, and the goal is to classify the bags based on their contents. However, the difficulty arises when the bags in the training set and testing come from a different distribution [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 14" title="Zhang W-J, Zhou Z-H (2014) Multi-instance learning with distribution change. In: Proceedings of the AAAI conference on artificial intelligence, vol. 28" href="/article/10.1007/s00521-024-09417-3#ref-CR14" id="ref-link-section-d590711142e579">14</a>]. Previous MIL studies assume that the instances of the bag in the training and testing data are sampled from the same distribution (either related or independent). However, this assumption is often violated in real-world tasks [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 9" title="Wei X-S, Wu J, Zhou Z-H (2016) Scalable algorithms for multi-instance learning. IEEE Trans Neural Netw Learn Syst 28(4):975987" href="/article/10.1007/s00521-024-09417-3#ref-CR9" id="ref-link-section-d590711142e582">9</a>, <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 13" title="Ilse M, Tomczak J, Welling M (2018) Attention-based deep multiple instance learning. In: International conference on machine learning, pp. 21272136. PMLR" href="/article/10.1007/s00521-024-09417-3#ref-CR13" id="ref-link-section-d590711142e585">13</a>, <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" title="Shi X, Xing F, Xie Y, Zhang Z, Cui L, Yang L (2020) Loss-based attention for deep multiple instance learning. In: Proceedings of the AAAI conference on artificial intelligence, vol. 34, pp. 57425749" href="#ref-CR15" id="ref-link-section-d590711142e588">15</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" title="Zhou Z-H, Sun Y-Y, Li Y-F (2009) Multi-instance learning by treating instances as non-IID samples. In: Proceedings of the 26th annual international conference on machine learning, pp. 12491256" href="#ref-CR16" id="ref-link-section-d590711142e588_1">16</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" title="Waqas M, Tahir MA, Qureshi R (2021) Ensemble-based instance relevance estimation in multiple-instance learning. In: 2021 9th European workshop on visual information processing (EUVIP), pp. 16. IEEE" href="#ref-CR17" id="ref-link-section-d590711142e588_2">17</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" title="Waqas M, Tahir MA, Qureshi R (2023) Deep Gaussian mixture model based instance relevance estimation for multiple instance learning applications. Appl Intell 53(9):1031010325" href="#ref-CR18" id="ref-link-section-d590711142e588_3">18</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 19" title="Waqas M, Tahir MA, Khan SA (2023) Robust bag classification approach for multi-instance learning via subspace fuzzy clustering. Expert Syst Appl 214:119113" href="/article/10.1007/s00521-024-09417-3#ref-CR19" id="ref-link-section-d590711142e591">19</a>].</p><p>For example, the case of MIL image classification is illustrated in Fig.<a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s00521-024-09417-3#Fig2">2</a>, where the image is considered a bag, and the extracted patches are considered instances. The instances related to the Fox concept are positive instances; instances related to other objects like cars and buildings are negative instances. Figure<a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s00521-024-09417-3#Fig2">2</a> illustrates the dissimilarity between different training bag distributions, where the training set contains images of the animal of interest in natural settings. However, some images in the training and testing set may be captured in a diverse environment or contain other similar animals.</p><p>In such cases, the instances in the bag may or may not have a relationship, and it can be challenging to ascertain the presence or absence of any underlying instance relationships. Therefore, determining the relationship between the instances in the bag becomes important to model performance, and the presumption of a specific instance relationship could potentially hinder the performance of the classification algorithm. In order to obtain better generalization, the classifier must distinguish between instances related to the fox concept, different animal species, other objects inside the bag, and their relationship. Thus, determining the relationship or independence of instances in the bag may enhance the classification process.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-2" data-title="Fig. 2"><figure><figcaption><b id="Fig2" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 2</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s00521-024-09417-3/figures/2" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs00521-024-09417-3/MediaObjects/521_2024_9417_Fig2_HTML.png?as=webp"><img aria-describedby="Fig2" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs00521-024-09417-3/MediaObjects/521_2024_9417_Fig2_HTML.png" alt="figure 2" loading="lazy" width="685" height="252"></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-2-desc"><p>The example of distribution change where the training examples are from distribution. <b>a</b> Shows the positive training example captured in different settings, and <b>b</b> presents the negative testing example. Green boxes mark the positive instances, while the negative instances are shown in red boxes</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s00521-024-09417-3/figures/2" data-track-dest="link:Figure2 Full size image" aria-label="Full size image figure 2" rel="nofollow"><span>Full size image</span><svg width="16" height="16" focusable="false" role="img" aria-hidden="true" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#icon-eds-i-chevron-right-small"></use></svg></a></div></figure></div><p>MIL algorithms [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 9" title="Wei X-S, Wu J, Zhou Z-H (2016) Scalable algorithms for multi-instance learning. IEEE Trans Neural Netw Learn Syst 28(4):975987" href="/article/10.1007/s00521-024-09417-3#ref-CR9" id="ref-link-section-d590711142e634">9</a>, <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 13" title="Ilse M, Tomczak J, Welling M (2018) Attention-based deep multiple instance learning. In: International conference on machine learning, pp. 21272136. PMLR" href="/article/10.1007/s00521-024-09417-3#ref-CR13" id="ref-link-section-d590711142e637">13</a>, <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" title="Shi X, Xing F, Xie Y, Zhang Z, Cui L, Yang L (2020) Loss-based attention for deep multiple instance learning. In: Proceedings of the AAAI conference on artificial intelligence, vol. 34, pp. 57425749" href="#ref-CR15" id="ref-link-section-d590711142e640">15</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" title="Zhou Z-H, Sun Y-Y, Li Y-F (2009) Multi-instance learning by treating instances as non-IID samples. In: Proceedings of the 26th annual international conference on machine learning, pp. 12491256" href="#ref-CR16" id="ref-link-section-d590711142e640_1">16</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" title="Waqas M, Tahir MA, Qureshi R (2021) Ensemble-based instance relevance estimation in multiple-instance learning. In: 2021 9th European workshop on visual information processing (EUVIP), pp. 16. IEEE" href="#ref-CR17" id="ref-link-section-d590711142e640_2">17</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" title="Waqas M, Tahir MA, Qureshi R (2023) Deep Gaussian mixture model based instance relevance estimation for multiple instance learning applications. Appl Intell 53(9):1031010325" href="#ref-CR18" id="ref-link-section-d590711142e640_3">18</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 19" title="Waqas M, Tahir MA, Khan SA (2023) Robust bag classification approach for multi-instance learning via subspace fuzzy clustering. Expert Syst Appl 214:119113" href="/article/10.1007/s00521-024-09417-3#ref-CR19" id="ref-link-section-d590711142e643">19</a>] are developed based on one of two assumptions: whether instances have a relationship or not. However, it is not theoretically guaranteed that instances in all bags follow the same assumption. Additionally, existing MIL algorithms do not explicitly account for the bag-wise relationship assumption. As a result, their performance could be improved since weak bag-level labels provide only limited supervision.</p><p>For example, to identify the essential instances in the bags, a weighted average bag pooling operation is proposed using attention-based deep neural networks (AbDMIL) [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 13" title="Ilse M, Tomczak J, Welling M (2018) Attention-based deep multiple instance learning. In: International conference on machine learning, pp. 21272136. PMLR" href="/article/10.1007/s00521-024-09417-3#ref-CR13" id="ref-link-section-d590711142e649">13</a>], where end-to-end trainable architectures are used to generate attention-based weights for each instance. The concept of attention pooling is further investigated in Shi et al.[<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 15" title="Shi X, Xing F, Xie Y, Zhang Z, Cui L, Yang L (2020) Loss-based attention for deep multiple instance learning. In: Proceedings of the AAAI conference on artificial intelligence, vol. 34, pp. 57425749" href="/article/10.1007/s00521-024-09417-3#ref-CR15" id="ref-link-section-d590711142e652">15</a>] by incorporating the attention loss mechanism. However, the existing attention-based pooling approaches [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 13" title="Ilse M, Tomczak J, Welling M (2018) Attention-based deep multiple instance learning. In: International conference on machine learning, pp. 21272136. PMLR" href="/article/10.1007/s00521-024-09417-3#ref-CR13" id="ref-link-section-d590711142e655">13</a>, <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 15" title="Shi X, Xing F, Xie Y, Zhang Z, Cui L, Yang L (2020) Loss-based attention for deep multiple instance learning. In: Proceedings of the AAAI conference on artificial intelligence, vol. 34, pp. 57425749" href="/article/10.1007/s00521-024-09417-3#ref-CR15" id="ref-link-section-d590711142e658">15</a>] and bag encoding strategies [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 9" title="Wei X-S, Wu J, Zhou Z-H (2016) Scalable algorithms for multi-instance learning. IEEE Trans Neural Netw Learn Syst 28(4):975987" href="/article/10.1007/s00521-024-09417-3#ref-CR9" id="ref-link-section-d590711142e661">9</a>, <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 17" title="Waqas M, Tahir MA, Qureshi R (2021) Ensemble-based instance relevance estimation in multiple-instance learning. In: 2021 9th European workshop on visual information processing (EUVIP), pp. 16. IEEE" href="/article/10.1007/s00521-024-09417-3#ref-CR17" id="ref-link-section-d590711142e665">17</a>] are based on the assumption that instances in the bag are independent and that no relationship exists between the instances of the bag. In this assumption, the relationship between the instances of the bags is ignored, which may result in neglecting the information in the bag [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 20" title="Shao Z, Bian H, Chen Y, Wang Y, Zhang J, Ji X et al (2021) Transmil: transformer based correlated multiple instance learning for whole slide image classification. Adv Neural Inf Process Syst 34:2136" href="/article/10.1007/s00521-024-09417-3#ref-CR20" id="ref-link-section-d590711142e668">20</a>, <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 21" title="Wei X-S, Zhou Z-H (2016) An empirical study on image bag generators for multi-instance learning. Mach Learn 105(2):155198" href="/article/10.1007/s00521-024-09417-3#ref-CR21" id="ref-link-section-d590711142e671">21</a>].</p><p>On the other hand, the assumption of relationship between instances is natural and may present a superior description of the data [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 22" title="Dietterich TG, Lathrop RH, Lozano-Prez T (1997) Solving the multiple instance problem with axis-parallel rectangles. Artif Intell 89(12):3171" href="/article/10.1007/s00521-024-09417-3#ref-CR22" id="ref-link-section-d590711142e677">22</a>]. Considering the different image patches as interrelated is more meaningful than assuming the opposite, specifically in multiple-instance image classification scenarios. The assumption of instance relation is also considered for MIL problems byZhou et al.[<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 16" title="Zhou Z-H, Sun Y-Y, Li Y-F (2009) Multi-instance learning by treating instances as non-IID samples. In: Proceedings of the 26th annual international conference on machine learning, pp. 12491256" href="/article/10.1007/s00521-024-09417-3#ref-CR16" id="ref-link-section-d590711142e680">16</a>]. However, these techniques mainly focus on the structural properties of the bag, and the instance relationships are modeled in terms of graph kernel learning. Additionally, this process is not end-to-end trainable.</p><p>In this paper, we propose the idea of generating bag representation vectors based on both assumptions and introduce the bag representation selection process to select a suitable representation for each bag, which addresses the limitation of the instance relationship assumption in existing MIL algorithms.</p><p>In the proposed algorithm, we incorporate bag-wise instance relationship assumption in the classification process by considering bags with varying instances as a batch, and bag representation vectors are generated for each bag based on the assumption of interaction and independence. We obtain information about the relationship between instances in a bag by using a vision transformer architecture to model the dependencies among them. Furthermore, the representation vectors for independent assumptions are derived from the mean, max, average, and attention pooling operations [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 13" title="Ilse M, Tomczak J, Welling M (2018) Attention-based deep multiple instance learning. In: International conference on machine learning, pp. 21272136. PMLR" href="/article/10.1007/s00521-024-09417-3#ref-CR13" id="ref-link-section-d590711142e689">13</a>], which do not consider the relationship of instances.</p><p>In addition, we propose a differentiable representation selection network to decide whether to consider instance relationships in the classification process for each bag. We refer to the proposed approach as a vision transformer-based instance weighting and representation selection subnetwork (ViT-IWRS).</p><p>The major contributions of the paper are:</p><ul class="u-list-style-bullet">
                <li>
                  <p>The vision transformer (ViT)-based approach is proposed to model the relationship between the instances of the bag. This process helps to generate a bag representation vector by considering the instance relationship.</p>
                </li>
                <li>
                  <p>To select informative bag representation from sets of generated bag representation vectors, a differentiable representation selection subnetwork (RSN) is proposed.</p>
                </li>
                <li>
                  <p>The weight-sharing approach is presented for simultaneous instance weight learning and bag classification for ViT. This method helps to strengthen the relationship between the loss and instance weighting processes.</p>
                </li>
              </ul><p>To demonstrate the generalization ability of the proposed approach, the experiments are performed on multiple types of data from different MIL application domains. For binary classification, five benchmark datasets are used: Musk1 and Musk2 [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 23" title="Sirinukunwattana K, Raza SEA, Tsang Y-W, Snead DR, Cree IA, Rajpoot NM (2016) Locality sensitive deep learning for detection and classification of nuclei in routine colon cancer histology images. IEEE Trans Med Imaging 35(5):11961206" href="/article/10.1007/s00521-024-09417-3#ref-CR23" id="ref-link-section-d590711142e720">23</a>] datasets for molecular activity predictions; Fox, Elephant, and Tiger datasets for image classification. For multi-class classification two datasets are used: multiple-instance MNIST (MIL-MNIST) [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 13" title="Ilse M, Tomczak J, Welling M (2018) Attention-based deep multiple instance learning. In: International conference on machine learning, pp. 21272136. PMLR" href="/article/10.1007/s00521-024-09417-3#ref-CR13" id="ref-link-section-d590711142e723">13</a>] dataset for handwritten digit classification; MIL-based CIFAR-10 datasets [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 15" title="Shi X, Xing F, Xie Y, Zhang Z, Cui L, Yang L (2020) Loss-based attention for deep multiple instance learning. In: Proceedings of the AAAI conference on artificial intelligence, vol. 34, pp. 57425749" href="/article/10.1007/s00521-024-09417-3#ref-CR15" id="ref-link-section-d590711142e726">15</a>] for object recognition. Additionally, the experiments are also conducted for real-world Colon Cancer detection histopathology dataset [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 24" title="Raykar VC, Krishnapuram B, Bi J, Dundar M, Rao RB (2008) Bayesian multiple instance learning: automatic feature selection and inductive transfer. In: Proceedings of the 25th international conference on machine learning, pp. 808815" href="/article/10.1007/s00521-024-09417-3#ref-CR24" id="ref-link-section-d590711142e729">24</a>].</p><p>The remainder of the paper is organized into the following sections: Sect.<a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s00521-024-09417-3#Sec2">2</a> presents the literature review. Section<a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s00521-024-09417-3#Sec5">3</a> explains the proposed methodology for (ViT-IWRS). The experimental setup is given in Sect.<a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s00521-024-09417-3#Sec13">4</a>. The obtained results are discussed in Sect.<a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s00521-024-09417-3#Sec21">5</a>, which follows the conclusive Sect.<a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s00521-024-09417-3#Sec38">6</a>.</p></div></div></section><section data-title="Literature review"><div class="c-article-section" id="Sec2-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec2"><span class="c-article-section__title-number">2 </span>Literature review</h2><div class="c-article-section__content" id="Sec2-content"><p>This section presents a summary of MIL algorithms in the literature. The MIL algorithms are divided into two categories: Classical MIL techniques and Neural network-based techniques. These categories are discussed in detail in the following subsections.</p><h3 class="c-article__sub-heading" id="Sec3"><span class="c-article-section__title-number">2.1 </span>Classical MIL techniques</h3><p>Classical MIL techniques can also be categorized into bag-space and instance-space algorithms. The instance-space algorithms classify each instance in the bag individually and aggregate the instance labels to determine the bag label [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 11" title="Ramon J, DeRaedt L (2000) Multi instance neural networks. In: Proceedings of the ICML-2000 Workshop on Attribute-value and Relational Learning, pp. 5360" href="/article/10.1007/s00521-024-09417-3#ref-CR11" id="ref-link-section-d590711142e765">11</a>, <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 25" title="Andrews S, Tsochantaridis I, Hofmann T (2002) Support vector machines for multiple-instance learning. In: NIPS, vol. 2, p. 7" href="/article/10.1007/s00521-024-09417-3#ref-CR25" id="ref-link-section-d590711142e768">25</a>]. Thus, these algorithms identify the key instances in the bag (instances that triggered the bag label). However, the unavailability of instance-level labels complicates the learning problem.</p><p>To tackle the complexity of the learning process, Andrews et al. [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 26" title="Amar RA, Dooly DR, Goldman SA, Zhang Q (2001) Multiple-instance learning of real-valued data. In: ICML, pp. 310. Citeseer" href="/article/10.1007/s00521-024-09417-3#ref-CR26" id="ref-link-section-d590711142e774">26</a>] proposed two support vector machine (SVM)-based solutions to solve MIL problems: Mi-SVM for instance-space classification and MI-SVM for bag-space classification. Diversity Density (DD) and nearest neighbor approach for real-valued target in MIL are proposed in Amar et al.[<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 27" title="Zhang Q, Goldman S (2001) EM-DD: An improved multiple-instance learning technique. In: Dietterich T, Becker S, Ghahramani Z(ed) Advances in neural information processing systems. MIT Press, 14. &#xA;                  https://proceedings.neurips.cc/paper_files/paper/2001/file/e4dd5528f7596dcdf871aa55cfccc53c-Paper.pdf&#xA;                  &#xA;                " href="/article/10.1007/s00521-024-09417-3#ref-CR27" id="ref-link-section-d590711142e777">27</a>], and a similar approach combining diversity density and expectation-maximization (EM) is proposed in Zhang and Goldman [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 28" title="Carbonneau M-A, Granger E, Raymond AJ, Gagnon G (2016) Robust multiple-instance learning ensembles using random subspace instance selection. Pattern Recogn 58:8399" href="/article/10.1007/s00521-024-09417-3#ref-CR28" id="ref-link-section-d590711142e780">28</a>]. These algorithms address MIL problems by assigning bag labels to the instances and training an instance-space model. However, these methods often fail when a complicated relationship between instances determines the bag label.</p><p>Random subspace clustering and instance selection approach (RSIS) is proposed in Carbonneau et al.[<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 29" title="Zhou Z-H, Zhang M-L (2007) Solving multi-instance problems with classifier ensemble based on constructive clustering. Knowl Inf Syst 11(2):155170" href="/article/10.1007/s00521-024-09417-3#ref-CR29" id="ref-link-section-d590711142e786">29</a>], where key instances are selected from positive bags. The selected instances are then used in the instance-space ensemble learning approach. However, the instance selection procedure in RSIS results in class imbalance problems and negatively affects performance. The constructive clustering ensemble (CCE) [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 30" title="Zhou Z-H, Xu J-M (2007) On the relation between multi-instance learning and semi-supervised learning. In: Proceedings of the 24th international conference on machine learning, pp. 11671174" href="/article/10.1007/s00521-024-09417-3#ref-CR30" id="ref-link-section-d590711142e789">30</a>] approach performs instance clustering to obtain a binary vector representation for the bag. The bit value in the binary vector determines the bag link to the clusters. However, the performance of CCE is comparatively low.</p><p>Bag-space techniques do not require access to instance labels, although they are not as explainable as instance-level approaches. For example, the graph-based kernel approach (mi-Graph) [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 16" title="Zhou Z-H, Sun Y-Y, Li Y-F (2009) Multi-instance learning by treating instances as non-IID samples. In: Proceedings of the 26th annual international conference on machine learning, pp. 12491256" href="/article/10.1007/s00521-024-09417-3#ref-CR16" id="ref-link-section-d590711142e795">16</a>] transforms the bag into a graph representation and employs a distance function to compare bags. Embedding space methods for bag classification adopt a fixed-size embedding vector used for bag classification. For example, Zhou et al. proposed two bag encoding techniques for MIL using Fisher vector encoding (miFV) and locally aggregated descriptors (miVLAD) [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 9" title="Wei X-S, Wu J, Zhou Z-H (2016) Scalable algorithms for multi-instance learning. IEEE Trans Neural Netw Learn Syst 28(4):975987" href="/article/10.1007/s00521-024-09417-3#ref-CR9" id="ref-link-section-d590711142e798">9</a>]. The miFV and miVLAD keep essential bag-level information in generated bag encodings with the help of dictionary learning. However, the bag-space classification algorithms lack any mechanism to learn appropriate feature representation. Other conventional MIL algorithms include semi-supervised SVMs for MIL (MissSVM) [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 31" title="Leistner C, Saffari A, Bischof H (2010) Miforests: Multiple-instance learning with randomized trees. In: European conference on computer vision, pp. 2942. Springer" href="/article/10.1007/s00521-024-09417-3#ref-CR31" id="ref-link-section-d590711142e801">31</a>], MIL with randomized trees [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 32" title="Li CH, Gondra I, Liu L (2012) An efficient parallel neural network-based multi-instance learning algorithm. J Supercomput 62(2):724740" href="/article/10.1007/s00521-024-09417-3#ref-CR32" id="ref-link-section-d590711142e804">32</a>], and many others [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 7" title="Carbonneau M-A, Cheplygina V, Granger E, Gagnon G (2018) Multiple instance learning: a survey of problem characteristics and applications. Pattern Recogn 77:329353" href="/article/10.1007/s00521-024-09417-3#ref-CR7" id="ref-link-section-d590711142e807">7</a>].</p><h3 class="c-article__sub-heading" id="Sec4"><span class="c-article-section__title-number">2.2 </span>Neural network-based MIL techniques</h3><p>This section introduces the related work based on neural network (NN) architectures for MIL. Traditionally, neural networks (NN) for MIL perform instance-level classification [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 33" title="Waqas M, Khan Z, Anjum S, Tahir MA (2020) Lung-wise tuberculosis analysis and automatic CT report generation with hybrid feature and ensemble learning. In: CLEF (Working Notes)" href="/article/10.1007/s00521-024-09417-3#ref-CR33" id="ref-link-section-d590711142e818">33</a>]. The convolution neural networks (CNN) are also used in MIL for feature extraction through multiple convolution layers [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" title="Abro WA, Aicher A, Rach N, Ultes S, Minker W, Qi G (2022) Natural language understanding for argumentative dialogue systems in the opinion building domain. Knowl-Based Syst 242:108318" href="#ref-CR34" id="ref-link-section-d590711142e821">34</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" title="Hanif M, Waqas M, Muneer A, Alwadain A, Tahir MA, Rafi M (2023) Deepsdc: deep ensemble learner for the classification of social-media flooding events. Sustainability 15(7):6049" href="#ref-CR35" id="ref-link-section-d590711142e821_1">35</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 36" title="Hoffman J, Pathak D, Darrell T, Saenko K (2015) Detector discovery in the wild: joint multiple instance and representation learning. In: Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 28832891" href="/article/10.1007/s00521-024-09417-3#ref-CR36" id="ref-link-section-d590711142e824">36</a>]. The best candidate search and instance positioning with the global max-pooling operation approach are explored in Hoffman[<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 37" title="Zhang C, Platt J, Viola P (2005) Multiple instance boosting for object detection. In: Weiss J, Sch\&#34;{o}lkopf B, Platt J(ed) Advances in neural information processing systems. MIT Press, 18" href="/article/10.1007/s00521-024-09417-3#ref-CR37" id="ref-link-section-d590711142e827">37</a>]. However, the max-pooling is not robust enough to find the influential instance, especially in the bag classification approach [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 15" title="Shi X, Xing F, Xie Y, Zhang Z, Cui L, Yang L (2020) Loss-based attention for deep multiple instance learning. In: Proceedings of the AAAI conference on artificial intelligence, vol. 34, pp. 57425749" href="/article/10.1007/s00521-024-09417-3#ref-CR15" id="ref-link-section-d590711142e830">15</a>].</p><p>To overcome the limitation of max-pooling, the concept of Noisy or [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 38" title="Shi X, Xing F, Xu K, Xie Y, Su H, Yang L (2017) Supervised graph hashing for histopathology image retrieval and classification. Med Image Anal 42:117128" href="/article/10.1007/s00521-024-09417-3#ref-CR38" id="ref-link-section-d590711142e836">38</a>], LSE, and generalized mean are introduced in Shi et al.[<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 39" title="Liu Y, Chen H, Wang Y, Zhang P (2021) Power pooling: an adaptive pooling function for weakly labelled sound event detection. In: 2021 International joint conference on neural networks (IJCNN), pp. 17. IEEE" href="/article/10.1007/s00521-024-09417-3#ref-CR39" id="ref-link-section-d590711142e839">39</a>]. However, these operators are non-trainable. In contrast, the use of an adaptive pooling approach and a fully connected network is proposed in Liu et al.[<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 40" title="Wang X, Yan Y, Tang P, Bai X, Liu W (2018) Revisiting multiple instance neural networks. Pattern Recogn 74:1524" href="/article/10.1007/s00521-024-09417-3#ref-CR40" id="ref-link-section-d590711142e842">40</a>]. MIL-based pooling approaches, e.g., mean and max-pooling operations, are proposed in Wang et al.[<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 41" title="Li G, Li C, Wu G, Ji D, Zhang H (2021) Multi-view attention-guided multiple instance detection network for interpretable breast cancer histopathological image diagnosis. IEEE Access 9:7967179684" href="/article/10.1007/s00521-024-09417-3#ref-CR41" id="ref-link-section-d590711142e845">41</a>], which is designed to extract features and perform backpropagation with the support of maximum response of instance feature extraction layers.</p><p>Contrary to the above discussed techniques, the attention-based pooling approach is considered as a kind of weighted average of instances in which the weights of the instances are obtained by trainable attention layers [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 42" title="Wang F, Jiang M, Qian C, Yang S, Li C, Zhang H, Wang X, Tang X (2017) Residual attention network for image classification. In: Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 31563164" href="/article/10.1007/s00521-024-09417-3#ref-CR42" id="ref-link-section-d590711142e851">42</a>]. This technique has been applied in several real-world problems, such as image classification and captioning [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 43" title="Dosovitskiy A, Beyer L, Kolesnikov A, Weissenborn D, Zhai X, Unterthiner T, Dehghani M, Minderer M, Heigold G, Gelly S et al.(2020) An image is worth 16x16 words: transformers for image recognition at scale. arXiv preprint &#xA;                  arXiv:2010.11929&#xA;                  &#xA;                " href="/article/10.1007/s00521-024-09417-3#ref-CR43" id="ref-link-section-d590711142e854">43</a>]. However, limited attention-based studies are available in the literature related to MIL. Attention-based instance pooling approach in Ilse[<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 13" title="Ilse M, Tomczak J, Welling M (2018) Attention-based deep multiple instance learning. In: International conference on machine learning, pp. 21272136. PMLR" href="/article/10.1007/s00521-024-09417-3#ref-CR13" id="ref-link-section-d590711142e857">13</a>] proposed two-layer (AbDMIL) and three-layer (Gated-AbDMIL) networks to attain instance weights. This approach focuses on binary classification problems and uses an additional layer for bag classification. The loss-based attention (LBA) approach [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 15" title="Shi X, Xing F, Xie Y, Zhang Z, Cui L, Yang L (2020) Loss-based attention for deep multiple instance learning. In: Proceedings of the AAAI conference on artificial intelligence, vol. 34, pp. 57425749" href="/article/10.1007/s00521-024-09417-3#ref-CR15" id="ref-link-section-d590711142e860">15</a>] proposed a weight-sharing approach among fully connected layers and attention layers. However, the attention pooling techniques [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 13" title="Ilse M, Tomczak J, Welling M (2018) Attention-based deep multiple instance learning. In: International conference on machine learning, pp. 21272136. PMLR" href="/article/10.1007/s00521-024-09417-3#ref-CR13" id="ref-link-section-d590711142e863">13</a>, <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 15" title="Shi X, Xing F, Xie Y, Zhang Z, Cui L, Yang L (2020) Loss-based attention for deep multiple instance learning. In: Proceedings of the AAAI conference on artificial intelligence, vol. 34, pp. 57425749" href="/article/10.1007/s00521-024-09417-3#ref-CR15" id="ref-link-section-d590711142e867">15</a>] assume no dependence among instances in the bag. Unlike previous attention-based techniques, the proposed ViT-IWRS generates several bag representations based on both assumptions and selects the suitable bag representation for the classification process.</p></div></div></section><section data-title="Proposed methodology"><div class="c-article-section" id="Sec5-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec5"><span class="c-article-section__title-number">3 </span>Proposed methodology</h2><div class="c-article-section__content" id="Sec5-content"><p>The proposed ViT-IWRS consists of four steps. In the first step, we propose a vision transformer-based approach to identify the dependencies between the bag instances. This process transforms input instances into latent representations using an embedding network and provides the latent transformation as input to a transformer encoder. The encoding process involves a multi-head-self-attention process that captures the global dependencies between the instances in the bag. With the output of the encoding process, we compute the weights for the bag instances in the second step. The weighting process ensures the assignment of higher weights to the essential instances in the bag. The process of instance embedding and transformer encoding is shown in Fig.<a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s00521-024-09417-3#Fig3">3</a>a, while the process of instance weighting is illustrated in Fig.<a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s00521-024-09417-3#Fig3">3</a>b.</p><p>The third step of the proposed approach involves generating bag representation vectors from instance weights for both instance relationship assumptions using encoder outputs and latent representations. Weights assigned to instances determine the composition of the representation vector and ensure that informative instances are represented more prominently. Figure<a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s00521-024-09417-3#Fig3">3</a>c illustrates the vector representation generation process. As a final step, the representation selection subnetwork (RSN) selects the final bag representation vector from a set of generated bag representation vectors. The RSN and bag classification process function is shown in Fig.<a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s00521-024-09417-3#Fig3">3</a>d. In the following subsection, we present problem formulation, a brief discussion of the vision transformer, and each step of the proposed approach in detail.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-3" data-title="Fig. 3"><figure><figcaption><b id="Fig3" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 3</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s00521-024-09417-3/figures/3" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs00521-024-09417-3/MediaObjects/521_2024_9417_Fig3_HTML.png?as=webp"><img aria-describedby="Fig3" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs00521-024-09417-3/MediaObjects/521_2024_9417_Fig3_HTML.png" alt="figure 3" loading="lazy" width="685" height="2020"></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-3-desc"><p>The Proposed ViT-IWRS framework. The top row in this block represents 3 different input bags (red, green, and blue) with a different number of instances (3, 4 and 5). Block (a) illustrates instance embedding and the transformer encoding process. The instance selection mechanism is shown in (b). The bag representation generation block is presented in (c). The representation weighting and bag classification process is shown in (d)</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s00521-024-09417-3/figures/3" data-track-dest="link:Figure3 Full size image" aria-label="Full size image figure 3" rel="nofollow"><span>Full size image</span><svg width="16" height="16" focusable="false" role="img" aria-hidden="true" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#icon-eds-i-chevron-right-small"></use></svg></a></div></figure></div><h3 class="c-article__sub-heading" id="Sec6"><span class="c-article-section__title-number">3.1 </span>Problem formulation</h3><p>In binary MIL classification problem, for a given bag <span class="mathjax-tex">\({\varvec{B}}_{i}=\left\{ {\varvec{x}}_{i, 1}, {\varvec{x}}_{i, 2}, {\varvec{x}}_{i, 3}, \ldots , {\varvec{x}}_{i, mi}\right\}\)</span> of <i>mi</i> total instances with <i>d</i> dimensions, where<span class="mathjax-tex">\({\varvec{x}}_{i, j}\)</span> represents jth instance of ith bag. The objective is to predict a bag target label <span class="mathjax-tex">\({\mathcal {Y}}_{i} \in \{1,0\}\)</span>. The prediction of bag label depends on the corresponding set of instance-level labels <span class="mathjax-tex">\(\left\{ y_{i, 1}, y_{i, 2}, \ldots , y_{i, m}\right\}\)</span>, where <span class="mathjax-tex">\(y_{i, j} \in \{1,0\}\)</span>. The instance-level labels remain unknown while the model training and <span class="mathjax-tex">\({\mathcal {Y}}_i\)</span> for binary classification is obtained as:</p><div id="Equ1" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex">$$\begin{aligned} {\mathcal {Y}}_i=\left\{ \begin{array}{c} 0 \;\; \text{ iff } \sum _{j=1}^{m} y_{i, j}=0 \\ 1 \quad \quad \quad \text{ otherwise } . \end{array}\right. \end{aligned}$$</span></div><div class="c-article-equation__number">
                    (1)
                </div></div><p>In this paper, we concentrate on bag-level classification for binary and multi-class MIL applications. Therefore, a representation vector is generated for the bag of instances and the model classifies the bag representation vector instead of individual instances.</p><p>Given a bag representation vector and corresponding bag label, the model generates a <span class="mathjax-tex">\(K-\)</span>dimensional vector of class scores <span class="mathjax-tex">\(s^K\)</span>, where <i>K</i> represents the number of classes. In this case, the bag label is determined by:</p><div id="Equ2" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex">$$\begin{aligned} {\mathcal {Y}}_i={\text {argmax}}_{k=0}^{K-1}\left( f(s)^{k}\right) , \end{aligned}$$</span></div><div class="c-article-equation__number">
                    (2)
                </div></div><p>where <span class="mathjax-tex">\(f(s)^{i}=\frac{\exp \left( s^{i}\right) }{\sum _{j=0}^{K-1} \exp \left( \rm{s}^{\rm{j}}\right) }\)</span> is Softmax function that squashes the score vector <span class="mathjax-tex">\(s^{k}\)</span> in the range between (0,1) and all the resulting elements add up to 1 and are interpreted as class probabilities.</p><h3 class="c-article__sub-heading" id="Sec7"><span class="c-article-section__title-number">3.2 </span>Vision transformer</h3><p>The Vision Transformer (ViT) is inspired by the concept of transformers in language processing models and can be seen as an alternative to the convolutional neural network (CNN) [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 44" title="Vaswani A, Shazeer N, Parmar N, Uszkoreit J, Jones L, Gomez AN, Kaiser , Polosukhin I (2017) Attention is all you need. Advances in neural information processing systems. Curran Associates, Inc., 30" href="/article/10.1007/s00521-024-09417-3#ref-CR44" id="ref-link-section-d590711142e1664">44</a>]. Vision Transformers (ViT) takes 1D patch embeddings as input. Therefore, the image is transformed into a sequence of two-dimensional flattened patches, and a trainable linear projection converts the generated patches to one-dimensional vectors. The projected image patches are called patch embeddings. A learnable embedding called class token is also prepended to patch embeddings. Moreover, the positional embeddings which are added to preserve the positional information of patches in the image.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-4" data-title="Fig. 4"><figure><figcaption><b id="Fig4" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 4</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s00521-024-09417-3/figures/4" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs00521-024-09417-3/MediaObjects/521_2024_9417_Fig4_HTML.png?as=webp"><img aria-describedby="Fig4" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs00521-024-09417-3/MediaObjects/521_2024_9417_Fig4_HTML.png" alt="figure 4" loading="lazy" width="685" height="467"></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-4-desc"><p>The vision transformer block is shown in (<b>a</b>), while the process of multi-head self-attention [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 45" title="Jang E, Gu S, Poole B (2017) Categorical Reparametrization with Gumbel-Softmax. In: Proceedings international conference on learning representations (ICLR). &#xA;                  https://openreview.net/pdf?id=rkE3y85ee&#xA;                  &#xA;                " href="/article/10.1007/s00521-024-09417-3#ref-CR45" id="ref-link-section-d590711142e1680">45</a>] is illustrated in (<b>b</b>)</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s00521-024-09417-3/figures/4" data-track-dest="link:Figure4 Full size image" aria-label="Full size image figure 4" rel="nofollow"><span>Full size image</span><svg width="16" height="16" focusable="false" role="img" aria-hidden="true" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#icon-eds-i-chevron-right-small"></use></svg></a></div></figure></div><p>Transform encoder [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 45" title="Jang E, Gu S, Poole B (2017) Categorical Reparametrization with Gumbel-Softmax. In: Proceedings international conference on learning representations (ICLR). &#xA;                  https://openreview.net/pdf?id=rkE3y85ee&#xA;                  &#xA;                " href="/article/10.1007/s00521-024-09417-3#ref-CR45" id="ref-link-section-d590711142e1697">45</a>] combines multi-head self-attention (MHSA) blocks with multi-layer perceptrons (MLP). Before each block, layer normalization (LN) is applied, and residual connections are used after each block. There are two layers of MLP and GELU nonlinearity in the transformer encoder. The details of the transformer encoder and MHSA process are shown in Fig.<a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s00521-024-09417-3#Fig4">4</a>. Vision transfer employs one or more stacked transformer encoder blocks in the encoding generation process. The generated class token from the last transformer encoder block is then employed for classification using a classification head. The classification head consists of MLP with one hidden layer.</p><h3 class="c-article__sub-heading" id="Sec8"><span class="c-article-section__title-number">3.3 </span>Vision transformer for bag encoding in MIL</h3><p>In MIL, the objective is classify a given bag <span class="mathjax-tex">\({\varvec{B}}_{i}=\left\{ {\varvec{x}}_{i, 1}, {\varvec{x}}_{i, 2}, {\varvec{x}}_{i, 3}, \ldots , {\varvec{x}}_{i, mi}\right\}\)</span> of <i>mi</i> instances, where <span class="mathjax-tex">\({\varvec{x}}_{i, j}\in {\mathbb {R}}^{1 \times d}\)</span>. In this case, the ViT can be employed to generate robust bag embeddings and determine dependencies among the bag instances. The self-attention in the transformer encoding process can allow instances in the bag to interact with each other. It can provide essential details about the relationship of instances in the bag, which can be used to generate a robust representation vector for the bag.</p><p>At first, each instance <span class="mathjax-tex">\({\varvec{x}}_{i, j}\)</span>in the bag <span class="mathjax-tex">\({\varvec{B}}_{i}\)</span> is transformed into a latent representation <span class="mathjax-tex">\({\varvec{h}}_{i, j}\)</span> using an embedding network. The process of instance embedding corresponds to the patch embedding process in standard ViT settings. However, the embedding network can consist of multi-layer perceptron (MLP) or convolution layers, depending upon the nature of the data. We used a similar design for the embedding network as previously used by Shi et al.[<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 15" title="Shi X, Xing F, Xie Y, Zhang Z, Cui L, Yang L (2020) Loss-based attention for deep multiple instance learning. In: Proceedings of the AAAI conference on artificial intelligence, vol. 34, pp. 57425749" href="/article/10.1007/s00521-024-09417-3#ref-CR15" id="ref-link-section-d590711142e1985">15</a>] and Ilse et al.[<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 13" title="Ilse M, Tomczak J, Welling M (2018) Attention-based deep multiple instance learning. In: International conference on machine learning, pp. 21272136. PMLR" href="/article/10.1007/s00521-024-09417-3#ref-CR13" id="ref-link-section-d590711142e1988">13</a>]. The details about the embedding network design are discussed in Sect.<a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s00521-024-09417-3#Sec31">5.9.1</a>. We refer to the generated latent instance representation <span class="mathjax-tex">\({\varvec{h}}_{i, j}\)</span> as instance embeddings. Similarly, the embeddings for all the instances in the bag <span class="mathjax-tex">\({\varvec{B}}_{i}\)</span> are grouped and referred to as bag embeddings <span class="mathjax-tex">\({\varvec{H}}_{i}^{[0]}=\left\{ {\varvec{h}}_{i, 1}, {\varvec{h}}_{i, 2}, \ldots . {\varvec{h}}_{i, m i}\right\}\)</span>. Afterward, the generated bag embeddings are prepended with a learnable class token <span class="mathjax-tex">\({\varvec{h}}_{i, 0}\)</span> and denoted by <span class="mathjax-tex">\({\varvec{H}}_{i}^{\prime [0]}=\left\{ {\varvec{h}}_{i, 0}, {\varvec{h}}_{i, 1}, {\varvec{h}}_{i, 2}, \ldots . {\varvec{h}}_{i, m i}\right\}\)</span>.</p><p>The class token aggregates global information from the entire bag, and it allows the model to make high-level decisions based on the overall content rather than relying solely on local instance information. The class token is typically fed into a classification head for image classification tasks. In the case of MIL, the class token diversifies the set of generated vector representations for the bag. The classification token is learnable embedding and can capture global dependencies and relationships in the bag. Thus, the classification token can be used as an additional bag representation vector. It can be used as an input for the representation selection network.</p><p>The generated bag embeddings serve as input to the encoder. At the start of the training process, the class token is randomly initialized and learned during the training process. The length of the class token is the same as the length of the instance embedding in the bags. The class token is used in the MHSA process in the same way as other instance embeddings of the bag and accumulates information from other instance embeddings [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 44" title="Vaswani A, Shazeer N, Parmar N, Uszkoreit J, Jones L, Gomez AN, Kaiser , Polosukhin I (2017) Attention is all you need. Advances in neural information processing systems. Curran Associates, Inc., 30" href="/article/10.1007/s00521-024-09417-3#ref-CR44" id="ref-link-section-d590711142e2341">44</a>]. Here, the positional embeddings are not used as bag representation follows a permutation invariant structure. The ViT encodes the given bag embeddings <span class="mathjax-tex">\({\varvec{H}}_{i}^{\prime [0]}\)</span> as:</p><div id="Equ3" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex">$$\begin{aligned} {\left\{ \begin{array}{ll} {\textbf{H}}_{i}^{\prime [0]}=\left\{ {\varvec{h}}_{i, 0}, {\varvec{h}}_{i, 1}, {\varvec{h}}_{i, 2}, \ldots {\varvec{h}}_{i, m i}\right\} , \\ {\varvec{H}}_{i}^{\prime [\ell -1]}={\text {MHSA}}\left( {\text {LN}} \left( {\textbf{H}}_{i}^{l-1}\right) \right) +{\textbf{H}}_{i}^{[\ell -1]}, &amp;{} \ell =1 \ldots L \\ {\varvec{H}}_{i}^{\prime [\ell ]}={\text {MLP}}\left( {\text {LN}} \left( {\varvec{H}}_{i}^{\prime [\ell -1]}\right) \right) +{\varvec{H}}_{i}^{\prime [\ell -1]}, &amp;{} \ell =1 \ldots L\end{array}\right. } \end{aligned}$$</span></div><div class="c-article-equation__number">
                    (3)
                </div></div><p>Where <span class="mathjax-tex">\(\ell\)</span> represents the index of the transformer encoder block, and <i>L</i> denotes the depth or the total number of encoder blocks. Discussion related to the depth of ViT and the number of heads in MHSA is presented in Sect.<a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s00521-024-09417-3#Sec34">5.9.4</a>. Additionally, the generated output of the encoding process is denoted by <span class="mathjax-tex">\({\varvec{H}}_{i}^{\prime [L]}=\)</span> <span class="mathjax-tex">\(\left[ {\varvec{h}}_{i, 0}^{[L]}, {\varvec{h}}_{i, 1}^{[L]}, {\varvec{h}}_{i, 2}^{[L]} \ldots .. {\varvec{h}}_{i, m i}^{[L]}\right]\)</span> where <span class="mathjax-tex">\({\varvec{h}}_{i, j}^{[L]}\)</span> and <span class="mathjax-tex">\({\varvec{h}}_{i, 0}^{[L]}\)</span> denote the output of the last transformer encoder block for the corresponding input instance embedding <span class="mathjax-tex">\({\varvec{h}}_{i, j}\)</span> and <span class="mathjax-tex">\({\varvec{h}}_{i,0}\)</span>, respectively.</p><p>Later, <span class="mathjax-tex">\({\varvec{H}}_{i}^{\prime [L]}\)</span> is used to generate bag representation vectors with the assumption of related instances, and <span class="mathjax-tex">\({\varvec{H}}_{i}^{[0]}\)</span> is used to generate bag representation vectors without instance relationship assumption, respectively. The process of instance embedding and bag encoding using ViT is illustrated in Fig.<a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s00521-024-09417-3#Fig3">3</a>a.</p><h3 class="c-article__sub-heading" id="Sec9"><span class="c-article-section__title-number">3.4 </span>Instance weight computation</h3><p>In this step, the weight for each instance in the bag is computed using the attention approach [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 13" title="Ilse M, Tomczak J, Welling M (2018) Attention-based deep multiple instance learning. In: International conference on machine learning, pp. 21272136. PMLR" href="/article/10.1007/s00521-024-09417-3#ref-CR13" id="ref-link-section-d590711142e3280">13</a>, <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 15" title="Shi X, Xing F, Xie Y, Zhang Z, Cui L, Yang L (2020) Loss-based attention for deep multiple instance learning. In: Proceedings of the AAAI conference on artificial intelligence, vol. 34, pp. 57425749" href="/article/10.1007/s00521-024-09417-3#ref-CR15" id="ref-link-section-d590711142e3283">15</a>]. This process highlights essential instances from the bag and assigns a higher weight to the informative instance. Later, the instances in the bag are pooled using a weighted average operation to obtain representation vectors for the bag. In this study, the weights of the transformer classification head are shared to learn instance weight and bag representation vector classification simultaneously. This process helps to enhance the connection between the loss and instance weighting process.</p><p>Let <span class="mathjax-tex">\({\textbf{W}} \in {\mathbb {R}}^{d \times K}\)</span> be a weight matrix and <span class="mathjax-tex">\({\textbf{b}} \in {\mathbb {R}}^{K}\)</span> be a bias vector of classification head <i>f</i>(:). Given the output of the last transformer encoder block <span class="mathjax-tex">\({\varvec{H}}_{i}^{\prime [L]}\)</span> the weights for the instance in the bag <span class="mathjax-tex">\({\varvec{B}}_{i}\)</span> are computed as:</p><div id="Equ4" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex">$$\begin{aligned} \underset{1 \le j \le m i}{\forall }\ \varvec{\alpha }_{i, j}=\frac{\sum _{c-0}^{K-1} \exp \left( {\varvec{h}}_{i, j}^{[L]} {\textbf{w}}^{{\textbf{c}}}+{\textbf{b}}^{{c}}\right) }{\sum _{t=1}^{{\text {mi}}} \sum _{c=0}^{K-1} \exp \left( {\varvec{h}}_{i, t}^{[L]} {\textbf{w}}^{{\textbf{c}}}+{\textbf{b}}^{{c}}\right) }, \end{aligned}$$</span></div><div class="c-article-equation__number">
                    (4)
                </div></div><p>where <span class="mathjax-tex">\({\textbf{w}}^{c} \in {\mathbb {R}}^{d}\)</span> is <i>c</i>th column vector of <span class="mathjax-tex">\({\textbf{W}}\)</span> and <span class="mathjax-tex">\(b^{c} \subset {\textbf{b}}\)</span> is corresponding bias. The obtained weights are then used to generate bag representation vectors in the next step. The process of weight computation is illustrated in Fig.<a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s00521-024-09417-3#Fig3">3</a>b.</p><h3 class="c-article__sub-heading" id="Sec10"><span class="c-article-section__title-number">3.5 </span>Computation of bag representation vectors</h3><p>After obtaining the weights of the instance in the bag, the next step is to compute bag representation vectors. This process transforms the bag with a variable number of instances to a manageable vector representation and transforms the MIL problem into a classical supervised learning problem. To classify the bags, one of the obtained vectors is selected using the representation selection subnetwork.</p><p>Given <span class="mathjax-tex">\({\varvec{H}}_{i}^{\prime [L]}=\)</span> <span class="mathjax-tex">\(\left[ {\varvec{h}}_{i, 0}^{[L]}, {\varvec{h}}_{i, 1}^{[L]}, {\varvec{h}}_{i, 2}^{[L]} \ldots .. {\varvec{h}}_{i, m i}^{[L]}\right]\)</span> and weights of instances <span class="mathjax-tex">\(\varvec{\alpha }_{i}\)</span> the representation vector for the bag <span class="mathjax-tex">\({\varvec{B}}_{i}\)</span> are computed as:</p><div id="Equ5" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex">$$\begin{aligned} \varvec{\psi }_{i}=\sum _{j=1}^{m i} \varvec{\alpha }_{i, j} \cdot {\varvec{h}}_{i, j}^{[L]}. \end{aligned}$$</span></div><div class="c-article-equation__number">
                    (5)
                </div></div><p>The computed bag representations <span class="mathjax-tex">\(\varvec{\psi }_{i}\)</span> involves the output of the transformer encoder, and <span class="mathjax-tex">\({\varvec{h}}_{i, 0}^{[L]}\)</span> is learned class token. The learning process of these vectors considers all the instances in the bag. Thus, these vectors incorporate the information related to the relationship of instances in the bag <span class="mathjax-tex">\({\varvec{B}}_{i}\)</span>.</p><p>Additionally, bag representation vectors without assuming instance relationship are obtained based on the bag embeddings <span class="mathjax-tex">\({\varvec{H}}_{i}^{[0]}=\)</span> <span class="mathjax-tex">\(\left\{ {\varvec{h}}_{i, 1}, {\varvec{h}}_{i, 2}, \ldots {\varvec{h}}_{i, m i}\right\}\)</span> as:</p><div id="Equ6" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex">$$\begin{aligned} \left\{ \begin{aligned} \varvec{\omega }_{i}&amp;=\sum _{j=1}^{m i} \varvec{\alpha }_{i, j} \cdot {\varvec{h}}_{i, j}^{[0]}, \\ \varvec{m a x}_{i}&amp;=\max _{1 \le j \le m i}\left( {\varvec{H}}_{i}^{[0]}\right) , \\ \varvec{\mu }_{i}&amp;=\frac{1}{m i} \sum _{j=1}^{m i} {\varvec{h}}_{i, j}^{[0]}, \end{aligned}\right. \end{aligned}$$</span></div><div class="c-article-equation__number">
                    (6)
                </div></div><p>where the <span class="mathjax-tex">\(\varvec{\omega }_{i}, \varvec{\mu }_{i}, \varvec{m a x}_{i}\)</span> represent the attention weighted average [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 13" title="Ilse M, Tomczak J, Welling M (2018) Attention-based deep multiple instance learning. In: International conference on machine learning, pp. 21272136. PMLR" href="/article/10.1007/s00521-024-09417-3#ref-CR13" id="ref-link-section-d590711142e4763">13</a>], mean, and max representation vectors, respectively. The computation of these representation vectors does not incorporate any dependencies or relationships between the instances of the bag. Therefore, <span class="mathjax-tex">\(\varvec{\omega }_{i}, \varvec{\mu }_{i}, \max _{i}\)</span> are based on the assumption of unrelated instances of <span class="mathjax-tex">\({\varvec{B}}_{i}\)</span>. Figure<a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s00521-024-09417-3#Fig3">3</a>c shows the representation vector generation process.</p><h3 class="c-article__sub-heading" id="Sec11"><span class="c-article-section__title-number">3.6 </span>Representation selection subnetwork (RSN)</h3><p>The instance in the bag can either be related or unrelated. Therefore, the representation vector generated by a correct distribution assumption will provide critical information to the classifier. In this case, RSN aims to select one of the representation vectors, which is most informative for the bag classification. RSN performs hard selection using Gumbel SoftMax in an end-to-end approach [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 46" title="Li X-C, Zhan D-C, Yang J-Q, Shi Y (2021) Deep multiple instance selection. Sci China Inf Sci 64(3):115" href="/article/10.1007/s00521-024-09417-3#ref-CR46" id="ref-link-section-d590711142e4856">46</a>]. This process is analogous to computing the softmax over a stochastically sampled set of points. The Gumbel-Max Trick separates the deterministic and stochastic parts of the sampling process using the reparameterization trick [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 46" title="Li X-C, Zhan D-C, Yang J-Q, Shi Y (2021) Deep multiple instance selection. Sci China Inf Sci 64(3):115" href="/article/10.1007/s00521-024-09417-3#ref-CR46" id="ref-link-section-d590711142e4859">46</a>, <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 47" title="LeCun Y, Cortes C, Burges C (2010) Mnist handwritten digit database. ATT Labs [Online]. Available: &#xA;                  http://yann.lecun.com/exdb/mnist&#xA;                  &#xA;                2" href="/article/10.1007/s00521-024-09417-3#ref-CR47" id="ref-link-section-d590711142e4862">47</a>]. It computes the log probabilities of given scores in the distribution and adds some noise to them from the Gumbel distribution. Finally, the argmax function is applied to find the class with the maximum value for each representation vector and generate a one-hot vector for use by the rest of the neural network.</p><p>At First, the previously computed <i>n</i> representation vectors for the bag <span class="mathjax-tex">\({\varvec{B}}_i\)</span> are combined to form a representation matrix <span class="mathjax-tex">\({\mathcal {R}}=\left[ {\varvec{h}}_{i,c l s}^{[L-1]}, \varvec{\psi }_i, \varvec{\mu }_{{i}}, \varvec{max}_{i}, \varvec{\omega }_i\right] \in {\mathbb {R}}^{n \times d}\)</span>, where <i>d</i> denotes the length of representation vectors. Afterward, the representation matrix <span class="mathjax-tex">\({\mathcal {R}}\)</span> is given as input to RSN (<span class="mathjax-tex">\({\mathcal {R}}\)</span>), which outputs the score vector <span class="mathjax-tex">\({\varvec{r}} \in {\mathbb {R}}^{n \times 1}\)</span> and representation selection code <span class="mathjax-tex">\({\varvec{u}}=\)</span> <span class="mathjax-tex">\(\left( u_1, u_2, \ldots , u_n\right)\)</span> are computed as:</p><div id="Equ7" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex">$$\begin{aligned} u_i=\frac{\exp \left( \frac{\left( \log \left( r_i\right) +g_i\right) }{\tau }\right) }{\sum _{j=1}^n \exp \left( \frac{\left( \log \left( r_i\right) +g_j\right) }{\tau }\right) }, \end{aligned}$$</span></div><div class="c-article-equation__number">
                    (7)
                </div></div><p>where <span class="mathjax-tex">\(g_i \sim\)</span> Gumbel <span class="mathjax-tex">\((0,1)=-\log (-\log (q)), q \sim\)</span> Uniform (0,1). Additionally, <span class="mathjax-tex">\(\tau \in (0, \infty )\)</span> is the temperature parameter, which determines the degree of approximation for <span class="mathjax-tex">\({\varvec{u}}\)</span> in relation to a one-hot vector. A smaller value of <span class="mathjax-tex">\(\tau\)</span> results in a harder <span class="mathjax-tex">\({\varvec{u}}\)</span>, whereas a higher <span class="mathjax-tex">\(\tau\)</span> leads to a smoother <span class="mathjax-tex">\({\varvec{u}}\)</span>. The obtained <i>u</i> is further used to generate a one-hot vector as:</p><div id="Equ8" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex">$$\begin{aligned} \begin{aligned}&amp;{\varvec{i}}^{\star }=\arg \max _i\left\{ u_i\right\} , \\&amp;{\varvec{e}}^*={\text {OneHot}}\left( {\varvec{i}}^{\star }\right) , \end{aligned} \end{aligned}$$</span></div><div class="c-article-equation__number">
                    (8)
                </div></div><p>where <span class="mathjax-tex">\(i^{\star }\)</span> denotes sampled index and <span class="mathjax-tex">\(e^*\)</span> represents the one-hot vector with the <span class="mathjax-tex">\(i^{\star }\)</span> the element being 1. Afterward, the bag representation vector for the bag <span class="mathjax-tex">\({\varvec{B}}_{{\varvec{i}}}\)</span> is selected as:</p><div id="Equ9" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex">$$\begin{aligned} {\varvec{v}}_i={\mathcal {R}}^T e^*. \end{aligned}$$</span></div><div class="c-article-equation__number">
                    (9)
                </div></div><p>The selected bag representation vector <span class="mathjax-tex">\({\varvec{v}}_{{\varvec{i}}}\)</span> is then used to classify the bag label by classification head <i>f</i>(:) as</p><div id="Equ10" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex">$$\begin{aligned} {\mathcal {Y}}_i=f\left( {\varvec{v}}_i\right) . \end{aligned}$$</span></div><div class="c-article-equation__number">
                    (10)
                </div></div><p>Furthermore, the details related to the number of layers in RSN are discussed in Sect.<a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s00521-024-09417-3#Sec32">5.9.2</a>.</p><h3 class="c-article__sub-heading" id="Sec12"><span class="c-article-section__title-number">3.7 </span>Loss function</h3><p>This section presents the loss function for the training of ViT-IWRS. The proposed loss scheme is derived from the concept of cross-entropy (CE) loss [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 15" title="Shi X, Xing F, Xie Y, Zhang Z, Cui L, Yang L (2020) Loss-based attention for deep multiple instance learning. In: Proceedings of the AAAI conference on artificial intelligence, vol. 34, pp. 57425749" href="/article/10.1007/s00521-024-09417-3#ref-CR15" id="ref-link-section-d590711142e5956">15</a>]. CE is a measure of dissimilarity between the true and predicted label.</p><p>Given a representation vector <span class="mathjax-tex">\(\varvec{{v}}\)</span> for the training bag <span class="mathjax-tex">\({\varvec{B}}_i\)</span>, and corresponding label <span class="mathjax-tex">\({\mathcal {Y}}_i \in\)</span> <span class="mathjax-tex">\(\{0,1, \cdots , K-1\}\)</span>, where <i>K</i> denotes the number of classes. Let <i>f</i>(:) represent a neural network and <span class="mathjax-tex">\(\varvec{z_i}=f(\varvec{{v}}) \in {\mathbb {R}}^{K}\)</span> be the class score vector for <span class="mathjax-tex">\({\varvec{B}}_i\)</span>. The estimated class probability of <span class="mathjax-tex">\({\varvec{B}}_i\)</span> belonging to the <i>k</i>-th class can be computed by using softmax function:</p><div id="Equ11" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex">$$\begin{aligned} q^k_{i}=\frac{\exp \left( z^k_{i}\right) }{\sum _{c=0}^{K-1} \exp \left( z^c_{i}\right) }, \end{aligned}$$</span></div><div class="c-article-equation__number">
                    (11)
                </div></div><p>where <span class="mathjax-tex">\(\exp (:)\)</span> represents the exponential function. For multi-class classification, the loss function can be written as:</p><div id="Equ12" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex">$$\begin{aligned} CE=-\sum _{c=0}^{K-1} {p}_{i}^{c} \log {q}_{i}^{c}, \end{aligned}$$</span></div><div class="c-article-equation__number">
                    (12)
                </div></div><p>where <span class="mathjax-tex">\({p}_{i}^{c} \in \{0,1\}^{K}\)</span> denote the true probability of the bag <span class="mathjax-tex">\({\varvec{B}}_{i}\)</span> belonging to the <span class="mathjax-tex">\(c_{t h}\)</span> class, and <span class="mathjax-tex">\({q}_{i}^{c}\)</span> is the estimated probability.</p><p>The target vector <span class="mathjax-tex">\({\textbf{p}}\)</span> is one-hot encodings in multi-class classification. In this case, if <span class="mathjax-tex">\({\varvec{B}}_{i}\)</span> belongs to the <i>k</i>-th class, there is only one element <span class="mathjax-tex">\(p_i^{k}\)</span> in the target vector which is not zero. So, only the positive class contributes to the loss computation process. Discarding the elements of the summation which are zero due to target labels in equation (<a data-track="click" data-track-label="link" data-track-action="equation anchor" href="/article/10.1007/s00521-024-09417-3#Equ12">12</a>), the loss function can be written as:</p><div id="Equ13" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex">$$\begin{aligned} CE=-\log \left( \frac{\exp \left( {z}_{i}^{k}\right) }{\sum _{c=0}^{k-1} \exp \left( {z}_{i}^{c}\right) }\right) . \end{aligned}$$</span></div><div class="c-article-equation__number">
                    (13)
                </div></div><p>Suppose that the training bag <span class="mathjax-tex">\({\varvec{B}}_i\)</span> belongs to the <i>k</i>th class. In this case, given the output of ViT <span class="mathjax-tex">\({\varvec{H}}_{i}^{\prime [L]}=\)</span> <span class="mathjax-tex">\(\left[ {\varvec{h}}_{i, 0}^{[L]}, {\varvec{h}}_{i, 1}^{[L]}, {\varvec{h}}_{i, 2}^{[L]} \ldots .. {\varvec{h}}_{i, m i}^{[L]}\right]\)</span>, the weights of instances <span class="mathjax-tex">\(\varvec{\alpha }_{i}\)</span>, and corresponding bag representation vector <span class="mathjax-tex">\({\varvec{v}}\)</span>, the loss for the bag <span class="mathjax-tex">\({\varvec{B}}_i\)</span> is computed as:</p><div id="Equ14" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex">$$\begin{aligned} L 1=-\log \left( \frac{\exp \left( {\varvec{v}} {\textbf{w}}^{k}+b^{k}\right) }{\sum _{c=0}^{K-1} \exp \left( {\varvec{v}} {\textbf{w}}^{c}+b^{c} \right) }\right) , \end{aligned}$$</span></div><div class="c-article-equation__number">
                    (14)
                </div></div><div id="Equ15" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex">$$\begin{aligned} L 2=\sum _{j=1}^{mi}\left( -\log \left( \frac{\exp \left( {\varvec{h}}_{i, j}^{[L]}{\textbf{w}}^{k}+b^{k}\right) }{\sum _{c=0}^{K-1} \exp \left( {\varvec{h}}_{i, c}^{[L]}{\textbf{w}}^{c}+b^{c}\right) }\right) \alpha _{i, j}\right) , \end{aligned}$$</span></div><div class="c-article-equation__number">
                    (15)
                </div></div><div id="Equ16" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex">$$\begin{aligned} L o s s=L 1+\lambda L 2. \end{aligned}$$</span></div><div class="c-article-equation__number">
                    (16)
                </div></div><p>where <span class="mathjax-tex">\({\textbf{w}}^{c} \in {\mathbb {R}}^{d}\)</span> is <i>c</i>th column vector of weight matrix <span class="mathjax-tex">\({\textbf{W}}\)</span> and <span class="mathjax-tex">\(b^{c}\)</span> is corresponding bias for classification head <i>f</i>(:).</p><p>The first term of the loss function focuses on bag classification loss, while the second one captures the attention loss, and <span class="mathjax-tex">\(\lambda\)</span> is a non-negative hyperparameter to balance between bag and attention loss. The discussion related to the impact of <span class="mathjax-tex">\(\lambda\)</span> is given in Sect.<a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s00521-024-09417-3#Sec33">5.9.3</a>.</p><p>The term <span class="mathjax-tex">\(L{1} \rightarrow 0\)</span> if any one instance in a bag <span class="mathjax-tex">\({\varvec{B}}_{i}\)</span> belongs to the <i>kth</i> class. However, in this case, it is not theoretically guaranteed that only one instance belongs to the <i>k</i>th class in the bag [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 15" title="Shi X, Xing F, Xie Y, Zhang Z, Cui L, Yang L (2020) Loss-based attention for deep multiple instance learning. In: Proceedings of the AAAI conference on artificial intelligence, vol. 34, pp. 57425749" href="/article/10.1007/s00521-024-09417-3#ref-CR15" id="ref-link-section-d590711142e7666">15</a>]. Therefore, it results in a high false negative rate for the instances in the positive bags. To address this issue, the <i>L</i>2 term is added to the objective function. This term ensures that more than one instance with higher weights contributes to the label. Furthermore, the <i>L</i>2 term is inspired by the fact that the weight of instance <span class="mathjax-tex">\({\varvec{x}}_{i, j}\)</span> become approximately zero when <span class="mathjax-tex">\(y_{i, j} \ne {\mathcal {Y}}_i\)</span>.</p></div></div></section><section data-title="Experimental setup"><div class="c-article-section" id="Sec13-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec13"><span class="c-article-section__title-number">4 </span>Experimental setup</h2><div class="c-article-section__content" id="Sec13-content"><p>This Section introduces the datasets used for experiments along with relevant evaluation measures. Additionally, a comparative analysis of existing methods is also provided.</p><h3 class="c-article__sub-heading" id="Sec14"><span class="c-article-section__title-number">4.1 </span>Details of datasets and evaluation measure</h3><p>The performance of ViT-IWRS is evaluated using different datasets for binary and multi-class classification problems. These datasets have been used to assess the performance of MIL algorithms in the literature and cover a range of MIL application domains, such as molecular activity prediction, image classification, object detection, and medical image classification. The details of these datasets are given below.</p><h4 class="c-article__sub-heading c-article__sub-heading--small" id="Sec15"><span class="c-article-section__title-number">4.1.1 </span>Benchmark MIL datasets</h4><p>The experiments are conducted on five MIL datasets related to binary classification problems: Musk1, Musk2, Elephant, Tiger, and Fox. These datasets are related to binary classification problems. The first two datasets (Musk1 and Musk2) cover the application of MIL for molecular drug activity predictions [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 23" title="Sirinukunwattana K, Raza SEA, Tsang Y-W, Snead DR, Cree IA, Rajpoot NM (2016) Locality sensitive deep learning for detection and classification of nuclei in routine colon cancer histology images. IEEE Trans Med Imaging 35(5):11961206" href="/article/10.1007/s00521-024-09417-3#ref-CR23" id="ref-link-section-d590711142e7774">23</a>]. These datasets are composed of molecular conformations of multiple shapes. The bag is formed based on the shape similarity, and the drugs effect is observed if one or more conformations are attached to the targeted bindings. The later three datasets: Elephant, Tiger, and Fox, are related to image classification [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 26" title="Amar RA, Dooly DR, Goldman SA, Zhang Q (2001) Multiple-instance learning of real-valued data. In: ICML, pp. 310. Citeseer" href="/article/10.1007/s00521-024-09417-3#ref-CR26" id="ref-link-section-d590711142e7777">26</a>]; features of image segments constitute the bags in these datasets. The positive bags hold one or more instances related to the animal of interest while the negative bags contain other animals. The details of these datasets are shown in Table<a data-track="click" data-track-label="link" data-track-action="table anchor" href="/article/10.1007/s00521-024-09417-3#Tab1">1</a>.</p><h4 class="c-article__sub-heading c-article__sub-heading--small" id="Sec16"><span class="c-article-section__title-number">4.1.2 </span>MIL-based MNIST dataset</h4><p>In addition to the existing benchmark MIL dataset, an additional dataset for multi-class classification is created from well-known MNIST digits (MIL-MINST) for digit classification [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 48" title="Krizhevsky A, Hinton G (2009) Learning multiple layers of features from tiny images. Technical report, University of Toronto. &#xA;                  https://www.cs.toronto.edu/~kriz/learning-features-2009-TR.pdf&#xA;                  &#xA;                " href="/article/10.1007/s00521-024-09417-3#ref-CR48" id="ref-link-section-d590711142e7791">48</a>]. The dataset consists of gray-scale digit images of size <span class="mathjax-tex">\(28 \times 28\)</span>, and the images are randomly selected to form a bag where each digit represents an instance. In this problem, we have used a labeling approach similar to [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 15" title="Shi X, Xing F, Xie Y, Zhang Z, Cui L, Yang L (2020) Loss-based attention for deep multiple instance learning. In: Proceedings of the AAAI conference on artificial intelligence, vol. 34, pp. 57425749" href="/article/10.1007/s00521-024-09417-3#ref-CR15" id="ref-link-section-d590711142e7818">15</a>], where bags with the target digits {3, 5, 9} are labeled {1, 2, 3} accordingly and if a bag does not include any of the target digits, it is labeled as 0. in the training process, the model is trained for 50, 100, 150, 200, 300, and 400 generated training bags, respectively, while the performance is evaluated on 1000 test bags.</p><h4 class="c-article__sub-heading c-article__sub-heading--small" id="Sec17"><span class="c-article-section__title-number">4.1.3 </span>MIL-based CIFAR-10 dataset</h4><p>We construct more challenging MIL datasets for multi-class classification using images from the CIFAR-10 dataset for object recognition MIL application [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 49" title="Ghaznavi F, Evans A, Madabhushi A, Feldman M (2013) Digital imaging in pathology: whole-slide imaging and beyond. Annu Rev Pathol 8:331359" href="/article/10.1007/s00521-024-09417-3#ref-CR49" id="ref-link-section-d590711142e7829">49</a>]. The CIFAR-10 dataset contains 60000 images divided into ten classes, each image is of size <span class="mathjax-tex">\(32 \times 32\)</span>, and classes are completely mutually exclusive. We employed a similar approach previously used in Shi et al.[<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 15" title="Shi X, Xing F, Xie Y, Zhang Z, Cui L, Yang L (2020) Loss-based attention for deep multiple instance learning. In: Proceedings of the AAAI conference on artificial intelligence, vol. 34, pp. 57425749" href="/article/10.1007/s00521-024-09417-3#ref-CR15" id="ref-link-section-d590711142e7856">15</a>] to evaluate the performance of ViT-IWRS on this dataset. The bags are formed by treating images as instances, and bags are normally distributed with a mean bag size of 10 and a variance of 2, respectively. The target classes are set to {airplane, automobile, bird}, and associated with the labels {'1', '2', '3'} accordingly. The bags related to target classes at most contain images from one of these three classes. The training sets are built with 500 and 5000 bags, while the test set is created with 1000 bags.</p><div class="c-article-table" data-test="inline-table" data-container-section="table" id="table-1"><figure><figcaption class="c-article-table__figcaption"><b id="Tab1" data-test="table-caption">Table 1 The details of MIL benchmark datasets</b></figcaption><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="table-link" data-track="click" data-track-action="view table" data-track-label="button" rel="nofollow" href="/article/10.1007/s00521-024-09417-3/tables/1" aria-label="Full size table 1"><span>Full size table</span><svg width="16" height="16" focusable="false" role="img" aria-hidden="true" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#icon-eds-i-chevron-right-small"></use></svg></a></div></figure></div><h4 class="c-article__sub-heading c-article__sub-heading--small" id="Sec18"><span class="c-article-section__title-number">4.1.4 </span>Colon cancer dataset</h4><p>Detecting cancerous regions in hematoxylin and eosin (H &amp;E) stained whole-slide images (WSI) are vital in clinical settings [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 50" title="Dimitriou N, Arandjelovi O, Caie PD (2019) Deep learning for whole slide image analysis: an overview. Front Med 6:264" href="/article/10.1007/s00521-024-09417-3#ref-CR50" id="ref-link-section-d590711142e8068">50</a>]. These images, also called digital pathology slides, can occupy several gigabytes of storage space [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 51" title="Asif A et al (2019) An embarrassingly simple approach to neural multiple instance classification. Pattern Recogn Lett 128:474479" href="/article/10.1007/s00521-024-09417-3#ref-CR51" id="ref-link-section-d590711142e8071">51</a>]. Presently, supervised approaches require pixel-level annotations, which demand significant time from pathologists. A successful solution to reduce pathologists workload is to use weak slide levels. For this study, we conducted experiments on colon cancer histopathology images [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 24" title="Raykar VC, Krishnapuram B, Bi J, Dundar M, Rao RB (2008) Bayesian multiple instance learning: automatic feature selection and inductive transfer. In: Proceedings of the 25th international conference on machine learning, pp. 808815" href="/article/10.1007/s00521-024-09417-3#ref-CR24" id="ref-link-section-d590711142e8074">24</a>] to test the efficiency of ViT-IWRS.</p><p>This dataset consists of 100H&amp;E images belonging to binary classes. These images feature a range of tissue appearances, including both normal and malignant regions. Every image has been marked with the majority of nuclei for each cell with a total of 22,444 nuclei and class labels such as epithelial, inflammatory, fibroblast, and miscellaneous. Every WSI represents a bag with several 27<span class="mathjax-tex">\(\times\)</span>27 patches. The bag is labeled as positive if it has one or more nuclei from the epithelial class.</p><h4 class="c-article__sub-heading c-article__sub-heading--small" id="Sec19"><span class="c-article-section__title-number">4.1.5 </span>Evaluation measure</h4><p>We evaluate the performance of the proposed ViT-IWRS in terms of bag classification accuracy. The experiments on benchmark datasets are performed using five runs of 10-fold cross-validation, and average performance is reported. For the MIL-based MNIST dataset, the experiments are performed with 1000 test bags and different numbers of training bags (50, 100, 150, 200, 300, and 400). The experiments are repeated 50 times for each train and test set, and average results are compared with existing state-of-the-art techniques. Similarly, the experiments are repeated thirty times with different training and testing data for MIL-based CIFAR-10 datasets, and average performance is reported. On the Colon Cancer dataset, we performed a 5-fold cross-validation, and average results are presented.</p><h3 class="c-article__sub-heading" id="Sec20"><span class="c-article-section__title-number">4.2 </span>Methods used for comparative study</h3><p>The proposed approach is compared with several state-of-the-art attention-based approaches and other benchmark bag-level classification techniques. The methods for performance comparison are selected based on good performance and the wide range of MIL solutions they offer. Some of the methods are briefly discussed below.</p><ul class="u-list-style-bullet">
                  <li>
                    <p>MIL NN [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 41" title="Li G, Li C, Wu G, Ji D, Zhang H (2021) Multi-view attention-guided multiple instance detection network for interpretable breast cancer histopathological image diagnosis. IEEE Access 9:7967179684" href="/article/10.1007/s00521-024-09417-3#ref-CR41" id="ref-link-section-d590711142e8121">41</a>]: This study proposes trainable pooling operators for MIL. In this work, the bag-level classification technique (MI-NET) directly produces the bag label. The instance-level classification technique (mi-NET) pools instance-level scores to produce the bag label. The pooling approach based on the residual connection ( MI-NET RC) is also proposed.</p>
                  </li>
                  <li>
                    <p>Ranking Loss-based Simple MIL (ESMIL) [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 52" title="Hahn M (2020) Theoretical limitations of self-attention in neural sequence models. Trans Assoc Comput Linguist 8:156171" href="/article/10.1007/s00521-024-09417-3#ref-CR52" id="ref-link-section-d590711142e8130">52</a>]: This paper presents a novel approach to differentiate between positive and negative bags by a simple pairwise bag-level ranking loss function. The proposed objective function ensures that the model assigns a higher score to the positive bags. Instead of using a threshold-based decision function, the proposed approach penalizes the network when it generates a lower score for positive bags compared to negative bags.</p>
                  </li>
                  <li>
                    <p>Attention-based Deep MIL (AbDMIL) [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 13" title="Ilse M, Tomczak J, Welling M (2018) Attention-based deep multiple instance learning. In: International conference on machine learning, pp. 21272136. PMLR" href="/article/10.1007/s00521-024-09417-3#ref-CR13" id="ref-link-section-d590711142e8139">13</a>]: This work proposed an attention approach to identify the weights of the instances in the bag. The authors proposed two architectures for attention-based pooling to solve MIL binary classification problem.</p>
                  </li>
                  <li>
                    <p>Loss-based Attention (LBA) [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 15" title="Shi X, Xing F, Xie Y, Zhang Z, Cui L, Yang L (2020) Loss-based attention for deep multiple instance learning. In: Proceedings of the AAAI conference on artificial intelligence, vol. 34, pp. 57425749" href="/article/10.1007/s00521-024-09417-3#ref-CR15" id="ref-link-section-d590711142e8148">15</a>]: This method extends the concepts of (AbDMIL) [11] and introduces collaborative training for attention and classification layers of the network.</p>
                  </li>
                  <li>
                    <p>Multiple-instance SVM (MI-SVM and mi-SVM) [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 26" title="Amar RA, Dooly DR, Goldman SA, Zhang Q (2001) Multiple-instance learning of real-valued data. In: ICML, pp. 310. Citeseer" href="/article/10.1007/s00521-024-09417-3#ref-CR26" id="ref-link-section-d590711142e8157">26</a>]: In this study, two algorithms mi-SVM and MI-SVM extend the use of SVM to solve multiple-instance learning problems. The MI-SVM maximizes the bag margin while SVM updates the hyper-plane based on the instance label assignments.</p>
                  </li>
                  <li>
                    <p>Classifier Ensemble with constructive clustering (CCE) [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 30" title="Zhou Z-H, Xu J-M (2007) On the relation between multi-instance learning and semi-supervised learning. In: Proceedings of the 24th international conference on machine learning, pp. 11671174" href="/article/10.1007/s00521-024-09417-3#ref-CR30" id="ref-link-section-d590711142e8167">30</a>]: This method represents the entire bag of instances from a binary vector, employing clustering and adopting an ensemble learning-based classification approach. The binary vector entries are set to 1 if any bag instance is a part of the cluster. Additionally, the clustering and models are trained on different data representations.</p>
                  </li>
                  <li>
                    <p>Multiple instances (Fisher Vector and VLAD) [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 9" title="Wei X-S, Wu J, Zhou Z-H (2016) Scalable algorithms for multi-instance learning. IEEE Trans Neural Netw Learn Syst 28(4):975987" href="/article/10.1007/s00521-024-09417-3#ref-CR9" id="ref-link-section-d590711142e8176">9</a>]: These methods are based on bag encoding generation techniques. These techniques are inspired by the widely used Fisher vector (FV) and VLAD encoding schemes for image classification</p>
                  </li>
                </ul></div></div></section><section data-title="Results and discussion"><div class="c-article-section" id="Sec21-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec21"><span class="c-article-section__title-number">5 </span>Results and discussion</h2><div class="c-article-section__content" id="Sec21-content"><p>In this Section, we present the results and discuss the performance of the proposed (ViT-IWRS )approach. First, we compare the performance of the proposed approach with state-of-the-art (SOTA) attention-based pooling approaches for MIL classification problems, including AbDMIL [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 13" title="Ilse M, Tomczak J, Welling M (2018) Attention-based deep multiple instance learning. In: International conference on machine learning, pp. 21272136. PMLR" href="/article/10.1007/s00521-024-09417-3#ref-CR13" id="ref-link-section-d590711142e8190">13</a>], Gated-AbDMIL [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 53" title="Frank E, Xu X (2008) Applying propositional learning algorithms to multi-instance data. Working paper series, Department of computer science, The University of Waikato. &#xA;                  https://books.google.com/books?id=5eaGzgEACAAJ&#xA;                  &#xA;                " href="/article/10.1007/s00521-024-09417-3#ref-CR53" id="ref-link-section-d590711142e8193">53</a>], and loss-based attention (LBA) [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 15" title="Shi X, Xing F, Xie Y, Zhang Z, Cui L, Yang L (2020) Loss-based attention for deep multiple instance learning. In: Proceedings of the AAAI conference on artificial intelligence, vol. 34, pp. 57425749" href="/article/10.1007/s00521-024-09417-3#ref-CR15" id="ref-link-section-d590711142e8196">15</a>]. Later, the proposed approach is compared to benchmark bag classification approaches.</p><h3 class="c-article__sub-heading" id="Sec22"><span class="c-article-section__title-number">5.1 </span>Comparison with SOTA attention-based pooling approaches</h3><p>The comparison of the ViT-IWRS with three SOTA attention techniques LBA [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 15" title="Shi X, Xing F, Xie Y, Zhang Z, Cui L, Yang L (2020) Loss-based attention for deep multiple instance learning. In: Proceedings of the AAAI conference on artificial intelligence, vol. 34, pp. 57425749" href="/article/10.1007/s00521-024-09417-3#ref-CR15" id="ref-link-section-d590711142e8206">15</a>] and AbDMIL [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 13" title="Ilse M, Tomczak J, Welling M (2018) Attention-based deep multiple instance learning. In: International conference on machine learning, pp. 21272136. PMLR" href="/article/10.1007/s00521-024-09417-3#ref-CR13" id="ref-link-section-d590711142e8209">13</a>] is depicted in Fig.<a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s00521-024-09417-3#Fig5">5</a>. Similar to the proposed ViT-IWRS, the algorithms estimate the weights of the instances using the attention mechanism and generate a representation vector for the bag. However, these techniques do not consider the relationship of instances in the bag. These approaches are implemented, and reproduced results are reported. The proposed ViT-IWRS achieves better results in all five datasets. For the Fox dataset, the proposed approach achieved 62.5% accuracy compared to the 60.5% and 59.5% accuracy achieved by LBA [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 15" title="Shi X, Xing F, Xie Y, Zhang Z, Cui L, Yang L (2020) Loss-based attention for deep multiple instance learning. In: Proceedings of the AAAI conference on artificial intelligence, vol. 34, pp. 57425749" href="/article/10.1007/s00521-024-09417-3#ref-CR15" id="ref-link-section-d590711142e8215">15</a>] and AbDMIL [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 13" title="Ilse M, Tomczak J, Welling M (2018) Attention-based deep multiple instance learning. In: International conference on machine learning, pp. 21272136. PMLR" href="/article/10.1007/s00521-024-09417-3#ref-CR13" id="ref-link-section-d590711142e8218">13</a>], respectively. Similarly, the ViT-IWRS approach attained 84.5% accuracy for the Tiger dataset, superior to the previous results of 83% by LBA [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 15" title="Shi X, Xing F, Xie Y, Zhang Z, Cui L, Yang L (2020) Loss-based attention for deep multiple instance learning. In: Proceedings of the AAAI conference on artificial intelligence, vol. 34, pp. 57425749" href="/article/10.1007/s00521-024-09417-3#ref-CR15" id="ref-link-section-d590711142e8222">15</a>]. In the case of the Elephant dataset, the proposed approach attained 87.4% accuracy.</p><p>For Musk1 and Musk2 datasets, the ViT-IWRS approach achieved 89.5% and 87.6% compared to the previous best performance of 88.6% and 87.3% accuracy, respectively. Overall, the performance of ViT-IWRS is superior to the counterpart attention-based techniques on all five benchmark datasets. The proposed ViT-IWRS is robust enough to ascertain the association among the instances. With the help of the RSN network, it can provide superior bag encoding.</p><p>The experimental results show that the prior assumption of instance relationship in the bag restricts the performance of AbDMIL and LBA. On the contrary, the proposed ViT-IWRS generates several bag representations without prior assumption of instance selection and simultaneously selects the informative vector through RSN. This ability generates a more effective vector representation for the bag and improves the models generalization ability.</p><h3 class="c-article__sub-heading" id="Sec23"><span class="c-article-section__title-number">5.2 </span>Comparison with benchmark techniques</h3><p>Performance comparison of ViT-IWRS with benchmark techniques is given in Table<a data-track="click" data-track-label="link" data-track-action="table anchor" href="/article/10.1007/s00521-024-09417-3#Tab2">2</a>. ViT-IWRS outperformed the performance of existing benchmark techniques on Elephant, Tiger, and Fox datasets. ViT-IWRS produced 62.5% accuracy for the Fox dataset compared to the highest 86.2% accuracy by MI-Net [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 41" title="Li G, Li C, Wu G, Ji D, Zhang H (2021) Multi-view attention-guided multiple instance detection network for interpretable breast cancer histopathological image diagnosis. IEEE Access 9:7967179684" href="/article/10.1007/s00521-024-09417-3#ref-CR41" id="ref-link-section-d590711142e8242">41</a>]. For the Elephant dataset, 87.4% accuracy outperformed the previous best accuracy of 62.1% accuracy of miFV [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 9" title="Wei X-S, Wu J, Zhou Z-H (2016) Scalable algorithms for multi-instance learning. IEEE Trans Neural Netw Learn Syst 28(4):975987" href="/article/10.1007/s00521-024-09417-3#ref-CR9" id="ref-link-section-d590711142e8245">9</a>]. Similarly, the ViT-IWRS produced 84.5% accuracy on the Tiger dataset and surpassed the previous best performance of 83.6% accuracy reported by MI-Net-RC [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 41" title="Li G, Li C, Wu G, Ji D, Zhang H (2021) Multi-view attention-guided multiple instance detection network for interpretable breast cancer histopathological image diagnosis. IEEE Access 9:7967179684" href="/article/10.1007/s00521-024-09417-3#ref-CR41" id="ref-link-section-d590711142e8248">41</a>].</p><p>In the case of Musk1 and Musk2 datasets, the ViT-IWRS produced comparable accuracy to several bag classification approaches. The Musk1 and Musk2 datasets are composed of molecular conformations with a small number of bags. It is usually difficult for neural networks to perform well as benchmark methods. Additionally, in the Musk1 and Musk2 datasets, molecular data follow a structure and can be represented using graphs; therefore, the graph representation-based techniques [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 16" title="Zhou Z-H, Sun Y-Y, Li Y-F (2009) Multi-instance learning by treating instances as non-IID samples. In: Proceedings of the 26th annual international conference on machine learning, pp. 12491256" href="/article/10.1007/s00521-024-09417-3#ref-CR16" id="ref-link-section-d590711142e8254">16</a>] are more suitable for these types of datasets. Thus, the performance of ViT-IWRS is limited in these datasets. However, in the case of image datasets, the ViT-IWRS performs considerably better than the benchmark approaches.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-5" data-title="Fig. 5"><figure><figcaption><b id="Fig5" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 5</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s00521-024-09417-3/figures/5" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs00521-024-09417-3/MediaObjects/521_2024_9417_Fig5_HTML.png?as=webp"><img aria-describedby="Fig5" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs00521-024-09417-3/MediaObjects/521_2024_9417_Fig5_HTML.png" alt="figure 5" loading="lazy" width="685" height="250"></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-5-desc"><p>The performance analysis of ViT-IWRS with SOTA attention-based MIL techniques, <b>a</b> shows the comparison on Musk1 and Musk2 datasets, while the performance comparison for image-related MIL dataset is given in (<b>b</b>)</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s00521-024-09417-3/figures/5" data-track-dest="link:Figure5 Full size image" aria-label="Full size image figure 5" rel="nofollow"><span>Full size image</span><svg width="16" height="16" focusable="false" role="img" aria-hidden="true" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#icon-eds-i-chevron-right-small"></use></svg></a></div></figure></div><div class="c-article-table" data-test="inline-table" data-container-section="table" id="table-2"><figure><figcaption class="c-article-table__figcaption"><b id="Tab2" data-test="table-caption">Table 2 The performance comparison of proposed ViT-IWRS with benchmark MIL techniques, the best accuracy is highlighted by boldface and italicized, while the second-best performance for each dataset is marked as simple boldface</b></figcaption><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="table-link" data-track="click" data-track-action="view table" data-track-label="button" rel="nofollow" href="/article/10.1007/s00521-024-09417-3/tables/2" aria-label="Full size table 2"><span>Full size table</span><svg width="16" height="16" focusable="false" role="img" aria-hidden="true" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#icon-eds-i-chevron-right-small"></use></svg></a></div></figure></div><h3 class="c-article__sub-heading" id="Sec24"><span class="c-article-section__title-number">5.3 </span>ViT-IWRS VS benchmark MIL techniques</h3><p>Benchmark MIL techniques such as mi-Net and MI-Net [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 41" title="Li G, Li C, Wu G, Ji D, Zhang H (2021) Multi-view attention-guided multiple instance detection network for interpretable breast cancer histopathological image diagnosis. IEEE Access 9:7967179684" href="/article/10.1007/s00521-024-09417-3#ref-CR41" id="ref-link-section-d590711142e9016">41</a>] adopt trainable pooling operations to generate vector representation for the bag. However, the proposed pooling operation considers the equal contribution of instances in the bag. Additionally, these techniques do not account for the instance relationship information in the pooling process. The bag encoding approaches such as miFV and miVLAD [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 56" title="Wilcoxon F (1992) Individual comparisons by ranking methods. In: Kotz S, Johnson NL (eds) Breakthroughs in statistics: methodology and distribution. Springer, Berlin, pp 196202" href="/article/10.1007/s00521-024-09417-3#ref-CR56" id="ref-link-section-d590711142e9019">56</a>] are based on dictionary learning techniques using the instance clustering process and incorporate all the instances of the bag in the encoding process. However, these techniques do not incorporate any instance weighting technique in the encoding process which may affect the performance of generated encoding. Likewise, Simple-MI [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 9" title="Wei X-S, Wu J, Zhou Z-H (2016) Scalable algorithms for multi-instance learning. IEEE Trans Neural Netw Learn Syst 28(4):975987" href="/article/10.1007/s00521-024-09417-3#ref-CR9" id="ref-link-section-d590711142e9022">9</a>] computes the instance-wise mean vector for the bag. In comparison with these algorithms, ViT-IWRS tackles the relationship assumption with instance weighing and bag representation selection process.</p><p>RSIS [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 29" title="Zhou Z-H, Zhang M-L (2007) Solving multi-instance problems with classifier ensemble based on constructive clustering. Knowl Inf Syst 11(2):155170" href="/article/10.1007/s00521-024-09417-3#ref-CR29" id="ref-link-section-d590711142e9028">29</a>] adopts a random subspace hard clustering approach to select a candidate instance from positive bags while the instances from negative bags are sampled randomly. The selected instances are classified using an ensemble learning technique in ambient space. However, the adopted instance selection process in RSIS results in a class imbalance problem. Similarly, CCE [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 30" title="Zhou Z-H, Xu J-M (2007) On the relation between multi-instance learning and semi-supervised learning. In: Proceedings of the 24th international conference on machine learning, pp. 11671174" href="/article/10.1007/s00521-024-09417-3#ref-CR30" id="ref-link-section-d590711142e9031">30</a>] groups training instances into <i>c</i> clusters and generates a <span class="mathjax-tex">\(c-\)</span>dimensional binary vector representation for the bag. The <i>i</i>th bits in the representation vector are set to one if corresponding bag instances are part of <i>i</i>th cluster. The proposed ViT-IWRS generates a robust bag representation vector by incorporating the information presented in all instances of the bag with different weights. Additionally, the generated bag representation vector using ViT-IWRS offers more information in the classification process than the classification of instances in ambient space or binary vector generated by RSIS [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 29" title="Zhou Z-H, Zhang M-L (2007) Solving multi-instance problems with classifier ensemble based on constructive clustering. Knowl Inf Syst 11(2):155170" href="/article/10.1007/s00521-024-09417-3#ref-CR29" id="ref-link-section-d590711142e9066">29</a>] and CCE [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 30" title="Zhou Z-H, Xu J-M (2007) On the relation between multi-instance learning and semi-supervised learning. In: Proceedings of the 24th international conference on machine learning, pp. 11671174" href="/article/10.1007/s00521-024-09417-3#ref-CR30" id="ref-link-section-d590711142e9069">30</a>].</p><p>Moreover, ESMIL [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 52" title="Hahn M (2020) Theoretical limitations of self-attention in neural sequence models. Trans Assoc Comput Linguist 8:156171" href="/article/10.1007/s00521-024-09417-3#ref-CR52" id="ref-link-section-d590711142e9075">52</a>] uses a ranking loss mechanism to assign a score to each instance in the bags. The proposed ranking loss function ensures that the highest-scoring instance in a positive bag receives a higher score than the highest-scoring instance in a negative bag. ESMIL distinguishes between positive and negative bags based on the highest-scoring instances from the bag of each category, and this process helps to maximize the AUC score. However, ESMIL ignores the contribution of other instances in the bag classification process. Additionally, the adopted training process lacks the ability to learn an efficient score function for bag classification. This property is essential for bag-level classification, and the selection of a suboptimal scoring function affects the models generalization ability. In contrast, ViT-IWRS assigns higher weights to the instances in the bag, which induces bag labels and generates a robust bag representation vector by combining the instance relationship and weighted impact of the instances. This ability helps to learn an efficient scoring function for bag-level classification.</p><p>Similarly, Mi-Graph [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 16" title="Zhou Z-H, Sun Y-Y, Li Y-F (2009) Multi-instance learning by treating instances as non-IID samples. In: Proceedings of the 26th annual international conference on machine learning, pp. 12491256" href="/article/10.1007/s00521-024-09417-3#ref-CR16" id="ref-link-section-d590711142e9081">16</a>] assumes instances of the bag have a relationship and adopts a graph kernel learning technique to transform a given bag into an undirected weighted graph. The nodes in the generated graph represent instances of the bag, and if the distance between the two nodes is smaller than a preset threshold, then a weighted edge is established between the nodes. The weight of the edge expresses the affinity of the two nodes. This approach is useful where details of the bag structure play an essential role in the bag classification process. In contrast, ViT-IWRS models instance dependencies through the MHSA process and simultaneously incorporates bag-wise instance relationship assumption in the classification process.</p><h3 class="c-article__sub-heading" id="Sec25"><span class="c-article-section__title-number">5.4 </span>Performance comparison on MIL-MNIST dataset</h3><div class="c-article-table" data-test="inline-table" data-container-section="table" id="table-3"><figure><figcaption class="c-article-table__figcaption"><b id="Tab3" data-test="table-caption">Table 3 The performance comparison of ViT-IWRS with SOTA attention techniques on MIL-MNIST dataset. The best accuracy is highlighted in boldface and italicized, while the second-best performance for each dataset is marked in simple boldface</b></figcaption><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="table-link" data-track="click" data-track-action="view table" data-track-label="button" rel="nofollow" href="/article/10.1007/s00521-024-09417-3/tables/3" aria-label="Full size table 3"><span>Full size table</span><svg width="16" height="16" focusable="false" role="img" aria-hidden="true" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#icon-eds-i-chevron-right-small"></use></svg></a></div></figure></div><p>For the multi-class classification problem, the MIL-MNIST dataset is generated. We used a bag generation approach similar to the one used in LBA [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 15" title="Shi X, Xing F, Xie Y, Zhang Z, Cui L, Yang L (2020) Loss-based attention for deep multiple instance learning. In: Proceedings of the AAAI conference on artificial intelligence, vol. 34, pp. 57425749" href="/article/10.1007/s00521-024-09417-3#ref-CR15" id="ref-link-section-d590711142e9552">15</a>] and AbDMIL [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 13" title="Ilse M, Tomczak J, Welling M (2018) Attention-based deep multiple instance learning. In: International conference on machine learning, pp. 21272136. PMLR" href="/article/10.1007/s00521-024-09417-3#ref-CR13" id="ref-link-section-d590711142e9555">13</a>]. The performance of the ViT-IWRS is compared with SOTA attention-based approaches, including LBA [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 15" title="Shi X, Xing F, Xie Y, Zhang Z, Cui L, Yang L (2020) Loss-based attention for deep multiple instance learning. In: Proceedings of the AAAI conference on artificial intelligence, vol. 34, pp. 57425749" href="/article/10.1007/s00521-024-09417-3#ref-CR15" id="ref-link-section-d590711142e9558">15</a>], AbDMIL, and Gated-AbDMIL [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 13" title="Ilse M, Tomczak J, Welling M (2018) Attention-based deep multiple instance learning. In: International conference on machine learning, pp. 21272136. PMLR" href="/article/10.1007/s00521-024-09417-3#ref-CR13" id="ref-link-section-d590711142e9561">13</a>]. The two approaches, AbDMIL and Gated-AbDMIL, were extended with Softmax output to support multi-class classification problems. The bag classification is also performed for max-instance, mean-instance, max-instance embedding, and mean-instance embedding. The max-embedding and mean-embedding are computed by the output of the previously discussed embedding network. The bag classification results in Table<a data-track="click" data-track-label="link" data-track-action="table anchor" href="/article/10.1007/s00521-024-09417-3#Tab3">3</a> show that the proposed ViT-IWRS produces better performance in most cases, especially in the case of large training sets of 150, 200, 300, and 400 bags, respectively.</p><h3 class="c-article__sub-heading" id="Sec26"><span class="c-article-section__title-number">5.5 </span>Comparison on MIL-based CIFAR-10 dataset</h3><p>To better evaluate the performance of the proposed ViT-IWRS, a larger and more challenging dataset is created based on CIFAR-10. The performance of ViT-IWRS is compared with SOTA methods, including LBA [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 15" title="Shi X, Xing F, Xie Y, Zhang Z, Cui L, Yang L (2020) Loss-based attention for deep multiple instance learning. In: Proceedings of the AAAI conference on artificial intelligence, vol. 34, pp. 57425749" href="/article/10.1007/s00521-024-09417-3#ref-CR15" id="ref-link-section-d590711142e9575">15</a>], AbDMIL [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 13" title="Ilse M, Tomczak J, Welling M (2018) Attention-based deep multiple instance learning. In: International conference on machine learning, pp. 21272136. PMLR" href="/article/10.1007/s00521-024-09417-3#ref-CR13" id="ref-link-section-d590711142e9578">13</a>], and Gated-AbDMIL [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 13" title="Ilse M, Tomczak J, Welling M (2018) Attention-based deep multiple instance learning. In: International conference on machine learning, pp. 21272136. PMLR" href="/article/10.1007/s00521-024-09417-3#ref-CR13" id="ref-link-section-d590711142e9581">13</a>], previously used for MIL-MNIST. The experiments are conducted for 500, and 5000 randomly generated training bags. Additionally, the experimental results of this dataset are presented in Table<a data-track="click" data-track-label="link" data-track-action="table anchor" href="/article/10.1007/s00521-024-09417-3#Tab4">4</a>. The results show that ViT-IWRS surpasses the previous best performance of LBA and produces 3.1% and 1.5% improved performance on 500 training bags and 5000 bags, respectively. The experimental results indicate that the proposed ViT-IWRS is robust in determining the dependencies among the bag instances in complex and challenging situations involving large datasets.</p><div class="c-article-table" data-test="inline-table" data-container-section="table" id="table-4"><figure><figcaption class="c-article-table__figcaption"><b id="Tab4" data-test="table-caption">Table 4 The experimental results on MIL-BASED CIFAR-10 dataset. The best accuracy is highlighted in boldface and italicized, while the second-best performance for each dataset is marked in simple boldface</b></figcaption><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="table-link" data-track="click" data-track-action="view table" data-track-label="button" rel="nofollow" href="/article/10.1007/s00521-024-09417-3/tables/4" aria-label="Full size table 4"><span>Full size table</span><svg width="16" height="16" focusable="false" role="img" aria-hidden="true" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#icon-eds-i-chevron-right-small"></use></svg></a></div></figure></div><h3 class="c-article__sub-heading" id="Sec27"><span class="c-article-section__title-number">5.6 </span>Performance comparison on colon cancer dataset</h3><p>We have evaluated the performance of ViT-IWRS algorithms on a real-life colon cancer dataset with weak labeling. Our comparison includes state-of-the-art techniques such as AbDMIL [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 13" title="Ilse M, Tomczak J, Welling M (2018) Attention-based deep multiple instance learning. In: International conference on machine learning, pp. 21272136. PMLR" href="/article/10.1007/s00521-024-09417-3#ref-CR13" id="ref-link-section-d590711142e9746">13</a>], Gated-AbDMIL [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 13" title="Ilse M, Tomczak J, Welling M (2018) Attention-based deep multiple instance learning. In: International conference on machine learning, pp. 21272136. PMLR" href="/article/10.1007/s00521-024-09417-3#ref-CR13" id="ref-link-section-d590711142e9749">13</a>], LBA [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 15" title="Shi X, Xing F, Xie Y, Zhang Z, Cui L, Yang L (2020) Loss-based attention for deep multiple instance learning. In: Proceedings of the AAAI conference on artificial intelligence, vol. 34, pp. 57425749" href="/article/10.1007/s00521-024-09417-3#ref-CR15" id="ref-link-section-d590711142e9752">15</a>], and ESMIL [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 52" title="Hahn M (2020) Theoretical limitations of self-attention in neural sequence models. Trans Assoc Comput Linguist 8:156171" href="/article/10.1007/s00521-024-09417-3#ref-CR52" id="ref-link-section-d590711142e9755">52</a>], as well as instance-level and embedding level max and mean pooling operations. The results show the effectiveness of ViT-IWRS on this dataset. Based on the results shown in Fig.<a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s00521-024-09417-3#Fig6">6</a>, it is evident that the proposed ViT-IWRS outperforms other state-of-the-art techniques. ViT-IWRS obtained 92.4% bag-level classification accuracy compared to the previous best of 90.3% by LBA [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 15" title="Shi X, Xing F, Xie Y, Zhang Z, Cui L, Yang L (2020) Loss-based attention for deep multiple instance learning. In: Proceedings of the AAAI conference on artificial intelligence, vol. 34, pp. 57425749" href="/article/10.1007/s00521-024-09417-3#ref-CR15" id="ref-link-section-d590711142e9762">15</a>]. ViT-IWRS achieves this by effectively managing Global and Local information about the bag. Furthermore, the representation selection process ensures that only the necessary bag representation vector is used in the classification process.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-6" data-title="Fig. 6"><figure><figcaption><b id="Fig6" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 6</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s00521-024-09417-3/figures/6" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs00521-024-09417-3/MediaObjects/521_2024_9417_Fig6_HTML.png?as=webp"><img aria-describedby="Fig6" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs00521-024-09417-3/MediaObjects/521_2024_9417_Fig6_HTML.png" alt="figure 6" loading="lazy" width="685" height="419"></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-6-desc"><p>The performance analysis of ViT-IWRS with SOTA attention-based MIL techniques on Colon Cancer histopathology dataset</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s00521-024-09417-3/figures/6" data-track-dest="link:Figure6 Full size image" aria-label="Full size image figure 6" rel="nofollow"><span>Full size image</span><svg width="16" height="16" focusable="false" role="img" aria-hidden="true" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#icon-eds-i-chevron-right-small"></use></svg></a></div></figure></div><h3 class="c-article__sub-heading" id="Sec28"><span class="c-article-section__title-number">5.7 </span>Statistical validation</h3><p>In this work, we evaluate the statistical significance of ViT-IWRS on MIL benchmark datasets using the Wilcoxon-signed rank test with a 95% confidence interval [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 57" title="Conover WJ (1999) Practical nonparametric statistics, vol 350. Wiley, New York" href="/article/10.1007/s00521-024-09417-3#ref-CR57" id="ref-link-section-d590711142e9790">57</a>, <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 58" title="Demar J (2006) Statistical comparisons of classifiers over multiple data sets. J Mach Learn Res 7:130" href="/article/10.1007/s00521-024-09417-3#ref-CR58" id="ref-link-section-d590711142e9793">58</a>]. Using statistical analysis, this test determines if there is a substantial difference between two related groups. This technique is preferable when the normality or equal variance assumptions are violated. These methods are tested using the same train-test distribution as ViT-IWRS.</p><p>Table<a data-track="click" data-track-label="link" data-track-action="table anchor" href="/article/10.1007/s00521-024-09417-3#Tab5">5</a> shows the <span class="mathjax-tex">\(p-\)</span>values for the Musk1 and Musk2 datasets. A <i>p</i>-value below 0.05 indicates that ViT-IWRS is statistically better than LBA [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 15" title="Shi X, Xing F, Xie Y, Zhang Z, Cui L, Yang L (2020) Loss-based attention for deep multiple instance learning. In: Proceedings of the AAAI conference on artificial intelligence, vol. 34, pp. 57425749" href="/article/10.1007/s00521-024-09417-3#ref-CR15" id="ref-link-section-d590711142e9827">15</a>], AbDMIL [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 13" title="Ilse M, Tomczak J, Welling M (2018) Attention-based deep multiple instance learning. In: International conference on machine learning, pp. 21272136. PMLR" href="/article/10.1007/s00521-024-09417-3#ref-CR13" id="ref-link-section-d590711142e9830">13</a>], Gated-AbDMIL [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 13" title="Ilse M, Tomczak J, Welling M (2018) Attention-based deep multiple instance learning. In: International conference on machine learning, pp. 21272136. PMLR" href="/article/10.1007/s00521-024-09417-3#ref-CR13" id="ref-link-section-d590711142e9834">13</a>], and ESMIL [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 52" title="Hahn M (2020) Theoretical limitations of self-attention in neural sequence models. Trans Assoc Comput Linguist 8:156171" href="/article/10.1007/s00521-024-09417-3#ref-CR52" id="ref-link-section-d590711142e9837">52</a>]. Likewise, in the case of the Musk2 dataset, ViT-IWRS is statistically significant compared to AbDMIL and Gated-AbDMIL. Table<a data-track="click" data-track-label="link" data-track-action="table anchor" href="/article/10.1007/s00521-024-09417-3#Tab6">6</a> shows the <i>p</i>-values for the Elephant, Tiger, and Fox datasets. The proposed ViT-IWRS is statistically significant for the Fox dataset compared to LBA [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 15" title="Shi X, Xing F, Xie Y, Zhang Z, Cui L, Yang L (2020) Loss-based attention for deep multiple instance learning. In: Proceedings of the AAAI conference on artificial intelligence, vol. 34, pp. 57425749" href="/article/10.1007/s00521-024-09417-3#ref-CR15" id="ref-link-section-d590711142e9846">15</a>], AbDMIL [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 13" title="Ilse M, Tomczak J, Welling M (2018) Attention-based deep multiple instance learning. In: International conference on machine learning, pp. 21272136. PMLR" href="/article/10.1007/s00521-024-09417-3#ref-CR13" id="ref-link-section-d590711142e9849">13</a>], Gated-AbDMIL [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 13" title="Ilse M, Tomczak J, Welling M (2018) Attention-based deep multiple instance learning. In: International conference on machine learning, pp. 21272136. PMLR" href="/article/10.1007/s00521-024-09417-3#ref-CR13" id="ref-link-section-d590711142e9853">13</a>], and ESMIL [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 52" title="Hahn M (2020) Theoretical limitations of self-attention in neural sequence models. Trans Assoc Comput Linguist 8:156171" href="/article/10.1007/s00521-024-09417-3#ref-CR52" id="ref-link-section-d590711142e9856">52</a>]. Similarly, for the Tiger and Elephant datasets, the ViT-IWRS is statistically better than AbDMIL [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 13" title="Ilse M, Tomczak J, Welling M (2018) Attention-based deep multiple instance learning. In: International conference on machine learning, pp. 21272136. PMLR" href="/article/10.1007/s00521-024-09417-3#ref-CR13" id="ref-link-section-d590711142e9859">13</a>], Gated-AbDMIL [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 13" title="Ilse M, Tomczak J, Welling M (2018) Attention-based deep multiple instance learning. In: International conference on machine learning, pp. 21272136. PMLR" href="/article/10.1007/s00521-024-09417-3#ref-CR13" id="ref-link-section-d590711142e9862">13</a>], and ESMIL [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 52" title="Hahn M (2020) Theoretical limitations of self-attention in neural sequence models. Trans Assoc Comput Linguist 8:156171" href="/article/10.1007/s00521-024-09417-3#ref-CR52" id="ref-link-section-d590711142e9865">52</a>]. The proposed ViT-IWRS showed statistical significance in comparison with AbDMI [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 13" title="Ilse M, Tomczak J, Welling M (2018) Attention-based deep multiple instance learning. In: International conference on machine learning, pp. 21272136. PMLR" href="/article/10.1007/s00521-024-09417-3#ref-CR13" id="ref-link-section-d590711142e9868">13</a>] and Gated-AbDMIL [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 13" title="Ilse M, Tomczak J, Welling M (2018) Attention-based deep multiple instance learning. In: International conference on machine learning, pp. 21272136. PMLR" href="/article/10.1007/s00521-024-09417-3#ref-CR13" id="ref-link-section-d590711142e9872">13</a>]. Similarly, ViT-IWRS exhibited statistical significance over ESMIL [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 52" title="Hahn M (2020) Theoretical limitations of self-attention in neural sequence models. Trans Assoc Comput Linguist 8:156171" href="/article/10.1007/s00521-024-09417-3#ref-CR52" id="ref-link-section-d590711142e9875">52</a>] and LBA [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 15" title="Shi X, Xing F, Xie Y, Zhang Z, Cui L, Yang L (2020) Loss-based attention for deep multiple instance learning. In: Proceedings of the AAAI conference on artificial intelligence, vol. 34, pp. 57425749" href="/article/10.1007/s00521-024-09417-3#ref-CR15" id="ref-link-section-d590711142e9878">15</a>] on four and two datasets, respectively.</p><p>We also used the Friedman rank test [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 59" title="Friedman M (1937) The use of ranks to avoid the assumption of normality implicit in the analysis of variance. J Am Stat Assoc 32(200):675701" href="/article/10.1007/s00521-024-09417-3#ref-CR59" id="ref-link-section-d590711142e9884">59</a>, <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 60" title="LeCun Y, Bottou L, Bengio Y, Haffner P (1998) Gradient-based learning applied to document recognition. Proc IEEE 86(11):22782324" href="/article/10.1007/s00521-024-09417-3#ref-CR60" id="ref-link-section-d590711142e9887">60</a>] to assess the overall performance of various algorithms and compare their performance across various datasets. This statistical test is designed to assess whether there are statistically significant differences among the means of three or more related groups. It involves ranking the data within each group and assigning a rank to each algorithm. In this ranking, the best algorithm is assigned the lowest rank, while the algorithm with the worst performance is assigned the highest rank. The rankings of the proposed and compared methods are determined with 95% significance and a critical distance diagram is plotted to illustrate the results in Fig.<a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s00521-024-09417-3#Fig7">7</a>. As shown in Fig.<a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s00521-024-09417-3#Fig7">7</a>, the proposed ViT-IWRS achieved the lowest rank (most important) among all compared techniques. This indicates that the performance of ViT-IWRS is superior to the compared methods.</p><div class="c-article-table" data-test="inline-table" data-container-section="table" id="table-5"><figure><figcaption class="c-article-table__figcaption"><b id="Tab5" data-test="table-caption">Table 5 The obtained <span class="mathjax-tex">\(p-\)</span>values of Wilcoxon-signed ranked test for Musk1 and Musk2 datasets</b></figcaption><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="table-link" data-track="click" data-track-action="view table" data-track-label="button" rel="nofollow" href="/article/10.1007/s00521-024-09417-3/tables/5" aria-label="Full size table 5"><span>Full size table</span><svg width="16" height="16" focusable="false" role="img" aria-hidden="true" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#icon-eds-i-chevron-right-small"></use></svg></a></div></figure></div><div class="c-article-table" data-test="inline-table" data-container-section="table" id="table-6"><figure><figcaption class="c-article-table__figcaption"><b id="Tab6" data-test="table-caption">Table 6 The obtained <span class="mathjax-tex">\(p-\)</span>values of Wilcoxon-signed ranked test for Elephant, Tiger, and Fox datasets</b></figcaption><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="table-link" data-track="click" data-track-action="view table" data-track-label="button" rel="nofollow" href="/article/10.1007/s00521-024-09417-3/tables/6" aria-label="Full size table 6"><span>Full size table</span><svg width="16" height="16" focusable="false" role="img" aria-hidden="true" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#icon-eds-i-chevron-right-small"></use></svg></a></div></figure></div><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-7" data-title="Fig. 7"><figure><figcaption><b id="Fig7" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 7</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s00521-024-09417-3/figures/7" rel="nofollow"><picture><img aria-describedby="Fig7" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs00521-024-09417-3/MediaObjects/521_2024_9417_Fig7_HTML.png" alt="figure 7" loading="lazy" width="685" height="228"></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-7-desc"><p>Critical distance diagram comparing the proposed ViT-IWRS against various MIL algorithms with a 95% confidence interval. The diagrams top line shows the algorithms average rank, with the most important rank at the left and the least significant rank at the right. The two algorithms are not considerably different if they are not connected by bold line</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s00521-024-09417-3/figures/7" data-track-dest="link:Figure7 Full size image" aria-label="Full size image figure 7" rel="nofollow"><span>Full size image</span><svg width="16" height="16" focusable="false" role="img" aria-hidden="true" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#icon-eds-i-chevron-right-small"></use></svg></a></div></figure></div><h3 class="c-article__sub-heading" id="Sec29"><span class="c-article-section__title-number">5.8 </span>Time efficiency comparison</h3><p>In this paper, the time efficiency of the proposed ViT-IWRS is empirically evaluated on five benchmark MIL datasets. The time costs of training do not include the time for data preparation. The proposed ViT-IWRS is compared with state-of-the-art counterparts, including LBA [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 15" title="Shi X, Xing F, Xie Y, Zhang Z, Cui L, Yang L (2020) Loss-based attention for deep multiple instance learning. In: Proceedings of the AAAI conference on artificial intelligence, vol. 34, pp. 57425749" href="/article/10.1007/s00521-024-09417-3#ref-CR15" id="ref-link-section-d590711142e10568">15</a>], AbDMIL [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 13" title="Ilse M, Tomczak J, Welling M (2018) Attention-based deep multiple instance learning. In: International conference on machine learning, pp. 21272136. PMLR" href="/article/10.1007/s00521-024-09417-3#ref-CR13" id="ref-link-section-d590711142e10571">13</a>], Gated-AbDMIL [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 13" title="Ilse M, Tomczak J, Welling M (2018) Attention-based deep multiple instance learning. In: International conference on machine learning, pp. 21272136. PMLR" href="/article/10.1007/s00521-024-09417-3#ref-CR13" id="ref-link-section-d590711142e10574">13</a>], and ESMIL [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 52" title="Hahn M (2020) Theoretical limitations of self-attention in neural sequence models. Trans Assoc Comput Linguist 8:156171" href="/article/10.1007/s00521-024-09417-3#ref-CR52" id="ref-link-section-d590711142e10577">52</a>]. The algorithms are trained for 100 Epochs, and the average training time in the log scale is shown in Fig.<a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s00521-024-09417-3#Fig8">8</a>. All the experiments are conducted on a machine with a Core i7 3.10 GHz CPU, RTX 3060 GPU, and 16GB of main memory.</p><p>Compared to AbDMIL, Gated-AbDMIL, and LBA, the training process for ViT-IWRS is more time-consuming. This is because ViT uses a self-attention mechanism with quadratic complexity, making it more computationally expensive than traditional attention algorithms. Notably, ViT-IWRS requires less training time than ESMIL, which involves a pairwise loss strategy, necessitating the adjustment of network weights across all pairs of positive and negative bags.</p><p>However, ViT-IWRS outperforms state-of-the-art algorithms on all types of datasets in terms of bag classification performance. This outcome underscores the proposed approachs effectiveness and ability to surpass the capabilities of current state-of-the-art techniques.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-8" data-title="Fig. 8"><figure><figcaption><b id="Fig8" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 8</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s00521-024-09417-3/figures/8" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs00521-024-09417-3/MediaObjects/521_2024_9417_Fig8_HTML.png?as=webp"><img aria-describedby="Fig8" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs00521-024-09417-3/MediaObjects/521_2024_9417_Fig8_HTML.png" alt="figure 8" loading="lazy" width="685" height="726"></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-8-desc"><p>The time efficiency analysis of ViT-IWRS with SOTA attention-based MIL techniques. The time comparisons on the Elephant, Tiger, and Fox datasets are shown in (<b>a</b><b>c</b>). The time comparison of the Musk1 and Musk2 datasets is illustrated in (<b>d</b>) and (<b>e</b>), respectively</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s00521-024-09417-3/figures/8" data-track-dest="link:Figure8 Full size image" aria-label="Full size image figure 8" rel="nofollow"><span>Full size image</span><svg width="16" height="16" focusable="false" role="img" aria-hidden="true" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#icon-eds-i-chevron-right-small"></use></svg></a></div></figure></div><h3 class="c-article__sub-heading" id="Sec30"><span class="c-article-section__title-number">5.9 </span>Parameter sensitivity analysis</h3><p>This section discusses the impact of different hyperparameters related to ViT-IWRS on performance. There are several parameters related to ViT-IWRS, such as the size of the RSN, the number of blocks, and the number of heads in ViT blocks. These parameters are tuned one at a time. While tuning one parameter, the other parameters are kept fixed. Initially, the number of transformer encoder blocks and layers in RSN is set to two, and the number of heads in MHSA is fixed to four, respectively. The details of the hyperparameters related to model training are given in Table <a data-track="click" data-track-label="link" data-track-action="table anchor" href="/article/10.1007/s00521-024-09417-3#Tab7">7</a>. The details of the embedding network are also presented in this section.</p><div class="c-article-table" data-test="inline-table" data-container-section="table" id="table-7"><figure><figcaption class="c-article-table__figcaption"><b id="Tab7" data-test="table-caption">Table 7 The details of hyperparameters used in the training of ViT-IWRS</b></figcaption><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="table-link" data-track="click" data-track-action="view table" data-track-label="button" rel="nofollow" href="/article/10.1007/s00521-024-09417-3/tables/7" aria-label="Full size table 7"><span>Full size table</span><svg width="16" height="16" focusable="false" role="img" aria-hidden="true" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#icon-eds-i-chevron-right-small"></use></svg></a></div></figure></div><h4 class="c-article__sub-heading c-article__sub-heading--small" id="Sec31"><span class="c-article-section__title-number">5.9.1 </span>Embedding network</h4><p>The proposed ViT-IWRS first transforms the bag instance to a latent representation using an embedding network. We adopted a similar setting for embedding networks as previously used in AbDMIL [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 13" title="Ilse M, Tomczak J, Welling M (2018) Attention-based deep multiple instance learning. In: International conference on machine learning, pp. 21272136. PMLR" href="/article/10.1007/s00521-024-09417-3#ref-CR13" id="ref-link-section-d590711142e10893">13</a>] and LBA [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 15" title="Shi X, Xing F, Xie Y, Zhang Z, Cui L, Yang L (2020) Loss-based attention for deep multiple instance learning. In: Proceedings of the AAAI conference on artificial intelligence, vol. 34, pp. 57425749" href="/article/10.1007/s00521-024-09417-3#ref-CR15" id="ref-link-section-d590711142e10896">15</a>]. The embedding network for benchmark datasets mainly consists of fully connected layers. In contrast, the MIL-MNIST and MIL-based CIFAR-10 datasets network comprises convolution layers with other related operations based on the LeNet5 architecture [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 61" title="Waqas M, Khan Z, Ahmed SU, Raza A (2023) MIL-Mixer: a robust bag encoding strategy for Multiple Instance Learning (mil) using MLP-Mixer. In 2023 18th IEEE International Conference on Emerging Technologies (ICET) 2226" href="/article/10.1007/s00521-024-09417-3#ref-CR61" id="ref-link-section-d590711142e10899">61</a>]. The details of the networks for the benchmark dataset and MIL-MNIST dataset are given in Table<a data-track="click" data-track-label="link" data-track-action="table anchor" href="/article/10.1007/s00521-024-09417-3#Tab8">8</a>.</p><div class="c-article-table" data-test="inline-table" data-container-section="table" id="table-8"><figure><figcaption class="c-article-table__figcaption"><b id="Tab8" data-test="table-caption">Table 8 The details of embedding network for benchmark and MIL-MNIST datasets. The parameters of convolution layers are constituted as Convolution(a,b,c,d), where a, b, c, and d represent kernel size, stride, padding and the number of kernels, respectively</b></figcaption><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="table-link" data-track="click" data-track-action="view table" data-track-label="button" rel="nofollow" href="/article/10.1007/s00521-024-09417-3/tables/8" aria-label="Full size table 8"><span>Full size table</span><svg width="16" height="16" focusable="false" role="img" aria-hidden="true" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#icon-eds-i-chevron-right-small"></use></svg></a></div></figure></div><h4 class="c-article__sub-heading c-article__sub-heading--small" id="Sec32"><span class="c-article-section__title-number">5.9.2 </span>Layers in representation selection subnetworks (RSN)</h4><p>This subnetwork comprises one or more fully connected layers, whereas the networks last layer consists of a single output neuron. The network learns a nonlinear representation selection function using a continuous output vector during training and generates a discretized one-hot vector in the testing. The layers in this subnetwork depend on the dataset representation diversity. The initial RSN comprises a fully connected layer with ReLU activation and dropout operation. Later, the layers to RSN are added with Tanh(:) followed by the dropout operation. The experiments show that two subnetwork layers are preferred for Musk1, Elephant, and Tiger datasets. Whereas, for Musk2 and Fox datasets, tree layer RSN is preferred. However, increasing the number of layers can result in overfitting. Furthermore, the number of layers for the MIL-MNIST, MIL-BASED CIFAR-10, and Colon Cancer datasets is set to one throughout the experiments. The detailed analysis of RSN size is given in Table in<a data-track="click" data-track-label="link" data-track-action="table anchor" href="/article/10.1007/s00521-024-09417-3#Tab9">9</a>.</p><div class="c-article-table" data-test="inline-table" data-container-section="table" id="table-9"><figure><figcaption class="c-article-table__figcaption"><b id="Tab9" data-test="table-caption">Table 9 Analysis of layers in RSN</b></figcaption><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="table-link" data-track="click" data-track-action="view table" data-track-label="button" rel="nofollow" href="/article/10.1007/s00521-024-09417-3/tables/9" aria-label="Full size table 9"><span>Full size table</span><svg width="16" height="16" focusable="false" role="img" aria-hidden="true" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#icon-eds-i-chevron-right-small"></use></svg></a></div></figure></div><h4 class="c-article__sub-heading c-article__sub-heading--small" id="Sec33"><span class="c-article-section__title-number">5.9.3 </span>Analysis of term <span class="mathjax-tex">\(\lambda\)</span> in loss function</h4><p>The loss function presented in Sect.<a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s00521-024-09417-3#Sec12">3.7</a> comprises <i>L</i>1 and <i>L</i>2, where <span class="mathjax-tex">\(\lambda\)</span> is a hyperparameter. The value of <span class="mathjax-tex">\(\lambda\)</span> plays a significant role in the model performance and interpretation. As discussed previously, the <i>L</i>1 term in the loss function can be decreased to a small value even when only one instance shares the label with the bag; when <span class="mathjax-tex">\(\lambda = 0\)</span>, the <i>L</i>2 term is removed from the objective, the model only focuses on the bag loss resulting in a low instance recall and may negatively affect the classification performance. We evaluated the impact of <span class="mathjax-tex">\(\lambda\)</span> on MIL-MNIST datasets of 50 training and 1000 testing bags, respectively. Figure<a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s00521-024-09417-3#Fig9">9</a> shows the performance of ViT-IWRS with <span class="mathjax-tex">\(\lambda \in \left[ 0,10^{-3}, 10^{-2}, 10^{-1}, 1, 1,10,\right]\)</span>. The experiments demonstrate the effectiveness of <span class="mathjax-tex">\(\lambda\)</span> in the loss function. The positive value of <span class="mathjax-tex">\(\lambda\)</span> between 1 and 10 improves the instances recall and bag classification performance.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-9" data-title="Fig. 9"><figure><figcaption><b id="Fig9" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 9</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s00521-024-09417-3/figures/9" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs00521-024-09417-3/MediaObjects/521_2024_9417_Fig9_HTML.png?as=webp"><img aria-describedby="Fig9" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs00521-024-09417-3/MediaObjects/521_2024_9417_Fig9_HTML.png" alt="figure 9" loading="lazy" width="685" height="442"></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-9-desc"><p>The analysis of the term <span class="mathjax-tex">\(\lambda\)</span> in loss function</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s00521-024-09417-3/figures/9" data-track-dest="link:Figure9 Full size image" aria-label="Full size image figure 9" rel="nofollow"><span>Full size image</span><svg width="16" height="16" focusable="false" role="img" aria-hidden="true" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#icon-eds-i-chevron-right-small"></use></svg></a></div></figure></div><h4 class="c-article__sub-heading c-article__sub-heading--small" id="Sec34"><span class="c-article-section__title-number">5.9.4 </span>Analysis for ViT depth and attention heads</h4><p>The ViT depth and the number of attention heads are the essential parameters in the proposed approach. First, we fixed the number of attention heads to four and the impact of ViT depth. Later, the best-performing depth is used to analyze the influence of attention heads. The experiments show that a depth of 3 is preferred for the Musk1 and Musk2 datasets, respectively, while the number of heads from 2 and 4 can produce better performance. This is due to the nature of the datasets. Additionally, where the structure information of the instances is important in addition to the instance relationship, adding ViT blocks and increasing the number of heads does not improve performance. For the fox, tiger, and Elephant datasets, 3, 2 and 3 blocks and 4 heads tend to perform well, respectively. It shows that these instances inside these datasets are highly related, and existing SOTA attention-based approaches do not consider this relation. The analysis of depth is shown in Fig.<a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s00521-024-09417-3#Fig10">10</a>, and the analysis of the number of heads in MHSA is illustrated in Fig.<a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s00521-024-09417-3#Fig11">11</a>, respectively. Furthermore, for the MIL-MNIST dataset, the depth is set to 1 and the number of heads is set to 4 throughout the experiments.</p><div class="c-article-table" data-test="inline-table" data-container-section="table" id="table-10"><figure><figcaption class="c-article-table__figcaption"><b id="Tab10" data-test="table-caption">Table 10 Details of ablation study, the performance is presented in classification accuracy</b></figcaption><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="table-link" data-track="click" data-track-action="view table" data-track-label="button" rel="nofollow" href="/article/10.1007/s00521-024-09417-3/tables/10" aria-label="Full size table 10"><span>Full size table</span><svg width="16" height="16" focusable="false" role="img" aria-hidden="true" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#icon-eds-i-chevron-right-small"></use></svg></a></div></figure></div><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-10" data-title="Fig. 10"><figure><figcaption><b id="Fig10" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 10</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s00521-024-09417-3/figures/10" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs00521-024-09417-3/MediaObjects/521_2024_9417_Fig10_HTML.png?as=webp"><img aria-describedby="Fig10" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs00521-024-09417-3/MediaObjects/521_2024_9417_Fig10_HTML.png" alt="figure 10" loading="lazy" width="685" height="286"></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-10-desc"><p>The analysis of transformer depth. The depth analysis for Musk1, Musk2, Elephant, Fox, and Tiger datasets is illustrated from (<b>a</b><b>e</b>), respectively</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s00521-024-09417-3/figures/10" data-track-dest="link:Figure10 Full size image" aria-label="Full size image figure 10" rel="nofollow"><span>Full size image</span><svg width="16" height="16" focusable="false" role="img" aria-hidden="true" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#icon-eds-i-chevron-right-small"></use></svg></a></div></figure></div><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-11" data-title="Fig. 11"><figure><figcaption><b id="Fig11" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 11</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s00521-024-09417-3/figures/11" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs00521-024-09417-3/MediaObjects/521_2024_9417_Fig11_HTML.png?as=webp"><img aria-describedby="Fig11" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs00521-024-09417-3/MediaObjects/521_2024_9417_Fig11_HTML.png" alt="figure 11" loading="lazy" width="685" height="300"></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-11-desc"><p>The analysis of the number of MHSA heads in transformer encoder. The analysis of attention heads for Musk1, Musk2, Elephant, Fox, and Tiger datasets is given from (<b>a</b><b>e</b>), respectively</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s00521-024-09417-3/figures/11" data-track-dest="link:Figure11 Full size image" aria-label="Full size image figure 11" rel="nofollow"><span>Full size image</span><svg width="16" height="16" focusable="false" role="img" aria-hidden="true" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#icon-eds-i-chevron-right-small"></use></svg></a></div></figure></div><h3 class="c-article__sub-heading" id="Sec35"><span class="c-article-section__title-number">5.10 </span>Ablation study</h3><p>The proposed ViT-IWRS consists of two essential processing blocks: the transformer encoding and RSN blocks. These blocks are shown in Fig.<a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s00521-024-09417-3#Fig3">3</a>a and d, respectively. The contribution of these two blocks to overall model performance is validated in the section. The performance of these two blocks is observed on the Musk1 dataset for binary classification and the MIL-MNIST dataset for multi-class classification problems. Additionally, the experiments on the MIL-MNIST dataset are performed 30 times using a training set of 50 bags and a test set of 1000 bags, and the average performance is presented.</p><h4 class="c-article__sub-heading c-article__sub-heading--small" id="Sec36"><span class="c-article-section__title-number">5.10.1 </span>Effect of RSN block</h4><p>In order to verify the impact of RSN, we replace this block with a simple average operation that computes the feature-wise average of the representation matrix <span class="mathjax-tex">\({{\mathcal {R}}}\)</span>. Later, the averaged vector is used for classification. The experimental results in Table <a data-track="click" data-track-label="link" data-track-action="table anchor" href="/article/10.1007/s00521-024-09417-3#Tab10">10</a> show that the removal of RSN from the proposed ViT-IWRS results in performance degradation. Therefore, the use of the RSN block is essential to achieve improved results.</p><h4 class="c-article__sub-heading c-article__sub-heading--small" id="Sec37"><span class="c-article-section__title-number">5.10.2 </span>Effect of transformer encoding</h4><p>In order to verify the impact of transformer encoding, we simply apply max and attention pooling on the output of the embedding network to obtain a bag representation vector. Afterward, the generated output vector is used for the classification process. This process is analogous to existing AbDMIL and LBA algorithms. The experimental results in Table<a data-track="click" data-track-label="link" data-track-action="table anchor" href="/article/10.1007/s00521-024-09417-3#Tab10">10</a> show that the removal of Transformer Encoding and RSN from the proposed ViT-IWRS results in performance degradation. Therefore, the use of this block is essential to attain improved results.</p></div></div></section><section data-title="Conclusion"><div class="c-article-section" id="Sec38-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec38"><span class="c-article-section__title-number">6 </span>Conclusion</h2><div class="c-article-section__content" id="Sec38-content"><p>In this work, we presented the application for a vision transformer for simultaneous instance weighting and bag encoding processes for MIL. The existing MIL algorithms presumed that the instances in the bag are either related or unrelated. However, this assumption may not apply to all bags in the dataset.</p><p>The proposed approach avoids the instance relationship assumption in a two-stage process. In the first stage, several bag representation vectors are generated for both relationship assumptions. In the second stage, the network decides whether to consider instances to be related or not using the representation selection module in the classification process. The experimental results show that the selection subnetwork robustly selects bag representation vectors in the bag classification process in an end-to-end trainable approach. The experiments are performed on diverse datasets related to images and molecular activity. The proposed approach outperformed several state-of-the-art attention pooling and benchmark bag classification techniques. Additionally, the proposed ViT-IWRS provides model interpretations for vision transformer architecture through an attention-based instance weighting approach. Thus, the proposed approach is suited for image classification, object detection, and high-risk MIL applications, such as computer-aided diagnostic and clinical decision support.</p><p>Although the proposed approach produces promising results on several datasets related to images, this approach is less computationally expensive as compared to existing pooling techniques. Furthermore, the performance of ViT-IWRS is effective when labels are entirely dependent on the structural properties of the instances, such as molecular datasets. The proposed loss function can be further extended to handle multi-instance multi-target regression problems, such as Drug Discovery and Environmental Monitoring. In the future, we intend to explore the application of the proposed approach to multiple-instance and multiple-label learning (MIML) tasks and incorporate the structural details of the bag into the self-attention process.</p></div></div></section>
                                </div>
                        
                    

                    
                        
                    

                    
                        
                    

                    
                    <section data-title="Data availability"><div class="c-article-section" id="data-availability-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="data-availability">Data availability</h2><div class="c-article-section__content" id="data-availability-content">
              
              <p>The datasets generated during and/or analyzed during the current study are publically available at <a href="http://www.uco.es/grupos/kdis/momil/">http://www.uco.es/grupos/kdis/momil/</a>.</p>
            </div></div></section><div id="MagazineFulltextArticleBodySuffix"><section aria-labelledby="Bib1" data-title="References"><div class="c-article-section" id="Bib1-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Bib1">References</h2><div class="c-article-section__content" id="Bib1-content"><div data-container-section="references"><ol class="c-article-references" data-track-component="outbound reference" data-track-context="references section"><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="1."><p class="c-article-references__text" id="ref-CR1">Zhou Z-H (2018) A brief introduction to weakly supervised learning. Natl Sci Rev 5(1):4453</p><p class="c-article-references__links u-hide-print"><a data-track="click_references" rel="nofollow noopener" data-track-label="10.1093/nsr/nwx106" data-track-item_id="10.1093/nsr/nwx106" data-track-value="article reference" data-track-action="article reference" href="https://doi.org/10.1093%2Fnsr%2Fnwx106" aria-label="Article reference 1" data-doi="10.1093/nsr/nwx106">Article</a>
    <a data-track="click_references" data-track-action="google scholar reference" data-track-value="google scholar reference" data-track-label="link" data-track-item_id="link" rel="nofollow noopener" aria-label="Google Scholar reference 1" href="http://scholar.google.com/scholar_lookup?&amp;title=A%20brief%20introduction%20to%20weakly%20supervised%20learning&amp;journal=Natl%20Sci%20Rev&amp;doi=10.1093%2Fnsr%2Fnwx106&amp;volume=5&amp;issue=1&amp;pages=44-53&amp;publication_year=2018&amp;author=Zhou%2CZ-H">
                    Google Scholar</a>
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="2."><p class="c-article-references__text" id="ref-CR2">Li M, Li X, Jiang Y, Zhang J, Luo H, Yin S (2022) Explainable multi-instance and multi-task learning for COVID-19 diagnosis and lesion segmentation in CT images. Knowl-Based Syst 252:109278</p><p class="c-article-references__links u-hide-print"><a data-track="click_references" rel="nofollow noopener" data-track-label="10.1016/j.knosys.2022.109278" data-track-item_id="10.1016/j.knosys.2022.109278" data-track-value="article reference" data-track-action="article reference" href="https://doi.org/10.1016%2Fj.knosys.2022.109278" aria-label="Article reference 2" data-doi="10.1016/j.knosys.2022.109278">Article</a>
    <a data-track="click_references" rel="nofollow noopener" data-track-label="link" data-track-item_id="link" data-track-value="pubmed reference" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=35783000" aria-label="PubMed reference 2">PubMed</a>
    <a data-track="click_references" rel="nofollow noopener" data-track-label="link" data-track-item_id="link" data-track-value="pubmed central reference" data-track-action="pubmed central reference" href="http://www.ncbi.nlm.nih.gov/pmc/articles/PMC9235304" aria-label="PubMed Central reference 2">PubMed Central</a>
    <a data-track="click_references" data-track-action="google scholar reference" data-track-value="google scholar reference" data-track-label="link" data-track-item_id="link" rel="nofollow noopener" aria-label="Google Scholar reference 2" href="http://scholar.google.com/scholar_lookup?&amp;title=Explainable%20multi-instance%20and%20multi-task%20learning%20for%20COVID-19%20diagnosis%20and%20lesion%20segmentation%20in%20CT%20images&amp;journal=Knowl-Based%20Syst&amp;doi=10.1016%2Fj.knosys.2022.109278&amp;volume=252&amp;publication_year=2022&amp;author=Li%2CM&amp;author=Li%2CX&amp;author=Jiang%2CY&amp;author=Zhang%2CJ&amp;author=Luo%2CH&amp;author=Yin%2CS">
                    Google Scholar</a>
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="3."><p class="c-article-references__text" id="ref-CR3">Liu Y, Wu YH, Wen P, Shi Y, Qiu Y, Cheng MM (2020) Leveraging instance-, image-and dataset-level information for weakly supervised instance segmentation. IEEE Trans. Pattern Anal. Mach. Intell. 44(3):14151428</p><p class="c-article-references__links u-hide-print"><a data-track="click_references" rel="nofollow noopener" data-track-label="10.1109/TPAMI.2020.3023152" data-track-item_id="10.1109/TPAMI.2020.3023152" data-track-value="article reference" data-track-action="article reference" href="https://doi.org/10.1109%2FTPAMI.2020.3023152" aria-label="Article reference 3" data-doi="10.1109/TPAMI.2020.3023152">Article</a>
    <a data-track="click_references" data-track-action="google scholar reference" data-track-value="google scholar reference" data-track-label="link" data-track-item_id="link" rel="nofollow noopener" aria-label="Google Scholar reference 3" href="http://scholar.google.com/scholar_lookup?&amp;title=Leveraging%20instance-%2C%20image-and%20dataset-level%20information%20for%20weakly%20supervised%20instance%20segmentation&amp;journal=IEEE%20Trans.%20Pattern%20Anal.%20Mach.%20Intell.&amp;doi=10.1109%2FTPAMI.2020.3023152&amp;volume=44&amp;issue=3&amp;pages=1415-1428&amp;publication_year=2020&amp;author=Liu%2CY&amp;author=Wu%2CYH&amp;author=Wen%2CP&amp;author=Shi%2CY&amp;author=Qiu%2CY&amp;author=Cheng%2CMM">
                    Google Scholar</a>
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="4."><p class="c-article-references__text" id="ref-CR4">Zhang Y, Liu S, Qu X, Shang X (2022) Multi-instance discriminative contrastive learning for brain image representation. Neural Comput Appl. <a href="https://doi.org/10.1007/s00521-022-07524-7" data-track="click_references" data-track-action="external reference" data-track-value="external reference" data-track-label="10.1007/s00521-022-07524-7">https://doi.org/10.1007/s00521-022-07524-7</a></p><p class="c-article-references__links u-hide-print"><a data-track="click_references" rel="noopener" data-track-label="10.1007/s00521-022-07524-7" data-track-item_id="10.1007/s00521-022-07524-7" data-track-value="article reference" data-track-action="article reference" href="https://link.springer.com/doi/10.1007/s00521-022-07524-7" aria-label="Article reference 4" data-doi="10.1007/s00521-022-07524-7">Article</a>
    <a data-track="click_references" rel="nofollow noopener" data-track-label="link" data-track-item_id="link" data-track-value="pubmed reference" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=36245795" aria-label="PubMed reference 4">PubMed</a>
    <a data-track="click_references" rel="nofollow noopener" data-track-label="link" data-track-item_id="link" data-track-value="pubmed central reference" data-track-action="pubmed central reference" href="http://www.ncbi.nlm.nih.gov/pmc/articles/PMC9553631" aria-label="PubMed Central reference 4">PubMed Central</a>
    <a data-track="click_references" data-track-action="google scholar reference" data-track-value="google scholar reference" data-track-label="link" data-track-item_id="link" rel="nofollow noopener" aria-label="Google Scholar reference 4" href="http://scholar.google.com/scholar_lookup?&amp;title=Multi-instance%20discriminative%20contrastive%20learning%20for%20brain%20image%20representation&amp;journal=Neural%20Comput%20Appl&amp;doi=10.1007%2Fs00521-022-07524-7&amp;publication_year=2022&amp;author=Zhang%2CY&amp;author=Liu%2CS&amp;author=Qu%2CX&amp;author=Shang%2CX">
                    Google Scholar</a>
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="5."><p class="c-article-references__text" id="ref-CR5">Antwi-Bekoe E, Liu G, Ainam J-P, Sun G, Xie X (2022) A deep learning approach for insulator instance segmentation and defect detection. Neural Comput Appl 34(9):72537269</p><p class="c-article-references__links u-hide-print"><a data-track="click_references" rel="noopener" data-track-label="10.1007/s00521-021-06792-z" data-track-item_id="10.1007/s00521-021-06792-z" data-track-value="article reference" data-track-action="article reference" href="https://link.springer.com/doi/10.1007/s00521-021-06792-z" aria-label="Article reference 5" data-doi="10.1007/s00521-021-06792-z">Article</a>
    <a data-track="click_references" data-track-action="google scholar reference" data-track-value="google scholar reference" data-track-label="link" data-track-item_id="link" rel="nofollow noopener" aria-label="Google Scholar reference 5" href="http://scholar.google.com/scholar_lookup?&amp;title=A%20deep%20learning%20approach%20for%20insulator%20instance%20segmentation%20and%20defect%20detection&amp;journal=Neural%20Comput%20Appl&amp;doi=10.1007%2Fs00521-021-06792-z&amp;volume=34&amp;issue=9&amp;pages=7253-7269&amp;publication_year=2022&amp;author=Antwi-Bekoe%2CE&amp;author=Liu%2CG&amp;author=Ainam%2CJ-P&amp;author=Sun%2CG&amp;author=Xie%2CX">
                    Google Scholar</a>
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="6."><p class="c-article-references__text" id="ref-CR6">Wang K, Liu J, Gonzlez D (2017) Domain transfer multi-instance dictionary learning. Neural Comput Appl 28:983992</p><p class="c-article-references__links u-hide-print"><a data-track="click_references" rel="noopener" data-track-label="10.1007/s00521-016-2406-5" data-track-item_id="10.1007/s00521-016-2406-5" data-track-value="article reference" data-track-action="article reference" href="https://link.springer.com/doi/10.1007/s00521-016-2406-5" aria-label="Article reference 6" data-doi="10.1007/s00521-016-2406-5">Article</a>
    <a data-track="click_references" data-track-action="google scholar reference" data-track-value="google scholar reference" data-track-label="link" data-track-item_id="link" rel="nofollow noopener" aria-label="Google Scholar reference 6" href="http://scholar.google.com/scholar_lookup?&amp;title=Domain%20transfer%20multi-instance%20dictionary%20learning&amp;journal=Neural%20Comput%20Appl&amp;doi=10.1007%2Fs00521-016-2406-5&amp;volume=28&amp;pages=983-992&amp;publication_year=2017&amp;author=Wang%2CK&amp;author=Liu%2CJ&amp;author=Gonz%C3%A1lez%2CD">
                    Google Scholar</a>
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="7."><p class="c-article-references__text" id="ref-CR7">Carbonneau M-A, Cheplygina V, Granger E, Gagnon G (2018) Multiple instance learning: a survey of problem characteristics and applications. Pattern Recogn 77:329353</p><p class="c-article-references__links u-hide-print"><a data-track="click_references" rel="nofollow noopener" data-track-label="10.1016/j.patcog.2017.10.009" data-track-item_id="10.1016/j.patcog.2017.10.009" data-track-value="article reference" data-track-action="article reference" href="https://doi.org/10.1016%2Fj.patcog.2017.10.009" aria-label="Article reference 7" data-doi="10.1016/j.patcog.2017.10.009">Article</a>
    <a data-track="click_references" rel="nofollow noopener" data-track-label="link" data-track-item_id="link" data-track-value="ads reference" data-track-action="ads reference" href="http://adsabs.harvard.edu/cgi-bin/nph-data_query?link_type=ABSTRACT&amp;bibcode=2018PatRe..77..329C" aria-label="ADS reference 7">ADS</a>
    <a data-track="click_references" data-track-action="google scholar reference" data-track-value="google scholar reference" data-track-label="link" data-track-item_id="link" rel="nofollow noopener" aria-label="Google Scholar reference 7" href="http://scholar.google.com/scholar_lookup?&amp;title=Multiple%20instance%20learning%3A%20a%20survey%20of%20problem%20characteristics%20and%20applications&amp;journal=Pattern%20Recogn&amp;doi=10.1016%2Fj.patcog.2017.10.009&amp;volume=77&amp;pages=329-353&amp;publication_year=2018&amp;author=Carbonneau%2CM-A&amp;author=Cheplygina%2CV&amp;author=Granger%2CE&amp;author=Gagnon%2CG">
                    Google Scholar</a>
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="8."><p class="c-article-references__text" id="ref-CR8">Cheplygina V, Tax DM, Loog M (2015) Dissimilarity-based ensembles for multiple instance learning. IEEE Trans Neural Netw Learn Syst 27(6):13791391</p><p class="c-article-references__links u-hide-print"><a data-track="click_references" rel="nofollow noopener" data-track-label="10.1109/TNNLS.2015.2424254" data-track-item_id="10.1109/TNNLS.2015.2424254" data-track-value="article reference" data-track-action="article reference" href="https://doi.org/10.1109%2FTNNLS.2015.2424254" aria-label="Article reference 8" data-doi="10.1109/TNNLS.2015.2424254">Article</a>
    <a data-track="click_references" data-track-action="google scholar reference" data-track-value="google scholar reference" data-track-label="link" data-track-item_id="link" rel="nofollow noopener" aria-label="Google Scholar reference 8" href="http://scholar.google.com/scholar_lookup?&amp;title=Dissimilarity-based%20ensembles%20for%20multiple%20instance%20learning&amp;journal=IEEE%20Trans%20Neural%20Netw%20Learn%20Syst&amp;doi=10.1109%2FTNNLS.2015.2424254&amp;volume=27&amp;issue=6&amp;pages=1379-1391&amp;publication_year=2015&amp;author=Cheplygina%2CV&amp;author=Tax%2CDM&amp;author=Loog%2CM">
                    Google Scholar</a>
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="9."><p class="c-article-references__text" id="ref-CR9">Wei X-S, Wu J, Zhou Z-H (2016) Scalable algorithms for multi-instance learning. IEEE Trans Neural Netw Learn Syst 28(4):975987</p><p class="c-article-references__links u-hide-print"><a data-track="click_references" rel="nofollow noopener" data-track-label="10.1109/TNNLS.2016.2519102" data-track-item_id="10.1109/TNNLS.2016.2519102" data-track-value="article reference" data-track-action="article reference" href="https://doi.org/10.1109%2FTNNLS.2016.2519102" aria-label="Article reference 9" data-doi="10.1109/TNNLS.2016.2519102">Article</a>
    <a data-track="click_references" rel="nofollow noopener" data-track-label="link" data-track-item_id="link" data-track-value="pubmed reference" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=26863679" aria-label="PubMed reference 9">PubMed</a>
    <a data-track="click_references" data-track-action="google scholar reference" data-track-value="google scholar reference" data-track-label="link" data-track-item_id="link" rel="nofollow noopener" aria-label="Google Scholar reference 9" href="http://scholar.google.com/scholar_lookup?&amp;title=Scalable%20algorithms%20for%20multi-instance%20learning&amp;journal=IEEE%20Trans%20Neural%20Netw%20Learn%20Syst&amp;doi=10.1109%2FTNNLS.2016.2519102&amp;volume=28&amp;issue=4&amp;pages=975-987&amp;publication_year=2016&amp;author=Wei%2CX-S&amp;author=Wu%2CJ&amp;author=Zhou%2CZ-H">
                    Google Scholar</a>
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="10."><p class="c-article-references__text" id="ref-CR10">Perronnin F, Snchez J, Mensink T (2010) Improving the fisher kernel for large-scale image classification. In: European Conference on Computer Vision, pp. 143156. Springer</p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="11."><p class="c-article-references__text" id="ref-CR11">Ramon J, DeRaedt L (2000) Multi instance neural networks. In: Proceedings of the ICML-2000 Workshop on Attribute-value and Relational Learning, pp. 5360</p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="12."><p class="c-article-references__text" id="ref-CR12">Kandemir M, Hamprecht FA (2015) Computer-aided diagnosis from weak supervision: a benchmarking study. Comput Med Imaging Graph 42:4450</p><p class="c-article-references__links u-hide-print"><a data-track="click_references" rel="nofollow noopener" data-track-label="10.1016/j.compmedimag.2014.11.010" data-track-item_id="10.1016/j.compmedimag.2014.11.010" data-track-value="article reference" data-track-action="article reference" href="https://doi.org/10.1016%2Fj.compmedimag.2014.11.010" aria-label="Article reference 12" data-doi="10.1016/j.compmedimag.2014.11.010">Article</a>
    <a data-track="click_references" rel="nofollow noopener" data-track-label="link" data-track-item_id="link" data-track-value="pubmed reference" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=25475486" aria-label="PubMed reference 12">PubMed</a>
    <a data-track="click_references" data-track-action="google scholar reference" data-track-value="google scholar reference" data-track-label="link" data-track-item_id="link" rel="nofollow noopener" aria-label="Google Scholar reference 12" href="http://scholar.google.com/scholar_lookup?&amp;title=Computer-aided%20diagnosis%20from%20weak%20supervision%3A%20a%20benchmarking%20study&amp;journal=Comput%20Med%20Imaging%20Graph&amp;doi=10.1016%2Fj.compmedimag.2014.11.010&amp;volume=42&amp;pages=44-50&amp;publication_year=2015&amp;author=Kandemir%2CM&amp;author=Hamprecht%2CFA">
                    Google Scholar</a>
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="13."><p class="c-article-references__text" id="ref-CR13">Ilse M, Tomczak J, Welling M (2018) Attention-based deep multiple instance learning. In: International conference on machine learning, pp. 21272136. PMLR</p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="14."><p class="c-article-references__text" id="ref-CR14">Zhang W-J, Zhou Z-H (2014) Multi-instance learning with distribution change. In: Proceedings of the AAAI conference on artificial intelligence, vol. 28</p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="15."><p class="c-article-references__text" id="ref-CR15">Shi X, Xing F, Xie Y, Zhang Z, Cui L, Yang L (2020) Loss-based attention for deep multiple instance learning. In: Proceedings of the AAAI conference on artificial intelligence, vol. 34, pp. 57425749</p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="16."><p class="c-article-references__text" id="ref-CR16">Zhou Z-H, Sun Y-Y, Li Y-F (2009) Multi-instance learning by treating instances as non-IID samples. In: Proceedings of the 26th annual international conference on machine learning, pp. 12491256</p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="17."><p class="c-article-references__text" id="ref-CR17">Waqas M, Tahir MA, Qureshi R (2021) Ensemble-based instance relevance estimation in multiple-instance learning. In: 2021 9th European workshop on visual information processing (EUVIP), pp. 16. IEEE</p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="18."><p class="c-article-references__text" id="ref-CR18">Waqas M, Tahir MA, Qureshi R (2023) Deep Gaussian mixture model based instance relevance estimation for multiple instance learning applications. Appl Intell 53(9):1031010325</p><p class="c-article-references__links u-hide-print"><a data-track="click_references" rel="noopener" data-track-label="10.1007/s10489-022-04045-7" data-track-item_id="10.1007/s10489-022-04045-7" data-track-value="article reference" data-track-action="article reference" href="https://link.springer.com/doi/10.1007/s10489-022-04045-7" aria-label="Article reference 18" data-doi="10.1007/s10489-022-04045-7">Article</a>
    <a data-track="click_references" data-track-action="google scholar reference" data-track-value="google scholar reference" data-track-label="link" data-track-item_id="link" rel="nofollow noopener" aria-label="Google Scholar reference 18" href="http://scholar.google.com/scholar_lookup?&amp;title=Deep%20Gaussian%20mixture%20model%20based%20instance%20relevance%20estimation%20for%20multiple%20instance%20learning%20applications&amp;journal=Appl%20Intell&amp;doi=10.1007%2Fs10489-022-04045-7&amp;volume=53&amp;issue=9&amp;pages=10310-10325&amp;publication_year=2023&amp;author=Waqas%2CM&amp;author=Tahir%2CMA&amp;author=Qureshi%2CR">
                    Google Scholar</a>
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="19."><p class="c-article-references__text" id="ref-CR19">Waqas M, Tahir MA, Khan SA (2023) Robust bag classification approach for multi-instance learning via subspace fuzzy clustering. Expert Syst Appl 214:119113</p><p class="c-article-references__links u-hide-print"><a data-track="click_references" rel="nofollow noopener" data-track-label="10.1016/j.eswa.2022.119113" data-track-item_id="10.1016/j.eswa.2022.119113" data-track-value="article reference" data-track-action="article reference" href="https://doi.org/10.1016%2Fj.eswa.2022.119113" aria-label="Article reference 19" data-doi="10.1016/j.eswa.2022.119113">Article</a>
    <a data-track="click_references" data-track-action="google scholar reference" data-track-value="google scholar reference" data-track-label="link" data-track-item_id="link" rel="nofollow noopener" aria-label="Google Scholar reference 19" href="http://scholar.google.com/scholar_lookup?&amp;title=Robust%20bag%20classification%20approach%20for%20multi-instance%20learning%20via%20subspace%20fuzzy%20clustering&amp;journal=Expert%20Syst%20Appl&amp;doi=10.1016%2Fj.eswa.2022.119113&amp;volume=214&amp;publication_year=2023&amp;author=Waqas%2CM&amp;author=Tahir%2CMA&amp;author=Khan%2CSA">
                    Google Scholar</a>
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="20."><p class="c-article-references__text" id="ref-CR20">Shao Z, Bian H, Chen Y, Wang Y, Zhang J, Ji X et al (2021) Transmil: transformer based correlated multiple instance learning for whole slide image classification. Adv Neural Inf Process Syst 34:2136</p><p class="c-article-references__links u-hide-print"><a data-track="click_references" data-track-action="google scholar reference" data-track-value="google scholar reference" data-track-label="link" data-track-item_id="link" rel="nofollow noopener" aria-label="Google Scholar reference 20" href="http://scholar.google.com/scholar_lookup?&amp;title=Transmil%3A%20transformer%20based%20correlated%20multiple%20instance%20learning%20for%20whole%20slide%20image%20classification&amp;journal=Adv%20Neural%20Inf%20Process%20Syst&amp;volume=34&amp;publication_year=2021&amp;author=Shao%2CZ&amp;author=Bian%2CH&amp;author=Chen%2CY&amp;author=Wang%2CY&amp;author=Zhang%2CJ&amp;author=Ji%2CX">
                    Google Scholar</a>
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="21."><p class="c-article-references__text" id="ref-CR61">Waqas M, Khan Z, Ahmed SU, Raza A (2023) MIL-Mixer: a robust bag encoding strategy for Multiple Instance Learning (mil) using MLP-Mixer. In 2023 18th IEEE International Conference on Emerging Technologies (ICET) 2226</p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="22."><p class="c-article-references__text" id="ref-CR21">Wei X-S, Zhou Z-H (2016) An empirical study on image bag generators for multi-instance learning. Mach Learn 105(2):155198</p><p class="c-article-references__links u-hide-print"><a data-track="click_references" rel="noopener" data-track-label="10.1007/s10994-016-5560-1" data-track-item_id="10.1007/s10994-016-5560-1" data-track-value="article reference" data-track-action="article reference" href="https://link.springer.com/doi/10.1007/s10994-016-5560-1" aria-label="Article reference 22" data-doi="10.1007/s10994-016-5560-1">Article</a>
    <a data-track="click_references" rel="nofollow noopener" data-track-label="link" data-track-item_id="link" data-track-value="mathscinet reference" data-track-action="mathscinet reference" href="http://www.ams.org/mathscinet-getitem?mr=3554909" aria-label="MathSciNet reference 22">MathSciNet</a>
    <a data-track="click_references" data-track-action="google scholar reference" data-track-value="google scholar reference" data-track-label="link" data-track-item_id="link" rel="nofollow noopener" aria-label="Google Scholar reference 22" href="http://scholar.google.com/scholar_lookup?&amp;title=An%20empirical%20study%20on%20image%20bag%20generators%20for%20multi-instance%20learning&amp;journal=Mach%20Learn&amp;doi=10.1007%2Fs10994-016-5560-1&amp;volume=105&amp;issue=2&amp;pages=155-198&amp;publication_year=2016&amp;author=Wei%2CX-S&amp;author=Zhou%2CZ-H">
                    Google Scholar</a>
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="23."><p class="c-article-references__text" id="ref-CR22">Dietterich TG, Lathrop RH, Lozano-Prez T (1997) Solving the multiple instance problem with axis-parallel rectangles. Artif Intell 89(12):3171</p><p class="c-article-references__links u-hide-print"><a data-track="click_references" rel="nofollow noopener" data-track-label="10.1016/S0004-3702(96)00034-3" data-track-item_id="10.1016/S0004-3702(96)00034-3" data-track-value="article reference" data-track-action="article reference" href="https://doi.org/10.1016%2FS0004-3702%2896%2900034-3" aria-label="Article reference 23" data-doi="10.1016/S0004-3702(96)00034-3">Article</a>
    <a data-track="click_references" data-track-action="google scholar reference" data-track-value="google scholar reference" data-track-label="link" data-track-item_id="link" rel="nofollow noopener" aria-label="Google Scholar reference 23" href="http://scholar.google.com/scholar_lookup?&amp;title=Solving%20the%20multiple%20instance%20problem%20with%20axis-parallel%20rectangles&amp;journal=Artif%20Intell&amp;doi=10.1016%2FS0004-3702%2896%2900034-3&amp;volume=89&amp;issue=1%E2%80%932&amp;pages=31-71&amp;publication_year=1997&amp;author=Dietterich%2CTG&amp;author=Lathrop%2CRH&amp;author=Lozano-P%C3%A9rez%2CT">
                    Google Scholar</a>
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="24."><p class="c-article-references__text" id="ref-CR23">Sirinukunwattana K, Raza SEA, Tsang Y-W, Snead DR, Cree IA, Rajpoot NM (2016) Locality sensitive deep learning for detection and classification of nuclei in routine colon cancer histology images. IEEE Trans Med Imaging 35(5):11961206</p><p class="c-article-references__links u-hide-print"><a data-track="click_references" rel="nofollow noopener" data-track-label="10.1109/TMI.2016.2525803" data-track-item_id="10.1109/TMI.2016.2525803" data-track-value="article reference" data-track-action="article reference" href="https://doi.org/10.1109%2FTMI.2016.2525803" aria-label="Article reference 24" data-doi="10.1109/TMI.2016.2525803">Article</a>
    <a data-track="click_references" rel="nofollow noopener" data-track-label="link" data-track-item_id="link" data-track-value="pubmed reference" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=26863654" aria-label="PubMed reference 24">PubMed</a>
    <a data-track="click_references" data-track-action="google scholar reference" data-track-value="google scholar reference" data-track-label="link" data-track-item_id="link" rel="nofollow noopener" aria-label="Google Scholar reference 24" href="http://scholar.google.com/scholar_lookup?&amp;title=Locality%20sensitive%20deep%20learning%20for%20detection%20and%20classification%20of%20nuclei%20in%20routine%20colon%20cancer%20histology%20images&amp;journal=IEEE%20Trans%20Med%20Imaging&amp;doi=10.1109%2FTMI.2016.2525803&amp;volume=35&amp;issue=5&amp;pages=1196-1206&amp;publication_year=2016&amp;author=Sirinukunwattana%2CK&amp;author=Raza%2CSEA&amp;author=Tsang%2CY-W&amp;author=Snead%2CDR&amp;author=Cree%2CIA&amp;author=Rajpoot%2CNM">
                    Google Scholar</a>
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="25."><p class="c-article-references__text" id="ref-CR24">Raykar VC, Krishnapuram B, Bi J, Dundar M, Rao RB (2008) Bayesian multiple instance learning: automatic feature selection and inductive transfer. In: Proceedings of the 25th international conference on machine learning, pp. 808815</p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="26."><p class="c-article-references__text" id="ref-CR25">Andrews S, Tsochantaridis I, Hofmann T (2002) Support vector machines for multiple-instance learning. In: NIPS, vol. 2, p. 7</p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="27."><p class="c-article-references__text" id="ref-CR26">Amar RA, Dooly DR, Goldman SA, Zhang Q (2001) Multiple-instance learning of real-valued data. In: ICML, pp. 310. Citeseer</p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="28."><p class="c-article-references__text" id="ref-CR27">Zhang Q, Goldman S (2001) EM-DD: An improved multiple-instance learning technique. In: Dietterich T, Becker S, Ghahramani Z(ed) Advances in neural information processing systems. MIT Press, 14. <a href="https://proceedings.neurips.cc/paper_files/paper/2001/file/e4dd5528f7596dcdf871aa55cfccc53c-Paper.pdf" data-track="click_references" data-track-action="external reference" data-track-value="external reference" data-track-label="https://proceedings.neurips.cc/paper_files/paper/2001/file/e4dd5528f7596dcdf871aa55cfccc53c-Paper.pdf">https://proceedings.neurips.cc/paper_files/paper/2001/file/e4dd5528f7596dcdf871aa55cfccc53c-Paper.pdf</a></p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="29."><p class="c-article-references__text" id="ref-CR28">Carbonneau M-A, Granger E, Raymond AJ, Gagnon G (2016) Robust multiple-instance learning ensembles using random subspace instance selection. Pattern Recogn 58:8399</p><p class="c-article-references__links u-hide-print"><a data-track="click_references" rel="nofollow noopener" data-track-label="10.1016/j.patcog.2016.03.035" data-track-item_id="10.1016/j.patcog.2016.03.035" data-track-value="article reference" data-track-action="article reference" href="https://doi.org/10.1016%2Fj.patcog.2016.03.035" aria-label="Article reference 29" data-doi="10.1016/j.patcog.2016.03.035">Article</a>
    <a data-track="click_references" rel="nofollow noopener" data-track-label="link" data-track-item_id="link" data-track-value="ads reference" data-track-action="ads reference" href="http://adsabs.harvard.edu/cgi-bin/nph-data_query?link_type=ABSTRACT&amp;bibcode=2016PatRe..58...83C" aria-label="ADS reference 29">ADS</a>
    <a data-track="click_references" data-track-action="google scholar reference" data-track-value="google scholar reference" data-track-label="link" data-track-item_id="link" rel="nofollow noopener" aria-label="Google Scholar reference 29" href="http://scholar.google.com/scholar_lookup?&amp;title=Robust%20multiple-instance%20learning%20ensembles%20using%20random%20subspace%20instance%20selection&amp;journal=Pattern%20Recogn&amp;doi=10.1016%2Fj.patcog.2016.03.035&amp;volume=58&amp;pages=83-99&amp;publication_year=2016&amp;author=Carbonneau%2CM-A&amp;author=Granger%2CE&amp;author=Raymond%2CAJ&amp;author=Gagnon%2CG">
                    Google Scholar</a>
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="30."><p class="c-article-references__text" id="ref-CR29">Zhou Z-H, Zhang M-L (2007) Solving multi-instance problems with classifier ensemble based on constructive clustering. Knowl Inf Syst 11(2):155170</p><p class="c-article-references__links u-hide-print"><a data-track="click_references" rel="noopener" data-track-label="10.1007/s10115-006-0029-3" data-track-item_id="10.1007/s10115-006-0029-3" data-track-value="article reference" data-track-action="article reference" href="https://link.springer.com/doi/10.1007/s10115-006-0029-3" aria-label="Article reference 30" data-doi="10.1007/s10115-006-0029-3">Article</a>
    <a data-track="click_references" data-track-action="google scholar reference" data-track-value="google scholar reference" data-track-label="link" data-track-item_id="link" rel="nofollow noopener" aria-label="Google Scholar reference 30" href="http://scholar.google.com/scholar_lookup?&amp;title=Solving%20multi-instance%20problems%20with%20classifier%20ensemble%20based%20on%20constructive%20clustering&amp;journal=Knowl%20Inf%20Syst&amp;doi=10.1007%2Fs10115-006-0029-3&amp;volume=11&amp;issue=2&amp;pages=155-170&amp;publication_year=2007&amp;author=Zhou%2CZ-H&amp;author=Zhang%2CM-L">
                    Google Scholar</a>
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="31."><p class="c-article-references__text" id="ref-CR30">Zhou Z-H, Xu J-M (2007) On the relation between multi-instance learning and semi-supervised learning. In: Proceedings of the 24th international conference on machine learning, pp. 11671174</p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="32."><p class="c-article-references__text" id="ref-CR31">Leistner C, Saffari A, Bischof H (2010) Miforests: Multiple-instance learning with randomized trees. In: European conference on computer vision, pp. 2942. Springer</p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="33."><p class="c-article-references__text" id="ref-CR32">Li CH, Gondra I, Liu L (2012) An efficient parallel neural network-based multi-instance learning algorithm. J Supercomput 62(2):724740</p><p class="c-article-references__links u-hide-print"><a data-track="click_references" rel="noopener" data-track-label="10.1007/s11227-012-0746-1" data-track-item_id="10.1007/s11227-012-0746-1" data-track-value="article reference" data-track-action="article reference" href="https://link.springer.com/doi/10.1007/s11227-012-0746-1" aria-label="Article reference 33" data-doi="10.1007/s11227-012-0746-1">Article</a>
    <a data-track="click_references" data-track-action="google scholar reference" data-track-value="google scholar reference" data-track-label="link" data-track-item_id="link" rel="nofollow noopener" aria-label="Google Scholar reference 33" href="http://scholar.google.com/scholar_lookup?&amp;title=An%20efficient%20parallel%20neural%20network-based%20multi-instance%20learning%20algorithm&amp;journal=J%20Supercomput&amp;doi=10.1007%2Fs11227-012-0746-1&amp;volume=62&amp;issue=2&amp;pages=724-740&amp;publication_year=2012&amp;author=Li%2CCH&amp;author=Gondra%2CI&amp;author=Liu%2CL">
                    Google Scholar</a>
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="34."><p class="c-article-references__text" id="ref-CR33">Waqas M, Khan Z, Anjum S, Tahir MA (2020) Lung-wise tuberculosis analysis and automatic CT report generation with hybrid feature and ensemble learning. In: CLEF (Working Notes)</p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="35."><p class="c-article-references__text" id="ref-CR34">Abro WA, Aicher A, Rach N, Ultes S, Minker W, Qi G (2022) Natural language understanding for argumentative dialogue systems in the opinion building domain. Knowl-Based Syst 242:108318</p><p class="c-article-references__links u-hide-print"><a data-track="click_references" rel="nofollow noopener" data-track-label="10.1016/j.knosys.2022.108318" data-track-item_id="10.1016/j.knosys.2022.108318" data-track-value="article reference" data-track-action="article reference" href="https://doi.org/10.1016%2Fj.knosys.2022.108318" aria-label="Article reference 35" data-doi="10.1016/j.knosys.2022.108318">Article</a>
    <a data-track="click_references" data-track-action="google scholar reference" data-track-value="google scholar reference" data-track-label="link" data-track-item_id="link" rel="nofollow noopener" aria-label="Google Scholar reference 35" href="http://scholar.google.com/scholar_lookup?&amp;title=Natural%20language%20understanding%20for%20argumentative%20dialogue%20systems%20in%20the%20opinion%20building%20domain&amp;journal=Knowl-Based%20Syst&amp;doi=10.1016%2Fj.knosys.2022.108318&amp;volume=242&amp;publication_year=2022&amp;author=Abro%2CWA&amp;author=Aicher%2CA&amp;author=Rach%2CN&amp;author=Ultes%2CS&amp;author=Minker%2CW&amp;author=Qi%2CG">
                    Google Scholar</a>
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="36."><p class="c-article-references__text" id="ref-CR35">Hanif M, Waqas M, Muneer A, Alwadain A, Tahir MA, Rafi M (2023) Deepsdc: deep ensemble learner for the classification of social-media flooding events. Sustainability 15(7):6049</p><p class="c-article-references__links u-hide-print"><a data-track="click_references" rel="nofollow noopener" data-track-label="10.3390/su15076049" data-track-item_id="10.3390/su15076049" data-track-value="article reference" data-track-action="article reference" href="https://doi.org/10.3390%2Fsu15076049" aria-label="Article reference 36" data-doi="10.3390/su15076049">Article</a>
    <a data-track="click_references" data-track-action="google scholar reference" data-track-value="google scholar reference" data-track-label="link" data-track-item_id="link" rel="nofollow noopener" aria-label="Google Scholar reference 36" href="http://scholar.google.com/scholar_lookup?&amp;title=Deepsdc%3A%20deep%20ensemble%20learner%20for%20the%20classification%20of%20social-media%20flooding%20events&amp;journal=Sustainability&amp;doi=10.3390%2Fsu15076049&amp;volume=15&amp;issue=7&amp;publication_year=2023&amp;author=Hanif%2CM&amp;author=Waqas%2CM&amp;author=Muneer%2CA&amp;author=Alwadain%2CA&amp;author=Tahir%2CMA&amp;author=Rafi%2CM">
                    Google Scholar</a>
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="37."><p class="c-article-references__text" id="ref-CR36">Hoffman J, Pathak D, Darrell T, Saenko K (2015) Detector discovery in the wild: joint multiple instance and representation learning. In: Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 28832891</p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="38."><p class="c-article-references__text" id="ref-CR37">Zhang C, Platt J, Viola P (2005) Multiple instance boosting for object detection. In: Weiss J, Sch\"{o}lkopf B, Platt J(ed) Advances in neural information processing systems. MIT Press, 18</p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="39."><p class="c-article-references__text" id="ref-CR38">Shi X, Xing F, Xu K, Xie Y, Su H, Yang L (2017) Supervised graph hashing for histopathology image retrieval and classification. Med Image Anal 42:117128</p><p class="c-article-references__links u-hide-print"><a data-track="click_references" rel="nofollow noopener" data-track-label="10.1016/j.media.2017.07.009" data-track-item_id="10.1016/j.media.2017.07.009" data-track-value="article reference" data-track-action="article reference" href="https://doi.org/10.1016%2Fj.media.2017.07.009" aria-label="Article reference 39" data-doi="10.1016/j.media.2017.07.009">Article</a>
    <a data-track="click_references" rel="nofollow noopener" data-track-label="link" data-track-item_id="link" data-track-value="pubmed reference" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=28783503" aria-label="PubMed reference 39">PubMed</a>
    <a data-track="click_references" data-track-action="google scholar reference" data-track-value="google scholar reference" data-track-label="link" data-track-item_id="link" rel="nofollow noopener" aria-label="Google Scholar reference 39" href="http://scholar.google.com/scholar_lookup?&amp;title=Supervised%20graph%20hashing%20for%20histopathology%20image%20retrieval%20and%20classification&amp;journal=Med%20Image%20Anal&amp;doi=10.1016%2Fj.media.2017.07.009&amp;volume=42&amp;pages=117-128&amp;publication_year=2017&amp;author=Shi%2CX&amp;author=Xing%2CF&amp;author=Xu%2CK&amp;author=Xie%2CY&amp;author=Su%2CH&amp;author=Yang%2CL">
                    Google Scholar</a>
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="40."><p class="c-article-references__text" id="ref-CR39">Liu Y, Chen H, Wang Y, Zhang P (2021) Power pooling: an adaptive pooling function for weakly labelled sound event detection. In: 2021 International joint conference on neural networks (IJCNN), pp. 17. IEEE</p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="41."><p class="c-article-references__text" id="ref-CR40">Wang X, Yan Y, Tang P, Bai X, Liu W (2018) Revisiting multiple instance neural networks. Pattern Recogn 74:1524</p><p class="c-article-references__links u-hide-print"><a data-track="click_references" rel="nofollow noopener" data-track-label="10.1016/j.patcog.2017.08.026" data-track-item_id="10.1016/j.patcog.2017.08.026" data-track-value="article reference" data-track-action="article reference" href="https://doi.org/10.1016%2Fj.patcog.2017.08.026" aria-label="Article reference 41" data-doi="10.1016/j.patcog.2017.08.026">Article</a>
    <a data-track="click_references" rel="nofollow noopener" data-track-label="link" data-track-item_id="link" data-track-value="ads reference" data-track-action="ads reference" href="http://adsabs.harvard.edu/cgi-bin/nph-data_query?link_type=ABSTRACT&amp;bibcode=2018PatRe..74...15W" aria-label="ADS reference 41">ADS</a>
    <a data-track="click_references" data-track-action="google scholar reference" data-track-value="google scholar reference" data-track-label="link" data-track-item_id="link" rel="nofollow noopener" aria-label="Google Scholar reference 41" href="http://scholar.google.com/scholar_lookup?&amp;title=Revisiting%20multiple%20instance%20neural%20networks&amp;journal=Pattern%20Recogn&amp;doi=10.1016%2Fj.patcog.2017.08.026&amp;volume=74&amp;pages=15-24&amp;publication_year=2018&amp;author=Wang%2CX&amp;author=Yan%2CY&amp;author=Tang%2CP&amp;author=Bai%2CX&amp;author=Liu%2CW">
                    Google Scholar</a>
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="42."><p class="c-article-references__text" id="ref-CR41">Li G, Li C, Wu G, Ji D, Zhang H (2021) Multi-view attention-guided multiple instance detection network for interpretable breast cancer histopathological image diagnosis. IEEE Access 9:7967179684</p><p class="c-article-references__links u-hide-print"><a data-track="click_references" rel="nofollow noopener" data-track-label="10.1109/ACCESS.2021.3084360" data-track-item_id="10.1109/ACCESS.2021.3084360" data-track-value="article reference" data-track-action="article reference" href="https://doi.org/10.1109%2FACCESS.2021.3084360" aria-label="Article reference 42" data-doi="10.1109/ACCESS.2021.3084360">Article</a>
    <a data-track="click_references" data-track-action="google scholar reference" data-track-value="google scholar reference" data-track-label="link" data-track-item_id="link" rel="nofollow noopener" aria-label="Google Scholar reference 42" href="http://scholar.google.com/scholar_lookup?&amp;title=Multi-view%20attention-guided%20multiple%20instance%20detection%20network%20for%20interpretable%20breast%20cancer%20histopathological%20image%20diagnosis&amp;journal=IEEE%20Access&amp;doi=10.1109%2FACCESS.2021.3084360&amp;volume=9&amp;pages=79671-79684&amp;publication_year=2021&amp;author=Li%2CG&amp;author=Li%2CC&amp;author=Wu%2CG&amp;author=Ji%2CD&amp;author=Zhang%2CH">
                    Google Scholar</a>
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="43."><p class="c-article-references__text" id="ref-CR42">Wang F, Jiang M, Qian C, Yang S, Li C, Zhang H, Wang X, Tang X (2017) Residual attention network for image classification. In: Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 31563164</p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="44."><p class="c-article-references__text" id="ref-CR43">Dosovitskiy A, Beyer L, Kolesnikov A, Weissenborn D, Zhai X, Unterthiner T, Dehghani M, Minderer M, Heigold G, Gelly S et al.(2020) An image is worth 16x16 words: transformers for image recognition at scale. arXiv preprint <a href="http://arxiv.org/abs/2010.11929" data-track="click_references" data-track-action="external reference" data-track-value="external reference" data-track-label="http://arxiv.org/abs/2010.11929">arXiv:2010.11929</a></p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="45."><p class="c-article-references__text" id="ref-CR44">Vaswani A, Shazeer N, Parmar N, Uszkoreit J, Jones L, Gomez AN, Kaiser , Polosukhin I (2017) Attention is all you need. Advances in neural information processing systems. Curran Associates, Inc., 30</p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="46."><p class="c-article-references__text" id="ref-CR45">Jang E, Gu S, Poole B (2017) Categorical Reparametrization with Gumbel-Softmax. In: Proceedings international conference on learning representations (ICLR). <a href="https://openreview.net/pdf?id=rkE3y85ee" data-track="click_references" data-track-action="external reference" data-track-value="external reference" data-track-label="https://openreview.net/pdf?id=rkE3y85ee">https://openreview.net/pdf?id=rkE3y85ee</a></p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="47."><p class="c-article-references__text" id="ref-CR46">Li X-C, Zhan D-C, Yang J-Q, Shi Y (2021) Deep multiple instance selection. Sci China Inf Sci 64(3):115</p><p class="c-article-references__links u-hide-print"><a data-track="click_references" rel="noopener" data-track-label="10.1007/s11432-020-3117-3" data-track-item_id="10.1007/s11432-020-3117-3" data-track-value="article reference" data-track-action="article reference" href="https://link.springer.com/doi/10.1007/s11432-020-3117-3" aria-label="Article reference 47" data-doi="10.1007/s11432-020-3117-3">Article</a>
    <a data-track="click_references" rel="nofollow noopener" data-track-label="link" data-track-item_id="link" data-track-value="mathscinet reference" data-track-action="mathscinet reference" href="http://www.ams.org/mathscinet-getitem?mr=4215367" aria-label="MathSciNet reference 47">MathSciNet</a>
    <a data-track="click_references" rel="nofollow noopener" data-track-label="link" data-track-item_id="link" data-track-value="cas reference" data-track-action="cas reference" href="/articles/cas-redirect/1:CAS:528:DC%2BB3cXhs1CjsbbN" aria-label="CAS reference 47">CAS</a>
    <a data-track="click_references" data-track-action="google scholar reference" data-track-value="google scholar reference" data-track-label="link" data-track-item_id="link" rel="nofollow noopener" aria-label="Google Scholar reference 47" href="http://scholar.google.com/scholar_lookup?&amp;title=Deep%20multiple%20instance%20selection&amp;journal=Sci%20China%20Inf%20Sci&amp;doi=10.1007%2Fs11432-020-3117-3&amp;volume=64&amp;issue=3&amp;pages=1-15&amp;publication_year=2021&amp;author=Li%2CX-C&amp;author=Zhan%2CD-C&amp;author=Yang%2CJ-Q&amp;author=Shi%2CY">
                    Google Scholar</a>
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="48."><p class="c-article-references__text" id="ref-CR47">LeCun Y, Cortes C, Burges C (2010) Mnist handwritten digit database. ATT Labs [Online]. Available: <a href="http://yann.lecun.com/exdb/mnist" data-track="click_references" data-track-action="external reference" data-track-value="external reference" data-track-label="http://yann.lecun.com/exdb/mnist">http://yann.lecun.com/exdb/mnist</a><b>2</b></p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="49."><p class="c-article-references__text" id="ref-CR48">Krizhevsky A, Hinton G (2009) Learning multiple layers of features from tiny images. Technical report, University of Toronto. <a href="https://www.cs.toronto.edu/~kriz/learning-features-2009-TR.pdf" data-track="click_references" data-track-action="external reference" data-track-value="external reference" data-track-label="https://www.cs.toronto.edu/~kriz/learning-features-2009-TR.pdf">https://www.cs.toronto.edu/~kriz/learning-features-2009-TR.pdf</a></p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="50."><p class="c-article-references__text" id="ref-CR49">Ghaznavi F, Evans A, Madabhushi A, Feldman M (2013) Digital imaging in pathology: whole-slide imaging and beyond. Annu Rev Pathol 8:331359</p><p class="c-article-references__links u-hide-print"><a data-track="click_references" rel="nofollow noopener" data-track-label="10.1146/annurev-pathol-011811-120902" data-track-item_id="10.1146/annurev-pathol-011811-120902" data-track-value="article reference" data-track-action="article reference" href="https://doi.org/10.1146%2Fannurev-pathol-011811-120902" aria-label="Article reference 50" data-doi="10.1146/annurev-pathol-011811-120902">Article</a>
    <a data-track="click_references" rel="nofollow noopener" data-track-label="link" data-track-item_id="link" data-track-value="cas reference" data-track-action="cas reference" href="/articles/cas-redirect/1:CAS:528:DC%2BC3sXltVKqs70%3D" aria-label="CAS reference 50">CAS</a>
    <a data-track="click_references" rel="nofollow noopener" data-track-label="link" data-track-item_id="link" data-track-value="pubmed reference" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=23157334" aria-label="PubMed reference 50">PubMed</a>
    <a data-track="click_references" data-track-action="google scholar reference" data-track-value="google scholar reference" data-track-label="link" data-track-item_id="link" rel="nofollow noopener" aria-label="Google Scholar reference 50" href="http://scholar.google.com/scholar_lookup?&amp;title=Digital%20imaging%20in%20pathology%3A%20whole-slide%20imaging%20and%20beyond&amp;journal=Annu%20Rev%20Pathol&amp;doi=10.1146%2Fannurev-pathol-011811-120902&amp;volume=8&amp;pages=331-359&amp;publication_year=2013&amp;author=Ghaznavi%2CF&amp;author=Evans%2CA&amp;author=Madabhushi%2CA&amp;author=Feldman%2CM">
                    Google Scholar</a>
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="51."><p class="c-article-references__text" id="ref-CR50">Dimitriou N, Arandjelovi O, Caie PD (2019) Deep learning for whole slide image analysis: an overview. Front Med 6:264</p><p class="c-article-references__links u-hide-print"><a data-track="click_references" rel="nofollow noopener" data-track-label="10.3389/fmed.2019.00264" data-track-item_id="10.3389/fmed.2019.00264" data-track-value="article reference" data-track-action="article reference" href="https://doi.org/10.3389%2Ffmed.2019.00264" aria-label="Article reference 51" data-doi="10.3389/fmed.2019.00264">Article</a>
    <a data-track="click_references" data-track-action="google scholar reference" data-track-value="google scholar reference" data-track-label="link" data-track-item_id="link" rel="nofollow noopener" aria-label="Google Scholar reference 51" href="http://scholar.google.com/scholar_lookup?&amp;title=Deep%20learning%20for%20whole%20slide%20image%20analysis%3A%20an%20overview&amp;journal=Front%20Med&amp;doi=10.3389%2Ffmed.2019.00264&amp;volume=6&amp;publication_year=2019&amp;author=Dimitriou%2CN&amp;author=Arandjelovi%C4%87%2CO&amp;author=Caie%2CPD">
                    Google Scholar</a>
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="52."><p class="c-article-references__text" id="ref-CR51">Asif A et al (2019) An embarrassingly simple approach to neural multiple instance classification. Pattern Recogn Lett 128:474479</p><p class="c-article-references__links u-hide-print"><a data-track="click_references" rel="nofollow noopener" data-track-label="10.1016/j.patrec.2019.10.022" data-track-item_id="10.1016/j.patrec.2019.10.022" data-track-value="article reference" data-track-action="article reference" href="https://doi.org/10.1016%2Fj.patrec.2019.10.022" aria-label="Article reference 52" data-doi="10.1016/j.patrec.2019.10.022">Article</a>
    <a data-track="click_references" rel="nofollow noopener" data-track-label="link" data-track-item_id="link" data-track-value="ads reference" data-track-action="ads reference" href="http://adsabs.harvard.edu/cgi-bin/nph-data_query?link_type=ABSTRACT&amp;bibcode=2019PaReL.128..474A" aria-label="ADS reference 52">ADS</a>
    <a data-track="click_references" data-track-action="google scholar reference" data-track-value="google scholar reference" data-track-label="link" data-track-item_id="link" rel="nofollow noopener" aria-label="Google Scholar reference 52" href="http://scholar.google.com/scholar_lookup?&amp;title=An%20embarrassingly%20simple%20approach%20to%20neural%20multiple%20instance%20classification&amp;journal=Pattern%20Recogn%20Lett&amp;doi=10.1016%2Fj.patrec.2019.10.022&amp;volume=128&amp;pages=474-479&amp;publication_year=2019&amp;author=Asif%2CA">
                    Google Scholar</a>
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="53."><p class="c-article-references__text" id="ref-CR52">Hahn M (2020) Theoretical limitations of self-attention in neural sequence models. Trans Assoc Comput Linguist 8:156171</p><p class="c-article-references__links u-hide-print"><a data-track="click_references" rel="nofollow noopener" data-track-label="10.1162/tacl_a_00306" data-track-item_id="10.1162/tacl_a_00306" data-track-value="article reference" data-track-action="article reference" href="https://doi.org/10.1162%2Ftacl_a_00306" aria-label="Article reference 53" data-doi="10.1162/tacl_a_00306">Article</a>
    <a data-track="click_references" data-track-action="google scholar reference" data-track-value="google scholar reference" data-track-label="link" data-track-item_id="link" rel="nofollow noopener" aria-label="Google Scholar reference 53" href="http://scholar.google.com/scholar_lookup?&amp;title=Theoretical%20limitations%20of%20self-attention%20in%20neural%20sequence%20models&amp;journal=Trans%20Assoc%20Comput%20Linguist&amp;doi=10.1162%2Ftacl_a_00306&amp;volume=8&amp;pages=156-171&amp;publication_year=2020&amp;author=Hahn%2CM">
                    Google Scholar</a>
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="54."><p class="c-article-references__text" id="ref-CR53">Frank E, Xu X (2008) Applying propositional learning algorithms to multi-instance data. Working paper series, Department of computer science, The University of Waikato. <a href="https://books.google.com/books?id=5eaGzgEACAAJ" data-track="click_references" data-track-action="external reference" data-track-value="external reference" data-track-label="https://books.google.com/books?id=5eaGzgEACAAJ">https://books.google.com/books?id=5eaGzgEACAAJ</a></p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="55."><p class="c-article-references__text" id="ref-CR54">Wang J, Zucker J-D (2000) Solving multiple-instance problem: a lazy learning approach. International Conference on Machine Learning. 1:11191126. <a href="https://api.semanticscholar.org/CorpusID:13896348" data-track="click_references" data-track-action="external reference" data-track-value="external reference" data-track-label="https://api.semanticscholar.org/CorpusID:13896348">https://api.semanticscholar.org/CorpusID:13896348</a></p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="56."><p class="c-article-references__text" id="ref-CR55">Wei X-S, Wu J, Zhou Z-H (2014) Scalable multi-instance learning. In: 2014 IEEE international conference on data mining, pp. 10371042. IEEE</p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="57."><p class="c-article-references__text" id="ref-CR56">Wilcoxon F (1992) Individual comparisons by ranking methods. In: Kotz S, Johnson NL (eds) Breakthroughs in statistics: methodology and distribution. Springer, Berlin, pp 196202</p><p class="c-article-references__links u-hide-print"><a data-track="click_references" rel="noopener" data-track-label="10.1007/978-1-4612-4380-9_16" data-track-item_id="10.1007/978-1-4612-4380-9_16" data-track-value="chapter reference" data-track-action="chapter reference" href="https://link.springer.com/doi/10.1007/978-1-4612-4380-9_16" aria-label="Chapter reference 57" data-doi="10.1007/978-1-4612-4380-9_16">Chapter</a>
    <a data-track="click_references" data-track-action="google scholar reference" data-track-value="google scholar reference" data-track-label="link" data-track-item_id="link" rel="nofollow noopener" aria-label="Google Scholar reference 57" href="http://scholar.google.com/scholar_lookup?&amp;title=Individual%20comparisons%20by%20ranking%20methods&amp;doi=10.1007%2F978-1-4612-4380-9_16&amp;pages=196-202&amp;publication_year=1992&amp;author=Wilcoxon%2CF">
                    Google Scholar</a>
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="58."><p class="c-article-references__text" id="ref-CR57">Conover WJ (1999) Practical nonparametric statistics, vol 350. Wiley, New York</p><p class="c-article-references__links u-hide-print"><a data-track="click_references" data-track-action="google scholar reference" data-track-value="google scholar reference" data-track-label="link" data-track-item_id="link" rel="nofollow noopener" aria-label="Google Scholar reference 58" href="http://scholar.google.com/scholar_lookup?&amp;title=Practical%20nonparametric%20statistics&amp;publication_year=1999&amp;author=Conover%2CWJ">
                    Google Scholar</a>
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="59."><p class="c-article-references__text" id="ref-CR58">Demar J (2006) Statistical comparisons of classifiers over multiple data sets. J Mach Learn Res 7:130</p><p class="c-article-references__links u-hide-print"><a data-track="click_references" rel="nofollow noopener" data-track-label="link" data-track-item_id="link" data-track-value="mathscinet reference" data-track-action="mathscinet reference" href="http://www.ams.org/mathscinet-getitem?mr=2274360" aria-label="MathSciNet reference 59">MathSciNet</a>
    <a data-track="click_references" data-track-action="google scholar reference" data-track-value="google scholar reference" data-track-label="link" data-track-item_id="link" rel="nofollow noopener" aria-label="Google Scholar reference 59" href="http://scholar.google.com/scholar_lookup?&amp;title=Statistical%20comparisons%20of%20classifiers%20over%20multiple%20data%20sets&amp;journal=J%20Mach%20Learn%20Res&amp;volume=7&amp;pages=1-30&amp;publication_year=2006&amp;author=Dem%C5%A1ar%2CJ">
                    Google Scholar</a>
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="60."><p class="c-article-references__text" id="ref-CR59">Friedman M (1937) The use of ranks to avoid the assumption of normality implicit in the analysis of variance. J Am Stat Assoc 32(200):675701</p><p class="c-article-references__links u-hide-print"><a data-track="click_references" rel="nofollow noopener" data-track-label="10.1080/01621459.1937.10503522" data-track-item_id="10.1080/01621459.1937.10503522" data-track-value="article reference" data-track-action="article reference" href="https://doi.org/10.1080%2F01621459.1937.10503522" aria-label="Article reference 60" data-doi="10.1080/01621459.1937.10503522">Article</a>
    <a data-track="click_references" data-track-action="google scholar reference" data-track-value="google scholar reference" data-track-label="link" data-track-item_id="link" rel="nofollow noopener" aria-label="Google Scholar reference 60" href="http://scholar.google.com/scholar_lookup?&amp;title=The%20use%20of%20ranks%20to%20avoid%20the%20assumption%20of%20normality%20implicit%20in%20the%20analysis%20of%20variance&amp;journal=J%20Am%20Stat%20Assoc&amp;doi=10.1080%2F01621459.1937.10503522&amp;volume=32&amp;issue=200&amp;pages=675-701&amp;publication_year=1937&amp;author=Friedman%2CM">
                    Google Scholar</a>
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="61."><p class="c-article-references__text" id="ref-CR60">LeCun Y, Bottou L, Bengio Y, Haffner P (1998) Gradient-based learning applied to document recognition. Proc IEEE 86(11):22782324</p><p class="c-article-references__links u-hide-print"><a data-track="click_references" rel="nofollow noopener" data-track-label="10.1109/5.726791" data-track-item_id="10.1109/5.726791" data-track-value="article reference" data-track-action="article reference" href="https://doi.org/10.1109%2F5.726791" aria-label="Article reference 61" data-doi="10.1109/5.726791">Article</a>
    <a data-track="click_references" data-track-action="google scholar reference" data-track-value="google scholar reference" data-track-label="link" data-track-item_id="link" rel="nofollow noopener" aria-label="Google Scholar reference 61" href="http://scholar.google.com/scholar_lookup?&amp;title=Gradient-based%20learning%20applied%20to%20document%20recognition&amp;journal=Proc%20IEEE&amp;doi=10.1109%2F5.726791&amp;volume=86&amp;issue=11&amp;pages=2278-2324&amp;publication_year=1998&amp;author=LeCun%2CY&amp;author=Bottou%2CL&amp;author=Bengio%2CY&amp;author=Haffner%2CP">
                    Google Scholar</a>
                </p></li></ol><p class="c-article-references__download u-hide-print"><a data-track="click" data-track-action="download citation references" data-track-label="link" rel="nofollow" href="https://citation-needed.springer.com/v2/references/10.1007/s00521-024-09417-3?format=refman&amp;flavour=references">Download references<svg width="16" height="16" focusable="false" role="img" aria-hidden="true" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#icon-eds-i-download-medium"></use></svg></a></p></div></div></div></section></div><section data-title="Acknowledgement"><div class="c-article-section" id="Ack1-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Ack1">Acknowledgement</h2><div class="c-article-section__content" id="Ack1-content"><p>This publication was jointly supported by Qatar University QUHI-CENG-22/23-548. The findings achieved herein are solely the responsibility of the authors. Open Access funding provided by the Qatar National Library.</p></div></div></section><section aria-labelledby="author-information" data-title="Author information"><div class="c-article-section" id="author-information-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="author-information">Author information</h2><div class="c-article-section__content" id="author-information-content"><h3 class="c-article__sub-heading" id="affiliations">Authors and Affiliations</h3><ol class="c-article-author-affiliation__list"><li id="Aff1"><p class="c-article-author-affiliation__address">FAST School of Computing, National University of Computer Emerging Science (FAST-NUCES), Karachi, Pakistan</p><p class="c-article-author-affiliation__authors-list">Muhammad Waqas&amp;Muhammad Atif Tahir</p></li><li id="Aff2"><p class="c-article-author-affiliation__address">Department of Imaging Physics, The University of Texas MD Anderson Cancer Center, Houston, TX, USA</p><p class="c-article-author-affiliation__authors-list">Muhammad Waqas&amp;Jia Wu</p></li><li id="Aff3"><p class="c-article-author-affiliation__address">College of information technology, United Arab Emirates University, Abu Dhabi, United Arab Emirates</p><p class="c-article-author-affiliation__authors-list">Muhammad Danish Author</p></li><li id="Aff4"><p class="c-article-author-affiliation__address">Department of Computer Science and Engineering, Qatar University, Doha, Qatar</p><p class="c-article-author-affiliation__authors-list">Sumaya Al-Maadeed</p></li><li id="Aff5"><p class="c-article-author-affiliation__address">Cybersecurity and Data Analytics Research Center, University of Sharjah, Sharjah, UAE</p><p class="c-article-author-affiliation__authors-list">Ahmed Bouridane</p></li></ol><div class="u-js-hide u-hide-print" data-test="author-info"><span class="c-article__sub-heading">Authors</span><ol class="c-article-authors-search u-list-reset"><li id="auth-Muhammad-Waqas-Aff1-Aff2"><span class="c-article-authors-search__title u-h3 js-search-name">Muhammad Waqas</span><div class="c-article-authors-search__list"><div class="c-article-authors-search__item c-article-authors-search__list-item--left"><a href="/search?sortBy=newestFirst&amp;dc.creator=Muhammad%20Waqas" class="c-article-button" data-track="click" data-track-action="author link - publication" data-track-label="link" rel="nofollow">View author publications</a></div><div class="c-article-authors-search__item c-article-authors-search__list-item--right"><p class="search-in-title-js c-article-authors-search__text"><span class="c-article-authors-search__links-text">Search author on:</span><span class="c-article-identifiers"><a class="c-article-identifiers__item" href="https://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=Muhammad%20Waqas" data-track="click" data-track-action="author link - pubmed" data-track-label="link" rel="nofollow">PubMed</a><span class="u-hide"></span><a class="c-article-identifiers__item" href="https://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Muhammad%20Waqas%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" data-track="click" data-track-action="author link - scholar" data-track-label="link" rel="nofollow">Google Scholar</a></span></p></div></div></li><li id="auth-Muhammad_Atif-Tahir-Aff1"><span class="c-article-authors-search__title u-h3 js-search-name">Muhammad Atif Tahir</span><div class="c-article-authors-search__list"><div class="c-article-authors-search__item c-article-authors-search__list-item--left"><a href="/search?sortBy=newestFirst&amp;dc.creator=Muhammad%20Atif%20Tahir" class="c-article-button" data-track="click" data-track-action="author link - publication" data-track-label="link" rel="nofollow">View author publications</a></div><div class="c-article-authors-search__item c-article-authors-search__list-item--right"><p class="search-in-title-js c-article-authors-search__text"><span class="c-article-authors-search__links-text">Search author on:</span><span class="c-article-identifiers"><a class="c-article-identifiers__item" href="https://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=Muhammad%20Atif%20Tahir" data-track="click" data-track-action="author link - pubmed" data-track-label="link" rel="nofollow">PubMed</a><span class="u-hide"></span><a class="c-article-identifiers__item" href="https://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Muhammad%20Atif%20Tahir%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" data-track="click" data-track-action="author link - scholar" data-track-label="link" rel="nofollow">Google Scholar</a></span></p></div></div></li><li id="auth-Muhammad_Danish-Author-Aff3"><span class="c-article-authors-search__title u-h3 js-search-name">Muhammad Danish Author</span><div class="c-article-authors-search__list"><div class="c-article-authors-search__item c-article-authors-search__list-item--left"><a href="/search?sortBy=newestFirst&amp;dc.creator=Muhammad%20Danish%20Author" class="c-article-button" data-track="click" data-track-action="author link - publication" data-track-label="link" rel="nofollow">View author publications</a></div><div class="c-article-authors-search__item c-article-authors-search__list-item--right"><p class="search-in-title-js c-article-authors-search__text"><span class="c-article-authors-search__links-text">Search author on:</span><span class="c-article-identifiers"><a class="c-article-identifiers__item" href="https://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=Muhammad%20Danish%20Author" data-track="click" data-track-action="author link - pubmed" data-track-label="link" rel="nofollow">PubMed</a><span class="u-hide"></span><a class="c-article-identifiers__item" href="https://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Muhammad%20Danish%20Author%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" data-track="click" data-track-action="author link - scholar" data-track-label="link" rel="nofollow">Google Scholar</a></span></p></div></div></li><li id="auth-Sumaya-Al_Maadeed-Aff4"><span class="c-article-authors-search__title u-h3 js-search-name">Sumaya Al-Maadeed</span><div class="c-article-authors-search__list"><div class="c-article-authors-search__item c-article-authors-search__list-item--left"><a href="/search?sortBy=newestFirst&amp;dc.creator=Sumaya%20Al-Maadeed" class="c-article-button" data-track="click" data-track-action="author link - publication" data-track-label="link" rel="nofollow">View author publications</a></div><div class="c-article-authors-search__item c-article-authors-search__list-item--right"><p class="search-in-title-js c-article-authors-search__text"><span class="c-article-authors-search__links-text">Search author on:</span><span class="c-article-identifiers"><a class="c-article-identifiers__item" href="https://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=Sumaya%20Al-Maadeed" data-track="click" data-track-action="author link - pubmed" data-track-label="link" rel="nofollow">PubMed</a><span class="u-hide"></span><a class="c-article-identifiers__item" href="https://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Sumaya%20Al-Maadeed%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" data-track="click" data-track-action="author link - scholar" data-track-label="link" rel="nofollow">Google Scholar</a></span></p></div></div></li><li id="auth-Ahmed-Bouridane-Aff5"><span class="c-article-authors-search__title u-h3 js-search-name">Ahmed Bouridane</span><div class="c-article-authors-search__list"><div class="c-article-authors-search__item c-article-authors-search__list-item--left"><a href="/search?sortBy=newestFirst&amp;dc.creator=Ahmed%20Bouridane" class="c-article-button" data-track="click" data-track-action="author link - publication" data-track-label="link" rel="nofollow">View author publications</a></div><div class="c-article-authors-search__item c-article-authors-search__list-item--right"><p class="search-in-title-js c-article-authors-search__text"><span class="c-article-authors-search__links-text">Search author on:</span><span class="c-article-identifiers"><a class="c-article-identifiers__item" href="https://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=Ahmed%20Bouridane" data-track="click" data-track-action="author link - pubmed" data-track-label="link" rel="nofollow">PubMed</a><span class="u-hide"></span><a class="c-article-identifiers__item" href="https://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Ahmed%20Bouridane%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" data-track="click" data-track-action="author link - scholar" data-track-label="link" rel="nofollow">Google Scholar</a></span></p></div></div></li><li id="auth-Jia-Wu-Aff2"><span class="c-article-authors-search__title u-h3 js-search-name">Jia Wu</span><div class="c-article-authors-search__list"><div class="c-article-authors-search__item c-article-authors-search__list-item--left"><a href="/search?sortBy=newestFirst&amp;dc.creator=Jia%20Wu" class="c-article-button" data-track="click" data-track-action="author link - publication" data-track-label="link" rel="nofollow">View author publications</a></div><div class="c-article-authors-search__item c-article-authors-search__list-item--right"><p class="search-in-title-js c-article-authors-search__text"><span class="c-article-authors-search__links-text">Search author on:</span><span class="c-article-identifiers"><a class="c-article-identifiers__item" href="https://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=Jia%20Wu" data-track="click" data-track-action="author link - pubmed" data-track-label="link" rel="nofollow">PubMed</a><span class="u-hide"></span><a class="c-article-identifiers__item" href="https://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Jia%20Wu%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" data-track="click" data-track-action="author link - scholar" data-track-label="link" rel="nofollow">Google Scholar</a></span></p></div></div></li></ol></div><h3 class="c-article__sub-heading" id="corresponding-author">Corresponding author</h3><p id="corresponding-author-list">Correspondence to
                <a id="corresp-c1" aria-label="email Muhammad Waqas" href="mailto:waqas.sheikh@nu.edu.pk">Muhammad Waqas</a>.</p></div></div></section><section data-title="Ethics declarations"><div class="c-article-section" id="ethics-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="ethics">Ethics declarations</h2><div class="c-article-section__content" id="ethics-content">
              
              
                <h3 class="c-article__sub-heading" id="FPar1">Conflict of interest</h3>
                <p>Authors declare no conflict of interest.</p>
              
            </div></div></section><section data-title="Additional information"><div class="c-article-section" id="additional-information-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="additional-information">Additional information</h2><div class="c-article-section__content" id="additional-information-content"><h3 class="c-article__sub-heading">Publisher's Note</h3><p>Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.</p></div></div></section><section data-title="Rights and permissions"><div class="c-article-section" id="rightslink-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="rightslink">Rights and permissions</h2><div class="c-article-section__content" id="rightslink-content">
                <p><b>Open Access</b>  This article is licensed under a Creative Commons Attribution 4.0 International License, which permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons licence, and indicate if changes were made. The images or other third party material in this article are included in the article's Creative Commons licence, unless indicated otherwise in a credit line to the material. If material is not included in the article's Creative Commons licence and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this licence, visit <a href="http://creativecommons.org/licenses/by/4.0/" rel="license">http://creativecommons.org/licenses/by/4.0/</a>.</p>
              <p class="c-article-rights"><a data-track="click" data-track-action="view rights and permissions" data-track-label="link" href="https://s100.copyright.com/AppDispatchServlet?title=Simultaneous%20instance%20pooling%20and%20bag%20representation%20selection%20approach%20for%20multiple-instance%20learning%20%28MIL%29%20using%20vision%20transformer&amp;author=Muhammad%20Waqas%20et%20al&amp;contentID=10.1007%2Fs00521-024-09417-3&amp;copyright=The%20Author%28s%29&amp;publication=0941-0643&amp;publicationDate=2024-02-16&amp;publisherName=SpringerNature&amp;orderBeanReset=true&amp;oa=CC%20BY">Reprints and permissions</a></p></div></div></section><section aria-labelledby="article-info" data-title="About this article"><div class="c-article-section" id="article-info-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="article-info">About this article</h2><div class="c-article-section__content" id="article-info-content"><div class="c-bibliographic-information"><div class="u-hide-print c-bibliographic-information__column c-bibliographic-information__column--border"><a data-crossmark="10.1007/s00521-024-09417-3" target="_blank" rel="noopener" href="https://crossmark.crossref.org/dialog/?doi=10.1007/s00521-024-09417-3" data-track="click" data-track-action="Click Crossmark" data-track-label="link" data-test="crossmark"><img loading="lazy" width="57" height="81" alt="Check for updates. Verify currency and authenticity via CrossMark" src="data:image/svg+xml;base64,<svg height="81" width="57" xmlns="http://www.w3.org/2000/svg"><g fill="none" fill-rule="evenodd"><path d="m17.35 35.45 21.3-14.2v-17.03h-21.3" fill="#989898"/><path d="m38.65 35.45-21.3-14.2v-17.03h21.3" fill="#747474"/><path d="m28 .5c-12.98 0-23.5 10.52-23.5 23.5s10.52 23.5 23.5 23.5 23.5-10.52 23.5-23.5c0-6.23-2.48-12.21-6.88-16.62-4.41-4.4-10.39-6.88-16.62-6.88zm0 41.25c-9.8 0-17.75-7.95-17.75-17.75s7.95-17.75 17.75-17.75 17.75 7.95 17.75 17.75c0 4.71-1.87 9.22-5.2 12.55s-7.84 5.2-12.55 5.2z" fill="#535353"/><path d="m41 36c-5.81 6.23-15.23 7.45-22.43 2.9-7.21-4.55-10.16-13.57-7.03-21.5l-4.92-3.11c-4.95 10.7-1.19 23.42 8.78 29.71 9.97 6.3 23.07 4.22 30.6-4.86z" fill="#9c9c9c"/><path d="m.2 58.45c0-.75.11-1.42.33-2.01s.52-1.09.91-1.5c.38-.41.83-.73 1.34-.94.51-.22 1.06-.32 1.65-.32.56 0 1.06.11 1.51.35.44.23.81.5 1.1.81l-.91 1.01c-.24-.24-.49-.42-.75-.56-.27-.13-.58-.2-.93-.2-.39 0-.73.08-1.05.23-.31.16-.58.37-.81.66-.23.28-.41.63-.53 1.04-.13.41-.19.88-.19 1.39 0 1.04.23 1.86.68 2.46.45.59 1.06.88 1.84.88.41 0 .77-.07 1.07-.23s.59-.39.85-.68l.91 1c-.38.43-.8.76-1.28.99-.47.22-1 .34-1.58.34-.59 0-1.13-.1-1.64-.31-.5-.2-.94-.51-1.31-.91-.38-.4-.67-.9-.88-1.48-.22-.59-.33-1.26-.33-2.02zm8.4-5.33h1.61v2.54l-.05 1.33c.29-.27.61-.51.96-.72s.76-.31 1.24-.31c.73 0 1.27.23 1.61.71.33.47.5 1.14.5 2.02v4.31h-1.61v-4.1c0-.57-.08-.97-.25-1.21-.17-.23-.45-.35-.83-.35-.3 0-.56.08-.79.22-.23.15-.49.36-.78.64v4.8h-1.61zm7.37 6.45c0-.56.09-1.06.26-1.51.18-.45.42-.83.71-1.14.29-.3.63-.54 1.01-.71.39-.17.78-.25 1.18-.25.47 0 .88.08 1.23.24.36.16.65.38.89.67s.42.63.54 1.03c.12.41.18.84.18 1.32 0 .32-.02.57-.07.76h-4.36c.07.62.29 1.1.65 1.44.36.33.82.5 1.38.5.29 0 .57-.04.83-.13s.51-.21.76-.37l.55 1.01c-.33.21-.69.39-1.09.53-.41.14-.83.21-1.26.21-.48 0-.92-.08-1.34-.25-.41-.16-.76-.4-1.07-.7-.31-.31-.55-.69-.72-1.13-.18-.44-.26-.95-.26-1.52zm4.6-.62c0-.55-.11-.98-.34-1.28-.23-.31-.58-.47-1.06-.47-.41 0-.77.15-1.07.45-.31.29-.5.73-.58 1.3zm2.5.62c0-.57.09-1.08.28-1.53.18-.44.43-.82.75-1.13s.69-.54 1.1-.71c.42-.16.85-.24 1.31-.24.45 0 .84.08 1.17.23s.61.34.85.57l-.77 1.02c-.19-.16-.38-.28-.56-.37-.19-.09-.39-.14-.61-.14-.56 0-1.01.21-1.35.63-.35.41-.52.97-.52 1.67 0 .69.17 1.24.51 1.66.34.41.78.62 1.32.62.28 0 .54-.06.78-.17.24-.12.45-.26.64-.42l.67 1.03c-.33.29-.69.51-1.08.65-.39.15-.78.23-1.18.23-.46 0-.9-.08-1.31-.24-.4-.16-.75-.39-1.05-.7s-.53-.69-.7-1.13c-.17-.45-.25-.96-.25-1.53zm6.91-6.45h1.58v6.17h.05l2.54-3.16h1.77l-2.35 2.8 2.59 4.07h-1.75l-1.77-2.98-1.08 1.23v1.75h-1.58zm13.69 1.27c-.25-.11-.5-.17-.75-.17-.58 0-.87.39-.87 1.16v.75h1.34v1.27h-1.34v5.6h-1.61v-5.6h-.92v-1.2l.92-.07v-.72c0-.35.04-.68.13-.98.08-.31.21-.57.4-.79s.42-.39.71-.51c.28-.12.63-.18 1.04-.18.24 0 .48.02.69.07.22.05.41.1.57.17zm.48 5.18c0-.57.09-1.08.27-1.53.17-.44.41-.82.72-1.13.3-.31.65-.54 1.04-.71.39-.16.8-.24 1.23-.24s.84.08 1.24.24c.4.17.74.4 1.04.71s.54.69.72 1.13c.19.45.28.96.28 1.53s-.09 1.08-.28 1.53c-.18.44-.42.82-.72 1.13s-.64.54-1.04.7-.81.24-1.24.24-.84-.08-1.23-.24-.74-.39-1.04-.7c-.31-.31-.55-.69-.72-1.13-.18-.45-.27-.96-.27-1.53zm1.65 0c0 .69.14 1.24.43 1.66.28.41.68.62 1.18.62.51 0 .9-.21 1.19-.62.29-.42.44-.97.44-1.66 0-.7-.15-1.26-.44-1.67-.29-.42-.68-.63-1.19-.63-.5 0-.9.21-1.18.63-.29.41-.43.97-.43 1.67zm6.48-3.44h1.33l.12 1.21h.05c.24-.44.54-.79.88-1.02.35-.24.7-.36 1.07-.36.32 0 .59.05.78.14l-.28 1.4-.33-.09c-.11-.01-.23-.02-.38-.02-.27 0-.56.1-.86.31s-.55.58-.77 1.1v4.2h-1.61zm-47.87 15h1.61v4.1c0 .57.08.97.25 1.2.17.24.44.35.81.35.3 0 .57-.07.8-.22.22-.15.47-.39.73-.73v-4.7h1.61v6.87h-1.32l-.12-1.01h-.04c-.3.36-.63.64-.98.86-.35.21-.76.32-1.24.32-.73 0-1.27-.24-1.61-.71-.33-.47-.5-1.14-.5-2.02zm9.46 7.43v2.16h-1.61v-9.59h1.33l.12.72h.05c.29-.24.61-.45.97-.63.35-.17.72-.26 1.1-.26.43 0 .81.08 1.15.24.33.17.61.4.84.71.24.31.41.68.53 1.11.13.42.19.91.19 1.44 0 .59-.09 1.11-.25 1.57-.16.47-.38.85-.65 1.16-.27.32-.58.56-.94.73-.35.16-.72.25-1.1.25-.3 0-.6-.07-.9-.2s-.59-.31-.87-.56zm0-2.3c.26.22.5.37.73.45.24.09.46.13.66.13.46 0 .84-.2 1.15-.6.31-.39.46-.98.46-1.77 0-.69-.12-1.22-.35-1.61-.23-.38-.61-.57-1.13-.57-.49 0-.99.26-1.52.77zm5.87-1.69c0-.56.08-1.06.25-1.51.16-.45.37-.83.65-1.14.27-.3.58-.54.93-.71s.71-.25 1.08-.25c.39 0 .73.07 1 .2.27.14.54.32.81.55l-.06-1.1v-2.49h1.61v9.88h-1.33l-.11-.74h-.06c-.25.25-.54.46-.88.64-.33.18-.69.27-1.06.27-.87 0-1.56-.32-2.07-.95s-.76-1.51-.76-2.65zm1.67-.01c0 .74.13 1.31.4 1.7.26.38.65.58 1.15.58.51 0 .99-.26 1.44-.77v-3.21c-.24-.21-.48-.36-.7-.45-.23-.08-.46-.12-.7-.12-.45 0-.82.19-1.13.59-.31.39-.46.95-.46 1.68zm6.35 1.59c0-.73.32-1.3.97-1.71.64-.4 1.67-.68 3.08-.84 0-.17-.02-.34-.07-.51-.05-.16-.12-.3-.22-.43s-.22-.22-.38-.3c-.15-.06-.34-.1-.58-.1-.34 0-.68.07-1 .2s-.63.29-.93.47l-.59-1.08c.39-.24.81-.45 1.28-.63.47-.17.99-.26 1.54-.26.86 0 1.51.25 1.93.76s.63 1.25.63 2.21v4.07h-1.32l-.12-.76h-.05c-.3.27-.63.48-.98.66s-.73.27-1.14.27c-.61 0-1.1-.19-1.48-.56-.38-.36-.57-.85-.57-1.46zm1.57-.12c0 .3.09.53.27.67.19.14.42.21.71.21.28 0 .54-.07.77-.2s.48-.31.73-.56v-1.54c-.47.06-.86.13-1.18.23-.31.09-.57.19-.76.31s-.33.25-.41.4c-.09.15-.13.31-.13.48zm6.29-3.63h-.98v-1.2l1.06-.07.2-1.88h1.34v1.88h1.75v1.27h-1.75v3.28c0 .8.32 1.2.97 1.2.12 0 .24-.01.37-.04.12-.03.24-.07.34-.11l.28 1.19c-.19.06-.4.12-.64.17-.23.05-.49.08-.76.08-.4 0-.74-.06-1.02-.18-.27-.13-.49-.3-.67-.52-.17-.21-.3-.48-.37-.78-.08-.3-.12-.64-.12-1.01zm4.36 2.17c0-.56.09-1.06.27-1.51s.41-.83.71-1.14c.29-.3.63-.54 1.01-.71.39-.17.78-.25 1.18-.25.47 0 .88.08 1.23.24.36.16.65.38.89.67s.42.63.54 1.03c.12.41.18.84.18 1.32 0 .32-.02.57-.07.76h-4.37c.08.62.29 1.1.65 1.44.36.33.82.5 1.38.5.3 0 .58-.04.84-.13.25-.09.51-.21.76-.37l.54 1.01c-.32.21-.69.39-1.09.53s-.82.21-1.26.21c-.47 0-.92-.08-1.33-.25-.41-.16-.77-.4-1.08-.7-.3-.31-.54-.69-.72-1.13-.17-.44-.26-.95-.26-1.52zm4.61-.62c0-.55-.11-.98-.34-1.28-.23-.31-.58-.47-1.06-.47-.41 0-.77.15-1.08.45-.31.29-.5.73-.57 1.3zm3.01 2.23c.31.24.61.43.92.57.3.13.63.2.98.2.38 0 .65-.08.83-.23s.27-.35.27-.6c0-.14-.05-.26-.13-.37-.08-.1-.2-.2-.34-.28-.14-.09-.29-.16-.47-.23l-.53-.22c-.23-.09-.46-.18-.69-.3-.23-.11-.44-.24-.62-.4s-.33-.35-.45-.55c-.12-.21-.18-.46-.18-.75 0-.61.23-1.1.68-1.49.44-.38 1.06-.57 1.83-.57.48 0 .91.08 1.29.25s.71.36.99.57l-.74.98c-.24-.17-.49-.32-.73-.42-.25-.11-.51-.16-.78-.16-.35 0-.6.07-.76.21-.17.15-.25.33-.25.54 0 .14.04.26.12.36s.18.18.31.26c.14.07.29.14.46.21l.54.19c.23.09.47.18.7.29s.44.24.64.4c.19.16.34.35.46.58.11.23.17.5.17.82 0 .3-.06.58-.17.83-.12.26-.29.48-.51.68-.23.19-.51.34-.84.45-.34.11-.72.17-1.15.17-.48 0-.95-.09-1.41-.27-.46-.19-.86-.41-1.2-.68z" fill="#535353"/></g></svg>"></a></div><div class="c-bibliographic-information__column"><h3 class="c-article__sub-heading" id="citeas">Cite this article</h3><p class="c-bibliographic-information__citation">Waqas, M., Tahir, M.A., Author, M.D. <i>et al.</i> Simultaneous instance pooling and bag representation selection approach for multiple-instance learning (MIL) using vision transformer.
                    <i>Neural Comput &amp; Applic</i> <b>36</b>, 66596680 (2024). https://doi.org/10.1007/s00521-024-09417-3</p><p class="c-bibliographic-information__download-citation u-hide-print"><a data-test="citation-link" data-track="click" data-track-action="download article citation" data-track-label="link" data-track-external="" rel="nofollow" href="https://citation-needed.springer.com/v2/references/10.1007/s00521-024-09417-3?format=refman&amp;flavour=citation">Download citation<svg width="16" height="16" focusable="false" role="img" aria-hidden="true" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#icon-eds-i-download-medium"></use></svg></a></p><ul class="c-bibliographic-information__list" data-test="publication-history"><li class="c-bibliographic-information__list-item"><p>Received<span class="u-hide">: </span><span class="c-bibliographic-information__value"><time datetime="2023-04-10">10 April 2023</time></span></p></li><li class="c-bibliographic-information__list-item"><p>Accepted<span class="u-hide">: </span><span class="c-bibliographic-information__value"><time datetime="2024-01-14">14 January 2024</time></span></p></li><li class="c-bibliographic-information__list-item"><p>Published<span class="u-hide">: </span><span class="c-bibliographic-information__value"><time datetime="2024-02-16">16 February 2024</time></span></p></li><li class="c-bibliographic-information__list-item"><p>Version of record<span class="u-hide">: </span><span class="c-bibliographic-information__value"><time datetime="2024-02-16">16 February 2024</time></span></p></li><li class="c-bibliographic-information__list-item"><p>Issue date<span class="u-hide">: </span><span class="c-bibliographic-information__value"><time datetime="2024-04">April 2024</time></span></p></li><li class="c-bibliographic-information__list-item c-bibliographic-information__list-item--full-width"><p><abbr title="Digital Object Identifier">DOI</abbr><span class="u-hide">: </span><span class="c-bibliographic-information__value">https://doi.org/10.1007/s00521-024-09417-3</span></p></li></ul><div data-component="share-box"><div class="c-article-share-box u-display-none" hidden=""><h3 class="c-article__sub-heading">Share this article</h3><p class="c-article-share-box__description">Anyone you share the following link with will be able to read this content:</p><button class="js-get-share-url c-article-share-box__button" type="button" id="get-share-url" data-track="click" data-track-label="button" data-track-external="" data-track-action="get shareable link">Get shareable link</button><div class="js-no-share-url-container u-display-none" hidden=""><p class="js-c-article-share-box__no-sharelink-info c-article-share-box__no-sharelink-info">Sorry, a shareable link is not currently available for this article.</p></div><div class="js-share-url-container u-display-none" hidden=""><p class="js-share-url c-article-share-box__only-read-input" id="share-url" data-track="click" data-track-label="button" data-track-action="select share url"></p><button class="js-copy-share-url c-article-share-box__button--link-like" type="button" id="copy-share-url" data-track="click" data-track-label="button" data-track-action="copy share url" data-track-external="">Copy shareable link to clipboard</button></div><p class="js-c-article-share-box__additional-info c-article-share-box__additional-info">
                            Provided by the Springer Nature SharedIt content-sharing initiative
                        </p></div></div><h3 class="c-article__sub-heading">Keywords</h3><ul class="c-article-subject-list"><li class="c-article-subject-list__subject"><span><a href="/search?query=Multiple-instance%20learning%20%28MIL%29&amp;facet-discipline=&#34;Computer%20Science&#34;" data-track="click" data-track-action="view keyword" data-track-label="link">Multiple-instance learning (MIL)</a></span></li><li class="c-article-subject-list__subject"><span><a href="/search?query=Vision%20transformers&amp;facet-discipline=&#34;Computer%20Science&#34;" data-track="click" data-track-action="view keyword" data-track-label="link">Vision transformers</a></span></li><li class="c-article-subject-list__subject"><span><a href="/search?query=Attention-based%20pooling&amp;facet-discipline=&#34;Computer%20Science&#34;" data-track="click" data-track-action="view keyword" data-track-label="link">Attention-based pooling</a></span></li><li class="c-article-subject-list__subject"><span><a href="/search?query=Bag%20representation%20selection&amp;facet-discipline=&#34;Computer%20Science&#34;" data-track="click" data-track-action="view keyword" data-track-label="link">Bag representation selection</a></span></li></ul><div data-component="article-info-list"></div></div></div></div></div></section>

                    
    <div id="researcher-profile-container">
        <h3>Profiles</h3>
        <ol>
            
                <li data-test="researcher-profile-data"  data-profile-index="0">
                    <span data-test="researcher-profile-name">Muhammad Waqas</span>
                    <a class="js-cta-popup-link c-article-authors-search__cta-link" href="/researchers/58175545SN" data-track="click_view_profile" data-test="researcher-profile-link">
                        <span class="eds-c-button eds-c-button--primary">
                            <svg class="c-article-authors-search__cta-icon" aria-hidden="true" focusable="false" width="24" height="24">
                                <use xlink:href="#icon-eds-i-user-single-medium"></use>
                            </svg><span>View author profile</span>
                        </span>
                    </a>
                </li>
            
        </ol>
    </div>


                    
                </div>
            </main>

            <div class="c-article-sidebar u-text-sm u-hide-print l-with-sidebar__sidebar" id="sidebar"
                 data-container-type="reading-companion" data-track-component="reading companion">
                <aside aria-label="reading companion">
                    

                    
                        
    <div class="app-card-service" data-test="article-checklist-banner">
        <div>
            <a class="app-card-service__link" data-track="click_presubmission_checklist" data-track-context="article page top of reading companion" data-track-category="pre-submission-checklist" data-track-action="clicked article page checklist banner test 2 old version" data-track-label="link" href="https://beta.springernature.com/pre-submission?journalId=521"
            data-test="article-checklist-banner-link">
            <span class="app-card-service__link-text">Use our pre-submission checklist</span>
            <svg class="app-card-service__link-icon" aria-hidden="true" focusable="false"><use xlink:href="#icon-eds-i-arrow-right-small"></use></svg>
            </a>
            <p class="app-card-service__description">Avoid common mistakes on your manuscript.</p>
        </div>
        <div class="app-card-service__icon-container">
            <svg class="app-card-service__icon" aria-hidden="true" focusable="false">
                <use xlink:href="#icon-eds-i-clipboard-check-medium"></use>
            </svg>
        </div>
    </div>

                    

                    
                        <div data-test="collections">
                            
    

                        </div>
                    

                    <div data-test="editorial-summary">
                        
                    </div>

                    <div class="c-reading-companion">
                        <div class="c-reading-companion__sticky" data-component="reading-companion-sticky"
                             data-test="reading-companion-sticky">
                            
                            
                                
                                    
                                
                            
                            <div
                                class="c-reading-companion__panel c-reading-companion__sections c-reading-companion__panel--active"
                                id="tabpanel-sections">
                                <div class="u-lazy-ad-wrapper u-mt-16 u-hide"
                                     data-component-mpu><div class="c-ad c-ad--300x250">
    <div class="c-ad__inner">
        <p class="c-ad__label">Advertisement</p>
        <div id="div-gpt-ad-MPU1"
             class="div-gpt-ad grade-c-hide"
             data-pa11y-ignore
             data-gpt
             data-gpt-unitpath="/270604982/springerlink/521/article"
             data-gpt-sizes="300x250" data-test="MPU1-ad"
             data-gpt-targeting="pos=MPU1;articleid=s00521-024-09417-3;">
        </div>
    </div>
</div>

<script>
    window.SN = window.SN || {};
    window.SN.libs = window.SN.libs || {};
    window.SN.libs.ads = window.SN.libs.ads || {};
    window.SN.libs.ads.slotConfig = window.SN.libs.ads.slotConfig || {};
    window.SN.libs.ads.slotConfig['MPU1'] = {
        'pos': 'MPU1',
        'type': 'MPU1',
    };
    window.SN.libs.ads.slotConfig['unitPath'] = '/270604982/springerlink/521/article';
</script>

</div>
                            </div>
                            <div
                                class="c-reading-companion__panel c-reading-companion__figures c-reading-companion__panel--full-width"
                                id="tabpanel-figures"></div>
                            <div
                                class="c-reading-companion__panel c-reading-companion__references c-reading-companion__panel--full-width"
                                id="tabpanel-references"></div>
                        </div>
                    </div>
                </aside>
            </div>
        </div>
    </article>
    
    <div class="app-elements" data-test="footer">
    <nav aria-label="expander navigation">



    
        <div class="eds-c-header__expander eds-c-header__expander--search" id="eds-c-header-popup-search">
            <h2 class="eds-c-header__heading">Search</h2>
            <div class="u-container">
                <search class="eds-c-header__search" role="search" aria-label="Search from the header">
                    <form method="GET" action="//link.springer.com/search"
                        
                            data-test="header-search"
                        
                            data-track="search"
                        
                            data-track-context="search from header"
                        
                            data-track-action="submit search form"
                        
                            data-track-category="unified header"
                        
                            data-track-label="form"
                        
					>
                        <label for="eds-c-header-search" class="eds-c-header__search-label">Search by keyword or author</label>
                        <div class="eds-c-header__search-container">
                            <input id="eds-c-header-search" class="eds-c-header__search-input" autocomplete="off" name="query" type="search" value="" required>
                            <button class="eds-c-header__search-button" type="submit">
                                <svg class="eds-c-header__icon" aria-hidden="true" focusable="false">
                                    <use xlink:href="#icon-eds-i-search-medium"></use>
                                </svg>
                                <span class="u-visually-hidden">Search</span>
                            </button>
                        </div>
                    </form>
                </search>
            </div>
        </div>
    


<div class="eds-c-header__expander eds-c-header__expander--menu" id="eds-c-header-nav">
    
        <h2 class="eds-c-header__heading">Navigation</h2>
        <ul class="eds-c-header__list">
            
                <li class="eds-c-header__list-item">
                   <a class="eds-c-header__link" href="https://link.springer.com/journals/"
                        
                            data-track="nav_find_a_journal"
                        
                            data-track-context="unified header"
                        
                            data-track-action="click find a journal"
                        
                            data-track-category="unified header"
                        
                            data-track-label="link"
                        
					>
                        Find a journal
                    </a>
                </li>
            
                <li class="eds-c-header__list-item">
                   <a class="eds-c-header__link" href="https://www.springernature.com/gp/authors"
                        
                            data-track="nav_how_to_publish"
                        
                            data-track-context="unified header"
                        
                            data-track-action="click publish with us link"
                        
                            data-track-category="unified header"
                        
                            data-track-label="link"
                        
					>
                        Publish with us
                    </a>
                </li>
            
                <li class="eds-c-header__list-item">
                   <a class="eds-c-header__link" href="https://link.springernature.com/home/"
                        
                            data-track="nav_track_your_research"
                        
                            data-track-context="unified header"
                        
                            data-track-action="click track your research"
                        
                            data-track-category="unified header"
                        
                            data-track-label="link"
                        
					>
                        Track your research
                    </a>
                </li>
            
        </ul>
    
</div>
</nav>
    <footer >
	<div class="eds-c-footer"
		
	>
		
			
				<div class="eds-c-footer__container">
		<div class="eds-c-footer__grid eds-c-footer__group--separator">
			
			<div class="eds-c-footer__group">
				<h3 class="eds-c-footer__heading">Discover content</h3>
				<ul class="eds-c-footer__list">
					
						<li class="eds-c-footer__item"><a class="eds-c-footer__link" href="https://link.springer.com/journals/a/1" data-track="nav_journals_a_z" data-track-action="journals a-z" data-track-context="unified footer" data-track-label="link">Journals A-Z</a></li>
					
						<li class="eds-c-footer__item"><a class="eds-c-footer__link" href="https://link.springer.com/books/a/1" data-track="nav_books_a_z" data-track-action="books a-z" data-track-context="unified footer" data-track-label="link">Books A-Z</a></li>
					
				</ul>
			</div>
			
			<div class="eds-c-footer__group">
				<h3 class="eds-c-footer__heading">Publish with us</h3>
				<ul class="eds-c-footer__list">
					
						<li class="eds-c-footer__item"><a class="eds-c-footer__link" href="https://link.springer.com/journals" data-track="nav_journal_finder" data-track-action="journal finder" data-track-context="unified footer" data-track-label="link">Journal finder</a></li>
					
						<li class="eds-c-footer__item"><a class="eds-c-footer__link" href="https://www.springernature.com/gp/authors" data-track="nav_publish_your_research" data-track-action="publish your research" data-track-context="unified footer" data-track-label="link">Publish your research</a></li>
					
						<li class="eds-c-footer__item"><a class="eds-c-footer__link" href="https://authorservices.springernature.com/go/sn/?utm_source&#x3D;SNLinkfooter&amp;utm_medium&#x3D;Web&amp;utm_campaign&#x3D;SNReferral" data-track="nav_language_editing" data-track-action="language editing" data-track-context="unified footer" data-track-label="link">Language editing</a></li>
					
						<li class="eds-c-footer__item"><a class="eds-c-footer__link" href="https://www.springernature.com/gp/open-science/about/the-fundamentals-of-open-access-and-open-research" data-track="nav_open_access_publishing" data-track-action="open access publishing" data-track-context="unified footer" data-track-label="link">Open access publishing</a></li>
					
				</ul>
			</div>
			
			<div class="eds-c-footer__group">
				<h3 class="eds-c-footer__heading">Products and services</h3>
				<ul class="eds-c-footer__list">
					
						<li class="eds-c-footer__item"><a class="eds-c-footer__link" href="https://www.springernature.com/gp/products" data-track="nav_our_products" data-track-action="our products" data-track-context="unified footer" data-track-label="link">Our products</a></li>
					
						<li class="eds-c-footer__item"><a class="eds-c-footer__link" href="https://www.springernature.com/gp/librarians" data-track="nav_librarians" data-track-action="librarians" data-track-context="unified footer" data-track-label="link">Librarians</a></li>
					
						<li class="eds-c-footer__item"><a class="eds-c-footer__link" href="https://www.springernature.com/gp/societies" data-track="nav_societies" data-track-action="societies" data-track-context="unified footer" data-track-label="link">Societies</a></li>
					
						<li class="eds-c-footer__item"><a class="eds-c-footer__link" href="https://www.springernature.com/gp/partners" data-track="nav_partners_and_advertisers" data-track-action="partners and advertisers" data-track-context="unified footer" data-track-label="link">Partners and advertisers</a></li>
					
				</ul>
			</div>
			
			<div class="eds-c-footer__group">
				<h3 class="eds-c-footer__heading">Our brands</h3>
				<ul class="eds-c-footer__list">
					
						<li class="eds-c-footer__item"><a class="eds-c-footer__link" href="https://link.springer.com/brands/springer" data-track="nav_imprint_Springer" data-track-action="Springer" data-track-context="unified footer" data-track-label="link">Springer</a></li>
					
						<li class="eds-c-footer__item"><a class="eds-c-footer__link" href="https://www.nature.com/" data-track="nav_imprint_Nature_Portfolio" data-track-action="Nature Portfolio" data-track-context="unified footer" data-track-label="link">Nature Portfolio</a></li>
					
						<li class="eds-c-footer__item"><a class="eds-c-footer__link" href="https://link.springer.com/brands/bmc" data-track="nav_imprint_BMC" data-track-action="BMC" data-track-context="unified footer" data-track-label="link">BMC</a></li>
					
						<li class="eds-c-footer__item"><a class="eds-c-footer__link" href="/brands/palgrave" data-track="nav_imprint_Palgrave_Macmillan" data-track-action="Palgrave Macmillan" data-track-context="unified footer" data-track-label="link">Palgrave Macmillan</a></li>
					
						<li class="eds-c-footer__item"><a class="eds-c-footer__link" href="https://www.apress.com/" data-track="nav_imprint_Apress" data-track-action="Apress" data-track-context="unified footer" data-track-label="link">Apress</a></li>
					
						<li class="eds-c-footer__item"><a class="eds-c-footer__link" href="https://link.springer.com/brands/discover" data-track="nav_imprint_Discover" data-track-action="Discover" data-track-context="unified footer" data-track-label="link">Discover</a></li>
					
				</ul>
			</div>
			
		</div>
	</div>

		
		
		<div class="eds-c-footer__container">
	
		<nav aria-label="footer navigation">
			<ul class="eds-c-footer__links">
				
					<li class="eds-c-footer__item">
						
						
							<button class="eds-c-footer__link" data-cc-action="preferences"
								 data-track="dialog_manage_cookies" data-track-action="Manage cookies" data-track-context="unified footer" data-track-label="link"><span class="eds-c-footer__button-text">Your privacy choices/Manage cookies</span></button>
						
					</li>
				
					<li class="eds-c-footer__item">
						
							<a class="eds-c-footer__link" href="https://www.springernature.com/gp/legal/ccpa"
								 data-track="nav_california_privacy_statement" data-track-action="california privacy statement" data-track-context="unified footer" data-track-label="link">Your US state privacy rights</a>
						
						
					</li>
				
					<li class="eds-c-footer__item">
						
							<a class="eds-c-footer__link" href="https://link.springer.com/accessibility"
								 data-track="nav_accessibility_statement" data-track-action="accessibility statement" data-track-context="unified footer" data-track-label="link">Accessibility statement</a>
						
						
					</li>
				
					<li class="eds-c-footer__item">
						
							<a class="eds-c-footer__link" href="https://link.springer.com/termsandconditions"
								 data-track="nav_terms_and_conditions" data-track-action="terms and conditions" data-track-context="unified footer" data-track-label="link">Terms and conditions</a>
						
						
					</li>
				
					<li class="eds-c-footer__item">
						
							<a class="eds-c-footer__link" href="https://link.springer.com/privacystatement"
								 data-track="nav_privacy_policy" data-track-action="privacy policy" data-track-context="unified footer" data-track-label="link">Privacy policy</a>
						
						
					</li>
				
					<li class="eds-c-footer__item">
						
							<a class="eds-c-footer__link" href="https://support.springernature.com/en/support/home"
								 data-track="nav_help_and_support" data-track-action="help and support" data-track-context="unified footer" data-track-label="link">Help and support</a>
						
						
					</li>
				
					<li class="eds-c-footer__item">
						
							<a class="eds-c-footer__link" href="https://link.springer.com/legal-notice"
								 data-track="nav_legal_notice" data-track-action="legal notice" data-track-context="unified footer" data-track-label="link">Legal notice</a>
						
						
					</li>
				
					<li class="eds-c-footer__item">
						
							<a class="eds-c-footer__link" href="https://support.springernature.com/en/support/solutions/articles/6000255911-subscription-cancellations"
								 data-track-action="cancel contracts here">Cancel contracts here</a>
						
						
					</li>
				
			</ul>
		</nav>
	
	
		
			<div class="eds-c-footer__user">
				<p class="eds-c-footer__user-info">
					
					<span data-test="footer-user-ip">71.193.247.28</span>
				</p>
				<p class="eds-c-footer__user-info" data-test="footer-business-partners">Not affiliated</p>
			</div>
		
	
	
		<a href="https://www.springernature.com/" class="eds-c-footer__link">
			<img src="/oscar-static/images/logo-springernature-white-19dd4ba190.svg" alt="Springer Nature" loading="lazy" width="200" height="20"/>
		</a>
	
	<p class="eds-c-footer__legal" data-test="copyright">&copy; 2025 Springer Nature</p>
</div>

	</div>
</footer>
</div>


    </body>
</html>


