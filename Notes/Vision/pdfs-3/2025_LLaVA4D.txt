                                              LLaVA-4D: Embedding SpatioTemporal Prompt
                                                 into LMMs for 4D Scene Understanding


                                                                               Hanyu Zhou1 , Gim Hee Lee1
                                                                1
                                                                    School of Computing, National University of Singapore
                                                                         {hy.zhou, gimhee.lee}@nus.edu.sg
arXiv:2505.12253v1 [cs.CV] 18 May 2025




                                                                                        Abstract

                                                     Despite achieving significant progress in 2D image understanding, large multi-
                                                     modal models (LMMs) struggle in the physical world due to the lack of spatial
                                                     representation. Typically, existing 3D LMMs mainly embed 3D positions as fixed
                                                     spatial prompts within visual features to represent the scene. However, these
                                                     methods are limited to understanding the static background and fail to capture
                                                     temporally varying dynamic objects. In this paper, we propose LLaVA-4D, a
                                                     general LMM framework with a novel spatiotemporal prompt for visual repre-
                                                     sentation in 4D scene understanding. The spatiotemporal prompt is generated by
                                                     encoding 3D position and 1D time into a dynamic-aware 4D coordinate embedding.
                                                     Moreover, we demonstrate that spatial and temporal components disentangled from
                                                     visual features are more effective in distinguishing the background from objects.
                                                     This motivates embedding the 4D spatiotemporal prompt into these features to
                                                     enhance the dynamic scene representation. By aligning visual spatiotemporal
                                                     embeddings with language embeddings, LMMs gain the ability to understand both
                                                     spatial and temporal characteristics of static background and dynamic objects in
                                                     the physical world. Additionally, we construct a 4D vision-language dataset with
                                                     spatiotemporal coordinate annotations for instruction fine-tuning LMMs. Extensive
                                                     experiments have been conducted to demonstrate the effectiveness of our method
                                                     across different tasks in 4D scene understanding.


                                         1    Introduction

                                         Large multimodal models (LMMs) [1, 2] aim to learn the representation alignment between language
                                         and other modalities such as vision [3] and audio [4]. They have been widely applied in multiple scene
                                         understanding tasks such as dense caption [5], visual QA [6, 7], scene grounding [8], etc. Although
                                         recent language-vision LMMs including LLaVA [2] and PaLI [9] have achieved great success in
                                         2D image understanding, they still face challenges in the 3D physical world. This is because these
                                         LMMs trained solely on 2D images lack the representation of 3D spatial characteristics to interact
                                         with the physical world. In this work, our purpose is to improve the characteristic representation and
                                         scene understanding of LMMs for the physical world.
                                         As shown in Fig. 1(a), existing 3D language-vision LMM methods [10, 11, 11] use 3D positions
                                         as spatial prompts which are then embedded into visual features to represent the whole scene. For
                                         example, Hong et al. [10] extract 2D visual features from multi-view images and use 3D positions to
                                         transform these features into corresponding 3D inputs for LMMs. However, these 3D LMM methods
                                         can only handle the static background with limited ability to understand dynamic objects in the scene.
                                         Unlike static backgrounds, dynamic objects exhibit temporally varying spatial characteristics such
                                         as position shifts and deformations. Unfortunately, existing 3D LMM frameworks neglect temporal
                                         aspects in using a unified spatial representation for the entire scene. As shown in Fig. 1(c-d), 3D


                                         Preprint.
                                                                                                 This is a black car that is slowly moving along the road. At t1,                                             Scan2Cap (C@0.5)
    It is a black car. It is at [x, y, z].
                                                                                                 it is at [x1, y1, z1]. At t2, it is at [x2, y2, z2]. ...
                                                                                                                                                                                                                    85.3

                            Large Language Model                                                                      Large Language Model
                                                                                                                                                                                        Chat4D                                            17.9 ScanQA
                                                                                                                                                                                                    54.6
                                     Spatial Prompt                                                                      SpatioTemporal Prompt                                          (TAcc)                                                     (B-4)

                                       x y z                                                                                  x y z t
                                 Embed                                                                                 Embed
        Vision                                3D Position                 What is on                  Vision                        4D Coordinate                 What is on
       Encoder                                 Encoding                   the    road?               Encoder                          Encoding                    the    road?                    58.9                                      53.2
                                                                          Please pro-                                                                             Please pro-           Chat4D                                                  ScanRef
                                                                          vide its co-                                                                            vide its co-        (SAcc@0.5)                                              (SAcc@0.5)
                                                                          ordinates.                                                                              ordinates.
                                             ...                                                                                     ...                                                                              54.3
                                                                                                                                                                                                            Multi3DRefer (F1@0.5)

                                                                                                                                                                                         LL3DA      PQ3D      LLaVA-3D       Video-3D LLM       LLaVA-4D
                 Multi-View Images                                        Instruction                        Multi-View Videos                                    Instruction
                                     (a) 3D LMM                                                                       (b) 4D LMM (Ours)                                                (c) Performance on Multiple Benchmarks
                                                   p=(4.30, 1.71, 0.18,
                                                    4.85, 1.19, 0.28)       4D Dense Caption                                        4D Visual Grounding                        4D Question Answering
                                                                                v1, t1           v2, t1           v2, t2            Q: Which object is the person lifting      Q: A object is located at [2.60, 0.34, 0.29]. Please describe this object.
                                                                                                                                    up near the wall? Please provide its       3D LMM: A green inflatable dinosaur with striped patterns and stubby
                                                                                                                                    coordinates.                               limbs, positioned horizontally and held at the front by a person in plaid.
                                                                                                                                    3D LMM: The object is a green              4D LMM: A shiny green inflatable dinosaur with dark stripes and short
                                                                                                                                    inflatable dinosaur. It is at [2.26,       limbs is being held by a person in plaid shirt and lifted upward in front of
                                                                            3D LMM: A man in sunglasses holds a green               0.54, 0.23, 3.20, 0.21, 0.39].             a mural wall.
                                 Point                                      inflatable dinosaur beside a wall with abstract         4D LMM: This object is a green
y                                                                           trees and a window.                                     inflatable dinosaur. At 1s, it is at       Q: Where is this object likely to go next?
                                            t=1s   t=2s                     4D LMM: A man in sunglasses lifts a green inflat-       [2.25, 0.51, 0.25, 3.18, 0.20, 0.39].      3D LMM: This object is landing on the ground.     ❎
      x
          p=(2.25, 0.50, 0.25,                                              able dinosaur beside a window, showing upward           At 2s, it is at [4.32, 1.74, 0.20, 4.89,   4D LMM: This object is moving upward during [1s-2s] and is likely to
           3.17, 0.22, 0.38)
                                                           View 2           motion over time against tree-like graffiti.            1.15, 0.25].                               reach the window area next.
      z                                  View 1
                                                                                                (d) Comparison on 4D Understanding Tasks

Figure 1: Illustration of 3D and 4D LMM paradigms for physical world understanding. (a) Existing
3D-LMMs encode 3D positions as spatial prompts but overlook dynamic objects. (b) Our 4D
LMM framework embeds 4D coordinates: position and time as spatiotemporal prompts to capture
both background and dynamic objects. (c) Performance comparison of LMMs on benchmarks. (d)
Comparison on 4D understanding tasks.

LLMs perform poorly on dynamic understanding tasks. This highlights the need to capture both
spatial and temporal information to improve scene understanding in dynamic physical environments.
As shown in Fig. 1, we propose a novel spatiotemporal prompt embedded with the visual repre-
sentation to model the spatiotemporal characteristics of the scene. We design the spatiotemporal
prompt based on the observation that dynamic objects and static backgrounds share similar 3D
positional encoding but differ significantly in motion patterns. This motivates the extension of 3D
positional encoding into a dynamic-aware 4D coordinate encoding to better differentiate objects
from the background. Additionally, we observe that visual features of a scene can be disentangled
into spatial and temporal components, which effectively distinguish objects from the background.
This inspires the design of a spatiotemporal-disentangled vision embedding for scene representation.
The 4D spatiotemporal prompt and visual spatiotemporal features jointly model the fine-grained
spatiotemporal characteristics of dynamic objects and static backgrounds, thereby enhancing the 4D
scene understanding of the physical world by LMMs.
In this paper, we propose LLaVA-4D, a general language-vision large multimodal model for 4D scene
understanding. As illustrated in Fig. 2, our LLaVA-4D includes a dynamic-aware 4D coordinate
encoding and a spatiotemporal-disentangled vision embedding. The 4D coordinate encoding module
constructs 4D coordinates for multi-view videos and incorporates optical flow to enhance spatiotem-
poral encoding. The vision embedding module disentangles visual features from multi-view videos
into spatial and temporal components, and enriches these spatiotemporal features with dynamic-aware
4D coordinate embeddings through cross-attention fusion. We further perform spatiotemporal encod-
ing on textual 4D coordinates within language embeddings, which are then aligned with the fused
visual spatiotemporal embeddings. In this unified framework, 4D coordinate encoding and visual
spatiotemporal embedding collaboratively enhance the modeling of dynamic and static elements to
improve 4D scene understanding in LMMs. Additionally, we present Chat4D, a 4D vision-language
dataset with spatiotemporal coordinate annotations designed to instruction-tune our model for more
effective 4D scene understanding. Our main contributions are summarized as follows:

• We are the first to propose a general vision-language large multimodal model for 4D scene
  understanding. Our model embeds a 4D spatiotemporal prompt into visual representation to enable
  LMMs to comprehend both dynamic objects and static backgrounds.
• We observe that backgrounds and objects share similar 3D spatial position encoding but exhibit
  distinct motion patterns in the temporal dimension. This motivates the design of a dynamic-aware
  4D coordinate encoding as a spatiotemporal prompt to distinguish objects from backgrounds.


                                                                                                                                2
• We discover that spatial and temporal components disentangled from visual features are more
  discriminative for background and objects. This inspires the spatiotemporal-disentangled vision
  embedding for scene representation.
• We build a 4D vision-language dataset with coordinate annotations for instruction fine-tuning and
  conduct extensive experiments to verify the effectiveness of our method in 4D scene understanding.

2    Related Work
2D Vision-Language LMMs. Leveraging the strong reasoning capabilities of large language models
[12, 13, 14], numerous vision-language LMMs [1, 15, 7, 16, 2, 17, 18] have been developed to learn
the correspondence between image-based visual and linguistic representations with broad applications
in 2D scene understanding tasks. However, these vision-language LMM methods cannot work well
when applied to the 3D physical world. This is because these LMMs lack the representation of 3D
spatial characteristics and can only learn visual knowledge within the camera plane from 2D images,
leading to underperformance of LMMs on 3D scene understanding. We thus aim to enhance the
visual representation and improve LMMs on 3D scene understanding.
3D Vision-Language LMMs. The main challenge in understanding the physical world is representing
3D spatial characteristics. Some researchers [10, 19, 20, 11, 21, 22, 23] address this by embedding
3D positions as spatial prompts within visual features, using point-based or image-based approaches.
Point-based methods [19, 20, 22, 21] reconstruct point clouds from 3D positions and use a 3D
vision encoder to extract features for LMMs. To reduce reliance on reconstruction precision [24],
image-based methods [11, 10, 23] encode multi-view images into 2D visual features concatenated
with embeddings of 3D positions. However, these methods use unified spatial representations that
limit their ability to capture dynamic objects with temporal variations. We thus propose a novel
spatiotemporal prompt for dynamic scene representation, and embed it into multi-view visual features
to enable 4D scene understanding in LMMs.
4D Vision & Language. A related line of research is 4D vision and language [25, 26, 27], which
focuses on modeling spatiotemporal characteristics. Li et al. [25] use 4D Gaussians [28] to represent
dynamic scenes for semantic caption queries of different targets. Deng et al. [26] introduce a 4D
encoder to directly extract scene visual features for alignment with object recognition texts. However,
these 4D models have two limitations. First, they are usually task-specific and can only handle similar
cases within the same data distribution of training set. Second, they adopt the same representation
strategy for dynamic objects and static background, which has the potential risk of misalignment
of heterogeneous features. In this work, we present the first general LMM for different tasks of 4D
scene understanding by disentangling visual features and embedding 4D spatiotemporal prompts to
differentiate objects and background.

3    Our LLaVA-4D
Overview. Fig. 2 shows the architecture of our LLaVA-4D. Given a multi-view video input sequence
I, our LLaVA-4D achieves 4D scene understanding progressively through the following three stages:
1) Dynamic-Aware 4D Coordinate Encoding (cf. Sec. 3.1). This is the 4D prompt construction
   stage where we construct 4D coordinate tensors [x, y, z, t] from multi-view videos using visual
   geometry, and perform spatiotemporal encoding PE(·), TE(·) on the coordinates. The encoded
   position and time are concatenated as a spatiotemporal prompt to guide visual fusion, i.e.:
                                  p4D = wp · [PE(x, y, z) ∥ TE(t) · β],                             (1)
   where wp is a learnable matrix and β is motion field for dynamic awareness of temporal encoding.
2) Spatiotemporal-Disentangled Vision Embedding (cf. Sec. 3.2). This is the visual representation
   stage where we extract visual features f from multi-view videos using a vision encoder, and
   disentangle these visual features into spatiotemporal components:
                                            fs , ft = STD(f ),                                      (2)
    where fs is spatial feature and ft is temporal feature. We further embed encoded 4D coordinate
    features into these spatiotemporal features via cross-attention fusion:
                                        fst = CAtt([fs , ft ], p4D ),                               (3)
    where fst denotes the output visual spatiotemporal feature with 4D awareness.


                                                   3
Re: This tiger is walking in snow, scanning its surroundings. At t1, it is at [x1, y1, z1]. At t2, it is at [x2, y2, z2]... At tn, it is at [xn, yn, zn].

                                                                                                                     Pretrained Large Language Model
  Visual Tokens                                                                                                                                                                                 Linguistic Tokens


                                                                                           Projector
                                                      Cross-Attention Fusion
                                                                                                                                                      Word                   Position                Time
                                                                                                                MLP                                  Encode                  Encode                 Encode
Spatialv                                                  Temporal
               1
                                                          vn                                                Concat
      v3            v2
                                        vn                                       t
           v4                  vn
                                             t1      t2        t3   tn

                                                                                                                                              Language
                                Vision                Disentangle                            Position                    Time        soft
                                                                                                                                                               Word           Position                  Time
                                                                                             Encode                     Encode               Embedding
                                Encode
                                                                                                         [x, y, z]          [t]
                                                                                                                                                                                  Text Tokenizer

                   tn t1                     tn t1                       tn t1
                                                                                                                                             What does the tiger at [x1, y1, z1] do at t1? Please provide
  View 1                            View 2                View n                            View 1             View 2      View n            the position of the tiger at each time.
                      Vision Input                                        Paired                     4D Coordinate                                                   Language Instruction
           y
                                                                                                                                                   Visual Spatial Token         Visual Temporal Token
  z                        x                                                     tn                 t1                            Flow                                                                     Word Token
                                                      P1                                  Motion                                                 Linguistic Position Token      Linguistic Time Token
                                      View 1               View 2                    P2            View n

Figure 2: Our LLaVA-4D consists of three stages: 1) 4D coordinate encoding. Encode 3D position
and 1D time with optical flow. 2) Vision embedding. Disentangle visual features into spatiotemporal
features and embed the encoded 4D coordinates via cross-attention fusion. 3) Language embedding.
Align textual position and time with the fused vision embedding for 4D scene understanding.

3) Coordinate-Aligned Language Embedding (cf. Sec. 3.3). This is the linguistic representation
   stage where visual spatiotemporal features are projected into the language embedding space
   using multi-layer perception: τvst = MLP(fst ), where τvst denotes visual spatiotemporal to-
   kens. Input instructions are tokenized into language space, where textual position and time are
   spatiotemporally encoded into corresponding linguistic tokens τlst .

Remarks. The output τvst and τlst denote the visual and linguistic representation with 4D coordinate
prompt, respectively. Subsequently, the LLM [13] utilizes these enhanced visual and linguistic tokens
to improve 4D scene understanding. Our unified framework embeds 4D coordinates as spatiotemporal
prompts into visual representations to enable spatiotemporal understanding of dynamic objects and
static background in LMMs. The following sections detail the design of each stage.

3.1        Dynamic-Aware 4D Coordinate Encoding

2D LLMs can capture spatial relationship between targets in images by using a vision encoder to
implicitly or explicitly incorporate 2D position encoding. Similarly, 4D scene understanding can be
enabled by the integration of 3D positon and 1D time into LMMs.
4D Coordinate Definition. Given an image from a certain view at timestamp t, we use SfM [29] for
camera pose P = [R | T ] and MVS [30] for depth D. Combined with intrinsic parameter K, we
transform 2D pixel coordinate x2D to world coordinate system via geometric projection [31, 32]:
                                                                                                         x3D = R−1 (D(x2D ) · K −1 x2D − T ),                                                                       (4)

where x3D = [x, y, z]⊤ denotes 3D position. After traversing all videos, we concatenate time and
corresponding 3D position to form the 4D coordinate tensor [x, y, z, t].
Spatiotemporal Encoding. We perform spatiotemporal encoding to convert the 4D coordinates into
learnable feature patterns. It is challenging to directly distinguish objects from the background solely
based on spatial dimensions such as multi-view images captured at a specific time. We circumvent
this challenge by adopting the same spatial position encoding strategy for objects and background via
learnable Fourier feature [33]:
                                                                                                   √
                                                                                          pxyz = 1/ d [cos([x, y, z]Wr⊤ ∥ sin([x, y, z]Wr⊤ ))],                                                                     (5)
where d denotes the dimension and Wr is the learnable parameter of the Fourier feature. From the
temporal dimension such as continuous video sequence at a certain view, objects and background


                                                                                                                                         4
                                      View 1                                                                                                                            Object

          ic        Scene                                                    Vision         Cluster
  4D Dynam
                                                                                                                                                                Visual Spatial Temporal

                                                                  t2        Encoder
                                                                       t1
                                                 P1
                                                                                                                                                   r
                                                                                                                                                ila




                                                                            Shared
                                                                                                                                      r   sim                  Feature Manifold
                                                                                                                                                               F
                                                                                                                                  ila
                                                                                                                             im
                         Background
                                                                  t2                             Object              d   iss
               t2
                                                                       t1
                    t1                View 2     P2
      Object                                                                                                                                           Background
                                                                             Vision       SpatioTemporal
                                                                            Encoder        Disentangle           r
                                                                  t2                                       Cluste                                                    Background
                                                                       t1
                                                                                                                                                                Visual Spatial Temporal
                                      View 3          Multi-View Videos

Figure 3: Feature distribution of static background and dynamic object in a 4D dynamic scene.
Visual features of dynamic objects appear scattered while static backgrounds are clustered. In contrast,
spatiotemporal features show clear discrimination between objects and background.

have different motion patterns and thus we add motion information into the temporal encoding:
                                                      √
                                               pt = 1/ d [cos(tWr⊤ ∥ sin(tWr⊤ ))] · (1 + Φ(vel)),                                                                                     (6)
where vel is estimated optical flow, and Φ(·) is softmax function. We further concatenate the
spatial and temporal encoding outputs to obtain dynamic-aware 4D coordinate embeddings as
the spatiotemporal prompt for subsequent visual fusion. Moreover, this spatiotemporal prompt is
extensible, where we can further add some other spatiotemporal attributes such as semantic and
action to guide the alignment between visual and linguistic representations (cf. Sec. 5.3 for details).

3.2    Spatiotemporal-Disentangled Vision Embedding

Unlike 2D image and 3D scene, 4D scene consists of spatial such as color and temporal such as
motion components. A unified visual representation for 4D scene usually suffers from misaligned het-
erogeneous features, which inspires us to disentangle visual features into spatiotemporal components.
Spatiotemporal Disentanglement. Spatial features mainly reflect the appearance of the entire
scene, while temporal features focus more on continuous varying in motion patterns. After obtaining
multi-view visual features fv,t , where v is the view and t is the time, we get the correlation of visual
features between different views at the same time as spatial features:
                                                                         ⊤
                                                        fs = Aggregate({fv=i,t fv=j,t | i ̸= j}).                                                                                     (7)
Next, we further calculate the correlation of visual features between adjacent images of continuous
time at the same view as temporal features:
                                                                            ⊤
                                                           ft = Aggregate({fv,t=i fv,t=i+1 }).                                                                                        (8)
To illustrate the importance of spatiotemporal features, we encode the selected object and background
regions of multi-view videos into visual, spatial and temporal features, and cluster these features for
visualization in Fig. 3. The visual features of the object region appear scattered, but the spatiotemporal
features of both the object and background regions are clustered. This indicates that spatiotemporal
features are highly discriminative for the entire scene. Consequently, disentangling visual features
into spatiotemporal components is essential for effective 4D scene representation.
Cross-Attention Fusion. Single disentangled spatiotemporal features cannot be localized to world
coordinate system. We need to further embed 4D coordinates into the spatiotemporal features for
localization. We first introduce an MLP to make the dimension of the 4D coordinate embeddings the
same as the dimension of the spatiotemporal features: p4D = MLP([pxyz ∥ pt ]). Next, we fuse the
4D coordinate embeddings with the spatiotemporal features via a cross-attention mechanism [34, 35]:
                                                                                                 √
                              q =wq p4D , k = wk [fs , ft ], v = wv [fs , ft ], a = softmax(qk⊤ / d),
                                                                                                                                                                                      (9)
                                 o = a · v, α = σ(MLPobj (p4D )), fst = α · o + (1 − α) · fs ,
where w is a learnable weight. As a result, we can obtain the 4D-aware visual spatiotemporal feature.

3.3    Coordinate-Aligned Language Embedding

Although vision embeddings can represent scene knowledge, LMMs understanding scenes require
the alignment between visual and linguistic representations. Since the input of large language model


                                                                                      5
                                                                 Stage 1: Content Alignment          451.1k         View 1                     View 2                      View n

                      )                                            2D DC      2D QA     3D DC       3D QA                                                          ...
                5%                                                                                                            t2                         t2                            t2
             5.                                                                                                                    t1                         t1                            t1
           (2




                                                   2D
                              VG      DC
                              66.4k 121.
      4D           QA                                                                                                  3D Detection                           ChatGPT-4V




                                                       (37
                                         7k




                          k
                    .3
                  76
                                                                 Stage 2: Coordinate Alignment       203.4k




                                                        .6%
            DC                                  QA                                                                                 (x, y, z)               t
           81.9k                               107.0k




                                                             )
                              Chat4D                                     2D VG                3D VG                Box                  Point             Time                 Color
           VG 2k
            10

                                  879.1k       VG                                                                             Text-Only                  GPT
               2.


                         QA                   101.2k
                      114             DC                                                                      Q: What is happening       Q: What is in the           Q: How many salmons?
                          .8k                                    Stage 3: Instruction Fine-Tuning    224.6k   in the scene?              pot? Please provide         Please describe them.
       3D                           107.6k
                                                                                                              A: A person in a cap       its coordinates.            A: Two. A fresh salmon
                (36                                                                                           and apron cooks            A: It is a salmon           fillet is gently being
                      .9%                                            4D DC          4D QA           4D VG     salmon in a kitchen        being heated. At t1,        cooked in a black pan
                              )                                                                               with large windows         it is at [x1, y1, z1,       and another lies raw on
                                                                                                              and utensils.              x2, y2, z2].                a white plate.

    (a) Overview of Chat4D Dataset                                      (b) Training Pipeline                          (c) 4D Instruction Generation

Figure 4: Overview of our dataset and training pipeline. (a) Chat4D dataset includes 2D, 3D, and
4D vision-language training sets for dense captioning, QA, and visual grounding. (b) Three-stage
training: stages 1-2 use 2D/3D data for initialization; stage 3 uses 4D data for instruction fine-tuning.
(c) Spatiotemporal characteristics are extracted as local descriptions to generate 4D instructions.

requires text-like tokens, we first build a multi-layer perception to project the fused spatiotemporal
features into language embedding space for preliminary alignment with linguistic representations.
This projected fused spatiotemporal features is the visual tokens denoted as τvst . Subsequently, we
tokenize the input instruction into the language space with word tokens τl , and apply the same
spatiotemporal encodings PE(·) and TE(·) to textual position tp and time tt:
                                                                       τs = PE(tp), τt = TE(tt).                                                                                      (10)
We further fuse the encoded position and time with corresponding word token: τlst = τl +ws τs +wt τt ,
where ws and wt denote the learnable weights. We concatenate 4D-aware visual tokens with
coordinate-aligned linguistic tokens for the LLM to reason. Particularly, 4D coordinate encoding
ensures spatiotemporal localization of the scene within our unified framework. Additionally, dis-
entangled vision embedding models spatiotemporal knowledge of the scene, and vision-language
alignment further enable the LMM to achieve interactive understanding of 4D scenes.

4      Dataset and Training Pipeline
4.1        Our Chat4D Dataset

Many vision-language datasets have been proposed to evaluate 2D/3D scene understanding of
LMMs. However, there is currently no vision-language dataset specifically designed for 4D scene
understanding in LMMs. To address this gap, we introduce the Chat4D dataset. As illustrated in Fig.
4(a-b), our dataset includes 2D, 3D and 4D vision-language data types, where 2D/3D data are used to
initialize multimodal spatiotemporal understanding and 4D data is used for instruction fine-tuning.
2D&3D Vision-Language Data. To develop multimodal spatiotemporal understanding, our model
requires image-format inputs and a large number of vision-language pairs. We thus integrate existing
standard 2D/3D spatiotemporal datasets [36, 37, 38, 39, 40, 41, 42, 43, 44, 45] and adapt specific text
instructions (e.g., 2D/3D position and time) to align with our spatiotemporal encoding strategy. This
approach effectively trains the LMM for spatial and temporal understanding. These datasets cover
dense captioning (DC), visual QA and visual grounding (VG) tasks with a total of 654.5K samples.
4D Vision-Language Data. We merge existing 4D dynamic scene reconstruction datasets to train the
LMM for 4D spatiotemporal understanding: iPhone [46], HyperNeRF [47], N3DV [48], Panoptic-
Sports [49], DAVIS [50], and Immersive [51]). Additionally, we develop a data generation approach
to produce paired 4D vision-language data for instruction fine-tuning. As shown in Fig. 4(c), we
utilize 3D object detection methods [52, 53] and GPT-4V [54] to extract local spatiotemporal infor-
mation such as category, position, time from multi-view videos. These extracted features are then
processed by text-only GPT to generate global 4D descriptions in instruction-following formats for
typical spatiotemporal understanding tasks to produce a dataset of 224.6K samples.

4.2        Training Pipeline

To ensure the stability of the training process and improve the performance of the model, we divide
the entire training into three stages in Fig. 4 (b) as follows:


                                                                                        6
Table 1: Quantitative results of LMMs for scene understanding tasks on different 3D and 4D datasets.
                                                  3D Benchmark                                     4D Benchmark
                Methods      Scan2Cap [43]       ScanQA [41] Multi3DRefer [44] ScanRef [55]        Chat4D (Ours)
                       C@0.5↑ B-4@0.5↑ M@0.5↑ C↑ B-4↑ M↑         F1@0.5↑       SAcc@0.5↑ C↑ B-4↑ SAcc@0.5↑ TAcc↑
       3D-LLM [10]       –        –         –   69.4 12.0 14.5      –               –       61.6 11.5    31.4     –
      Chat-3D v2 [56]   63.9     31.8       –   87.6 14.0 –        41.6            38.4     81.8 13.7    39.5     –
        LL3DA [20]      65.2     36.8      26.0 76.8 13.5 15.9      –               –       72.3 11.9    46.2     –
      3D-LLaVA [21]     78.8     36.9      27.1 92.6 17.1 18.4      –               –       85.1 16.0    52.0     –
3D
   Grounded 3D-LLM [57] 70.6     35.5       –   72.7 13.4 –        40.6            44.1     66.3 12.2    43.7     –
         PQ3D [58]      80.3     36.0      29.1 87.8 – 17.8        50.1            51.2     84.7 14.3    51.5     –
      LLaVA-3D [11]     79.2     41.1      30.2 91.7 14.5 20.7      –              42.2     87.4 14.8    45.6     –
    Video-3D LLM [23]   83.8     42.4      28.9 102.1 16.2 19.8    52.7            51.7     89.4 16.1    52.8     –
4D   LLaVA-4D (Ours)    85.3     45.7      31.3 97.8 17.9 21.2     54.3            53.2     93.5 17.2    58.9    54.6


                                                  1.5s                1.7s                1.9s               2.1s                2.3s

                 t=1.5s
                                        View 1



    y
            x
                          t=2.3s
        z
                                        View 2




                View 1       View 2

    User                           What does the person do? Please provide its coordinates.

    LLaVA-3D                       A person is riding a horse, jumping over an obstacle in an outdoor equestrian arena. It is at [11.26, 2.10,
                                   -7.35, 12.48, 0.36, -7.10].
    Video-3D LLM                   The person is horseback riding in a sandy arena, jumping over a high obstacle during a show jumping com-
                                   petition, surrounded by fences and other jump setups. It is at [11.04, 2.05, -7.30, 12.39, 0.31, -7.05].
    LLaVA-4D                       This person is riding a brown horse in a sandy arena, approaching the hurdle, jumping high, clearing it
                                   mid-air, landing smoothly, and continuing toward the next obstacle. At 1.5s, it is at [11.15, 2.08, -7.32,
                                   12.45, 0.30, -7.15]. At ..., it is at [...]. At 2.3s, it is at [12.80, 1.84, -6.80, 14.26, 0.12, -6.51].

                          Figure 5: Visual comparison of LMMs on 4D scene understanding.

Stage 1: Content Alignment. The training sets of the DC and QA tasks in the 2D&3D vision-
language data of our Chat4D are used to initially align the content between visual and linguistic
representations. This provides a foundational spatiotemporal understanding for the proposed model.
At this stage, only parameters of the cross-attention fusion and the projector are updated. The 4D
spatiotemporal coordinate features p4D are temporarily set as zero padding.
Stage 2: Spatiotemporal Coordinate Alignment. In order to further improve the fine-grained
understanding capability of our model under the spatiotemporal coordinate prompt, we use the
training data of the VG task in the 2D&3D vision-language subset of our Chat4D to refine the
spatiotemporal coordinate alignment between visual and linguistic representations. At this stage, we
update all trainable parameters of 4D coordinate encoding and cross-attention fusion modules while
keeping all other modules frozen.
Stage 3: 4D Task Instruction Fine-Tuning. To further improve our model for 4D scene under-
standing, we use 4D vision-language data of Chat4D to enhance the generalization of our model
for fine-grained spatiotemporal understanding with 4D coordinates through a multi-task instruction
fine-tuning strategy. All trainable parameters are updated while the vision encoder remains frozen.

5       Experiments
5.1         Experiment Setup

Implements Details. Our LLaVA-4D model utilizes the pre-trained weights of LLaVA-1.5-7B [17]
and the vision encoder of CLIP-ViT-L-336px [3, 59]. Cross-attention fusion module is a transformer-
based network architecture [34]. The whole model is trained on 8 RTX 4090 GPUs over 86 hours
using AdamW as the optimizer. In training stages 1-2, we set the learning rate to 1.0e − 4 with a
batch size of 48. We use a learning rate of 1.0e − 5 with a batch size of 16 in training stage 3.
Comparison Methods. Since there are currently no other public 4D LMMs for comparison, we
compare our model with 3D LMMs: 3D-LLM [10], Chat-3D v2 [56], LL3DA [20], 3D-LLaVA


                                                                          7
          Table 2: Effect of visual representation modules.                               Table 3: Role of coordinate encoding.
         Coor. embed Feat. disent. Feat. fusion C↑ B-4↑ SAcc@0.5↑ TAcc↑                       Encoding target C↑ B-4↑ SAcc@0.5↑ TAcc↑
              ×           ×             ×       62.3 11.7  34.8    12.7                        w/o Encoding 75.0 12.1    47.2    46.8
              √
                          ×             ×       85.4 15.1  51.5    47.5                        w/ 3D position 88.6 15.3  53.4    47.0
              √           √
                                        ×       89.0 16.5  54.3    51.2                          w/ 1D time    82.7 14.0 48.5    52.7
              √           √             √
                                                93.5 17.2  58.9    54.6                       w/ 4D coordinate 93.5 17.2 58.9    54.6



                                             Object
View 1




                                      Map                                            Object                                    Object Temporal
                         tn t2
                                 t1
View 2




                                      Map                                Map      Temporal           Spatial     Map              Object Spatial
                         tn t2
                                 t1
                  ...
View n




                                             Background                                 Background                     Background Spatial
                                      Map
                         tn t2
                                 t1         w/ Visual Feature Encoding         w/ Spatiotemporal Disentangling         w/ 4D Coordinate Embedding

Figure 6: Feature visualization at different stages. Spatiotemporal disentanglement improves the
discriminability of background and objects, which are further separated by 4D coordinate embedding.

[21], Grounded 3D-LLM [57], PQ3D [58], LLaVA-3D [11], and Video-3D LLM [23]. For a fair
comparison, all methods are trained on the same evaluation benchmark via instruction fine-tuning.
Evaluation Metric. We compare all competing methods on multiple 3D datasets: Scan2Cap [43],
ScanQA [41], ScanRef [55] and Multi3DRefer [44] and our Chat4D dataset. We evaluate the
quality of generated text response for Scan2Cap and ScanQA in terms of CiDEr (C), BLEU-4 (B-4),
METEOR (M). We choose the F1 metric of object prediction precision for Multi3DRefer, and the
accuracy of intersection over unions for grounding task from ScanRef. The metrics are also applicable
to the evaluation on our Chat4D, where grounding accuracy is divided into spatial and temporal
components: S/TAcc. Note that the experiments in Sec. 5.3 are evaluated on 4D tasks.

5.2         Comparison with State-of-the-Art Models

Quantitative Results. In Table 1, we compare the competing methods on 3D and 4D datasets. For
3D understanding comparison, our method performs better than other methods. This shows that
spatial features disentangled from multi-view images have stronger representation than ordinary
visual features for 3D scene. For 4D understanding comparison, our method achieve a significant
strength due to dynamic-aware 4D coordinate as spatiotemporal prompt.
Qualitative Results. In Fig. 5, we select a typical 4D scene from our Chat4D dataset to visualize the
comparison between our model and 3D LMMs. The results show that 3D LMMs cannot respond to
timestamp and corresponding temporal information, while our method can understand the temporal
content. This is because 3D LMMs lack the representation of temporal characteristic. In contrast, our
method introduces the spatiotemporal prompt to enhance the dynamic representation of 4D scenes.

5.3         Ablation Study and Discussion

Effect of Visual Representation Modules. In Table 2, we verify the effectiveness of coordinate
embedding, feature disentanglement and feature fusion modules in visual representation. Coordinate
embedding is the key to improving the overall performance of 4D understanding tasks by a large
margin. Feature disentanglement improves the upper limit of 4D scene understanding to a certain
extent by strengthening the representation of spatial and temporal characteristics. Feature fusion
further enhances the spatiotemporal understanding ability of the LMM.
Role of 4D Coordinate Encoding. In Table 3, we analyze the impact of 3D position encoding and
1D time encoding on the performance of 4D understanding. When 4D coordinates are not encoded,
the spatiotemporal understanding performance of the LMM is negatively affected to a certain extent.
3D position encoding mainly contributes to the spatial understanding ability of the LMM, and 1D
time encoding can further improve the performance of temporal understanding.


                                                                           8
Table 4: Discussion on spatiotemporal fusion.                 Table 5: Impact of textual coordinate encoding.
       Fusion strategy C↑ B-4↑ SAcc@0.5↑ TAcc↑                          Text instruction      C↑ B-4↑ SAcc@0.5↑ TAcc↑
      w/ Concatenation 89.0 16.5  54.3    51.2                          w/o Coordinate        83.5 13.2  43.2    25.8
        w/ Weighting   89.5 16.5  55.1    51.4                                   w/o Encoding 90.1 16.7  56.3    53.0
                                                                  w/ Coordinate
        w/ Attention   93.5 17.2  58.9    54.6                                    w/ Encoding 93.5 17.2  58.9    54.6


           Spatial Semantic                               Temporal Action
                                                                                      1.4s               1.9s              2.4s




    User          How many persons in the scene?   User            What is the person doing in this scene?

    LLaVA-4D      Four.                            LLaVA-4D        This person is performing a dance move on a street, spinning on one
                                                                   hand, and switching leg positions mid-air.
    User          Please segment these persons.
                                                   User            Please segment this dance move.
    LLaVA-4D      Sure, [SEG].
                                                   LLaVA-4D        At 1.4s, [SEG]. At 1.9s, [SEG]. At 2.4s, [SEG].

 Figure 7: Visualization of spatiotemporal prompt extended to other spatiotemporal vision tasks.

How Spatiotemporal Features Work? We study how spatiotemporal features work within our
model in Fig. 6. Initially, the visual features of objects and backgrounds appear relatively scattered.
After spatiotemporal disentanglement, object features are distinctly divided into spatial and temporal
components with background features remain clustered. Incorporating coordinate embedding further
organizes object features into two distinct sets: spatial and temporal where background features
consolidate into a unified spatial set. This demonstrates the strong spatiotemporal representation
capability of our method for dynamic objects and static backgrounds.
Choice of Spatiotemporal Fusion Strategy. Table 4 shows attention-based fusion outperforms
concatenation and weighting fusion strategies. This is because concatenation and weighting rely on
global unified fusion with fixed weights. In contrast, attention-based fusion can dynamically adjust
the fusion weights of spatiotemporal features according to 4D coordinate embedding. This allows the
LMM to effectively focus more on meaningful spatiotemporal features for 4D understanding.
Impact of Textual Coordinate Encoding. Table 5 ablates the impact of textual coordinate encoding
on scene understanding by the LMM. We deduce: 1) Textual coordinates as instructions improve
the fine-grained spatiotemporal understanding of the LMM. 2) Textual coordinate encoding further
improves the upper limit of 4D spatiotemporal understanding. This is because coordinate encoding
helps minimize the risk of large language models misinterpreting coordinate values.
Extensibility of Spatiotemporal Prompt. To verify that the spatiotemporal prompt in our model is
an extensible feature, we introduce additional spatial semantic and temporal action masks as prompts
based on the encoded 4D coordinates to train our model. As shown in Fig. 7, our model can reason
about the visual features of semantic and action based on specific text instructions. Incorporating
spatiotemporal prompts thus enhances the generality of our model for various vision tasks.
Limitation. While our model performs well on most 3D and 4D dynamic scenes, it struggles with
fast-moving objects due to motion blur from frame-based cameras. This reduces discriminability of
the spatiotemporal feature and thus weakens 4D understanding. In future work, we plan to incorporate
event cameras [60] with high temporal resolution to improve dynamic representation.


6     Conclusion

In this work, we propose LLaVA-4D, a first general vision-language LMM for 4D scene understanding.
We introduce a dynamic-aware 4D coordinate encoding as a spatiotemporal prompt for scene content
localization. Additionally, we propose a spatiotemporal-disentangled vision embedding method
that integrates 4D spatiotemporal prompts into disentangled spatiotemporal features for effective
scene representation. By aligning visual spatiotemporal embeddings with language embeddings, our
approach allows LMMs to comprehend the 4D physical world. To support training, we construct
Chat4D, a comprehensive dataset covering 2D, 3D and 4D vision-language data for multimodal
spatiotemporal understanding. Extensive experiments validate the effectiveness of our method.


                                                              9
References
 [1] Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Arthur Mensch,
     Katie Millican, David Moore, Michael Needham, et al. Flamingo: a visual language model for few-shot
     learning. Advances in Neural Information Processing Systems, 35:23716–23732, 2022.

 [2] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. In Advances in
     Neural Information Processing Systems (NeurIPS), 2023.

 [3] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish
     Sastry, Amanda Askell, Pam Mishkin, Jack Clark, et al. Learning transferable visual models from natural
     language supervision. In International Conference on Machine Learning, pages 8748–8763. PMLR, 2021.

 [4] Yujia Gong, Changhan Wang, Yun Tang Wang, and Jiatao Gu. Contrastive learning for speech translation.
     In International Conference on Learning Representations, 2022.

 [5] Jianfeng Wang, Jianwei Yang, Xiaowei Wang, Lu Yuan, Lei Zhang, Yejin Choi, and Jianfeng Gao. Git:
     A generative image-to-text transformer for vision and language. In Advances in Neural Information
     Processing Systems, volume 35, pages 18002–18014, 2022.

 [6] Junnan Li, Dongxu Li, Caiming Xiong, and Steven Hoi. BLIP: Bootstrapping language-image pre-training
     for unified vision-language understanding and generation. In Proceedings of the 39th International
     Conference on Machine Learning, volume 162 of Proceedings of Machine Learning Research, pages
     12888–12900. PMLR, 2022.

 [7] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. BLIP-2: Bootstrapping language-image pre-
     training with frozen image encoders and large language models. In Proceedings of the 40th International
     Conference on Machine Learning, volume 202 of Proceedings of Machine Learning Research, pages
     19730–19742. PMLR, 2023.

 [8] Aishwarya Kamath, Mannat Singh, Yann LeCun, Ishan Misra, Gabriel Synnaeve, Nicolas Carion, and
     Karteek Alahari. Mdetr: Modulated detection for end-to-end multi-modal understanding. In Proceedings
     of the IEEE/CVF International Conference on Computer Vision, pages 1780–1790, 2021.

 [9] Mandy Chen, Adams Wei Yu, Hamid Palangi, Paul Smolensky, Yinfei Yang, Xiaowei Yuan, Kathy Meier-
     Hellstern, Jianfeng Gao, Ed Chi, et al. Pali: A jointly-scaled multilingual language-image model. arXiv
     preprint arXiv:2303.07892, 2023.

[10] Yining Hong, Haoyu Zhen, Peihao Chen, Shuhong Zheng, Yilun Du, Zhenfang Chen, and Chuang Gan.
     3d-llm: Injecting the 3d world into large language models. Advances in Neural Information Processing
     Systems, 36:20482–20494, 2023.

[11] Chenming Zhu, Tai Wang, Wenwei Zhang, Jiangmiao Pang, and Xihui Liu. Llava-3d: A simple yet
     effective pathway to empowering lmms with 3d-awareness. arXiv preprint arXiv:2409.18125, 2024.

[12] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind
     Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners.
     Advances in neural information processing systems, 33:1877–1901, 2020.

[13] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix,
     Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation
     language models. arXiv preprint arXiv:2302.13971, 2023.

[14] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay
     Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and
     fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023.

[15] Jun Chen, Deyao Zhu, Xiaoqian Shen, Xiang Li, Zechun Liu, Pengchuan Zhang, Raghuraman Krish-
     namoorthi, Vikas Chandra, Yunyang Xiong, and Mohamed Elhoseiny. Minigpt-v2: large language model
     as a unified interface for vision-language multi-task learning. arXiv preprint arXiv:2310.09478, 2023.

[16] Ji Lin, Hongxu Yin, Wei Ping, Pavlo Molchanov, Mohammad Shoeybi, and Song Han. Vila: On pre-
     training for visual language models. In Proceedings of the IEEE/CVF conference on computer vision and
     pattern recognition, pages 26689–26699, 2024.

[17] Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee. Improved baselines with visual instruction
     tuning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages
     26296–26306, 2024.


                                                    10
[18] Hongyu Li, Jinyu Chen, Ziyu Wei, Shaofei Huang, Tianrui Hui, Jialin Gao, Xiaoming Wei, and Si Liu.
     Llava-st: A multimodal large language model for fine-grained spatial-temporal understanding. arXiv
     preprint arXiv:2501.08282, 2025.

[19] Zehan Wang, Haifeng Huang, Yang Zhao, Ziang Zhang, and Zhou Zhao. Chat-3d: Data-efficiently tuning
     large language model for universal dialogue of 3d scenes. arXiv preprint arXiv:2308.08769, 2023.

[20] Sijin Chen, Xin Chen, Chi Zhang, Mingsheng Li, Gang Yu, Hao Fei, Hongyuan Zhu, Jiayuan Fan,
     and Tao Chen. Ll3da: Visual interactive instruction tuning for omni-3d understanding reasoning and
     planning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages
     26428–26438, 2024.

[21] Jiajun Deng, Tianyu He, Li Jiang, Tianyu Wang, Feras Dayoub, and Ian Reid. 3d-llava: Towards generalist
     3d lmms with omni superpoint transformer. arXiv preprint arXiv:2501.01163, 2025.

[22] Hongyan Zhi, Peihao Chen, Junyan Li, Shuailei Ma, Xinyu Sun, Tianhang Xiang, Yinjie Lei, Mingkui Tan,
     and Chuang Gan. Lscenellm: Enhancing large 3d scene understanding using adaptive visual preferences.
     arXiv preprint arXiv:2412.01292, 2024.

[23] Duo Zheng, Shijia Huang, and Liwei Wang. Video-3d llm: Learning position-aware video representation
     for 3d scene understanding. arXiv preprint arXiv:2412.00493, 2024.

[24] Bernhard Kerbl, Georgios Kopanas, Thomas Leimkühler, and George Drettakis. 3d gaussian splatting for
     real-time radiance field rendering. ACM Trans. Graph., 42(4):139–1, 2023.

[25] Wanhua Li, Renping Zhou, Jiawei Zhou, Yingwei Song, Johannes Herter, Minghan Qin, Gao Huang, and
     Hanspeter Pfister. 4d langsplat: 4d language gaussian splatting via multimodal large language models.
     arXiv preprint arXiv:2503.10437, 2025.

[26] Zhichao Deng, Xiangtai Li, Xia Li, Yunhai Tong, Shen Zhao, and Mengyuan Liu. Vg4d: Vision-language
     model goes 4d video recognition. In 2024 IEEE International Conference on Robotics and Automation
     (ICRA), pages 5014–5020. IEEE, 2024.

[27] Jingtao Sun, Yaonan Wang, Mingtao Feng, Yulan Guo, Ajmal Mian, and Mike Zheng Shou. L4d-track:
     Language-to-4d modeling towards 6-dof tracking and shape reconstruction in 3d point cloud stream. In
     IEEE Conf. Comput. Vis. Pattern Recog., pages 21146–21156, 2024.

[28] Guanjun Wu, Taoran Yi, Jiemin Fang, Lingxi Xie, Xiaopeng Zhang, Wei Wei, Wenyu Liu, Qi Tian, and
     Xinggang Wang. 4d gaussian splatting for real-time dynamic scene rendering. In IEEE Conf. Comput. Vis.
     Pattern Recog., pages 20310–20320, 2024.

[29] Johannes L Schonberger and Jan-Michael Frahm. Structure-from-motion revisited. In IEEE Conf. Comput.
     Vis. Pattern Recog., pages 4104–4113, 2016.

[30] Steven M Seitz, Brian Curless, James Diebel, Daniel Scharstein, and Richard Szeliski. A comparison and
     evaluation of multi-view stereo reconstruction algorithms. In 2006 IEEE computer society conference on
     computer vision and pattern recognition (CVPR’06), volume 1, pages 519–528. IEEE, 2006.

[31] Yuliang Zou, Zelun Luo, and Jia-Bin Huang. Df-net: Unsupervised joint learning of depth and flow using
     cross-task consistency. In Eur. Conf. Comput. Vis., pages 1–18. Springer, 2018.

[32] Tinghui Zhou, Matthew Brown, Noah Snavely, and David G Lowe. Unsupervised learning of depth and
     ego-motion from video. In IEEE Conf. Comput. Vis. Pattern Recog., pages 1851–1858, 2017.

[33] Yang Li, Si Si, Gang Li, Cho-Jui Hsieh, and Samy Bengio. Learnable fourier features for multi-dimensional
     spatial positional encoding. Advances in Neural Information Processing Systems, 34:15816–15829, 2021.

[34] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz
     Kaiser, and Illia Polosukhin. Attention is all you need. Adv. Neural Inform. Process. Syst., 30, 2017.

[35] Chun-Fu Richard Chen, Quanfu Fan, and Rameswar Panda. Crossvit: Cross-attention multi-scale vision
     transformer for image classification. In Proceedings of the IEEE/CVF international conference on computer
     vision, pages 357–366, 2021.

[36] Tsai-Shien Chen, Aliaksandr Siarohin, Willi Menapace, Ekaterina Deyneka, Hsiang-wei Chao, Byung Eun
     Jeon, Yuwei Fang, Hsin-Ying Lee, Jian Ren, Ming-Hsuan Yang, et al. Panda-70m: Captioning 70m videos
     with multiple cross-modality teachers. In Proceedings of the IEEE/CVF Conference on Computer Vision
     and Pattern Recognition, pages 13320–13331, 2024.


                                                     11
[37] Yi Wang, Yinan He, Yizhuo Li, Kunchang Li, Jiashuo Yu, Xin Ma, Xinhao Li, Guo Chen, Xinyuan
     Chen, Yaohui Wang, et al. Internvid: A large-scale video-text dataset for multimodal understanding and
     generation. arXiv preprint arXiv:2307.06942, 2023.
[38] Ruipu Luo, Ziwang Zhao, Min Yang, Junwei Dong, Da Li, Pengcheng Lu, Tao Wang, Linmei Hu, Minghui
     Qiu, and Zhongyu Wei. Valley: Video assistant with large language model enhanced ability. arXiv preprint
     arXiv:2306.07207, 2023.
[39] Muhammad Maaz, Hanoona Rasheed, Salman Khan, and Fahad Shahbaz Khan. Video-chatgpt: Towards
     detailed video understanding via large vision and language models. arXiv preprint arXiv:2306.05424,
     2023.
[40] Hanoona Rasheed, Muhammad Maaz, Sahal Shaji, Abdelrahman Shaker, Salman Khan, Hisham Cholakkal,
     Rao M Anwer, Eric Xing, Ming-Hsuan Yang, and Fahad S Khan. Glamm: Pixel grounding large multimodal
     model. In IEEE Conf. Comput. Vis. Pattern Recog., pages 13009–13018, 2024.
[41] Daichi Azuma, Taiki Miyanishi, Shuhei Kurita, and Motoaki Kawanabe. Scanqa: 3d question answering
     for spatial scene understanding. In IEEE Conf. Comput. Vis. Pattern Recog., pages 19129–19139, 2022.
[42] Ruiyuan Lyu, Jingli Lin, Tai Wang, Xiaohan Mao, Yilun Chen, Runsen Xu, Haifeng Huang, Chenming Zhu,
     Dahua Lin, and Jiangmiao Pang. Mmscan: A multi-modal 3d scene dataset with hierarchical grounded
     language annotations. Advances in Neural Information Processing Systems, 37:50898–50924, 2024.
[43] Zhenyu Chen, Ali Gholami, Matthias Nießner, and Angel X Chang. Scan2cap: Context-aware dense
     captioning in rgb-d scans. In IEEE Conf. Comput. Vis. Pattern Recog., pages 3193–3203, 2021.
[44] Yiming Zhang, ZeMing Gong, and Angel X Chang. Multi3drefer: Grounding text description to multiple
     3d objects. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 15225–
     15236, 2023.
[45] Xiaojian Ma, Silong Yong, Zilong Zheng, Qing Li, Yitao Liang, Song-Chun Zhu, and Siyuan Huang.
     Sqa3d: Situated question answering in 3d scenes. arXiv preprint arXiv:2210.07474, 2022.
[46] Hang Gao, Ruilong Li, Shubham Tulsiani, Bryan Russell, and Angjoo Kanazawa. Monocular dynamic
     view synthesis: A reality check. Advances in Neural Information Processing Systems, 35:33768–33780,
     2022.
[47] Keunhong Park, Utkarsh Sinha, Peter Hedman, Jonathan T Barron, Sofien Bouaziz, Dan B Goldman,
     Ricardo Martin-Brualla, and Steven M Seitz. Hypernerf: A higher-dimensional representation for topologi-
     cally varying neural radiance fields. arXiv preprint arXiv:2106.13228, 2021.
[48] Tianye Li, Mira Slavcheva, Michael Zollhoefer, Simon Green, Christoph Lassner, Changil Kim, Tanner
     Schmidt, Steven Lovegrove, Michael Goesele, Richard Newcombe, et al. Neural 3d video synthesis from
     multi-view video. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition,
     pages 5521–5531, 2022.
[49] Jonathon Luiten, Georgios Kopanas, Bastian Leibe, and Deva Ramanan. Dynamic 3d gaussians: Tracking
     by persistent dynamic view synthesis. In 2024 International Conference on 3D Vision (3DV), pages
     800–809. IEEE, 2024.
[50] Federico Perazzi, Jordi Pont-Tuset, Brian McWilliams, Luc Van Gool, Markus Gross, and Alexander
     Sorkine-Hornung. A benchmark dataset and evaluation methodology for video object segmentation. In
     IEEE Conf. Comput. Vis. Pattern Recog., pages 724–732, 2016.
[51] Michael Broxton, John Flynn, Ryan Overbeck, Daniel Erickson, Peter Hedman, Matthew Duvall, Jason
     Dourgarian, Jay Busch, Matt Whalen, and Paul Debevec. Immersive light field video with a layered mesh
     representation. ACM Trans. Graph., 39(4):86–1, 2020.
[52] Yingfei Liu, Junjie Yan, Fan Jia, Shuailin Li, Aqi Gao, Tiancai Wang, and Xiangyu Zhang. Petrv2:
     A unified framework for 3d perception from multi-camera images. In Int. Conf. Comput. Vis., pages
     3262–3272, 2023.
[53] Danila Rukhovich, Anna Vorontsova, and Anton Konushin. Imvoxelnet: Image to voxels projection for
     monocular and multi-view general-purpose 3d object detection. In Proceedings of the IEEE/CVF Winter
     Conference on Applications of Computer Vision, pages 2397–2406, 2022.
[54] Zhengyuan Yang, Linjie Li, Kevin Lin, Jianfeng Wang, Chung-Ching Lin, Zicheng Liu, and Lijuan Wang.
     The dawn of lmms: Preliminary explorations with gpt-4v (ision). arXiv preprint arXiv:2309.17421, 9(1):1,
     2023.


                                                     12
[55] Dave Zhenyu Chen, Angel X Chang, and Matthias Nießner. Scanrefer: 3d object localization in rgb-d
     scans using natural language. In European conference on computer vision, pages 202–221. Springer, 2020.

[56] Haifeng Huang, Zehan Wang, Rongjie Huang, Luping Liu, Xize Cheng, Yang Zhao, Tao Jin, and Zhou
     Zhao. Chat-3d v2: Bridging 3d scene and large language models with object identifiers. arXiv preprint
     arXiv:2312.08168, 2023.

[57] Yilun Chen, Shuai Yang, Haifeng Huang, Tai Wang, Runsen Xu, Ruiyuan Lyu, Dahua Lin, and Jiangmiao
     Pang. Grounded 3d-llm with referent tokens. arXiv preprint arXiv:2405.10370, 2024.
[58] Ziyu Zhu, Zhuofan Zhang, Xiaojian Ma, Xuesong Niu, Yixin Chen, Baoxiong Jia, Zhidong Deng, Siyuan
     Huang, and Qing Li. Unifying 3d vision-language understanding via promptable queries. In European
     Conference on Computer Vision, pages 188–206. Springer, 2024.
[59] Mehdi Cherti, Romain Beaumont, Ross Wightman, Mitchell Wortsman, Gabriel Ilharco, Cade Gordon,
     Christoph Schuhmann, Ludwig Schmidt, and Jenia Jitsev. Reproducible scaling laws for contrastive
     language-image learning. In Proceedings of the IEEE/CVF conference on computer vision and pattern
     recognition, pages 2818–2829, 2023.

[60] Guillermo Gallego, Tobi Delbruck, Garrick Orchard, Chiara Bartolozzi, Brian Taba, Andrea Censi, et al.
     Event-based vision: A survey. IEEE Transactions on Pattern Analysis and Machine Intelligence, 44(1):154–
     180, 2020.




                                                    13
