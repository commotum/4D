# 2024_DAPE Survey Answers

## 1. Basic Metadata
Title: DAPE: Data-Adaptive Positional Encoding for Length Extrapolation.
Authors: Chuanyang Zheng; Yihang Gao; Han Shi; Minbin Huang; Jingyao Li; Jing Xiong; Xiaozhe Ren; Michael Ng; Xin Jiang; Zhenguo Li; Yu Li. (Title page, p.1)
Year: 2024.
Venue: NeurIPS 2024.
Evidence (title/venue):
- "DAPE: Data-Adaptive Positional Encoding for Length Extrapolation" (Title page, p.1)
- "38th Conference on Neural Information Processing Systems (NeurIPS 2024)." (Title page, p.1)

## 2. One-Sentence Contribution Summary
The paper proposes a data-adaptive positional encoding (DAPE) that dynamically adjusts positional encoding based on input context to improve length extrapolation performance.
Evidence: "we propose a Data-Adaptive Positional Encoding (DAPE) method, which dynamically and semantically adjusts based on input context and learned fixed priors." (Abstract)

## 3. Tasks Evaluated
Task: Language modeling (perplexity evaluation)
Task type: Other (language modeling / perplexity evaluation)
Dataset(s): Arxiv; Books3
Domain: Not specified in the paper.
Evidence: "Datasets. Our analysis involves training language models on the Arxiv and Books3 datasets, which are frequently used benchmarks for evaluating model performance [52, 13, 41, 24]." (Section 4, Datasets)

Task: EVEN PAIRS (CHE)
Task type: Other (formal language recognition)
Dataset(s): Chomsky Hierarchy Evaluation Benchmark (CHE)
Domain: formal language recognition
Evidence: "E VEN PAIRS                        50.04     91.27      99.98       96.60        73.52   57.50    73.86   99.99   99.58     100" (Table 2); "Experiments on Chomsky Hierarchy Evaluation Benchmark (CHE)" (Appendix D); "tasks derived from the domain of formal language recognition." (Appendix D)

Task: MODULAR ARITHMETIC (SIMPLE) (CHE)
Task type: Other (formal language recognition)
Dataset(s): Chomsky Hierarchy Evaluation Benchmark (CHE)
Domain: formal language recognition
Evidence: "M ODULAR A RITHMETIC (S IMPLE )    19.95     20.39      21.35       20.84        20.02   21.79    21.09   23.58   24.47    24.46" (Table 2); "Experiments on Chomsky Hierarchy Evaluation Benchmark (CHE)" (Appendix D); "tasks derived from the domain of formal language recognition." (Appendix D)

Task: PARITY CHECK (CHE)
Task type: Other (formal language recognition)
Dataset(s): Chomsky Hierarchy Evaluation Benchmark (CHE)
Domain: formal language recognition
Evidence: "DAPE-Alibi achieved scores of 50.30 on PARITY C HECK and 99.38 on B UCKET S ORT tasks, com-" and "pared to the highest scores of 50.97 and 99.57, respectively, demonstrating competitive performances." (Section 4.7); "Experiments on Chomsky Hierarchy Evaluation Benchmark (CHE)" (Appendix D); "tasks derived from the domain of formal language recognition." (Appendix D)

Task: CYCLE NAVIGATION (CHE)
Task type: Other (formal language recognition)
Dataset(s): Chomsky Hierarchy Evaluation Benchmark (CHE)
Domain: formal language recognition
Evidence: "50%, except for M ODULAR A RITHMETIC (S IMPLE ), C YCLE NAVIGATION, B UCKET S ORT, S OLVE E QUATION and M ODULAR A RITHMETIC, where it is 20%." (Table 2 caption); "Experiments on Chomsky Hierarchy Evaluation Benchmark (CHE)" (Appendix D); "tasks derived from the domain of formal language recognition." (Appendix D)

Task: STACK MANIPULATION (CHE)
Task type: Other (formal language recognition)
Dataset(s): Chomsky Hierarchy Evaluation Benchmark (CHE)
Domain: formal language recognition
Evidence: "S TACK M ANIPULATION               59.92     65.92      61.49       64.73        66.42   66.93    69.33   68.18   72.04    70.90" (Table 2); "Experiments on Chomsky Hierarchy Evaluation Benchmark (CHE)" (Appendix D); "tasks derived from the domain of formal language recognition." (Appendix D)

Task: REVERSE STRING (CHE)
Task type: Other (formal language recognition)
Dataset(s): Chomsky Hierarchy Evaluation Benchmark (CHE)
Domain: formal language recognition
Evidence: "R EVERSE S TRING                   52.76     67.28      65.23       65.59        71.09   71.54    65.89   73.37   70.74    76.40" (Table 2); "Experiments on Chomsky Hierarchy Evaluation Benchmark (CHE)" (Appendix D); "tasks derived from the domain of formal language recognition." (Appendix D)

Task: MODULAR ARITHMETIC (CHE)
Task type: Other (formal language recognition)
Dataset(s): Chomsky Hierarchy Evaluation Benchmark (CHE)
Domain: formal language recognition
Evidence: "M ODULAR A RITHMETIC               31.00     30.70      31.25       31.74        30.56   24.79    30.92   31.34   32.37    31.50" (Table 2); "Experiments on Chomsky Hierarchy Evaluation Benchmark (CHE)" (Appendix D); "tasks derived from the domain of formal language recognition." (Appendix D)

Task: SOLVE EQUATION (CHE)
Task type: Other (formal language recognition)
Dataset(s): Chomsky Hierarchy Evaluation Benchmark (CHE)
Domain: formal language recognition
Evidence: "S OLVE E QUATION                   20.00     19.97      21.85       22.93        19.92   21.15    22.06   20.03   22.49    22.42" (Table 2); "Experiments on Chomsky Hierarchy Evaluation Benchmark (CHE)" (Appendix D); "tasks derived from the domain of formal language recognition." (Appendix D)

Task: DUPLICATE STRING (CHE)
Task type: Other (formal language recognition)
Dataset(s): Chomsky Hierarchy Evaluation Benchmark (CHE)
Domain: formal language recognition
Evidence: "D UPLICATE S TRING                 52.77     65.44      64.97       67.66        65.13   66.72    69.03   70.84   72.95    72.71" (Table 2); "Experiments on Chomsky Hierarchy Evaluation Benchmark (CHE)" (Appendix D); "tasks derived from the domain of formal language recognition." (Appendix D)

Task: MISSING DUPLICATE (CHE)
Task type: Other (formal language recognition)
Dataset(s): Chomsky Hierarchy Evaluation Benchmark (CHE)
Domain: formal language recognition
Evidence: "M ISSING D UPLICATE                50.38     49.78      63.37       72.34        74.21   79.06    79.27   83.41   87.57    89.17" (Table 2); "Experiments on Chomsky Hierarchy Evaluation Benchmark (CHE)" (Appendix D); "tasks derived from the domain of formal language recognition." (Appendix D)

Task: ODDS FIRST (CHE)
Task type: Other (formal language recognition)
Dataset(s): Chomsky Hierarchy Evaluation Benchmark (CHE)
Domain: formal language recognition
Evidence: "O DDS F IRST                       52.77     58.61      61.00       61.57        59.88   62.59    63.28   63.78   67.08    66.34" (Table 2); "Experiments on Chomsky Hierarchy Evaluation Benchmark (CHE)" (Appendix D); "tasks derived from the domain of formal language recognition." (Appendix D)

Task: BINARY ADDITION (CHE)
Task type: Other (formal language recognition)
Dataset(s): Chomsky Hierarchy Evaluation Benchmark (CHE)
Domain: formal language recognition
Evidence: "B INARY A DDITION                  54.63     55.78      55.59       56.96        54.72   56.35    55.70   59.71   60.88    56.62" (Table 2); "Experiments on Chomsky Hierarchy Evaluation Benchmark (CHE)" (Appendix D); "tasks derived from the domain of formal language recognition." (Appendix D)

Task: COMPUTE SQRT (CHE)
Task type: Other (formal language recognition)
Dataset(s): Chomsky Hierarchy Evaluation Benchmark (CHE)
Domain: formal language recognition
Evidence: "C OMPUTE S QRT                     50.47     51.11      51.88       51.63        50.63   51.11    50.80   51.64   51.33    52.46" (Table 2); "Experiments on Chomsky Hierarchy Evaluation Benchmark (CHE)" (Appendix D); "tasks derived from the domain of formal language recognition." (Appendix D)

Task: BUCKET SORT (CHE)
Task type: Other (formal language recognition)
Dataset(s): Chomsky Hierarchy Evaluation Benchmark (CHE)
Domain: formal language recognition
Evidence: "DAPE-Alibi achieved scores of 50.30 on PARITY C HECK and 99.38 on B UCKET S ORT tasks, com-" and "pared to the highest scores of 50.97 and 99.57, respectively, demonstrating competitive performances." (Section 4.7); "Experiments on Chomsky Hierarchy Evaluation Benchmark (CHE)" (Appendix D); "tasks derived from the domain of formal language recognition." (Appendix D)

## 4. Domain and Modality Scope
- Evaluation is performed on multiple domains within the same modality (text/sequence), via natural language datasets and formal language recognition tasks. Evidence: "Datasets. Our analysis involves training language models on the Arxiv and Books3 datasets, which are frequently used benchmarks for evaluating model performance [52, 13, 41, 24]." (Section 4, Datasets); "Experiments on Chomsky Hierarchy Evaluation Benchmark (CHE)" and "tasks derived from the domain of formal language recognition." (Appendix D)
- Multiple modalities: Not specified in the paper.
- Domain generalization or cross-domain transfer: Not claimed.

## 5. Model Sharing Across Tasks
| Task | Shared Weights? | Fine-Tuned? | Separate Head? | Evidence |
| --- | --- | --- | --- | --- |
| Language modeling (Arxiv/Books3) | Not specified in the paper. | Not specified in the paper. | Not specified in the paper. | "Datasets. Our analysis involves training language models on the Arxiv and Books3 datasets, which are frequently used benchmarks for evaluating model performance [52, 13, 41, 24]." (Section 4, Datasets) |
| CHE benchmark tasks | Not specified in the paper. | Not specified in the paper. | Not specified in the paper. | "Experiments on Chomsky Hierarchy Evaluation Benchmark (CHE)" and "Following the framework established by [21, 57], we conduct evaluations of our DAPE on a suite of tasks derived from the domain of formal language recognition." (Appendix D) |

## 6. Input and Representation Constraints
- Training sequence lengths for LM experiments: "Experiment settings. Initially, we compare DAPE with other baselines at training lengths of 128, 512, and 1024, with model size 125M decoder-only Transformers [9]" (Section 4, Experiment settings)
- Evaluation sequence length for LM experiments: "Finally, we successfully train the model on sequence length 128 and achieve better performance at evaluation sequence length 8192, compared with other static positional encoding methods, revealing the benefit of the adaptive positional encoding method." (Abstract)
- CHE sequence length range: "Training is conducted on sequences whose lengths are uniformly distributed, sampled from U (1, N ), with N set to 40. Evaluation is performed on sequences that vary in length from N + 1 to M , where M equals 500." (Appendix D)
- Perplexity computation uses last tokens: "In this work, we initially utilize the model to process the entire input sentence and subsequently select the final 256 tokens for perplexity computation." (Section 5)
- Fixed input resolution, patch size, fixed number of tokens (beyond sequence-length settings), fixed dimensionality: Not specified in the paper.

## 7. Context Window and Attention Structure
- Maximum sequence length reported for LM evaluation: 8192. Evidence: "Finally, we successfully train the model on sequence length 128 and achieve better performance at evaluation sequence length 8192, compared with other static positional encoding methods, revealing the benefit of the adaptive positional encoding method." (Abstract)
- Sequence length fixed or variable: Variable across settings (e.g., training lengths 128/512/1024; CHE training lengths U(1, N ) with N=40 and evaluation up to M=500). Evidence: "Experiment settings. Initially, we compare DAPE with other baselines at training lengths of 128, 512, and 1024, with model size 125M decoder-only Transformers [9], whose configuration is shown in Appendix B." (Section 4) and "Training is conducted on sequences whose lengths are uniformly distributed, sampled from U (1, N ), with N set to 40. Evaluation is performed on sequences that vary in length from N + 1 to M , where M equals 500." (Appendix D)
- Attention type: Softmax attention with quadratic cost is described. Evidence: "The attention block was originally designed by applying softmax to the key-query multiplication, which requires quadratic computational cost." (Introduction)
- Mechanisms to manage computational cost: The paper analyzes added cost of DAPE but does not describe windowed/hierarchical/sparse attention for its method. Evidence: "Computational costs analysis. Here, we evaluate the additional computational costs introduced by the DAPE method, compared with the classical positional encoding methods (e.g., Alibi and Kerple)." (Section 3)

## 8. Positional Encoding (Critical Section)
- Mechanism used: Data-adaptive positional encoding based on semantic information and positional indicator, represented by MLPs, compatible with additive relative PEs. Evidence: "DAPE dynamically adjusts the PE based on the semantic information (e.g., the current attention value) a and the positional indicator b. The proposed PE is represented by MLPs due to their universal approximatability, i.e., MLPs(a, b). We note that DAPE is compatible with all additive relative PEs and offers advantages in terms of interpretability and ease of implementation." (Introduction)
- Where applied: The paper specifies DAPE in the pre-softmax attention logit. Evidence: "the pre-softmax attention logit incorporated with DAPE is formulated as" (Section 3.2)
- Fixed/modified/compared across experiments: DAPE is compared against multiple relative PE baselines (e.g., Alibi, Kerple, FIRE). Evidence: "Compared to Alibi, Kerple, and FIRE, the adapted versions DAPE-Alibi, DAPE-Kerple, and DAPE-FIRE demonstrate consistently and significantly better intra-length performance." (Section 4.1)

## 9. Positional Encoding as a Variable
- Core research variable: Yes. Evidence: "In this paper, we introduce a data-adaptive positional encoding (DAPE) method, inspired by the limitations of static PEs." (Introduction)
- Multiple positional encodings compared: Yes (Alibi, Kerple, FIRE vs DAPE variants). Evidence: "Compared to Alibi, Kerple, and FIRE, the adapted versions DAPE-Alibi, DAPE-Kerple, and DAPE-FIRE demonstrate consistently and significantly better intra-length performance." (Section 4.1)
- Claims that PE choice is not critical or secondary: Not claimed.

## 10. Evidence of Constraint Masking
- Model sizes: 125M and 350M are explicitly used; 2.7B and 6.7B are reported in Appendix H. Evidence: "Experiment settings. Initially, we compare DAPE with other baselines at training lengths of 128, 512, and 1024, with model size 125M decoder-only Transformers [9]" and "Subsequently, we evaluate the performance of larger model size 350M" (Section 4); "H    The result on 2.7B and 6.7B" (Appendix H)
- Dataset sizes: Not specified in the paper.
- Attribution of gains: Improvements with scaling are discussed, but gains are attributed to semantically adaptive PEs rather than data scaling. Evidence: "DAPE enhances performance with increasing model sizes." and "These results confirm that DAPE retains its efficacy and continues to perform well even as the model size is increased, mainly due to the adoption of semantically adaptive PEs." (Section 4.2)

## 11. Architectural Workarounds
- Additional MLPs on pre-softmax attentions: "It can also be regarded as a new transformer architecture that empower the transformer with additional MLPs on pre-softmax attentions." (Section 3.2)
- Residual connections for positional information: "A variant of DAPE with residual connections. It is well-known that deep neural networks may suffer from gradient vanishing. To further enhance the practical performances, we introduce the residual connection for positional information." (Section 3.2)
- Multi-head DAPE mechanism: "Multi-head DAPE. In its simplest form, DAPE is considered for a single-head case as described in Equation 2 and Equation 3. However, adopting a multi-head mechanism significantly enhances model capabilities. To effectively combine both semantic and positional information, the DAPE in a multi-head setup processes the key-query similarities and bias matrices from all heads." (Section 3.2)
- Windowed attention, hierarchical stages, token pooling/merging, task-specific heads, fixed grid assumptions: Not specified in the paper.

## 12. Explicit Limitations and Non-Claims
- Stated limitations: The NeurIPS checklist states that limitations are discussed in the Time Cost section. Evidence: "Justification: The we discuss the limitation in the Time Cost part (Section 4.5) of Experiment." (NeurIPS Paper Checklist)
- Future work statements: Not specified in the paper.
- Explicit non-claims about what the model does not attempt to do: Not specified in the paper.
