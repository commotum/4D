1. Basic Metadata
- Title: "Maximizing the Position Embedding for Vision Transformers with Global Average Pooling" (p. 18154)
- Authors: "Wonjun Lee1,2 , Bumsub Ham1 , Suhyun Kim2 *" (p. 18154)
- Year: "Copyright © 2025, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved." (p. 18154)
- Venue: "The Thirty-Ninth AAAI Conference on Artificial Intelligence (AAAI-25)" (p. 18154)

2. One-Sentence Contribution Summary
- The paper proposes MPVG to maximize the effectiveness of position embedding in a Layer-wise Vision Transformer with global average pooling to resolve the GAP + Layer-wise performance conflict and improve accuracy.

3. Tasks Evaluated
- Image classification
  - Task type: Classification
  - Datasets: ImageNet-1K; CIFAR-100
  - Domain: computer vision images
  - Evidence: "We evaluate the performance of our methods on ImageNet-1K (Deng et al. 2009) and CIFAR-100 (Krizhevsky, Hin- ton et al. 2009)." (Image Classification, p. 18158)
  - Evidence: "suitable for computer vision tasks such as image classifica- tion (Dosovitskiy et al. 2020; Touvron et al. 2021; Liu et al. 2021), object detection (Carion et al. 2020; Zhu et al. 2020), and semantic segmentation (Zheng et al. 2021; Wang et al. 2021; Strudel et al. 2021)." (Related Work, p. 18155)
- Object detection
  - Task type: Detection
  - Dataset: COCO 2017
  - Domain: computer vision images
  - Evidence: "On object detection, we evaluate our methods on COCO 2017 (Lin et al. 2014)." (Object Detection, p. 18158)
- Semantic segmentation
  - Task type: Segmentation
  - Dataset: ADE20K
  - Domain: computer vision images
  - Evidence: "On semantic segmentation, we evaluate our methods on ADE20K (Zhou et al. 2019)." (Semantic Segmentation, p. 18159)

4. Domain and Modality Scope
- Single domain or multiple domains: Multiple datasets within the same modality (computer vision). Evidence: "This adaptation makes it suitable for computer vision tasks such as image classifica- tion (Dosovitskiy et al. 2020; Touvron et al. 2021; Liu et al. 2021), object detection (Carion et al. 2020; Zhu et al. 2020), and semantic segmentation (Zheng et al. 2021; Wang et al. 2021; Strudel et al. 2021)." (Related Work, p. 18155) plus dataset-specific evaluation quotes above.
- Multiple modalities: Not specified in the paper.
- Domain generalization or cross-domain transfer: Not claimed.

5. Model Sharing Across Tasks
| Task | Shared Weights? | Fine-Tuned? | Separate Head? | Evidence |
| --- | --- | --- | --- | --- |
| Image classification (ImageNet-1K, CIFAR-100) | Not specified across tasks; per-dataset training; T2T-ViT is transferred to CIFAR-100 | Yes for T2T-ViT-7 on CIFAR-100; otherwise not specified | MLP head for class prediction | "We evaluate the performance of our methods on ImageNet-1K (Deng et al. 2009) and CIFAR-100 (Krizhevsky, Hin- ton et al. 2009)." (Image Classification, p. 18158); "we transfer our pretrained T2T-ViT to downstream datasets such as CIFAR-100 and finetune the pretrained T2T-ViT-7 for 60 epochs" (Image Classification, p. 18158); "the output of this token is then used to make class predic- tions via Multi-Layer Perceptron (MLP)" (Introduction, p. 18154) |
| Object detection (COCO 2017) | Yes, ImageNet-1K pretrained DeiT-Ti backbone used | Yes (trained for detection) | Mask R-CNN head | "For comparison, DeiT-Ti model pretrained on ImageNet-1K with each method is used." (Table 3, p. 18158); "we select the ViT-Adapter-Ti (Chen et al. 2022) model based on Mask R- CNN (He et al. 2017)" (Object Detection, p. 18158) |
| Semantic segmentation (ADE20K) | Yes, ImageNet-1K pretrained DeiT-Ti backbone used | Yes (trained for segmentation) | UperNet head | "For comparison, DeiT-Ti model pretrained on ImageNet-1K with each method is used." (Table 4, p. 18159); "We select the ViT-Adapter- Ti (Chen et al. 2022) model based on UperNet (Xiao et al. 2018)" (Semantic Segmentation, p. 18159) |

6. Input and Representation Constraints
- Fixed input resolution in experiments: "All vision transformers are trained on 224×224 resolution images for 300 epochs, ex- cept T2T-ViT-7, which is trained for 310 epochs." (Image Classification, p. 18158); "ViT-Lite was trained for 310 epochs on 32×32 resolution im- ages with a batch size of 128." (Image Classification, p. 18158)
- Fixed patch size / tokenization: "N represents the number of patches, cal- culated as HW/P 2 , where H and W are the height and width of the image, and P × P is the resolution of each patch." (Preliminary: Absolute Position Embedding, p. 18156)
- Fixed number of tokens: Determined by H, W, and P as above. (Preliminary: Absolute Position Embedding, p. 18156)
- Fixed dimensionality (2D image): "H and W are the height and width of the image." (Preliminary: Absolute Position Embedding, p. 18156)
- Padding/resizing requirements: Not specified in the paper.

7. Context Window and Attention Structure
- Maximum sequence length: Not specified in the paper; token count is defined as "N represents the number of patches, cal- culated as HW/P 2" (Preliminary: Absolute Position Embedding, p. 18156).
- Fixed or variable: Not explicitly stated; experiments use fixed resolutions ("224×224" and "32×32" resolutions). (Image Classification, p. 18158)
- Attention type: Not specified (no statement of global/windowed/sparse). The paper only defines attention as "Multi-head Self-Attention is denoted as MSA." (Methodology, p. 18157)
- Computational cost mechanisms: "GAP results in even less computational complexity because it eliminates the need to compute the attention interaction between the class token and the image patches." (Related Work, p. 18155)

8. Positional Encoding (Critical Section)
- Mechanism: Absolute PE is used and added to tokens: "The method of absolute position embedding used in vision transformers is as follows. As shown in Fig 3-(a), PE is added to the token embedding before they are input into the layer." (Preliminary: Absolute Position Embedding, p. 18156)
- Where applied:
  - Layer-wise delivery: "the Layer-wise structure uses independent LN for token embedding(x ) and PE. PE is delivered in each layer as follows :" (Preliminary: Layer-wise Structure, p. 18157)
  - PVG details: "we combine two structural approaches: (1) adding token embedding and PE before inputting the layer. (2) delivering PE to each layer except the 0th layer. We call this method as PVG." (Maximizing the Position Embedding with GAP, p. 18157)
  - MPVG adds PE to Last LN: "y = LN(xL+1 ) + LN' (pos0 )" (Eq. 11, p. 18157)
- Fixed vs modified/ablated: PE handling is modified and ablated: "We conduct experiments to investigate the impact of varying the PE values passed to the Last LN in MPVG." (Ablation Study, p. 18160)

9. Positional Encoding as a Variable
- Core research variable: "We propose a simple yet effective method called MPVG, which maximizes the effect of PE in the GAP method." (Introduction, p. 18155)
- Multiple PE variants compared: "In this section, we propose two methods, MPVG and PVG, to validate our hypothesis." (Maximizing the Position Embedding with GAP, p. 18157) and "We conduct experiments to investigate the impact of varying the PE values passed to the Last LN in MPVG." (Ablation Study, p. 18160)
- PE not critical? Not claimed; instead, PE is described as impactful: "This counterbalancing effect of PE has a significant im- pact on the performance of vision transformers." (Analysis, p. 18159)

10. Evidence of Constraint Masking
- Model size(s) reported (parameter counts):
  - "DeiT-Ti            LaPE        5.721       72.94" (Table 1, p. 18157)
  - "DeiT-S             LaPE       22.059       80.39" (Table 1, p. 18157)
  - "DeiT-B             LaPE       86.586       82.15" (Table 1, p. 18157)
  - "Swin-Ti            LaPE       28.599       81.48" (Table 1, p. 18158)
  - "CeiT-Ti            LaPE        6.361       76.89" (Table 1, p. 18158)
  - "T2T-ViT-7           LaPE        4.313       72.01" (Table 1, p. 18158)
  - "ViT-Lite            LaPE        3.744       75.52" (Table 2, p. 18158)
  - "T2T-ViT-7            LaPE        4.082       83.41" (Table 2, p. 18158)
- Dataset size(s): Not specified in the paper (dataset names only).
- Attribution of gains: Performance improvements are attributed to PE behavior rather than scale: "This validates our hypothesis and proves that our method is an effective approach to maximizing PE in the GAP method." (Analysis, p. 18159); "This counterbalancing effect of PE has a significant im- pact on the performance of vision transformers." (Analysis, p. 18159)
- Scaling model/data or training tricks as primary factor: Not claimed in the paper.

11. Architectural Workarounds
- GAP to replace class token and reduce cost: "global aver- age pooling (GAP) has been preferred over the class token method due to its translation-invariant characteristics and su- perior performance" (Introduction, p. 18154); "GAP results in even less computational complexity because it eliminates the need to compute the attention interaction between the class token and the image patches." (Related Work, p. 18155)
- Layer-wise PE delivery to improve expressiveness: "the Layer-wise structure uses independent LN for token embedding(x ) and PE. PE is delivered in each layer as follows :" (Preliminary: Layer-wise Structure, p. 18157)
- PVG architectural changes: "we combine two structural approaches: (1) adding token embedding and PE before inputting the layer. (2) delivering PE to each layer except the 0th layer." (Maximizing the Position Embedding with GAP, p. 18157)
- MPVG adds PE to Last LN: "y = LN(xL+1 ) + LN' (pos0 )" (Eq. 11, p. 18157)
- Task-specific heads: "we select the ViT-Adapter-Ti (Chen et al. 2022) model based on Mask R- CNN (He et al. 2017)" (Object Detection, p. 18158); "We select the ViT-Adapter- Ti (Chen et al. 2022) model based on UperNet (Xiao et al. 2018)" (Semantic Segmentation, p. 18159)

12. Explicit Limitations and Non-Claims
- Limitation: "MPVG has a poten- tial limitation in that it is incompatible with the class token method." (Conclusion, p. 18160)
- Future work: "we will further explore the broader applicability of MPVG" (Conclusion, p. 18160)
- Explicit non-claims about open-world or unrestrained multi-task learning: Not specified in the paper.

13. Constraint Profile (Synthesis)
Constraint Profile:
- Domain scope: Single modality (computer vision) across multiple datasets/tasks (image classification, object detection, semantic segmentation). Evidence: "This adaptation makes it suitable for computer vision tasks such as image classifica- tion (Dosovitskiy et al. 2020; Touvron et al. 2021; Liu et al. 2021), object detection (Carion et al. 2020; Zhu et al. 2020), and semantic segmentation (Zheng et al. 2021; Wang et al. 2021; Strudel et al. 2021)." (Related Work, p. 18155)
- Task structure: Multiple tasks evaluated separately on ImageNet-1K, CIFAR-100, COCO 2017, and ADE20K (Section 3 evidence).
- Representation rigidity: Fixed image resolutions in experiments (224×224, 32×32) and patch-based tokenization where "N represents the number of patches, cal- culated as HW/P 2 , where H and W are the height and width of the image, and P × P is the resolution of each patch." (Image Classification, p. 18158; Preliminary: Absolute Position Embedding, p. 18156)
- Model sharing vs specialization: Detection and segmentation use ImageNet-1K pretrained backbones and task-specific heads (Mask R-CNN, UperNet). (Table 3/Table 4 and task sections)
- Role of positional encoding: Central variable (Layer-wise PE delivery and MPVG add PE to Last LN, with ablations). (Sections 8-9)

14. Final Classification
- Multi-task, single-domain.
- Justification: The paper evaluates multiple computer vision tasks ("image classifica- tion (Dosovitskiy et al. 2020; Touvron et al. 2021; Liu et al. 2021), object detection (Carion et al. 2020; Zhu et al. 2020), and semantic segmentation (Zheng et al. 2021; Wang et al. 2021; Strudel et al. 2021)") (Related Work, p. 18155) and reports separate evaluations on ImageNet-1K/CIFAR-100, COCO 2017, and ADE20K. All tasks are within the vision modality, and no multi-modal evaluation is described.
