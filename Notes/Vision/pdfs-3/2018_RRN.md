### 1. Basic Metadata
- Title: Recurrent Relational Networks.
  Evidence: "Recurrent Relational Networks" (Title page).
- Authors: Rasmus Berg Palm; Ulrich Paquet; Ole Winther.
  Evidence: "Rasmus Berg Palm" / "Ulrich Paquet" / "Ole Winther" (Title page).
- Year: 2018.
  Evidence: "32nd Conference on Neural Information Processing Systems (NeurIPS 2018), Montréal, Canada." (Title page).
- Venue (conference/journal/arXiv): 32nd Conference on Neural Information Processing Systems (NeurIPS 2018), Montréal, Canada.
  Evidence: "32nd Conference on Neural Information Processing Systems (NeurIPS 2018), Montréal, Canada." (Title page).

### 2. One-Sentence Contribution Summary
The paper introduces the recurrent relational network, a graph-based module for many-step relational reasoning, and demonstrates it on QA and puzzle-style relational tasks.

### 3. Tasks Evaluated
- bAbI question-answering tasks
  - Task type: Classification; Reasoning / relational.
  - Dataset(s): bAbI (Facebook) QA dataset.
  - Domain: text (natural language).
  - Evidence: "bAbI is a text based QA dataset from Facebook [Weston et al., 2015] designed as a set of prerequisite tasks for reasoning. It consists of 20 types of tasks, with 10,000 questions each, including deduction, induction, spatial and temporal reasoning." (Section 3.1)
  - Evidence: "The target is a single word, in this case “garden”, one-hot encoded over the full bAbI vocabulary of 177 words." (Section 3.1)

- Pretty-CLEVR relational QA
  - Task type: Classification; Reasoning / relational.
  - Dataset(s): Pretty-CLEVR.
  - Domain: synthetic scenes (state descriptions; also rendered images).
  - Evidence: "Pretty-CLEVR consists of scenes with eight colored shapes and associated questions. Questions are of the form: “Starting at object X which object is N jumps away?”." (Section 3.2)
  - Evidence: "If the start object is defined by color, the answer is a shape, and vice versa." (Section 3.2)
  - Evidence: "We also render the scenes as images." (Section 3.2)
  - Evidence: "Since we are solely interested in examining the effect of multiple steps of relational reasoning we train on the state descriptions of the scene." (Section 3.2)

- Sudoku solving
  - Task type: Reasoning / relational; Other (constraint-satisfaction puzzle solving).
  - Dataset(s): Author-created Sudoku dataset (train/validation/test).
  - Domain: symbolic 9x9 grid of digits.
  - Evidence: "A Sudoku consists of 81 cells that are arranged in a 9-by-9 grid, which must be filled with digits 1 to 9 so that each digit appears exactly once in each row, column and 3-by-3 non-overlapping box, with a number of digits given 1 ." (Introduction)
  - Evidence: "We create training, validation and testing sets totaling 216,000 Sudoku puzzles with a uniform distribution of givens between 17 and 34." (Section 3.3)

- Age arithmetic
  - Task type: Reasoning / relational; Other (arithmetic reasoning).
  - Dataset(s): Not specified in the paper (details deferred to supplementary material).
  - Domain: text statements about ages.
  - Evidence: "The task is to infer the age of a person given a single absolute age and a set of age differences, e.g. “Alice is 20 years old. Alice is 4 years older than Bob. Charlie is 6 years younger than Bob. How old is Charlie?”." (Section 3.4)

### 4. Domain and Modality Scope
- Evaluation spans multiple domains and modalities: text QA (bAbI), synthetic scenes (Pretty-CLEVR; state descriptions and rendered images), symbolic grid puzzles (Sudoku), and text arithmetic statements (age arithmetic).
  - Evidence (text QA): "bAbI is a text based QA dataset from Facebook [Weston et al., 2015] designed as a set of prerequisite tasks for reasoning." (Section 3.1)
  - Evidence (synthetic scenes): "Pretty-CLEVR consists of scenes with eight colored shapes and associated questions." and "We also render the scenes as images." (Section 3.2)
  - Evidence (symbolic grid): "A Sudoku consists of 81 cells that are arranged in a 9-by-9 grid, which must be filled with digits 1 to 9 so that each digit appears exactly once in each row, column and 3-by-3 non-overlapping box, with a number of digits given 1 ." (Introduction)
  - Evidence (text arithmetic): "The task is to infer the age of a person given a single absolute age and a set of age differences, e.g. “Alice is 20 years old. Alice is 4 years older than Bob. Charlie is 6 years younger than Bob. How old is Charlie?”." (Section 3.4)
- Domain generalization / cross-domain transfer: Not claimed.

### 5. Model Sharing Across Tasks
| Task | Shared Weights? | Fine-Tuned? | Separate Head? | Evidence |
| --- | --- | --- | --- | --- |
| bAbI (20 tasks) | Yes, across the 20 bAbI tasks (joint training). | Not specified in the paper. | Not specified in the paper (single output for whole graph described). | "Trained jointly on all 20 tasks using the 10,000 training samples." and "At each step, we sum the node hidden states and pass that through a MLP to get a single output for the whole graph." (Section 3.1) |
| Pretty-CLEVR | Not specified in the paper. | Not specified in the paper. | Not specified in the paper. | "We train our network for four steps and compare to a single step relational network and a baseline MLP that considers the entire scene state, all pairwise distances, and the question as a single vector." (Section 3.2) |
| Sudoku | Not specified in the paper. | Not specified in the paper. | Not specified in the paper. | "We run the network for 32 steps and at every step the output function r maps each node hidden state to nine output logits corresponding to the nine possible digits." (Section 3.3) |
| Age arithmetic | Not specified in the paper. | Not specified in the paper. | Not specified in the paper. | "The task is to infer the age of a person given a single absolute age and a set of age differences, e.g. “Alice is 20 years old. Alice is 4 years older than Bob. Charlie is 6 years younger than Bob. How old is Charlie?”." (Section 3.4) |

### 6. Input and Representation Constraints
- bAbI: variable number of facts capped at 20; fully connected graph; LSTM encodings with fixed hidden size.
  - Evidence: "To map the questions into a graph we treat the facts related to a question as the nodes in a fully connected graph up to a maximum of the last 20 facts." (Section 3.1)
  - Evidence: "The fact and question sentences are both encoded by Long Short Term Memory (LSTM) [Hochreiter and Schmidhuber, 1997] layers with 32 hidden units each." (Section 3.1)
- Pretty-CLEVR: fixed 8 objects; fully connected undirected graph; object features include position.
  - Evidence: "Pretty-CLEVR consists of scenes with eight colored shapes and associated questions." (Section 3.2)
  - Evidence: "We consider each scene as a fully connected undirected graph with 8 nodes. The feature vector for each object consists of the position, shape and color." (Section 3.2)
- Sudoku: fixed 9x9 grid (81 cells); node features include row/column positions and digit.
  - Evidence: "A Sudoku consists of 81 cells that are arranged in a 9-by-9 grid, which must be filled with digits 1 to 9 so that each digit appears exactly once in each row, column and 3-by-3 non-overlapping box, with a number of digits given 1 ." (Introduction)
  - Evidence: "We consider each of the 81 cells in the 9x9 Sudoku grid a node in a graph, with edges to and from each other cell in the same row, column and box." and "The node features xi are the output of a MLP which takes as input the digit for the cell (0-9, 0 if not given), and the row and column position (1-9)." (Section 3.3)
- Fixed/variable input resolution, patch size, padding/resizing: Not specified in the paper (inputs are graphs/state descriptions rather than image patches).

### 7. Context Window and Attention Structure
- Maximum sequence length / number of nodes:
  - bAbI: "up to a maximum of the last 20 facts." (Section 3.1)
  - Pretty-CLEVR: "fully connected undirected graph with 8 nodes." (Section 3.2)
  - Sudoku: "81 cells in the 9x9 Sudoku grid" as nodes. (Section 3.3)
- Fixed vs variable:
  - bAbI: variable, capped at 20 facts (Section 3.1).
  - Pretty-CLEVR and Sudoku: fixed (8 nodes; 81 nodes) (Sections 3.2, 3.3).
- Attention type / message aggregation:
  - Global message passing on fully connected graphs for bAbI and Pretty-CLEVR; structured neighbors for Sudoku.
    - Evidence: "fully connected graph up to a maximum of the last 20 facts." (Section 3.1)
    - Evidence: "fully connected undirected graph with 8 nodes." (Section 3.2)
    - Evidence: "edges to and from each other cell in the same row, column and box." (Section 3.3)
  - Aggregation is a sum, described as implicit attention: "the sum can be seen as an implicit attention mechanism over the incoming messages." (Discussion)
- Mechanisms to manage computational cost (windowing/pooling/token pruning): Not specified in the paper.

### 8. Positional Encoding (Critical Section)
- Positional information is provided as explicit input features rather than a transformer-style positional encoding.
  - Evidence (Pretty-CLEVR): "The feature vector for each object consists of the position, shape and color." (Section 3.2)
  - Evidence (Sudoku): "The node features xi are the output of a MLP which takes as input the digit for the cell (0-9, 0 if not given), and the row and column position (1-9)." (Section 3.3)
- Where applied: Input features only; no positional encoding mechanism beyond input features is described. (Not specified beyond the quotes above.)
- Fixed across experiments / modified per task / ablated or compared: Not specified in the paper (see Section 9 for a row/column feature ablation in Sudoku).

### 9. Positional Encoding as a Variable
- The paper does not present multiple positional encodings; it does ablate row/column features in Sudoku:
  - Evidence: "We also examined the importance of the row and column features by multiplying the row and column embeddings by zero and re-tested our trained network." (Section 3.3)
- Multiple positional encodings compared: Not specified in the paper.
- Claim that PE choice is not critical/secondary: Not specified in the paper.

### 10. Evidence of Constraint Masking (Scale vs Structure)
- Dataset sizes:
  - bAbI: "It consists of 20 types of tasks, with 10,000 questions each, including deduction, induction, spatial and temporal reasoning." (Section 3.1)
  - Pretty-CLEVR: "We create 100,000 random scenes, and 128 questions for each (8 start objects, 0-7 jumps, output is color or shape), resulting in 12.8M questions." (Section 3.2)
  - Sudoku: "We create training, validation and testing sets totaling 216,000 Sudoku puzzles with a uniform distribution of givens between 17 and 34." (Section 3.3)
- Model size indicators:
  - "The fact and question sentences are both encoded by Long Short Term Memory (LSTM) [Hochreiter and Schmidhuber, 1997] layers with 32 hidden units each." (Section 3.1)
  - "Alternatively the model learns to compress all the relevant fact-relations into the 128 floats resulting from the sum over the node hidden states, and perform the remaining reasoning steps in the output MLP." (Section 3.1)
- Performance attribution:
  - Architectural (multi-step reasoning): "This shows that multiple steps are crucial for complex relational reasoning." (Section 3.3)
  - Training tricks: "We find that using dropout and appending the question encoding to the fact encodings is important for the performance." (Section 3.1)
- Scaling model size or data as primary driver: Not specified in the paper.

### 11. Architectural Workarounds
- Recurrent message passing:
  - "At each step t each node has a hidden state vector hti , which is initialized to the features, such that h0i = xi . At each step t, each node sends a message to each of its neighboring nodes." (Section 2)
- Message aggregation by summation:
  - "Since a node needs to consider all the incoming messages we sum them with" (Section 2)
- Loss at every step to encourage convergence:
  - "A distinctive feature of our proposed model is that we minimize the cross entropy between the output and target distributions at every step." (Section 2)
- Fully connected graphs when edges are unknown:
  - "If the edges are unknown, the graph can be assumed to be fully connected." (Section 2)
- Implicit attention over messages:
  - "the sum can be seen as an implicit attention mechanism over the incoming messages." (Discussion)

### 12. Explicit Limitations and Non-Claims
- bAbI limitations:
  - "As bAbI is not particularly challenging from a relational reasoning point of view, we introduce Pretty-CLEVR, a new diagnostic dataset for relational reasoning." (Abstract)
  - "It’s possible that there are superficial correlations in the tasks that the model learns to exploit." (Section 3.1)
- Potential training limitation:
  - "One potential issue with having a loss at every step is that it might encourage the network to learn a greedy algorithm that gets stuck in a local minima." (Discussion)
- Explicit non-claims about symbolic reasoning:
  - "We do not aim to bridge symbolic and sub-symbolic methods. Instead we stay completely in the sub-symbolic realm. We do not introduce or consider any explicit logic, aim to discover (fuzzy) logic rules, or attempt to include prior knowledge in the form of logical constraints." (Related work)
