## 1. Basic Metadata
- Title: Searching Latent Program Spaces.
- Authors: Matthew V. Macfarlane; Clement Bonnet.
- Year: 2025.
- Venue: NeurIPS 2025 (39th Conference on Neural Information Processing Systems).
Evidence:
- "Searching Latent Program Spaces" (Title block).
- "arXiv:2411.08706v3 [cs.LG] 25 Nov 2025" (Title block).
- "39th Conference on Neural Information Processing Systems (NeurIPS 2025)." (First page footer).

## 2. One-Sentence Contribution Summary
The paper proposes the Latent Program Network (LPN), a neural architecture that learns a latent program space and performs test-time search to adapt from input-output examples rather than relying on predefined DSLs or full test-time parameter tuning (Abstract; Section 1).
Evidence: "we propose the Latent Program Network (LPN), a novel architecture that builds in test-time search directly into neural models." (Abstract).

## 3. Tasks Evaluated
1) Pattern task
- Task type: Other (program synthesis / grid-to-grid transformation).
- Dataset(s): Pattern task (synthetic grids).
- Domain: Synthetic 2D grids (10x10 inputs, 4x4 patterns).
- Evidence: "It generates 10x10 black input grids with a blue pixel indicating where a 4x4 program-specific pattern should be pasted." (Section 5.1 Setup).

2) ARC-AGI (ARC-AGI 2024 challenge)
- Task type: Other (program synthesis / grid-to-grid transformation).
- Dataset(s): ARC-AGI 2024 challenge; training on re-arc.
- Domain: Synthetic 2D grids up to 30x30 with 10 colors.
- Evidence: "We consider the ARC-AGI 2024 challenge [Chollet et al., 2024] as the testing domain for our method." (Section 5.1 Setup).
- Evidence: "Each of its tasks is composed of input-output pairs represented as 2D grid of shape up to 30x30, whose cells can take any of 10 colors." (Section 3 Background).
- Evidence: "We evaluate LPN on the ARC-AGI benchmark by training it on the re-arc dataset [Hodel, 2024],a dataset designed to be in distribution relative to the ARC training set" (Section A.2 ARC-AGI).

3) String/sequence manipulation task (sequence ablation)
- Task type: Other (program synthesis / sequence transformation).
- Dataset(s): Synthetic sequence dataset.
- Domain: Sequences of discrete numbers 0-4.
- Evidence: "This synthetic dataset features a vast program space, with over 100 million unique programs, each defined by composing 3 to 5 parameterized rules that transform sequences of numbers (ranging from 0 to 4)." (Section B.7.1 Dataset).

4) ARC-AGI training-task decoder validation (task-specific overfit evaluation)
- Task type: Other (program synthesis / grid-to-grid transformation).
- Dataset(s): ARC-AGI training set tasks with re-arc generators.
- Domain: Synthetic 2D grids (ARC-AGI format).
- Evidence: "We show 5 of the 400 total tasks from the ARC-AGI training set, and for each of these tasks, we train a small LPN architecture of 800k parameters (except for the last task which required a bigger model with 8.7M parameters) on the corresponding task generator from re-arc [Hodel, 2024]." (Section B.3 Validating the Decoder).

## 4. Domain and Modality Scope
- Evaluation spans multiple domains/input structures: 2D grid tasks (Pattern and ARC-AGI) and 1D sequence tasks (string manipulation). Evidence for grids: "Each of its tasks is composed of input-output pairs represented as 2D grid of shape up to 30x30, whose cells can take any of 10 colors." (Section 3 Background). Evidence for sequences: "...rules that transform sequences of numbers (ranging from 0 to 4)." (Section B.7.1 Dataset).
- OOD/generalization is explicitly studied within tasks: "A significant challenge to deep learning methods is generalizing to out-of-distribution (OOD) tasks. We study the OOD behavior of LPN, in-context learning, and parameter fine-tuning on the Pattern task..." (Section 5.5 Adapting Out-Of-Distribution).
- Cross-domain transfer is not claimed. (Not specified in the paper.)

## 5. Model Sharing Across Tasks
| Task | Shared Weights? | Fine-Tuned? | Separate Head? | Evidence |
| --- | --- | --- | --- | --- |
| Pattern task | No (task-specific training described) | No weight fine-tuning reported; test-time latent search only | Not specified | "For each training method, we train a small 1M-parameter model for 20k steps..." (Section 5.2 Pattern Task). |
| ARC-AGI | Single model trained on re-arc (no joint multi-task training stated across datasets) | Yes (Grad 0 then Grad 1 fine-tune within ARC-AGI training) | Not specified | "We train a 178M-parameter LPN... We train LPN in Grad 0 mode for 95k steps and then fine tune in Grad 1 for a further 5k steps." (Section A.2 ARC-AGI). |
| String/sequence manipulation | Not specified (described as a separate ablation) | Not specified | Not specified | "we perform an ablation on a synthetic sequence task and replicate the analysis previously conducted on the pattern dataset." (Section B.7.1 Dataset). |
| ARC-AGI decoder validation (5 tasks) | No (per-task models) | Not specified | Not specified | "for each of these tasks, we train a small LPN architecture of 800k parameters..." (Section B.3 Validating the Decoder). |

## 6. Input and Representation Constraints
- Fixed dimensionality for ARC-AGI tasks: "Each of its tasks is composed of input-output pairs represented as 2D grid of shape up to 30x30, whose cells can take any of 10 colors." (Section 3 Background).
- Padding/flattening and shape prefix: "Both the encoder and decoder are small transformers that take flattened padded grids as inputs. The actual number of rows and columns is prefixed to each sequence." (Figure 21, Section G Architecture).
- Fixed per-grid sequence size with shape prefix: "Each grid sequence is prefixed with shape information, namely two extra values for the number of rows and columns, resulting in sequences of 902 values." (Section G Architecture).
- Padding mask: "Padded tokens, determined by the shape values, are masked..." (Section G.1 Encoder).
- Pattern task fixed size: "It generates 10x10 black input grids with a blue pixel indicating where a 4x4 program-specific pattern should be pasted." (Section 5.1 Setup).
- Patch size: Not specified in the paper.
- Fixed number of tokens for non-grid sequence tasks: Not specified in the paper.

## 7. Context Window and Attention Structure
- Maximum sequence length (encoder): "Each grid sequence contains 902 values, and we add an extra CLS token for the output embedding, resulting in a total sequence length of 1805 for the encoder transformer." (Section G.1 Encoder).
- Fixed vs variable length: padding and masking indicate fixed-length sequences after padding. Evidence: "We model the input and output images as 2D grids, which we pad and flatten..." and "Padded tokens, determined by the shape values, are masked..." (Section G).
- Attention type: global (full) attention with masking. Evidence: "In the encoder the attention mask is non-causal, allowing all non-padded tokens to attend to each other during encoding." (Section G.1 Encoder).
- Decoder attention: causal on output portion. Evidence: "Since the decoder generates the output autoregressively, the attention mask is causal on the output grid portion of the sequence (the second half)." (Section G.2 Decoder).
- Compute-management mechanism: "by performing latent program aggregation and recombination in latent space, LPN removes the quadratic cost of attention when scaling specification size." (Section 1 Introduction).

## 8. Positional Encoding (Critical Section)
- Mechanism: RoPE (rotary position embedding). Evidence: "For both the Encoder and Decoder grid positions are encoded using RoPE [Su et al., 2024] for both row and column indices." (Section G Architecture).
- Where applied: grid positions in both encoder and decoder (row and column indices). Evidence: same quote as above (Section G Architecture).
- Input-only vs every layer: Not specified in the paper.
- Fixed across experiments vs modified: The paper reports testing with and without rotational embeddings, and hyperparameter tables show different RoPE settings across experiments. Evidence: "We conducted hyperparameter testing to determine whether to use rotational embeddings. We performed testing by repeating the decoder validation experiment with and without rotational embeddings." (Section E Hyperparameters). Evidence of settings: "RoPE                                       False" (Table 12/13/14) and "RoPE                                   True" (Table 16, ARC-AGI).

## 9. Positional Encoding as a Variable
- Treated as a hyperparameter/architectural choice rather than a core research variable. Evidence: "We conducted hyperparameter testing to determine whether to use rotational embeddings... with and without rotational embeddings." (Section E Hyperparameters).
- Multiple positional encodings compared: compared RoPE vs no RoPE (with/without rotational embeddings). Evidence: same quote as above (Section E).
- Claim that PE choice is not critical or secondary: Not specified in the paper.

## 10. Evidence of Constraint Masking (Scale vs Structure)
- Model sizes reported: "we train a small 1M-parameter model" (Section 5.2 Pattern Task); "We train a 178M-parameter LPN" (Section A.2 ARC-AGI); "we train a small LPN architecture of 800k parameters (except for the last task which required a bigger model with 8.7M parameters)" (Section B.3 Validating the Decoder).
- Dataset size reported: "in terms of data, this amounts to 51M I/O pairs." (Section A.2 ARC-AGI). Sequence dataset size: "over 100 million unique programs" (Section B.7.1 Dataset).
- Performance gains attributed to scaling test-time compute: "All variations of LPN training show strong scaling in performance as the number of gradient steps at test time is scaled." (Section 5.2 Pattern Task). "Out of distribution, LPN doubles its performance by using latent space search at test-time (scaling FLOPs from 2e11 to 2e15)." (Section 5.7 ARC-AGI Results).
- Model-size scaling mention: "Relative to the pattern task, TTT is less susceptible to overfitting on ARC-AGI, possibly due to the use of larger model sizes." (Section 5.7 ARC-AGI Results).

## 11. Architectural Workarounds
- Latent bottleneck and search: "we explicitly factorize inference into three core components... First, we introduce a bottleneck that encourages the network to learn an explicit representation of programs via a compact latent space... Secondly, we introduce a method for searching this latent space" (Section 4).
- Mean aggregation across pairs (permutation invariant, reduces sequence dependence): "By encoding each pair independently and aggregating using the mean LPN is permutation invariant to the specification order" (Section 4.1 Encoder).
- Latent aggregation to avoid attention scaling: "by performing latent program aggregation and recombination in latent space, LPN removes the quadratic cost of attention when scaling specification size." (Section 1 Introduction).
- Fixed grid/padding assumptions: "We model the input and output images as 2D grids, which we pad and flatten..." and "Padded tokens, determined by the shape values, are masked..." (Section G / G.1).

## 12. Explicit Limitations and Non-Claims
- Stated limitations/future work: "A limitation of this work is the limited diversity of programs on which LPN is trained... We limit gradient-based search to standard optimizers. Future work could explore alternative optimization methods, such as evolution strategies, and explore discrete program representations to enhance compositional generalization." (Limitations and Future Work).
- Explicit non-claims about broader capabilities (e.g., open-world, unrestrained multi-task learning): Not specified in the paper.

## 13. Constraint Profile (Synthesis)
Constraint Profile:
- Domain scope: Multiple constrained domains (2D grid ARC-AGI/Pattern and 1D sequence task), all synthetic program-synthesis settings.
- Task structure: Programming-by-example with input-output pairs; tasks are grid-to-grid or sequence transformations.
- Representation rigidity: Fixed 2D grid representation up to 30x30 with padding/flattening to fixed-length sequences (902 per grid) and masking; Pattern task fixed 10x10.
- Model sharing vs specialization: Separate models/experiments for Pattern, sequence ablation, and ARC-AGI; ARC-AGI uses a single model trained on re-arc with internal Grad 0 to Grad 1 fine-tuning.
- Role of positional encoding: RoPE used for grid positions in encoder/decoder; compared against no RoPE and settings vary across experiments.

## 14. Final Classification
Classification: Multi-task, multi-domain (constrained).
Justification: The paper evaluates on multiple tasks across at least two input structures/domains (2D grid ARC-AGI/Pattern and 1D sequence transformations), each defined as programming-by-example with constrained synthetic inputs (Sections 3, 5.1, B.7.1). It does not claim open-world or cross-domain transfer, focusing instead on task-specific OOD generalization and test-time latent search within these constrained settings (Section 5.5).
