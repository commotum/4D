# Survey Answers: VATT (2021)

## 1. Basic Metadata
- Title: VATT: Transformers for Multimodal Self-Supervised Learning from Raw Video, Audio and Text.
  - Evidence (Title page, p.1): "VATT: Transformers for Multimodal Self-Supervised Learning from Raw Video, Audio and Text"
- Authors: Hassan Akbari; Liangzhe Yuan; Rui Qian; Wei-Hong Chuang; Shih-Fu Chang; Yin Cui; Boqing Gong.
  - Evidence (Title page, p.1): "Hassan Akbari" ... "Liangzhe Yuan" ... "Rui Qian" ... "Wei-Hong Chuang" ... "Shih-Fu Chang" ... "Yin Cui" ... "Boqing Gong"
- Year: 2021.
  - Evidence (Title page, p.1): "arXiv:2104.11178v3 [cs.CV] 7 Dec 2021"
- Venue: NeurIPS 2021 (also on arXiv).
  - Evidence (Title page, p.1): "35th Conference on Neural Information Processing Systems (NeurIPS 2021), Sydney, Australia."

## 2. One-Sentence Contribution Summary
The paper introduces VATT, a convolution-free Transformer framework for self-supervised multimodal representation learning from raw video, audio, and text for downstream tasks.

## 3. Tasks Evaluated
Task 1: Video action recognition
- Task type: Classification.
- Dataset(s): UCF101, HMDB51, Kinetics-400, Kinetics-600, Moments in Time.
- Domain: Video (natural videos).
- Evidence (Appendix A.1.2, p.17): "Video action recognition: We evaluate the visual representations on UCF101 [81] (101 classes, 13,320 videos), HMDB51 [52] (51 classes, 6,766 videos), Kinetics-400 [14] (400 classes, 234,584 videos), Kinetics-600 [15] (600 classes, 366,016 videos), and Moments in Time [61] (339 classes, 791,297 videos)."

Task 2: Audio event classification
- Task type: Classification (multi-label).
- Dataset(s): ESC50, AudioSet.
- Domain: Audio (audio clips/waveforms).
- Evidence (Appendix A.1.2, p.17): "Audio event classification: We use ESC50 [66] (50 classes, 2000 audio clips) and AudioSet [33] (527 classes, ∼2M audio clips) to evaluate our audio Transformer on audio event classification."

Task 3: Zero-shot text-to-video retrieval
- Task type: Other (retrieval).
- Dataset(s): YouCook2, MSR-VTT.
- Domain: Video-text (multimodal retrieval).
- Evidence (Appendix A.1.2, p.17): "Zero-shot video retrieval: We evaluate the quality of our video-text common space representations by zero-shot text-to-video retrieval on two of the most established datasets in this area: YouCook2 [109] and MSR-VTT [98] with 3.1k and 1k video-text pairs, respectively."

Task 4: Image classification
- Task type: Classification.
- Dataset(s): ImageNet.
- Domain: Images.
- Evidence (Appendix A.1.2, p.17): "Image classification: Although there exists a domain gap between images and the video datasets used for pre-training VATT, we test the learned vision Transformer in the image domain. We fine-tune the last checkpoint of the vision Transformer on ImageNet [22] with no modification to our architecture or the tokenization pipeline."

Additional task listing confirmation:
- Evidence (Abstract, p.1): "evaluate its performance by the downstream tasks of video action recognition, audio event classification, image classification, and text-to-video retrieval."

## 4. Domain and Modality Scope
- Modalities used: video, audio, text.
  - Evidence (Section 3.1, p.4): "VATT operates on raw signals. The vision-modality input consists of 3-channel RGB pixels of video frames, the audio input is in the form of air density amplitudes (waveforms), and the text input is a sequence of words."
- Evaluation domains: multiple domains and modalities (video, audio, text, and image domain transfer).
  - Evidence (Appendix A.1.2, p.17): "Image classification: Although there exists a domain gap between images and the video datasets used for pre-training VATT, we test the learned vision Transformer in the image domain."
- Domain generalization / cross-domain transfer claim: Yes, cross-domain transfer to images is claimed.
  - Evidence (Abstract, p.1): "Transferring to image classification leads to 78.7% top-1 accuracy on ImageNet compared to 64.7% by training the same Transformer from scratch, showing the generalizability of our model despite the domain gap between videos and images."

## 5. Model Sharing Across Tasks
- Weight-sharing options across modalities are explicitly considered.
  - Evidence (Section 3, p.4): "There are two major settings: 1) The backbone Transformers are separate and have specific weights for each modality, and 2) The Transformers share weights, namely, there is a single backbone Transformer applied to any of the modalities."

| Task | Shared Weights? | Fine-Tuned? | Separate Head? | Evidence |
| --- | --- | --- | --- | --- |
| Video action recognition | Sometimes (modality-agnostic backbone shares weights across video/audio/text; modality-specific uses separate weights). | Yes for Kinetics/Moments; linear classifier on frozen backbone for UCF101/HMDB51. | Yes (linear classifier for small datasets). | "There are two major settings: 1) The backbone Transformers are separate... 2) The Transformers share weights" (Section 3, p.4). "we freeze the vision backbone and use its outputs to train a linear classifier" and "we fine-tune our vision backbone" (Appendix A.1.2, p.17). "We fine-tune VATT’s vision Transformer on Kinetics-400, Kinetics-600, and Moments in Time" (Section 4.2.1, p.6). |
| Audio event classification | Sometimes (modality-agnostic backbone shares weights; modality-specific uses separate weights). | Yes for AudioSet; linear classifier on frozen backbone for ESC50. | Yes (linear classifier for ESC50). | "There are two major settings..." (Section 3, p.4). "We use ESC50 to train a linear classifier on top of the frozen audio Transformer... We also use AudioSet to fine-tune our audio backbone" (Appendix A.1.2, p.17). "We fine-tune VATT’s audio Transformer on AudioSet" (Section 4.2.2, p.7). |
| Image classification | Uses vision backbone (modality-specific or shared settings not specified for this task). | Yes (fine-tuned on ImageNet). | Not specified in the paper. | "We fine-tune the vision Transformer in VATT-BBS on ImageNet" (Section 4.2.3, p.7). "We fine-tune the last checkpoint of the vision Transformer on ImageNet" (Appendix A.1.2, p.17). |
| Zero-shot text-to-video retrieval | Uses video/text backbones and common-space projection heads; shared-vs-separate depends on modality-agnostic vs modality-specific settings. | Not specified beyond using pre-trained VATT-MBS for retrieval evaluation. | Uses common-space projection heads for comparison. | "We feed video-text pairs to VATT-MBS, and extract representations in the Svt space" (Section 4.2.4, p.7). "we define a semantically hierarchical common space" and projection heads to map modalities (Section 3.3, p.5). |

## 6. Input and Representation Constraints
- Modalities and raw inputs are fixed to video/audio/text signals.
  - Evidence (Section 3.1, p.4): "VATT operates on raw signals. The vision-modality input consists of 3-channel RGB pixels of video frames, the audio input is in the form of air density amplitudes (waveforms), and the text input is a sequence of words."
- Video patching constraint and 3D voxel structure.
  - Evidence (Section 3.1, p.4): "We partition an entire video clip of size T × H × W to a sequence of dT /te · dH/he · dW/we patches, where each patch contains t × h × w × 3 voxels."
- Audio patching constraint (1D waveform segments).
  - Evidence (Section 3.1, p.4): "The raw audio waveform is a 1D input with length T 0 , and we partition it to dT 0 /t0 e segments each containing t0 waveform amplitudes."
- Patch sizes are fixed for experiments.
  - Evidence (Appendix A.2.1, p.17): "We use patch sizes of 4 × 16 × 16 and 128 for video and raw waveform tokenization, respectively."
- Fixed sampling/resizing for video pre-training and evaluation.
  - Evidence (Appendix A.2.1, p.17): "During pre-training, we sample 32 frames at 10 fps... resized to 224 × 224" and "For video fine-tuning and evaluation, 32 frames with a temporal stride of 2 are sampled at 25 fps (2.56 seconds) with a crop size of 320 × 320."
- Text length constraint (clipping/padding to max length).
  - Evidence (Appendix A.2.1, p.17): "The resulting sequence retains a maximum of 16 words by either clipping or padding."
- Audio fine-tuning input duration constraint.
  - Evidence (Appendix A.3, p.18): "Hence, we employ the duration of 6.4s with 24kHz sampling rate (153.6k total input samples)."
- Image adaptation constraint (replicate image to fit video patching pipeline).
  - Evidence (Section 4.2.3, p.7): "to satisfy the voxel-to-patch layer we replicate the input image 4 times and feed it to the network. The network sees the input as a single-frame video clip and performs spatial self-attention."

## 7. Context Window and Attention Structure
- Maximum sequence length:
  - Text: explicitly capped.
    - Evidence (Appendix A.2.1, p.17): "The resulting sequence retains a maximum of 16 words by either clipping or padding."
  - Video/audio: Not specified in the paper as a maximum token count (only defined by patching formulas and input sampling).
- Fixed vs variable sequence length:
  - Text is explicitly fixed to a maximum length via clipping/padding.
    - Evidence (Appendix A.2.1, p.17): "maximum of 16 words by either clipping or padding."
  - Video/audio sequence lengths are determined by patching and by the fixed sampling sizes used in experiments.
    - Evidence (Section 3.1, p.4): "We partition an entire video clip of size T × H × W to a sequence of dT /te · dH/he · dW/we patches" and (Appendix A.2.1, p.17): "we sample 32 frames at 10 fps... resized to 224 × 224."
- Attention type: standard global self-attention (no windowing/hierarchy described).
  - Evidence (Section 3.2, p.4): "We use a standard self-attention [88] as the Multi-Head-Attention (MHA) module."
- Computational cost control: DropToken token pruning.
  - Evidence (Section 3.1.1, p.4): "DropToken randomly drops a portion of the video and audio tokens from each input sequence during training... This is crucial for reducing the computational cost because a Transformer’s computation complexity is quadratic."

## 8. Positional Encoding (Critical Section)
- Video positional encoding: learnable, dimension-specific (temporal + horizontal + vertical) absolute embeddings for patch positions.
  - Evidence (Section 3.1, p.4): "To encode the position of these patches, we define a dimension-specific sequence of learnable embeddings as follows: ei,j,k = eTemporal i +eHorizontal j + eVertical k" and "This scheme allows us to use dT /te + dH/he + dW/we positional embeddings to encode all the dT /te · dH/he · dW/we patches in a video clip."
- Audio positional encoding: learnable embeddings per waveform segment.
  - Evidence (Section 3.1, p.4): "We use dT 0 /t0 e learnable embeddings to encode the position of each waveform segment."
- Text positional encoding: removed absolute position embeddings; uses learnable relative bias in attention (first layer).
  - Evidence (Section 3.2, p.4): "In our text model, we remove the position encoding ePOS and add a learnable relative bias to each attention score of the first layer in the MHA module."
- Where applied:
  - Added to input token embeddings for video/audio (and general formulation).
    - Evidence (Section 3.2, p.4): "zin = [xAGG ; x0 WP ; x1 WP ; . . . ; xN WP ] + ePOS".
- Fixed vs modified per task:
  - Positional embeddings are adjusted for higher-resolution video fine-tuning via interpolation and then fine-tuned.
    - Evidence (Appendix A.2.5, p.18): "The video frame resolution is 320 × 320, which results in an increase in the number of positional encoding weights... To generate the new positional embeddings, we create a new set of positional encoding buckets by bi-cubic interpolation from the original buckets. After this step, we fine-tune the entire network, including the positional encoding buckets, end-to-end."

## 9. Positional Encoding as a Variable
- The paper treats positional encoding primarily as an architectural component, not a core research variable; only limited experimentation is reported.
  - Evidence (Appendix A.2.5, p.18): "We tried fixed positional embeddings (solely based on interpolation for the missing locations) and did not observe significant improvements."
- Are multiple positional encodings compared? Not specified in the paper beyond the above fixed-vs-fine-tuned experiment and the text-relative-bias choice.
- Does the paper claim PE choice is “not critical” or secondary? Not specified in the paper (no explicit claim). 

## 10. Evidence of Constraint Masking (Scale, Data, Tricks)
- Model sizes (scale of parameters).
  - Evidence (Appendix A.2.2, p.17): "Small ... 20.9 M"; "Base ... 87.9 M"; "Medium ... 155.0 M"; "Large ... 306.1 M".
- Pre-training data scale.
  - Evidence (Appendix A.1.1, p.17): "HowTo100M... contains 1.2M unique videos... resulting in 136M video-audio-text triplets" and "AudioSet consists of 10-second clips sampled from two million videos from YouTube."
- Downstream dataset sizes (scale of evaluation data).
  - Evidence (Appendix A.1.2, p.17): e.g., "Kinetics-400 ... 234,584 videos"; "Kinetics-600 ... 366,016 videos"; "Moments in Time ... 791,297 videos"; "AudioSet ... ∼2M audio clips".
- Training scale and compute.
  - Evidence (Appendix A.2.4, p.18): "batch size of 2048"; "500k steps in total"; "trained for 3 days using 256 TPUs (v3)."
- Gains attributed to pre-training rather than training from scratch (scale/data effect).
  - Evidence (Section 4.2.1, p.6): "we train a variant from scratch without any pre-training and observe the top-1 and top-5 accuracies of 26.4% and 51.8% on Kinetics-400... The low accuracies verify the efficacy of our pre-training strategy for VATT."
- Training trick to manage token scale (DropToken).
  - Evidence (Section 3.1.1, p.4): "DropToken randomly drops a portion of the video and audio tokens from each input sequence during training... [to reduce] computational cost."

## 11. Architectural Workarounds
- Token reduction to manage quadratic attention cost (DropToken).
  - Evidence (Section 3.1.1, p.4): "DropToken randomly drops a portion of the video and audio tokens from each input sequence during training... computational complexity is quadratic." 
- Modality-agnostic shared backbone as a design to reuse parameters across modalities.
  - Evidence (Abstract, p.1): "we study a modality-agnostic, single-backbone Transformer by sharing weights among the three modalities."
- Modality-specific tokenization and projection before the Transformer.
  - Evidence (Section 3, p.4): "We feed each modality to a tokenization layer, where the raw input is projected to an embedding vector followed by a Transformer."
- Common-space projection heads for cross-modal alignment.
  - Evidence (Section 3.3, p.5): "we define a semantically hierarchical common space mapping" and "projection heads to respectively map the video and audio Transformers’ outputs to the video-audio common space Sva... project the text Transformer’s outputs... to video-text common space, Svt."
- Image adaptation to fit video patching pipeline.
  - Evidence (Section 4.2.3, p.7): "to satisfy the voxel-to-patch layer we replicate the input image 4 times and feed it to the network. The network sees the input as a single-frame video clip."

## 12. Explicit Limitations and Non-Claims
- Limitations explicitly stated:
  - Evidence (Conclusion, p.10): "Firstly, not all videos have organic audio or speech, while our approach depends on meaningful multimodal correspondences."
  - Evidence (Conclusion, p.10): "the text modality currently consists of speech transcripts, which are noisy and sometimes sparse."
  - Evidence (Conclusion, p.10): "The models could be biased if one applies our approach to the multimodal videos that are not representative enough."
  - Evidence (Conclusion, p.10): "our method is still demanding in computation, though we managed to avoid the need for human labels."
- Explicit non-claims about open-world learning, unrestrained multi-task learning, or meta-learning: Not specified in the paper.

## 13. Constraint Profile (Synthesis)
- Domain scope: Multimodal inputs (video/audio/text) with explicit transfer to the image domain; evaluation spans video, audio, text-video retrieval, and ImageNet image classification.
- Task structure: Multiple supervised downstream tasks (classification and retrieval) evaluated on fixed, named datasets.
- Representation rigidity: Fixed patching/tokenization schemes with specified patch sizes; fixed frame sampling and resizing; text clipped/padded to max length.
- Model sharing vs specialization: Supports both modality-specific backbones and a shared modality-agnostic backbone; downstream tasks are fine-tuned or evaluated with frozen backbones and task-specific heads.
- Positional encoding: Learnable positional embeddings for video/audio; relative bias for text; positional embeddings are interpolated and fine-tuned for higher-resolution video fine-tuning.

## 14. Final Classification
Classification: Multi-task, multi-domain (constrained).
Justification: The paper evaluates multiple tasks across multiple modalities and domains, including video action recognition, audio event classification, text-to-video retrieval, and image classification (Appendix A.1.2, p.17). Despite this breadth, evaluation is restricted to specific datasets with fixed experimental pipelines and fine-tuning/linear-head protocols, indicating a constrained multi-domain setup rather than unrestrained multi-task learning.
