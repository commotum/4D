## 1. Basic Metadata

- **Title:** A Length-Extrapolatable Transformer.  
  Quote: "A Length-Extrapolatable Transformer" (Title block, p.14590)
- **Authors:** Yutao Sun, Li Dong, Barun Patra, Shuming Ma, Shaohan Huang, Alon Benhaim, Vishrav Chaudhary, Xia Song, Furu Wei.  
  Quote: "Yutao Sun1∗, Li Dong2 , Barun Patra2 , Shuming Ma2 , Shaohan Huang2 / Alon Benhaim2 , Vishrav Chaudhary2 , Xia Song2 , Furu Wei2" (Title block, p.14590)
- **Year:** 2023.  
  Quote: "July 9-14, 2023 ©2023 Association for Computational Linguistics" (Proceedings header, p.14590)
- **Venue:** Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (ACL 2023), Volume 1: Long Papers.  
  Quote: "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics / Volume 1: Long Papers, pages 14590–14604" (Proceedings header, p.14590)

---

## 2. One-Sentence Contribution Summary

The paper proposes XPOS and a length-extrapolatable Transformer design to improve Transformers’ ability to handle longer sequences by increasing attention resolution and using blockwise causal attention for inference.

---

## 3. Tasks Evaluated

### Task: Language Modeling (long-document perplexity evaluation)
- **Task type:** Generation (language modeling / next-token prediction)
- **Datasets used:** PG22 (Project Gutenberg books post-2019), QMSum (SCROLLS), Arxiv, NarrativeQA
- **Domain:** Natural language text (books, meeting transcripts/summaries, scientific articles, narrative QA text)
- **Evidence / Quotes:**
  - Task definition:  
    "We measure perplexity on long document datasets, which can show the model’s ability for long-  
    dependency modeling." (Section 4.2, p.14594)
  - PG22 dataset:  
    "We use books from Project Gutenberg whose years are later than 2019 to en-  
    sure no overlap with PG19, and we name it as  
    PG22." (Section 4.2, p.14594)
  - QMSum dataset:  
    "Besides, we pick QMSum (Zhong et al., 2021) from SCROLLS (Shaham et al., 2022) with  
    above 9k length on average." (Section 4.2, p.14594)
  - Additional datasets (appendix):  
    "Besides the experiments in Section 4, we run lan-  
    guage modeling evaluation on Arxiv and Narra-  
    tiveQA (Kočiskỳ et al., 2018)." (Appendix A, p.14600)

---

## 4. Domain and Modality Scope

- **Single domain or multiple domains?** Multiple domains within the same modality (text).  
  Evidence: use of multiple text datasets across different sources/domains:  
  "We use books from Project Gutenberg..." and "we pick QMSum... from SCROLLS..." (Section 4.2, p.14594);  
  plus "language modeling evaluation on Arxiv and NarrativeQA" (Appendix A, p.14600).
- **Multiple modalities?** No. All evaluations are on text datasets (language modeling).
- **Domain generalization / cross-domain transfer claimed?** **Not claimed.**  

---

## 5. Model Sharing Across Tasks

Evidence that models are pre-trained once and evaluated on datasets:
"To fairly evaluate different Transformer variants,  
we pre-train the Transformer from scratch." (Section 4.1, p.14594)  
"We measure perplexity on long document datasets..." (Section 4.2, p.14594)

| Task (Dataset) | Shared Weights? | Fine-Tuned? | Separate Head? | Evidence |
| --- | --- | --- | --- | --- |
| Language Modeling (PG22) | Yes (same pre-trained LM) | Not specified | Not specified | "we pre-train the Transformer from scratch" + evaluation via perplexity (Section 4.1–4.2, p.14594) |
| Language Modeling (QMSum) | Yes (same pre-trained LM) | Not specified | Not specified | "we pre-train the Transformer from scratch" + QMSum evaluation (Section 4.1–4.2, p.14594) |
| Language Modeling (Arxiv) | Yes (same pre-trained LM) | Not specified | Not specified | "we run language modeling evaluation on Arxiv" (Appendix A, p.14600) |
| Language Modeling (NarrativeQA) | Yes (same pre-trained LM) | Not specified | Not specified | "we run language modeling evaluation on ... NarrativeQA" (Appendix A, p.14600) |

---

## 6. Input and Representation Constraints

- **Max training length / fixed length during pre-training:**  
  "The maximal length is 1024 for saving  
  memory and extrapolation evaluation." (Section 4.1, p.14594)
- **Variable evaluation lengths:**  
  "For experiment results in Table 2, we divide the  
  same input into the target length to fairly compare  
  the perplexity of different lengths." (Section 4.2, p.14594)
- **Blockwise input partitioning for BCA:**  
  "if the pre-training length  
  is l, we divide the query as blocks with l/2 length,  
  and each query interacts with its own block and  
  the last block." (Section 3.3, p.14593)
- **Tokenizer / representation:**  
  "We use the tokenizer from GPT2 (Radford  
  et al., 2019)." (Section 4.1, p.14594)
- **Patch size / fixed number of tokens / fixed 2D input assumptions:** **Not specified in the paper.**

---

## 7. Context Window and Attention Structure

- **Maximum sequence length (reported in experiments):** 8192.  
  Evidence: "length extends to 8192." (Section 4.2, p.14594)
- **Fixed or variable sequence length:**  
  Training uses a fixed maximum length (1024), evaluation varies by length.  
  Evidence: "The maximal length is 1024..." (Section 4.1, p.14594) and  
  "we divide the same input into the target length..." (Section 4.2, p.14594)
- **Attention type:**  
  - Training: full causal attention (vanilla Transformer).  
    "Our language model is trained on shorter texts in the same way as vanilla Transformers, i.e., using  
    causal masking." (Figure 3 caption, p.14593)
  - Inference for long sequences: blockwise causal attention (windowed).  
    "During inference, we use blockwise  
    causal attention for longer sequences..." (Figure 3 caption, p.14593)  
    "During inference, we use  
    blockwise masking ... for self-attention." (Section 3.3, p.14593)
- **Mechanisms to manage computational cost / long context:**  
  "we use block-  
  wise causal attention because it is cache-friendly  
  and easy to implement." (Section 3.3, p.14593)  
  "the training speed of Blockwise Attention is 1.5x  
  faster than using sliding windows." (Section 4.4.2, p.14596)

---

## 8. Positional Encoding (Critical Section)

- **Mechanism:** Relative positional encoding based on RoPE with an added exponential decay (XPOS).  
  Evidence:  
  "we introduce a relative position embed-  
  ding to explicitly maximize attention resolu-  
  tion." (Abstract, p.14590)  
  "Then, we generalize its mathematical form, where  
  an exponential decay is added to the rotation ma-  
  trix." (Introduction, p.14590–14591)
- **Where applied:** Query and key in attention.  
  Evidence: "Before calculating attention,  
  the query and key are encoded with position in-  
  formation." (Section 3.2, p.14592)
- **Fixed vs modified across experiments / ablations:** Compared against alternatives and ablated.  
  Evidence:  
  "we run the evaluation using  
  different position embeddings (i.e., Absolute, Al-  
  ibi, RO PE, and X P OS) with or without blockwise  
  causal attention." (Section 4.4.2, p.14596)  
  "we discuss the necessity of  
  the combination of vector rotation and exponential  
  decay." (Section 4.4.1, p.14595)

---

## 9. Positional Encoding as a Variable

- **Core research variable?** Yes.  
  Evidence: "We propose an extrapolatable position embed-  
  ding and use blockwise causal attention to  
  improve length extrapolation." (Introduction contributions, p.14590)
- **Multiple positional encodings compared?** Yes.  
  Evidence: "we run the evaluation using  
  different position embeddings (i.e., Absolute, Al-  
  ibi, RO PE, and X P OS)..." (Section 4.4.2, p.14596)
- **Claim that PE choice is “not critical” or secondary?** **Not claimed.**

---

## 10. Evidence of Constraint Masking (Scale vs. Structure)

- **Model size(s):**  
  "We use  
  1024 hidden dimensions, 16 heads, and 24 layers," (Section 4.1, p.14594)  
  Appendix confirms: "24 layers, 1024 hidden size, 4096 FFN inner hid-  
  den size, and 16 attention heads." (Appendix B, p.14600)
- **Dataset size(s):** **Not specified in the paper** (dataset names listed but not sizes).  
  Evidence of datasets used: "The training corpus includes a  
  subset of the Pile... Books3, OpenWebText2, Stack Exchange, PubMed Ab-  
  stracts, Wikipedia, Gutenberg (PG-19), BookCor-  
  pus2, NIH ExPorter, and Pile-CC datasets." (Section 4.1, p.14594)
- **Attribution of gains (scale vs architecture/tricks):**  
  Gains are attributed to XPOS and attention masking rather than scaling model/data.  
  Evidence:  
  "The experiment shows that X P OS gets better per-  
  formance on language modeling." (Section 4.2, p.14594)  
  "The results support that BCA helps the model dis-  
  tinguish positions better, achieving higher attention  
  resolution." (Section 4.3, p.14594–14595)

---

## 11. Architectural Workarounds

- **Blockwise causal attention / windowed attention:**  
  "During inference, we use  
  blockwise masking ... for self-attention." (Section 3.3, p.14593)  
  "we use block-  
  wise causal attention because it is cache-friendly  
  and easy to implement." (Section 3.3, p.14593)
- **Block partitioning for long context:**  
  "if the pre-training length  
  is l, we divide the query as blocks with l/2 length,  
  and each query interacts with its own block and  
  the last block." (Section 3.3, p.14593)
- **Compatibility with other local attention methods:**  
  "Without losing generality, our method is  
  also compatible with Sliding Attention and other  
  local attention variants." (Section 4.4.2, p.14596)
- **XPOS (relative PE with decay) as architectural mechanism:**  
  "an exponential decay is added to the rotation ma-  
  trix." (Introduction, p.14590–14591)

---

## 12. Explicit Limitations and Non-Claims

- **Limitations:**  
  "In this work, we focus on causal language mod-  
  eling. It needs additional efforts to integrate the  
  proposed methods into bidirectional attention, such  
  as masked language modeling (Devlin et al., 2019)." (Limitations, p.14597)  
  "Moreover, X P OS introduces about 6% inference  
  cost compared with absolute position embeddings,  
  although it accelerates training convergence." (Limitations, p.14597)
