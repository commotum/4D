### 1. Basic Metadata
- Title: "EVA02-AT: Egocentric Video-Language Understanding with Spatial-Temporal Rotary Positional Embeddings and Symmetric Optimization" (p. 1)
- Authors: "Xiaoqi Wang, Student Member, IEEE, Yi Wang, Member, IEEE, Lap-Pui Chau, Fellow, IEEE" (p. 1)
- Year: 2025 ("IEEE TRANSACTIONS ON IMAGE PROCESSING, MAY 2025", p. 1; "arXiv:2506.14356v1 [cs.CV] 17 Jun 2025", p. 1)
- Venue: IEEE Transactions on Image Processing ("IEEE TRANSACTIONS ON IMAGE PROCESSING, MAY 2025", p. 1); arXiv preprint ("arXiv:2506.14356v1 [cs.CV] 17 Jun 2025", p. 1)

### 2. One-Sentence Contribution Summary
The paper introduces EVA02-AT, which "efficiently transfers an image-based CLIP model into a unified video encoder via a single-stage pretrain-
ing" and adds "spatial-temporal rotary positional embeddings along with joint attention" plus the "Symmetric Multi-Similarity
(SMS) loss" for egocentric video-language tasks (Abstract, p. 1).

### 3. Tasks Evaluated
- Task: EgoMCQ (Ego4D Multiple-Choice Questions); Task type: Other (multiple-choice video-language QA); Dataset(s): Ego4D (EgoMCQ benchmark); Domain: egocentric video-language. Evidence: "After pretraining, we evaluate models on the Ego4D Multiple-Choice Questions (EgoMCQ) benchmark." (p. 6); "We conduct the experiments on three egocentric
datasets: Ego4D, Epic-Kitchens-100 (EK-100), and Charades-Ego." (p. 6)
- Task: EK-100 multi-instance retrieval (MIR); Task type: Other (video-text retrieval); Dataset(s): Epic-Kitchens-100 (EK-100); Domain: egocentric video-language. Evidence: "Before fine-tuning, we directly evaluate the pretrained model
on EK-100â€™s multi-instance retrieval (MIR) challenge" (p. 6); "We conduct the experiments on three egocentric
datasets: Ego4D, Epic-Kitchens-100 (EK-100), and Charades-Ego." (p. 6)
- Task: Charades-Ego action recognition / video-to-text action recognition; Task type: Classification + Other (video-to-text action recognition); Dataset(s): Charades-Ego; Domain: egocentric video-language. Evidence: "the Charades-Ego action recognition challenge" (p. 6); "CharadesEgo Video-to-Text action
recognition task." (p. 8); "We conduct the experiments on three egocentric
datasets: Ego4D, Epic-Kitchens-100 (EK-100), and Charades-Ego." (p. 6)

### 4. Domain and Modality Scope
- Evaluation scope: Multiple datasets within the same modality (egocentric video-language). Evidence: "We conduct the experiments on three egocentric
datasets: Ego4D, Epic-Kitchens-100 (EK-100), and Charades-Ego." (p. 6); "Egocentric videoâ€“language understanding" (Abstract, p. 1).
- Domain generalization / cross-domain transfer: Not claimed.

### 5. Model Sharing Across Tasks
| Task | Shared Weights? | Fine-Tuned? | Separate Head? | Evidence |
| --- | --- | --- | --- | --- |
| EgoMCQ (Ego4D Multiple-Choice Questions) | Yes (shared pretrained weights) | No (evaluated after pretraining) | Not specified in the paper. | "After pretraining, we evaluate models on the Ego4D Multiple-Choice Questions (EgoMCQ) benchmark." (p. 6) |
| EK-100 MIR | Yes (pretrained model shared; then per-task fine-tune) | Yes | Not specified in the paper. | "Before fine-tuning, we directly evaluate the pretrained model
on EK-100â€™s multi-instance retrieval (MIR) challenge" (p. 6); "After that, we
fine-tune the pretrained model on the training set of these two
benchmarks, respectively" (p. 6) |
| Charades-Ego action recognition | Yes (pretrained model shared; then per-task fine-tune) | Yes | Not specified in the paper. | "Before fine-tuning, we directly evaluate the pretrained model
on EK-100â€™s multi-instance retrieval (MIR) challenge and
the Charades-Ego action recognition challenge" (p. 6); "After that, we
fine-tune the pretrained model on the training set of these two
benchmarks, respectively" (p. 6) |

### 6. Input and Representation Constraints
- Video input is explicitly structured with channels/frames/height/width: "an input video
sequence v âˆˆ Rğ¶ Ã—ğ‘‡ Ã—ğ» Ã—ğ‘Š , where ğ¶, ğ‘‡, ğ», ğ‘Š represents chan-
nels, number of frames, height, and length" (p. 4).
- Patchified token sequence length is tied to frames and patch grid: "yielding a patchified feature
of dimension R ğµÃ— (ğ‘‡ Ã— ğ‘ 2 ) Ã—ğ·" (p. 4).
- Patch embedding uses a fixed tube-conv kernel: "we employ a 3D convolution, also known
as tube convolution [12], with a convolution kernel of 1Ã— ğ‘Ã— ğ‘." (p. 4).
- Learnable spatial/temporal positional embeddings replicated across frames/patches: "We introduce two distinct learnable positional embeddings:
a temporal positional embedding ğ‘ƒğ‘¡ âˆˆ Rğ‘‡ Ã—ğ· and a spatial
positional embedding ğ‘ƒ ğ‘¥ ğ‘¦ âˆˆ R ğ‘ 2 Ã—ğ· . Each temporal positional
embedding is replicated ğ‘ 2 times across the patches of a frame,
while each spatial positional embedding is replicated ğ‘‡ times
to cover all frames." (p. 4)
- Fixed resolution in experiments: "frames are sampled uniformly from each clip at a resolution of
3 Ã— 224 Ã— 224" (p. 6).
- Fixed number of frames per setting: "We evenly sample 4 frames for each video clip." (p. 6); "During
fine-tuning, 16 frames are sampled for each video clip" (p. 6).
- Padding/resizing requirements: Not specified in the paper.

### 7. Context Window and Attention Structure
- Sequence length (tokens) is tied to frames and patch grid: "yielding a patchified feature
of dimension R ğµÃ— (ğ‘‡ Ã— ğ‘ 2 ) Ã—ğ·" (p. 4).
- Sequence length is fixed per experiment setting (T fixed by frame sampling): "We evenly sample 4
frames for each video clip." (p. 6); "During fine-tuning, 16 frames are sampled
for each video clip" (p. 6).
- Attention type: joint + global. Evidence: "joint attention
blocks that process both spatial and temporal information
simultaneously" (p. 4); "the attention score between query and key becomes a global
attention among all the patches in the video clip instead of
the spatial attention on a single frame." (p. 5)
- Cost-management mechanisms (windowing/pooling/token pruning): Not specified in the paper.

### 8. Positional Encoding (Critical Section)
- Mechanism: spatial-temporal RoPE plus learnable spatial/temporal positional embeddings. Evidence: "we introduce spatial-temporal rotary
positional embeddings along with joint attention" (Abstract, p. 1); "We introduce two distinct learnable positional embeddings:
a temporal positional embedding ğ‘ƒğ‘¡ âˆˆ Rğ‘‡ Ã—ğ· and a spatial
positional embedding ğ‘ƒ ğ‘¥ ğ‘¦ âˆˆ R ğ‘ 2 Ã—ğ·" (p. 4).
- Where applied: learnable embeddings before transformer blocks; RoPE in attention (QK-RoPE). Evidence: "Here, ğ‘ƒğ‘¡ğ‘† and ğ‘ƒğ‘‡ğ‘¥ ğ‘¦ denote the final spatial and temporal
positional embeddings before the transformer blocks." (p. 4); "Since we use the standard
QK-RoPE, the output of our joint spatial-temporal attention at
ğ‘˜-th layer can be expressed as:" (p. 5)
- Applied across the entire feature dimension: "we thus apply the spatial RoPE and temporal
RoPE on the entire dimension instead of manually dividing
the dimension into uneven slides." (p. 5)
- Positional encoding compared/ablated: "In table IV, we change the temporal
positional embedding to (a) the learnable positional embed-
ding, (b) 1D-RoPE embedding, and (c) learnable positional
embedding with RoPE embedding." (p. 9)

### 9. Positional Encoding as a Variable
- Treated as a research variable (temporal PE ablation): "In table IV, we change the temporal
positional embedding to (a) the learnable positional embed-
ding, (b) 1D-RoPE embedding, and (c) learnable positional
embedding with RoPE embedding." (p. 9)
- Multiple PEs compared: same evidence as above (p. 9).
- Claim about criticality: The paper notes limited sensitivity: "changing the temporal positional embedding will not in-
fluence the performance significantly, but (c) still outperforms
all the other settings." (p. 9)

### 10. Evidence of Constraint Masking
- Model sizes (parameter counts reported): "EVA02-AT          EgoClip            CLIP-EVA02-AT-B                 86 + 63" and "EVA02-AT         EgoClip+            CLIP-EVA02-AT-L                304 + 124" (Table III, p. 8).
- Dataset sizes: "the EgoClip is
proposed by EgoVLP [8], which contains 3.8 million video-
text pairs for training" and "The EgoClip+ is proposed by LaViLa
[10], which has a 35-million corpus" (p. 6).
- Performance gains attributed to training/architecture choices: "our SMS loss drives
much of this improvement." (p. 8); "EVA02-AT architectures consistently outperform vanilla ViTs" (p. 8).
- Scaling effect noted: "Scaling to a large-size model, the gain boosts
to 9.0% in average mAP" (p. 8).

### 11. Architectural Workarounds
- Single-stage transfer to reduce pretraining cost: "EVA02-AT first efficiently transfers an image-based CLIP
model into a unified video encoder via a single-stage pretrain-
ing." (Abstract, p. 1)
- Joint spatial-temporal attention instead of divided attention: "joint attention
blocks that process both spatial and temporal information
simultaneously are adopted, rather than the divided spatial
and temporal attention used in typical video encoders" (p. 4).
- Integrated spatial-temporal RoPE across full dimension to avoid manual splits: "we thus apply the spatial RoPE and temporal
RoPE on the entire dimension instead of manually dividing
the dimension into uneven slides." (p. 5)
- Tube-conv patch embedding to capture temporal info while keeping image-encoder compatibility: "we employ a 3D convolution, also known
as tube convolution [12], with a convolution kernel of 1Ã— ğ‘Ã— ğ‘.
This convolutional operation effectively captures both the
spatial and temporal information of the video during the
patch embedding phase. The inclusion of temporal dimensions
allows the image encoder to act as a video encoder." (p. 4)
- Training framework + SMS loss to account for negative pair correlations: "we introduce the Symmetric Multi-Similarity
(SMS) loss and a novel training framework that advances all
soft labels for both positive and negative pairs" (Abstract, p. 1).

### 12. Explicit Limitations and Non-Claims
Not specified in the paper.

### 13. Constraint Profile (Synthesis)
- Domain scope: Evaluations are on egocentric video-language datasets (Ego4D, EK-100, Charades-Ego), all within the same modality (p. 6; Abstract, p. 1).
- Task structure: Multiple task types (EgoMCQ MCQ, EK-100 MIR retrieval, Charades-Ego action recognition) within egocentric video-language (p. 6, p. 8).
- Representation rigidity: Fixed video resolution and fixed sampled frame counts per setting (3 Ã— 224 Ã— 224; 4 or 16 frames) with patchified token length tied to T and p (p. 4, p. 6).
- Model sharing vs specialization: Single pretraining on Ego4D-derived data, then zero-shot evaluation and per-benchmark fine-tuning (p. 6).
- Positional encoding role: Central architectural element (integrated spatial-temporal RoPE + learnable PE) and explicitly ablated (p. 1, p. 4, p. 9).

### 14. Final Classification
Multi-task, single-domain. The paper evaluates multiple distinct tasksâ€”EgoMCQ, EK-100 MIR, and Charades-Ego action recognitionâ€”across "three egocentric
datasets" (p. 6), making it multi-task. All evaluations are within egocentric video-language data ("Egocentric videoâ€“language understanding," Abstract, p. 1), so the domain is single (egocentric video-language) rather than multi-domain.
