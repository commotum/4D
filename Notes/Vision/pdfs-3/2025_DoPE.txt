DoPE: Denoising Rotary Position Embedding
Jing Xiong1 * , Liyang Fan3* , Hui Shen2 , Zunhai Su1 ,
Min Yang3‚Ä† , Lingpeng Kong1 , and Ngai Wong1
1
The University of Hong Kong 2 University of Michigan, Ann Arbor
3
Shenzhen Institute of Advanced Technology, Chinese Academy of Sciences
Contact: junexiong@connect.hku.hk Project: https://The-physical-picture-of-LLMs.github.io

arXiv:2511.09146v1 [cs.CL] 12 Nov 2025

Abstract
QRi = RŒ∏(i) Qi ,

Rotary Position Embedding (RoPE) in Transformer models has inherent limits that weaken
length extrapolation. We reinterpret the attention map with positional encoding as a
noisy feature map, and propose Denoising
Positional Encoding (D O PE), a training-free
method based on truncated matrix entropy to
detect outlier frequency bands in the feature
map. Leveraging the noise characteristics of
the feature map, we further reparameterize it
with a parameter-free Gaussian distribution to
achieve robust extrapolation. Our method theoretically reveals the underlying cause of the
attention sink phenomenon and its connection
to truncated matrix entropy. Experiments on
needle-in-a-haystack and many-shot in-context
learning tasks demonstrate that D O PE significantly improves retrieval accuracy and reasoning stability across extended contexts (up to
64K tokens). The results show that the denoising strategy for positional embeddings effectively mitigates attention sinks and restores balanced attention patterns, providing a simple yet
powerful solution for improving length generalization.

1

‚Ä¢ We introduce the truncated matrix entropy to
identify noisy heads and model the position
encoding as a parameter-free Gaussian distribution to achieve length extrapolation.
‚Ä¢ We theoretically and empirically identify that
the low-frequency alignment of positional encodings used for computing attention score
is the fundamental cause of the attention sink
and structural sparsity phenomena such as retrieval heads.

The position encoding is a key component of the
large language models (LLMs), influencing interactions among tokens. Attention scores are computed
as the dot product of query and key vectors:

‚Ä†

Corresponding author

‚Ä¢ We find that attention heads with strong extrapolation ability exhibit low-rank structures;
keeping their position encodings and removing those of high-rank heads yields up to a
10-point improvement without training.

(1)

Position encodings are often added to the query
and key vectors to incorporate sequence order.
Among various techniques, Rotary Position Embedding (RoPE) (Su et al., 2024) is widely used
due to its ability to encode relative positions directly within the dot-product operation, where the
query and key are rotated as:
* Equal contribution

(2)

However, previous work such as DAPE (Zheng
et al., 2024), which replaces position encoding
with additional MLPs on attention scores, and
NoPE (Wang et al., 2024), which operates without any positional encoding, both suggest that
the classical RoPE schemes may constrain Transformer (Vaswani et al., 2017) performance.
In this work, we conceptualize position encoding as a noisy feature map via the truncated matrix
entropy (Xiong et al., 2024). We measure the noise
level of the features and employ a parameter-free
Gaussian distribution to perform length extrapolation. Specifically, the main contributions of this
paper are as follows:

Introduction

R
Attn(i, j) ‚àù ‚ü®QR
i , Kj ‚ü©.

KjR = RŒ∏(j) Kj .

2

Background

In this section, we first review the Transformer architecture of Vaswani et al. (2017), with a focus on
its multi-head attention mechanism. Then, we analyze the role of rotary position embeddings (RoPE)
in shaping the attention patterns.

2.1

Multi-Head Self-Attention

We consider a causal language model implemented
as a decoder-only Transformer. Given token representations X ‚àà Rn√ód , we form queries, keys, and
values via
Q = X√óWQ ,

K = X√óWK ,

V = X√óWV ,
(3)

where a common choice is œâf = b‚àí2f /dh and
Œ∏(m) denotes the vector of per-pair rotation phases
at position m RoPE rotates queries/keys as
QR
i = R(Œ∏i )Qi ,

KjR = R(Œ∏j )Kj .

(10)

Relative-position property. For any positions
i, j and vectors q, k ‚àà Rdh ,

with projection matrices
WQ , WK , WV ‚àà Rd√ó(h√ódh ) ,

where d denotes the model‚Äôs hidden dimension,
and equal per-head widths dh (so each head has
dimension dh and the concatenated width is h√ódh ).
Reshape to per-head tensors:
Q, K, V ‚àà Rh√ón√ódh .

(5)

Let M ‚àà R1√ón√ón be the causal mask, with 0 on
and below the diagonal and ‚àí‚àû above. Masked
attention per head is


QK‚ä§
A = softmax ‚àö
+ M ‚àà Rh√ón√ón ,
dh

(6)

O = AV ‚àà Rh√ón√ódh ,

(7)

where O denotes the output of self-attention.
2.2

‚ü®R(Œ∏i )q, R(Œ∏j )k‚ü© = ‚ü®q, R(Œ∏(j‚àíi) )k‚ü©,

(4)

Rotary Position Embedding

Most LLMs adopt Rotary Position Embedding
(RoPE) (Su et al., 2024) as their default positional encoding mechanism, which has become
the de facto standard in contemporary architectures.
RoPE encodes token positions by rotating each
query/key vector on a sequence of two-dimensional
planes. This formulation allows attention scores to
depend on relative positional offsets while preserving the simple dot-product structure.
Definition. Let the per-head width be dh (assume
dh is even). Split a head into dh /2 complex components by pairing dimensions (2f, 2f +1). f indexes
the dh /2 two-dimensional subspaces, each corresponding to a pair of consecutive feature dimensions used to apply a distinct rotation frequency.
For an integer position m and a frequency schedule
d /2‚àí1
{œâf }fh=0 (with base b>1), define:


cos œï ‚àí sin œï
R(œï) =
,
(8)
sin œï
cos œï
RŒ∏(m) = diag(R(œâ0 m), . . . , R(œâdh /2‚àí1 m)),
(9)

(11)

so the score depends on the relative offset (j‚àíi)
while preserving the efficiency of the dot product.

3

Related Work

Length Extrapolation With RoPE. RoPE has
been widely adopted (Su et al., 2024). It is
employed in models such as LLaMA2 (Touvron
et al., 2023), LLaMA3 (Dubey et al., 2024),
and Qwen3 (Yang et al., 2025) where token order is encoded by rotating vectors at positiondependent angles. Moreover, RoPE and its variants are extensively used in a wide range of recent
large models, including Qwen (Bai et al., 2023),
Qwen2 (Team, 2024a), Qwen2.5 (Team, 2024b),
Qwen2.5-VL (Bai et al., 2025), Qwen3 (Yang et al.,
2025), Mistral (Jiang et al., 2023), Gemma (Team
et al., 2024a), Gemma-2 (Team et al., 2024b), and
Gemma-3 (Team et al., 2025).
However, when the input length exceeds the
training length by several times (Peng et al., 2023;
Chen et al., 2023; Ding et al., 2024), model performance degrades severely. This degradation is
not unique to RoPE; similar issues occur with other
positional encodings such as ALiBi (Press et al.,
2021) and Kerple (Chi et al., 2022). To address this
problem, different approaches have been proposed.
For instance, FIRE (Li et al., 2023) alleviates longcontext degradation by introducing learnable positional encodings, where MLPs are used to generate suitable positional representations. In contrast,
NTK-based methods (Peng et al., 2023) improve
long-context extrapolation by modifying the frequency spectrum, thereby extending the context
length and enhancing stability on long sequences.
Length Extrapolation Without Positional Encoding. Positional encodings provide sequence
awareness and enhance a model‚Äôs expressive capacity (Shaw et al., 2018; Yun et al., 2019; Luo
et al., 2022). While several studies (Zuo et al.,
2024; Haviv et al., 2022; K√∂cher et al., 2025)

have shown that causal decoder-based Transformers can implicitly capture token order information,
NoPE (Kazemnejad et al., 2023) further demonstrates that the causal mask itself inherently encodes positional relationships.
More recently, data-dependent positional encoding methods, such as DAPE (Zheng et al., 2024)
and DAPE v2 (Zheng et al., 2024), have been proposed to enhance length extrapolation by treating
positional encodings as input-dependent feature
maps of the attention. However, these approaches
still rely on a learnable parameter matrix to construct the positional encoding.

4

Denoising Positional Encoding

4.1

Outlier Features

‚ü®u, R(Œ∏j )k‚ü© ‚â• ‚à•k‚à• cos Œ≥k

Form the Gram (covariance) matrix on this band:
Œ£k =

N
X

Convention. Let QR , KR ‚àà RN √ódh denote the
RoPE-rotated queries/keys. For RoPE band f , define the band-projected matrices
K‚Ä≤f = Pf KR ,

(12)

R2√ódh

where Pf
‚àà
selects coordinates
(2f, 2f +1). We work per head; extension to multiple heads is by concatenation.
Band-wise Gram Matrix. Consider a single
RoPE frequency band acting on a 2-D subspace
via planar rotations. For position j, let the key
projected onto this band be
b
kj = Œ≤j R(Œ∏j ) k,

Œ≤j ‚â• Œ≤min > 0,

b
kj b
k‚ä§
j

j=1

=

N
X

(15)
Œ≤j2 R(Œ∏j ) kk‚ä§ R(Œ∏j )‚ä§ .

j=1

Spectral Lower Bound. Let the sum of projected
P
b
keys be S = N
j=1 kj and set x = S/‚à•S‚à•. By the
Rayleigh quotient and Cauchy‚ÄìSchwarz,

=

(13)

‚â•

N
1  X b 2
‚ü®x, kj ‚ü©
N j=1

=

‚à•S‚à•2
.
N

(16)

Using the cone condition with x aligned to the
mean direction gives
‚à•S‚à• =

N
X

Œ≤j R(Œ∏j )k

j=1

‚â•

N
X

(17)
Œ≤j ‚ü®x, R(Œ∏j )k‚ü©

j=1

‚â• N Œ≤min ‚à•k‚à• cos Œ≥k ,

and hence the spectral lower bound
2
Œªmax (Œ£k ) ‚â• N Œ≤min
‚à•k‚à•2 cos2 Œ≥k ,
(18)
p
‚àö
œÉ1 (K‚Ä≤f ) = Œªmax (Œ£k ) ‚â• Œ≤min |k‚à• N cos Œ≥k .

An entirely analogous argument for queries matrix on the same band yields
œÉ1 (Q‚Ä≤f ) ‚â• Œ±min ‚à•q‚à•

‚àö
N cos Œ≥q ,

(19)

with amplitudes Œ±k ‚â• Œ±min > 0 and cone
half‚Äìangle Œ≥q for the query directions. Now consider the attention score submatrix contributed by
this band,
Af =

where Œ≤j denotes the per-position magnitude of
the key on this RoPE band (i.e., the ‚Ñì2 norm of
its component in the 2-D plane), Œ≤min > 0 is a
fixed lower bound ensuring nondegeneracy over the
visible window, Œ∏j is the RoPE phase at position j,
and k is a unit direction in that plane.
Assume a low‚Äìfrequency cone condition (Deshpande et al., 2014) within the positional encoding
window: for angles Œ∏j there exists a unit vector u

N
X
(‚ü®x, b
kj ‚ü©)2
j=1

Spectral Analysis via the Gram Matrix

Q‚Ä≤f = Pf QR ,

for all j. (14)

Œªmax (Œ£k ) ‚â• x‚ä§ Œ£k x

Recent studies (Jin et al., 2025; Qiao and Huang,
2025) reveal that RoPE can induce outlier channels in the query and key representations: a small
subset of low-frequency rotary bands exhibit abnormally large ‚Ñì2 norms, and give rise to distinctive row/column ‚Äúbright-band‚Äù patterns in the
QK ‚ä§ matrix. This subsection formalizes this phenomenon through a simple spectral analysis of individual RoPE bands.
4.2

aligned with the mean direction and a half‚Äìangle
Œ≥k < œÄ2 such that

Q‚Ä≤f K‚Ä≤f‚ä§
‚àö
.
d

(20)

Aligning left/right singular directions of Q‚Ä≤f and
K‚Ä≤f (up to an angular mismatch œà between their
principal directions) gives the product‚Äìtype lower
bound
1
œÉ1 (Af ) ‚â≥ ‚àö œÉ1 (Q‚Ä≤f ) œÉ1 (K‚Ä≤f ) cos œà
d
Œ±min Œ≤min
‚àö
‚â≥
N ‚à•q‚à• ‚à•k‚à• cos Œ≥Q cos Œ≥K cos œà.
d
(21)

RoPE

Head Selection
Q for Head 1

‚Ä¶

Q for Head 3

Q for Head 2

Denoised Head

Q for Head n

Applied Dynamic NTK
Position Embedding

Q for Head 2

Denoised NTK

Denoising

K for Head 2

K for Head 2
Mask NTK at
Low Frequency Indices

Q for Head 2

‚Ä¶

‚Ä¶

K for Head 3

K for Head 3

Q for Head 3

Q for Head 3

ùëêùëúùë† ùúÉ0

sin ùúÉ0

Mask dimension of frequencies i

ùëî ùúÉùëñ = ·âä
Head 2
Q for Head 1
Q for Head 2

Low
Entropy

Head 1

Truncated
ERank

Head 2

Entropy
Rank

‚Ä¶

Head 3

RoPE

Q for Head 2
Q for Head 3

‚Ä¶

Q for Head n

Head 3

‚Ä¶

Head 1

High
Entropy

Head n

‚Ä¶

Extrapolated Head

K for Head 2

‚Ä¶

Q for Head 2

‚Ä¶

K for Head 3

10
cos ùúÉùëñ sin ùúÉùëñ

2ùúã
training_length

ùúÉùëñ < ùúÉ
ùúÉùëñ ‚â• ùúÉ

K for Head 3

Unmasked Band
cos ùúÉùëñ sin ùúÉùëñ

sin ùúÉùëñ

ùëêùëúùë† ùúÉùëñ

ùëî ùúÉùëñ

~

Masked Band
10

high-frequency

low-frequency
freq < Œ∏

freq ‚â• Œ∏

DoPE-by-Gaussian

DoPE-by-Parts

Head n
Q for Head 3

0

NTK

K for Head 2
Keep NTK at
All Frequency Indices

ùúÉ=

sin ùúÉùëñ

ùëêùëúùë† ùúÉùëñ

1

Q for Head 3

Figure 1: Visualization of DoPE

where œà is the angle between the principal left
singular direction of Q‚Ä≤f and the principal right
singular direction of K‚Ä≤f in the band plane, and
if œàÃÑ := Ã∏ (uQ , uK ) denotes the angle between
the band-wise mean directions, then by the cone
conditions |œà ‚àí œàÃÑ|‚â§ Œ≥Q + Œ≥K , hence cos œà ‚â•
cos(œàÃÑ + Œ≥Q + Œ≥K ).
These inequalities formalize the following physical picture: when a RoPE band is sufficiently
low‚Äìfrequency so that its rotations stay within a
cone (no phase reversals over the context), the projected keys and queries add coherently in that 2D
plane. The resulting Gram matrices acquire a dominant eigenvalue that scales like
‚àö Œò(N ) (hence top
singular values scale like Œò( N )), producing a
pronounced principal direction‚Äîthe large‚Äìnorm
‚Äúspike‚Äù observed in practice. When both Q and K
cohere on the same band, the score matrix gains
an approximately rank‚Äìone dominant component
with a large top singular value, which then yields
row/column‚Äìwise bright bands after the softmax.
Applying Lemma A.1 to Af gives
Œ±min Œ≤min
‚àö
max |(Af )ij | ‚â≥
‚à•q‚à• ‚à•k‚à• cos Œ≥Q cos Œ≥K cos œà,
i,j
d
(22)

i.e. at least one score entry remains ‚Ñ¶(1) rather
than vanishing with N .
4.3

Denoising via Truncated Matrix Entropy

Multi-band Matrix Entropy. For attention head
h, let the RoPE-rotated key matrix be kR
h ‚àà
RN √ódh , and let Pk project onto RoPE frequency
band f . The band-wise Gram matrix is
Œ£h,f = k‚Ä≤h,f‚ä§ k‚Ä≤h,f =

N
X

(h,f ) b(h,f )‚ä§
b
kj
kj
‚àà R2√ó2 .

j=1

(23)
Let
Œ£ÃÉh,f =

Œ£h,f
tr(Œ£h,f )

(24)

be its trace-normalized form. The matrix entropy
of band f in head h is defined as
Hh,f = ‚àí tr(Œ£ÃÉh,f log Œ£ÃÉh,f ), 0 ‚â§ Hh,f ‚â§ log 2.
(25)
Aggregating over all RoPE bands within the
head gives
dh /2

1 X
Hh =
Hh,f ,
dh /2

0 ‚â§ Hh ‚â§ log 2.

f =1

(26)
A small Hh indicates that several RoPE bands collapse to low-rank spectra (dominated by coherent
low-frequency spikes), while a large Hh implies
isotropic covariance and thus a denoised, balanced
positional encoding within the head.
Truncated Matrix Entropy. Following Xiong
et al. (2024), we introduce the shorthand for the
effective rank of the band Gram matrix
eh = exp(Hh ),

(27)

so that

1
.
(28)
eh
A smaller œÅh,f (low spectral entropy) corresponds
to a more concentrated spectrum and thus a larger
principal eigenvalue. Combining this with the earlier spectral bound suggests the approximate proportionality
œÅ‚àíHh =

Œªmax (Œ£h ) ‚àù œÅ‚àíHh .

(29)

In practice, we are often interested not in the full
effective rank, but in its truncated version, which
discounts negligible spectral mass. Given eigenvalues Œª1 ‚â• Œª2 ‚â• ¬∑ ¬∑ ¬∑ ‚â• Œªr of Œ£ÃÉh and a threshold
r ‚àà {1, 8, 16, 32, 64}, the truncated effective rank
is defined as
!
r
X
Œª
Œª
i
Pr i
œÅrh = exp ‚àí
log Pr
,
Œª
j
j=1
j=1 Œªj
i=1
(30)

where r denotes the number of the top r singular values. This truncated version measures the
effective dimensionality of the dominant spectrum,
ignoring low-energy tails.
For low-frequency RoPE bands, œÅrh typically collapses to O(1), reflecting a near rank‚Äìone structure,
while for denoised or decorrelated representations
it approaches the full dimension (‚âà 2 in the band
plane), indicating isotropy.
Head Selection. To avoid uniformly modifying
all attention heads, we perform selection at the
head level based on the truncated matrix entropy. A
smaller œÅhr indicates that this head exhibits stronger
near-rank-one ‚Äúspike‚Äù structures across multiple
frequency bands and is thus more likely to produce
bright-band artifacts. Accordingly, we define a
head-level mask
mh = 1[œÅhr ‚â• œÑ ],

(31)

where the threshold œÑ can be chosen as a quantile
(e.g., selecting the lowest-œÑ heads). Only when
mh = 1 do we remove the positional encoding for
this head; otherwise, the RoPE positional encoding of that head remains unchanged (all bands are
retained).
In this way, only heads with low truncated matrix
entropy (i.e., more ‚Äúspiky‚Äù and anisotropic spectra)
have their low-entropy frequency bands attenuated
or masked, while other heads preserve their original RoPE encoding. This targeted denoising suppresses coherent low-frequency modes responsible
for positional artifacts, without degrading overall
positional representation capacity.
DoPE-by-parts. In practice, denoising can be
achieved by selectively attenuating or removing
RoPE components associated with low matrix entropy (small œÅrh ), i.e., frequency bands whose Gram
spectra are highly concentrated and thus dominated
by a single coherent direction. These bands correspond to ‚Äúoutlier‚Äù rotary modes that produce brightband artifacts in the attention map.
Formally, for the selected head h, we construct
a frequency band mask based on the threshold:
h
i
mh,f = 1 Œ∏f ‚â§ Œ∏ ,
(32)
where Œ∏ is a threshold chosen to retain sufficiently isotropic bands while filtering those dominated by low-rank spikes.
Œ∏=

2œÄ
,
L

(33)

where L is the training length. The denoised key
representation is then obtained by
dh /2

KR,D
h

=

X

mh,f P‚ä§f Pf KR
h,

(34)

f =1

and analogously for queries QR,D
h . This operation
removes coherent low-rank positional modes while
preserving isotropic components that contribute to
balanced attention, and eliminates the persistent
‚Äúbright-band‚Äù patterns (i.e., attention sinks) in the
attention score matrix, achieving a denoised and
more uniform positional encoding.
DoPE-by-all. In this variant, denoising is performed by applying the head-level mask to the entire positional encoding of each head, rather than
completely zeroing out the head. Specifically, for
each head h, we multiply its RoPE-rotated queries
and keys by the scalar mask mh ‚àà {0, 1}:
KR,D
= mh KR
h,
h

QR,D
= mh QR
h.
h

(35)

Heads with mh = 0 (i.e., low truncated matrix
entropy œÅhr ) have their entire positional encoding
removed, while those with mh = 1 retain their
original RoPE positional features. This ‚ÄúDoPE-byall‚Äù strategy masks the positional encoding at the
head level in a single step, removing anisotropic
or low-rank positional modes while preserving the
remaining heads‚Äô balanced representations.
DoPE-by-Gaussian. In this variant, denoising
is performed by applying the head-level mask to
the positional encoding and replacing the removed
parts with Gaussian noise. Specifically, for each
head h, we define
KR,D
= mh KR
h + (1 ‚àí mh ) œµK,h ,
h

(36)

QR,D
= mh QR
h + (1 ‚àí mh ) œµQ,h ,
h

(37)

‚àº N (0, œÉ 2 I) are Gaussian noise

where œµK,h , œµQ,h
matrices whose variance œÉ 2 matches the empirical
variance of the retained RoPE components. Thus,
heads with mh = 0 (low truncated matrix entropy)
have their positional encoding replaced by isotropic
Gaussian samples, while those with mh = 1 retain their original RoPE positional features. This
‚ÄúDoPE-by-Gaussian‚Äù strategy suppresses coherent
low-rank positional modes and injects isotropic
randomness that restores spectral diversity, acting
as a stochastic regularization mechanism for the
attention representation.

Table 1: Summary of experimental configurations and results for denoising strategies. The Indicator column
denotes whether the matrix entropy is computed using the Query or Key representations for selecting specific heads.
Entropy Type denotes the entropy measure used: Vanilla matrix entropy or Trunc-r (truncated effective rank œÅrh,k
with threshold r). # Heads indicates the number of attention heads selected for denoising. Criterion refers to the
computation stage of entropy: ntk (after applying the NTK positional encoding), pre_ntk (before NTK scaling), or
post_rope (after RoPE application). Sort Order specifies the masking direction: ASC removes heads with the lower
entropy (i.e., low-entropy heads), while DESC removes heads with the higher entropy (i.e., high-entropy heads).
Results are reported for two extrapolation lengths: 24,756 (24k) and 65,536 (64k). This table is filtered to show the
top 3 results for the Noisy (64k) and Original (64k) settings, respectively, for each parameter combination.
Method

Indicator

Entropy Type

# Heads

Criterion

Sort Order

Noisy (24k)

Original (24k)

Noisy (64k)

Original (64k)

‚Äì

‚Äì

‚Äì

‚Äì

‚Äì

75.417

91.896

40.417

60.938

D O PE-by-Gaussian
D O PE-by-Gaussian
D O PE-by-Gaussian
D O PE-by-Gaussian
D O PE-by-Gaussian
D O PE-by-Gaussian
D O PE-by-Gaussian
D O PE-by-Gaussian
D O PE-by-Gaussian
D O PE-by-Gaussian
D O PE-by-Gaussian

Query
Key
Key
Query
Key
Key
Key
Key
Query
Query
Query

Full
Trunc-32
Trunc-16
Trunc-16
Trunc-16
Trunc-8
Trunc-4
Full
Trunc-1
Trunc-1
Full

5
3
5
5
3
1
3
2
5
3
5

post_ntk_query
post_ntk_key
pre_ntk_key
pre_ntk_query
pre_ntk_key
post_ntk_key
post_ntk_key
post_ntk_key
post_ntk_query
post_ntk_query
post_ntk_query

DESC
ASC
ASC
ASC
ASC
DESC
DESC
DESC
ASC
ASC
ASC

62.521
84.354
77.417
77.104
77.438
75.250
65.833
73.229
75.167
72.583
44.833

94.938
94.396
93.708
93.563
93.125
92.229
89.354
90.188
92.938
89.688
76.188

23.208
40.875
40.604
25.521
41.271
45.667
45.375
44.229
42.208
41.479
44.042

36.813
60.896
60.313
46.813
60.021
64.042
61.979
64.292
70.083
69.438
65.854

D O PE-by-parts
D O PE-by-parts
D O PE-by-parts
D O PE-by-parts
D O PE-by-parts
D O PE-by-parts
D O PE-by-parts
D O PE-by-parts
D O PE-by-parts
D O PE-by-parts
D O PE-by-parts

Key
Query
Key
Key
Key
Query
Query
Query
Query
Query
Query

Trunc-32
Trunc-32
Trunc-32
Trunc-32
Trunc-32
Trunc-16
Trunc-8
Trunc-8
Trunc-32
Full
Trunc-32

30
25
30
20
25
2
2
3
3
3
5

post_rope_key
post_ntk_query
post_ntk_key
post_ntk_key
post_ntk_key
post_ntk_query
post_ntk_query
post_ntk_query
post_rope_query
post_rope_query
post_ntk_query

ASC
ASC
ASC
ASC
ASC
DESC
DESC
DESC
ASC
ASC
DESC

76.229
76.604
76.458
76.042
76.104
75.438
75.229
75.271
74.500
74.125
75.438

93.063
93.042
92.875
92.854
92.771
92.354
91.771
92.146
92.125
92.479
91.958

40.312
40.458
40.771
40.188
40.021
42.729
42.521
42.438
40.313
40.125
40.938

60.375
61.917
61.333
60.625
61.083
60.729
61.104
59.583
62.208
62.146
62.125

D O PE-by-all
D O PE-by-all
D O PE-by-all
D O PE-by-all
D O PE-by-all
D O PE-by-all
D O PE-by-all
D O PE-by-all
D O PE-by-all
D O PE-by-all
D O PE-by-all

Key
Key
Key
Key
Query
Key
Key
Query
Query
Query
Query

Trunc-32
Trunc-16
Trunc-16
Full
Full
Trunc-8
Trunc-4
Trunc-1
Trunc-1
Trunc-1
Trunc-1

3
3
3
3
3
1
3
2
5
3
3

post_ntk_key
post_rope_key
pre_ntk_key
post_ntk_key
pre_ntk_query
post_ntk_key
post_ntk_key
post_ntk_query
post_ntk_query
post_ntk_query
post_rope_query

ASC
DESC
ASC
DESC
ASC
DESC
DESC
DESC
ASC
ASC
DESC

81.958
65.958
76.583
75.625
73.542
74.917
65.958
75.104
75.000
73.104
46.771

93.833
93.771
93.729
93.271
93.250
92.000
89.813
92.354
92.917
90.063
87.521

40.917
35.354
41.354
39.729
39.333
46.000
45.292
44.292
42.729
41.646
27.000

61.271
61.063
57.833
58.021
63.146
63.625
62.646
64.146
70.083
69.708
69.104

Dynamic NTK

5

Experiment

5.1

Experimental Setup

The "needle-in-a-haystack" synthesis task presents
a particularly challenging problem in the field of
natural language processing and information retrieval. The essence of this task is to identify and
synthesize highly relevant but sparse information
from large volumes of data, where the key insights
are hidden amidst vast amounts of irrelevant or
less useful content. This challenge is akin to finding a "Needle-in-a-Haystack", where the desired
information is not only rare but often deeply embedded in long, complex documents or across multiple sources. The experiments are divided into two
parts: original setups and noisy setups.
Original Setups. In the baseline experiments, we
insert the needle at various positions within the con-

text under three settings of context length‚Äî24K
and 64K tokens. This design enables us to assess
the model‚Äôs capacity to retrieve specific information across different contextual spans.
Noisy Setups. In contrast, the noisy experiments
are performed under the same two context-length
configurations (24K and 64K tokens), wherein attention sink symbols are placed adjacent to the needle to introduce controlled perturbations. This table
is filtered to show the top 3 results for the Noisy
(64k) and Original (64k) settings, respectively, for
each parameter combination. This experimental design enables a systematic evaluation of the model‚Äôs
robustness and stability under noisy or unreliable
data, providing deeper insights into its resilience
and potential real-world applicability.
In contrast, the noisy experiments are conducted

Table 2: Summary of experimental configurations and results for denoising strategies on Qwen2.5-Math-7B
extrapolation in the Many-Shot In-Context Learning task. Model is extrapolated from 4K context to 16K context
window. Experiments utilize In-Context Learning (ICL) constructed from the nlile/hendrycks-MATH-benchmark
dataset. Two experimental settings are evaluated: (1) Needle Insertion‚Äîproblem inserted at specific depth positions
within ICL haystack, with four possible positions (beginning, 1/3, 2/3, end); (2) Skip Needle‚Äîno problem insertion,
baseline performance. Indicator specifies whether denoising is applied to Query or Key representations. Entropy
(Œµ)
Type denotes the entropy measure used: Full (full matrix entropy Hh,k ) or Trunc-Œµ (truncated effective rank œÅh,k
with threshold Œµ). # Heads indicates the number of attention heads selected for denoising. Criterion refers to the
computation stage of entropy: ntk (after NTK scaling), pre_ntk (before NTK scaling), or post_rope (after RoPE
application). Sort Order determines the selection direction‚ÄîDESC (highest entropy) or ASC (lowest entropy).
Results are accuracy scores on 100 sampled MATH problems (400 total configurations across 4 insertion positions).
Method

Indicator

Entropy Type

# Heads

Criterion

Sort Order

Needle Insert (8K)

Skip Needle (8K)

Needle Insert (16K)

Skip Needle (16K)

Zero-shot Baseline
Many-shot Baseline

‚Äì
‚Äì

‚Äì
‚Äì

‚Äì
‚Äì

‚Äì
‚Äì

‚Äì
‚Äì

0.430
0.373

0.430
0.370

0.430
0.240

0.430
0.230

D O PE-by-Gaussian
D O PE-by-Gaussian
D O PE-by-Gaussian
D O PE-by-Gaussian
D O PE-by-Gaussian
D O PE-by-Gaussian
D O PE-by-Gaussian
D O PE-by-Gaussian
D O PE-by-Gaussian
D O PE-by-Gaussian

Query
Query
Query
Query
Query
Query
Query
Query
Query
Query

Trunc-1
Trunc-16
Trunc-1
Trunc-4
Trunc-1
Trunc-4
Trunc-1
Full
Full
Trunc-16

1
1
3
5
5
3
2
1
3
3

post_ntk_query
post_ntk_query
post_ntk_query
post_ntk_query
post_ntk_query
post_ntk_query
post_ntk_query
post_ntk_query
post_ntk_query
post_ntk_query

ASC
ASC
ASC
ASC
ASC
ASC
ASC
DESC
DESC
ASC

0.393
0.380
0.375
0.375
0.318
0.358
0.345
0.388
0.370
0.355

0.410
0.360
0.370
0.440
0.440
0.430
0.380
0.400
0.340
0.420

0.228
0.225
0.238
0.225
0.238
0.223
0.258
0.258
0.255
0.248

0.250
0.250
0.220
0.190
0.220
0.210
0.240
0.230
0.270
0.260

D O PE-by-parts
D O PE-by-parts
D O PE-by-parts
D O PE-by-parts
D O PE-by-parts
D O PE-by-parts
D O PE-by-parts
D O PE-by-parts
D O PE-by-parts
D O PE-by-parts

Query
Query
Query
Query
Query
Query
Query
Query
Query
Query

Trunc-1
Trunc-16
Trunc-4
Trunc-4
Trunc-8
Trunc-1
Trunc-16
Full
Full
Full

1
2
5
3
3
5
5
1
2
3

post_ntk_query
post_ntk_query
post_ntk_query
post_ntk_query
post_ntk_query
post_ntk_query
post_ntk_query
post_ntk_query
post_ntk_query
post_ntk_query

ASC
ASC
ASC
ASC
ASC
ASC
ASC
DESC
DESC
DESC

0.388
0.380
0.368
0.360
0.363
0.355
0.365
0.375
0.400
0.388

0.410
0.330
0.390
0.420
0.390
0.350
0.380
0.350
0.380
0.390

0.230
0.245
0.220
0.240
0.220
0.245
0.243
0.245
0.258
0.258

0.250
0.260
0.260
0.230
0.180
0.240
0.260
0.240
0.230
0.250

D O PE-by-all
D O PE-by-all
D O PE-by-all
D O PE-by-all
D O PE-by-all
D O PE-by-all
D O PE-by-all
D O PE-by-all
D O PE-by-all
D O PE-by-all
D O PE-by-all
D O PE-by-all

Query
Query
Query
Query
Query
Query
Query
Query
Query
Query
Query
Query

Trunc-1
Trunc-4
Trunc-8
Trunc-1
Trunc-1
Trunc-4
Trunc-8
Trunc-16
Full
Trunc-1
Trunc-16
Trunc-16

1
2
2
5
3
5
3
5
3
2
1
3

post_ntk_query
post_ntk_query
post_ntk_query
post_ntk_query
post_ntk_query
post_ntk_query
post_ntk_query
post_ntk_query
post_ntk_query
post_ntk_query
post_ntk_query
post_ntk_query

ASC
ASC
ASC
ASC
ASC
ASC
ASC
ASC
DESC
ASC
ASC
ASC

0.395
0.383
0.383
0.338
0.353
0.375
0.375
0.360
0.393
0.363
0.365
0.353

0.430
0.390
0.390
0.480
0.440
0.440
0.440
0.360
0.350
0.380
0.370
0.340

0.235
0.215
0.225
0.243
0.258
0.220
0.205
0.263
0.258
0.243
0.228
0.253

0.240
0.240
0.220
0.220
0.210
0.200
0.190
0.240
0.210
0.250
0.250
0.240

Table 3: Ablation study: Performance on 64k extrapolation using attention heads selected at different sequence
lengths. Each configuration uses heads identified from sequences of length 24k, 32k, 48k, 56k, and 64k, then
evaluates on the 64k task under both Noisy and Vanilla conditions. Scores demonstrate the impact of head selection
length on final performance.
24k Heads
Method
Dynamic NTK
D O PE-by-Gaussian
D O PE-by-Gaussian
D O PE-by-parts
D O PE-by-parts
D O PE-by-all
D O PE-by-all

32k Heads

48k Heads

56k Heads

64k Heads

Indicator

Entropy Type

# Heads

Criterion

Sort Order

Noisy

Vanilla

Noisy

Vanilla

Noisy

Vanilla

Noisy

Vanilla

Noisy

‚Äì

‚Äì

‚Äì

‚Äì

‚Äì

40.417

60.938

40.417

60.938

40.417

60.938

40.417

60.938

40.417

60.938

Key
Query
Query
Query
Key
Query

Trunc-8
Trunc-1
Trunc-16
Trunc-32
Trunc-8
Trunc-1

1
5
2
3
1
5

post_ntk_key
post_ntk_query
post_ntk_query
post_rope_query
post_ntk_key
post_ntk_query

DESC
ASC
DESC
ASC
DESC
ASC

40.896
35.667
41.792
40.313
40.625
37.063

62.438
56.708
61.271
61.688
62.208
61.292

40.417
30.354
41.479
39.333
40.541
32.687

63.125
61.979
65.020
66.979
65.229
65.000

28.666
43.604
41.479
39.333
29.604
43.000

60.104
69.166
65.020
66.979
59.979
75.187

28.666
38.020
41.479
39.333
29.604
40.458

60.104
69.854
65.020
66.979
59.979
73.812

45.667
42.208
42.729
40.313
46.000
42.729

64.042
70.083
60.729
62.208
63.625
70.083

under the same two context-length settings (24K
and 64K tokens), but noise tokens such as the startof-sequence symbol (which easily form attention
sinks) are inserted after the needle to emulate imperfect conditions. This experimental design enables
us to assess the model‚Äôs robustness and stability
in the presence of noise or attention sink, thereby
offering insights into the relationship between the
model‚Äôs attention sink and matrix entropy.

Vanilla

Hyperparameters of Head Selection. Head selection is performed globally across all (l √ó h)
attention heads, where l is the number of layers
and h is the number of heads per layer (32 layers
√ó 32 heads = 1,024 total heads for LLaMA-38B; 28 layers √ó 28 heads = 784 total heads for
Qwen2.5-Math-7B). We experiment with selecting
1‚Äì32 heads based on either ascending (ASC, selecting lowest entropy heads) or descending (DESC,
selecting highest entropy heads) order.

Table 4: Ablation study on attention head identification.
The table compares the performance when using different datasets (MATH vs. NIH) to select attention heads
for denoising, followed by testing the results in manyshot in-context learning. All experiments are conducted
on 8K context with Qwen2.5-Math-7B model. Head
selection is performed using Query representations with
post-NTK criterion. Results show accuracy scores on
MATH problems across two settings: Needle Insertion
(answer inserted within ICL haystack) and Skip Needle
(a setting where answers are not inserted into the context).
Method
Zero-shot Baseline
Many-shot Baseline

Entropy Type # Heads Selection Dataset Needle Insert (8K) Skip Needle (8K)
‚Äì
‚Äì

‚Äì
‚Äì

‚Äì
‚Äì

0.430
0.240

0.430
0.230

5
1
3
1
5
1

MATH
MATH
MATH
MATH
MATH
MATH

0.375
0.393
0.360
0.388
0.338
0.395

0.440
0.410
0.420
0.410
0.480
0.430

3
1
2
2
5
2

NIH
NIH
NIH
NIH
NIH
NIH

0.365
0.375
0.372
0.350
0.360
0.417

0.390
0.410
0.330
0.390
0.420
0.390

Heads selected using MATH dataset
D O PE-by-Gaussian
D O PE-by-Gaussian
D O PE-by-parts
D O PE-by-parts
D O PE-by-all
D O PE-by-all

Trunc-4
Trunc-1
Trunc-4
Trunc-1
Trunc-1
Trunc-1

Heads selected using NIH dataset
D O PE-by-Gaussian
D O PE-by-Gaussian
D O PE-by-parts
D O PE-by-parts
D O PE-by-all
D O PE-by-all

Full
Trunc-16
Trunc-16
Full
Trunc-16
Trunc-1

Entropy can be computed at three different
stages in the forward pass. By comparing and analyzing these three stages, we can capture different
aspects of positional encoding effects: (1) pre-NTK:
entropy is computed on the original query/key representations after the projection layer but before
any PE is applied, reflecting the model‚Äôs behavior
without positional encoding; (2) post-NTK: entropy
is computed after applying Dynamic-NTK scaling
to the RoPE base frequency Œ∏, capturing the effect
of frequency scaling on the covariance structure;
(3) post-RoPE: entropy is computed after the full
RoPE rotation has been applied to the query/key
representations, measuring the final positional encoding‚Äôs impact on attention patterns.
Additionally, entropy can be computed separately for query representations, key representations. This yields six possible entropy computation strategies (3 stages √ó 2 components), plus
the option to compute entropy on both query and
key jointly. In our experiments, we denote these
configurations using the notation Criterion (e.g.,
post_ntk_query means entropy is computed on
query representations after NTK scaling). For the
NIH task, head selection is performed using sequences at the target extrapolation length (e.g.,
heads identified on 64K sequences are used for
64K evaluation, as shown in Table 3). For the
many-shot in-context learning task, we compare

head selection using entropy scores computed on
both the MATH dataset and the NIH dataset to
evaluate cross-task transferability (Table 4).
All experiments are conducted using
SGLang (Zheng et al., 2023) (v0.5.3rc0) with
the FlashAttention-3 backend (Shah et al., 2024).
Tensor parallelism is enabled for multi-GPU
inference when necessary. CUDA graphs are
disabled to support dynamic context lengths.
5.2

Main results

We conduct experiments under two settings: original setups and noisy setups. Our results are presented in Table 1. Our findings are summarized
as follows: (i) The model exhibits a sharp performance degradation after introducing attention
sink tokens, i.e., under the noisy setting. (ii) Under the shorter context setting (24k tokens), D O PE
achieves its best performance when Gaussian noise
is added to the positional encodings, improving
from the 75.417 baseline to 84.354. This indirectly
supports the insight that the layer-wise accumulation of repeatedly applied positional encodings is
well modeled by a Gaussian distribution. (iii) Truncated matrix entropy and (vallina) matrix entropy
exhibit distinctly different patterns. For the truncated variant, we sort values in descending order
and prune the low-entropy heads; for the matrix
entropy, we sort in ascending order and prune the
high-entropy heads. Both strategies perform well,
but truncated matrix entropy typically achieves better results. (iv) In extremely sparse regimes‚Äîfor
example, with a 64K context length‚Äîusing the
truncated matrix entropy with r = 1 (which can
be regarded as equivalent to the spectral norm,
i.e., œÉmax (Œ£)) yields the best results. This indicates that the sparser the setting, the sharper (more
sparse) the singular value distribution becomes.
5.3

Many-Shot In-Context Learning

We present the model‚Äôs performance under manyshot in-context learning (MICL) scenarios (Agarwal et al., 2024) in Table 2. Experiments are conducted both with test exemplar inserted into the
in-context exemplars (needle-in-a-haystack) and
without test exemplars (in-context learning). This
task not only depends on the model‚Äôs ability to find
a needle in a haystack, but also tests whether the
model can identify similar reasoning patterns from
the context. In simple terms, we have the following
findings.

High Entropy Head (Average)
Key: layer_5_head_11

Last Token Attention
Needle Position

0.025

High Entropy Head (Last Token)
Key: layer_4_head_12
Average Attention
Needle Position

0.012

0.08

0.015
0.010

0.008
0.006
0.004

0.005

0.005

0.002

0.000

0.000

0.000

0

5000

10000
15000
Token Position

20000

25000

0

5000

Low Entropy Head (Average)
Key: layer_1_head_2

20000

5000

10000
15000
Token Position

20000

25000

0

0.025

0.0005

0.0000

0.0000
0

5000

10000
15000
Token Position

20000

25000

0.010

10000
15000
Token Position

20000

0.015
0.010
0.005

0.000
5000

25000

0.020

0.015

0.005

0

20000

Last Token Attention
Needle Position

0.025

Attention Weight

Average Attention

Attention Weight

Average Attention

0.0010

10000
15000
Token Position

Low Entropy Head (Last Token)
Key: layer_5_head_11
Average Attention
Needle Position

0.020

0.0005

5000

Low Entropy Head (Average)
Key: layer_5_head_11

0.0015

0.0010

0.04

0.00
0

25000

Last Token Attention
Needle Position

0.0020

0.0015

0.06

0.02

Low Entropy Head (Last Token)
Key: layer_1_head_2
Average Attention
Needle Position

0.0020

10000
15000
Token Position

Attention Weight

0.010

Average Attention

Attention Weight

0.020

0.015

Last Token Attention
Needle Position

0.10

0.010

0.020
Average Attention

High Entropy Head (Average)
Key: layer_4_head_12

High Entropy Head (Last Token)
Key: layer_5_head_11
Average Attention
Needle Position

0.025

0.000
0

25000

5000

(a) DoPE by Vanilla Matrix Entropy.

10000
15000
Token Position

20000

25000

0

5000

10000
15000
Token Position

20000

25000

(b) DoPE by Truncated Matrix Entropy.

Figure 2: Comparison of attention weight entropy across all heads and top-16 heads (depth 0).
Cosine Similarity of High-Entropy Head L5H11 Near Question Tokens
Needle Inserted at 0% Context Depth

0

Needle Start/End

1.00

0.75

5

Cosine Similarity of Low-Entropy Head L1H2 Near Question Tokens
Needle Inserted at 0% Context Depth

0

Needle Start/End

0.75

5

0.50

0.50

0.00
20

0.25

25

0.50

0

20

40

60
80
Cosine Dimension (0-127)

100

120

Cosine Similarity of High-Entropy Head L4H12 Near Question Tokens
Needle Inserted at 0% Context Depth
Needle Start/End

0.50

0.75

0

20

40

60
80
Cosine Dimension (0-127)

100

120

1.00

Figure 4: Low matrix entropy head (Layer 1, Head 2)

1.00

Cosine Similarity of Low-Entropy Head L5H11 Near Question Tokens
Needle Inserted at 0% Context Depth

0

0.75

5

0.25

30

1.00

Figure 3: High matrix entropy head (Layer 5, Head 11)
0

0.00
20

25

0.75

30

0.25
15

Cosine Similarity

0.25
15

Token Position (Absolute)

10

Cosine Similarity

Needle Start/End

1.00

0.75

5

0.50

0.50
10

0.00
20

Cosine Similarity

0.25
15

0.25

25

0.50

0.75

30

0

20

40

60
80
Cosine Dimension (0-127)

100

120

1.00

Token Position (Absolute)

10

0.25
15
0.00
20

Cosine Similarity

Token Position (Absolute)

10

Token Position (Absolute)

1.00

0.25

25

0.50

0.75

30

0

20

40

60
80
Cosine Dimension (0-127)

100

120

1.00

Figure 5: High truncated matrix entropy example (Layer
4, Head 12)

Figure 6: Low truncated matrix entropy example (Layer
5, Head 11)

The Curse of Length. At an appropriate length,
MICL can significantly enhance the model‚Äôs reasoning ability. However, when the length extends to
16K, the model‚Äôs final reasoning ability drops significantly. More exemplars does not lead to better
performance, indirectly demonstrating that complex reasoning is constrained by the extrapolation
length.

The Curse of Shortcut. We inserted exemplars
of the test samples into the in-context examples.
Surprisingly, rather than copying the correct answers in a ‚Äúneedle-in-a-haystack‚Äù manner, the
model‚Äôs overall performance dropped substantially
at the 24K and 64K context lengths.

5.4

Matrix Entropy Meets Attention Sink

In this section, we directly visualize the attention
distributions of the heads identified by the hightruncated matrix entropy. Figure 2 clearly reveals
the connection between truncated matrix entropy
and the attention distribution, particularly the attention sink.
We can conclude from Fig. 2b that when the
truncated matrix identify low-entropy heads, they
tend to produce severe attention sink (recency bias),
while the remaining high-entropy heads correctly
allocate attention to the inserted needle. In contrast, in Fig. 2a, the overall high matrix entropy
indicates a serious attention sink, and although the
low-entropy heads generate relatively normal attention distributions, they fail to locate the correct
needle position.
5.5

RoPE Induces Low-rankness

To demonstrate the low-rank characteristics of the
truncated matrix entropy left by the attention heads,
this section visualizes the similarity of token representations and the principal component singular value components of attention heads selected
by our two distinct entropy metrics: Vanilla Matrix Entropy (denoted Hh ) and Top-16 Truncated
(16)
Matrix Entropy (denoted œÅh ). Figures 3 and 6
visualize the cosine similarity between the query
vectors of tokens (Y-axis, token position) and the
eigenvectors corresponding to their full 128 hiddendimensional eigenvalues (X-axis). This projection
onto a k = 128 basis allows us to observe the
effective dimensionality each head utilizes. Our
analysis reveals a insight: the two entropy metrics
identify two functionally distinct attention heads
with low-rank structures.
Low-rankness. We observe a clear low-rankness
in the heads selected by the entropy. Figure 4 and
Figure 5 shows a head keep by its low matrix entropy and high truncated matrix entropy, respectively. Visually, its similirity (color) is mainly concentrated in the first few of the total dimensions.
This ‚Äúlow-rank‚Äù head shows that it relies on only a
few dimensions to perform extrapolation, meaning
that heads with good extrapolation performance
(retrieval heads) use only a small subset of features
for retrieval.
Periodicity. Figure 5 shows that the similarity
distribution of the head selected by the truncated
matrix entropy exhibits clear periodicity along the

vertical axis (sequence length), compared to the
head selected by the matrix entropy shown in Figure 4. This observation also reveals why the truncated matrix entropy can identify heads that perform better in extrapolation than those selected by
the full matrix entropy.

6

Conclusion

We presented Denoising Positional Encoding
(D O PE), a parameter-free approach that mitigates
low-rank artifacts in Rotary Position Embedding
through truncated matrix entropy analysis. By identifying noisy heads and suppressing low-entropy
frequency bands, D O PE effectively reduces attention sinks and restores balanced positional representations. Experiments on long-context and noisy
settings confirm that D O PE improves extrapolation and reasoning stability across various models. This work highlights that truncated matrix entropy provides a simple yet principled direction for
enhancing positional encoding and long-sequence
generalization in Transformers.

References
Rishabh Agarwal, Avi Singh, Lei Zhang, Bernd Bohnet,
Luis Rosias, Stephanie Chan, Biao Zhang, Ankesh
Anand, Zaheer Abbas, Azade Nova, and 1 others.
2024. Many-shot in-context learning. Advances in
Neural Information Processing Systems, 37:76930‚Äì
76966.
Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang,
Xiaodong Deng, Yang Fan, Wenbin Ge, Yu Han, Fei
Huang, and 1 others. 2023. Qwen technical report.
arXiv preprint arXiv:2309.16609.
Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie
Wang, Jun Tang, and 1 others. 2025. Qwen2. 5-vl
technical report. arXiv preprint arXiv:2502.13923.
Guanzheng Chen, Xin Li, Zaiqiao Meng, Shangsong
Liang, and Lidong Bing. 2023. CLEX: Continuous
length extrapolation for large language models. In
International Conference on Learning Representations.
Ta-Chung Chi, Ting-Han Fan, Peter J Ramadge, and
Alexander Rudnicky. 2022. KERPLE: Kernelized
relative positional embedding for length extrapolation. Advances in Neural Information Processing
Systems, 35:8386‚Äì8399.
Yash Deshpande, Andrea Montanari, and Emile Richard.
2014. Cone-constrained principal component analysis. Advances in Neural Information Processing
Systems, 27.
Yiran Ding, Li Lyna Zhang, Chengruidong Zhang,
Yuanyuan Xu, Ning Shang, Jiahang Xu, Fan Yang,
and Mao Yang. 2024. LongRoPE: Extending llm context window beyond 2 million tokens. arXiv preprint
arXiv:2402.13753.
Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey,
Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman,
Akhil Mathur, Alan Schelten, Amy Yang, Angela
Fan, and 1 others. 2024. The llama 3 herd of models.
arXiv e-prints, pages arXiv‚Äì2407.
emoZilla. 2023. Dynamically scaled rope further
increases performance of long context llama with
zero fine-tuning. https://www.reddit.com/r/
LocalLLaMA/comments/14mrgpr/dynamically_
scaled_rope_further_increases/. Reddit post.

Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul
Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. 2021. Measuring mathematical problem solving with the math dataset. In Proceedings of
the Thirty-Fifth Conference on Neural Information
Processing Systems (NeurIPS) Datasets Benchmarks
Track.
Albert Q. Jiang, Alexandre Sablayrolles, and 1 others.
2023. Mistral 7b. arXiv preprint arXiv:2310.06825.
Mingyu Jin, Kai Mei, Wujiang Xu, Mingjie Sun, Ruixiang Tang, Mengnan Du, Zirui Liu, and Yongfeng
Zhang. 2025. Massive values in self-attention modules are the key to contextual knowledge understanding. arXiv preprint arXiv:2502.01563.
Amirhossein Kazemnejad, Inkit Padhi, Karthikeyan
Natesan Ramamurthy, Payel Das, and Siva Reddy.
2023. The impact of positional encoding on length
generalization in transformers. Advances in Neural
Information Processing Systems, 36:24892‚Äì24928.
Chris K√∂cher, Alexander Kozachinskiy, Anthony Widjaja Lin, Marco S√§lzer, and Georg Zetzsche.
2025. Nope: The counting power of transformers with no positional encodings. arXiv preprint
arXiv:2505.11199.
et al. Li. 2023. Qwen technical report. arXiv preprint
arXiv:2309.16609.
Shanda Li, Chong You, Guru Guruganesh, Joshua
Ainslie, Santiago Ontanon, Manzil Zaheer, Sumit
Sanghai, Yiming Yang, Sanjiv Kumar, and Srinadh
Bhojanapalli. 2023. Functional interpolation for relative positions improves long context transformers.
In International Conference on Learning Representations.
Shengjie Luo, Shanda Li, Shuxin Zheng, Tie-Yan Liu,
Liwei Wang, and Di He. 2022. Your transformer may
not be as powerful as you expect. Advances in Neural
Information Processing Systems, 35:4301‚Äì4315.
Bowen Peng, Jeffrey Quesnelle, Honglu Fan, and Enrico
Shippole. 2023. YaRN: Efficient context window
extension of large language models. In International
Conference on Learning Representations.
Ofir Press, Noah Smith, and Mike Lewis. 2021. Train
short, test long: Attention with linear biases enables
input length extrapolation. In International Conference on Learning Representations.

Aaron Grattafiori, Abhimanyu Dubey, Abhinav Jauhri,
Abhinav Pandey, Abhishek Kadian, Ahmad AlDahle, . . . , and 1 others. 2024. The llama 3 herd
of models. arXiv preprint arXiv:2407.21783.

Ye Qiao and Sitao Huang. 2025. Q-roar: Outlier-aware
rescaling for rope position interpolation in quantized
long-context llms. arXiv preprint arXiv:2509.14391.

Adi Haviv, Ori Ram, Ofir Press, Peter Izsak, and Omer
Levy. 2022. Transformer language models without
positional encodings still learn positional information.
In Findings of the Association for Computational
Linguistics: EMNLP 2022, pages 1382‚Äì1390.

Jay Shah, Ganesh Bikshandi, Ying Zhang, Vijay
Thakkar, Pradeep Ramani, and Tri Dao. 2024.
Flashattention-3: Fast and accurate attention with
asynchrony and low-precision. Advances in Neural
Information Processing Systems, 37:68658‚Äì68685.

Peter Shaw, Jakob Uszkoreit, and Ashish Vaswani. 2018.
Self-attention with relative position representations.
In Proceedings of the 2018 Conference of the North
American Chapter of the Association for Computational Linguistics: Human Language Technologies,
Volume 2 (Short Papers), pages 464‚Äì468.
Jianlin Su, Murtadha Ahmed, Yu Lu, Shengfeng Pan,
Wen Bo, and Yunfeng Liu. 2024. Roformer: Enhanced transformer with rotary position embedding.
Neurocomputing, 568:127063.
Gemma Team, Aishwarya Kamath, Johan Ferret, Shreya
Pathak, Nino Vieillard, Ramona Merhej, Sarah Perrin,
Tatiana Matejovicova, Alexandre Ram√©, Morgane
Rivi√®re, and 1 others. 2025. Gemma 3 technical
report. arXiv preprint arXiv:2503.19786.
Gemma Team, Thomas Mesnard, Cassidy Hardin,
Robert Dadashi, Surya Bhupatiraju, Shreya Pathak,
Laurent Sifre, Morgane Rivi√®re, Mihir Sanjay Kale,
Juliette Love, and 1 others. 2024a. Gemma: Open
models based on gemini research and technology.
arXiv preprint arXiv:2403.08295.
Gemma Team, Morgane Riviere, Shreya Pathak,
Pier Giuseppe Sessa, Cassidy Hardin, Surya Bhupatiraju, L√©onard Hussenot, Thomas Mesnard, Bobak
Shahriari, Alexandre Ram√©, and 1 others. 2024b.
Gemma 2: Improving open language models at a
practical size. arXiv preprint arXiv:2408.00118.
Qwen Team. 2024a. Qwen2 technical report. arXiv
preprint arXiv:2407.10671, 2.
Qwen Team. 2024b. Qwen2.5 technical report. arXiv
preprint arXiv:2412.15115.
Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier
Martinet, Marie-Anne Lachaux, Timoth√©e Lacroix,
Baptiste Rozi√®re, Naman Goyal, Eric Hambro, Faisal
Azhar, and 1 others. 2023. LLaMA: Open and efficient foundation language models. arXiv preprint
arXiv:2302.13971.
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob
Uszkoreit, Llion Jones, Aidan N Gomez, ≈Åukasz
Kaiser, and Illia Polosukhin. 2017. Attention is all
you need. Advances in Neural Information Processing Systems, 30.
Jie Wang, Tao Ji, Yuanbin Wu, Hang Yan, Tao Gui,
Qi Zhang, Xuanjing Huang, and Xiaoling Wang.
2024. Length generalization of causal transformers without position encoding. arXiv preprint
arXiv:2404.12224.
Jing Xiong, Jianghan Shen, Fanghua Ye, Chaofan Tao,
Zhongwei Wan, Jianqiao Lu, Xun Wu, Chuanyang
Zheng, Zhijiang Guo, Lingpeng Kong, and 1 others. 2024. Uncomp: Uncertainty-aware long-context
compressor for efficient large language model inference. arXiv preprint arXiv:2410.03090.

An Yang, Anfeng Li, Baosong Yang, Beichen Zhang,
Binyuan Hui, Bo Zheng, Bowen Yu, Chang
Gao, Chengen Huang, Chenxu Lv, and 1 others.
2025. Qwen3 technical report. arXiv preprint
arXiv:2505.09388.
An Yang, Beichen Zhang, Binyuan Hui, Bofei Gao,
Bowen Yu, Chengpeng Li, Dayiheng Liu, Jianhong Tu, Jingren Zhou, Junyang Lin, Keming Lu,
Mingfeng Xue, Runji Lin, Tianyu Liu, Xingzhang
Ren, and Zhenru Zhang. 2024. Qwen2.5-math technical report: Toward mathematical expert model via
self-improvement. arXiv preprint arXiv:2409.12122.
Chulhee Yun, Srinadh Bhojanapalli, Ankit Singh
Rawat, Sashank J Reddi, and Sanjiv Kumar.
2019. Are transformers universal approximators of
sequence-to-sequence functions? arXiv preprint
arXiv:1912.10077.
Chuanyang Zheng, Yihang Gao, Han Shi, Minbin
Huang, Jingyao Li, Jing Xiong, Xiaozhe Ren,
Michael Ng, Xin Jiang, Zhenguo Li, and 1 others.
2024. Dape: Data-adaptive positional encoding for
length extrapolation. Advances in Neural Information Processing Systems.
Lianmin Zheng, Liangsheng Yin, Zhiqiang Xie, Jeff
Huang, Chuyue Sun, Cody_Hao Yu, Shiyi Cao, Christos Kozyrakis, Ion Stoica, Joseph E Gonzalez, and
1 others. 2023. Efficiently programming large language models using sglang.
Chunsheng Zuo, Pavel Guerzhoy, and Michael
Guerzhoy. 2024. Position information emerges in
causal transformers without positional encodings via
similarity of nearby embeddings. arXiv preprint
arXiv:2501.00073.

A

Appendix

A.1

Proofs

Lemma A.1 (Entry-level lower bound (rectangular)). Let M ‚àà Rm√ón with largest singular value
œÉ1 (M). Then
œÉ1 (M)
max |(M)ij | ‚â• ‚àö
.
i,j
mn

(38)

context length of 32K tokens, while LLaMA-3-8B
is trained with a 8K-token context window. To
support longer contexts beyond their pre-training
limits, we apply RoPE-based extrapolation (e.g.,
Dynamic-NTK), which rescales RoPE frequencies
to improve stability and retrieval performance in
extended-context settings.

Hyperparameter. All experiments use greedy
decoding with temperature set to 0.0 and top-p set
Proof. By the Frobenius/spectral norm relation,
to 1.0. For the needle-in-a-haystack (NIH) task on
m X
n
X
LLaMA-3-8B-Instruct, we set max_new_tokens to
‚à•M‚à•2F =
(Mij )2 ‚â§ ( max|(Mij )|)2 mn,
i,j
50 with stop conditions including newline chari=1 j=1
acters (<0x0A>) and stop token ID 144. For
œÉ1 (M)
‚à•M‚à•F
‚â• ‚àö
,
‚áí
max|(M)ij | ‚â• ‚àö
the many-shot in-context learning (MICL) task
i,j
mn
mn
(39)
on Qwen2.5-Math-7B, we set max_new_tokens
P
since ‚à•M‚à•2F = r œÉr (M)2 ‚â• œÉ1 (M)2 .
to 2048 with stop sequences </s>, <|im_end|>,
<|endoftext|>,
and Problem: to prevent generatRemark A.2. In the square case m = n =
ing
additional
problems.
Context buffers of 200 toN , Lemma A.1 reduces to maxi,j |(M)ij |‚â•
kens (NIH) and 2,300 tokens (MICL) are reserved
œÉ1 (M)/N .
for prompt templates and final questions.
Corollary A.3 (Band contribution,
rectangular
‚àö
For RoPE extrapolation, we apply Dynamiccase). Let Ak = Q‚Ä≤k K‚Ä≤k‚ä§ / d ‚àà Rm√ón be the atNTK
scaling (emoZilla, 2023) with the scaltention score submatrix contributed by RoPE band
‚Ä≤
m√ó2
‚Ä≤
n√ó2
ing
factor
computed as Œ± = Ltarget /Loriginal ,
k, with Qk ‚àà R
and Kk ‚àà R
. Under
where Ltarget ‚àà {24K, 64K, 128K} for NIH exthe cone conditions and amplitude lower bounds
Œ±i ‚â• Œ±min > 0, Œ≤j ‚â• Œ≤min > 0, we have
periments and Ltarget = 16K for MICL experi‚àö
ments, while Loriginal corresponds to each model‚Äôs
œÉ1 (Q‚Ä≤k ) ‚â• Œ±min ‚à•q‚à• m cos Œ≥Q ,
(40)
‚àö
pre-trained maximum position embeddings (32K
œÉ1 (K‚Ä≤k ) ‚â• Œ≤min ‚à•k‚à• n cos Œ≥K ,
for Qwen-1.5-7B, 8K for LLaMA-3-8B-Instruct,
and 4K for Qwen2.5-Math-7B). For LLaMAand aligning principal directions up to angle œà
gives
3, we additionally evaluate NTK-by-parts (Peng
et al., 2023) with low_freq_factor= 1.0 and
1
œÉ1 (Ak ) ‚â≥ ‚àö œÉ1 (Q‚Ä≤k ) œÉ1 (K‚Ä≤k ) cos œà
high_freq_factor= 32.0. The NIH task uses
d
Œ±min Œ≤min ‚àö
10 uniformly spaced depth positions (0%, 10%, ...,
‚àö
‚â≥
mn ‚à•q‚à• ‚à•k‚à• cos Œ≥Q cos Œ≥K cos œà.
d
100%) for needle insertion at each context length.
(41)
The MICL task evaluates 100 sampled problems
Applying Lemma A.1 to Ak yields the entry-level
from the MATH dataset (Hendrycks et al., 2021),
bound
with needle insertion at four fixed depth positions
œÉ1 (Ak )
max |(Ak )ij | ‚â• ‚àö
(0%, 33%, 67%, 100%, corresponding to begini,j
mn
(42)
ning, 1/3, 2/3, and end) within the in-context examŒ±min Œ≤min
‚àö
‚â≥
‚à•q‚à• ‚à•k‚à• cos Œ≥Q cos Œ≥K cos œà,
ples, yielding 400 total test configurations.
d
For D O PE, Gaussian noise is sampled from
i.e. at least one score entry contributed by band
N (0, 1) with standard deviation œÉ = 1.0, usk remains ‚Ñ¶(1) under the stated assumptions (does
ing a fixed random seed (42) to ensure repronot decay with m, n).
ducibility. The truncated matrix entropy is computed by retaining the top-k singular values where
A.2 Experimental Setup
k ‚àà {1, 4, 8, 16, 32}, with k = 1 corresponding to
Models. Qwen-1.5-7B (Li, 2023), Qwen2.5using only the spectral norm œÉmax (Œ£). We also
Math-7B (Yang et al., 2024) and LLaMA-3-8Bevaluate the full (untruncated) matrix entropy for
Instruct (Grattafiori et al., 2024) are decoder-only
comparison.
transformer models that employ Rotary Positional
Embeddings (RoPE) for encoding positional information. Qwen-1.5-7B is trained with a maximum

