Number of distinct tasks evaluated: 3 (perplexity on PG19; retrieval via Long-eval; retrieval via needle in a haystack). Evidence: "Our evaluation focused on two aspects: (1) Perplexity: we use PG19 dataset ...; (2) Retrieval ... We choose a) Long-eval ... and b) needle in a haystack (NIH)" (2024_RoPEBase.txt:587-594).

Number of trained model instances required to cover all tasks: 1 (the paper frames perplexity and retrieval as a single evaluation setup on the same LLMs, with no task-specific heads or separate task-specific models described). Evidence: evaluation is jointly defined across perplexity and retrieval in one setup (2024_RoPEBase.txt:587-594).

$$
\boxed{
\frac{3\ \text{tasks}}{1\ \text{model}} = 3
}
$$
