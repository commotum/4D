## 1. Basic Metadata
- Title: "Adaptive Computation Time for Recurrent Neural Networks." (Title page)
  - Evidence: "Adaptive Computation Time for Recurrent Neural Networks" (Title page)
- Authors: "Alex Graves." (Title page)
  - Evidence: "Alex Graves" (Title page)
- Year: 2017. (Title page)
  - Evidence: "arXiv:1603.08983v6 [cs.NE] 21 Feb 2017" (Title page)
- Venue: arXiv. (Title page)
  - Evidence: "arXiv:1603.08983v6 [cs.NE]" (Title page)

## 2. One-Sentence Contribution Summary
The paper introduces Adaptive Computation Time (ACT), a deterministic and differentiable mechanism that lets recurrent neural networks learn how many computation steps to take between receiving an input and emitting an output.

## 3. Tasks Evaluated
### Parity
- Task name: Parity of binary vectors
- Task type: Classification (binary)
- Dataset(s): Synthetic; data generated online
- Domain: Synthetic binary vectors
- Evidence:
  - "We gauged the ability of ACT to infer an inherently sequential algorithm from statically presented data by presenting large binary vectors to the network and asking it to determine the parity." (Section 3.1 Parity)
  - "The input vectors had 64 elements, of which a random number from 1 to 64 were randomly set to 1 or −1 and the rest were set to 0." (Section 3.1 Parity)
  - "The data for the synthetic tasks was generated online and cross-validation was therefore not needed." (Section 3 Experiments)

### Logic
- Task name: Binary logic operations
- Task type: Classification (binary)
- Dataset(s): Synthetic; data generated online
- Domain: Synthetic logic-gate sequences
- Evidence:
  - "Each input sequence consists of a random number from 1 to 10 of size 102 input vectors." (Section 3.2 Logic)
  - "The binary target bB+1 for each input is the truth value yielded by recursively applying the B binary gates in the vector to the two initial bits b1 , b0 ." (Section 3.2 Logic)
  - "The data for the synthetic tasks was generated online and cross-validation was therefore not needed." (Section 3 Experiments)

### Addition
- Task name: Integer addition (cumulative sum)
- Task type: Classification (multi-digit)
- Dataset(s): Synthetic; data generated online
- Domain: Synthetic digit sequences
- Evidence:
  - "The addition task presents the network with a input sequence of 1 to 5 size 50 input vectors." (Section 3.3 Addition)
  - "The required output is the cumulative sum of all inputs up to the current one, represented as a set of 6 simultaneous classifications for the 6 possible digits in the sum." (Section 3.3 Addition)
  - "The data for the synthetic tasks was generated online and cross-validation was therefore not needed." (Section 3 Experiments)

### Sort
- Task name: Sorting real numbers
- Task type: Classification (index prediction)
- Dataset(s): Synthetic; data generated online
- Domain: Synthetic real-number sequences
- Evidence:
  - "The sort task requires the network to sort sequences of 2 to 15 numbers drawn from a standard normal distribution in ascending order." (Section 3.4 Sort)
  - "the random numbers were presented one at a time as inputs, and the required output was the sequence of indices into the number sequence placed in sorted order" (Section 3.4 Sort)
  - "The data for the synthetic tasks was generated online and cross-validation was therefore not needed." (Section 3 Experiments)

### Wikipedia Character Prediction
- Task name: Character prediction / language modeling
- Task type: Classification (next-byte prediction)
- Dataset(s): Hutter prize Wikipedia dataset
- Domain: Natural language text (bytes)
- Evidence:
  - "The Wikipedia task is character prediction on text drawn from the Hutter prize Wikipedia dataset [15]." (Section 3.5 Wikipedia Character Prediction)
  - "the raw unicode text was used, including XML tags and markup characters, with one byte presented per input timestep and the next byte predicted as a target." (Section 3.5 Wikipedia Character Prediction)

## 4. Domain and Modality Scope
- Domain scope: Multiple domains within the same modality (sequence data: synthetic tasks + language).
  - Evidence: "We tested recurrent neural networks (RNNs) with and without ACT on four synthetic tasks and one real-world language processing task." (Section 3 Experiments)
- Multiple modalities? Not specified in the paper.
- Domain generalization or cross-domain transfer? Not claimed.

## 5. Model Sharing Across Tasks
| Task | Shared Weights? | Fine-Tuned? | Separate Head? | Evidence |
| --- | --- | --- | --- | --- |
| Parity | Not specified in the paper. | Not specified in the paper. | Yes (task-specific output). | "The network architecture was a simple RNN with a single hidden layer containing 128 tanh units and a single sigmoidal output unit" (Section 3.1 Parity) |
| Logic | Not specified in the paper. | Not specified in the paper. | Yes (task-specific output). | "The network architecture was single-layer LSTM with 128 cells. The output was a single sigmoidal unit" (Section 3.2 Logic) |
| Addition | Not specified in the paper. | Not specified in the paper. | Yes (task-specific output). | "Each classification is modelled by a size 11 softmax" (Section 3.3 Addition) |
| Sort | Not specified in the paper. | Not specified in the paper. | Yes (task-specific output). | "The output layer was a size 15 softmax" (Section 3.4 Sort) |
| Wikipedia Character Prediction | Not specified in the paper. | Not specified in the paper. | Yes (task-specific output). | "LSTM networks were used with a single layer of 1500 cells and a size 256 softmax classification layer." (Section 3.5 Wikipedia Character Prediction) |

## 6. Input and Representation Constraints
- General sequence input: "When applied to an input sequence x = (x1 , . . . , xT ), R computes the state sequence s = (s1 , . . . , sT ) and the output sequence y = (y1 , . . . , yT )" (Section 2 Adaptive Computation Time)
- Parity input size and sequence length: "The input vectors had 64 elements" and "Each training sequence consisted of a single input and target vector" (Section 3.1 Parity)
- Logic input size, padding, and variable length: "Each input sequence consists of a random number from 1 to 10 of size 102 input vectors." and "The remaining 10 − B chunks were zeroed to indicate that no further binary operations were defined for that vector." (Section 3.2 Logic)
- Addition input size and zero padding: "The addition task presents the network with a input sequence of 1 to 5 size 50 input vectors." and "The first 10D elements of the vector are a concatenation of one-hot encodings of the D digits in the number, and the remainder of the vector is set to 0." (Section 3.3 Addition)
- Sort input representation and padding: "Each size 2 input vector consists of one real number and one binary flag to indicate the end of sequence to be sorted; inputs following the sort sequence are set to zero and marked in black." (Figure 16 caption)
- Wikipedia input representation and fixed sequence length: "one byte presented per input timestep and the next byte predicted as a target." and "Sequences of 500 consecutive bytes were randomly chosen from the training set" (Section 3.5 Wikipedia Character Prediction)
- Fixed input resolution / patch size / 2D grid assumptions: Not specified in the paper.

## 7. Context Window and Attention Structure
- Sequence length (variable across tasks):
  - Parity: "Each training sequence consisted of a single input and target vector" (Section 3.1 Parity)
  - Logic: "Each input sequence consists of a random number from 1 to 10 of size 102 input vectors." (Section 3.2 Logic)
  - Addition: "The addition task presents the network with a input sequence of 1 to 5 size 50 input vectors." (Section 3.3 Addition)
  - Sort: "The sort task requires the network to sort sequences of 2 to 15 numbers" (Section 3.4 Sort)
  - Wikipedia: "Sequences of 500 consecutive bytes were randomly chosen from the training set" (Section 3.5 Wikipedia Character Prediction)
- Fixed vs variable: Variable across synthetic tasks; fixed 500-length sequences for Wikipedia. (Evidence above)
- Attention type: Not specified in the paper (no attention mechanism described).
  - Related mention (future work): "ACT promises to be particularly interesting for recurrent architectures containing soft attention modules" (Section 4 Conclusion)
- Mechanisms for computational cost:
  - "The approach pursued here is to augment the network output with a sigmoidal halting unit whose activation determines the probability that computation should continue." (Section 1 Introduction)
  - "We therefore take the more pragmatic approach of adding a time cost to the loss function to encourage faster solutions." (Section 1 Introduction)
  - "a hard limit M on the maximum allowed value of N (t) can be imposed to avoid excessive space and time costs." (Section 2.1 Limiting Computation Time)

## 8. Positional Encoding (Critical Section)
- Positional encoding mechanism: Not specified in the paper.
- Where applied: Not specified in the paper.
- Fixed vs modified vs ablated: Not specified in the paper.

## 9. Positional Encoding as a Variable
- Core research variable? Not specified in the paper.
- Multiple positional encodings compared? Not specified in the paper.
- PE described as "not critical" or secondary? Not specified in the paper.

## 10. Evidence of Constraint Masking
- Model sizes:
  - Parity: "single hidden layer containing 128 tanh units and a single sigmoidal output unit" (Section 3.1 Parity)
  - Logic: "single-layer LSTM with 128 cells. The output was a single sigmoidal unit" (Section 3.2 Logic)
  - Addition: "The network was single-layer LSTM with 512 memory cells." (Section 3.3 Addition)
  - Sort: "The network was single-layer LSTM with 512 cells. The output layer was a size 15 softmax" (Section 3.4 Sort)
  - Wikipedia: "single layer of 1500 cells and a size 256 softmax classification layer" (Section 3.5 Wikipedia Character Prediction)
- Dataset size / training scale:
  - Synthetic data: "The data for the synthetic tasks was generated online" (Section 3 Experiments)
  - Training iterations: "The synthetic tasks were all trained for 1M iterations" and "The character prediction task was trained for 10K iterations." (Section 3 Experiments)
- Performance attribution:
  - "Overall, performance is dramatically improved by the use of ACT" (Abstract)
  - "In this case ACT does not yield large gains in performance" (Abstract)
- Scaling claims: Not specified in the paper (no explicit claims that gains come from scaling model size or data).

## 11. Architectural Workarounds
- Adaptive computation / variable depth: "Adaptive Computation Time (ACT) modifies the conventional setup by allowing R to perform a variable number of state transitions and compute a variable number of outputs at each input step." (Section 2 Adaptive Computation Time)
- Halting unit: "augment the network output with a sigmoidal halting unit whose activation determines the probability that computation should continue." (Section 1 Introduction)
- Time cost regularization: "adding a time cost to the loss function to encourage faster solutions." (Section 1 Introduction)
- Hard cap on computation: "a hard limit M on the maximum allowed value of N (t) can be imposed to avoid excessive space and time costs." (Section 2.1 Limiting Computation Time)

## 12. Explicit Limitations and Non-Claims
- Limitation (sensitivity to time penalty): "One weakness of the current algorithm is that it is quite sensitive to the time penalty parameter that controls the relative cost of computation time versus prediction error." (Section 4 Conclusion)
- Future work: "An important direction for future work will be to find ways of automatically determining and adapting the trade-off between accuracy and speed." (Section 4 Conclusion)
- Task-specific limitation: "In this case ACT does not yield large gains in performance" (Abstract)
- Non-claims about open-world / unrestrained multi-task learning / meta-learning: Not specified in the paper.
