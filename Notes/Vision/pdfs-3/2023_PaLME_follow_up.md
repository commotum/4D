Number of distinct tasks evaluated: 36 (TAMP q1-q4 and p1-p2; Language-Table Tasks 1-3; mobile manipulation affordance prediction, failure detection, and long-horizon planning; general vision-language tasks OK-VQA, VQA v2, COCO captioning; and 21 general language benchmarks). `2023_PaLME.txt:963` `2023_PaLME.txt:966` `2023_PaLME.txt:969` `2023_PaLME.txt:975` `2023_PaLME.txt:979` `2023_PaLME.txt:984` `2023_PaLME.txt:605` `2023_PaLME.txt:608` `2023_PaLME.txt:611` `2023_PaLME.txt:546` `2023_PaLME.txt:547` `2023_PaLME.txt:548` `2023_PaLME.txt:596` `2023_PaLME.txt:598` `2023_PaLME.txt:600` `2023_PaLME.txt:670` `2023_PaLME.txt:671` `2023_PaLME.txt:672`

Number of trained model instances required to cover all tasks: 1. The paper states that a single model trained on a mixture of datasets across diverse tasks and robot embodiments can simultaneously achieve high performance on all of those tasks, and the generalist vision-language evaluations use the same checkpoint across tasks. `2023_PaLME.txt:458` `2023_PaLME.txt:459` `2023_PaLME.txt:460` `2023_PaLME.txt:655` `2023_PaLME.txt:656`

$$
\boxed{
\frac{36\ \text{tasks}}{1\ \text{model}} = 36
}
$$
`2023_PaLME.txt:963` `2023_PaLME.txt:966` `2023_PaLME.txt:969` `2023_PaLME.txt:975` `2023_PaLME.txt:979` `2023_PaLME.txt:984` `2023_PaLME.txt:605` `2023_PaLME.txt:608` `2023_PaLME.txt:611` `2023_PaLME.txt:546` `2023_PaLME.txt:547` `2023_PaLME.txt:548` `2023_PaLME.txt:596` `2023_PaLME.txt:598` `2023_PaLME.txt:600` `2023_PaLME.txt:670` `2023_PaLME.txt:671` `2023_PaLME.txt:672` `2023_PaLME.txt:458` `2023_PaLME.txt:459` `2023_PaLME.txt:460` `2023_PaLME.txt:655` `2023_PaLME.txt:656`
