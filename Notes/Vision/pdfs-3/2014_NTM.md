# Neural Turing Machines (2014) - Survey Answers

## 1. Basic Metadata
- Title: Neural Turing Machines
- Authors: Alex Graves; Greg Wayne; Ivo Danihelka
- Year: 2014
- Venue: arXiv (arXiv:1410.5401v2 [cs.NE] 10 Dec 2014)

## 2. One-Sentence Contribution Summary
The paper proposes the Neural Turing Machine, which augments a neural network controller with an external memory matrix accessed by differentiable read/write operations to enable learning algorithmic tasks.
Evidence: "A Neural Turing Machine (NTM) architecture contains two basic components: a neural network controller and a memory bank." (Section 3 Neural Turing Machines)
Evidence: "Unlike a standard network, it also interacts with a memory matrix using selective read and write operations." (Section 3 Neural Turing Machines)
Evidence: "This section presents preliminary experiments on a set of simple algorithmic tasks such as copying and sorting data sequences." (Section 4 Experiments)

## 3. Tasks Evaluated

Task: Copy
- Task type: Reconstruction; Other (sequence copying)
- Dataset(s): Synthetic sequences of random binary vectors; 8-bit vectors; length 1-20
- Domain: Synthetic binary sequences
- Evidence: "The network is presented with an input sequence of random binary vectors followed by a delimiter flag." (Section 4.1 Copy)
- Evidence: "The networks were trained to copy sequences of eight bit random vectors, where the sequence lengths were randomised between 1 and 20." (Section 4.1 Copy)

Task: Repeat Copy
- Task type: Generation; Other (sequence repetition)
- Dataset(s): Random binary vectors plus scalar repeat count; sequence length and repetitions 1-10
- Domain: Synthetic binary sequences
- Evidence: "The repeat copy task extends copy by requiring the network to output the copied sequence a specified number of times and then emit an end-of-sequence marker." (Section 4.2 Repeat Copy)
- Evidence: "The network receives random-length sequences of random binary vectors, followed by a scalar value indicating the desired number of copies, which appears on a separate input channel." (Section 4.2 Repeat Copy)
- Evidence: "The networks were trained to reproduce sequences of size eight random binary vectors, where both the sequence length and the number of repetitions were chosen randomly from one to ten." (Section 4.2 Repeat Copy)

Task: Associative Recall
- Task type: Reasoning / relational; Generation
- Dataset(s): Items are sequences of binary vectors separated by delimiters; 3 vectors of 6 bits each; 2-6 items per episode
- Domain: Synthetic binary sequences
- Evidence: "we define an item as a sequence of binary vectors that is bounded on the left and right by delimiter symbols. After several items have been propagated to the network, we query by showing a random item, and we ask the network to produce the next item." (Section 4.3 Associative Recall)
- Evidence: "In our experiments, each item consisted of three six-bit binary vectors (giving a total of 18 bits per item). During training, we used a minimum of 2 items and a maximum of 6 items in a single episode." (Section 4.3 Associative Recall)

Task: Dynamic N-Grams
- Task type: Other (sequence prediction / next-bit prediction)
- Dataset(s): Binary sequences generated from random 6-Gram distributions; sequences of length 200 bits
- Domain: Synthetic binary sequences
- Evidence: "The goal of the dynamic N-Grams task was to test whether NTM could rapidly adapt to new predictive distributions." (Section 4.4 Dynamic N-Grams)
- Evidence: "We considered the set of all possible 6-Gram distributions over binary sequences." (Section 4.4 Dynamic N-Grams)
- Evidence: "We then generated a particular training sequence by drawing 200 successive bits using the current lookup table.4 The network observes the sequence one bit at a time and is then asked to predict the next bit." (Section 4.4 Dynamic N-Grams)

Task: Priority Sort
- Task type: Generation; Other (sequence sorting)
- Dataset(s): Random binary vectors with scalar priorities in [-1, 1]; input length 20; target length 16
- Domain: Synthetic binary sequences
- Evidence: "A sequence of random binary vectors is input to the network along with a scalar priority rating for each vector. The priority is drawn uniformly from the range [-1, 1]. The target sequence contains the binary vectors sorted according to their priorities, as depicted in Figure 16." (Section 4.5 Priority Sort)
- Evidence: "Each input sequence contained 20 binary vectors with corresponding priorities, and each target sequence was the 16 highest-priority vectors in the input." (Section 4.5 Priority Sort)

## 4. Domain and Modality Scope
- Single domain? Yes. All evaluations are on synthetic binary sequences (random binary vectors / bits).
  Evidence: "The network is presented with an input sequence of random binary vectors followed by a delimiter flag." (Section 4.1 Copy)
  Evidence: "We considered the set of all possible 6-Gram distributions over binary sequences." (Section 4.4 Dynamic N-Grams)
- Multiple domains within the same modality? Not specified in the paper.
- Multiple modalities? Not specified in the paper.
- Domain generalization or cross-domain transfer claims? Not claimed.

## 5. Model Sharing Across Tasks

| Task | Shared Weights? | Fine-Tuned? | Separate Head? | Evidence |
| --- | --- | --- | --- | --- |
| Copy | Not specified in the paper. | Not specified in the paper. | Not specified in the paper. | "Tables 1 to 3 give details about the network configurations and learning rates used in the experiments." (Section 4.6 Experimental Details) |
| Repeat Copy | Not specified in the paper. | Not specified in the paper. | Not specified in the paper. | "Tables 1 to 3 give details about the network configurations and learning rates used in the experiments." (Section 4.6 Experimental Details) |
| Associative Recall | Not specified in the paper. | Not specified in the paper. | Not specified in the paper. | "Tables 1 to 3 give details about the network configurations and learning rates used in the experiments." (Section 4.6 Experimental Details) |
| Dynamic N-Grams | Not specified in the paper. | Not specified in the paper. | Not specified in the paper. | "Tables 1 to 3 give details about the network configurations and learning rates used in the experiments." (Section 4.6 Experimental Details) |
| Priority Sort | Not specified in the paper. | Not specified in the paper. | Not specified in the paper. | "Tables 1 to 3 give details about the network configurations and learning rates used in the experiments." (Section 4.6 Experimental Details) |

## 6. Input and Representation Constraints
- Memory representation size is fixed by N and M: "Let Mt be the contents of the N × M memory matrix at time t, where N is the number of memory locations, and M is the vector size at each location." (Section 3.1 Reading)
- Copy task inputs are fixed-length vectors with variable sequence length: "The network is presented with an input sequence of random binary vectors followed by a delimiter flag." (Section 4.1 Copy); "The networks were trained to copy sequences of eight bit random vectors, where the sequence lengths were randomised between 1 and 20." (Section 4.1 Copy)
- Repeat Copy adds a scalar repeat count and uses fixed-length vectors; sequence length and repeats vary: "The network receives random-length sequences of random binary vectors, followed by a scalar value indicating the desired number of copies, which appears on a separate input channel." (Section 4.2 Repeat Copy); "The networks were trained to reproduce sequences of size eight random binary vectors, where both the sequence length and the number of repetitions were chosen randomly from one to ten." (Section 4.2 Repeat Copy)
- Associative Recall items are fixed-size (3 vectors of 6 bits) with variable number of items: "we define an item as a sequence of binary vectors that is bounded on the left and right by delimiter symbols." (Section 4.3 Associative Recall); "In our experiments, each item consisted of three six-bit binary vectors (giving a total of 18 bits per item). During training, we used a minimum of 2 items and a maximum of 6 items in a single episode." (Section 4.3 Associative Recall)
- Dynamic N-Grams use fixed context length and fixed sequence length: "We considered the set of all possible 6-Gram distributions over binary sequences." (Section 4.4 Dynamic N-Grams); "We then generated a particular training sequence by drawing 200 successive bits using the current lookup table.4 The network observes the sequence one bit at a time and is then asked to predict the next bit." (Section 4.4 Dynamic N-Grams)
- Priority Sort uses fixed input and output sizes: "Each input sequence contained 20 binary vectors with corresponding priorities, and each target sequence was the 16 highest-priority vectors in the input." (Section 4.5 Priority Sort)
- Fixed or variable input resolution? Not specified in the paper.
- Fixed patch size? Not specified in the paper.
- Fixed number of tokens? Not specified in the paper (beyond task-specific sequence lengths listed above).
- Fixed dimensionality (e.g., strictly 2D)? Not specified in the paper.
- Padding or resizing requirements? Not specified in the paper.

## 7. Context Window and Attention Structure
- Maximum sequence length: Not specified globally; sequence lengths are task-specific (e.g., Copy: 1-20, Dynamic N-Grams: 200 bits, Priority Sort: 20 inputs).
  Evidence: "The networks were trained to copy sequences of eight bit random vectors, where the sequence lengths were randomised between 1 and 20." (Section 4.1 Copy)
  Evidence: "We then generated a particular training sequence by drawing 200 successive bits using the current lookup table.4 The network observes the sequence one bit at a time and is then asked to predict the next bit." (Section 4.4 Dynamic N-Grams)
  Evidence: "Each input sequence contained 20 binary vectors with corresponding priorities, and each target sequence was the 16 highest-priority vectors in the input." (Section 4.5 Priority Sort)
- Sequence length fixed or variable? Variable in some tasks (Copy, Repeat Copy, Associative Recall); fixed in others (Dynamic N-Grams length 200, Priority Sort input 20).
  Evidence: "The networks were trained to copy sequences of eight bit random vectors, where the sequence lengths were randomised between 1 and 20." (Section 4.1 Copy)
  Evidence: "During training, we used a minimum of 2 items and a maximum of 6 items in a single episode." (Section 4.3 Associative Recall)
- Attention type: Global soft attention over memory locations via weighting vectors, with content-based and location-based addressing.
  Evidence: "Let wt be a vector of weightings over the N locations emitted by a read head at time t." (Section 3.1 Reading)
  Evidence: "The length M read vector rt returned by the head is defined as a convex combination of the row-vectors Mt (i) in memory:" (Section 3.1 Reading)
  Evidence: "The location-based addressing mechanism is designed to facilitate both simple iteration across the locations of the memory and random-access jumps. It does so by implementing a rotational shift of a weighting." (Section 3.3.2 Focusing by Location)
- Mechanisms to manage computational cost / memory access: Focused sparse interaction with memory.
  Evidence: "The degree of blurriness is determined by an attentional “focus” mechanism that constrains each read and write operation to interact with a small portion of the memory, while ignoring the rest. Because interaction with the memory is highly sparse, the NTM is biased towards storing data without interference." (Section 3 Neural Turing Machines)
- Context window limitation tied to memory size:
  Evidence: "The limiting factor was the size of the memory (128 locations), after which the cyclical shifts wrapped
around and previous writes were overwritten." (Section 4.1 Copy, footnote 2)

## 8. Positional Encoding (Critical Section)
- Positional encoding mechanism used: Not specified in the paper.
- Where it is applied: Not specified in the paper.
- Fixed across experiments / modified / ablated: Not specified in the paper.
- Verbatim quotes identifying PE choice: Not specified in the paper.

## 9. Positional Encoding as a Variable
- Core research variable or fixed assumption? Not specified in the paper.
- Multiple positional encodings compared? Not specified in the paper.
- PE choice claimed "not critical" or secondary? Not specified in the paper.

## 10. Evidence of Constraint Masking
- Model size(s):
  Evidence (NTM, feedforward controller):
  "Task                                  #Heads           Controller Size    Memory Size     Learning Rate    #Parameters"
  "Copy                                       1                100                128 × 20       10−4             17, 162"
  "Priority Sort                              8                512                128 × 20     3 × 10−5          508, 305"
  (Table 1: NTM with Feedforward Controller Experimental Settings)
  Evidence (LSTM baselines):
  "Task            Network Size      Learning Rate   #Parameters"
  "Copy              3 × 256           3 × 10−5       1, 352, 969"
  "Repeat Copy       3 × 512           3 × 10−5       5, 312, 007"
  (Table 3: LSTM Network Experimental Settings)
  Evidence (parameter scaling statement):
  "Note that the number of LSTM parameters grows quadratically with"
  "the number of hidden units (due to the recurrent connections in the hidden layers). This"
  "contrasts with NTM, where the number of parameters does not increase with the number of"
  "memory locations." (Section 4.6 Experimental Details)
- Dataset size(s):
  Evidence: "We then generated a particular training sequence by drawing 200 successive bits using the current lookup table.4 The network observes the sequence one bit at a time and is then asked to predict the next bit." (Section 4.4 Dynamic N-Grams)
  Evidence: "Each input sequence contained 20 binary vectors with corresponding priorities, and each target sequence was the 16 highest-priority vectors in the input." (Section 4.5 Priority Sort)
  Evidence: "During training, we used a minimum of 2 items and a maximum of 6 items in a single episode." (Section 4.3 Associative Recall)
- Performance gains attributed to scaling model size or data? Not specified in the paper.
- Performance gains attributed to architecture or training?
  Evidence: "These two results suggest that NTM’s external memory is a more effective way of maintaining the data structure than LSTM’s internal state." (Section 4.3 Associative Recall)
- Training tricks:
  Evidence: "For all experiments, the RMSProp algorithm was used for training in the form described in (Graves, 2013) with momentum of 0.9." (Section 4.6 Experimental Details)

## 11. Architectural Workarounds
- External memory with read/write heads and differentiable access:
  Evidence: "A Neural Turing Machine (NTM) architecture contains two basic components: a neural network controller and a memory bank." (Section 3 Neural Turing Machines)
  Evidence: "Unlike a standard network, it also interacts with a memory matrix using selective read and write operations." (Section 3 Neural Turing Machines)
- Sparse attention/focus on memory locations:
  Evidence: "The degree of blurriness is determined by an attentional “focus” mechanism that constrains each read and write operation to interact with a small portion of the memory, while ignoring the rest." (Section 3 Neural Turing Machines)
- Content-based and location-based addressing (random-access + iteration):
  Evidence: "These weightings arise by combining two addressing mechanisms with complementary facilities." (Section 3.3 Addressing Mechanisms)
  Evidence: "The location-based addressing mechanism is designed to facilitate both simple iteration across the locations of the memory and random-access jumps. It does so by implementing a rotational shift of a weighting." (Section 3.3.2 Focusing by Location)
- Multi-head usage to manage task complexity (Priority Sort):
  Evidence: "Note that eight parallel read and write heads were needed for best performance with a feedforward controller on this task; this may reflect the difficulty of sorting vectors using only unary vector operations (see Section 3.4)." (Section 4.5 Priority Sort)

## 12. Explicit Limitations and Non-Claims
- Memory capacity limits generalization:
  Evidence: "The limiting factor was the size of the memory (128 locations), after which the cyclical shifts wrapped
around and previous writes were overwritten." (Section 4.1 Copy, footnote 2)
- Repeat Copy counting failure beyond training range:
  Evidence: "NTM succeeds with longer sequences and is able to perform more than ten repetitions; however it is unable to keep count of of how many repeats it has completed, and does not predict the end marker correctly." (Section 4.2 Repeat Copy)
- Dynamic N-Grams does not reach optimal cost:
  Evidence: "As shown in Figure 13, NTM achieves a small, but significant performance advantage over LSTM, but never quite reaches the optimum cost." (Section 4.4 Dynamic N-Grams)
- Explicit non-claims about open-world learning, unrestrained multi-task learning, or meta-learning: Not specified in the paper.
