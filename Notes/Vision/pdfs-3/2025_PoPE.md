## 1. Basic Metadata
Title: Decoupling the “What” and “Where” with Polar Coordinate Positional Embedding
Authors: Anand Gopalakrishnan; Robert Csordás; Jürgen Schmidhuber; Michael C. Mozer
Year: 2025
Venue: arXiv preprint (arXiv:2509.10534v2 [cs.LG])
Evidence:
- "Anand Gopalakrishnan1∗ Robert Csordás2 † Jürgen Schmidhuber1,3 Michael C. Mozer4" (title page, p.1)
- "arXiv:2509.10534v2 [cs.LG] 22 Dec 2025" (title page, p.1)

## 2. One-Sentence Contribution Summary
The paper proposes PoPE, a modified relative positional encoding that disentangles content (“what”) and position (“where”) in attention to improve sequence-modeling accuracy and length extrapolation compared to RoPE.

## 3. Tasks Evaluated
### Indirect Indexing (diagnostic character indexing)
Task type: Classification (final-token accuracy); Reasoning / relational (pointer arithmetic)
Dataset(s): Indirect Indexing (procedurally generated)
Domain: Synthetic character strings
Evidence:
- "We introduce a task that requires identifying a target character within a variable-length source string. The target is defined to be at a specified relative offset (left or right) from a specified source character." (Section 4 Results, p.4)
- "Predicting the final (target) character of this sequence requires models to learn to independently manipulate the content and positional information of tokens and to apply pointer arithmetic operations." (Section 4 Results, p.4)
- "We compare RoPE (Su et al., 2024) against PoPE by training two Transformer models with cross-entropy loss applied only on the final (target) token and evaluated on the accuracy of final token." (Section 4 Results, p.4)
- "We generate source strings of length between 20 and 40 characters from the set of uppercase [A-Z] and lowercase [a-z] letters by uniform sampling without replacement." (Appendix A.1, p.13)
- "The format of each examples is: <source string>, <source character>, <shift>, <target character> and ’,’ as a delimiter and the model is given the entire sequence except the target character." (Appendix A.1, p.13)

### Sequence modeling of symbolic music (JSB, MAESTRO)
Task type: Generation (sequence modeling with cross-entropy loss)
Dataset(s): Bach-Chorales (JSB); MAESTRO
Domain: Symbolic music (MIDI sequences)
Evidence:
- "Sequence modeling of symbolic music. We train Transformer models using cross-entropy loss on MIDI-based inputs with a maximum length of 2048 from two popular music datasets, Bach-Chorales (JSB) (Boulanger-Lewandowski et al., 2012) and MAESTRO (Hawthorne et al., 2019)." (Section 4 Results, p.5)
- "Bach-Chorales. This dataset (JSB) consists of 4-part scored choral music, which are represented as a matrix with rows corresponding to voices and columns to time discretized to 16th notes." (Appendix A.1, p.13)
- "MAESTRO. The dataset contains about 200 hours of paired audio and MIDI recordings from ten years of International Piano-e-Competition." (Appendix A.1, p.13)

### Sequence modeling of human genome (HRG)
Task type: Generation (next-token prediction)
Dataset(s): Human Reference Genome (HRG)
Domain: Genomic sequences (DNA)
Evidence:
- "We train a Transformer on sequences from the Human Reference Genome dataset (Dalla-Torre et al., 2025) using the standard next-token prediction loss." (Section 4 Results, p.5)
- "The human reference genome (HRG) dataset was constructed by considering all autosomal and sex chromosomes sequences from reference assembly GRCh38/hg38 and reached a total of 3.2 billion nucleotides." (Appendix A.1, p.13)

### Language modeling on OpenWebText
Task type: Generation (language modeling)
Dataset(s): OpenWebText
Domain: Natural language text (web)
Evidence:
- "Language modeling on OpenWebText. We test PoPE’s efficacy on language modeling by training Transformers of three sizes on the OpenWebText dataset (Gokaslan & Cohen, 2019)." (Section 4 Results, p.5)
- "OpenWebText. This dataset is an open source effort to reproduce OpenAI’s WebText dataset (Gokaslan & Cohen, 2019) which was used to train GPT-2 (Radford et al., 2019)." (Appendix A.1, p.13)

### Zero-shot downstream task: LAMBADA
Task type: Classification (last-word accuracy)
Dataset(s): LAMBADA
Domain: Natural language text
Evidence:
- "We evaluate the zero-shot performance of the Transformers pretrained on OpenWebText on six downstream tasks, namely: LAMBADA Paperno et al. (2016), BLiMP (Warstadt et al., 2020), Children’s Book Test (CBT) (Hill et al., 2016), HellaSwag (Zellers et al., 2019), PIQA (Bisk et al., 2020), and ARC-E (Clark et al., 2018)." (Section 4 Results, p.6)
- "evaluate the top-one accuracy on the last word (which can be multiple tokens; we use greedy decoding)." (Section 4 Results, p.6)

### Zero-shot downstream task: BLiMP
Task type: Classification (accuracy reported)
Dataset(s): BLiMP
Domain: Natural language text
Evidence:
- "We evaluate the zero-shot performance of the Transformers pretrained on OpenWebText on six downstream tasks, namely: LAMBADA Paperno et al. (2016), BLiMP (Warstadt et al., 2020), Children’s Book Test (CBT) (Hill et al., 2016), HellaSwag (Zellers et al., 2019), PIQA (Bisk et al., 2020), and ARC-E (Clark et al., 2018)." (Section 4 Results, p.6)
- "For CBT and BLiMP, we measure the accuracy for each task and report the average accuracy over all tasks." (Section 4 Results, p.6)

### Zero-shot downstream task: Children’s Book Test (CBT)
Task type: Classification (accuracy reported)
Dataset(s): Children’s Book Test (CBT)
Domain: Natural language text
Evidence:
- "We evaluate the zero-shot performance of the Transformers pretrained on OpenWebText on six downstream tasks, namely: LAMBADA Paperno et al. (2016), BLiMP (Warstadt et al., 2020), Children’s Book Test (CBT) (Hill et al., 2016), HellaSwag (Zellers et al., 2019), PIQA (Bisk et al., 2020), and ARC-E (Clark et al., 2018)." (Section 4 Results, p.6)
- "For CBT and BLiMP, we measure the accuracy for each task and report the average accuracy over all tasks." (Section 4 Results, p.6)

### Zero-shot downstream task: HellaSwag
Task type: Not specified in the paper
Dataset(s): HellaSwag
Domain: Natural language text
Evidence:
- "We evaluate the zero-shot performance of the Transformers pretrained on OpenWebText on six downstream tasks, namely: LAMBADA Paperno et al. (2016), BLiMP (Warstadt et al., 2020), Children’s Book Test (CBT) (Hill et al., 2016), HellaSwag (Zellers et al., 2019), PIQA (Bisk et al., 2020), and ARC-E (Clark et al., 2018)." (Section 4 Results, p.6)

### Zero-shot downstream task: PIQA
Task type: Not specified in the paper
Dataset(s): PIQA
Domain: Natural language text
Evidence:
- "We evaluate the zero-shot performance of the Transformers pretrained on OpenWebText on six downstream tasks, namely: LAMBADA Paperno et al. (2016), BLiMP (Warstadt et al., 2020), Children’s Book Test (CBT) (Hill et al., 2016), HellaSwag (Zellers et al., 2019), PIQA (Bisk et al., 2020), and ARC-E (Clark et al., 2018)." (Section 4 Results, p.6)

### Zero-shot downstream task: ARC-E
Task type: Not specified in the paper
Dataset(s): ARC-E
Domain: Natural language text
Evidence:
- "We evaluate the zero-shot performance of the Transformers pretrained on OpenWebText on six downstream tasks, namely: LAMBADA Paperno et al. (2016), BLiMP (Warstadt et al., 2020), Children’s Book Test (CBT) (Hill et al., 2016), HellaSwag (Zellers et al., 2019), PIQA (Bisk et al., 2020), and ARC-E (Clark et al., 2018)." (Section 4 Results, p.6)

### Test-time length extrapolation (PG-19)
Task type: Generation (language modeling / perplexity)
Dataset(s): PG-19 test split
Domain: Natural language books
Evidence:
- "assess zero-shot perplexity on much longer sequences (up to 10240 tokens) from the test split of the PG-19 dataset (Rae et al., 2020)." (Section 4 Results, p.6)
- "PG-19. This dataset includes a set of books extracted from the Project Gutenberg books library, that were published before 1919." (Appendix A.1, p.13)

## 4. Domain and Modality Scope
- Evaluation spans multiple domains within the same modality (sequence modeling over music, genomics, and natural language). Evidence: "On autoregressive sequence modeling in music, genomic, and natural language domains, Transformers using PoPE as the positional encoding scheme outperform baselines using RoPE with respect to evaluation loss (perplexity) and downstream task performance." (Abstract, p.1)
- A synthetic character-string diagnostic task is also used. Evidence: "We introduce a task that requires identifying a target character within a variable-length source string." (Section 4 Results, p.4)
- Multiple modalities: Not specified in the paper.
- Domain generalization / cross-domain transfer: Not claimed.

## 5. Model Sharing Across Tasks
| Task | Shared Weights? | Fine-Tuned? | Separate Head? | Evidence |
| --- | --- | --- | --- | --- |
| Indirect Indexing | Not specified in the paper | Not specified in the paper | Not specified in the paper | "In all experiments, we compare our method, PoPE, to the popular RoPE (Su et al., 2024) scheme using two Transformers with identical model and training hyperparameters, the only difference being their positional encoding schemes." (Section 4 Results, p.4) |
| Symbolic music (JSB) | Not specified in the paper | Not specified in the paper | Not specified in the paper | "Bach-Chorales. This dataset (JSB) consists of 4-part scored choral music..." (Appendix A.1, p.13) |
| Symbolic music (MAESTRO) | Not specified in the paper | Not specified in the paper | Not specified in the paper | "MAESTRO. The dataset contains about 200 hours of paired audio and MIDI recordings..." (Appendix A.1, p.13) |
| Human Reference Genome (HRG) | Not specified in the paper | Not specified in the paper | Not specified in the paper | "We train a Transformer on sequences from the Human Reference Genome dataset (Dalla-Torre et al., 2025) using the standard next-token prediction loss." (Section 4 Results, p.5) |
| OpenWebText language modeling | Not specified in the paper | Not specified in the paper | Not specified in the paper | "Language modeling on OpenWebText. We test PoPE’s efficacy on language modeling by training Transformers of three sizes on the OpenWebText dataset (Gokaslan & Cohen, 2019)." (Section 4 Results, p.5) |
| LAMBADA | Yes (same OpenWebText-pretrained models) | No (zero-shot) | Not specified in the paper | "We evaluate the zero-shot performance of the Transformers pretrained on OpenWebText on six downstream tasks, namely: LAMBADA..." (Section 4 Results, p.6) |
| BLiMP | Yes (same OpenWebText-pretrained models) | No (zero-shot) | Not specified in the paper | "We evaluate the zero-shot performance of the Transformers pretrained on OpenWebText on six downstream tasks, namely:... BLiMP..." (Section 4 Results, p.6) |
| CBT | Yes (same OpenWebText-pretrained models) | No (zero-shot) | Not specified in the paper | "We evaluate the zero-shot performance of the Transformers pretrained on OpenWebText on six downstream tasks, namely:... Children’s Book Test (CBT)..." (Section 4 Results, p.6) |
| HellaSwag | Yes (same OpenWebText-pretrained models) | No (zero-shot) | Not specified in the paper | "We evaluate the zero-shot performance of the Transformers pretrained on OpenWebText on six downstream tasks, namely:... HellaSwag..." (Section 4 Results, p.6) |
| PIQA | Yes (same OpenWebText-pretrained models) | No (zero-shot) | Not specified in the paper | "We evaluate the zero-shot performance of the Transformers pretrained on OpenWebText on six downstream tasks, namely:... PIQA..." (Section 4 Results, p.6) |
| ARC-E | Yes (same OpenWebText-pretrained models) | No (zero-shot) | Not specified in the paper | "We evaluate the zero-shot performance of the Transformers pretrained on OpenWebText on six downstream tasks, namely:... ARC-E..." (Section 4 Results, p.6) |
| PG-19 length extrapolation | Yes (OpenWebText-pretrained models evaluated zero-shot) | Mixed: zero-shot for PoPE; PoPE+ft/YaRN are fine-tuned | Not specified in the paper | "We examine models pretrained on OpenWebText using a sequence length (context window) of 1024 tokens and assess zero-shot perplexity on much longer sequences (up to 10240 tokens) from the test split of the PG-19 dataset" (Section 4 Results, p.6); "Both YaRN and PoPE+ft models are finetuned on sequences of length 4096 from the OpenWebText dataset" (Section 4 Results, p.6) |

## 6. Input and Representation Constraints
- Indirect Indexing uses variable-length character strings: "We generate source strings of length between 20 and 40 characters from the set of uppercase [A-Z] and lowercase [a-z] letters by uniform sampling without replacement." (Appendix A.1, p.13)
- Indirect Indexing tokenization is character-level and specifies allowed symbols: "We use character-level tokenization, i.e. all uppercase and lowercase letters, individual digits, the delimiter symbol, plus and minus signs are separate tokens." (Appendix A.1, p.13)
- Indirect Indexing input format is fixed: "The format of each examples is: <source string>, <source character>, <shift>, <target character> and ’,’ as a delimiter..." (Appendix A.1, p.13)
- Sequence length constraints are explicitly listed per dataset: "Sequence length                    40                1024       2048      2048         1000" (Table 7, p.13)
- OpenWebText has a fixed maximum pretraining length and tokenizer: "maximum sequence length of 1024 for pretraining. We use the GPT-2 tokenizer with a vocabulary size of 50257." (Appendix A.1, p.13)
- JSB uses serialized matrix inputs with a fixed maximum length: "We use a maximum sequence length of 2048 for training..." (Appendix A.1, p.13)
- MAESTRO sequences are truncated to a maximum length: "divide it into sequences with a maximum length of 2048" (Appendix A.1, p.13)
- HRG sequences have a fixed maximum length and vocabulary size: "sequences with a maximum length of 1000 tokens and vocabulary size of 4107." (Section 4 Results, p.5)
- Fixed patch size, fixed 2D input grid, or image-specific constraints: Not specified in the paper.

## 7. Context Window and Attention Structure
- Maximum sequence lengths used in training are specified per dataset: "Sequence length                    40                1024       2048      2048         1000" (Table 7, p.13)
- Test-time length extrapolation evaluates much longer contexts: "assess zero-shot perplexity on much longer sequences (up to 10240 tokens) from the test split of the PG-19 dataset" (Section 4 Results, p.6)
- Variable-length input is explicitly used for Indirect Indexing: "variable-length source string" (Section 4 Results, p.4)
- Attention type: "we use a decoder-only Transformer architecture (Vaswani et al., 2017; Radford et al., 2018) with causal masking for autoregressive sequence modeling." (Section 4 Results, p.4)
- Mechanisms for computational cost: "We implemented PoPE using Triton, starting from the example code for Flash Attention 2 (Dao, 2024)." (Section 3 Method, p.3)
- Windowed/hierarchical/sparse attention, pooling, or token pruning mechanisms: Not specified in the paper.

## 8. Positional Encoding (Critical Section)
- Positional encoding mechanism: PoPE (relative positional encoding) compared against RoPE.
  Evidence: "We proposed a new relative positional encoding technique called PoPE whose query-key attention scores are based on a computation that decouples the match based on content and the match based on position." (Section 6 Conclusion, p.8)
  Evidence: "RoPE (Su et al., 2024) is the dominant approach to incorporate positional information... It produces an attention score for each query-key pair that is based on both how well they match and their relative positions in the input sequence." (Section 2 Background, p.2)
- Where applied: in attention score computation between queries and keys.
  Evidence: "RoPE first rotates each component c in the 2D plane by an angle proportional to the key and query positions." (Section 2 Background, p.2)
- Fixed or modified across experiments: positional encoding is the main experimental variable.
  Evidence: "the only difference being their positional encoding schemes." (Section 4 Results, p.4)
- Ablations/alternatives: "We run ablation experiments by training the 124M model with PoPE variants that do not use either the softplus activation, σ(), or the learnable bias vector δ" (Section 4 Results, p.5)
- Whether applied at input only vs every layer: The paper notes that relative positional embeddings inject positional information across layers.
  Evidence: "relative positional embeddings (Su et al., 2024) which inject this information at every layer." (Section 5 Related Work, p.8)

## 9. Positional Encoding as a Variable
- Core research variable: Yes.
  Evidence: "the only difference being their positional encoding schemes." (Section 4 Results, p.4)
- Multiple positional encodings compared: Yes (RoPE vs PoPE; PoPE ablations).
  Evidence: "we compare our method, PoPE, to the popular RoPE (Su et al., 2024) scheme" (Section 4 Results, p.4)
  Evidence: "We run ablation experiments by training the 124M model with PoPE variants that do not use either the softplus activation, σ(), or the learnable bias vector δ" (Section 4 Results, p.5)
- Claim that PE choice is not critical or secondary: Not claimed.

## 10. Evidence of Constraint Masking (Scale vs. Structure)
- Model sizes evaluated: "from 124M to 774M parameters." (Abstract, p.1)
- Dataset sizes (examples):
  - Indirect Indexing: "We generate a train/validation/test splits of size 1M/10k/10k respectively." (Appendix A.1, p.13)
  - OpenWebText: "The training and validation splits roughly contain 9B and 4M tokens respectively" (Appendix A.1, p.13)
  - PG-19: "test split of the dataset containing 100 books or roughly 7M tokens." (Appendix A.1, p.13)
- Performance gains attributed to positional encoding rather than scaling alone.
  Evidence: "Transformers using PoPE as the positional encoding scheme outperform baselines using RoPE" (Abstract, p.1)
  Evidence: "the only difference being their positional encoding schemes." (Section 4 Results, p.4)
- Claims that performance gains are primarily due to scaling model size/data or training tricks: Not explicitly stated; emphasis is on positional encoding changes.

## 11. Architectural Workarounds
- Efficient attention implementation for PoPE: "We implemented PoPE using Triton, starting from the example code for Flash Attention 2 (Dao, 2024)." (Section 3 Method, p.3)
- Base architecture choice: "we use a decoder-only Transformer architecture... with causal masking for autoregressive sequence modeling." (Section 4 Results, p.4)
- Windowed attention, hierarchical stages, token pooling/merging, or pruning: Not specified in the paper.

## 12. Explicit Limitations and Non-Claims
- Limitations or future work: Not specified in the paper.
- Explicit non-claims about scope/societal impact: "We consider our work to be fundamental research on sequence modeling with Transformers with no direct societal implications." (Section 7 Ethics Statement, p.9)
