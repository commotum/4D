### 1. Basic Metadata
- Title: "Beyond A∗: Better Planning with Transformers via Search Dynamics Bootstrapping" (Title block / first page)
- Authors: "Lucas Lehnert1 , Sainbayar Sukhbaatar1 , DiJia Su1 , Qinqing Zheng1 , Paul Mcvay1 , Michael Rabbat1 , Yuandong Tian1" (Title block / first page)
- Year: 2024. Evidence: "arXiv:2402.14083v2 [cs.AI] 26 Apr 2024" (arXiv header / first page)
- Venue: arXiv. Evidence: "arXiv:2402.14083v2 [cs.AI] 26 Apr 2024" (arXiv header / first page)

### 2. One-Sentence Contribution Summary
Contribution (one sentence): The paper trains an encoder-decoder Transformer to predict A∗ search dynamics and then fine-tunes it via search dynamics bootstrapping to solve complex planning tasks with fewer search steps than A∗.
Evidence: "We demonstrate how to train Transformers to robustly solve complex planning tasks." (Our work) "This is accomplished by training an encoder-decoder Transformer model to predict the search dynamics of the A∗ search algorithm." (Abstract / first page) "This model is obtained through search dynamics bootstrapping, a method that first trains a Transformer to imitate A∗ ’s search process and then fine-tunes the model to find an optimal plan within fewer search steps." (Our work)

### 3. Tasks Evaluated
Task: Maze navigation
- Task type: Other (planning / shortest-path)
- Dataset(s) used: Deterministic A∗ dataset; Non-deterministic A∗ dataset (synthetic token sequences)
- Domain: synthetic gridworld planning (token sequences)
- Evidence (task list): "We consider two domains: maze navigation (Figure 1(a)) and solving Sokoban puzzles (Figure 5 in Appendix C)." (Section 3 Problem Setup)
- Evidence (task definition): "In maze navigation, the goal is to find the shortest path through an n-by-n maze." (Section 3 Problem Setup)
- Evidence (datasets): "Deterministic A∗ dataset: Sequences are generated by executing A∗ in a deterministic fashion (by ordering child nodes and breaking equal cost ties deterministically)." (Section 4 Experiments) "Non-deterministic A∗ dataset: Sequences are generated by executing A∗ in a non-deterministic fashion (by randomly ordering child nodes and breaking equal cost ties randomly)." (Section 4 Experiments)
- Evidence (synthetic modality): "Our experiments use synthetically generated datasets with a synthetic language and vocabulary." (Our work)

Task: Sokoban puzzles
- Task type: Other (planning / puzzle)
- Dataset(s) used: Non-deterministic A∗ dataset (synthetic token sequences)
- Domain: synthetic gridworld planning (token sequences)
- Evidence (task list): "We consider two domains: maze navigation (Figure 1(a)) and solving Sokoban puzzles (Figure 5 in Appendix C)." (Section 3 Problem Setup)
- Evidence (task definition): "In Sokoban, a worker can move up, down, left, or right and has to push each box onto a dock to solve the puzzle." (Section 3 Problem Setup)
- Evidence (datasets): "we repeat our experiments for Sokoban puzzles using our non-deterministic A∗ implementation." (Section 4.2 Solving Sokoban puzzles)
- Evidence (synthetic modality): "Our experiments use synthetically generated datasets with a synthetic language and vocabulary." (Our work)

### 4. Domain and Modality Scope
- Single domain? No. Evidence: "We consider two domains: maze navigation (Figure 1(a)) and solving Sokoban puzzles (Figure 5 in Appendix C)." (Section 3 Problem Setup)
- Multiple domains within the same modality? Yes; both are token-sequence planning tasks. Evidence: "To train a Transformer to perform planning, we express a planning task and its optimal solution plan as a sequence of words, called tokens." (Our work)
- Multiple modalities? Not specified in the paper.
- Domain generalization or cross-domain transfer claimed? Not claimed. (No explicit statement found.)

### 5. Model Sharing Across Tasks
| Task | Shared Weights? | Fine-Tuned? | Separate Head? | Evidence |
| --- | --- | --- | --- | --- |
| Maze navigation | No — trained from scratch per task | Not specified in the paper. | Not specified in the paper. | "In the first experiment set, we train a set of encoder-decoder Transformer models to predict optimal plans for maze navigation tasks." (Section 4.1) "Because every model is trained from scratch, the resulting models are specifically trained to only predict sequences that outline optimal plans for a set of different planning tasks." (Section 3.1) |
| Sokoban puzzles | No — trained separately on Sokoban data | Yes (Searchformer fine-tuning within Sokoban) | Not specified in the paper. | "we repeat our experiments for Sokoban puzzles using our non-deterministic A∗ implementation." (Section 4.2) "Subsequently, the search-augmented model is fine-tuned on the new short sequence training dataset." (Section 3.3) |

### 6. Input and Representation Constraints
- Fixed or variable input resolution: Variable grid sizes are used (e.g., 10 × 10 mazes; 7 × 7 Sokoban). Evidence: "In maze navigation, the goal is to find the shortest path through an n-by-n maze." (Section 3 Problem Setup) "If the plan had a length of at least the mazes width or height (e.g. for 10 × 10 mazes the optimal plan needs to contain at least 10 steps), then the task was added into the dataset." (Appendix C Dataset generation) "For Sokoban, a 7 × 7 grid was sampled and two additional wall cells were added as obstacles to the interior of the map." (Appendix C Dataset generation)
- Fixed patch size: Not specified in the paper.
- Fixed number of tokens: Not fixed; variable-length sequences. Evidence: "mapping a tokenized prompt x1:n of arbitrary length n" (Appendix A.1) "different prompt-response pairs can have different prompt and response lengths." (Appendix A.2)
- Fixed dimensionality (e.g., strictly 2D): 2D grids are assumed for tasks. Evidence: "n-by-n maze" (Section 3 Problem Setup) "7 × 7 grid" (Appendix C Dataset generation)
- Any padding or resizing requirements: Not specified in the paper.
- Token/format constraints: "Solution-only sequences of the format <prompt><plan>, where the <prompt> part encodes a task description and the <plan> part the optimal plan." (Section 3.1) "Search-augmented sequences of the format <prompt><trace><plan>, where the <trace> part encodes A∗ ’s execution trace." (Section 3.1) "we express a planning task and its optimal solution plan as a sequence of words, called tokens." (Our work)
- Maximum token length constraint: "the resulting token sequences were partially very long and reached almost 100000 tokens. Due to GPU memory requirements, the Sokoban dataset was further sliced to only include sequences of with at most 10000 tokens." (Appendix C Dataset generation)

### 7. Context Window and Attention Structure
- Maximum sequence length: "the resulting token sequences were partially very long and reached almost 100000 tokens. Due to GPU memory requirements, the Sokoban dataset was further sliced to only include sequences of with at most 10000 tokens." (Appendix C Dataset generation)
- Fixed or variable sequence length: Variable. Evidence: "mapping a tokenized prompt x1:n of arbitrary length n" (Appendix A.1) "different prompt-response pairs can have different prompt and response lengths." (Appendix A.2)
- Attention type: Global causal attention (encoder-decoder Transformer). Evidence: "The encoder network fθ enc and decoder network gθ dec internally both use a number of stacked causal attention layers to form an encoder-decoder Transformer" (Appendix A.1) "the used backward-causal decoder network constructs for an n-step sequence an n × n attention map." (Section 4.1)
- Mechanisms to manage computational cost (windowing, pooling, pruning, etc.): Not specified in the paper for attention; the only explicit constraint is data slicing due to GPU memory (see max sequence length quote above).

### 8. Positional Encoding (Critical Section)
- Mechanism: RoPE (Rotary Position Embeddings). Evidence: "an adaptation of the encoder-decoder T5 architecture (Raffel et al., 2020) is used that integrates Rotary Position Embeddings (RoPE) (Su et al., 2023)." (Section 3.2)
- Where applied: Applied to Q and K in attention blocks (encoder and decoder). Evidence: "Apply RoPE to Q and K only" (Figure 4) "each tokens position is encoded by applying RoPE embeddings (Su et al., 2023) as indicated in Figure 4." (Appendix B)
- Fixed across experiments or modified per task: Fixed. Evidence: "All models used a RoPE frequency of 10000." (Appendix B)
- Ablated or compared against alternatives: Not specified in the paper.

### 9. Positional Encoding as a Variable
- Core research variable or fixed assumption? Fixed architectural assumption (RoPE is stated as part of the architecture with no alternative comparison). Evidence: "an adaptation of the encoder-decoder T5 architecture (Raffel et al., 2020) is used that integrates Rotary Position Embeddings (RoPE) (Su et al., 2023)." (Section 3.2)
- Multiple positional encodings compared? Not specified in the paper.
- Claim that PE choice is “not critical” or secondary? Not specified in the paper.

### 10. Evidence of Constraint Masking
- Model sizes: "15M model ... 46M model ... 175M model ... 747M model" (Appendix B, Table 2)
- Dataset sizes: "We vary the training dataset size and model size (the number of optimized parameters) between different training runs" (Section 4.1) "In the low training data regime (100,000 training sequences and less), performance of the solution-only model degrades significantly" (Section 4.1) "very small training datasets (50,000 training sequences)" (Section 4.1)
- Performance gains attributed to execution-trace supervision (structure in data): "including execution traces into the training data increases performance on an independent test task set—despite a 10× to 100× increase in the length of the generated sequences." (Our work)
- Scaling model size alone is not the primary driver: "Increasing the number of parameters of the solution-only models does not significantly improve their performance in the low data regime" (Section 4.1) "Even increasing the parameterization of a solution-only model to 747 million parameters only leads to a marginal performance improvement." (Section 4.2)
- Training trick / bootstrapping gains: "fine-tuning the model resulted in a significant performance improvement, reducing the rate of incorrect and non-optimal solutions by 40% and 30% respectively." (Section 4.3) "search dynamics bootstrapping, a method that first trains a Transformer to imitate A∗ ’s search process and then fine-tunes the model to find an optimal plan within fewer search steps." (Our work)

### 11. Architectural Workarounds
- Search dynamics bootstrapping to shorten traces: "search dynamics bootstrapping, a method that first trains a Transformer to imitate A∗ ’s search process and then fine-tunes the model to find an optimal plan within fewer search steps." (Our work)
- Encoder-decoder T5 with RoPE as the architectural baseline: "an adaptation of the encoder-decoder T5 architecture (Raffel et al., 2020) is used that integrates Rotary Position Embeddings (RoPE) (Su et al., 2023)." (Section 3.2)
- Fixed grid assumptions for tasks: "n-by-n maze" (Section 3 Problem Setup) "For Sokoban, a 7 × 7 grid was sampled" (Appendix C Dataset generation)
- Data-length workaround due to compute: "the resulting token sequences were partially very long and reached almost 100000 tokens. Due to GPU memory requirements, the Sokoban dataset was further sliced to only include sequences of with at most 10000 tokens." (Appendix C Dataset generation)
- Windowed attention, hierarchical stages, token pooling/merging, task-specific heads: Not specified in the paper.

### 12. Explicit Limitations and Non-Claims
- Limitation (compute/sequence length): "the trace length may grow exponentially in the length of an optimal plan (see Figure 7), and training on the resulting token sequence data can become computationally very expensive." (Section 5.1 Limitations)
- Limitation (sequence length vs LLMs): "the presented experiments use token sequences that are significantly longer than the sequences used to train LLMs such as Llama 2" (Section 5.1 Limitations)
- Future work directions: "use curriculum learning: starting from simple tasks with reasonably long execution traces, train and fine-tune the Searchformer to reduce the trace length" (Section 5.2 Future work) "explore other planning algorithms or integrate better heuristics or value functions into A∗ search" (Section 5.2 Future work) "Integrating hierarchical planning methods and temporal abstractions" (Section 5.2 Future work)
- Explicit scope statement: "Our work focuses on symbolic planning tasks and uses synthetic datasets for training." (Section 6 Broader Impact)
- Explicit non-claims about open-world learning, unrestrained multi-task learning, or meta-learning: Not specified in the paper.
