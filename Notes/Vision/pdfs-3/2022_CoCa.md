## 1. Basic Metadata
- Title: "CoCa: Contrastive Captioners are Image-Text Foundation Models" (p.1)
- Authors: Jiahui Yu; Zirui Wang; Vijay Vasudevan; Legg Yeung; Mojtaba Seyedhosseini; Yonghui Wu (p.1)
- Year: 2022 ("arXiv:2205.01917v2 [cs.CV] 14 Jun 2022") (p.1)
- Venue: arXiv ("arXiv:2205.01917v2 [cs.CV] 14 Jun 2022") (p.1)

## 2. One-Sentence Contribution Summary
- "This paper presents Contrastive Captioner (CoCa), a minimalist design to pretrain an image-text encoder-decoder foundation model jointly with contrastive loss and captioning loss, thereby subsuming model capabilities from contrastive approaches like CLIP and generative methods like SimVLM." (Abstract, p.1)

## 3. Tasks Evaluated
- Image classification / image recognition
  - Task type: Classification
  - Dataset(s): ImageNet; ImageNet-A; ImageNet-R; ImageNet-V2; ImageNet-Sketch; ObjectNet
  - Domain: Images
  - Evidence: "Our visual recognition experiments are conducted on ImageNet [9] as image recognition benchmark" (p.8)
  - Evidence: "Table 4: Zero-shot image classification results on ImageNet [9], ImageNet-A [64], ImageNet-R [65], ImageNet-V2 [66], ImageNet-Sketch [67] and ObjectNet [68]." (p.9)

- Video action recognition
  - Task type: Classification
  - Dataset(s): Kinetics-400; Kinetics-600; Kinetics-700; Moments-in-Time
  - Domain: Videos
  - Evidence: "and multiple video datasets including Kinetics-400 [57], Kinetics-600 [58], Kinetics-700 [59], Moments-in-Time [60] as test-beds for video action recognition" (p.8)

- Image-text retrieval
  - Task type: Other (crossmodal retrieval)
  - Dataset(s): MSCOCO; Flickr30K
  - Domain: Image-text pairs
  - Evidence: "We evaluate CoCa on the two standard image-text retrieval benchmarks: MSCOCO [63] and Flickr30K [62]." (p.9)

- Video-text retrieval
  - Task type: Other (crossmodal retrieval)
  - Dataset(s): MSR-VTT
  - Domain: Video-text pairs
  - Evidence: "We evaluate video-text retrieval using CoCa on MSR-VTT [71] using the full split." (p.9)

- Visual question answering (VQA v2)
  - Task type: Classification; Reasoning / relational
  - Dataset(s): VQA v2
  - Domain: Image + text
  - Evidence: "We consider three popular multimodal understaning benchmarks: visual question answering (VQA
v2 [75])" (p.10)
  - Evidence: "For VQA v2 [75], we follow prior work and formulate the task as a classification problem over 3,129 most frequent answers
in the training set." (p.18)

- Visual entailment (SNLI-VE)
  - Task type: Classification; Reasoning / relational
  - Dataset(s): SNLI-VE
  - Domain: Image + text
  - Evidence: "We consider three popular multimodal understaning benchmarks: ... visual entailment (SNLI-VE [76])" (p.10)
  - Evidence: "Similarly for SNLI-VE, the
image and the textual hypothesis are fed to encoder and decoder separately, and the classifier is
trained to predict the relation between them as entailment, neutral or contradiction." (p.18)

- Visual reasoning (NLVR2)
  - Task type: Classification; Reasoning / relational
  - Dataset(s): NLVR2
  - Domain: Image + text
  - Evidence: "We consider three popular multimodal understaning benchmarks: ... visual reasoning (NLVR2 [77])." (p.10)
  - Evidence: "For NLVR2, we
create two input pairs of each image and the text description, and concatenate them as input to the
classifier." (p.19)

- Image captioning
  - Task type: Generation
  - Dataset(s): MSCOCO (Karpathy-test split); NoCaps
  - Domain: Image-to-text
  - Evidence: "We finetune CoCa with the captioning loss
LCap only on MSCOCO [63] captioning task and evaluate on both MSCOCO Karpathy-test split
and NoCaps [78] online evaluation." (p.10)

## 4. Domain and Modality Scope
- Single domain? No. Evidence: "Our visual recognition experiments are conducted on ImageNet [9] as image recognition benchmark,
and multiple video datasets including Kinetics-400 [57], Kinetics-600 [58], Kinetics-700 [59],
Moments-in-Time [60] as test-beds for video action recognition" (p.8)
- Multiple domains within the same modality? Yes (images and videos within vision). Evidence: same as above (p.8).
- Multiple modalities? Yes. Evidence: "We evaluate CoCa on the two standard image-text retrieval benchmarks: MSCOCO [63] and Flickr30K [62]." (p.9); "We evaluate video-text retrieval using CoCa on MSR-VTT [71] using the full split." (p.9); "We consider three popular multimodal understaning benchmarks: visual question answering (VQA v2 [75]), visual entailment (SNLI-VE [76]), and visual reasoning (NLVR2 [77])." (p.10)
- Domain generalization or cross-domain transfer? Not explicitly claimed. The paper notes "our
model demonstrates effective generalization under zero-shot evaluation" (p.9) but does not label this as domain generalization or cross-domain transfer.

## 5. Model Sharing Across Tasks
The paper indicates shared pretrained weights across task categories: "CoCa sets
new state-of-the-art results on tasks of all three categories with a single pretrained checkpoint." (p.8)

| Task | Shared Weights? | Fine-Tuned? | Separate Head? | Evidence |
| --- | --- | --- | --- | --- |
| Image classification (zero-shot) | Yes | No | No (embedding match) | "are interested in the zero-shot setting where all parameters are frozen after pretraining and directly
used to extract embeddings." (p.9); "we use the aligned image/text
embeddings to perform zero-shot image classification by matching images with label names without
finetuning." (p.9) |
| Image classification (frozen-feature) | Yes (frozen encoder) | No | Yes (pooler + softmax) | "we only learn a new pooler to aggregate features." (p.6); "For frozen-feature evaluation, we add an attentional pooling layer (pooler)
on top of the output sequence of visual features and an additional softmax cross entropy loss layer" (p.18) |
| Image classification (finetuning) | Yes | Yes | Yes (pooler + classifier) | "we further finetune CoCa en-
coders on image and video datasets individually" (p.8); "finetune both encoder and pooler." (p.18) |
| Video action recognition (frozen-feature) | Yes (frozen encoder) | No | Yes (pooler + softmax) | "we learn an additional
pooler on top of the spatial and temporal feature tokens
with a softmax cross-entropy loss." (p.6) |
| Video action recognition (finetuning) | Yes | Yes | Yes (pooler + softmax) | "we further finetune CoCa en-
coders on image and video datasets individually" (p.8); "finetune both encoder and pooler." (p.18) |
| Image-text retrieval (MSCOCO/Flickr30K) | Yes | No | No (embedding similarity) | "are interested in the zero-shot setting where all parameters are frozen after pretraining and directly
used to extract embeddings." (p.9); "We evaluate CoCa on the two standard image-text retrieval benchmarks: MSCOCO [63] and Flickr30K [62]." (p.9) |
| Video-text retrieval (MSR-VTT) | Yes | No | No (embedding similarity) | "are interested in the zero-shot setting where all parameters are frozen after pretraining and directly
used to extract embeddings." (p.9); "We evaluate video-text retrieval using CoCa on MSR-VTT [71] using
the full split." (p.9) |
| VQA v2 | Yes | Yes | Yes (pooler + linear classifier) | "train linear classifiers on top of the decoder outputs" (p.10); "Encoder LR           2e-5" (Table 10, p.18) |
| SNLI-VE | Yes | Yes | Yes (pooler + linear classifier) | "the classifier is
trained to predict the relation between them" (p.18); "Encoder LR           5e-5" (Table 10, p.18) |
| NLVR2 | Yes | Yes | Yes (pooler + linear classifier) | "concatenate them as input to the
classifier." (p.19); "Encoder LR           2e-5" (Table 10, p.18) |
| Image captioning (MSCOCO/NoCaps) | Yes | Yes | Decoder used for generation (no separate classifier head mentioned) | "We finetune CoCa with the captioning loss
LCap only on MSCOCO [63] captioning task and evaluate on both MSCOCO Karpathy-test split
and NoCaps [78] online evaluation." (p.10); "finetune the model on the training split of MSCOCO to predict for MSCOCO test
split and NoCaps online evaluation." (p.19) |

## 6. Input and Representation Constraints
- Image resolution / patch size / image tokens: "Following ALIGN [13], we pretrain with image
resolution of 288×288 and patch size 18×18, resulting in a total of 256 image tokens." (p.5)
- Resolution change during pretraining: "Following
[12, 13, 14], we continue pretraining for one epoch on a higher resolution of 576×576." (p.7)
- Text input tokenization: "To tokenize text
input, we use a sentence-piece model [43, 44] with a vocabulary size of 64k trained on the sampled
pretraining dataset." (p.6)
- Text representation token: "we append a learnable [CLS] token at the end
of the input sentence" (p.5)
- Video frame sampling (for retrieval): "For zero-shot video-text retrieval, we use an even sim-
pler approach by computing the mean embedding of 16
frames of the video (frames are uniformly sampled from
a video)." (p.6)
- Video frame processing (for recognition): "We first take multiple frames
of a video and feed each frame into the shared image
encoder individually" (p.6)
- Fixed dimensionality (e.g., strictly 2D): Not specified in the paper.
- Padding/resizing requirements: Not specified in the paper.

## 7. Context Window and Attention Structure
- Maximum sequence length (images): "resulting in a total of 256 image tokens." (p.5)
- Maximum sequence length (video retrieval): "mean embedding of 16
frames of the video" (p.6)
- Maximum sequence length (text): Not specified in the paper.
- Fixed or variable sequence length: Resolution changes from 288×288 to 576×576 during pretraining (p.5, p.7), implying sequence length changes across training stages.
- Attention type: Causal self-attention and cross-attention. Evidence: "decodes texts with a causal masking transformer decoder" (p.4); "the bottom nuni
unimodal decoder layers encode the input text as latent vectors with causally-masked self-attention,
and the top nmulti multimodal layers further apply causally-masked self-attention and together with
cross-attention to the output of the visual encoder." (p.5); "All decoder layers prohibit tokens from attending
to future tokens" (p.5)
- Mechanisms to manage compute: Task-specific attentional pooling. Evidence: "a pooler is a single multi-head
attention layer with nquery learnable queries" and "We use attentional poolers in pretraining
for generative loss nquery = 256 and contrastive loss nquery = 1." (p.5); "the pooler has
a single query token thus the computation of pooling
over all spatial and temporal tokens is not expensive." (p.6)

## 8. Positional Encoding (Critical Section)
- Positional encoding mechanism: Not specified in the paper.
- Where applied: Not specified in the paper.
- Fixed/modified/ablated: Not specified in the paper.

## 9. Positional Encoding as a Variable
- Core research variable or fixed assumption: Not specified in the paper.
- Multiple positional encodings compared: Not specified in the paper.
- Claims PE choice is not critical/secondary: Not specified in the paper.

## 10. Evidence of Constraint Masking (Scale vs Structure)
- Model sizes: "CoCa-Base ... Total Params
... 383M"; "CoCa-Large ... 787M"; "CoCa ... 2.1B" (Table 1, p.5)
- Dataset scale: "We use the JFT-3B
dataset [21] with label names as the paired texts, and the ALIGN dataset [13] with noisy alt-texts." (p.6)
- Scaling discussion: "Figure 5: Image classification scaling performance of model sizes." (p.8)
- Training objectives as a performance factor: "Compared to the contrastive-only model, CoCa significantly
improves both zero-shot alignment and VQA" (p.11)
- Primary attribution of gains to scaling model size/data vs architecture/training tricks: Not explicitly specified in the paper.

## 11. Architectural Workarounds
- Decoupled decoder to get unimodal and multimodal representations: "we split the decoder into unimodal and multimodal components,
by skipping the cross-attention mechanism in the unimodal decoder layers" (p.5)
- Cross-attention only in upper decoder layers: "the top nmulti multimodal layers further apply causally-masked self-attention and together with
cross-attention to the output of the visual encoder." (p.5)
- Task-specific attentional pooling as a task adapter: "CoCa
adopts task-specific attentional pooling [42] to customize visual representations to be used for
different types of training objectives and downstream tasks." (p.5)
- Pooler design for efficiency: "the pooler has
a single query token thus the computation of pooling
over all spatial and temporal tokens is not expensive." (p.6)
- Video handling via per-frame encoding and pooling: "We first take multiple frames
of a video and feed each frame into the shared image
encoder individually" and "we learn an additional
pooler on top of the spatial and temporal feature tokens
with a softmax cross-entropy loss." (p.6)
- Linear classifier heads for multimodal tasks: "apply another attentional pooler with a single query to extract embedding from the
decoder output, and train a linear classifier on top of the pooled embedding." (p.18)

## 12. Explicit Limitations and Non-Claims
- Limitation on deployment readiness: "additional analysis
of the data and the resulting model is necessary before the use of the models in practice." (p.12)
- Robustness limitations: "our
models use the same pretraining data as previous methods [13, 21, 32, 33] and additional analysis
of the data and the resulting model is necessary before the use of the models in practice. We show
CoCa models are more robust on corrupted images, but it could still be vulnerable to other image
corruptions that are not yet captured by current evaluation sets or in real-world scenarios." (p.12)
- Broader impacts caveat: "further community exploration is required to understand the broader impacts
including but not limited to fairness, social bias and potential misuse." (p.13)
- Non-claim about optimization: "It is noteworthy that we do not use CIDEr-specific
optimization [79] for simplicity." (p.10)
- Explicit non-claims about open-world/multi-domain generality: Not specified in the paper.

## 13. Constraint Profile (Synthesis)
- Domain scope: Evaluates on images and videos (ImageNet and multiple video datasets) and vision-language benchmarks (image-text retrieval, VQA, SNLI-VE, NLVR2, captioning).
- Task structure: Multi-task across recognition, retrieval, multimodal reasoning, and caption generation, each evaluated on fixed benchmark datasets.
- Representation rigidity: Uses fixed 288×288 inputs with 18×18 patches (256 image tokens) during pretraining, later 576×576; sentence-piece tokenization with 64k vocab; video retrieval uses 16 uniformly sampled frames.
- Model sharing vs specialization: Single pretrained checkpoint used across tasks; zero-shot tasks freeze parameters, while frozen-feature and finetuned settings add task-specific poolers/heads or update encoder/decoder.
- Positional encoding: Not specified.

## 14. Final Classification
Multi-task, multi-domain (constrained). The paper evaluates across multiple task types (recognition, retrieval, multimodal understanding, captioning) and both image and video datasets, and includes vision-language inputs (e.g., "image-text retrieval" and VQA/NLVR2). At the same time, the evaluation is confined to specific benchmarks and uses a single pretrained checkpoint with zero-shot, frozen-feature, or finetuning protocols rather than open-ended multi-domain learning.
