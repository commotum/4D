1. Basic Metadata
- Title: Less is More: Recursive Reasoning with Tiny Networks.
  Evidence: "Less is More: Recursive Reasoning with Tiny Networks" (p. 1)
- Authors: Alexia Jolicoeur-Martineau.
  Evidence: "Alexia Jolicoeur-Martineau" (p. 1)
- Year: 2025.
  Evidence: "arXiv:2510.04871v1 [cs.LG] 6 Oct 2025" (p. 1)
- Venue: arXiv (cs.LG).
  Evidence: "arXiv:2510.04871v1 [cs.LG] 6 Oct 2025" (p. 1)

2. One-Sentence Contribution Summary
The paper proposes the Tiny Recursive Model (TRM), a simpler recursive reasoning approach using a single tiny network to improve generalization on hard puzzle tasks.
Evidence:
"We pro-
pose Tiny Recursive Model (TRM), a much
simpler recursive reasoning approach that
achieves significantly higher generalization
than HRM, while using a single tiny network
with only 2 layers." (p. 1)
"biologically inspired method beats Large Lan-
guage models (LLMs) on hard puzzle tasks
such as Sudoku, Maze, and ARC-AGI" (p. 1)

3. Tasks Evaluated
Overall task framing:
"hard puzzle tasks
such as Sudoku, Maze, and ARC-AGI" (p. 1)

- Task: Sudoku-Extreme
  - Task type: Reasoning / relational; Other (puzzle solving)
  - Dataset(s): Sudoku-Extreme
  - Domain: grid-based Sudoku puzzles (9x9 grid)
  - Evidence:
    "Sudoku-Extreme consists of extremely difficult Su-
    doku puzzles (Dillion, 2025; Palm et al., 2018; Park,
    2018) (9x9 grid), for which only 1K training samples
    are used to test small-sample learning. Testing is done
    on 423K samples." (p. 7)

- Task: Maze-Hard
  - Task type: Reasoning / relational; Other (pathfinding)
  - Dataset(s): Maze-Hard
  - Domain: grid-based mazes (30x30)
  - Evidence:
    "Maze-Hard consists of 30x30 mazes
    generated by the procedure by Lehnert et al. (2024)
    whose shortest path is of length above 110; both the
    training set and test set include 1000 mazes." (p. 7)

- Task: ARC-AGI-1
  - Task type: Reasoning / relational; Other (geometric puzzle solving)
  - Dataset(s): ARC-AGI-1 (with ConceptARC augmentation)
  - Domain: geometric puzzles on grids
  - Evidence:
    "ARC-AGI-1 and ARC-AGI-2 are geometric puzzles" (p. 7)
    "Each puzzle task consists of 2-3 input–output demon-
    stration pairs and 1-2 test inputs to be solved." (p. 7)
    "The maximum grid size is 30x30. ARC-AGI-1 con-
    tains 800 tasks, while ARC-AGI-2 contains 1120 tasks." (p. 7)
    "We also augment our data with the 160 tasks from
    the closely related ConceptARC dataset (Moskvichev
    et al., 2023)." (p. 7)

- Task: ARC-AGI-2
  - Task type: Reasoning / relational; Other (geometric puzzle solving)
  - Dataset(s): ARC-AGI-2 (with ConceptARC augmentation)
  - Domain: geometric puzzles on grids
  - Evidence:
    "ARC-AGI-1 and ARC-AGI-2 are geometric puzzles" (p. 7)
    "Each puzzle task consists of 2-3 input–output demon-
    stration pairs and 1-2 test inputs to be solved." (p. 7)
    "The maximum grid size is 30x30. ARC-AGI-1 con-
    tains 800 tasks, while ARC-AGI-2 contains 1120 tasks." (p. 7)
    "We also augment our data with the 160 tasks from
    the closely related ConceptARC dataset (Moskvichev
    et al., 2023)." (p. 7)

4. Domain and Modality Scope
- Evaluation spans multiple domains within the same modality (grid-based puzzle tasks: Sudoku, mazes, ARC-AGI geometric puzzles).
  Evidence:
  "Sudoku-Extreme consists of extremely difficult Su-
  doku puzzles (Dillion, 2025; Palm et al., 2018; Park,
  2018) (9x9 grid)" (p. 7)
  "Maze-Hard consists of 30x30 mazes" (p. 7)
  "ARC-AGI-1 and ARC-AGI-2 are geometric puzzles" (p. 7)
  "The maximum grid size is 30x30." (p. 7)
- Multiple modalities? Not specified in the paper.
- Domain generalization or cross-domain transfer claimed? Not claimed in the paper.

5. Model Sharing Across Tasks
| Task | Shared Weights? | Fine-Tuned? | Separate Head? | Evidence |
| --- | --- | --- | --- | --- |
| Sudoku-Extreme | Not specified in the paper. | Not specified in the paper. | Not specified in the paper. | "For Sudoku-Extreme and Maze-Hard, we train for 60k epochs with learning rate 1e-4 and weight decay 1.0." (p. 10) |
| Maze-Hard | Not specified in the paper. | Not specified in the paper. | Not specified in the paper. | "For Sudoku-Extreme and Maze-Hard, we train for 60k epochs with learning rate 1e-4 and weight decay 1.0." (p. 10) |
| ARC-AGI-1 | Not specified in the paper. | Not specified in the paper. | Not specified in the paper. | "For ARC-AGI, we train for 100K epochs with learning rate 1e-4 (with 1e-2 learning rate for the embeddings) and weight decay 0.1." (p. 10) |
| ARC-AGI-2 | Not specified in the paper. | Not specified in the paper. | Not specified in the paper. | "For ARC-AGI, we train for 100K epochs with learning rate 1e-4 (with 1e-2 learning rate for the embeddings) and weight decay 0.1." (p. 10) |

6. Input and Representation Constraints
- Input/output shape and padding: "Both input and output are
assumed to have shape [B, L] (when the shape differs,
padding tokens can be added), where B is the batch-
size and L is the context-length." (p. 2)
- Embedded representation shape: "Once the input is embedded,
the shape becomes [B, L, D] where D is the embedding
size." (p. 2)
- Fixed grid sizes for tasks:
  "Sudoku-Extreme consists of extremely difficult Su-
  doku puzzles ... (9x9 grid)" (p. 7)
  "Maze-Hard consists of 30x30 mazes" (p. 7)
  "The maximum grid size is 30x30." (p. 7)
  "This worked well on Sudoku 9x9
grids, given the small and fixed context length" (p. 6)
- Added embeddings to inputs: "Both HRM and TRM add an embedding of
shape [0, 1, D] on Sudoku-Extreme and Maze-Hard to
the input. For ARC-AGI, each puzzle ... is given a specific embedding of shape
[0, 1, D]" (p. 10)
- Fixed patch size? Not specified in the paper.
- Fixed number of tokens? Not specified in the paper (beyond context length L and task grid sizes).
- Resizing/padding requirements beyond padding tokens? Not specified in the paper.

7. Context Window and Attention Structure
- Maximum sequence length: Not specified in the paper. The only explicit max is grid size: "The maximum grid size is 30x30." (p. 7)
- Fixed or variable length: "Both input and output are
assumed to have shape [B, L] (when the shape differs,
padding tokens can be added), where B is the batch-
size and L is the context-length." (p. 2)
- Attention type:
  "Each network is a 4-layer Transformers architec-
ture" (p. 2)
  "Self-attention is particularly good for long-context
lengths when L ≫ D" (p. 6)
  "we can replace
the self-attention layer with a multilayer perceptron
(MLP) applied on the sequence length." (p. 6)
- Mechanisms to manage computational cost:
  "a halting
mechanism is incorporated to determine whether the
model should terminate early" and "They call this
method Adaptive computational time (ACT)." (p. 3)
  "By removing the continue
loss, we remove the need for the expensive second for-
ward pass" (p. 6)

8. Positional Encoding (Critical Section)
- Mechanism used: Rotary embeddings (RoPE).
  Evidence: "rotary embeddings (Su et al., 2024)" (p. 2)
- Where applied: Listed as part of the Transformer architecture.
  Evidence: "Each network is a 4-layer Transformers architec-
ture ... rotary embeddings (Su et al., 2024)" (p. 2)
- Fixed across experiments / modified / ablated: Not specified in the paper.

9. Positional Encoding as a Variable
- Core research variable? Not specified in the paper.
- Fixed architectural assumption? Not specified in the paper.
- Multiple positional encodings compared? Not specified in the paper.
- Any claim that PE choice is not critical? Not specified in the paper.

10. Evidence of Constraint Masking
- Small models and small data: "trained with small models (27M parameters)
on small data (∼ 1000 examples)." (p. 1)
- Parameter efficiency and ARC-AGI results: "With only 7M parameters,
TRM obtains 45% test-accuracy on ARC-AGI-
1 and 8% on ARC-AGI-2" (p. 1)
- Dataset sizes (small-sample regimes): "(9x9 grid), for which only 1K training samples
are used to test small-sample learning. Testing is done
on 423K samples." (p. 7) and "both the
training set and test set include 1000 mazes." (p. 7)
- Heavy data augmentation: "While these datasets are small, heavy data-augmentation is used in order to improve generalization. Sudoku-Extreme uses 1000 shuffling (done without breaking the Sudoku rules) augmentations per data example. Maze-Hard uses 8 dihedral transformations per data example. ARC-AGI uses 1000 data augmentations (color permutation, dihedral-group, and translations transformations) per data example." (p. 7)
- Gains attributed to architectural choices and overfitting control:
  "adding layers decreased gen-
eralization due to overfitting" and "using 2 layers (instead
of 4 layers) maximized generalization." (p. 6)
  "However, when data is too scarce and model size is
large, there can be an overfitting penalty" (p. 6)
  "Using an MLP
instead of self-attention, we obtain better generaliza-
tion on Sudoku-Extreme" (p. 6)
  "we integrate Exponential Moving Average (EMA)... We find that it prevents sharp collapse
and leads to higher generalization" (p. 6)

11. Architectural Workarounds
- Deep supervision to increase effective depth: "To improve effective depth, deep supervision is used.
This consists of reusing the previous latent features
(zH and zL) as initialization for the next forward pass... At most
Nsup = 16 supervision steps are used." (p. 3)
- Adaptive computational time (ACT) to early-stop and reduce compute: "a halting
mechanism is incorporated to determine whether the
model should terminate early... They call this
method Adaptive computational time (ACT)." (p. 3)
- Removing the extra forward pass for ACT: "By removing the continue
loss, we remove the need for the expensive second for-
ward pass" (p. 6)
- Attention-free option for small fixed context length: "we can replace
the self-attention layer with a multilayer perceptron
(MLP) applied on the sequence length." (p. 6)
- EMA for stability on small data: "we
integrate Exponential Moving Average (EMA) of the
weights... We find that it prevents sharp collapse
and leads to higher generalization" (p. 6)
- Task-specific input embeddings: "Both HRM and TRM add an embedding of
shape [0, 1, D] on Sudoku-Extreme and Maze-Hard to
the input. For ARC-AGI, each puzzle ... is given a specific embedding of shape
[0, 1, D]" (p. 10)

12. Explicit Limitations and Non-Claims
- Not guaranteed to be optimal everywhere: "every choice made is not guaranteed to
be optimal on every dataset." (p. 8)
- Need for scaling laws / architectural variation: "Different
problem settings may require different architectures
or number of parameters. Scaling laws are needed
to parametrize these networks optimally." (p. 8)
- Lack of theory for recursion benefits: "the
question of why recursion helps so much compared
to using a larger and deeper network remains to be
explained; we suspect it has to do with overfitting, but
we have no theory to back this explaination." (p. 8)
- Non-claim about generative modeling: "Currently, recursive reasoning models such as HRM
and TRM are supervised learning methods rather than
generative models. This means that given an input
question, they can only provide a single deterministic
answer... Thus, it would be interesting to extend TRM
to generative tasks." (p. 8)
