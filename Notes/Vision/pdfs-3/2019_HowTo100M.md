## 1. Basic Metadata
- Title: "HowTo100M: Learning a Text-Video Embedding by Watching Hundred Million Narrated Video Clips" (Title page)
- Authors: "Antoine Miech"; "Dimitri Zhukov"; "Jean-Baptiste Alayrac"; "Makarand Tapaswi"; "Ivan Laptev"; "Josef Sivic" (Title page)
- Year: "arXiv:1906.03327v2 [cs.CV] 31 Jul 2019" (Title page)
- Venue: arXiv ("arXiv:1906.03327v2 [cs.CV] 31 Jul 2019") (Title page)

## 2. One-Sentence Contribution Summary
The paper proposes learning a joint text-video embedding from automatically transcribed narrations by introducing the HowTo100M dataset and showing improved text-video retrieval and action localization performance. (Abstract: "we propose instead to learn such embeddings from video data with readily available natural language annotations in the form of automatically transcribed narrations"; "we introduce HowTo100M"; "text-video embedding trained on this data leads to state-of-the-art results for text-to-video retrieval and action localization")

## 3. Tasks Evaluated
- Task: Action step localization (instructional videos)
  - Task type: Detection / Segmentation (temporal localization)
  - Dataset(s): CrossTask
  - Domain: Instructional videos
  - Evidence: "we evaluate our learned embedding on the tasks of localizing steps in instructional videos of CrossTask [68]" (Section 5); "Action step localization. We evaluate localization of action steps in instructional videos on the recent CrossTask dataset [68]." (Section 5.2); "CrossTask includes 18 tasks and 2.7k instructional videos with manually annotated action segments." (Section 5.2)

- Task: Text-based video retrieval / video clip retrieval (text-to-video retrieval)
  - Task type: Other (text-to-video retrieval / clip retrieval)
  - Dataset(s): YouCook2, MSR-VTT, LSMDC
  - Domain: Cooking videos (YouCook2), generic YouTube videos (MSR-VTT), movie clips (LSMDC)
  - Evidence: "we demonstrate that a text-video embedding trained on this data leads to state-of-the-art results for text-to-video retrieval" (Abstract); "Text-based video retrieval. We also evaluate our learned embedding on the task of video clip retrieval using natural language queries." (Section 5.2); "we evaluate our learned embedding on the tasks of ... text-based video retrieval on YouCook2 [67], MSR-VTT [58] and LSMDC [44] datasets." (Section 5)
  - Dataset/domain evidence: "YouCook2 [67] is a cooking video dataset collected from YouTube." (Section 5.2); "MSR-VTT [58] is a dataset of generic videos collected from 257 popular video queries depicting 20 categories (including music, sports, movie, etc.) from YouTube." (Section 5.2); "LSMDC [44] is a dataset of movie clips." (Section 5.2)

## 4. Domain and Modality Scope
- Domain scope: Multiple domains within the same modality (video). Evidence: "We provide experimental results for a variety of domains ranging from instructional videos in CrossTask [68], cooking videos in YouCook2 [67], generic YouTube videos in MSR-VTT [58] to movie video clips in LSMDC [44]." (Section 5)
- Modality scope: Multiple modalities (video + text). Evidence: "Learning text-video embeddings" (Abstract); "Text-based video retrieval... using natural language queries." (Section 5.2)
- Domain generalization / cross-domain transfer claim: Claimed. Evidence: "Finally, we show that this embedding transfers well to other domains: fine-tuning on generic Youtube videos (MSR-VTT dataset) and movies (LSMDC dataset) outperforms models trained on these datasets alone." (Abstract)

## 5. Model Sharing Across Tasks
| Task | Shared Weights? | Fine-Tuned? | Separate Head? | Evidence |
| --- | --- | --- | --- | --- |
| CrossTask action step localization | Yes, uses the same pre-trained embedding weights | No (off-the-shelf) | Not specified in the paper | "We apply our model trained only on HowTo100M to the problem of step localization" (Section 5.2) |
| YouCook2 text-to-video retrieval | Pre-trained weights reused | Yes (pre-trained then fine-tuned per dataset) | Not specified in the paper | "fine-tuning our model pre-trained on HowTo100M on YouCook2 results in a significant improve-ment" (Section 5.5); Table 5 notes "PT: HowTo100M" and "FT: YouCook2" |
| MSR-VTT text-to-video retrieval | Pre-trained weights reused | Yes (pre-trained then fine-tuned per dataset) | Not specified in the paper | "We compare our model trained on (i) HowTo100M only, (ii) MSR-VTT only and (iii) pre-trained on HowTo100M and then fine-tuned on MSR-VTT" (Section 5.5) |
| LSMDC text-to-video retrieval | Pre-trained weights reused | Yes (pre-trained then fine-tuned per dataset) | Not specified in the paper | "pre-training our model on HowTo100M and fine-tuning it on LSMDC also provides improvements upon a model directly trained on LSMDC." (Section 5.5) |

## 6. Input and Representation Constraints
- Fixed/variable input resolution: Not specified in the paper.
- Fixed patch size: Not specified in the paper.
- Fixed number of tokens: Not specified in the paper.
- Fixed dimensionality: Yes. Evidence: "In practice, dv = 4, 096, dc = 4, 096 and d = 4, 096 resulting in a model composed of 67M parameters." (Section 4)
- Video feature sampling and aggregation: "2D features are extracted with the ImageNet pre-trained Resnet-152 [14] at the rate of one frame per second. 3D features are extracted with the Kinetics [4] pre-trained ResNeXt-101 16-frames model [12] to obtain 1.5 features per second. We aggregate features from longer video clips by the temporal max-pooling and concatenate 2D and 3D features to form a single 4096 dimensional vector for each video clip." (Section 5.1)
- Text representation: "The caption feature c is the output of a shallow 1D-CNN on top of pre-computed word embeddings." (Section 4); "For the word representations, we use the GoogleNews pre-trained word2vec embedding model [34]." (Section 5.1)
- Dataset-level constraints affecting inputs: "ignore videos that have less than 100 words" and "Finally, we remove videos longer than 2,000 seconds." (Section 3.1)

## 7. Context Window and Attention Structure
- Maximum sequence length: Not specified in the paper.
- Fixed or variable sequence length: Not specified in the paper; however, long clips are aggregated: "We aggregate features from longer video clips by the temporal max-pooling" (Section 5.1).
- Attention type (global/windowed/hierarchical/sparse): Not specified in the paper.
- Mechanisms to manage computational cost: "In order to not waste computation efforts, we use every sampled mini-batch pair as a negative anchor" (Appendix B); temporal max-pooling of long clips (Section 5.1).

## 8. Positional Encoding (Critical Section)
- Positional encoding mechanism: Not specified in the paper.
- Where applied: Not specified in the paper.
- Fixed/modified/ablated: Not specified in the paper.

## 9. Positional Encoding as a Variable
- Treated as core research variable vs. fixed assumption: Not specified in the paper.
- Multiple positional encodings compared: Not specified in the paper.
- Claims about PE not being critical/secondary: Not specified in the paper.

## 10. Evidence of Constraint Masking (Scale vs. Structure)
- Model size: "In practice, dv = 4, 096, dc = 4, 096 and d = 4, 096 resulting in a model composed of 67M parameters." (Section 4)
- Dataset size: "HowTo100M: a large-scale dataset of 136 million video clips sourced from 1.22M narrated instructional web videos" (Abstract)
- Evidence that gains are attributed to scaling data: "Finally, we demonstrate the importance of scale in HowTo100M to learn better joint video-text embeddings." (Section 5); "There is a clear improvement over all evaluated tasks with the gradual increase in the amount of training data. Interestingly, we do not observe any saturation" (Section 5.4)
- Primary attribution: Scaling data (not model size) is emphasized; no explicit claim that larger model size is the main driver.

## 11. Architectural Workarounds
- Temporal pooling for long clips: "We aggregate features from longer video clips by the temporal max-pooling" (Section 5.1)
- Feature concatenation: "concatenate 2D and 3D features to form a single 4096 dimensional vector for each video clip." (Section 5.1)
- Context gating in embedding: "the second term corresponds to a context gating function [31] with an output ranging between 0 and 1, which role is to modulate the output of the linear layer." (Section 4)
- Training workaround for relevance (not architectural but used to manage weak pairing): "we apply an intra-video negative sampling strategy" to ensure the embedding focuses on relevant aspects (Section 4)
- Task-specific heads or architectural branching: Not specified in the paper.

## 12. Explicit Limitations and Non-Claims
- Stated limitations / data noise: "HowTo100M does not have clean annotated captions." (Section 3.1); "our captions are not manually annotated, but automatically obtained through the narration. Thus, they can be thought of as weakly paired." (Section 3.2); "our captions are often incomplete, lack punctuation, or are grammatically incorrect sentences, as they come from continuous narration and often ASR." (Section 3.2); "narrations need not necessarily describe what is seen in the video... some captions from HowTo100M do not correlate with their corresponding video clips" (Appendix C)
- Explicit non-claims about open-world learning, unrestrained multi-task learning, or meta-learning: Not specified in the paper.
