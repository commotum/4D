## 1. Basic Metadata
- Title: End-to-End Object Detection with Transformers
  Evidence (Title page):
  > "End-to-End Object Detection with Transformers"
- Authors: Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier, Alexander Kirillov, and Sergey Zagoruyko
  Evidence (Title page):
  > "Nicolas Carion? , Francisco Massa? , Gabriel Synnaeve, Nicolas Usunier,
  > Alexander Kirillov, and Sergey Zagoruyko"
- Year: 2020
  Evidence (Title page):
  > "arXiv:2005.12872v3 [cs.CV] 28 May 2020"
- Venue: arXiv
  Evidence (Title page):
  > "arXiv:2005.12872v3 [cs.CV] 28 May 2020"

## 2. One-Sentence Contribution Summary
DETR formulates object detection as direct set prediction using a transformer encoder-decoder with bipartite matching loss to remove hand-designed components like anchors and NMS.

Evidence (Abstract):
> "We present a new method that views object detection as a direct set prediction problem."
> "Our approach streamlines the detection pipeline, effectively removing the need for many hand-designed components like a non-maximum suppression procedure or anchor generation"
> "The main ingredients of the new framework, called DEtection TRansformer or DETR, are a set-based global loss that forces unique predictions via bipartite matching, and a transformer encoder-decoder architecture."

## 3. Tasks Evaluated
### Task 1: Object detection
- Task type: Detection
- Dataset(s): COCO 2017 detection (COCO)
- Domain: Not specified in the paper.
- Evidence (Introduction; Experiments):
  > "The goal of object detection is to predict a set of bounding boxes and category
  > labels for each object of interest."
  > "We evaluate DETR on one of the most popular object detection datasets,
  > COCO [24]"
  > "We perform experiments on COCO 2017 detection and panoptic seg-
  > mentation datasets [24,18], containing 118k training images and 5k validation
  > images."
  > "Starting from the initial image ximg ∈ R3×H0 ×W0 (with 3 color
  > channels2 )"

### Task 2: Panoptic segmentation
- Task type: Segmentation (panoptic)
- Dataset(s): COCO 2017 panoptic segmentation (COCO panoptic)
- Domain: Not specified in the paper.
- Evidence (Abstract; Section 4.4; Experiments):
  > "Moreover, DETR can be easily generalized to produce panoptic segmentation
  > in a unified manner."
  > "Panoptic segmentation [19] has recently attracted a lot of attention from the
  > computer vision community."
  > "In this section we demonstrate that such a head can be used to produce panoptic
  > segmentation [19] by treating stuff and thing classes in a unified way."
  > "We perform experiments on COCO 2017 detection and panoptic seg-
  > mentation datasets [24,18], containing 118k training images and 5k validation
  > images."

## 4. Domain and Modality Scope
- Single domain or multiple domains: Single domain within a single modality (images); multiple domains are not described.
  Evidence (Section 3.2; Experiments):
  > "Starting from the initial image ximg ∈ R3×H0 ×W0 (with 3 color
  > channels2 )"
  > "We perform experiments on COCO 2017 detection and panoptic seg-
  > mentation datasets [24,18]"
- Multiple modalities: Not specified in the paper.
- Domain generalization or cross-domain transfer claimed: Not claimed.

## 5. Model Sharing Across Tasks
| Task | Shared Weights? | Fine-Tuned? | Separate Head? | Evidence |
| --- | --- | --- | --- | --- |
| Object detection | N/A (base DETR task) | Not specified | Detection FFN head | "The final prediction is com-<br>puted by a 3-layer perceptron with ReLU activation function and hidden dimen-<br>sion d, and a linear projection layer. The FFN predicts the normalized center<br>coordinates, height and width of the box w.r.t. the input image, and the lin-<br>ear layer predicts the class label using a softmax function." (Section 3.2) |
| Panoptic segmentation | Yes, DETR weights reused with added mask head | Yes (optional two-step: train DETR then freeze and train mask head) | Yes (mask head) | "DETR can be naturally extended by adding a mask head on top of the decoder outputs." (Section 4.4) <br> "The mask head can be trained either jointly, or in a two steps process, where we train DETR for boxes only, then freeze all the weights and train only the mask head for 25 epochs." (Section 4.4) |

## 6. Input and Representation Constraints
- Input modality and dimensionality: 2D RGB images (3 color channels).
  Evidence (Section 3.2):
  > "Starting from the initial image ximg ∈ R3×H0 ×W0 (with 3 color
  > channels2 )"
- Padding/size normalization within batch:
  Evidence (footnote in Section 3.2):
  > "The input images are batched together, applying 0-padding adequately to ensure
  > they all have the same dimensions (H0 , W0 ) as the largest image of the batch."
- Backbone spatial resolution / downsampling:
  Evidence (Section 3.2):
  > "a conventional CNN backbone generates a lower-resolution activation
  > map f ∈ RC×H×W . Typical values we use are C = 2048 and H, W = H0 W0
  > 32 , 32 ."
- Fixed number of decoder queries / tokens:
  Evidence (Section 3.1; Appendix A.4):
  > "DETR infers a fixed-size set of N predictions, in a single pass through the
  > decoder, where N is set to be significantly larger than the typical number of
  > objects in an image."
  > "All models were trained with N = 100 decoder query slots."
- Fixed learned object queries (output positional embeddings):
  Evidence (Abstract; Section 3.2):
  > "Given a fixed small set of learned object queries, DETR reasons about the re-
  > lations of the objects and the global image context to directly output
  > the final set of predictions in parallel."
  > "These in-
  > put embeddings are learnt positional encodings that we refer to as object queries,
  > and similarly to the encoder, we add them to the input of each attention layer."
- Resizing constraints during training:
  Evidence (Section 4):
  > "We use scale augmentation, resizing the input images such that the shortest
  > side is at least 480 and at most 800 pixels while the longest at most 1333"
- Fixed patch size / token grid: Not specified in the paper.

## 7. Context Window and Attention Structure
- Maximum sequence length: Not specified in the paper.
- Sequence length fixed or variable:
  - Encoder length depends on spatial size H×W.
    Evidence (Section 3.2):
    > "we collapse the spatial dimensions of z0 into one dimension, resulting in a d×HW
    > feature map."
  - Decoder length is fixed at N queries.
    Evidence (Section 3.2):
    > "The decoder follows the standard architecture of the
    > transformer, transforming N embeddings of size d"
- Attention type: Global self-attention over full sequences.
  Evidence (Introduction; Section 2.2):
  > "The self-attention
  > mechanisms of transformers, which explicitly model all pairwise interactions be-
  > tween elements in a sequence"
  > "Attention mechanisms [2] are neural net-
  > work layers that aggregate information from the entire input sequence."
- Mechanisms to manage computational cost:
  - CNN downsampling before transformer.
    Evidence (Section 3.2):
    > "a conventional CNN backbone generates a lower-resolution activation
    > map f ∈ RC×H×W . Typical values we use are C = 2048 and H, W = H0 W0
    > 32 , 32 ."
  - Fixed small set of object queries in decoder (limits decoder sequence length).
    Evidence (Abstract):
    > "Given a fixed small set of learned object queries"

## 8. Positional Encoding (Critical Section)
- Mechanism: Fixed absolute 2D sine/cosine positional encodings for spatial positions; learned output positional encodings (object queries).
  Evidence (Section 3.2; Appendix A.4):
  > "Since the
  > transformer architecture is permutation-invariant, we supplement it with fixed
  > positional encodings [31,3] that are added to the input of each attention layer."
  > "These in-
  > put embeddings are learnt positional encodings that we refer to as object queries,
  > and similarly to the encoder, we add them to the input of each attention layer."
  > "In our model we use a fixed absolute
  > encoding to represent these spatial positions. We adopt a generalization of the
  > original Transformer [47] encoding to the 2D case [31]. Specifically, for both
  > spatial coordinates of each embedding we independently use d2 sine and cosine
  > functions with different frequencies. We then concatenate them to get the final
  > d channel positional encoding."
- Where applied:
  - Spatial positional encodings added to each attention layer in the encoder (and decoder as configured).
  - Output positional encodings (object queries) added to each decoder attention layer.
  Evidence (Section 3.2; Table 3):
  > "we supplement it with fixed
  > positional encodings [31,3] that are added to the input of each attention layer."
  > "These in-
  > put embeddings are learnt positional encodings that we refer to as object queries,
  > and similarly to the encoder, we add them to the input of each attention layer."
  > "baseline (last row),
  > which has fixed sine pos. encodings passed at every attention layer in both the encoder
  > and the decoder. Learned embeddings are shared between all layers." (Table 3)
- Fixed across experiments vs modified: Positional encodings are compared/ablated (not fixed across all experiments).
  Evidence (Section 4.2):
  > "We experiment with various combinations of fixed and
  > learned encodings, results can be found in table 3."

## 9. Positional Encoding as a Variable
- Treated as a research variable: Yes (ablation comparisons).
  Evidence (Section 4.2):
  > "We experiment with various combinations of fixed and
  > learned encodings, results can be found in table 3."
- Multiple positional encodings compared: Yes (Table 3 compares none vs sine vs learned, input vs attention).
  Evidence (Table 3 caption):
  > "Results for different positional encodings compared to the baseline (last row),
  > which has fixed sine pos. encodings passed at every attention layer in both the encoder
  > and the decoder."
- PE “not critical” claim: Not specified in the paper. Instead, the paper states they are important.
  Evidence (Section 4.2):
  > "Given these ablations, we conclude that transformer components: the global
  > self-attention in encoder, FFN, multiple decoder layers, and positional encodings,
  > all significantly contribute to the final object detection performance."

## 10. Evidence of Constraint Masking (Scale vs Structure)
- Model size(s):
  Evidence (Section 4.1; Table 1):
  > "model with 6 transformer and 6 decoder layers of width 256 with 8 attention
  > heads. Like Faster R-CNN with FPN this model has 41.3M parameters, out of
  > which 23.5M are in ResNet-50, and 17.8M are in the transformer."
  > "DETR                          86/28         41M    42.0 62.4 44.2   20.5 45.8 61.1"
  > "DETR-R101                     152/20        60M    43.5 63.8 46.4   21.9 48.0 61.8"
- Dataset size(s):
  Evidence (Section 4):
  > "We perform experiments on COCO 2017 detection and panoptic seg-
  > mentation datasets [24,18], containing 118k training images and 5k validation
  > images."
- Performance gains attributed to:
  - Architectural global attention for large objects:
    Evidence (Introduction):
    > "DETR demonstrates
    > significantly better performance on large objects, a result likely enabled by the
    > non-local computations of the transformer."
  - Training schedule (long training) and augmentation (random crops):
    Evidence (Section 4):
    > "This schedule adds 1.5 AP compared
    > to the shorter schedule."
    > "we also apply random crop augmentations during training, improving the per-
    > formance by approximately 1 AP."
  - Higher-resolution backbone variant (DETR-DC5) with compute tradeoff:
    Evidence (Section 4):
    > "This modification
    > increases the resolution by a factor of two, thus improving performance for small
    > objects, at the cost of a 16x higher cost in the self-attentions of the encoder,
    > leading to an overall 2x increase in computational cost."
- Scaling data size as a primary driver: Not specified in the paper.
- Scaling model size as a primary driver: Not specified in the paper.

## 11. Architectural Workarounds
- Bipartite matching set loss to avoid duplicate predictions (replaces NMS/anchors):
  Evidence (Abstract; Section 3.1):
  > "The main
  > ingredients of the new framework, called DEtection TRansformer or
  > DETR, are a set-based global loss that forces unique predictions via bi-
  > partite matching"
  > "Our loss produces
  > an optimal bipartite matching between predicted and ground truth objects"
- Removal of hand-designed components (anchors, NMS):
  Evidence (Abstract; Introduction):
  > "Our approach streamlines the detection
  > pipeline, effectively removing the need for many hand-designed compo-
  > nents like a non-maximum suppression procedure or anchor generation"
  > "DETR simplifies the
  > detection pipeline by dropping multiple hand-designed components that encode
  > prior knowledge, like spatial anchors or non-maximal suppression."
- CNN backbone with downsampled feature map before transformer:
  Evidence (Section 3.2):
  > "a conventional CNN backbone generates a lower-resolution activation
  > map f ∈ RC×H×W . Typical values we use are C = 2048 and H, W = H0 W0
  > 32 , 32 ."
- Fixed number of object queries (slots):
  Evidence (Section 3.1; Appendix A.4):
  > "DETR infers a fixed-size set of N predictions, in a single pass through the
  > decoder"
  > "All models were trained with N = 100 decoder query slots."
- Auxiliary decoding losses after each decoder layer:
  Evidence (Section 3.2):
  > "We found helpful to use auxiliary losses [1] in
  > decoder during training, especially to help the model output the correct number
  > of objects of each class. We add prediction FFNs and Hungarian loss after each
  > decoder layer."
- Panoptic segmentation head with FPN-like upsampling:
  Evidence (Section 4.4):
  > "DETR can be naturally extended by adding a mask head on
  > top of the decoder outputs."
  > "To make the final prediction and increase the
  > resolution, an FPN-like architecture is used."

## 12. Explicit Limitations and Non-Claims
- Lower performance on small objects:
  Evidence (Introduction):
  > "It obtains, however, lower perfor-
  > mances on small objects."
- Training/optimization challenges and small objects:
  Evidence (Conclusion):
  > "This new design for detectors also comes with new challenges, in particular
  > regarding training, optimization and performances on small objects."
- Fixed upper bound on number of detectable objects (query slots):
  Evidence (Appendix A.5):
  > "By design, DETR cannot predict more
  > objects than it has query slots, i.e. 100 in our experiments."
- Explicit non-claims about open-world learning, unrestrained multi-task learning, or meta-learning: Not specified in the paper.
