                                           VideoBERT: A Joint Model for Video and Language Representation Learning


                                                        Chen Sun, Austin Myers, Carl Vondrick, Kevin Murphy, and Cordelia Schmid

                                                                                                    Google Research
arXiv:1904.01766v2 [cs.CV] 11 Sep 2019




                                                                        Season the steak with     Carefully place the steak   Flip the steak to the   Now let it rest and enjoy
                                                           input text
                                                                        salt and pepper.          to the pan.                 other side.             the delicious steak.




                                             VideoBERT
                                                        output video


                                                                                        output
                                                                  input
                                                                                         video
                                                                  video
                                                                                        futures



                                                                          VideoBERT

                                         Figure 1: VideoBERT text-to-video generation and future forecasting. (Above) Given some recipe text divided into
                                         sentences, y = y1:T , we generate a sequence of video tokens x = x1:T by computing x∗t = arg maxk p(xt = k|y) using
                                         VideoBERT. (Below) Given a video token, we show the top three future tokens forecasted by VideoBERT at different time
                                         scales. In this case, VideoBERT predicts that a bowl of flour and cocoa powder may be baked in an oven, and may become a
                                         brownie or cupcake. We visualize video tokens using the images from the training set closest to centroids in feature space.

                                                                   Abstract                                        1. Introduction
                                             Self-supervised learning has become increasingly impor-                  Deep learning can benefit a lot from labeled data [24],
                                         tant to leverage the abundance of unlabeled data avail-                   but this is hard to acquire at scale. Consequently there has
                                         able on platforms like YouTube. Whereas most existing                     been a lot of recent interest in “self supervised learning”,
                                         approaches learn low-level representations, we propose a                  where we train a model on various “proxy tasks”, which we
                                         joint visual-linguistic model to learn high-level features                hope will result in the discovery of features or representa-
                                         without any explicit supervision. In particular, inspired                 tions that can be used in downstream tasks. A wide variety
                                         by its recent success in language modeling, we build upon                 of such proxy tasks have been proposed in the image and
                                         the BERT model to learn bidirectional joint distributions                 video domains. However, most of these methods focus on
                                         over sequences of visual and linguistic tokens, derived from              low level features (e.g., textures) and short temporal scales
                                         vector quantization of video data and off-the-shelf speech                (e.g., motion patterns that last a second or less). We are in-
                                         recognition outputs, respectively. We use VideoBERT in nu-                terested in discovering high-level semantic features which
                                         merous tasks, including action classification and video cap-              correspond to actions and events that unfold over longer
                                         tioning. We show that it can be applied directly to open-                 time scales (e.g. minutes), since such representations would
                                         vocabulary classification, and confirm that large amounts                 be useful for various video understanding tasks.
                                         of training data and cross-modal information are critical to                 In this paper, we exploit the key insight that human
                                         performance. Furthermore, we outperform the state-of-the-                 language has evolved words to describe high-level objects
                                         art on video captioning, and quantitative results verify that             and events, and thus provides a natural source of “self”
                                         the model learns high-level semantic features.                            supervision. In particular, we present a simple way to
                                                                                                                   model the relationship between the visual domain and the


                                                                                                               1
                                 Cut the cabbage into      Put cabbage in the wok   Add soy sauce and ...    Put on a plate the dish is
                    input text
                                 pieces.                   and stir fry.            then keep stir frying.   now ready to be served.




     VideoBERT
                 output video


                                                 output
                           input
                                                  video
                           video
                                                 futures



                                   VideoBERT

 Figure 2: Additional text-to-video generation and future forecasting examples from VideoBERT, see Figure 1 for details.
linguistic domain by combining three off-the-shelf meth-                   Section 4 presents results on activity recognition and video
ods: an automatic speech recognition (ASR) system to con-                  captioning tasks; and Section 5 concludes.
vert speech into text; vector quantization (VQ) applied to
low-level spatio-temporal visual features derived from pre-                2. Related Work
trained video classfication models; and the recently pro-
posed BERT model [6] for learning joint distributions over                     Supervised learning. Some of the most successful ap-
sequences of discrete tokens.                                              proaches for video representation learning have leveraged
    More precisely, our approach is to apply BERT to learn a               large labeled datasets (e.g., [9, 19, 36, 7]) to train convolu-
model of the form p(x, y), where x is a sequence of “visual                tional neural networks for video classification. However, it
words”, and y is a sequence of spoken words. Given such                    is very expensive to collect such labeled data, and the cor-
a joint model, we can easily tackle a variety of interesting               responding label vocabularies are often small and not ca-
tasks. For example, we can perform text-to-video predic-                   pable of representing the nuances of many kinds of actions
tion, which can be used to automatically illustrate a set of               (e.g., “sipping” is slightly different than “drinking” which
instructions (such as a recipe), as shown in the top examples              is slightly different than “gulping”). In addition, these ap-
of Figure 1 and 2. We can also perform the more traditional                proaches are designed for representing short video clips,
video-to-text task of dense video captioning [10] as shown                 typically a few seconds long. The main difference to our
in Figure 6. In Section 4.6, we show that our approach                     work is that we focus on the long-term evolution of events
to video captioning significantly outperforms the previous                 in video, and we do not use manually provided labels.
state-of-the-art [39] on the YouCook II dataset [38].                          Unsupervised learning. Recently, a variety of ap-
    We can also use our model in a “unimodal” fashion. For                 proaches for learning density models from video have been
example, the implied marginal distribution p(x) is a lan-                  proposed. Some use a single static stochastic variable,
guage model for visual words, which we can use for long-                   which is then “decoded” into a sequence using an RNN,
range forecasting. This is illustrated in the bottom examples              either using a VAE-style loss [32, 35] or a GAN-style loss
of Figure 1 and 2. Of course, there is uncertainty about the               [31, 17]. More recent work uses temporal stochastic vari-
future, but the model can generate plausible guesses at a                  ables, e.g., the SV2P model of [4] and the SVGLP model
much higher level of abstraction than other deep generative                of [5]. There are also various GAN-based approaches, such
models for video, such as those based on VAEs or GANs                      as the SAVP approach of [13] and the MoCoGAN approach
(see e.g., [4, 5, 13, 27]), which tend to predict small changes            of [27]. We differ from this work in that we use the BERT
to low level aspects of the scene, such as the location or pose            model, without any explicit stochastic latent variables, ap-
of a small number of objects.                                              plied to visual tokens derived from the video. Thus our
    In summary, our main contribution in this paper is a                   model is not a generative model of pixels, but it is a gen-
simple way to learn high level video representations that                  erative model of features derived from pixels, which is an
capture semantically meaningful and temporally long-range                  approach that has been used in other work (e.g., [30]).
structure. The remainder of this paper describes this con-                     Self-supervised learning. To avoid the difficulties of
tribution in detail. In particular, Section 2 briefly reviews              learning a joint model p(x1:T ), it has become popular to
related work; Section 3 describes how we adapt the recent                  learn conditional models of the form p(xt+1:T |x1:t ), where
progress in natural language modeling to the video domain;                 we partition the signal into two or more blocks, such as gray
scale and color, or previous frame and next frame (e.g.,           where φl (x) is the l’th potential function, with parameters
[18]), and try to predict one from the other (see e.g., [23]       θ, and Z is the partition function.
for an overview). Our approach is similar, except we use              The above model is permutation invariant. In order to
quantized visual words instead of pixels. Furthermore, al-         capture order information, we can “tag” each word with its
though we learn a set conditional distributions, our model is      position in the sentence. The BERT model learns an embed-
a proper joint generative model, as explained in Section 3.        ding for each of the word tokens, as well as for these tags,
    Cross-modal learning. The multi-modal nature of video          and then sums the embedding vectors to get a continuous
has also been an extensive source of supervision for learn-        representation for each token. The log potential (energy)
ing video representations, which our paper builds on. Since        functions for each location are defined by
most videos contain synchronized audio and visual signals,
                                                                                     log φl (x|θ) = xTl fθ (x\l )
the two modalities can supervise each other to learn strong
self-supervised video representations [3, 20, 21]. In this         where xl is a one-hot vector for the l’th token (and its tag),
work, we use speech (provided by ASR) rather than low-             and
level sounds as a source of cross-modal supervision.
                                                                          x\l = (x1 , . . . , xl−1 , MASK, xl+1 , . . . , xL )
    Natural language models. We build upon recent
progress in the NLP community, where large-scale lan-              The function f (x\l ) is a multi-layer bidirectional trans-
guage models such as ELMO [22] and BERT [6] have                   former model [28] that takes an L × D1 tensor, contain-
shown state-of-the-art results for various NLP tasks, both at      ing the D1 -dimensional embedding vectors corresponding
the word level (e.g., POS tagging) and sentence level (e.g.,       to x\l , and returns an L × D2 tensor, where D2 is the size
semantic classification). The BERT model is then extended          of the output of each transformer node. See [6] for details.
to pre-train on multi-lingual data [12]. Our paper builds on       The model is trained to approximately maximize the pseudo
the BERT model to capture structure in both the linguistic         log-likelihood
and visual domains.                                                                              L
                                                                                                 X
    Image and video captioning. There has been much re-                         L(θ) = Ex∼D            log p(xl |x\l ; θ)
cent work on image captioning (see e.g., [11, 8, 15]), which                                     l=1
is a model of the form p(y|x), where y is the manually pro-
                                                                   In practice, we can stochastically optimize the logloss
vided caption and x is the image. There has also been some
                                                                   (computed from the softmax predicted by the f function)
work on video captioning, using either manually provided
                                                                   by sampling locations as well as training sentences.
temporal segmentation or estimated segmentations (see e.g.,
                                                                      BERT can be extended to model two sentences by con-
[10, 39]). We use our joint p(x, y) model and apply it to
                                                                   catenating them together. However, we are often not only
video captioning, and achieve state-of-the-art results, as we
                                                                   interested in simply modeling the extended sequence, but
discuss in Section 4.6.
                                                                   rather relationships between the two sentences (e.g., is this a
    Instructional videos. Various papers (e.g., [16, 2, 10,        pair of consecutive or randomly selected sentences). BERT
38, 39]) have trained models to analyse instructional videos,      accomplishes this by prepending every sequence with a spe-
such as cooking. We differ from this work in that we do not        cial classification token, [CLS], and by joining sentences
use any manual labeling, and we learn a large-scale genera-        with a special separator token, [SEP]. The final hidden state
tive model of both words and (discretized) visual signals.         corresponding to the [CLS] token is used as the aggregate
                                                                   sequence representation from which we predict a label for
3. Models                                                          classification tasks, or which may otherwise be ignored. In
   In this section, we briefly summarize the BERT model,           addition to differentiating sentences with the [SEP] token,
and then describe how we extend it to jointly model video          BERT also optionally tags each token by the sentence it
and language data.                                                 comes from. The corresponding joint model can be written
                                                                   as p(x, y, c), where x is the first sentence, y is the second,
3.1. The BERT model                                                and c = {0, 1} is a label indicating whether the sentences
                                                                   were separate or consecutive in the source document.
   BERT [6] proposes to learn language representations by             For consistency with the original paper, we also add a
using a “masked language model” training objective. In             [SEP] token to the end of the sequence, even though it
more detail, let x = {x1 , . . . , xL } be a set of discrete to-   is not strictly needed. So, a typical masked-out training
kens, xl ∈ X . We can define a joint probability distribution      sentence pair may look like this: [CLS] let’s make
over this set as follows:                                          a traditional [MASK] cuisine [SEP] orange
                      L                     L
                                                            !      chicken with [MASK] sauce [SEP].  The corre-
                1 Y                        X
                                                                   sponding class label in this case would be c = 1, indicating
   p(x|θ) =             φl (x|θ) ∝ exp         log φl (x|θ)
              Z(θ)                                                 that x and y are consecutive.
                    l=1                   l=1
   [CLS]    Place     the     steak      in      the      pan     [>]                                                          [SEP]



    T1       T2       T3        T4       T5       T6       T7     T8       T9        T10       T11        T12        T13        T14


                                                  VideoBERT
   E[CLS]   EPlace    Ethe    E[MASK]    Ein     Ethe     Epan    E[>]   Ev(    )   E[MASK]   Ev(    )   Ev(    )   Ev(    )   E[SEP]


   [CLS]    Place     the    [MASK]      in      the      pan     [>]               [MASK]                                     [SEP]


Figure 3: Illustration of VideoBERT in the context of a video and text masked token prediction, or cloze, task. This task also
allows for training with text-only and video-only data, and VideoBERT can furthermore be trained using a linguistic-visual
alignment classification objective (not shown here, see text for details).


3.2. The VideoBERT model                                         tween different videos, we randomly pick a subsampling
                                                                 rate of 1 to 5 steps for the video tokens. This not only helps
    To extend BERT to video, in such a way that we may           the model be more robust to variations in video speeds, but
still leverage pretrained language models and scalable im-       also allows the model to capture temporal dynamics over
plementations for inference and learning, we decided to          greater time horizons and learn longer-term state transi-
make minimal changes, and transform the raw visual data          tions. We leave investigation into other ways of combining
into a discrete sequence of tokens. To this end, we propose      video and text to future work.
to generate a sequence of “visual words” by applying hi-
                                                                     Overall, we have three training regimes corresponding
erarchical vector quantization to features derived from the
                                                                 to the different input data modalities: text-only, video-only
video using a pretrained model. See Section 4.2 for details.
                                                                 and video-text. For text-only and video-only, the standard
Besides its simplicity, this approach encourages the model
                                                                 mask-completion objectives are used for training the model.
to focus on high level semantics and longer-range temporal
                                                                 For text-video, we use the linguistic-visual alignment clas-
dynamics in the video. This is in contrast to most existing
                                                                 sification objective described above. The overall training
self-supervised approaches to video representation learning,
                                                                 objective is a weighted sum of the individual objectives.
which learn low-level properties such as local textures and
                                                                 The text objective forces VideoBERT to do well at language
motions, as discussed in Section 2.
                                                                 modeling; the video objective forces it to learn a “language
    We can combine the linguistic sentence (derived from the     model for video”, which can be used for learning dynam-
video using ASR) with the visual sentence to generate data       ics and forecasting; and the text-video objective forces it to
such as this: [CLS] orange chicken with [MASK]                   learn a correspondence between the two domains.
sauce [>] v01 [MASK] v08 v72 [SEP], where v01                        Once we have trained the model, we can use it in a va-
and v08 are visual tokens, and [>] is a special token we in-     riety of downstream tasks, and in this work we quantita-
troduce to combine text and video sentences. See Figure 3        tively evaluate two applications. In the first application, we
for an illustration.                                             treat it as a probabilistic model, and ask it to predict or im-
    While this cloze task extends naturally to sequences of      pute the symbols that have been MASKed out. We illustrate
linguistic and visual tokens, applying a next sentence pre-      this in Section 4.4, where we perform “zero-shot” classifi-
diction task, as used by BERT, is less straightforward. We       cation. In the second application, we extract the predicted
propose a linguistic-visual alignment task, where we use the     representation (derived from the internal activations of the
final hidden state of the [CLS] token to predict whether the     model) for the [CLS] token, and use that dense vector as
linguistic sentence is temporally aligned with the visual sen-   a representation of the entire input. This can be combined
tence. Note that this is a noisy indicator of semantic relat-    with other features derived from the input to be used in a
edness, since even in instructional videos, the speaker may      downstream supervised learning task. We demonstrate this
be referring to something that is not visually present.          in Section 4.6, where we perform video captioning.
    To combat this, we first randomly concatenate neighbor-
ing sentences into a single long sentence, to allow the model    4. Experiments and Analysis
to learn semantic correspondence even if the two are not
well aligned temporally. Second, since the pace of state            In this section we describe our experimental setup, and
transitions for even the same action can vary greatly be-        show quantitative and qualitative results.
4.1. Dataset                                                     convolutions to an Inception network [25] backbone. We
                                                                 take the feature activations before the final linear classifier
    Deep learning models, in both language and vision do-
                                                                 and apply 3D average pooling to obtain a 1024-dimension
mains, have consistently demonstrated dramatic gains in
                                                                 feature vector. We pretrain the S3D network on the Kinet-
performance with increasingly large datasets. For example,
                                                                 ics [9] dataset, which covers a wide spectrum of actions
the “large” BERT model (which we use) was pretrained on
                                                                 from YouTube videos, and serves as a generic representa-
the concatenation of the BooksCorpus (800M words) and
                                                                 tion for each individual clip.
English Wikipedia (2,500M words).
                                                                    We tokenize the visual features using hierarchical k-
    Therefore, we would like to train VideoBERT with a
                                                                 means. We adjust the number of hierarchy levels d and the
comparably large-scale video dataset. Since we are inter-
                                                                 number of clusters per level k by visually inspecting the co-
ested in the connection between language and vision, we
                                                                 herence and representativeness of the clusters. We set d = 4
would like to find videos where the spoken words are more
                                                                 and k = 12, which yields 124 = 20736 clusters in total.
likely to refer to visual content. Intuitively, this is often
                                                                 Figure 4 illustrates the result of this “vector quantization”
the case for instructional videos, and we focus on cooking
                                                                 process.
videos specifically, since it is a well studied domain with
                                                                    For each ASR word sequence, we break the stream
existing annotated datasets available for evaluation. Unfor-
                                                                 of words into sentences by adding punctuation using an
tunately, such datasets are relatively small, so we turn to
                                                                 off-the-shelf LSTM-based language model. For each sen-
YouTube to collect a large-scale video dataset for training.
                                                                 tence, we follow the standard text preprocessing steps from
    We extract a set of publicly available cooking videos
                                                                 BERT [6] and tokenize the text into WordPieces [33]. We
from YouTube using the YouTube video annotation sys-
                                                                 use the same vocabulary provided by the authors of BERT,
tem to retrieve videos with topics related to “cooking” and
                                                                 which contains 30,000 tokens.
“recipe”. We also filter videos by their duration, removing
                                                                    Unlike language which can be naturally broken into sen-
videos longer than 15 minutes, resulting in a set of 312K
                                                                 tences, it is unclear how to break videos into semantically
videos. The total duration of this dataset is 23,186 hours, or
                                                                 coherent segments. We use a simple heuristic to address
roughly 966 days. For reference, this is more than two or-
                                                                 this problem: when an ASR sentence is available, it is as-
ders of magnitude larger than the next largest cooking video
                                                                 sociated with starting and ending timestamps, and we treat
dataset, YouCook II, which consists of 2K videos with a to-
                                                                 video tokens that fall into that time period as a segment.
tal duration of 176 hours [38].
                                                                 When ASR is not available, we simply treat 16 tokens as a
    To obtain text from the videos, we utilize YouTube’s au-
                                                                 segment.
tomatic speech recognition (ASR) toolkit provided by the
YouTube Data API [1] to retrieve timestamped speech in-          4.3. Model Pre-training
formation. The API returns word sequences and the pre-
dicted language type. Among the 312K videos, 180K have              We initialize the BERT weights from a text pre-trained
ASR that can be retrieved by the API, and 120K of these          checkpoint. Specifically, we use the BERTLARGE model re-
are predicted to be in English. In our experiments, while we     leased by the authors of [6], using the same backbone archi-
use all videos for the video-only objective, we only use text    tecture: it has 24 layers of Transformer blocks, where each
from English ASR for VideoBERT’s text-only and video-            block has 1024 hidden units and 16 self-attention heads.
text objectives.                                                    We add support for video tokens by appending 20,736
    We evaluate VideoBERT on the YouCook II dataset [38],        entries to the word embedding lookup table for each of
which contains 2000 YouTube videos averaging 5.26 min-           our new “visual words”. We initialize these entries with
utes in duration, for a total of 176 hours. The videos have      the S3D features from their corresponding cluster centroids.
manually annotated segmentation boundaries and captions.         The input embeddings are frozen during pretraining.
On average there are 7.7 segments per video, and 8.8 words          Our model training process largely follows the setup of
per caption. We use the provided dataset split, with 1333        BERT: we use 4 Cloud TPUs in the Pod configuration with
videos for training and 457 for validation. To avoid po-         a total batch size of 128, and we train the model for 0.5
tential bias during pretraining, we also remove any videos       million iterations, or roughly 8 epochs. We use the Adam
which appear in YouCook II from our pretraining set.             optimizer with an initial learning rate of 1e-5, and a linear
                                                                 decay learning rate schedule. The training process takes
4.2. Video and Language Preprocessing                            around 2 days.
   For each input video, we sample frames at 20 fps, and
                                                                 4.4. Zero-shot action classification
create clips from 30-frame (1.5 seconds) non-overlapping
windows over the video. For each 30-frame clip, we apply            Once pretrained, the VideoBERT model can be used
a pretrained video ConvNet to extract its features. In this      for “zero-shot” classification on novel datasets, such as
work, we use the S3D [34] which adds separable temporal          YouCook II (By “zero-shot” we mean the model is not
Figure 4: Examples of video sentence pairs from the pretraining videos. We quantize each video segment into a token, and
then represent it by the corresponding visual centroid. For each row, we show the original frames (left) and visual centroids
(right). We can see that the tokenization process preserves semantic information rather than low-level visual appearance.

trained on YouCook II data nor with the same label ontol-
ogy used in YouCook II). More precisely, we want to com-
pute p(y|x) where x is the sequence visual tokens, and y is
a sequence of words. Since the model is trained to predict
sentences, we define y to be the fixed sentence, “now let
me show you how to [MASK] the [MASK],” and ex-
tract the verb and noun labels from the tokens predicted in
the first and second masked slots, respectively. See Figure 5
for some qualitative results.
    For quantitative evaluation, we use the YouCook II
dataset. In [37], the authors collected ground truth bound-
ing boxes for the 63 most common objects for the validation
set of YouCook II. However, there are no ground truth la-
bels for actions, and many other common objects are not
labeled. So, we collect action and object labels, derived
from the ground truth captions, to address this shortcoming.
We run an off-the-shelf part-of-speech tagger on the ground
truth captions to retrieve the 100 most common nouns and
45 most common verbs, and use these to derive ground truth
labels. While VideoBERT’s word piece vocabulary gives
it the power to effectively perform open-vocabulary clas-
sification, it is thus more likely to make semantically cor-
rect predictions that do not exactly match the more limited
ground truth. So, we report both top-1 and top-5 classifica-
tion accuracy metrics, where the latter is intended to miti-
gate this issue, and we leave more sophisticated evaluation
techniques for future work. Lastly, if there is more than
one verb or noun associated with a video clip, we deem a         Figure 5: Using VideoBERT to predict nouns and verbs
prediction correct if it matches any of those. We report the     given a video clip. See text for details. The video clip is
performance on the validation set of YouCook II.                 first converted into video tokens (two are shown here for
    Table 1 shows the top-1 and top-5 accuracies of              each example), and then visualized using their centroids.
VideoBERT and its ablations. To verify that VideoBERT
actually makes use of video inputs, we first remove the
video inputs to VideoBERT, and use just the language
                     Method              Supervision      verb top-1 (%)     verb top-5 (%)     object top-1 (%)     object top-5 (%)
                    S3D [34]                  yes              16.1               46.9                13.2                 30.9
               BERT (language prior)          no               0.0                 0.0                 0.0                 0.0
            VideoBERT (language prior)        no               0.4                 6.9                 7.7                 15.3
             VideoBERT (cross modal)          no               3.2                43.3                13.1                 33.7

                   Table 1: Action classification performance on YouCook II dataset. See text for details.
                      Method      Data size   verb top-1 (%)     verb top-5 (%)      object top-1 (%)     object top-5 (%)
                    VideoBERT        10K            0.4               15.5                   2.9                 17.8
                    VideoBERT        50K            1.1               15.7                   8.7                 27.3
                    VideoBERT       100K            2.9               24.5                  11.2                 30.6
                    VideoBERT       300K            3.2               43.3                  13.1                 33.7

          Table 2: Action classification performance on YouCook II dataset as a function of pre-training data size.
model p(y) to perform prediction. We also use the lan-                     tures for the video tokens and the masked out text tokens,
guage prior from the text-only BERT model, that was not                    take their average and concatenate the two together, to be
fine-tuned on cooking videos. We can see that VideoBERT                    used by a supervised model in a downstream task.
significantly outperforms both baselines. As expected, the                     We evaluate the extracted features on video captioning,
language prior of VideoBERT is adapted to cooking sen-                     following the setup from [39], where the ground truth video
tences, and is better than the vanilla BERT model.                         segmentations are used to train a supervised model map-
   We then compare with a fully supervised classifier that                 ping video segments to captions. We use the same model
was trained using the training split of YouCook II. We                     that they do, namely a transformer encoder-decoder, but we
use the pre-computed S3D features (same as the inputs to                   replace the inputs to the encoder with the features derived
VideoBERT), applying average pooling over time, followed                   from VideoBERT described above. We also concatenate the
by a linear classifier. Table 1 shows the results. As we                   VideoBERT features with average-pooled S3D features; as
can see, the supervised framework outperforms VideoBERT                    a baseline, we also consider using just S3D features without
in top-1 verb accuracy, which is not surprising given that                 VideoBERT. We set the number of Transformer block lay-
VideoBERT has an effectively open vocabulary. (See Fig-                    ers to 2, the hidden unit size to 128, and Dropout probability
ure 5 for an illustration of the ambiguity of the action la-               to 0.4. We use a 5-fold cross validation on the training split
bels.) However, the top-5 accuracy metric reveals that                     to set the hyper-parameters, and report performance on the
VideoBERT achieves comparable performance to the fully                     validation set. We train the model for 40K iterations with
supervised S3D baseline, without using any supervision                     batch size of 128. We use the same Adam optimizer as in
from YouCook II, indicating that the model is able to per-                 VideoBERT pre-training, and set the initial learning rate to
form competitively in this “zero-shot” setting.                            1e-3 with a linear decay schedule.
                                                                               Table 3 shows the results. We follow the standard prac-
4.5. Benefits of large training sets                                       tice in machine translation and compute BLEU and ME-
    We also studied the impact of the size of the pretrain-                TEOR scores micro-averaged at corpus level, and also re-
ing dataset. For this experiment, we take random subsets                   port ROUGE-L [14] and CIDEr [29] scores. For the base-
of 10K, 50K and 100K videos from the pretraining set,                      line method [39], we recompute the metrics using the
and pretrain VideoBERT using the same setup as above,                      predictions provided by the authors. We can see that
for the same number of epochs. Table 2 shows the perfor-                   VideoBERT consistently outperforms the S3D baseline, es-
mance. We can see that the accuracy grows monotonically                    pecially for CIDEr. We can also see that cross-modal pre-
as the amount of data increases, showing no signs of satura-               training outperforms the video-only version. Furthermore,
tion. This indicates that VideoBERT may benefit from even                  by concatenating the features from VideoBERT and S3D,
larger pretraining datasets.                                               the model achieves the best performance across all metrics1 .
                                                                               Figure 6 shows some qualitative results. We note that
4.6. Transfer learning for captioning                                      the predicted word sequence is rarely exactly equal to the
   We further demonstrate the effectiveness of VideoBERT                   ground truth, which explains why the metrics in Table 3
when used as a feature extractor. To extract features given                (which measure n-gram overlap) are all low in absolute
only video inputs, we again use a simple fill-in-the-blank                 value. However, semantically the results seem reasonable.
task, by appending the video tokens to a template sentence                    1 The metrics used by [39] are macro-averaged at video level and may
“now let’s [MASK] the [MASK] to the [MASK],                                suffer from undesirable sparsity artifacts. Using their provided evaluation
and then [MASK] the [MASK].” We extract the fea-                           code, VideoBERT + S3D has B@4 of 1.79, and METEOR of 10.80.
                                  Method            BLEU-3       BLEU-4   METEOR     ROUGE-L     CIDEr
                               Zhou et al. [39]      7.53         3.84     11.55       27.44      0.38
                                 S3D [34]            6.12         3.24      9.52       26.09      0.31
                           VideoBERT (video only)    6.33         3.81     10.81       27.14      0.47
                                VideoBERT            6.80         4.04     11.01       27.50      0.49
                             VideoBERT + S3D         7.59         4.33     11.94       28.80      0.55


Table 3: Video captioning performance on YouCook II. We follow the setup from [39] and report captioning performance on
the validation set, given ground truth video segments. Higher numbers are better.




Figure 6: Examples of generated captions by VideoBERT and the S3D baseline. In the last example, VideoBERT fails to
exploit the full temporal context, since it misses the paper towel frame.

5. Discussion and conclusion                                         explicitly model visual patterns at multiple temporal scales,
                                                                     instead of our current approach, that skips frames but builds
   This paper adapts the powerful BERT model to learn a              a single vocabulary.
joint visual-linguistic representation for video. Our exper-
imental results demonstrate that we are able to learn high-             Beyond improving the model, we plan to assess our ap-
level semantic representations, and we outperform the state-         proach on other video understanding tasks, and on other do-
of-the-art for video captioning on the YouCook II dataset.           mains besides cooking. (For example, we may use the re-
We also show that this model can be used directly for open-          cently released COIN dataset of manually labeled instruc-
vocabulary classification, and that its performance grows            tional videos [26].) We believe the future prospects for large
monotonically with the size of training set.                         scale representation learning from video and language look
   This work is a first step in the direction of learning            quite promising.
such joint representations. For many applications, includ-
ing cooking, it is important to use spatially fine-grained vi-       Acknowledgements. We would like to thank Jack Hessel,
sual representations, instead of just working at the frame or        Bo Pang, Radu Soricut, Baris Sumengen, Zhenhai Zhu, and
clip level, so that we can distinguish individual objects and        the BERT team for sharing amazing tools that greatly fa-
their attributes. We envision either using pretrained object         cilitated our experiments; Justin Gilmer, Abhishek Kumar,
detection and semantic segmentation models, or using unsu-           David Ross, and Rahul Sukthankar for helpful discussions.
pervised techniques for broader coverage. We also want to            Chen would like to thank Y. M. for inspiration.
References                                                          [18] Ishan Misra, C Lawrence Zitnick, and Martial Hebert. Shuf-
                                                                         fle and learn: unsupervised learning using temporal order
 [1] YouTube Data API. https://developers.google.                        verification. In ECCV, 2016. 3
     com/youtube/v3/docs/captions. 5                                [19] Mathew Monfort, Alex Andonian, Bolei Zhou, Kandan Ra-
 [2] Jean-Baptiste Alayrac, Piotr Bojanowski, Nishant Agrawal,           makrishnan, Sarah Adel Bargal, Yan Yan, Lisa Brown,
     Josef Sivic, Ivan Laptev, and Simon Lacoste-Julien. Unsu-           Quanfu Fan, Dan Gutfreund, Carl Vondrick, et al. Moments
     pervised learning from narrated instruction videos. In CVPR,        in time dataset: one million videos for event understanding.
     2016. 3                                                             TPAMI, 2019. 2
 [3] Yusuf Aytar, Carl Vondrick, and Antonio Torralba. Sound-       [20] Andrew Owens, Phillip Isola, Josh McDermott, Antonio Tor-
     net: Learning sound representations from unlabeled video.           ralba, Edward H Adelson, and William T Freeman. Visually
     In NeurIPS, 2016. 3                                                 indicated sounds. In CVPR, 2016. 3
 [4] Mohammad Babaeizadeh, Chelsea Finn, Dumitru Erhan,             [21] Andrew Owens, Jiajun Wu, Josh H McDermott, William T
     Roy H Campbell, and Sergey Levine. Stochastic variational           Freeman, and Antonio Torralba. Ambient sound provides
     video prediction. In ICLR, 2018. 2                                  supervision for visual learning. In ECCV, 2016. 3
 [5] Emily Denton and Rob Fergus. Stochastic video generation       [22] Matthew E Peters, Mark Neumann, Mohit Iyyer, Matt Gard-
     with a learned prior. In ICML, 2018. 2                              ner, Christopher Clark, Kenton Lee, and Luke Zettlemoyer.
                                                                         Deep contextualized word representations. In NAACL, 2018.
 [6] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina
                                                                         3
     Toutanova.      BERT: Pre-training of deep bidirectional
                                                                    [23] Marc Aurelio Ranzato and Alex Graves. Deep unsupervised
     transformers for language understanding. arXiv preprint
                                                                         learning. NIPS Tutorial, 2018. 3
     arXiv:1810.04805, 2018. 2, 3, 5
                                                                    [24] Chen Sun, Abhinav Shrivastava, Saurabh Singh, and Abhi-
 [7] Chunhui Gu, Chen Sun, David A Ross, Carl Vondrick, Car-
                                                                         nav Gupta. Revisiting unreasonable effectiveness of data in
     oline Pantofaru, Yeqing Li, Sudheendra Vijayanarasimhan,
                                                                         deep learning era. In ICCV, 2017. 1
     George Toderici, Susanna Ricco, Rahul Sukthankar, et al.
                                                                    [25] Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet,
     AVA: A video dataset of spatio-temporally localized atomic
                                                                         Scott E. Reed, Dragomir Anguelov, Dumitru Erhan, Vincent
     visual actions. In CVPR, 2018. 2
                                                                         Vanhoucke, and Andrew Rabinovich. Going deeper with
 [8] Andrej Karpathy and Li Fei-Fei. Deep visual-semantic align-         convolutions. arXiv preprint arXiv:1409.4842, 2014. 5
     ments for generating image descriptions. In CVPR, 2015. 3      [26] Yansong Tang, Dajun Ding, Yongming Rao, Yu Zheng,
 [9] Will Kay, Joao Carreira, Karen Simonyan, Brian Zhang,               Danyang Zhang, Lili Zhao, Jiwen Lu, and Jie Zhou. COIN:
     Chloe Hillier, Sudheendra Vijayanarasimhan, Fabio Viola,            A large-scale dataset for comprehensive instructional video
     Tim Green, Trevor Back, Paul Natsev, et al. The kinetics hu-        analysis. In CVPR, 2019. 8
     man action video dataset. arXiv preprint arXiv:1705.06950,     [27] Sergey Tulyakov, Ming-Yu Liu, Xiaodong Yang, and Jan
     2017. 2, 5                                                          Kautz. MoCoGAN: Decomposing motion and content for
[10] Ranjay Krishna, Kenji Hata, Frederic Ren, Li Fei-Fei, and           video generation. In CVPR, 2018. 2
     Juan Carlos Niebles. Dense-Captioning events in videos. In     [28] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszko-
     ICCV, 2017. 2, 3                                                    reit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Illia
[11] Girish Kulkarni, Visruth Premraj, Sagnik Dhar, Siming Li,           Polosukhin. Attention is all you need. In NIPS, 2017. 3
     Yejin Choi, Alexander C Berg, and Tamara L Berg. Baby          [29] Ramakrishna Vedantam, C Lawrence Zitnick, and Devi
     talk: Understanding and generating image descriptions. In           Parikh. Cider: Consensus-based image description evalua-
     CVPR, 2011. 3                                                       tion. In CVPR, 2015. 7
[12] Guillaume Lample and Alexis Conneau. Cross-lingual lan-        [30] Carl Vondrick, Hamed Pirsiavash, and Antonio Torralba. An-
     guage model pretraining. arXiv preprint arXiv:1901.07291,           ticipating visual representations from unlabeled video. In
     2019. 3                                                             CVPR, 2016. 2
[13] Alex X Lee, Richard Zhang, Frederik Ebert, Pieter Abbeel,      [31] Carl Vondrick, Hamed Pirsiavash, and Antonio Torralba.
     Chelsea Finn, and Sergey Levine. Stochastic adversarial             Generating videos with scene dynamics. In NeurIPS, 2016.
     video prediction. arXiv:1804.01523, 2018. 2                         2
                                                                    [32] Jacob Walker, Carl Doersch, Abhinav Gupta, and Martial
[14] Chin-Yew Lin. Rouge: A package for automatic evaluation
                                                                         Hebert. An uncertain future: Forecasting from static images
     of summaries. Text Summarization Branches Out, 2004. 7
                                                                         using variational autoencoders. In ECCV, 2016. 2
[15] Jiasen Lu, Jianwei Yang, Dhruv Batra, and Devi Parikh.         [33] Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V Le,
     Neural baby talk. In CVPR, 2018. 3                                  Mohammad Norouzi, Wolfgang Macherey, Maxim Krikun,
[16] Jonathan Malmaud, Jonathan Huang, Vivek Rathod, Nick                Yuan Cao, Qin Gao, Klaus Macherey, et al. Google’s
     Johnston, Andrew Rabinovich, and Kevin Murphy. What’s               neural machine translation system: Bridging the gap be-
     cookin’? interpreting cooking videos using text, speech and         tween human and machine translation. arXiv preprint
     vision. In NAACL, Mar. 2015. 3                                      arXiv:1609.08144, 2016. 5
[17] Michael Mathieu, Camille Couprie, and Yann LeCun. Deep         [34] Saining Xie, Chen Sun, Jonathan Huang, Zhuowen Tu, and
     multi-scale video prediction beyond mean square error. In           Kevin Murphy. Rethinking spatiotemporal feature learning
     ICLR, 2016. 2                                                       for video understanding. In ECCV, 2018. 5, 7, 8
[35] Tianfan Xue, Jiajun Wu, Katherine Bouman, and Bill Free-
     man. Visual dynamics: Probabilistic future frame synthesis
     via cross convolutional networks. In NIPS, 2016. 2
[36] Hang Zhao, Zhicheng Yan, Heng Wang, Lorenzo Torresani,
     and Antonio Torralba. Slac: A sparsely labeled dataset
     for action classification and localization. arXiv preprint
     arXiv:1712.09374, 2017. 2
[37] Luowei Zhou, Nathan Louis, and Jason J Corso. Weakly-
     supervised video object grounding from text by loss weight-
     ing and object interaction. In BMVC, 2018. 6
[38] Luowei Zhou, Chenliang Xu, and Jason J Corso. Towards
     automatic learning of procedures from web instructional
     videos. In AAAI, 2018. 2, 3, 5
[39] Luowei Zhou, Yingbo Zhou, Jason J. Corso, Richard Socher,
     and Caiming Xiong. End-to-end dense video captioning with
     masked transformer. In CVPR, 2018. 2, 3, 7, 8
Figure A1: Visualizations for video to text prediction. For each example, we show the key frames from the original video
(top left) and the associated ASR outputs (top right), we then show the centroid images of video tokens (bottom left) and the
top predicted verbs and nouns by VideoBERT (bottom right). Note that the ASR outputs are not used to predict verbs and
nouns.
Figure A2: Visualizations for video to video prediction. Given an input video token, we show the top 3 predicted video
tokens 2 steps away in the future. We visualize each video token by the centroids.
Figure A3: Visualizations for text to video prediction. In particular, we make small changes to the input text, and compare
how the generated video tokens vary. We show top 2 retrieved video tokens for each text query.
