Number of distinct tasks evaluated: 26
- Task 1: arc_challenge (commonsense reasoning). (2024_NoPE.pdf, Table 1)
- Task 2: arc_easy (commonsense reasoning). (2024_NoPE.pdf, Table 1)
- Task 3: boolq (commonsense reasoning). (2024_NoPE.pdf, Table 1)
- Task 4: hellaswag (commonsense reasoning). (2024_NoPE.pdf, Table 1)
- Task 5: openbookqa (commonsense reasoning). (2024_NoPE.pdf, Table 1)
- Task 6: piqa (commonsense reasoning). (2024_NoPE.pdf, Table 1)
- Task 7: winogrande (commonsense reasoning). (2024_NoPE.pdf, Table 1)
- Task 8: PG19 (long sequence language modeling). (2024_NoPE.pdf, Section 4.2; Table 2)
- Task 9: Proof-pile (long sequence language modeling). (2024_NoPE.pdf, Section 4.2; Table 2)
- Task 10: Passkey Retrieval (synthetic long-context task). (2024_NoPE.pdf, Section 4.3)
- Task 11: NQA (LongBench single-doc QA). (2024_NoPE.pdf, Table 3)
- Task 12: Qsp (LongBench single-doc QA). (2024_NoPE.pdf, Table 3)
- Task 13: MulF (LongBench single-doc QA). (2024_NoPE.pdf, Table 3)
- Task 14: HpQA (LongBench multi-doc QA). (2024_NoPE.pdf, Table 3)
- Task 15: 2WQA (LongBench multi-doc QA). (2024_NoPE.pdf, Table 3)
- Task 16: Musq. (LongBench multi-doc QA). (2024_NoPE.pdf, Table 3)
- Task 17: GRpt (LongBench summarization). (2024_NoPE.pdf, Table 3)
- Task 18: QSum (LongBench summarization). (2024_NoPE.pdf, Table 3)
- Task 19: MulN (LongBench summarization). (2024_NoPE.pdf, Table 3)
- Task 20: TREC (LongBench few-shot learning). (2024_NoPE.pdf, Table 3)
- Task 21: TrQA (LongBench few-shot learning). (2024_NoPE.pdf, Table 3)
- Task 22: SSum (LongBench few-shot learning). (2024_NoPE.pdf, Table 3)
- Task 23: PsgC (LongBench synthetic). (2024_NoPE.pdf, Table 3)
- Task 24: PsgR (LongBench synthetic). (2024_NoPE.pdf, Table 3)
- Task 25: Lcc (LongBench code). (2024_NoPE.pdf, Table 3)
- Task 26: Re-P (LongBench code). (2024_NoPE.pdf, Table 3)

Number of trained model instances required to cover all tasks: 1
- The paper trains a single NoPE base model and evaluates it across long sequence language modeling, passkey retrieval, and LongBench. (2024_NoPE.pdf, Section 4)
- The same NoPE model is evaluated on the commonsense reasoning benchmarks in Table 1, with no task-specific heads or fine-tuning described. (2024_NoPE.pdf, Table 1; Section 4.1)

$$
\boxed{
\frac{26\ \text{tasks}}{1\ \text{model}} = 26
}
$$
