## 1. Basic Metadata
- Title: "H-ARC: A Robust Estimate of Human Performance on the Abstraction and Reasoning Corpus Benchmark" (Title, p.1)
- Authors: "Solim LeGris, Wai Keen Vong, Brenden M. Lake and Todd M. Gureckis" (Title, p.1)
- Year: 2024 ("arXiv:2409.01374v1 [cs.AI] 2 Sep 2024", Title page, p.1)
- Venue: arXiv ("arXiv:2409.01374v1 [cs.AI] 2 Sep 2024", Title page, p.1)

## 2. One-Sentence Contribution Summary
This paper provides a more robust estimate of human performance on ARC by evaluating 1729 humans across the full 400 training and 400 evaluation tasks. ("we obtain a more robust estimate of human performance by evaluating 1729 humans on the full set of 400 training and 400 evaluation tasks from the original ARC problem set", Abstract, p.1)

## 3. Tasks Evaluated
Task 1:
- Task name: ARC training tasks (training split)
- Task type: Other (visual program synthesis / abstract reasoning)
- Dataset(s) used: ARC training set (400 tasks)
- Domain: abstract visual grids
- Quotes:
  - "The Abstraction and Reasoning Corpus (ARC) is a visual program synthesis benchmark" (Abstract, p.1)
  - "Each task requires inferring an underlying transformation rule or program from a series of training input-output pairs which consist of abstract visual grids (see Figure 1), and to use this rule to correctly generate an output grid given a novel test input." (Introduction, p.1)
  - "We collected human data on each of the 400 training tasks and 400 evaluation tasks from ARC in two separate phases (extending the subset of 40 training tasks previously collected and described in [8])." (Methods 2.1 Design, p.2)

Task 2:
- Task name: ARC evaluation tasks (evaluation split)
- Task type: Other (visual program synthesis / abstract reasoning)
- Dataset(s) used: ARC evaluation set (400 tasks)
- Domain: abstract visual grids
- Quotes:
  - "The Abstraction and Reasoning Corpus (ARC) is a visual program synthesis benchmark" (Abstract, p.1)
  - "Each task requires inferring an underlying transformation rule or program from a series of training input-output pairs which consist of abstract visual grids (see Figure 1), and to use this rule to correctly generate an output grid given a novel test input." (Introduction, p.1)
  - "We collected human data on each of the 400 training tasks and 400 evaluation tasks from ARC in two separate phases" (Methods 2.1 Design, p.2)

## 4. Domain and Modality Scope
- Single domain? Yes. Evidence: "abstract visual grids" (Introduction, p.1) and ARC is a "visual program synthesis benchmark" (Abstract, p.1).
- Multiple domains within the same modality? Not specified in the paper.
- Multiple modalities? Not specified in the paper.
- Domain generalization / cross-domain transfer claimed?
  - Out-of-distribution generalization is claimed: "designed to test challenging out-of-distribution generalization in humans and machines" (Abstract, p.1).
  - Cross-domain transfer: Not claimed.

## 5. Model Sharing Across Tasks
| Task | Shared Weights? | Fine-Tuned? | Separate Head? | Evidence |
| --- | --- | --- | --- | --- |
| ARC training tasks | Not specified in the paper. | Not specified in the paper. | Not specified in the paper. | Not specified in the paper. |
| ARC evaluation tasks | Not specified in the paper. | Not specified in the paper. | Not specified in the paper. | Not specified in the paper. |

## 6. Input and Representation Constraints
- Training/test pair counts: "Each task has 1–10 training input-output pairs, and 1–3 test input-output pairs." (Methods 2.1 Design, p.2)
- Test example selection: "we opted to evaluate humans using only the first test example for all tasks." (Methods 2.1 Design, p.2)
- Variable grid size / resizing allowed: "Participants could resize the grid height and width" (Methods 2.3 Experiment, p.3).
- Output grid initialization: "A reset button allowed participants to revert the output grid back to the initial state, a 3 × 3 black grid." (Methods 2.3 Experiment, p.3)
- Evidence of variable output size across splits: "Output grid size. An independent samples t-test confirmed that output grid size (number of grid cells) is significantly larger on average in the evaluation set (M =235.1, SD=246.7) than in the training set (M =136.2, SD=164.9), t(798) = 6.81, p < .001." (Results 3.2, p.7)
- Fixed patch size? Not specified in the paper.
- Fixed number of tokens? Not specified in the paper.
- Fixed dimensionality (e.g., strictly 2D)? Not specified in the paper.
- Padding requirements? Not specified in the paper.

## 7. Context Window and Attention Structure
- Maximum sequence length: Not specified in the paper.
- Fixed or variable sequence length: Not specified in the paper.
- Attention type (global/windowed/hierarchical/sparse): Not specified in the paper.
- Mechanisms to manage computational cost (windowing/pooling/pruning): Not specified in the paper.

## 8. Positional Encoding (Critical Section)
- Positional encoding mechanism: Not specified in the paper.
- Where it is applied: Not specified in the paper.
- Fixed/modified/ablated: Not specified in the paper.

## 9. Positional Encoding as a Variable
- Treated as core research variable or fixed assumption? Not specified in the paper.
- Multiple positional encodings compared? Not specified in the paper.
- Claim that PE choice is “not critical” or secondary? Not specified in the paper.

## 10. Evidence of Constraint Masking
- Model size(s): Not specified in the paper.
- Dataset size(s):
  - "the full set of 400 training and 400 evaluation tasks from the original ARC problem set" (Abstract, p.1)
  - "We collected human data on each of the 400 training tasks and 400 evaluation tasks from ARC" (Methods 2.1 Design, p.2)
  - "Total number of participants                783                 946" (Table 2, p.5)
- Performance gains attributed to scaling model size, scaling data, architectural hierarchy, or training tricks: Not specified in the paper.

## 11. Architectural Workarounds
Not specified in the paper.

## 12. Explicit Limitations and Non-Claims
- Contextual factors limit the interpretation of averages: "average performance on ARC tasks as reported here can be affected by many contextual factors and does not reflect some absolute measure of human performance." (Discussion 4.1, p.9)
- Bounds are acknowledged as unrealistic: "We consider these pessimistic and optimistic estimates somewhat unrealistic." (Results 3.1, p.6)
- Difficulty explanation is inconclusive: "Although it remains unclear why evaluation set ARC puzzles are harder" (Discussion 4.4, p.10)
- Need for deeper analysis: "Although we provide a comprehensive estimate of human performance on ARC tasks and preliminary analyses, more in-depth analyses of essential elements of people’s problem solving strategies such as state space trajectories, action traces and natural language descriptions are needed." (Conclusion, p.10)
- Limited inference from small samples per task: "this result simply means that we did not find anyone in a set of 10 that could solve the problem, not that they are in-principle not solvable." (Results 3.1, p.6)
