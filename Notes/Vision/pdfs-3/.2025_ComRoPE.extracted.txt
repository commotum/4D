                                         ComRoPE: Scalable and Robust Rotary Position Embedding Parameterized by
                                                          Trainable Commuting Angle Matrices

                                                   Hao Yu1 Tangyu Jiang1† Shuning Jia1,2 Shannan Yan1 Shunning Liu1
                                                Haolong Qian1 Guanghao Li1 Shuting Dong1 Huaisong Zhang1 Chun Yuan1†
                                                                               1                               2
                                                                                   Tsinghua University             Shenzhen University
                                                                      longinyh@gmail.com, jiangtangyu,yuanc@sz.tsinghua.edu.cn
arXiv:2506.03737v1 [cs.CV] 4 Jun 2025




                                                                      Abstract
                                                                                                                   their effectiveness across a wide range of domains. At the
                                        The Transformer architecture has revolutionized various                    core of the Transformer model lies the attention mechanism,
                                        fields since it was proposed, where positional encoding                    which enables the model to selectively focus on different
                                        plays an essential role in effectively capturing sequential                parts of the input based on relevance, thereby effectively
                                        order and context. Therefore, Rotary Positional Encoding                   capturing long-range dependencies and contextual relation-
                                        (RoPE) was proposed to alleviate these issues, which in-                   ships.
                                        tegrates positional information by rotating the embeddings                    However, since the attention mechanism is insensitive to
                                        in the attention mechanism. However, RoPE utilizes man-                    the fundamental position information, it cannot inherently
                                        ually defined rotation matrices, a design choice that fa-                  capture the order of elements in the data. In order to al-
                                        vors computational efficiency but limits the model’s flexibil-             leviate this issue, positional embeddings are added to the
                                        ity and adaptability. In this work, we propose ComRoPE,                    input representation, providing the model with the neces-
                                        which generalizes RoPE by defining it in terms of train-                   sary positional information to process the sequence order
                                        able commuting angle matrices. Specifically, we demon-                     and structure effectively.
                                        strate that pairwise commutativity of these matrices is es-                    A variety of positional encoding methods have been pro-
                                        sential for RoPE to achieve scalability and positional ro-                 posed, which can be divided into two categories: Abso-
                                        bustness. We formally define the RoPE Equation, which is                   lute Positional Encoding (APE) and Relative Positional En-
                                        an essential condition that ensures consistent performance                 coding (RPE). APE explicitly encodes the absolute posi-
                                        with position offsets. Based on the theoretical analysis,                  tion of each token by generating fixed positional embed-
                                        we present two types of trainable commuting angle ma-                      dings, which are directly added to the input embeddings
                                        trices as sufficient solutions to the RoPE equation, which                 at the beginning of training [35]. However, APE struggles
                                        significantly improve performance, surpassing the current                  with handling long sequences and exhibits high sensitivity
                                        state-of-the-art method by 1.6% at training resolution and                 to positional shifts, limiting the scalability and robustness
                                        2.9% at higher resolution on the ImageNet-1K dataset. Fur-                 of models [14]. In contrast, RPE does not directly mod-
                                        thermore, our framework shows versatility in generalizing                  ify input embeddings; instead, it incorporates relative posi-
                                        to existing RoPE formulations and offering new insights                    tional information within the attention mechanism, enabling
                                        for future positional encoding research. To ensure repro-                  the model to effectively capture positional relationships be-
                                        ducibility, the source code and instructions are available at              tween tokens. Among all the existing works, Rotary Posi-
                                        https://github.com/Longin-Yu/ComRoPE.                                      tion Embedding (RoPE) [32] has gained significant atten-
                                                                                                                   tion due to the advantage of applying a rotational transfor-
                                        1. Introduction
                                                                                                                   mation to token embeddings. It encodes relative positions
                                        The Transformer architecture [35] has been widely adopted                  by treating each pair of features as coordinates and rotating
                                        across various fields, including Natural Language Process-                 them by an angle proportional to their position, enabling all
                                        ing (NLP) [2, 6, 10, 25, 26] and Computer Vision (CV) [7].                 tokens to interact within the attention mechanism regardless
                                        Moreover, an increasing number of Transformer-based ap-                    of distance.
                                        plications [3, 15, 16, 18, 19, 38–41] continue to demonstrate                  However, existing RoPE methods face several key chal-
                                            † Corresponding author.  This work was done when Shuning Jia was       lenges: i) The essential components (i.e., the RoPE ma-
                                        an intern at Tsinghua University.                                          trices) of previous RoPE approaches rely on 2D rotation
groups, which simplify computations but consequently re-            2. Related work
strict their feature projection capabilities, especially in high-
dimensional spaces [32]. ii) Moreover, the majority of              2.1. Position information in attention
the rotation matrices of RoPE require to be manually de-
                                                                    Transformers [35] utilize the attention mechanism to cap-
signed, leading to insufficient capability and suboptimal
                                                                    ture similarities within sequences. However, they lack in-
performance[24, 29]. iii) Finally, previous attempts [23] to
                                                                    herent sequential information and cannot capture the posi-
extend the rotation group often prioritize design simplicity,
                                                                    tional information of each token. To address this limitation,
making it difficult to consistently satisfy relative position
                                                                    positional encoding [6, 28, 30] was introduced. As research
dependency—a critical property of RoPE that ensures posi-
                                                                    progressed, positional encoding generally evolved into two
tional robustness against offsets.
                                                                    types: APE [6, 33, 35] and RPE [13, 21, 30, 32]. [35] first
Our objectives. This work aims to develop a novel RoPE              proposed using APE in the form of sine and cosine func-
method for Transformers that is both scalable and robust.           tions, effectively representing the positional relationships
Specifically, we seek to extend the rotation group from the         within the input sequence. This positional encoding method
existing 2D representation to a larger subgroup of the spe-         achieved remarkable results in natural language processing,
cial orthogonal group, allowing for higher degrees of free-         becoming a foundational component for many NLP tasks.
dom while preserving consistent behavior with respect to            However, the fixed nature of positional encoding limited
position offsets. Unlike existing methods that rely on man-         the model’s ability to generalize to longer input sequences.
ually designed non-trainable rotation matrices, which suffer        In response, subsequent research introduced learnable abso-
from limited expressiveness and reduced robustness, our ap-         lute positional encoding. [6] proposed further enhancing the
proach is designed to offer richer feature representation ca-       model’s expressive power by incorporating learnable posi-
pabilities. This framework addresses the scalability limita-        tion embeddings, particularly excelling in tasks such as sen-
tions of current approaches and enhances their robustness           tence alignment and context representation. Although APE
against positional transformations.                                 provides positional information to enhance the model’s un-
                                                                    derstanding of sequences, it shows limitations in handling
Our contributions. This work introduces ComRoPE, a                  long sequences and cross-sequence scenarios. Hierarchi-
novel framework that significantly enhances positional en-          cal ViT such as Swin Transformer [20], introduced Relative
coding in Transformers. ComRoPE leverages trainable an-             Position Bias (RPB) [20, 29, 30] to handle large numbers of
gle matrices, extending the RoPE mechanism with higher              tokens with limited positional embeddings [12]. Related re-
scalability and robustness. We identify the pairwise com-           search has explored alternative encoding methods, such as
mutativity of these matrices as a necessary and sufficient          RPE, as a replacement for APE to better capture complex
condition for effective positional encoding, thereby unify-         dependency structures. iRPE [36] proposed an improved
ing various existing RoPE formulations under a single the-          RPB by combining relative position embedding.
oretical framework. The contributions are summarized as
follows:                                                            2.2. Rotary position embedding
• We formally define the RoPE function parameterized by             Building on these developments, RoFormer [32] combined
  angle matrices and prove that pairwise commutativity is a         the advantages of APE and RPE, proposing RoPE. Cur-
  necessary and sufficient condition, offering a unified the-       rently, RoPE is widely used in Large Language Models
  ory that encompasses several existing RoPE variants.              (LLMs), such as LLaMA [34] and Vicuna [4]. This ap-
• We introduce ComRoPE, a scalable and robust solution              proach enhances model performance on tasks involving
  that leverages two types of trainable commuting angle             long-text semantics and multi-turn dialogues, improving ex-
  matrix sets as sufficient solutions to the RoPE equation,         trapolation capability. To better adapt to the characteris-
  capturing richer positional representations without the           tics of two-dimensional data such as images, researchers
  need for manual design.                                           extended RoPE to two-dimensional sequences (2D-RoPE).
• Our extensive experiments show that ComRoPE sur-                  LieRE [23] extends RoPE to a more generalized form by
  passes the current state-of-the-art LieRE, achieving a per-       introducing a rotation-based positional encoding method
  formance increase of 1.6% at training resolution and              grounded in Lie group theory. Unified-IO 2 [22] applies
  2.9% at higher resolutions on the ImageNet-1K classi-             2D-RoPE within its multimodal architecture; EVA-02 [8],
  fication task while delivering strong results across other        FiT [8] these pioneering works used 2D RoPE with axial
  benchmarks.                                                       frequencies (2D Axial RoPE), but had limitations in pro-
• We explore further applications of ComRoPE, providing             cessing in the diagonal direction. Therefore, RoPE for ViT
  valuable insights for advancing position encoding tech-           [12] proposes to use mixed axial frequency for 2D RoPE,
  niques in future Transformer-based models.                        named RoPE-Mixed.
3. Method                                                                                                                                                                                Definition 4 (RoPE Equation). Let f : Rd × RN → Rd be
                                                                                                                                                                                         an RPE function. f is said to be a RoPE function if and
In this section, we begin by introducing key definitions                                                                                                                                 only if there exists a matrix-valued function Rf ( · ) such
and reformulating the RoPE paradigm within multi-axial                                                                                                                                   that the following conditions hold for all x, y ∈ RN and
attention mechanisms, which we collectively refer to as                                                                                                                                  q, k ∈ Rd in RPE Equation:
the RoPE Equations. Next, we present our main theorem,
which establishes the necessary and sufficient conditions for                                                                                                                                        \label {eq:rope-equation} \left \{ \begin {array}{l} \vspace {1ex} f(\bm q, \bm x) = \mathbf {R}_f(\bm x) \bm q\\ \vspace {1ex} \rho (\bm q, \bm k) = \bm q^\top \bm k \\ g(\bm q,\bm k, \bm x-\bm y) = \bm q^\top \mathbf {R}_f(\bm y-\bm x) \; \bm k \end {array} \right . 
RoPE functions parameterized by angle matrices. Finally,
                                                                                                                                                                                                                                                                                                                                                                                                                                                                    (2)
we provide solutions to the RoPE Equations based on the
propositions that outline the sufficient conditions.
3.1. Preliminaries                                                                                                                                                                       We refer to Eq. (2) as the RoPE Equation.
                                                               N                             d×d
We use R( · ) : R → R             to denote a matrix-value                                                                                                                                  By substituting the RoPE equation into the RPE equa-
function. We denote the vectors and matrices as the lower                                                                                                                                tion, we obtain that the RoPE function satisfies the follow-
and uppercase bold font, respectively. We first recall two                                                                                                                               ing property.
fundamental definitions of matrices as follows:
Definition 1 (Matrix Exponential). The exponential of a                                                                                                                                  Proposition 1.  f  is said to be a RoPE function if and only
square matrix  \mathbf {A} , denoted as  e^{\mathbf A}  or \qopname \relax o{exp}(\mathbf A), is defined using                                                                           if the matrix-valued function  \mathbf {R}_f(\;\cdot \;)  satisfies:
the matrix exponential series such that:
                                                                                                                                                                                                                                                             \mathbf {R}_f(\bm {x})^\top \mathbf {R}_f(\bm {y}) = \mathbf {R}_f(\bm {y} - \bm {x}), 

    e^\mathbf {A} = \exp (\mathbf {A}) = \sum _{k=0}^{\infty } \frac {\mathbf {A}^k}{k!} = \mathbf I + \mathbf {A} + \frac {\mathbf {A}^2}{2!} + \frac {\mathbf {A}^3}{3!} + \cdots ,    for all  \bm {x}, \bm {y} \in \mathbb {R}^N  .
                                                                                                                                                                                            Note that the definition of RoPE Equation demonstrates
This series converges for any square matrix  \mathbf {A} .
                                                                                                                                                                                         that the position encoding should only be dependent on the
Definition 2 (Commuting Matrices). Two square matrices                                                                                                                                   relative position of the tokens, which thus be robust against
 \mathbf {A}_1  and  \mathbf {A}_2  are said to commute if their product is indepen-                                                                                                     the offset operations. We further expand the definition of
dent of the order of multiplication, i.e.,                                                                                                                                               the RoPE function to a parameterized one (i.e., Definition
                                                                  \mathbf {A}_1 \mathbf {A}_2 = \mathbf {A}_2 \mathbf {A}_1.                                                             6) via rotation matrices (i.e., Definition 5) as follows1 .

A set of square matrices  \{\mathbf {A}_1, \mathbf {A}_2, \cdots , \mathbf {A}_N\}  is said to                                                                                           Definition 5 (Parameterized Rotation Matrix). Let
pairwise commute if every pair of matrices within the set                                                                                                                                                                                                             be a matrix-valued func-
                                                                                                                                                                                          \mathbf {R}(\;\cdot \;; \mathcal A): \mathbb {R}^N \rightarrow \mathbb {R}^{d \times d} 

commutes with each other. That is, for all  i, j  such that                                                                                                                              tion parameterized by  N  skew-symmetric matrices
 1 \leq i, j \leq N  ,                                                                                                                                                                    \mathcal A = \{ \mathbf {A}_1, \mathbf {A}_2, \ldots , \mathbf {A}_N \} . We say that  \mathbf {R}(\;\cdot \;; \mathcal {A})  is a
                         \mathbf {A}_i \mathbf {A}_j = \mathbf {A}_j \mathbf {A}_i.                                                                                                      rotation matrix function parameterized by angle matrices
                                                                                                                                                                                         A if it can be expressed as:
    To clarify and better illustrate the main theorems pre-
sented in the following sections, we first reformulate and
unify the definitions of the RoPE paradigm, specifically in                                                                                                                                                                                                   \label {eq:exp-solution} \mathbf {R}(\bm {x}; \mathcal {A}) = \exp \left (\sum _{i=1}^{N} \mathbf A_i x_i\right ),                                                                                    (3)
the context of multi-axial attention mechanisms. RoPE was
initially proposed by Su et al. [32] as a positional encoding
method based on relative position dependencies. However,                                                                                                                                 where  \bm {x} = (x_1, x_2, \ldots , x_N) \in \mathbb {R}^N  and  \exp (\;\cdot \;)  denotes
previous work provided only conceptual and descriptive de-                                                                                                                               the matrix exponential.
scriptions of RPE and RoPE without offering a rigorous for-                                                                                                                              Definition 6 (Parameterized RoPE Function). Let f : Rd ×
mal definition. In this work, we provide the formal defini-                                                                                                                              RN → Rd be a RoPE function. f is said to be parameter-
tions of both RPE and RoPE.                                                                                                                                                              ized by angle matrices if and only if there exists a rotation
Definition 3 (RPE Equation). Let f : Rd × RN → Rd be                                                                                                                                     matrix function Rf ( · ; A) parameterized by angle matrices
a positional encoding function, and ρ : Rd × Rd → R be                                                                                                                                   A such that the RoPE Equation (i.e., Eq. (2)) holds.
a similarity function. f is said to be a RPE function if and                                                                                                                                In this case, Rf ( · ; A) is referred to as the rotation
only if there exists a function g : Rd × Rd × RN → R such                                                                                                                                matrix function of RoPE function f ( · ; A) parameter-
that the following conditions hold for all x, y ∈ RN and                                                                                                                                 ized by angle matrices A. For simplicity, we slightly abuse
q, k ∈ Rd :                                                                                                                                                                              the notation and refer to R as the rotation matrix of RoPE
                                                                                                                                                                                         function f .
                                 \label {eq:rpe-equation} g(\bm q,\bm k,\bm x-\bm y)=\rho (f(\bm q,\bm x), f(\bm k, \bm y)),                                                    (1)
                                                                                                                                                                                             1 For clarification, the definitions of the rotation matrix and its exponen-

We refer to Eq. (1) as the RPE Equation.                                                                                                                                                 tial representation can be found in Appendix A.
Figure 1. Overview of ComRoPE. Features are arranged into several blocks, each representing a distinct point in the feature space. The
positions, along with the angle matrices, define the rotation matrix, which is an element of the special orthogonal group. The rotation
transformation projects a feature point onto another point on the surface of the same hypersphere.


3.2. Main theorems                                                                                                                                                                                                        sometimes R(xq ; A)⊤ R(xk ; A) ̸= R(xq −xk ; A) because
                                                                                                                                                                                                                          the probability that two random matrices commute is small,
Based on the formal definitions above, we present our key
                                                                                                                                                                                                                          where xq and xk are two arbitrary coordinates. Thus, R
theoretical results in the following theorem.
                                                                                                                                                                                                                          proposed by LieRE does not consistently satisfy the require-
Theorem 1. Let  \mathbf {R}(\;\cdot \;; \mathcal {A}): \mathbb {R}^N \rightarrow \mathbb {R}^{d \times d} \; (N>1) be                                                                                                     ments of the RoPE Equation.
a rotation matrix function parameterized by angle matrices
A. The rotation difference R(x)⊤ R(y) can be represented
                                                                                                                                                                                                                          3.3. Construction of pairwise commuting matrices
by the location difference y − x if and only if A pairwise                                                                                                                                                                In this section, we elaborate on concrete ways to con-
commute.                                                                                                                                                                                                                  struct the pairwise commuting matrices to solve the RoPE
    The proof of Theorem 1 is shown in Appendix A.                                                                                                                                                                        Equation parameterized by angle matrices. Note that if
    Theorem 1 together with Proposition 1 demonstrate that                                                                                                                                                                two matrices are both block diagonal with the same block
a function f is a RoPE function parameterized by angle ma-                                                                                                                                                                sizes, where the corresponding blocks are commutative,
trices if and only if the angle matrices A pairwise commute.                                                                                                                                                              then these two matrices are commutative. Formally speak-
In other words, to construct a relative positional encoding                                                                                                                                                               ing, for two matrices M, N ∈ Rd×d , they commute if
method for an attention mechanism that is robust to offset,                                                                                                                                                                        \left \{ \begin {array}{l} \vspace {1ex} \mathbf M = \mathrm {diag}(\mathbf M_1,\mathbf M_2, \cdots ,\mathbf M_b) \\ \vspace {5px} \mathbf N = \mathrm {diag}(\mathbf N_1, \mathbf N_2, \cdots ,\mathbf N_b) \\ \vspace {5px} \mathbf M_i,\;\mathbf N_i \in \mathbb R^{b \times b} \quad \quad \forall i \in \{1, 2, \cdots , m\} \\ \mathbf M_i \mathbf N_i = \mathbf N_i \mathbf M_i \quad \quad \forall i \in \{1, 2, \cdots , m\} \\ \end {array} \right . 
it suffices to establish an angle matrix set that pairwise com-
mutes. Thus, in the following section, we focus on the con-                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                (5)
struction of A to satisfy this particular requirement. We call
our method ComRoPE to indicate the commutativity of an-
gle matrices in RoPE function. The overview of ComRoPE
is shown in Figure 1.                                                                                                                                                                                                     where b denotes block size that is a factor of d and m = db .
Remark 1. Among all the previous RoPE methods, LieRE                                                                                                                                                                          Thus, to present solutions to RoPE Equation parame-
[23] is the most related one to ours which solves Equa-                                                                                                                                                                   terized by angle matrices, it suffices to partition the angle
tion 3 by directly training the skew-symmetric matrix set A.                                                                                                                                                              matrices Ai in Eq. (3) into m blocks Bi1 , Bi2 , · · · , Bim
However, it is worth noting that in the following equation of                                                                                                                                                             where:
                                                                                                                                                                                                                                     \label {eq:definition-of-matrix-b} \left \{ \begin {array}{l} \vspace {1ex} \mathbf A_i = \mathrm {diag}(\mathbf B_{i1}, \mathbf B_{i2}, \cdots , \mathbf B_{im}) \\ \mathbf B_{ij} \in \mathbb R^{b \times b} \quad \quad \forall j \in \{1, 2, \cdots , m\} \\ \end {array} \right . 
their implementation
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           (6)
     \resizebox {.42\textwidth }{!}{$ (\mathbf R(\bm x_q; \mathcal A) \bm q)^\top (\mathbf R(\bm x_k; \mathcal A) \bm k) = \bm q^\top \mathbf R(\bm x_q; \mathcal A)^\top \mathbf R(\bm x_k; \mathcal A) \bm k $},  (4)
    Defining Bj = {B1j , B2j , · · · , BN j }, if Bj pair-
wise commutes for all j ∈ {1, 2, · · · , m}, then A pair-
wise commutes. Note that B1 , B2 , · · · , Bm are equiv-
alent. Thus, without causing confusion, we will uni-
formly use B = {B1 , B2 , · · · , BN } to represent Bj =
{B1j , B2j , · · · , BN j } in the following.
    Currently, there is no general method that provides the
necessary and sufficient conditions for ensuring that arbi-
trary trainable skew-symmetric matrices commute. There-
fore, we aim to establish sufficient conditions for enforcing
this constraint. More concretely, we present Proposition 2
and Proposition 3 to construct two different variants of ma-
trices, respectively.
Proposition 2. A set of matrices B = {B1 , B2 , · · · , BN }
pairwise commutes if all but one of them are zero matrices,
i.e., ∃ k ∈ {1, 2, · · · , N } s.t.
                                                                                                                                                                                                                             Figure 2. Different patch sizes result in different relative relation-
                                                                                                        \mathbf B_i = \mathbf O \; (\forall i \ne k)                                                                   (7)
                                                                                                                                                                                                                             ships.
   Based on Proposition 2, we propose Trainable RoPE
parameterized by Axial-Partition Angle Matrices
(ComRoPE-AP). We divide the m blocks into N parts,                                                                                                                                                                           becomes unreasonable to define their relative relationships
such that each part is responsible for the rotation of a                                                                                                                                                                     solely based on the span of patches. As shown in Figure
specific axis. In this case, m should be a multiple of N .                                                                                                                                                                   2, the same locations with varying patch sizes can result in
Specifically,                                                                                                                                                                                                                significantly different relative relationships.
                                                                                                                                                                                                                                 As a result, applying relative coordinates is a better
                                                                                                                                                                                                                             method for measuring relative relationships in images. For
     \label {eq:rope-ap} \mathbf B_{ij} = \left \{ \begin {array}{ll} \mathbf P_j - \mathbf P_j^\top , & \quad \text {if} \quad j\equiv i \pmod N \\ \mathbf O, & \quad \text {otherwise} \\ \end {array} \right .     (8)
                                                                                                                                                                                                                             an image with shape H × W , we scale both the height H
                                                                                                                                                                                                                             and the width W to 1. Therefore, for a pixel located at
where {Pj }m j=1 represents a set of trainable matrices, O                                                                                                                                                                                                                           h w
                                                                                                                                                                                                                             (h, w) in the raw image, its coordinate is treated as ( H  , W ).
denotes a zero matrix, and ≡ indicates congruence modulo.                                                                                                                                                                    For high-dimensional coordinates, we perform the same op-
Proposition 3. A set of matrices B = {B1 , B2 , · · · , BN }                                                                                                                                                                 eration, i.e., transform a raw coordinate (x1 , x2 , · · · , xN )
pairwise commutes if they pairwise linearly dependent, i.e.,                                                                                                                                                                 in multi-axial canvas with shape (X1 , X2 , · · · , XN ) into
                                                                                                                                                                                                                               x1 x2          xN
∃ λ1 , λ2 , · · · , λN ∈ R s.t.                                                                                                                                                                                              (X   , ,··· , X
                                                                                                                                                                                                                                1 X2           N
                                                                                                                                                                                                                                                 ).

                                                                \lambda _1\mathbf B_1 = \lambda _2 \mathbf B_2 = \cdots = \lambda _N\mathbf B_N                                                                        (9)   Center offset. When we project a patch to a feature tensor
                                                                                                                                                                                                                             with a coordinate, we aggregate all information of the patch
   Based on Proposition 3, we propose Trainable RoPE                                                                                                                                                                         into a specified location. Simply, we adopt the center point
parameterized by Linearly-Dependent Angle Matrices                                                                                                                                                                           of a patch as the aggregation location.
(ComRoPE-LD). Specifically, We train a base matrix P
with scaling factors {θi }N
                          i=1 . Then we obtain:                                                                                                                                                                              3.4.2         Position perturbation
              \mathcal B = \{ \mathbf B_i = \theta _i (\mathbf P -\mathbf P^\top )\; |\; i = 1, 2, \cdots , N \}                                                                                                      (10)   To achieve better robustness and excellent performance
                                                                                                                                                                                                                             across different scales during inference, we add per-
3.4. Implementation details of coordinates and im-                                                                                                                                                                           turbations to the coordinates of the patches. Specif-
      provements                                                                                                                                                                                                             ically, for a patch with center (x1 , · · · , xN ) and size
                                                                                                                                                                                                                             (∆X1 , · · · , ∆XN ), during training, we formulate its loca-
3.4.1                    Relative scaling and center offset
                                                                                                                                                                                                                             tion as:
Relative scaling. In language models, positions are typ-
ically treated as discrete. Additionally, due to the rela-                                                                                                                                                                       \mathcal N \left ( \left ( x_1, \cdots , x_N \right )^\top ; \text {diag}(\sigma \Delta X_1 , \cdots , \sigma \Delta X_N )^2 \right )  (11)
tionship between tokens and the inherent uncertainty in se-
quence length, there is no need to scale these positions into                                                                                                                                                                where σ is a hyper-parameter called perturbation intensity.
a specified range. However, in the case of images, posi-                                                                                                                                                                     Additionally, we truncate the location within the patch area,
                                                                                                                                                                                                                             i.e., xk − ∆X          ∆Xk
                                                                                                                                                                                                                                  
tions are continuous. When using different patch sizes, it                                                                                                                                                                                2 , xk + 2       .
                                                                                                                                                                                                                                            k
     Position Encoding         Perturbation                                                Evaluation Resolution
          Method                 Intensity         112       128      192                   224     256    320                    384     448      512
                                      1           30.04     38.69    56.4                  58.76    60.02        59.27        57.04       54.10   50.99
             APE
                                      0           19.71     33.43    55.97                 58.62    55.31        52.63        49.68       46.39   42.66
                                      1           36.94     45.48    60.72                 63.09    63.24        62.12        59.24       55.51   51.11
        Vanilla RoPE
                                      0           36.41     44.48    59.97                 62.03    62.54        61.36        58.56       54.79   50.58
                                      1           38.03     46.97    62.22                 64.36    64.99        63.78        61.15       57.92   53.74
            LieRE
                                      0           38.22     46.85    62.01                 64.07    64.54        63.46        60.74       56.89   52.51
                                      1           35.75     46.18    62.82                 65.32    65.83        64.78        61.88       58.21   54.02
   ComRoPE-AP (ours)
                                      0           37.17     46.95    62.63                 64.76    65.11        64.35        61.62       58.10   53.81
                                      1           38.30     47.28    63.53                 65.49    65.95        65.27        62.62       59.11   55.29
   ComRoPE-LD (ours)
                                      0           36.88     46.54    62.89                 65.27    65.66        64.27        61.83       57.87   53.64
Table 1. Accuracy of 2D classification on ImageNet. Models are trained at a resolution of 224 × 224 and evaluated at varying resolutions.


4. Experiments                                                         various resolutions. Since APE is inherently fixed and dis-
                                                                       crete, bilinear interpolation is applied to adapt it to different
In this section, we evaluate the performance of various po-
                                                                       resolutions during evaluation. Optimization methods and
sitional encoding methods on classic vision tasks. We first            hyper-parameters are detailed in Appendix C.
assess their scalability in 2D image classification across
different resolutions. Additionally, we conduct object de-
                                                                                                                                                  PE Type
                                                                       Accuracy (%)

tection experiments to demonstrate the generalizability of                            60                                                          APE
our approach. To further examine the ability to handle                                50                                                          Vanilla RoPE
                                                                                                                                                  LieRE
higher-dimensional data, we perform 3D classification ex-                             40                                                          ComRoPE-AP
periments, which are detailed in Appendix B.                                                                   Resolution                         ComRoPE-LD
                                                                                           128   192 224 256      320       384     448    512
4.1. 2D classification                                                                                  (a) Perturbation intensity = 0

4.1.1   Setup                                                                                                                                     PE Type
                                                                       Accuracy (%)




                                                                                      60                                                          APE
Baselines and model architecture. We evaluate our pro-                                                                                            Vanilla RoPE
                                                                                      50                                                          LieRE
posed methods (ComRoPE-LD and ComRoPE-AP) against                                                                                                 ComRoPE-AP
                                                                                      40                       Resolution                         ComRoPE-LD
APE, vanilla RoPE (as introduced by RoFormer), and
LieRE. To isolate the effects of positional encoding, we uti-                              128   192 224 256      320       384     448    512
                                                                                                        (b) Perturbation intensity = 1
lize a standard Vision Transformer (ViT-B/16) architecture
with minimal modifications. The APE codebook is removed                Figure 3. Accuracy on ImageNet-1K for various positional encod-
for methods that do not employ APE, and self-attention lay-            ing methods. The results for the same perturbation intensity are
ers are replaced with RoPE self-attention parameterized by             presented together for better comparison. For better visualization,
angle matrices. This design highlights the performance dif-            evaluation resolution at 112 × 112 is not included in these figures.
ferences among various positional encoding methods. A
block size of 8 is used in practice. More details are pro-
vided in Appendix D.                                                   4.1.2                Main results
Training and evaluation protocol. All models are trained
at a standard resolution of 224 × 224 and evaluated across             The accuracy metrics for each positional encoding method
multiple resolutions to test their robustness and scalability          across various resolutions are summarized in Table 1. Ad-
on the ImageNet-1K dataset [5]. The models are trained                 ditionally, Figure 3 visually compares performance under
from scratch using randomly initialized parameters, ensur-             different levels of perturbation intensity separately.
ing no influence from pre-trained weights or external pri-             Overall performance. Across all evaluations, APE consis-
ors. To maintain fairness and reproducibility, we apply only           tently exhibits the lowest accuracy, corroborating previous
basic data augmentation techniques, such as resizing and               findings regarding its limitations in dynamic contexts. The
random cropping, focusing on relative performance com-                 vanilla RoPE shows a modest improvement over APE but
parisons rather than achieving absolute accuracy. The pri-             remains less effective. In contrast, the trainable angle ma-
mary evaluation metric is accuracy on the test set across              trices, namely LieRE, ComRoPE-AP, and ComRoPE-LD,
demonstrate significantly higher accuracy across all reso-                      PE Method            AP                   AP50                           AP75                          APs    APm     APl
lutions. Notably, ComRoPE-LD achieves the best perfor-                         APE                  44.0                    66.6                           47.7                        28.2   46.8    58.4
mance among the three, suggesting that its inherent linear                    LieRE                 44.5                    67.3                           48.4                        29.0   46.9    58.7
dependencies may enhance flexibility and structural learn-                 ComRoPE-LD               44.7                    67.6                           48.5                        29.2   47.1    60.0
ing capabilities.
                                                                                       Table 2. Results of object detection on MS COCO.
Accuracy at training resolution. At the training resolution
of 224×224, all three methods with trainable angle matrices                     0.45
(ComRoPE-LD, ComRoPE-AP, and LieRE) achieve com-
                                                                                0.40
parable accuracy, significantly outperforming both APE and
                                                                                0.35
the standard RoPE, which underscores the effectiveness of                                   29% Less Iterations




                                                                          mAP
RoPE parameterized by trainable angle matrices. Notably,                        0.30
                                                                                                                                                                                         ComRoPE-LD
ComRoPE-LD surpasses the current state-of-the-art LieRE                         0.25                                                                                                     LieRE
by 1.6%2 .                                                                                                                            Epoch                                              APE
                                                                                0.20
Scaling to higher resolution. At resolutions beyond the                                 0     5      10                    15            20                                 25             30   35
training size, LieRE shows the steepest decline in accuracy                        Figure 4. Results over the whole training procedure.
among the three trainable RoPE variants, indicating greater
sensitivity to resolution changes. In contrast, ComRoPE-                  4.3. Ablation study
LD and ComRoPE-AP exhibit a more gradual decrease in                      4.3.1        Impact of commutativity
performance, thanks to their commutative properties that
enhance positional robustness. Specifically, ComRoPE-LD                   To evaluate the significance of the commutativity of angle
outperforms LieRE by 2.9% at a resolution of 512 × 512.                   matrices, we conduct experiments on the LieRE by intro-
Significance of commutativity. These findings illustrate                  ducing a coordinate offset. Specifically, before inference,
the effectiveness of trainable commutative angle matrices,                a random offset is applied uniformly across all coordinates
particularly ComRoPE-LD, in maintaining accuracy and                      within the image. The offset is sampled from a Gaussian
scalability across diverse resolutions. The results under-                distribution as follows:
score the importance of commutativity in ensuring robust
                                                                                                                                                                                                       (12)
RoPE parameterized by trainable angle matrices for vision
                                                                                                         \mathcal {N} \left ( 0; \; \rho ^2 \cdot \mathbf {I}_{N \times N} \right ) 
tasks. Furthermore, to better understand the role of commu-               where ρ represents the standard deviation of the random off-
tativity, we conduct additional experiments by introducing                set, and IN ×N is the identity matrix of size N × N . It is
coordinate offsets in LieRE (see Section 4.3.1 for details).              important to note that applying the same offset to all coor-
                                                                          dinates does not influence the relative positional dependen-
4.2. Object detection                                                     cies between the patches. The other experimental settings
                                                                          remain consistent with those in the 2D classification tasks.
To demonstrate the generalizability of our approach, we                       The results shown in Figure 5 demonstrate that the base-
conduct object detection experiments using the framework                  line model’s performance deteriorates significantly as the
from [37]. We adopt ViT-S as our backbone and apply Com-                  standard deviation of the offset increases. In contrast, our
RoPE to the attention layers. To ensure consistency with the              proposed model, ComRoPE, maintains consistent perfor-
pre-trained model, we initialize the angle matrix to zero.                mance across all offset values. This is due to the commuta-
   We evaluate ComRoPE-LD, LieRE, and APE on the                          tivity of the angle matrices, which allows the model to re-
MS COCO dataset [17]. As summarized in Table 2,                           main invariant to such coordinate shifts. The robustness of
both ComRoPE-LD and LieRE outperform APE, with                            ComRoPE to this type of perturbation highlights its capac-
ComRoPE-LD achieving slightly better performance than                     ity to preserve relative positional information, even in the
LieRE while only using nearly half the number of extra pa-                presence of modification introduced by coordinate offsets.
rameters.
   To compare training efficiency, we plot the results for                4.3.2        Impact of block size
each epoch in Figure 4. Our findings indicate that both
ComRoPE-LD and LieRE converge faster than APE, requir-                    In this section, we examine the impact of block size on
ing 29% fewer iterations to achieve the same results as APE.              2D classification using the ImageNet dataset. We maintain
                                                                          the same experimental setup as in our primary experiments,
   2 Relative improvement is calculated as target
                                                                          varying the block size from 2 to 8. The results are presented
                                                  = baseline × (1 +
improvement), where the improvement denotes the percentage increase
                                                                          in Figure 6. Our findings indicate that larger block sizes
over the baseline performance. This formula applies throughout the fol-   consistently improve performance by extending the rotation
lowing sections.                                                          transformation space to a more significant subgroup of the
                                                                                                        offset std                  60




                                                                                                                     Accuracy (%)
                60                                                                                            0.2                   55
                50                                                                                            0.5
Accuracy (%)


                                                                                                              1.0                   50
                40                                                                                                                         Position Perturbation Intensity
                                                                                                              2.0                   45      0.0       0.2        0.5         1.0
                30                                                                                            5.0                                                                        Resolution
                                                                                                                                         192 224 256             320               384          448   512
                20
                                                                                                                                                                  (a) APE
                10
                      100         150     200         250     300     350        400     450   500     Resolution                   65




                                                                                                                     Accuracy (%)
Figure 5. Effect of coordinate offset on LieRE. As the standard                                                                     60
deviation of the offset increases, the performance of the base-                                                                            Position Perturbation Intensity
                                                                                                                                    55      0.0       0.2        0.5         1.0         Resolution
line model deteriorates, while ComRoPE remains unaffected (un-
painted).                                                                                                                                192 224 256             320               384          448   512
                                                                                                                                                            (b) ComRoPE-LD
                     Block Size                                                                                      Figure 7. Accuracy on ImageNet over varying position perturba-
Accuracy (%)




               60           8
                            4                                                                                        tion intensity.
                            2
               50
                                                                    Resolution                                       the improvement was relatively modest. This is likely due
                      128               192     224     256            320             384       448          512
                                                              (a) LieRE                                              to the inherent robustness of the RoPE design with angle
                                                                                                                     matrices, which is already well-equipped to handle varia-
                     Block Size
                                                                                                                     tions in position.
Accuracy (%)




               60           8
                            4
                            2
               50                                                                                                    4.4. Applications
                                                                    Resolution
                      128               192     224     256            320             384       448          512
                                                                                                                     In our approach, when the angle matrix is an all-zero matrix,
                                                        (b) ComRoPE-AP
                                                                                                                     the rotation matrix becomes the identity matrix, causing
                     Block Size                                                                                      RoPE Attention to reduce to the standard Attention mech-
Accuracy (%)




               60           8
                            4
                            2                                                                                        anism. When the block size of the angle matrix is set to
               50                                                                                                    2, ComRoPE-AP effectively reduces to the commonly used
                                                                    Resolution
                      128               192     224     256            320             384       448          512    RoPE Attention in language models. This demonstrates
                                                        (c) ComRoPE-LD                                               that our method can represent standard Attention and var-
Figure 6. Accuracy on ImageNet for various block sizes. Larger                                                       ious common variants of RoPE Attention. Therefore, dur-
block size results in better performance.                                                                            ing the fine-tuning stage, we can replace standard Attention
                                                                                                                     with our method, load the pre-trained weights, and fine-tune
particular orthogonal group by introducing additional pa-                                                            them under the new paradigm. In other words, ComRoPE
rameters and computation time. When the block size is                                                                can be seamlessly integrated into the fine-tuning process,
small, the associated costs are minimal. However, as the                                                             even if it was not applied during pre-training. Additional
block size increases, the primary term of time complexity                                                            experiments can be found in Appendix B.
grows to O(Lndb^2) from O(Lnd\cdot \frac {d}{h}), which becomes sig-
nificant. Therefore, we limit the block size to a maximum                                                            5. Conclusion
of 8 to balance performance with additional costs. In other
words, we select a block size of 8 to optimize performance                                                           In this work, we proposed ComRoPE, a novel and adap-
while keeping the extra computational cost manageable.                                                               tive framework for Rotary Position Embedding (RoPE) pa-
                                                                                                                     rameterized by trainable angle matrices. We rigorously for-
                                                                                                                     mulate the RoPE Equation and then establish a necessary
4.3.3                    Utility of position perturbation
                                                                                                                     and sufficient condition for its solution. Our approach ef-
In this section, we explore the impact of positional pertur-                                                         fectively overcomes the scalability and robustness limita-
bations. We conducted experiments on ComRoPE-LD and                                                                  tions of existing RoPE methods by eliminating the need
APE using the ImageNet dataset, with the results presented                                                           for manually designed rotation matrices and introducing a
in Figure 7.                                                                                                         more flexible, scalable solution. Extensive experimental re-
   As shown in Figure 7a, APE is highly sensitive to posi-                                                           sults show that ComRoPE outperforms the existing posi-
tional perturbations, leading to significant performance im-                                                         tional encoding methods across various tasks. Furthermore,
provement (+19.5% when increasing intensity from 0 to 1)                                                             our framework generalizes existing RoPE formulations and
when these perturbations are introduced. For RoPE with                                                               demonstrates the potential for broader application in Trans-
angle matrices shown in Figure 7b, positional perturbations                                                          former models, offering insights and a solid foundation for
also resulted in some performance gains (+2.9%), though                                                              future research in positional encoding techniques.
Acknowledgment                                                       [14] Amirhossein Kazemnejad, Inkit Padhi, Karthikeyan Nate-
                                                                          san Ramamurthy, Payel Das, and Siva Reddy. The impact
This work is supported by the National Key R&D Pro-                       of positional encoding on length generalization in transform-
gram of China (2022YFB4701400/4701402), SSTIC Grant                       ers. Advances in Neural Information Processing Systems, 36:
(KJZD20230923115106012, KJZD20230923114916032,                            24892–24928, 2023. 1
GJHZ20240218113604008) and Beijing Key Lab of Net-                   [15] Hanyu Lai, Xiao Liu, Iat Long Iong, Shuntian Yao, Yux-
worked Multimedia.                                                        uan Chen, Pengbo Shen, Hao Yu, Hanchen Zhang, Xiaohan
                                                                          Zhang, Yuxiao Dong, et al. Autowebglm: A large language
References                                                                model-based web navigating agent. In Proceedings of the
                                                                          30th ACM SIGKDD Conference on Knowledge Discovery
 [1] Philipp Bader, Sergio Blanes, and Fernando Casas. Comput-            and Data Mining, pages 5295–5306, 2024. 1
     ing the matrix exponential with an optimized taylor polyno-     [16] Hongyu Li, Jinyu Chen, Ziyu Wei, Shaofei Huang, Tianrui
     mial approximation. Mathematics, 7:1174, 2019. 4                     Hui, Jialin Gao, Xiaoming Wei, and Si Liu. Llava-st: A
 [2] Tom B Brown. Language models are few-shot learners.                  multimodal large language model for fine-grained spatial-
     arXiv preprint arXiv:2005.14165, 2020. 1                             temporal understanding. arXiv preprint arXiv:2501.08282,
 [3] Yirui Chen, Xudong Huang, Quan Zhang, Wei Li, Mingjian               2025. 1
     Zhu, Qiangyu Yan, Simiao Li, Hanting Chen, Hailin Hu, Jie       [17] Tsung-Yi Lin, Michael Maire, Serge Belongie, Lubomir
     Yang, et al. Gim: A million-scale benchmark for genera-              Bourdev, Ross Girshick, James Hays, Pietro Perona, Deva
     tive image manipulation detection and localization. arXiv            Ramanan, C. Lawrence Zitnick, and Piotr Dollár. Microsoft
     preprint arXiv:2406.16531, 2024. 1                                   coco: Common objects in context. arXiv preprint arXiv:
 [4] Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao             1405.0312, 2014. 7
     Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao            [18] Xiao Liu, Hanyu Lai, Hao Yu, Yifan Xu, Aohan Zeng,
     Zhuang, Joseph E Gonzalez, et al. Vicuna: An open-source             Zhengxiao Du, Peng Zhang, Yuxiao Dong, and Jie Tang. We-
     chatbot impressing gpt-4 with 90%* chatgpt quality, march            bglm: towards an efficient web-enhanced question answer-
     2023. URL https://lmsys. org/blog/2023-03-30-vicuna, 3(5),           ing system with human preferences. In Proceedings of the
     2023. 2                                                              29th ACM SIGKDD conference on knowledge discovery and
 [5] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, K. Li, and            data mining, pages 4549–4560, 2023. 1
     Li Fei-Fei. Imagenet: A large-scale hierarchical image          [19] Xiao Liu, Hao Yu, Hanchen Zhang, Yifan Xu, Xuanyu Lei,
     database. 2009 IEEE Conference on Computer Vision and                Hanyu Lai, Yu Gu, Hangliang Ding, Kaiwen Men, Kejuan
     Pattern Recognition, pages 248–255, 2009. 6                          Yang, et al. Agentbench: Evaluating llms as agents. In The
 [6] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina               Twelfth International Conference on Learning Representa-
     Toutanova. Bert: Pre-training of deep bidirectional trans-           tions, 2024. 1
     formers for language understanding. In North American           [20] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng
     Chapter of the Association for Computational Linguistics,            Zhang, Stephen Lin, and Baining Guo. Swin transformer:
     2019. 1, 2                                                           Hierarchical vision transformer using shifted windows. In
 [7] Alexey Dosovitskiy. An image is worth 16x16 words:                   Proceedings of the IEEE/CVF international conference on
     Transformers for image recognition at scale. arXiv preprint          computer vision, pages 10012–10022, 2021. 2
     arXiv:2010.11929, 2020. 1                                       [21] Antoine Liutkus, Ondřej Cıfka, Shih-Lun Wu, Umut Sim-
 [8] Yuxin Fang, Quan Sun, Xinggang Wang, Tiejun Huang, Xin-              sekli, Yi-Hsuan Yang, and Gael Richard. Relative positional
     long Wang, and Yue Cao. Eva-02: A visual representation              encoding for transformers with linear complexity. In Interna-
     for neon genesis. Image and Vision Computing, 149:105171,            tional Conference on Machine Learning, pages 7067–7079.
     2024. 2                                                              PMLR, 2021. 2
 [9] Jean H. Gallier. Basics of classical lie groups: The exponen-   [22] Jiasen Lu, Christopher Clark, Sangho Lee, Zichen Zhang,
     tial map, lie groups, and lie algebras. 2001. 3                      Savya Khosla, Ryan Marten, Derek Hoiem, and Aniruddha
[10] Team GLM, Aohan Zeng, Bin Xu, Bowen Wang, Chenhui                    Kembhavi. Unified-io 2: Scaling autoregressive multimodal
     Zhang, Da Yin, Dan Zhang, Diego Rojas, Guanyu Feng,                  models with vision language audio and action. In Proceed-
     Hanlin Zhao, et al. Chatglm: A family of large language              ings of the IEEE/CVF Conference on Computer Vision and
     models from glm-130b to glm-4 all tools. arXiv preprint              Pattern Recognition, pages 26439–26455, 2024. 2
     arXiv:2406.12793, 2024. 1                                       [23] Sophie Ostmeier, Brian Axelrod, Michael E Moseley, Ak-
[11] Larry C. Grove. Classical groups and geometric algebra.              shay Chaudhari, and Curtis Langlotz. Liere: Generalizing
     2001. 2                                                              rotary position encodings. arXiv preprint arXiv:2406.10322,
[12] Byeongho Heo, Song Park, Dongyoon Han, and Sangdoo                   2024. 2, 4
     Yun. Rotary position embedding for vision transformer.          [24] Ofir Press, Noah A Smith, and Mike Lewis. Train short,
     arXiv preprint arXiv:2403.13298, 2024. 2                             test long: Attention with linear biases enables input length
[13] Max Horn, Kumar Shridhar, Elrich Groenewald, and                     extrapolation. arXiv preprint arXiv:2108.12409, 2021. 2
     Philipp FM Baumann. Translational equivariance in kernel-       [25] A Radford. Improving language understanding by generative
     izable attention. arXiv preprint arXiv:2102.07680, 2021. 2           pre-training. 2018. 1
[26] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario        [40] Quan Zhang, Yuxin Qi, Xi Tang, Jinwei Fang, Xi Lin, Ke
     Amodei, Ilya Sutskever, et al. Language models are unsu-             Zhang, and Chun Yuan. IMDPrompter: Adapting SAM
     pervised multitask learners. OpenAI blog, 1(8):9, 2019. 1            to image manipulation detection by cross-view automated
[27] Alec Radford, Jong Wook Kim, Chris Hallacy, A. Ramesh,               prompt learning. In The Thirteenth International Conference
     Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda                 on Learning Representations, 2025.
     Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and       [41] Quan Zhang, Yuxin Qi, Xi Tang, Rui Yuan, Xi Lin, Ke
     I. Sutskever. Learning transferable visual models from nat-          Zhang, and Chun Yuan. Rethinking pseudo-label guided
     ural language supervision. International Conference on Ma-           learning for weakly supervised temporal action localization
     chine Learning, 2021. 3                                              from the perspective of noise correction. arXiv preprint
[28] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee,             arXiv:2501.11124, 2025. 1
     Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and
     Peter J Liu. Exploring the limits of transfer learning with a
     unified text-to-text transformer. Journal of machine learning
     research, 21(140):1–67, 2020. 2
[29] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee,
     Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and
     Peter J Liu. Exploring the limits of transfer learning with a
     unified text-to-text transformer. Journal of machine learning
     research, 21(140):1–67, 2020. 2
[30] Peter Shaw, Jakob Uszkoreit, and Ashish Vaswani. Self-
     attention with relative position representations.       arXiv
     preprint arXiv:1803.02155, 2018. 2
[31] Khurram Soomro, Amir Roshan Zamir, and Mubarak Shah.
     Ucf101: A dataset of 101 human actions classes from videos
     in the wild. arXiv preprint arXiv: 1212.0402, 2012. 3
[32] Jianlin Su, Murtadha Ahmed, Yu Lu, Shengfeng Pan, Wen
     Bo, and Yunfeng Liu. Roformer: Enhanced transformer with
     rotary position embedding. Neurocomputing, 568:127063,
     2024. 1, 2, 3
[33] Sainbayar Sukhbaatar, Jason Weston, Rob Fergus, et al. End-
     to-end memory networks. Advances in neural information
     processing systems, 28, 2015. 2
[34] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier
     Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste
     Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, et al.
     Llama: Open and efficient foundation language models.
     arXiv preprint arXiv:2302.13971, 2023. 2
[35] A Vaswani. Attention is all you need. Advances in Neural
     Information Processing Systems, 2017. 1, 2
[36] Kan Wu, Houwen Peng, Minghao Chen, Jianlong Fu, and
     Hongyang Chao. Rethinking and improving relative posi-
     tion encoding for vision transformer. In Proceedings of the
     IEEE/CVF International Conference on Computer Vision,
     pages 10033–10041, 2021. 2
[37] Chunlong Xia, Xinliang Wang, Feng Lv, Xin Hao, and
     Yifeng Shi. Vit-comer: Vision transformer with convolu-
     tional multi-scale feature interaction for dense predictions.
     Computer Vision and Pattern Recognition, 2024. 7
[38] Quan Zhang and Yuxin Qi. Can mllms guide weakly-
     supervised temporal action localization tasks?          arXiv
     preprint arXiv:2411.08466, 2024. 1
[39] Quan Zhang, Xiaoyu Liu, Wei Li, Hanting Chen, Jun-
     chao Liu, Jie Hu, Zhiwei Xiong, Chun Yuan, and Yunhe
     Wang. Distilling semantic priors from sam to efficient image
     restoration models. In Proceedings of the IEEE/CVF Con-
     ference on Computer Vision and Pattern Recognition, pages
     25409–25419, 2024.
       ComRoPE: Scalable and Robust Rotary Position Embedding Parameterized by
                        Trainable Commuting Angle Matrices
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           Supplementary Material
A. Theorems and proofs                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  above. Thus, we obtain:

A.1. Proof of the main theorem
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   \begin {split} f(t) & = \frac {e^{\mathbf {A}t} e^{\mathbf {B}t} - e^{(\mathbf {A} + \mathbf {B})t}}{t^2} \\ & = \frac {\mathbf {A}^2 + 2\mathbf {A}\mathbf {B} + \mathbf {B}^2}{4} - \frac {(\mathbf {A} + \mathbf {B})^2}{4} + o(1) \\ & = \frac {\mathbf {A}\mathbf {B} - \mathbf {B}\mathbf {A}}{4} + o(1). \end {split} 
To prove our main theorem (i.e., Theorem 1), we first pro-
pose some lemmas and prove them.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        (17)

Lemma 1. Matrices A, B ∈ Rn×n commute if and only if
eAx eBy = eAx+By for all x, y ∈ R.
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        Taking the limit as t → 0, we have:
Proof.
1) Necessity (⇒). By the definition of eA , we have:
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                \lim _{t \rightarrow 0} f(t) = \frac {\mathbf {A}\mathbf {B} - \mathbf {B}\mathbf {A}}{4}.                                                                                                                                              (18)

                                                                       \begin {split} e^{\mathbf {A} + \mathbf {B}} & = \sum _{n=0}^{\infty } \frac {(\mathbf {A} + \mathbf {B})^n}{n!} \\ & = \sum _{n=0}^{\infty } \frac {\sum _{k=0}^{n} \binom {n}{k} \mathbf {A}^k \mathbf {B}^{n-k}}{n!} \\ & = \sum _{n=0}^{\infty } \sum _{k=0}^{n} \frac {\mathbf {A}^k \mathbf {B}^{n-k}}{k!(n-k)!} \\ & = \left (\sum _{k=0}^{\infty } \frac {\mathbf {A}^k}{k!}\right ) \left (\sum _{m=0}^{\infty } \frac {\mathbf {B}^m}{m!}\right ) \\ & = e^{\mathbf {A}} e^{\mathbf {B}}. \end {split}                 Since eAx eBy = eAx+By , we have f (t) = 0, which im-
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        plies AB = BA.
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           ■

                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        Lemma 2. Matrices A1 , A2 , . . . , Am ∈ Rn×n (m > 1)
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 (13)   pairwise commute if and only if:

                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              e^{\mathbf {A}_1 x_1} e^{\mathbf {A}_2 x_2} \cdots e^{\mathbf {A}_m x_m} = e^{\mathbf {A}_1 x_1 + \mathbf {A}_2 x_2 + \cdots + \mathbf {A}_m x_m}                                                                                                                                                                                                                                                         (19)

                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        for all x1 , x2 , . . . , xm ∈ R.

                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        Proof. For m = 2, the theorem holds by Lemma 1. Sup-
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        pose the theorem holds for all 2 ≤ m ≤ k. We prove it for
Substituting A, B with Ax, By, we obtain:                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               m = k + 1.
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        1) Necessity (⇒). Assuming:
                                                                                                                                                                                                                                                                                                           e^{\mathbf {A}x + \mathbf {B}y} = e^{\mathbf {A}x} e^{\mathbf {B}y}.                                                                                                                                                                                                                  (14)
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  e^{\mathbf {A}_1 x_1} e^{\mathbf {A}_2 x_2} \cdots e^{\mathbf {A}_k x_k} = e^{\mathbf {A}_1 x_1 + \mathbf {A}_2 x_2 + \cdots + \mathbf {A}_k x_k},                                                                                                                                                                                                                                                    (20)
2) Sufficiency (⇐). We have:
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        we split A1 x1 + A2 x2 + · · · + Ak+1 xk+1 into two parts:

  \begin {split} e^{\mathbf {A}t} e^{\mathbf {B}t} & = \left (\sum _{n=0}^{\infty } \frac {t^n \mathbf {A}^n}{n!}\right ) \left (\sum _{m=0}^{\infty } \frac {t^m \mathbf {B}^m}{m!}\right ) \\ & = \mathbf {I} + t(\mathbf {A} + \mathbf {B}) + t^2 \cdot \frac {\mathbf {A}^2 + 2\mathbf {A}\mathbf {B} + \mathbf {B}^2}{4} + o(t^2), \end {split}                                                                                                                                                                                                                                                       \begin {split} & \mathbf {A}_1 x_1 + \mathbf {A}_2 x_2 + \cdots + \mathbf {A}_{k+1} x_{k+1} \\ & = (\mathbf {A}_1 x_1 + \mathbf {A}_2 x_2 + \cdots + \mathbf {A}_k x_k) + (\mathbf {A}_{k+1} x_{k+1}). \end {split} 
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        (21)

                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        Since A1 , A2 , . . . , Ak+1 commute in pairs, A1 x1 +
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 (15)   A2 x2 + · · · + Ak xk and Ak+1 xk+1 also commute. Thus:

and                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 \begin {split} &e^{\mathbf {A}_1 x_1 + \mathbf {A}_2 x_2 + \cdots + \mathbf {A}_{k+1} x_{k+1}} \\ &= e^{(\mathbf {A}_1 x_1 + \mathbf {A}_2 x_2 + \cdots + \mathbf {A}_k x_k)} e^{\mathbf {A}_{k+1} x_{k+1}} \\ &= e^{\mathbf {A}_1 x_1} e^{\mathbf {A}_2 x_2} \cdots e^{\mathbf {A}_{k+1} x_{k+1}}. \end {split} 
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        (22)
                    \begin {split} e^{(\mathbf {A}+\mathbf {B})t} & = \sum _{n=0}^{\infty } \frac {((\mathbf {A} + \mathbf {B})t)^n}{n!} \\ & = \mathbf {I} + t(\mathbf {A} + \mathbf {B}) + t^2 \cdot \frac {(\mathbf {A} + \mathbf {B})^2}{4} + o(t^2). \end {split} 

                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        2) Sufficiency (⇐). Let xk+1 = 0. Then:

                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 (16)             e^{\mathbf {A}_1 x_1} e^{\mathbf {A}_2 x_2} \cdots e^{\mathbf {A}_k x_k} = e^{\mathbf {A}_1 x_1 + \mathbf {A}_2 x_2 + \cdots + \mathbf {A}_k x_k},                                                                                                                                                                                                                                                    (23)

Let t2 f (t) be the difference between the two expressions                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              implying that A1 , A2 , . . . , Ak commute in pairs.
                 Position Encoding                                                                                                                                                                                      Perturbation                                                                                                            Evaluation Resolution
                      Method                                                                                                                                                                                              Intensity                                                                      112     128     192                     224     256    320                                                                                                                                                                        384                       448                  512
                                                                                                                                                                                                                                                                                1                        48.10   55.25   76.50                    93.10                                             76.70                                                           71.48                                                           74.36                            62.23           53.18
                                                                                 APE
                                                                                                                                                                                                                                                                                0                        45.54   55.18   76.48                    92.87                                             75.91                                                           70.70                                                           73.70                            60.43           48.79
                                                                                                                                                                                                                                                                                1                        47.28   54.96   75.69                    93.79                                             77.66                                                           72.53                                                           74.72                            65.19           57.34
                                       Vanilla RoPE
                                                                                                                                                                                                                                                                                0                        48.12   55.21   76.47                    94.12                                             76.72                                                           71.59                                                           74.47                            62.28           53.99
                                                                                                                                                                                                                                                                                1                        48.97   56.15   77.33                    94.43                                             78.35                                                           73.20                                                           77.33                            65.74           58.23
                                                                        LieRE
                                                                                                                                                                                                                                                                                0                        48.75   55.46   78.16                    94.24                                             78.91                                                           72.92                                                           76.86                            65.35           56.85
                                                                                                                                                                                                                                                                                1                        50.14   55.63   77.47                    94.37                                             79.27                                                           73.56                                                           76.66                            67.68           59.34
                                  ComRoPE-AP
                                                                                                                                                                                                                                                                                0                        48.06   55.63   75.72                    94.26                                             75.75                                                           70.93                                                           74.72                            64.91           57.98
                                                                                                                                                                                                                                                                                1                        49.89   56.60   79.21                    94.24                                             80.27                                                           74.22                                                           78.60                            67.46           60.39
                                 ComRoPE-LD
                                                                                                                                                                                                                                                                                0                        48.70   56.46   78.30                    94.48                                             79.27                                                           74.58                                                           76.86                            66.02           57.68

Table 3. Accuracy of 3D classification on UCF-101. Models are trained at a resolution of 224 × 224 and evaluated at varying resolutions.


   For any p ∈ {1, 2, . . . , k}, set all xi = 0 except for xp                                                                                                                                                                                                                                                             For any i, j ∈ {1, 2, . . . , m}, set xk = 0 for all k ̸= i and
and xk+1 . This yields:                                                                                                                                                                                                                                                                                                    yk = 0 for all k ̸= j. This leads to:

                                                                     e^{\mathbf {A}_p x_p} e^{\mathbf {A}_{k+1} x_{k+1}} = e^{\mathbf {A}_p x_p + \mathbf {A}_{k+1} x_{k+1}},                                                                                                                                     (24)                                                                                              e^{\mathbf {A}_i x_i} e^{\mathbf {A}_j y_j} = e^{\mathbf {A}_i x_i + \mathbf {A}_j y_j}.                                                                                                       (29)

which implies Ap and Ak+1 commute.                                                                                                                                                                                                                                                                               Thus,     By Lemma 1, this implies that Ai and Aj commute. There-
A1 , A2 , . . . , Ak+1 commute in pairs.                                                                                                                                                                                                                                                                                   fore, matrices A1 , A2 , . . . , Am ∈ Rn×n (m > 1) pairwise
                                                                                                                                                                                                                                                                                                                    ■      commute.
                                                                                                                                                                                                                                                                                                                                                                                    ■
Lemma 3. Matrices A1 , A2 , . . . , Am ∈ Rn×n (m > 1)
pairwise commute if and only if there exists a function f :                                                                                                                                                                                                                                                                Proof of Theorem 1. Recall that eA is an orthogonal ma-
Rm → Rn×n such that:                                                                                                                                                                                                                                                                                                       trix if A is skew-symmetric, which implies R(x; A)⊤ =
                                                                                                                                                                                                                                                                                                                           R(x; A)−1 = R(−x; A). Thus, we have:
     \label {eq:f_eqs_exey} \begin {split} & f(x_1 + y_1, x_2 + y_2, \ldots , x_m + y_m) \\ & = e^{\mathbf {A}_1 x_1 + \mathbf {A}_2 x_2 + \cdots + \mathbf {A}_m x_m} e^{\mathbf {A}_1 y_1 + \mathbf {A}_2 y_2 + \cdots + \mathbf {A}_m y_m} \end {split} 
                                                                                                                                                                                                                                                                                                                  (25)           \label {eq:prf-of-theo} \begin {split} & \mathbf R(\bm x; \mathcal {A})^\top \mathbf R(\bm y; \mathcal {A}) \\ &= e^{-\mathbf {A}_1 x_1 - \mathbf {A}_2 x_2 - \cdots - \mathbf {A}_N x_N} e^{\mathbf {A}_1 y_1 + \mathbf {A}_2 y_2 + \cdots + \mathbf {A}_N y_N}. \end {split} 
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   (30)

for all x1 , y1 , x2 , y2 , . . . , xm , ym ∈ R.
                                                                                                                                                                                                                                                                                                                           By Lemma 3 and Equation 30, A pairwise commute if and
Proof. 1) Necessity (⇒). By Lemma 2, we can easily ver-                                                                                                                                                                                                                                                                    only if there exists a function f : RN → Rd×d such that:
ify that the following f satisfies the condition:
                                                                                                                                                                                                                                                                                                                                                                                      \label {eq:f_eqs_exey} \begin {split} & f(y_1 - x_1, y_2 - x_2, \ldots , y_N - x_N) \\ & = \mathbf R(\bm x; \mathcal {A})^\top \mathbf R(\bm y; \mathcal {A}). \end {split} 
                          \begin {split} & f(x_1 + y_1, x_2 + y_2, \ldots , x_m + y_m) \\ & = e^{\mathbf {A}_1 (x_1 + y_1) + \mathbf {A}_2 (x_2 + y_2) + \cdots + \mathbf {A}_m (x_m + y_m)}. \end {split}                                                                                                                                                                                                                                                                                                                                                                                                         (31)
                                                                                                                                                                                                                                                                                                                  (26)
                                                                                                                                                                                                                                                                                                                           Therefore, the theorem holds.
2) Sufficiency (⇐). From Equation 31, let xk be replaced                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             ■
with xk + yk and yk with 0. We obtain:
                                                                                                                                                                                                                                                                                                                           A.2. Explanation of rotation matrix and its expo-
            \begin {split} & f(x_1 + y_1, x_2 + y_2, \ldots , x_m + y_m) \\ & = e^{\mathbf {A}_1 (x_1 + y_1) + \cdots + \mathbf {A}_m (x_m + y_m)} e^{\mathbf {A}_1 \cdot 0 + \cdots + \mathbf {A}_m \cdot 0} \\ & = e^{\mathbf {A}_1 (x_1 + y_1) + \cdots + \mathbf {A}_m (x_m + y_m)}. \end {split}                            nential representation
                                                                                                                                                                                                                                                                                                                  (27)     Following the definition in [11], we first demonstrate the
                                                                                                                                                                                                                                                                                                                           definition of rotation group and rotation matrix:

                                                                                                                                                                                                                                                                                                                           Definition 7 (Rotation Group and Rotation Matrix). A spe-
Comparing this with Equation equation 31, we get:                                                                                                                                                                                                                                                                          cial orthogonal group in Rn , denoted SO(n), is the set of
                                                                                                                                                                                                                                                                                                                           all n × n orthogonal matrices with determinant 1, i.e.,
             \begin {split} & e^{\mathbf {A}_1 x_1 + \mathbf {A}_2 x_2 + \cdots + \mathbf {A}_m x_m} e^{\mathbf {A}_1 y_1 + \mathbf {A}_2 y_2 + \cdots + \mathbf {A}_m y_m} \\ &= e^{\mathbf {A}_1 (x_1 + y_1) + \cdots + \mathbf {A}_m (x_m + y_m)}. \end {split} 
                                                                                                                                                                                                                                                                                                                  (28)
                                                                                                                                                                                                                                                                                                                                               SO(n) = \{ \mathbf R \in \mathbb {R}^{n \times n} \mid \mathbf R^\top \mathbf R = \mathbf I, \det (\mathbf R) = 1 \}. 
We use the terms rotation group and special orthogonal               all five methods. This is likely because its fixed and man-
group interchangeably. Any matrix in the rotation group is           ually defined parameters cannot be loaded seamlessly. In
called a rotation matrix.                                            other words, it must adapt the pre-trained latent space dur-
                                                                     ing fine-tuning to effectively complete the task, which may
   To establish Definition 5, there is a necessary proposition       result in suboptimal performance.
to ensure the correctness of the exponential representation
of a rotation matrix:
                                                                                          Method              Accuracy
Proposition 4. Any rotation matrix R can be represented
by exp(A) where A is a skew-symmetric matrix.                                             APE                   79.91
                                                                                      Vanilla RoPE              79.82
   Proposition 4 is a well-known result in Lie theory, as                                LieRE                  80.12
detailed in [9]. Specifically, the matrix R in Proposition 4                       ComRoPE-AP (ours)            80.11
belongs to the Lie group SO(n). The associated Lie algebra                         ComRoPE-LD (ours)            80.17
of this group is so(n), within which the skew-symmetric
matrix A resides.                                                    Table 4. Accuracy of fine-tuned models with different positional
                                                                     encoding methods on ImageNet.
B. More experiments
B.1. 3D classification
To assess the ability to handle higher dimensions beyond
                                                                     C. Details of configuration
2D, we conduct a 3D classification task on UCF-101 [31].             C.1. Configuration of 2D classification
The details of the model and configuration can be found in
Appendix C.                                                          Configuration of 2D classfication task is shown in Table 5.
   The results shown in Table 3 demonstrate similar results
in 2D experiments, that ComRoPE performs best when res-
olution increases beyond the training resolution, displaying                                 Key              Value
the resolution robustness of ComRoPE.                                                     Layers                 12
                                                                                        Image Size              224
B.2. Fine-tune on pre-trained model                                                      Patch Size              16
Recall that we represent the RoPE function parameterized                             Hidden Dimension           768
by angle matrices as defined in Equation 3. If all elements in                        Attention Heads            12
A = {Ai }N   i=1 are initialized as zero matrices (i.e., ∀i, Ai =
                                                                                        Batch Size             6144
O), the behavior of this RoPE function degenerates into a                                Optimizer           AdamW
standard attention mechanism. This is because, in this case,                           Weight Decay            0.01
R(x; A) = exp(O) = I for any input x.                                                  Learning Rate           10−3
   On the other hand, if A = {Ai }N      i=1 is initialized as de-
                                                                                       LR Scheduler           cosine
scribed in Appendix D, the RoPE function reduces to the                                Warmup Ratio            0.02
vanilla RoPE formulation.                                                                 Epochs                200
   These observations demonstrate that our method can
represent both the standard attention mechanism and var-             Table 5. Model and training configuration of 2D classification ex-
ious common RoPE attention variants. Therefore, during               periments.
fine-tuning, standard attention or vanilla RoPE can be re-
placed with our method. Pre-trained weights can be loaded
and fine-tuned under this new paradigm seamlessly, even if           C.2. Configuration of 3D classification
ComRoPE was not applied during the pre-training phase.
   As an example, we fine-tune the Vision Transformer                Because the vanilla RoPE and ComRoPE-AP require that
pre-trained in CLIP [27] on ImageNet by simply replacing             the head dimension be a multiple coordinate dimension,
the standard attention mechanism with each RoPE method.              standard ViT-Base is not applicable. We modified the model
Specifically, we fine-tune the model for 4 epochs using a            parameters to make it possible to conduct experiments on all
batch size of 3456 and a learning rate of 3 × 10−4 .                 of the five positional encoding methods. Besides, because
   The results, presented in Table 4, show that ComRoPE-             the data size of UCF-101 is not too large, using a smaller
LD achieves the best performance. An interesting observa-            model is more appropriate. All the details are shown in Ta-
tion is that vanilla RoPE exhibits the lowest accuracy among         ble 6.
                                                                                                                                                                                                     Key                                                                                                                                                                                                               Value                           cally, the table highlights their commutativity (i.e., the com-
                                                                                                                                                                                                                                                                                                                                                                                                                                                       mutativity of angle matrices when represented in the RoPE
                                                                                                                  Layers                                                                                                                                                                                                                                                                   8                                                           form parameterized by angle matrices), the number of addi-
                                                                                                                Image Size                                                                                                                                                                                                                                                                224                                                          tional parameters, and the extra time complexity introduced
                                                                                                               Frame Count                                                                                                                                                                                                                                                                 8                                                           by the positional encoding module.
                                                                                                                 Patch Size                                                                                                                                                                                                                                                                16
                                                                                                             Hidden Dimension                                                                                                                                                                                                                                                             384                                                          E.1. APE
                                                                                                              Attention Heads                                                                                                                                                                                                                                                              8
                                                                                                                Batch Size                                                                                                                                                                                                                                                                768                                                          For a Transformer that takes n embeddings with d features
                                                                                                                 Optimizer                                                                                                                                                                                                                                                             AdamW                                                           as inputs, the extra parameters of position encoding are the
                                                                                                               Weight Decay                                                                                                                                                                                                                                                               0.01                                                         tensors in the position code book, i.e., n × d. The extra
                                                                                                               Learning Rate                                                                                                                                                                                                                                                          1.2 × 10−4                                                       computation is to add position embeddings onto the original
                                                                                                               LR Scheduler                                                                                                                                                                                                                                                             cosine                                                         features. Therefore, the extra time complexity is O(n × d).
                                                                                                               Warmup Ratio                                                                                                                                                                                                                                                               0.02                                                         E.2. RoPE parameterized by angle matrices
                                                                                                                  Epochs                                                                                                                                                                                                                                                                   80
                                                                                                                                                                                                                                                                                                                                                                                                                                                       We unify RoPE with angle matrices whose rotation process
Table 6. Model and training configuration of 3D classification ex-                                                                                                                                                                                                                                                                                                                                                                                     is presented in Algorithm 1, where n, h, d, b, N represents
periments.                                                                                                                                                                                                                                                                                                                                                                                                                                             sequence length, number of heads, dimension of hidden
                                                                                                                                                                                                                                                                                                                                                                                                                                                       states, block size, and number of axes respectively. In this
                                                                                                                                                                                                                                                                                                                                                                                                                                                       part, we focus on extra parameters and time complexity on
D. Reformulation of baseline RoPE methods in                                                                                                                                                                                                                                                                                                                                                                                                           each layer.
    detail
D.1. Vanilla RoPE                                                                                                                                                                                                                                                                                                                                                                                                                                      Algorithm 1 Rotation of query and key matrices

Firstly, note that we can represent a 2D rotation matrix in                                                                                                                                                                                                                                                                                                                                                                                              In 1: query matrix Q with shape (n, h, hd )
the exponential form:                                                                                                                                                                                                                                                                                                                                                                                                                                    In 2: key matrix K with shape (n, h, hd )
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     d
      \left ( \begin {array}{cc} \mathrm {cos}(\alpha ) & -\mathrm {sin}(\alpha ) \\ \mathrm {sin}(\alpha ) & \mathrm {cos}(\alpha ) \end {array} \right ) = \exp (\left ( \begin {array}{cc} 0 & -\alpha \\ \alpha & 0 \end {array} \right ) )                                                                                                                                                                                                          In 3: angle base matrix A with shape (N, h, hb , b, b)
                                                                                                                                                                                                                                                 (32)                                                                                                                                                                                                    In 4: patch positions P with shape (n, N )
                                                                                                                                                                                                                                                                                                                                                                                                                                                         Out: rotated query and key matrices Q̂, K̂
The solution proposed by RoFormer, which we call vanilla                                                                                                                                                                                                                                                                                                                                                                                                 for axis = 1 to N do
RoPE here, can be regarded as a special type of ComRoPE-                                                                                                                                                                                                                                                                                                                                                                                                   Maxis ← Aaxis ⊙ Paxis
AP with block size 2 and non-trainable Pj in Equation 8                                                                                                                                                                                                                                                                                                                                                                                                  end forP
where:                                                                                                                                                                                                                                                                                                                                                                                                                                                   M ← Maxis where M has a shape of (n, h, hb     d
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          , b, b)
                                                                                                                                                                                                                                                                                                                                                                                                                                                                    M                            d d
                                                                                                                                                                                                                                                                                                                                                                                                                                                         R ← diag(e , dim = 2) with shape (n, h, h , h )
                         \begin {split} \mathbf P_j & = \left ( \begin {array}{cc} \mathrm {cos}(m\theta ^{\frac {2N}{d} \cdot j}) & -\mathrm {sin}(m\theta ^{\frac {2N}{d} \cdot j}) \\ \mathrm {sin}(m\theta ^{\frac {2N}{d} \cdot j}) & \mathrm {cos}(m\theta ^{\frac {2N}{d} \cdot j}) \end {array} \right ) \\ & = \exp (m\theta ^{\frac {2N}{d} \cdot j}\left ( \begin {array}{cc} 0 & -1 \\ 1 & 0 \end {array} \right )) \end {split}             Q̂ ← RQ, K̂ ← RK
                                                                                                                                                                                                                                                                                                                                                                                                                                                (33)
                                                                                                                                                                                                                                                                                                                                                                                                                                                         return Q̂, K̂

                                                                                                                                                                                                                                                                                                                                                                                                                                                          Angle base matrix A is defined by the RoPE method,
In practice, RoFormer adopts θ = 10000−1 as the hyper-                                                                                                                                                                                                                                                                                                                                                                                                 and the extra parameters are brought by the definition of A.
parameter of the rotation base.                                                                                                                                                                                                                                                                                                                                                                                                                        Time complexity of 1) calculating the element-wise product
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    d
D.2. LieRE                                                                                                                                                                                                                                                                                                                                                                                                                                             over each axis is O(n×h× hb    ×b2 ) = O(ndb); 2) calculat-
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       d
                                                                                                                                                                                                                                                                                                                                                                                                                                                       ing sum of M is O(N ×n×h× hb      ×b2 ) = O(ndbN ); 3) cal-
For LieRE, the blocks are independent and trainable.                                                                                                                                                                                                                                                                                                                                                                                                                                            d
                                                                                                                                                                                                                                                                                                                                                                                                                                                       culating matrix exponential is O(n×h× hb   ×b3 = O(ndb2 )
Hence, we directly define Bij in Equation 6 as:
                                                                                                                                                                                                                                                                                                                                                                                                                                                       based on [1]; 4) applying rotation is O(n × h × ( hd )2 ) =
                                                                                                                                                                                                                                                                                                                                                                                                                                                             2
                                                                                                                                                                                                          \mathbf {B}_{ij} = \mathbf {P}_{ij} - \mathbf {P}_{ij}^\top ,                                                                                                                                                                         (34)   O( ndh ). Thus, the overall time complexity of rotation is
                                                                                                                                                                                                                                                                                                                                                                                                                                                                             2

where Pij is a trainable matrix.                                                                                                                                                                                                                                                                                                                                                                                                                       O(ndbN + ndb2 + ndh ).

E. Analysis and comparison of complexity and                                                                                                                                                                                                                                                                                                                                                                                                           E.2.1   Vanilla RoPE
    extra consumption
                                                                                                                                                                                                                                                                                                                                                                                                                                                       No extra parameters are presented in vanilla RoPE, and
Table 7 presents an overview of the properties of the po-                                                                                                                                                                                                                                                                                                                                                                                              the angle base matrix A can be calculated during pre-
sitional encoding methods evaluated in this work. Specifi-                                                                                                                                                                                                                                                                                                                                                                                             processing. Besides, in vanilla RoPE, block size b = 2,
        Positional Encoding Method          Commutativity         Extra Parameters               Extra Time Complexity
                   APE                            –                      nd                          O(nd)
                                                                                                                         2
               Vanilla RoPE                      Yes                      0               O(Lnd(bN + b2 + hd )) ≈ O( Lnd
                                                                                                                      h )
                  LieRE                      Commonly Not               LN db                 O(Lnd(bN + b2 + hd ))
            ComRoPE-AP (ours)                    Yes                     Ldb                  O(Lnd(bN + b2 + hd ))
            ComRoPE-LD (ours)                    Yes                  Ld(b + Nb )             O(Lnd(bN + b2 + hd ))

Table 7. Comparison of different types of positional encoding methods. n represents for count of patches (tokens), d represents for
dimension of hidden states, L represents for count of layers, b represents for block size, N represents for count of axes, and h represents
the count of attention heads.


so hd ≫ bN + b2 = 2N + 4 in most cases. Thus,                                                           Block      Standard
count of extra parameters are 0 and extra time complexity                               Method
                        2          2
                                                                                                         Size     Deviations
is O(ndbN + ndb2 + ndh ) ≈ O( ndh ) where b = 2.
                                                                                       LieRE                         0.326
                                                                                    ComRoPE-AP             2         0.271
E.2.2    LieRE                                                                      ComRoPE-LD                       0.384
For LieRE, the angle base matrix can be formulated as A =                              LieRE                         0.246
P−P⊤ where the parameters in P are all independent. The                             ComRoPE-AP             4         0.208
only extra step to get A from P is the subtraction whose                            ComRoPE-LD                       0.278
time complexity is O(N db). Thus, count of extra parame-
                  d                                                                    LieRE                         0.195
ters are N × h × hb × b2 = N db and extra time complexity
                       2                               2                            ComRoPE-AP             8         0.171
is O(ndbN +ndb2 + ndh +ndb) = O(ndbN +ndb2 + ndh ).                                 ComRoPE-LD                       0.238

E.2.3    ComRoPE-AP                                                     Table 8. The standard deviations of elements in angle matrices
                                                                        obtained from the 2D classification experiments.
For ComRoPE-AP, we compose the angle base matrix A
                           d
whose shape is (N, h, hb     , b, b) with matrices with shape
         d
(N, h, hbN , b, b) by filling the blocks that are irrelevant to             To provide a clearer view of the long-tail distribution,
the corresponding coordinate axes with zeros. Thus, simi-               we present the density plot using both linear and logarith-
                                                 d
larly, count of extra parameters are N × h × hbN     × b2 = db          mic scales in Figure 8. From the linear scale plot, it can be
                                                        2
and extra time complexity is O(ndbN + ndb2 + ndh ).                     observed that elements near zero exhibit the highest vari-
                                                                        ance in the angle matrices of LieRE, while ComRoPE-AP
E.2.4    ComRoPE-LD                                                     demonstrates the most moderate variance. On the other
                                                                        hand, the logarithmic scale reveals notable differences in
For ComRoPE-LD, the angle base matrices in A are pair-                  range. For instance, ComRoPE-LD retains a broader dis-
wise linearly dependent on the first dimension (i.e., axis di-          tribution at values farther from zero. Consequently, as in-
mension). Therefore, it can be presented by a matrix with               dicated in Table 8, ComRoPE-LD exhibits the largest over-
            d
shape (h, hb  , b, b) and a multiplication factor with shape            all variance among the angle matrix elements. This phe-
        d
(N, h, hb ) by a multiplication step with time complexity               nomenon is likely due to the linear dependencies between
                d
O(N × h × hb        × b2 ) = O(N db). Thus, count of extra              angle matrices across different coordinate axes, which ne-
                       d                  d
parameters are h × hb    × b2 + N × h × hb  = d(b + Nb ), and           cessitate significant frequency differences to distinguish
extra time complexity is O(ndbN + ndb2 + ndh ).
                                                  2
                                                                        them effectively.

F. Distribution of elements in angle matrices                           G. More Analysis
In this section, we analyze the element distribution in angle           Computational complexity and time consumption. Time
matrices obtained from the 2D classification experiments.               consumption is shown in Table 9. While small block sizes
Specifically, we extract all elements from the upper triangu-           should have minimal impact, parallel optimization issues
lar parts of the matrices. The standard deviations of these             in torch.matrix exp lower GPU utilization, increas-
elements are summarized in Table 8, and their density plot              ing time costs for LieRE and ComRoPE.
is presented in Figure 8.                                               Application to LLMs. ComRoPE can be incorporated into
                                                                                                           ComRoPE-AP                      100
            5                                                                                              ComRoPE-LD
                                                                                                           LieRE                          10 2
            4                                                                                                                             10 4
  Density




                                                                                                                               Density
            3                                                                                                                             10 6
                                                                                                                                          10 8
            2                                                                                                                            10 10
                                                                                                                                         10 12                                       ComRoPE-AP
            1                                                                                                                                                                        ComRoPE-LD
                                                                                                                                         10 14                                       LieRE
            0
                       0.6              0.4              0.2             0.0             0.2             0.4             0.6                          4                2               0              2              4
                                                           Element Value                                                                                                       Element Value
                                            (a) Block Size = 2, linear scale                                                                                    (b) Block Size = 2, log scale

                                                                                                           ComRoPE-AP                     101                                                               ComRoPE-AP
            8                                                                                              ComRoPE-LD                                                                                       ComRoPE-LD
                                                                                                                                          100
            7                                                                                              LieRE                                                                                            LieRE
                                                                                                                                         10 1
            6
                                                                                                                                         10 2
  Density




                                                                                                                               Density
            5
            4                                                                                                                            10 3
            3                                                                                                                            10 4
            2                                                                                                                            10 5
            1                                                                                                                            10 6
            0
                       0.6              0.4              0.2             0.0             0.2             0.4             0.6                     3          2              1          0           1           2           3
                                                           Element Value                                                                                                       Element Value
                                            (c) Block Size = 4, linear scale                                                                                    (d) Block Size = 4, log scale
            12                                                                                             ComRoPE-AP                     101                                                               ComRoPE-AP
                                                                                                           ComRoPE-LD                                                                                       ComRoPE-LD
            10                                                                                             LieRE                          100                                                               LieRE
             8                                                                                                                           10 1
  Density




                                                                                                                               Density




                                                                                                                                         10 2
             6
                                                                                                                                         10 3
             4
                                                                                                                                         10 4
             2                                                                                                                           10 5
             0
                         0.6                0.4           0.2            0.0             0.2             0.4             0.6                    2.0   1.5        1.0           0.5   0.0    0.5       1.0      1.5       2.0
                                                               Element Value                                                                                               Element Value
                                            (e) Block Size = 8, linear scale                                                                                    (f) Block Size = 8, log scale

Figure 8. Density distribution of elements in the upper triangular sections of angle matrices from 2D classification experiments. Subfigures
(a-b), (c-d), and (e-f) show the distributions for different block sizes: 2, 4, and 8, respectively.


                                     Vanilla      LieRE        ComRoPE-LD      ComRoPE-AP      ComRoPE-LD      ComRoPE-AP
             Method
                                     RoPE      block = 8 × 8   block = 4 × 4   block = 4 × 4   block = 8 × 8   block = 8 × 8       training prohibitively expensive. Addressing this bottle-
  Training Time per Epoch (min)        16           21              19              19              20              20
 Inference Time on Valid Split (s)     32           36              35              34              35              35             neck, thereby unlocking full-scale LLM experiments, re-
           Table 9. Computational costs compared on A800×4.                                                                        mains a key priority for future work.


large language models as a drop-in substitute for the ro-
tary position embeddings used in most pre-trained check-
points, requiring no extra architectural changes during fine-                                                                      Implementation with sota codebase and settings. Our
tuning. Because language modeling operates along a sin-                                                                            work compares RoPE designs under consistent settings to
gle sequence dimension, the commuting property of our                                                                              highlight relative advantages, as demonstrated by our ex-
angle matrices holds automatically. At present, however,                                                                           periments. For a more thorough and convincing compari-
the torch.matrix exp implementation incurs substan-                                                                                son, we conduct additional experiments in the RoPE-Mixed
tial memory overhead on large models, making end-to-end                                                                            codebase with DeiT data augmentation (C.f. Table 10).
 Position Encoding                     Evaluation Resolution
      Method         128     192     224    256    320     384       448     512
    RoPE-Mixed       68.99   79.75   81.42   82.31   82.75   82.11   80.61   78.39
   ComRoPE-AP        68.48   80.94   82.01   82.59   82.43   81.65   80.58   79.75
   ComRoPE-LD        69.88   79.91   81.78   83.24   83.36   82.32   80.79   78.97

Table 10. Results with DeiT recipe. RoPE-Mixed corresponds
to ComRoPE-LD with 2 × 2 blocks. ComRoPE-LD consistently
outperforms RoPE-Mixed.


H. Limitations
Despite the merits of our approach, it has two notable con-
straints that call for further investigation. The first one
is computational overhead. Our implementation depends
on torch.matrix exp, which is slow and memory-
intensive on large models. Cutting training time and GPU
memory use is therefore an urgent engineering goal. And
the other is strict commutativity restrictions. We currently
require relatively strong conditions for the angle matrices
to commute, which may restrict the expressiveness of the
resulting embeddings. Identifying weaker—yet still suffi-
cient—conditions could broaden the method’s capacity and
applicability. Addressing these two issues will be the cor-
nerstone of our future work, paving the way for more effi-
cient training and richer modeling flexibility.
