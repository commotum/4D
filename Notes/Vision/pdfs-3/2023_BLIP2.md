## 1. Basic Metadata
- Title: BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models.
  - Evidence (Title block): "BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models"
- Authors: Junnan Li; Dongxu Li; Silvio Savarese; Steven Hoi.
  - Evidence (Title block): "Junnan Li Dongxu Li Silvio Savarese Steven Hoi"
- Year: 2023.
  - Evidence (Title block): "arXiv:2301.12597v3 [cs.CV] 15 Jun 2023"
- Venue: arXiv (arXiv preprint).
  - Evidence (Title block): "arXiv:2301.12597v3 [cs.CV] 15 Jun 2023"

## 2. One-Sentence Contribution Summary
BLIP-2 introduces a vision-language pre-training method that bootstraps from frozen unimodal models using a Q-Former to bridge vision and language efficiently.
- Evidence (Section 3. Method): "We propose BLIP-2, a new vision-language pre-training method that bootstraps from frozen pre-trained unimodal models."

## 3. Tasks Evaluated
### 3.1 Instructed Zero-shot Image-to-Text Generation
- Task type: Generation.
- Dataset(s): Not specified in the paper.
- Domain: Images (no specific domain stated).
- Evidence (Section 4.1 Instructed Zero-shot Image-to-Text Generation): "BLIP-2 effectively enables a LLM to understand images while preserving its capability in following text prompts, which allows us to control image-to-text generation with instructions."
- Evidence (Key advantages bullet list): "BLIP-2 can be prompted to perform zero-shot image-to-text generation that follows natural language instructions, which enables emerging capabilities such as visual knowledge reasoning, visual conversation, etc."

### 3.2 Zero-shot Visual Question Answering (VQA)
- Task type: Generation; Reasoning / relational (question answering).
- Dataset(s): VQAv2; GQA; OK-VQA.
- Domain: Visual (images; not further specified).
- Evidence (Section 4.1): "Zero-shot VQA. We perform quantitative evaluation on the zero-shot visual question answering task."
- Evidence (Section 4. Experiment): "BLIP-2 achieves state-of-the-art result on the VQAv2 (Goyal et al., 2017) and GQA (Hudson & Manning, 2019) datasets."
- Evidence (Section 4. Experiment): "On the OK-VQA (Marino et al., 2019) dataset, BLIP-2 comes secondary to Flamingo80B."

### 3.3 Image Captioning
- Task type: Generation.
- Dataset(s): COCO; NoCaps (zero-shot transfer evaluation).
- Domain: Images (no specific domain stated).
- Evidence (Section 4.2 Image Captioning): "We finetune BLIP-2 models for the image captioning task, which asks the model to generate a text description for the image’s visual content."
- Evidence (Section 4.2 Image Captioning): "We perform finetuning on COCO, and evaluate on both COCO test set and zero-shot transfer to NoCaps (Agrawal et al., 2019) validation set."

### 3.4 Visual Question Answering (Supervised Fine-tuning)
- Task type: Generation; Reasoning / relational (question answering).
- Dataset(s): VQAv2 (train/val); Visual Genome (training samples).
- Domain: Visual (images; not further specified).
- Evidence (Section 4.3 Visual Question Answering): "Given annotated VQA data, we finetune the parameters of the Q-Former and the image encoder while keeping the LLM frozen. We finetune with the open-ended answer generation loss, where the LLM receives Q-Former’s output and the question as input, and is asked to generate the answer."
- Evidence (Section 4.3 Visual Question Answering): "Following BLIP, our VQA data includes the training and validation splits from VQAv2, as well as training samples from Visual Genome."

### 3.5 Image-Text Retrieval
- Task type: Other (retrieval).
- Dataset(s): COCO; Flickr30K.
- Domain: Image-text pairs (no specific domain stated).
- Evidence (Section 4.4 Image-Text Retrieval): "Since image-text retrieval does not involve language generation, we directly finetune the first-stage-pretrained model w/o LLM."
- Evidence (Section 4.4 Image-Text Retrieval): "We then evaluate the model for both image-to-text retrieval and text-to-image retrieval on COCO and Flickr30K (Plummer et al., 2015) datasets."

## 4. Domain and Modality Scope
- Single domain? Not specified; evaluation spans multiple datasets within the same vision-language modality.
  - Evidence (Section 4.2): "We perform finetuning on COCO, and evaluate on both COCO test set and zero-shot transfer to NoCaps (Agrawal et al., 2019) validation set."
  - Evidence (Section 4.4): "We then evaluate the model for both image-to-text retrieval and text-to-image retrieval on COCO and Flickr30K (Plummer et al., 2015) datasets."
  - Evidence (Section 4. Experiment): "BLIP-2 achieves state-of-the-art result on the VQAv2 (Goyal et al., 2017) and GQA (Hudson & Manning, 2019) datasets."
  - Evidence (Section 4. Experiment): "On the OK-VQA (Marino et al., 2019) dataset, BLIP-2 comes secondary to Flamingo80B."
- Multiple domains within the same modality? Yes; multiple image-text datasets are evaluated (COCO, NoCaps, Flickr30K, VQAv2, GQA, OK-VQA).
- Multiple modalities? Not specified beyond vision-language (image + text).
  - Evidence (Section 3.2): "In the representation learning stage, we connect Q-Former to a frozen image encoder and perform pre-training using image-text pairs."
- Domain generalization / cross-domain transfer? Yes; zero-shot transfer to out-domain images is claimed.
  - Evidence (Section 4.2 / Table 3 discussion): "significant improvement on NoCaps over existing methods, demonstrating strong generalization ability to out-domain images."

## 5. Model Sharing Across Tasks
| Task | Shared Weights? | Fine-Tuned? | Separate Head? | Evidence |
| --- | --- | --- | --- | --- |
| Instructed zero-shot image-to-text generation | Not explicitly stated; described as zero-shot prompting of the LLM | No (zero-shot prompting) | Not specified; generation via LLM | "BLIP-2 effectively enables a LLM to understand images while preserving its capability in following text prompts, which allows us to control image-to-text generation with instructions. We simply append the text prompt after the visual prompt as input to the LLM." (Section 4.1) |
| Zero-shot VQA | Not explicitly stated; described as zero-shot evaluation | No (zero-shot) | Not specified; generation via LLM | "Zero-shot VQA. We perform quantitative evaluation on the zero-shot visual question answering task." (Section 4.1) |
| Image captioning | Fine-tuned per task (Q-Former + image encoder; LLM frozen) | Yes | Not specified; generation via LLM | "We keep the LLM frozen during finetuning, and updates the parameters of the Q-Former together with the image encoder." (Section 4.2) |
| VQA (supervised) | Fine-tuned per task (Q-Former + image encoder; LLM frozen) | Yes | Not specified; generation via LLM | "Given annotated VQA data, we finetune the parameters of the Q-Former and the image encoder while keeping the LLM frozen." (Section 4.3) |
| Image-text retrieval | Fine-tuned per task (first-stage model, no LLM) | Yes | Yes (ITM linear classifier) | "Since image-text retrieval does not involve language generation, we directly finetune the first-stage-pretrained model w/o LLM." (Section 4.4); "We feed each output query embedding into a two-class linear classifier to obtain a logit..." (Section 3.2) |

## 6. Input and Representation Constraints
- Input resolution (pre-training): fixed 224×224 with resizing/augmentation.
  - Evidence (Section 3.4 Model Pre-training): "We use images of size 224×224, augmented with random resized cropping and horizontal flipping."
- Input resolution (fine-tuning): fixed per task; varies across experiments (examples shown in appendix tables).
  - Evidence (Table 7): "Image resolution                                                     364"
  - Evidence (Table 8): "Image resolution                                               490"
- Patch size / image encoder granularity: ViT-L/14 and ViT-g/14 (patch size 14).
  - Evidence (Section 3.3 Pre-trained image encoder and LLM): "(1) ViT-L/14 from CLIP (Radford et al., 2021) and (2) ViT-g/14 from EVA-CLIP (Fang et al., 2022)."
- Fixed number of query tokens / output features: 32 queries; fixed-size output independent of input resolution.
  - Evidence (Section 3.1): "It extracts a fixed number of output features from the image encoder, independent of input image resolution."
  - Evidence (Section 3.1): "In our experiments, we use 32 queries where each query has a dimension of 768..."
- Fixed dimensionality: query hidden size 768.
  - Evidence (Section 3.1): "each query has a dimension of 768 (same as the hidden dimension of the Q-Former)."
- Example image token count from encoder: 257 × 1024 (ViT-L/14).
  - Evidence (Section 3.1): "The size of Z (32 × 768) is much smaller than the size of frozen image features (e.g. 257 × 1024 for ViT-L/14)."
- Padding requirements: Not specified in the paper.

## 7. Context Window and Attention Structure
- Maximum sequence length: Not specified in the paper.
- Fixed vs. variable sequence length: Query length is fixed (32 queries); text sequence length not specified.
  - Evidence (Section 3.1): "In our experiments, we use 32 queries..."
  - Evidence (Section 3.1): "It extracts a fixed number of output features from the image encoder, independent of input image resolution."
- Attention type: Global self-attention with cross-attention; task-specific attention masks.
  - Evidence (Section 3.1): "The queries interact with each other through self-attention layers, and interact with frozen image features through cross-attention layers (inserted every other transformer block)."
  - Evidence (Section 3.1): "Depending on the pre-training task, we apply different self-attention masks to control query-text interaction."
- Mechanisms to manage computational cost: fixed-size query bottleneck; frozen unimodal models / lightweight Q-Former.
  - Evidence (Section 3.1): "It extracts a fixed number of output features from the image encoder, independent of input image resolution."
  - Evidence (Key advantages bullet list): "Due to the use of frozen unimodal models and a lightweight Q-Former, BLIP-2 is more compute-efficient than exisiting state-of-the-arts."

## 8. Positional Encoding (Critical Section)
- Positional encoding mechanism used: Not specified in the paper.
- Where it is applied: Not specified in the paper.
- Fixed/modified/ablated across tasks: Not specified in the paper.

## 9. Positional Encoding as a Variable
- Treated as a core research variable or fixed assumption: Not specified in the paper.
- Multiple positional encodings compared: Not specified in the paper.
- Claim that PE choice is “not critical” or secondary: Not specified in the paper.

## 10. Evidence of Constraint Masking (Scale vs. Structure)
- Model size(s):
  - Evidence (Section 3.1): "In total, Q-Former contains 188M parameters."
- Dataset size(s):
  - Evidence (Section 3.4 Model Pre-training): "We use the same pre-training dataset as BLIP with 129M images in total, including COCO (Lin et al., 2014), Visual Genome (Krishna et al., 2017), CC3M (Sharma et al., 2018), CC12M (Changpinyo et al., 2021), SBU (Ordonez et al., 2011), and 115M images from the LAION400M dataset (Schuhmann et al., 2021)."
- Performance gains attributed to scaling model size: Yes.
  - Evidence (Section 4.1): "We make a promising observation from Table 2: a stronger image encoder or a stronger LLM both lead to better performance."
- Performance gains attributed to scaling data: Not specified in the paper.
- Performance gains attributed to architectural hierarchy: Not specified in the paper.
- Performance gains attributed to training tricks / staging: Yes (representation learning stage matters).
  - Evidence (Figure 5 caption): "Without representation learning, the Q-Former fails the bridge the modality gap, leading to significantly lower performance on zero-shot VQA."

## 11. Architectural Workarounds
- Q-Former bottleneck (fixed number of outputs independent of input resolution):
  - Evidence (Section 3.1): "It extracts a fixed number of output features from the image encoder, independent of input image resolution."
- Learnable query tokens and cross-attention every other block:
  - Evidence (Section 3.1): "The queries interact with each other through self-attention layers, and interact with frozen image features through cross-attention layers (inserted every other transformer block)."
- Task-specific self-attention masks to control query-text interaction:
  - Evidence (Section 3.1): "Depending on the pre-training task, we apply different self-attention masks to control query-text interaction."
- FC adapter to map Q-Former outputs into LLM embedding space (soft visual prompts):
  - Evidence (Section 3.3): "We use a fully-connected (FC) layer to linearly project the output query embeddings Z into the same dimension as the text embedding of the LLM. The projected query embeddings are then prepended to the input text embeddings. They function as soft visual prompts..."
- Frozen unimodal models and lightweight Q-Former for efficiency:
  - Evidence (Key advantages bullet list): "Due to the use of frozen unimodal models and a lightweight Q-Former, BLIP-2 is more compute-efficient than exisiting state-of-the-arts."
- Two-stage pre-training to bridge the modality gap:
  - Evidence (Section 3. Method): "Querying Transformer (Q-Former) pre-trained in two stages: (1) vision-language representation learning stage with a frozen image encoder and (2) vision-to-language generative learning stage with a frozen LLM."

## 12. Explicit Limitations and Non-Claims
- In-context learning not improved for VQA with BLIP-2:
  - Evidence (Section 5. Limitation): "Recent LLMs can perform in-context learning given few-shot examples. However, our experiments with BLIP-2 do not observe an improved VQA performance when providing the LLM with in-context VQA examples."
- Limitation attributed to dataset structure (single image-text pair per sample):
  - Evidence (Section 5. Limitation): "We attribute the lack of in-context learning capability to our pre-training dataset, which only contains a single image-text pair per sample."
- Future work: create interleaved image-text dataset:
  - Evidence (Section 5. Limitation): "We aim to create a similar dataset in future work."
- Image-to-text generation risks/limitations:
  - Evidence (Section 5. Limitation): "BLIP-2’s image-to-text generation could have unsatisfactory results due to various reasons including inaccurate knowledge from the LLM, activating the incorrect reasoning path, or not having up-to-date information about new image content (see Figure 7)."
- Inherited LLM risks (offensive language, bias, privacy):
  - Evidence (Section 5. Limitation): "Furthermore, due to the use of frozen models, BLIP-2 inherits the risks of LLMs, such as outputting offensive language, propagating social bias, or leaking private information."
- Other explicit non-claims (e.g., open-world learning, unrestrained multi-task learning, meta-learning): Not specified in the paper.
