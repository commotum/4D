# MAE Paper Survey Responses

## 1. Basic Metadata
- Title: "Masked Autoencoders Are Scalable Vision Learners" (Title page)
- Authors: "Kaiming He∗,† Xinlei Chen∗ Saining Xie Yanghao Li Piotr Dollár Ross Girshick" (Title page)
- Year: 2021 ("arXiv:2111.06377v3 [cs.CV] 19 Dec 2021", Title page/Abstract)
- Venue: arXiv ("arXiv:2111.06377v3 [cs.CV] 19 Dec 2021", Title page/Abstract)

## 2. One-Sentence Contribution Summary
- The paper proposes masked autoencoders as scalable self-supervised vision learners that mask random image patches and reconstruct missing pixels using a ViT-based encoder/decoder design for efficient large-model pre-training ("This paper shows that masked autoencoders (MAE) are scalable self-supervised learners for computer vision." and "Our MAE approach is simple: we mask random patches of the input image and reconstruct the missing pixels.", Abstract; "MAE encoder. Our encoder is a ViT [16] but applied only on visible, unmasked patches.", Section 3. Approach).

## 3. Tasks Evaluated

### Task: Masked image reconstruction (self-supervised pre-training)
- Task type: Reconstruction
- Dataset(s): ImageNet-1K (IN1K) training set
- Domain: Images (paper explicitly frames inputs as images/pixels)
- Evidence:
  - "We do self-supervised pre-training on the ImageNet-1K (IN1K) [13] training set." (Section 4. ImageNet Experiments)
  - "Reconstruction target. Our MAE reconstructs the input by predicting the pixel values for each masked patch." (Section 4. ImageNet Experiments)
  - "Our MAE approach is simple: we mask random patches of the input image and reconstruct the missing pixels." (Abstract)

### Task: ImageNet-1K classification (fine-tuning)
- Task type: Classification
- Dataset(s): ImageNet-1K (IN1K)
- Domain: Images
- Evidence:
  - "Then we do supervised training to evaluate the representations with (i) end-to-end fine-tuning or (ii) linear probing." (Section 4. ImageNet Experiments)
  - "We report top-1 validation accuracy of a single 224×224 crop." (Section 4. ImageNet Experiments)

### Task: ImageNet-1K classification (linear probing)
- Task type: Classification
- Dataset(s): ImageNet-1K (IN1K)
- Domain: Images
- Evidence:
  - "Then we do supervised training to evaluate the representations with (i) end-to-end fine-tuning or (ii) linear probing." (Section 4. ImageNet Experiments)

### Task: COCO object detection
- Task type: Detection
- Dataset(s): COCO
- Domain: Images
- Evidence:
  - "Object detection and segmentation. We fine-tune Mask R-CNN [24] end-to-end on COCO [37]." (Section 5. Transfer Learning Experiments)
  - "We report box AP for object detection and mask AP for instance segmentation." (Section 5. Transfer Learning Experiments)

### Task: COCO instance segmentation
- Task type: Segmentation
- Dataset(s): COCO
- Domain: Images
- Evidence:
  - "Object detection and segmentation. We fine-tune Mask R-CNN [24] end-to-end on COCO [37]." (Section 5. Transfer Learning Experiments)
  - "We report box AP for object detection and mask AP for instance segmentation." (Section 5. Transfer Learning Experiments)

### Task: ADE20K semantic segmentation
- Task type: Segmentation
- Dataset(s): ADE20K
- Domain: Images
- Evidence:
  - "Semantic segmentation. We experiment on ADE20K [72] using UperNet [63] (see A.4)." (Section 5. Transfer Learning Experiments)

### Task: iNaturalist classification
- Task type: Classification
- Dataset(s): iNaturalists (iNat 2017/2018/2019)
- Domain: Images
- Evidence:
  - "Classification tasks. Table 6 studies transfer learning on the iNaturalists [56] and Places [71] tasks (see A.5)." (Section 5. Transfer Learning Experiments)

### Task: Places classification
- Task type: Classification
- Dataset(s): Places (Places205, Places365)
- Domain: Images
- Evidence:
  - "Classification tasks. Table 6 studies transfer learning on the iNaturalists [56] and Places [71] tasks (see A.5)." (Section 5. Transfer Learning Experiments)

### Task: Robustness evaluation on ImageNet variants
- Task type: Classification (evaluation on variant datasets)
- Dataset(s): ImageNet variants (IN-Corruption, IN-Adversarial, IN-Rendition, IN-Sketch)
- Domain: Images
- Evidence:
  - "Table 13. Robustness evaluation on ImageNet variants (top-1 accuracy, except for IN-C [27] which evaluates mean corruption error)." (Table 13 caption)
  - "In Table 13 we evaluate the robustness of our models on different variants of ImageNet validation sets. We use the same models fine-tuned on original ImageNet (Table 3) and only run inference on the different validation sets, without any specialized fine-tuning." (Appendix C. Robustness Evaluation on ImageNet)

## 4. Domain and Modality Scope
- Modality: Single modality (images). Evidence: "Our MAE approach is simple: we mask random patches of the input image and reconstruct the missing pixels." (Abstract)
- Domains/datasets evaluated: Multiple datasets within the same modality (ImageNet-1K, COCO, ADE20K, iNaturalists, Places). Evidence:
  - "We do self-supervised pre-training on the ImageNet-1K (IN1K) [13] training set." (Section 4. ImageNet Experiments)
  - "We fine-tune Mask R-CNN [24] end-to-end on COCO [37]." (Section 5. Transfer Learning Experiments)
  - "We experiment on ADE20K [72] using UperNet [63] (see A.4)." (Section 5. Transfer Learning Experiments)
  - "Classification tasks. Table 6 studies transfer learning on the iNaturalists [56] and Places [71] tasks (see A.5)." (Section 5. Transfer Learning Experiments)
- Domain generalization or cross-domain transfer claims: Not claimed explicitly. The paper does claim transfer learning across downstream tasks: "Transfer performance in downstream tasks outperforms supervised pre-training and shows promising scaling behavior." (Abstract) and "We evaluate transfer learning in downstream tasks using the pre-trained models in Table 3." (Section 5. Transfer Learning Experiments)

## 5. Model Sharing Across Tasks
| Task | Shared Weights? | Fine-Tuned? | Separate Head? | Evidence |
| --- | --- | --- | --- | --- |
| Masked image reconstruction (pre-training) | N/A (base pre-training) | N/A | Decoder used for reconstruction | "We do self-supervised pre-training on the ImageNet-1K (IN1K) [13] training set." and "Our MAE reconstructs the input by predicting the pixel values for each masked patch." (Section 4. ImageNet Experiments) |
| ImageNet-1K classification (fine-tuning) | Yes (pre-trained MAE) | Yes | Yes (classifier) | "Then we do supervised training to evaluate the representations with (i) end-to-end fine-tuning or (ii) linear probing." (Section 4. ImageNet Experiments); "We extract features from the encoder output for fine-tuning and linear probing." and "This token will be treated as the class token for training the classifier in linear probing and fine-tuning." (Appendix A.1) |
| ImageNet-1K classification (linear probing) | Yes (pre-trained MAE) | No (frozen encoder) | Yes (linear classifier) | "Then we do supervised training to evaluate the representations with (i) end-to-end fine-tuning or (ii) linear probing." (Section 4. ImageNet Experiments); "We extract features from the encoder output for fine-tuning and linear probing." and "This token will be treated as the class token for training the classifier in linear probing and fine-tuning." (Appendix A.1) |
| COCO object detection / instance segmentation | Yes (pre-trained MAE backbone) | Yes | Yes (Mask R-CNN heads) | "We evaluate transfer learning in downstream tasks using the pre-trained models in Table 3." and "We fine-tune Mask R-CNN [24] end-to-end on COCO [37]." (Section 5. Transfer Learning Experiments) |
| ADE20K semantic segmentation | Yes (pre-trained MAE backbone) | Yes | Yes (UperNet head) | "We evaluate transfer learning in downstream tasks using the pre-trained models in Table 3." (Section 5. Transfer Learning Experiments); "We experiment on ADE20K [72] using UperNet [63] (see A.4)." and "We fine-tune end-to-end for 100 epochs" (Appendix A.4) |
| iNaturalist / Places classification | Yes (pre-trained MAE) | Yes | Yes (classification head) | "Classification tasks. Table 6 studies transfer learning on the iNaturalists [56] and Places [71] tasks (see A.5)." and "We follow the setting in Table 9 for iNaturalist and Places fine-tuning (Table 6)." (Section 5 / Appendix A.5) |
| ImageNet variants robustness evaluation | Yes (same ImageNet-fine-tuned models) | No specialized fine-tuning | Same classifier | "We use the same models fine-tuned on original ImageNet (Table 3) and only run inference on the different validation sets, without any specialized fine-tuning." (Appendix C. Robustness Evaluation on ImageNet) |

## 6. Input and Representation Constraints
- Patchification: "Following ViT [16], we divide an image into regular non-overlapping patches." (Section 3. Approach)
- Masking procedure: "Then we sample a subset of patches and mask (i.e., remove) the remaining ones." (Section 3. Approach)
- Patch size (per model family): "The ViT models are B/16, L/16, H/14 [16]." (Table 3 caption)
- Fixed input resolution in experiments: "We report top-1 validation accuracy of a single 224×224 crop." (Section 4. ImageNet Experiments)
- Alternative input resolution reported: "All results are on an image size of 224, except for ViT-H with an extra result on 448." (Table 3 caption)
- Example patch/token count (implied by fixed grid): "The masking ratio is 80%, leaving only 39 out of 196 patches." (Figure 2 caption)
- Resizing/cropping used in training: "augmentation                    RandomResizedCrop" (Table 8. Pre-training setting) and "augmentation                    RandomResizedCrop" (Table 9. End-to-end fine-tuning setting)
- Padding requirements: Not specified in the paper.
- Explicit statement about variable-length inputs: Not specified in the paper.
- Explicit statement about dimensionality constraints (e.g., strictly 2D): Not specified in the paper.

## 7. Context Window and Attention Structure
- Maximum sequence length: Not specified in the paper. Example patch count is given for one setting: "The masking ratio is 80%, leaving only 39 out of 196 patches." (Figure 2 caption)
- Fixed vs. variable sequence length: Fixed image sizes are reported per experiment (224 or 448), but variable-length handling is not described. Evidence: "We report top-1 validation accuracy of a single 224×224 crop." (Section 4. ImageNet Experiments) and "All results are on an image size of 224, except for ViT-H with an extra result on 448." (Table 3 caption)
- Attention type: Transformer blocks with multi-head self-attention. Evidence: "It has a stack of Transformer blocks [57], and each block consists of a multi-head self-attention block and an MLP block, both having LayerNorm (LN) [1]." (Appendix A.1)
- Computational cost mechanisms:
  - Asymmetric encoder/decoder with sparse visible patches: "Our encoder is a ViT [16] but applied only on visible, unmasked patches." and "The full set is handled by a lightweight decoder, described next." (Section 3. Approach)
  - High masking ratio for efficiency and task difficulty: "Second, we find that masking a high proportion of the input image, e.g., 75%, yields a nontrivial and meaningful self-supervisory task." (Abstract)
  - Reduced compute by skipping mask tokens in encoder: "Moreover, by skipping the mask token in the encoder, we greatly reduce training computation. In Table 1c, we reduce the overall training FLOPs by 3.3×. This leads to a 2.8× wall-clock speedup in our implementation (see Table 2)." (Section 4.1)

## 8. Positional Encoding (Critical Section)
- Mechanism: Absolute sine-cosine positional embeddings. Evidence: "Our MAE adds positional embeddings [57] (the sine-cosine version) to both the encoder and decoder inputs." (Appendix A.1)
- Where applied: "We add positional embeddings to all tokens in this full set; without this, mask tokens would have no information about their location in the image." (Section 3. Approach); "to both the encoder and decoder inputs" (Appendix A.1)
- Fixed vs. modified per task: Pre-training uses sine-cosine positional embeddings and no relative position, but relative position bias is enabled for segmentation transfer: "Our MAE does not use relative position or layer scaling (which are used in the code of [2])." (Appendix A.1) and "The semantic segmentation code of [2] uses relative position bias [49]. Our MAE pre-training does not use it. For fair comparison, we turn on relative position bias only during transfer learning, initialized as zero." (Appendix A.4)

## 9. Positional Encoding as a Variable
- Core research variable or fixed assumption: Fixed architectural assumption (sine-cosine positional embeddings); not presented as a primary variable. Evidence: "Our MAE adds positional embeddings [57] (the sine-cosine version) to both the encoder and decoder inputs." (Appendix A.1)
- Multiple positional encodings compared: Not explicitly compared as an ablation in the main experiments. The only stated change is enabling relative position bias during transfer learning for ADE20K: "For fair comparison, we turn on relative position bias only during transfer learning, initialized as zero." (Appendix A.4)
- Claim that PE choice is not critical/secondary: Not specified in the paper.

## 10. Evidence of Constraint Masking (Scale vs. Structure)
- Model size scaling evidence:
  - "Our scalable approach allows for learning high-capacity models that generalize well: e.g., a vanilla ViT-Huge model achieves the best accuracy (87.8%) among methods that use only ImageNet-1K data." (Abstract)
  - "We obtain 86.9% accuracy using ViT-H (224 size). By fine-tuning with a 448 size, we achieve 87.8% accuracy, using only IN1K data." (Section 4.2)
  - "and more importantly, we observe significant gains by scaling up models." (Section 1. Introduction)
- Dataset size evidence:
  - "methods that use only ImageNet-1K data." (Abstract)
  - "We do self-supervised pre-training on the ImageNet-1K (IN1K) [13] training set." (Section 4. ImageNet Experiments)
- Architectural/training factors tied to performance/efficiency:
  - "Second, we find that masking a high proportion of the input image, e.g., 75%, yields a nontrivial and meaningful self-supervisory task." (Abstract)
  - "Moreover, by skipping the mask token in the encoder, we greatly reduce training computation. In Table 1c, we reduce the overall training FLOPs by 3.3×. This leads to a 2.8× wall-clock speedup in our implementation (see Table 2)." (Section 4.1)
  - "The accuracy improves steadily with longer training. Indeed, we have not observed saturation of linear probing accuracy even at 1600 epochs." (Section 4.1)

## 11. Architectural Workarounds
- Asymmetric encoder-decoder with sparse visible patches: "Our encoder is a ViT [16] but applied only on visible, unmasked patches." and "The full set is handled by a lightweight decoder, described next." (Section 3. Approach)
- High masking ratio strategy: "Second, we find that masking a high proportion of the input image, e.g., 75%, yields a nontrivial and meaningful self-supervisory task." (Abstract)
- Skipping mask tokens in the encoder to reduce computation: "Mask token. An important design of our MAE is to skip the mask token [M] in the encoder and apply it later in the lightweight decoder." (Section 4.1)
- Lightweight decoder compute budget: "For example, our default decoder has <10% computation per token vs. the encoder." (Section 4. ImageNet Experiments)
- Multi-scale adaptation for detection: "We adapt the vanilla ViT for the use of an FPN backbone [36] in Mask R-CNN [24]." (Appendix A.3)
- Task-specific segmentation head: "We use UperNet [63] following the semantic segmentation code of [2]." (Appendix A.4)

## 12. Explicit Limitations and Non-Claims
- Stated limitations / cautions (Broader impacts):
  - "The proposed method predicts content based on learned statistics of the training dataset and as such will reflect biases in those data, including ones with negative societal impacts. The model may generate inexistent content. These issues warrant further research and consideration when building upon this work to generate images." (Broader impacts)
- Explicit statements about what the model does NOT attempt to do (open-world learning, unrestrained multi-task learning, meta-learning): Not specified in the paper.
