## 1. Basic Metadata
- Title: YaRN: Efficient Context Window Extension of Large Language Models
- Authors: Bowen Peng; Jeffrey Quesnelle; Honglu Fan; Enrico Shippole
- Year: 2023 ("arXiv:2309.00071v2 [cs.CL] 1 Nov 2023") (p. 1)
- Venue: arXiv preprint ("Preprint. Under review.") (p. 1)

## 2. One-Sentence Contribution Summary
The paper proposes YaRN, a compute-efficient method to extend the context window of RoPE-based large language models so they can use much longer contexts with fewer training tokens and steps.

## 3. Tasks Evaluated
### Task: Long sequence language modeling (perplexity)
- Task type: Generation (language modeling/perplexity)
- Dataset(s): GovReport; Proof-pile
- Domain: Not specified in the paper.
- Evidence: "To evaluate the long sequence language modeling performances, we use the GovReport [18] and" (Section 4.3.1, p. 8); "Proof-pile [4] datasets both of which contain many long sequence samples." (Section 4.3.1, p. 8)

### Task: Passkey retrieval
- Task type: Other (retrieval)
- Dataset(s): Not specified in the paper.
- Domain: Text (explicitly described as "text")
- Evidence: "ability to retrieve a simple passkey" (Section 4.3.2, p. 9); "(i.e., a five-digit number) from amongst a large amount of otherwise meaningless text." (Section 4.3.2, p. 9)

### Task: ARC-Challenge (25-shot)
- Task type: Other (not specified in the paper)
- Dataset(s): ARC-Challenge
- Domain: Not specified in the paper.
- Evidence: "Specifically, we use 25-shot ARC-Challenge [11], 10-shot" (Section 4.3.3, p. 9)

### Task: HellaSwag (10-shot)
- Task type: Other (not specified in the paper)
- Dataset(s): HellaSwag
- Domain: Not specified in the paper.
- Evidence: "Specifically, we use 25-shot ARC-Challenge [11], 10-shot" (Section 4.3.3, p. 9); "HellaSwag [41], 5-shot MMLU [17], and 0-shot TruthfulQA [23]." (Section 4.3.3, p. 9)

### Task: MMLU (5-shot)
- Task type: Other (not specified in the paper)
- Dataset(s): MMLU
- Domain: Not specified in the paper.
- Evidence: "HellaSwag [41], 5-shot MMLU [17], and 0-shot TruthfulQA [23]." (Section 4.3.3, p. 9)

### Task: TruthfulQA (0-shot)
- Task type: Other (not specified in the paper)
- Dataset(s): TruthfulQA
- Domain: Not specified in the paper.
- Evidence: "HellaSwag [41], 5-shot MMLU [17], and 0-shot TruthfulQA [23]." (Section 4.3.3, p. 9)

## 4. Domain and Modality Scope
- Evaluation domain/modality: Multiple datasets within the same modality (language/LLMs). Evidence: "Transformer-based Large Language Models[40] (LLMs) have become the near-ubiquitous choice for" (Section 1, p. 2); "To evaluate the long sequence language modeling performances, we use the GovReport [18] and" (Section 4.3.1, p. 8); "Specifically, we use 25-shot ARC-Challenge [11], 10-shot" (Section 4.3.3, p. 9)
- Multiple modalities? Not specified in the paper.
- Domain generalization or cross-domain transfer? Not claimed; the paper discusses context-length extrapolation and transfer across scale factors: "We show in 4.3.1 that the s = 32 model successfully extrapolates up to 128k context using only 64k" (Section 4.2, p. 8); "This demonstrates successful transfer learning from" / "s = 16 to s = 32" (Section 4.2, p. 8)

## 5. Model Sharing Across Tasks
| Task | Shared Weights? | Fine-Tuned? | Separate Head? | Evidence |
| --- | --- | --- | --- | --- |
| Long sequence language modeling (GovReport/Proof-pile) | Not specified in the paper. | Yes. | Not specified in the paper. | "For s = 16 we fine-tuned for 400 steps with global batch" (Section 4.1, p. 7); "To evaluate the long sequence language modeling performances, we use the GovReport [18] and" (Section 4.3.1, p. 8) |
| Passkey retrieval | Not specified in the paper. | Yes. | Not specified in the paper. | "For s = 16 we fine-tuned for 400 steps with global batch" (Section 4.1, p. 7); "context window sizes ranging from 8k to 128k. Both 7b and 13b models fine-tuned using YaRN at" (Section 4.3.2, p. 9) |
| ARC-Challenge (25-shot) | Not specified in the paper. | Yes. | Not specified in the paper. | "For s = 16 we fine-tuned for 400 steps with global batch" (Section 4.1, p. 7); "Specifically, we use 25-shot ARC-Challenge [11], 10-shot" (Section 4.3.3, p. 9) |
| HellaSwag (10-shot) | Not specified in the paper. | Yes. | Not specified in the paper. | "For s = 16 we fine-tuned for 400 steps with global batch" (Section 4.1, p. 7); "HellaSwag [41], 5-shot MMLU [17], and 0-shot TruthfulQA [23]." (Section 4.3.3, p. 9) |
| MMLU (5-shot) | Not specified in the paper. | Yes. | Not specified in the paper. | "For s = 16 we fine-tuned for 400 steps with global batch" (Section 4.1, p. 7); "HellaSwag [41], 5-shot MMLU [17], and 0-shot TruthfulQA [23]." (Section 4.3.3, p. 9) |
| TruthfulQA (0-shot) | Not specified in the paper. | Yes. | Not specified in the paper. | "For s = 16 we fine-tuned for 400 steps with global batch" (Section 4.1, p. 7); "HellaSwag [41], 5-shot MMLU [17], and 0-shot TruthfulQA [23]." (Section 4.3.3, p. 9) |

## 6. Input and Representation Constraints
- Fixed or variable input resolution: Not specified in the paper.
- Fixed patch size: Not specified in the paper.
- Fixed number of tokens / sequence length constraints: "As language models are usually pre-trained with a fixed context length, it is natural to ask how to" (Section 2.2, p. 3); "PG19 dataset [29] chunked into 64k segments bookended with the BOS and EOS token." (Section 4.1, p. 7)
- Variable sequence lengths during inference: "In a lot of use cases, multiple forward-passes are performed with varying sequence lengths from 1 to" / "the maximal context size." (Section 3.3, p. 6)
- Fixed dimensionality (e.g., strictly 2D): "positional information is one-dimensional," (Section 3.1, p. 5)
- Padding or resizing requirements: Not specified in the paper.

## 7. Context Window and Attention Structure
- Maximum sequence length: "context window sizes ranging from 8k to 128k." (Section 4.3.2, p. 9); "to 128k context length" (Abstract, p. 1)
- Fixed or variable sequence length: Fixed-length training chunks ("PG19 dataset [29] chunked into 64k segments bookended with the BOS and EOS token.") (Section 4.1, p. 7); variable lengths during inference ("multiple forward-passes are performed with varying sequence lengths from 1 to" / "the maximal context size.") (Section 3.3, p. 6)
- Attention type (global/windowed/hierarchical/sparse): Not specified in the paper. One explicit setup disables windowing: "sliding window attention size was set" / "to the context window size, effectively disabling sliding window attention." (Appendix B.4, p. 18)
- Mechanisms to manage computational cost: kv-caching for reuse ("kv-caching [8] is applied so that we can reuse the previous" / "key-value vectors and improve the overall efficiency.") (Section 3.3, p. 7); Flash Attention 2 used for efficiency ("Fully Sharded Data Parallelism [42] and Flash Attention 2 [13]") (Section 4.1, p. 7)

## 8. Positional Encoding (Critical Section)
- Mechanism: RoPE (Rotary Position Embeddings). Evidence: "The basis of our work is the Rotary Position Embedding (RoPE) introduced in [34]." (Section 2.1, p. 2)
- Type: Relative positional encoding (RoPE listed among relative schemes). Evidence: "relative positional encodings are T5 Relative Bias [30], RoPE [34], XPos [35], and ALiBi [27]." (Section 1, p. 2)
- Where it is applied (input only / every layer / attention bias): Not specified in the paper.
- Fixed vs modified per task: Modified; multiple RoPE-based interpolation methods are introduced and used. Evidence: "extend the context length by slightly modifying RoPE via Position Interpolation (PI) and fine-tuning" (Section 1, p. 2); "Definition 3 By the "YaRN method", we refer to a combination of the attention scaling in Eq. 21 and" (Section 3.4, p. 7)
- Ablated or compared against alternatives: Yes, compared against other RoPE extensions. Evidence: "length via PI (LLongMA-2 7b5 ), "NTK-aware" and YaRN." (Section 4.3.1, p. 8)

## 9. Positional Encoding as a Variable
- Core research variable or fixed assumption? Core research variable. Evidence: "extend the context length by slightly modifying RoPE via Position Interpolation (PI) and fine-tuning" (Section 1, p. 2); "Definition 3 By the "YaRN method", we refer to a combination of the attention scaling in Eq. 21 and" (Section 3.4, p. 7)
- Multiple positional encodings compared? Yes. Evidence: "length via PI (LLongMA-2 7b5 ), "NTK-aware" and YaRN." (Section 4.3.1, p. 8)
- Claim PE choice is not critical or secondary? Not specified in the paper.

## 10. Evidence of Constraint Masking
- Model size(s): "For training, we extended the Llama 2 [39] 7B and 13B parameter models." (Section 4.1, p. 7)
- Dataset size(s) / training scale: "PI (s = 2)       1B           8k" and "YaRN (s = 2)      400M          8k" (Table 1, Section 4.3.1, p. 8); "For s = 16 we fine-tuned for 400 steps with global batch" (Section 4.1, p. 7)
- Performance gains attributed to scaling model size? Not specified in the paper.
- Performance gains attributed to scaling data or training steps? The paper emphasizes reduced data/steps: "requiring 10x less tokens and 2.5x less training" (Abstract, p. 1)
- Performance gains attributed to architectural/training tricks? Yes; YaRN method is emphasized: "The YaRN method combines all our findings and surpasses all previous methods in both fine-tuned" (Section 3.4, p. 7)

## 11. Architectural Workarounds
- Position Interpolation (PI) for context extension: "extend the context length by slightly modifying RoPE via Position Interpolation (PI) and fine-tuning" (Section 1, p. 2)
- NTK-aware interpolation to address high-frequency loss: "the "NTK-aware" interpolation was developed in [6]. Instead of scaling every dimension" (Section 3.1, p. 4)
- NTK-by-parts interpolation to preserve local distances: "In order to remedy this issue, given the two previous observations that we have found, we choose not" / "to interpolate the higher frequency dimensions at all while always interpolating the lower frequency" (Section 3.2, p. 5)
- Dynamic Scaling for varying sequence lengths: "multiple forward-passes are performed with varying sequence lengths from 1 to" / "the maximal context size." (Section 3.3, p. 6); "We call this inference-time method the Dynamic Scaling method." (Section 3.3, p. 7)
- Attention scaling (temperature t) as a mechanism to adjust attention: "In addition to the previous interpolation techniques, we also observe that introducing a temperature t" / "on the logits before the attention softmax has a uniform impact on perplexity regardless of the data" (Section 3.4, p. 7)
- kv-caching for efficiency: "kv-caching [8] is applied so that we can reuse the previous" / "key-value vectors and improve the overall efficiency." (Section 3.3, p. 7)

## 12. Explicit Limitations and Non-Claims
- Compute limitation on scale factor testing: "Due to compute" / "constraints, we test only s = 32 by further fine-tuning the s = 16 model for 200 steps using the same" (Section 4.2, p. 8)
- Non-claim about comparison with ReRoPE due to compatibility: "two attention passes during inference, we do not consider it for comparison." (Section 2.4, p. 4)
- Non-claim about LM-Infinite being outside embedding interpolation: "it is not an embedding interpolation method and is not immediately compatible with" (Section 2.4, p. 4)
- Future work: Not specified in the paper.
