                                                                     LieRE: Lie Rotational Positional Encodings


                                         Sophie Ostmeier 1 * Brian Axelrod * Maya Varma 1 Michael Moseley 2 Akshay Chaudhari 2 † Curtis Langlotz 2 †


                                                                 Abstract                                    dependencies crucial for tasks ranging from natural lan-
                                                                                                             guage to image understanding (Vaswani et al., 2017).
                                              Transformer architectures rely on position encod-
                                              ings to model the spatial structure of input data.             This challenge has sparked extensive research into posi-
                                                                                                             tional encodings, which inject order information into the
arXiv:2406.10322v5 [cs.CV] 16 Aug 2025




                                              Rotary Position Encoding (RoPE) is a widely used
                                              method in language models that encodes relative                otherwise order-agnostic attention mechanism. The field
                                              positions through fixed, block-diagonal, rotation              has evolved from early approaches using fixed sinusoidal
                                              matrices applied to key-query interactions. We                 positional embeddings (Vaswani et al., 2017) to more so-
                                              hypothesize that this inductive bias limits their              phisticated learned embeddings (Shaw et al., 2018a; Devlin
                                              RoPE’s effectiveness for modalities with high                  et al., 2019; Dosovitskiy et al., 2020). Recent advances
                                              dimensional structure. Lie Relative Encodings                  have introduced increasingly dynamic methods, including
                                              (LieRE) introduce a principled generalization of               relative position representations (Shaw et al., 2018b; Doso-
                                              RoPE, aimed at increasing the representational                 vitskiy et al., 2020) and rotary positional embeddings (Su
                                              capacity of positional encodings in transform-                 et al., 2024; Heo et al., 2024), demonstrating the critical role
                                              ers. Instead of fixed 2D rotations, LieRE learns               of position encoding in enabling transformers to effectively
                                              dense skew-symmetric matrices (Lie algebra el-                 process ordered sequences.
                                              ements), which are then differentiable mapped                  In particular, Rotary Position Encoding (RoPE) has emerged
                                              to form high-dimensional rotation matrices (Lie                as an elegant solution for encoding relative positional infor-
                                              group elements). This results in richer, learn-                mation in transformers (Su et al., 2024). RoPE works by
                                              able, and continuous, encodings of both relative               applying a rotation matrix to each token’s keys and queries,
                                              and absolute positional information. We demon-                 where the rotation angle depends on the token’s absolute
                                              strate the effectiveness of LieRE on 2D and 3D                 position in the sequence. The key insight is that when two
                                              vision tasks, showing that it generalizes well to              tokens interact through the attention inner product, their
                                              higher input resolutions while maintaining compu-              rotated representations naturally encode their relative dis-
                                              tational efficiency. The code and checkpoints are              tance. For example, when a token at position five attends to
                                              publicly available at https://github.com/                      a token at position two, RoPE enables the model to under-
                                              StanfordMIMI/LieRE.                                            stand that these tokens are three positions apart, regardless
                                                                                                             of their absolute positions in the sequence. This translation-
                                                                                                             invariant property makes RoPE particularly efficient at cap-
                                         1. Introduction                                                     turing position-dependent patterns in text, which has led to
                                                                                                             its adoption in popular open-source language models such
                                         The attention mechanism, particularly within transformer            as LLaMA and Mixtral.
                                         architectures, has revolutionized machine learning across
                                         diverse domains. However, attention is inherently permuta-          Despite RoPE’s success in sequential tasks (Touvron et al.,
                                         tion invariant—it cannot leverage the sequential order of its       2023; Chowdhery et al., 2023), it faces several fundamental
                                         inputs as information directly. This fundamental limitation         limitations. First, RoPE is designed for one-dimensional
                                         necessitates additional mechanisms to encode positional in-         sequence data with an exponential decay in frequencies,
                                         formation, enabling models to capture sequential and spatial        making it suboptimal for spatial relationships in images or
                                                                                                             spatiotemporal patterns in videos where distant pixels may
                                            *                        1
                                              Equal contribution       Computer Science Department,          have strong correlations. Second, when handling higher
                                         Stanford University, USA 2 Radiology Department, Stan-              dimensional data, the challenge compounds significantly–
                                         ford University, USA. Correspondence to: Sophie Ost-                learned relative position encodings must capture exponen-
                                         meier <sostm@stanford.edu>, Brian Axelrod <baxelro-
                                         dresearch@gmail.com>. † Equal senior authorship.                    tially many relative positions for n-dimensional data, as
                                         Proceedings of the 42 nd International Conference on Machine        demonstrated in 2D image data where positions must be en-
                                         Learning, Vancouver, Canada. PMLR 267, 2025. Copyright 2025         coded both horizontally and vertically (Shaw et al., 2018a).
                                         by the author(s).

                                                                                                         1
                                          LieRE: Lie Rotational Positional Encodings




Figure 1. Overview of how LieRE encodes spatial information, where N denotes the number of input dimensions, i.e. N = 3 for the
3D image. A is a learnable skew symmetric matrix and Ri = exp(A[xi yi zi ]) ∈ Rd×d is the rotation matrix for the ith patch in the
flattened input, where j corresponds to a different patch. d is the head dimension.


Third, RoPE’s reliance on sparse, block-diagonal rotation            highlighting their crucial role in spatial understanding.
matrices with handcrafted basis functions constrains its abil-
ity to learn complex spatial dependencies (Chu et al., 2024).        2. Related Work
Our key insight is that Lie group theory provides a natu-
                                                                     2.1. Position Encodings
ral framework for generalizing relative position encodings
to higher dimensions. We introduce Lie Relative Encod-               We split the review of positional encodings into: a) absolute,
ings (LieRE), which replaces RoPE’s rotation matrices with           b) relative, and c) contextual.
learned, dense rotations derived from Lie group genera-
                                                                     Absolute position in this context refers to a position with
tors. LieRE learns a basis of skew-symmetric
                                           Pnmatrices Ai ,           respect to a consistent reference, usually the start of a text or
computes rotation matrices R(p) = exp( i=0 pi Ai ) for
                                                                     top left corner of an image. Absolute encodings generally
n-dimensional positions, and applies these rotations to keys
                                                                     operate on a per token-level, modifying the embedding of
and queries in the attention mechanism. The relative posi-
                                                                     a token to encode the location of the token in the input.
tions are then naturally captured through the inner product
                                                                     Methods such as sinusoidal and learned absolute encodings
of the rotated keys and queries.
                                                                     directly add vectors to the input token embedding (Vaswani
This approach addresses RoPE’s limitations in two key ways:          et al., 2017; Devlin et al., 2019; Dosovitskiy et al., 2020).
(1) it handles higher-dimensional spatial relationships
                                                                     Relative position encodings, in contrast, encode the relative
through Lie groups and (2) it increases representational
                                                                     positions of two tokens. One strategy is to learn an set of
capacity through variably dense, learned rotation ma-
                                                                     embeddings for position deltas which can be incorporated
trices while requiring only 0.68% of parameters in a ViT-B
                                                                     into the attention mechanism (Shaw et al., 2018b; Liu et al.,
model. Importantly, LieRE can be implemented with a
                                                                     2021; 2022). However, this incurs quadratic computational
single modification to existing architectures.
                                                                     cost in terms of the number of tokens as a separate embed-
To assess the impact of LieRE and other positional encod-            ding is required for every pair of tokens. Rotary Position
ings on ViT performance, we evaluate several encoding                Encodings (RoPE) avoid this cost by rotating the key and
schemes across diverse tasks, including 2D and 3D image              query vectors before the attention inner product. The al-
classification. Additionally, we investigate a fundamental           gebraic properties of the block-diagonal rotation matrices
spatial reasoning task where models must identify where              used in RoPE ensures only relative positional information is
an arrow points. Despite their sophistication, contempo-             captured in the attention mechanism (Su et al., 2024). RoPE
rary multimodal LLMs struggle with this seemingly simple             is quite widely used in open source LLMs including the
task. Our experiments reveal that successful completion of           PaLM, Llama and Mixtral models (Touvron et al., 2023;
this task specifically requires relative position encodings,         Chowdhery et al., 2023; Jiang et al., 2024). However, RoPE


                                                                 2
                                            LieRE: Lie Rotational Positional Encodings




Figure 2. Comparisons of basis matrix A of our work, Rope-Mixed (Heo et al., 2024), VisionLlama (Chu et al., 2024). First row: learnable
parameter (yellow), not learnable parameter (blue) in basis matrix. Second row: rotation matrices exp(Ax) shared across the stem (gray)
or different for each layer and head (colorful). MHA := Multi-Head-Attention, FF := Feed Forward, L[index] := Layer index, RxH :=
Rotation Matrix per Head, LieREθ := the number of LieRE parameters.


can perform poorly on inference for larger context sizes than          cally focused on adapting RoPE to image tasks. Both Vision-
the model was trained on. This has spurred an active line of           Llama and RoPE-Mixed present relative position encodings
work extending RoPE to longer contexts, work which we                  inspired by RoPE capable of encoding 2D positional data
review later.                                                          (Chu et al., 2024; Heo et al., 2024). The primary difference
                                                                       is that RoPE-Mixed has a learnable component, whereas
We refer to the last category of positional encodings as
                                                                       VisionLlama does not.
contextual position embeddings. This category is defined
by encodings that aim to capture semantic positional in-
formation lost in traditional absolute and relative position           2.3. Efficient Scaling beyond Sequence Data
encodings, often motivated by reasoning or mathematical                There is also an extensive line of work improving the perfor-
tasks. Contextual Position encodings achieve (CoPE) this               mance of transformers for modalities with dimensionality
by allowing the model to learn how the position is computed            greater than one. Axial Attention (Ho et al., 2019) reduces
(Golovneva et al., 2024). Abacus embeddings enable trans-              computational complexity by applying attention along spe-
formers to learn how to handle arithmetic by better exposing           cific axes (e.g., rows and columns in images), enabling trans-
the digit structure of numbers (McLeish et al., 2024).                 formers to scale efficiently with high-dimensional data. Per-
                                                                       ceiver utilizes latent tokens to compress high-dimensional
2.2. Extensions of RoPE                                                inputs into a smaller set, improving scalability as input
                                                                       size and dimensionality grow. These methods address the
The efficiency and popularity of RoPE have led to several
                                                                       inefficiencies of traditional transformers when applied to
lines of work building off of it.
                                                                       high-dimensional data (Jaegle et al., 2021). Additionally,
One notable one is context extension, which aims to address            techniques like Swin and Vmamba optimize compute for vi-
the fact that RoPE NLP models trained on short documents               sual data through structuring how information flows through
tend to perform poorly on long documents. Methods like                 the network (Liu et al., 2021; 2024). Swin Transformer in-
NTK-aware context extension, YaRN and LongRoPE focus                   troduces a hierarchical approach with shifted windows, lim-
on enabling already trained models to handle long context,             iting attention to local regions to reduce complexity while
both with and without finetuning (Ding et al., 2024; Peng              capturing global context. Vmamba, on the other hand, pro-
et al., 2023; Tworkowski et al., 2024; Chen et al., 2023).             poses a visual state space model that represents images as a
                                                                       collection of spatial states, allowing attention to be applied
The final, and most relevant, line of work has been specifi-

                                                                   3
                                          LieRE: Lie Rotational Positional Encodings

efficiently across large-scale visual inputs by exploiting spa-       allowing us to strengthen the statement in (1) to
tial locality and reducing redundant computation.
                                                                      exp(U − V ) = exp(U ) exp(V )−1 = exp(V )−1 exp(U ).
                                                                                                                       (2)
2.4. Equivariant Networks
A related branch of work encoding problem structure fo-               Our work examines the tradeoff between using the stronger
cuses on equivariance. We say that a model T is equivariant           property in (2) or increased capacity and the weaker property
with respect to if T (f (x)) = g(T (x))g (Tai et al., 2019).          (1).
Where with relative position encoding we often want to
be able to encode translation invariance, equivariance pro-           3.2. Attention Mechanism
vides a more general framework. Equivariance has been                 LieRE is a modification of the standard attention mechanism
applied to improve performance on problems with a wide ar-            to introduce positional information, which we review below.
ray of structures, ranging from rotation-invariance (Esteves          The modification we propose is independent of whether we
et al., 2017; 2018; Worrall et al., 2017), 3D reference frame-        use multiple heads, so we focus on single-headed attention
invariance (Liao & Smidt, 2022; Fuchs et al., 2020) and               for simplicity.
many others. The subset of these works that focus on gen-
erating equivariant token embeddings for transformers can             Let X ∈ Rn×d be the set of input embeddings and
be combined directly with LieRE or another rotation-based             WQ , WK , Wv be learnable matrices. Let Q = XWQ , K =
position encoding.                                                    XWK , V = XWV be the keys, queries and values re-
                                                                                                                             ⊤
                                                                      spectively. The outputs are computed as scores = QK√
                                                                                                                           dk
                                                                                                                               ,
2.5. Lie Groups in Machine Learning                                   W = softmax(scores) and final outputs z = WV . We let
                                                                      Qi and Ki denote the ith rows of Q and K respectively.
Lie groups have also had extensive use in machine learning.
The range of works is diverse, ranging from algebraic sig-
nal processing (Kumar et al., 2024), automated discovery              4. Method
of symmetries (Forestano et al., 2023) to state estimation
                                                                      LieRE is a simple modification to the attention mechanism
(Falorsi et al., 2019). Furthermore (Gallier & Quaintance,
                                                                      that is presented in Algorithm 3a. Recall that we assume
2020) provides a friendly introduction to differential geom-
                                                                      that positions are n-dimensional vectors, a matrix A is skew-
etry and lie groups that may be of interest to the reader.
                                                                      symmetric if AT = −A, and that the matrix exponential
                                                                      of a skew-symmetric matrix, call it A, is always a high
3. Background                                                         dimensional rotation matrix.
3.1. Lie Groups in the Context of Attention                           When encoding positions p ∈ Rn , LieRE learns a skew-
                                                                      symmetric basis of matrices {Ai } for i ∈ [n]. It en-
In this section, we aim to provide a minimal introduction                                                           P
                                                                                                                    n
to Lie groups so that the reader is able to understand the            codes a position by writing it in this basis,   pi Ai . We
                                                                                                                     i=0
mathematical motivations behind LieRE. Lie groups are                 then map the resulting skew-symmetric matrix to a high-
well studied, especially in the context of representation             dimensional rotation via the matrix exponential. R(p) =
theory, and standard texts including (Fulton & Harris, 2013)               n      
                                                                            P
are able to provide a more extensive introduction to the              exp     pi Ai (Figure 1). Learning in the space of skew-
                                                                            i=0
subject.                                                              symmetric matrices allows us to sidestep some of the dif-
In this context, Lie groups are smooth sets of matrices that          ficulty that would come from learning on the manifold of
are closed under matrix multiplication and inversion. For             rotation matrices (Figure 2).
every Lie group, the matrix exponential provides a smooth             LieRE uses the rotation matrix computed above to modify
bijective map from a subset of Rn×n , also known as the               the keys and queries of the standard attention mechanism.
Lie Algebra, to the Lie group. The exponential map is                 LieRE’s final step is to modify token i’s query and keys as
a diffeomorphism and has the following key property for               Q′i = R(pi )Qi and Ki′ = R(pi )Ki . This modifies the score
U, V ∈ Rn×n close together:                                           between tokens i, j to be XiT WQT R(pi )T R(pj )WK Xj . Re-
                                                                      calling that RT = R−1 for any orthogonal matrix R helps
 exp(U − V ) = exp(−V + U ) ≈ exp(V )−1 exp(U ) (1)                   illustrate the encoding of relative positions in (1). Note that
                                                                      the only difference between LieRE and RoPE-Mixed is that
                                                                      the latter constrains the rotations to be block-diagonal with
Both RoPE (in the context of text) and RoPE-Mixed use
                                                                      block size two.
block-diagonal rotation matrices with 2D rotations as blocks
3b. These form a special Lie group that is commutative,               We include the pseudocode for the LieRE attention in Algo-

                                                                  4
         Under
        Under   review
              review as as a conference
                        a conference    paper
                                     paper    at ICLR
                                           at ICLR    2025
                                                    2025

                                                            LieRE: Lie Rotational Positional Encodings

  216
216       Algorithm
        Algorithm     1 LieRE
                    1 LieRE      Attention
                              Attention                                   Algorithm
                                                                       Algorithm         2 RoPE
                                                                                    2 RoPE           Attention
                                                                                                Attention
  217
217             procedure                                                      procedure
  218
218
         1: 1:procedure L IELRE_R
                             IE RE_R         (p,(p,
                                      OTATIONS
                                    OTATIONS     A)A)                   1: 1:
                                                                            procedure     ROR    O PE(X,
                                                                                               PE(X,
                                                                                                 1
                                                                                                              p, d)
                                                                                                          p, d)
         2: 2:     d →  dimension(p)
                 d → dimension(p) ! ! p # #                             2:  2:     ω  →
                                                                                ω → 10000   1               ↓i
                                                                                                   2i/d↓i ↔ [0, ↔ d)
                                                                                                                   [0, d)
  219                                                                                      10000
                                                                        3: 3: forfor       0i to0 dtostep
                                                                                                      d step    2 do
219                                      p "
                                                                                              2i/d

                   return  matrix_exp" A A                                          i →i →                  2 do
  220
220      3: 3: return   matrix_exp           p i pii i                  4:  4:                          i
                                                                                        iXrot →i X cosipω ↗i+1
                                                                                    Xrot →
                                                                                                                  i
                                                                                                X cos pω ↗ X X sin sin
                                                                                                                              i+1
                                                                                                                                   pωi pω
                                                                                                                                          i
  221                                           i=0                                         i+1
221                                          i=0                                                          i          i i+1i+1               i
                end
         4: 4:end    procedure
                  procedure                                             5: 5:       Xrot Xrot→ →
                                                                                        i+1
                                                                                                   X iXsin sin
                                                                                                            pωipω+X     + X coscos   pωi pω
  222
222                                                                         6:     end
                                                                                     forfor
  223
223
                procedure
         5: 5:procedure    L IELRE_A
                                 IE RE_A   TTENTION
                                        TTENTION    (Q,(Q, K,K,   V, A) 6: 7: endreturn
                                                               V, A)                          Xrotated
                           tokenPositions                               7:      return Xrotated
  224
224      6: 6: p →  p→  tokenPositions                                      8: end  procedure
                                                                        8: end procedure
  225
225      7: 7: R R  →→   L IELRE_R
                               IE RE_R    OTATIONS
                                       OTATIONS   (p,(p,A)A)
                    // Multiply
         8: 8: // Multiply      eacheach
                                       keykey
                                           andand  query
                                                query             by by 9: 9:
                                                              vector
                                                          vector               procedure
                                                                            procedure     ROR    O PE_A
                                                                                               PEA           TTENTION
                                                                                                      TTENTION         (Q, K,  (Q,VK,
                                                                                                                                    ) V)
  226
226
             thethe rotation
                 rotation    forfor  that
                                  that    token.
                                       token.                          10: 10:     p  →   tokenPositions
                                                                                p → tokenPositions
  227
227
         9: 9:      K      →    B ATCH   M  AT
                  Krot → BATCH M AT M UL(R, K)
                       rot                     M UL (R,    K)          11: 11:  d→ d→     embeddingDimension
                                                                                      embeddingDimension
  228
228
        10:10:      Q
                  Qrot rot →    B ATCH   M AT M  UL
                        → BATCH M AT M!UL!(R, Q)# # (R,    Q)          12: 12:  Krot → ROR
                                                                                   K  rot →       O PE(K,
                                                                                                PE(K,         p, d)
                                                                                                          p, d)
  229
229                                                                        13:     Q      →    R    PE(Q,
                                                                                      rot RO PE(Q, p, d) d)
                                                                                                  O           p,
                                                   Q TK    T           13:      Qrot →                        $ $ Qrot       % %
  230
230
                    Attention
        11:11: Attention       →→     softmax↑Qrot↑Krotrot rot V V
                                   softmax                                                                               T KT
                                                     dim(K)
                                                 dim(K)                            Attention
                                                                       14:14: Attention       →→       softmax → d rotV V
                                                                                                   softmax
                                                                                                                 Q rot K  →
                                                                                                                         rot
  231
231                 return     Attention                                                                               d
        12:12: return      Attention                                               return
                                                                       15:15: return          Attention
                                                                                          Attention
  232           end  procedure
232     13:13:end procedure                                                 endend
                                                                       16:16:       procedure
                                                                                 procedure
  233
233
  234
234         (a) Lie Rotary Embedding (LieRE) attention mechanism.                        (b) Rotary Position Embedding (RoPE) attention mechanism.
  235
235
                                         Figure 3. Comparison of the LieRE and RoPE-Mixed attention mechanisms.
  236
236
  237
237     ByBy default,
           default, thethe skew
                         skew   bases
                              bases areare learned
                                        learned    separately
                                                separately forfor every
                                                               every    layer
                                                                     layer andand attention
                                                                               attention    head
                                                                                         head    except
                                                                                              except     in the
                                                                                                     in the
  238
238         experimental
         experimental
        Table   1. 2D imagesection
                              and section
                                     3D videofocused
                                         focused     on on    sharing
                                                          sharing
                                               classification            parameters
                                                                Top-1parameters
                                                                       Accuracy          across
                                                                                    across
                                                                                     Table        heads
                                                                                            2.heads   and
                                                                                               P-values  and  layers.
                                                                                                           layers.
                                                                                                        comparing  RoPE-Mixed and LieRE8 across
  239
239     (95% confidence intervals) results. All models use 85.2M parame-              datasets.
  240   tersAdjusting
         Adjusting
             for 2D tasks   the
                         theand    skew-symmetric
                               skew-symmetric
                                  88.7M  parameters forbasis basis
                                                            3D       matrices’
                                                                 matrices’
                                                               task (Krizhevsky    block
                                                                               block        width
                                                                                         width
                                                                                      Dataset       allows
                                                                                                 allows      us incrementally
                                                                                                         us to   to incrementally        adjust
                                                                                                                                     adjust
                                                                                                                      P-value (RoPE-Mixed  vs.the the8 )
                                                                                                                                               LieRE
240
            capacity
        etcapacity
           al., 2009; Dengallocated
                      allocated
                             et al.,     towards
                                      towards
                                     2009; Soomro    position
                                                 position
                                                    et al., 2012;  encoding.
                                                               encoding.
                                                                  Flanders et WeWe
                                                                              al.,      specify
                                                                                   specify    thethe
                                                                                      CIFAR-100        basis
                                                                                                   basis     block
                                                                                                          block      width
                                                                                                                 width   as aassubscript,
                                                                                                                                  a subscript,
                                                                                                                                 1.3 × 10−5    eg. eg.
  241
241
            LieRE
        2020)           . When       not specified,thethe     block
                                                                  sizesize is equal     to the  head  dimension.    If we
                                                                                                                       set set
                                                                                                                            the the
                                                                                                                                 6.3 ×block   size
                                                                                                                                                to to
                 ∗
         LieRE     equivalent   to
                                 notDeiT  architecture,  ∗∗
                                                             equivalent to equal
                                                                           Vivit
                   8. 8
                      When            specified,         block          is         to ImageNet-1k
                                                                                       the  head  dimension.    If we            block 10size
                                                                                                                                         −3

  242
242     (spatio-temporal) architecture.                                               UCF101                                     7.1 × 10−4
        2, 2,
           wewe recover
              recover   RoPE-Mixed
                      RoPE-Mixed   (Heo
                                 (Heo    et al.,
                                      et al.,    2024).
                                              2024).                                   384 × 384 (Resolution Invariance)              4.0 × 10−4
  243
243      Method               CIFAR-100          ImageNet-1k        UCF101

  244
244      Abs. Pos. E.∗,∗∗
                          63.9 (62.9-65.8) 66.1 (65.7-66.5)         40.9 (40.5-41.3)

  245
245     5 5 EE
         VisionLlama RoPE 65.5 (64.6-66.5) 65.4 (65.0-65.8)
         RoPE-Mixed   XPERIMENTS
                  XPERIMENTS
                          68.8 (67.9-69.7) 68.8 (68.4-69.2)
                                                                    45.0 (44.6-45.4)
                                                                    46.3 (45.9-46.7)
  246
246      LieRE8               70.3 (69.4-71.2)   69.6 (69.2-70.0)   47.0 (46.6-47.4)
  247
247      In  In
          LieRE  order
              order
                64     to to   isolate
                           isolate    thethe     effect
                                            effect
                                 70.0 (69.1-70.9)         of  changing
                                                      of(68.9-69.7)
                                                    69.3  changing    44.7the the    position
                                                                                position
                                                                           (44.3-45.1)           encoding,
                                                                                             encoding,
                                                                                             5. Experimentswe weuseuse   a plain
                                                                                                                     a plain        standard
                                                                                                                                standard        transformer
                                                                                                                                            transformer
                                                         1 1
  248
248          backbone
         backbone        andand     training
                                training          regimewith
                                              regime           with    comparable
                                                                   comparable             baseline
                                                                                       baseline       performance
                                                                                                   performance      on on    CIFAR100
                                                                                                                        CIFAR100         andand    ImageNet
                                                                                                                                              ImageNet
         withwith(Lee
                    (Lee et al.,  2022).
                             et al.,  2022).  WeWe  implement
                                                        implement    relative
                                                                         relative position   We   evaluate
                                                                                              encoding
                                                                                       position   encoding   LieRE
                                                                                                           types     across
                                                                                                                  including
                                                                                                               types          a LieRE,
                                                                                                                      including  range  ofRope-Mixed,
                                                                                                                                     LieRE,tasks  to assess its
                                                                                                                                               Rope-Mixed,
249
  249                                                                                        impact onsizes
                                                                                                         transformer
250
  250
         Visionllama,
             Visionllama,      andand absolute.
                                           absolute.WeWe      useuse thethestandard
                                                                                standard  backbone
                                                                                              backbone              of performance
                                                                                                                 of ViT-Tiny,
                                                                                                             sizes      ViT-Tiny,       in 2D
                                                                                                                                    ViT-B
                                                                                                                                        ViT-B  andViT-L
                                                                                                                                             and and 3D vision,
                                                                                                                                                        ViT-L
                                                                                             spatial reasoning, and resolution generalization. Our goal
         (Dosovitskiy
             (Dosovitskiy      et al.,
                                  et    2020).
                                      al.,  2020).  AllAllexperiments
                                                              experiments     useuse RandAugment
                                                                                         RandAugment     (Cubuk
                                                                                                              (Cubuket al.,
                                                                                                                        et   2020).
                                                                                                                            al.,       WeWe
                                                                                                                                  2020).    avoid   using
                                                                                                                                                avoid   using
251
  251   rithm 3a in addition to the standard RoPE attention (Algo-                           is to isolate the contribution of positional encodings by
         pre-trained
             pre-trained    weights
                                weights   in   order
                                              in   orderto maximize
                                                           to   maximize
        rithm 3b). In practice, we compute the rotation matrices at
                                                                             the   comparability
                                                                                  the   comparability  of   results
                                                                                                            of       between
                                                                                                                results  between   methods.
                                                                                                                                       methods. In  order
                                                                                             using consistent architectures, training procedures, andIn order
                                                                                                                                                            hy-
252
  252    to   ensure
             to  ensure  a  fair
                             a    comparison,
                                fair   comparison,      we weexplicitly
                                                                 explicitly
        the start of the forward pass. By default, the skew symmet-          avoidavoid tuning
                                                                                           tuninghyperparameters
                                                                                                     hyperparameters    and  anduse  the
                                                                                                                                    use   same
                                                                                                                                         the  samedefault
                                                                                                                                                      default
                                                                                             perparameters across all methods. To this end, we utilize a
253
  253    hyperparameters
        ric  bases   are learned for
             hyperparameters              all all
                                        for
                                    separately experiments
                                                   experiments
                                                   for every layer(Appendix
                                                                      (Appendix
                                                                      and attention B).B).
                                                                                         Westandard
                                                                                             Weevaluate   twotwo
                                                                                                   evaluate
                                                                                                       recipe   versions
                                                                                                               and versions
                                                                                                                    build onoftop
                                                                                                                                LieRE,the distinguished
                                                                                                                                 ofofLieRE,   distinguished
                                                                                                                                          vision  transformers
254      byby the   basis
  254   head      the
                except    inmatrix
                        basis thematrix tiletile
                                              sizes
                                   experimental       of
                                                  sizes   64 64
                                                          of
                                                      section  and    8, referred
                                                                    and
                                                                 focused  8,on    shar-to as
                                                                              referred        LieRE
                                                                                           torepository
                                                                                               as LieRE    and  LieRE
                                                                                                           64 and
                                                                                                      64 (Yoshioka,     8 , respectively.
                                                                                                                    LieRE
                                                                                                                       2024) 8 ,and
                                                                                                                                 respectively.Notably,
                                                                                                                                     verify that         a a
                                                                                                                                                  Notably,
                                                                                                                                                 our  baselines
255
  255    tiletile
        ing    size   of of
                   size
              parameters   2 corresponds
                               2 corresponds
                              across    heads and to RoPE-Mixed.
                                                      to  RoPE-Mixed.
                                                       layers.       Adjusting the           perform similarly to prior work (Lee et al., 2022). We avoid
256
  256   skew-symmetric basis matrices’ block width allows us to            using pre-trained weights in order to help reproducibility
257
  257   incrementally  adjust
         5.15.1 DATASETS      the capacity
                                     TASKS allocated  towards  position    and comparability of the results between methods. We pro-
                  DATASETS     ANDAND    TASKS
258
  258
        encoding. We specify the basis block width as a subscript,         vide confidence interval estimates using bootstrap (B=1000,
        eg.
         Our LieRE 8 . When are
              experiments     not designed
                                   specified, the  block sizethe
                                              to evaluate      is equal
                                                                  efficacy α =
                                                                           of      0.05). We
                                                                               LieRE           evaluate  two versions ofencoding
                                                                                                                             LieRE, distin-
259
  259       Our  experiments     are designed     to evaluate    the efficacy  of LieRE64 andandLieRE  8 as8 aasposition
                                                                                                    LieRE           a position  encoding
        to the head dimension. If we set the block size to 2, we           guished by the64basis matrix block-diagonal      sizes of 64 and
         across both
            across both2D2Dandand3D3Ddata.  WeWe
                                         data.   evaluate
                                                     evaluate LieRE
                                                                 LieREon on
                                                                         thethe
                                                                             classification    of 2D  (images)      andand
                                                                                                                        3D 3D(videos)
260
  260   recover RoPE-Mixed (Heo et al., 2024). For a ViT-B model           8, referred to as LieRE64 and LieRE8 , respectively.(videos)
                                                                                 classification    of 2D   (images)                Notably,
261
  261
         data. For
            data.   UCF101
                  For  UCF101   (3D)(Soomro
        the head dimension is 64.  (3D)(Soomro   et  al.,
                                                     et   2012)
                                                         al.,     and
                                                              2012)    ImageNet-1k
                                                                     and  ImageNet-1k   (2D)  (Deng
                                                                           a tile size of 2 (2D)
                                                                                                      et to
                                                                                                  (Deng
                                                                                            corresponds
                                                                                                          al., 2009),
                                                                                                           etRoPE-Mixed.
                                                                                                               al.,     we we
                                                                                                                    2009),  focus   on on
                                                                                                                                 focus
262     accuracy.
          accuracy.ForFor
                       CIFAR-100  (2D),
                          CIFAR-100      where
                                     (2D),     training
                                           where        is less
                                                  training       resource
                                                            is less       intensive,
                                                                    resource         we we
                                                                              intensive, alsoalso
                                                                                               evaluate LieRE’s
                                                                                                  evaluate LieRE’s
  262
263
        data and training compute efficiency. The baseline
          data and training compute efficiency. The baselineperformance    of UCF101    dataset
                                                            5 performance of UCF101 dataset withwith
  263
264
  264
265
        5.1.1
          5.1.1 2D2D
                   C LASSIFICATION
                     C LASSIFICATION
  265
266
  266   ForFor
            2D2D
               data wewe
                       evaluate performance on on
                                               thethe
                                                   CIFAR-100 andand
                                                                 ImageNet-1k image classification tasktask
                 data    evaluate performance         CIFAR-100     ImageNet-1k image classification
                                                 LieRE: Lie Rotational Positional Encodings

5.1. 2D Image Classification
                                                                         Table 3. Comparing different positional encodings on synthetic
We begin with CIFAR-100 and ImageNet-1k benchmarks                       task on base model (85M)∗ equivalent to DeiT architecture across
to evaluate LieRE in 2D vision tasks. All models use ViT-                different resolutions, 108 × 108, 168 × 168, 276 × 276
                                                                          Method                        108                168                276
based architectures trained from scratch with standard data
                                                                          Abs. Pos. E.∗                 45.1 (43.1-47.7)   41.0 (39.0-43.2)   40.1 (38.2-42.0)
augmentations (RandAugment). We compare LieRE to ab-                      Abs. Pos. E.∗ (2M examples)   47.2 (45.2-49.1)   -                  -
solute positional encodings, RoPE-Mixed (Heo et al., 2024),               VisionLlama RoPE              46.4 (44.0-47.9)   48.6 (46.0-49.9)   48.6 (46.2-50.1)
                                                                          RoPE-Mixed                    100 (99.5-100)     98.6 (97.6-98.7)   88.6 (87.4-89.9)
and VisionLLaMA (Chu et al., 2024). Table 1 shows that
                                                                          LieRE8                        99.5 (99.2-99.8)   99.7 (99.4-99.9)   99.7 (99.5-99.9)
LieRE outperforms all baselines. On CIFAR-100, LieRE8                     LieRE64                       100 (99.5-100)     100 (99.6-100)     100 (99.7-100)
achieves a statistically significant relative improvement in
top-1 accuracy relatively of 10.0% over absolute encodings,
7.3% over VisionLLaMA, and 2.2% over RoPE-Mixed. Sim-                    5.2. Synthetic Spatial Reasoning Task
ilar trends hold on ImageNet (Table 2).
We also investigate performance across model sizes (ViT-
Tiny, Base, Large). As shown in Table 6 and Figure 8,
LieRE8 consistently outperforms other methods.




                    70                                                   Figure 5. The model is asked to identify the direction of the arrow
                                                                         pointed to by the base of the Y. Here said arrow points down. This
                    65                                                   task requires understanding spatial relationships.
                    60
     Max Accuracy




                    55                                                   Recent observations indicate that even advanced models like
                                                                         ChatGPT-4 and Claude Sonnet 3.5 struggle with basic spa-
                    50                             LieRE_8               tial reasoning tasks. To investigate whether these limitations
                                                   LieRE_64              stem from positional encoding mechanisms, we designed a
                    45                             RoPE-Mixed
                                                   VisionLlaMA           synthetic image classification task (Shah et al., 2024). The
                    40                             Absolute PE           task presents a 108×108 pixel image containing a 9×9 grid
                         0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0             (81 cells). Within this grid, we randomly place eight arrows
                                    Relative Dataset Size                and six letters (A, B, C, D, E, and Y), ensuring that one
                                                                         arrow is placed in the direction of the base of the ”Y”. The
Figure 4. Performance comparison of different positional embed-          objective is to identify the direction of this specific arrow.
ding methods across varying dataset sizes. The plot shows the            To introduce visual complexity, we include four spurious
peak accuracy achieved by LieRE8 , LieRE64 , RoPE-Mixed, Vi-             letters and seven additional arrows as distractors. Figure 5
sionLlaMA, and Absolute PE when trained on different fractions           illustrates an example of this task setup. We train the mod-
of the CIFAR-100 dataset. Both LieRE variants and RoPE-Mixed             els on 800,000 examples and observe that they generally
consistently outperform other methods, with their advantage be-          converge after the first 400,000 examples.
coming particularly pronounced in data-scarce scenarios.
                                                                         We verified that none of ChatGPT 4o, Claude Sonnet 3.5
                                                                         and Gemeni Pro 1.5 are able to solve this task.
                                                                         Table 3 outlines the performance of different positional en-
                                                                         coding methods on a synthetic task using a base model
                                                                         (85M parameters) equivalent to the DeiT architecture. We
                                                                         evaluate the models across three different input resolutions
To evaluate robustness in low-data regimes, we perform a                 (108 × 108, 168 × 168, and 276 × 276 pixels), revealing
data ablation study. Figure 4 shows that LieRE variants and              differences in scalability and effectiveness. While absolute
RoPE-Mixed maintain significantly higher accuracy than                   positional encoding shows degraded performance as reso-
baselines when training on only 20–90% of the CIFAR-100                  lution increases (from 45.1% to 40.1%) and RoPE-Mixed
dataset. This highlights LieRE’s data efficiency.                        demonstrates strong but degrading performance at higher

                                                                     6
                                              LieRE: Lie Rotational Positional Encodings

                                                                                                                   CIFAR100                                                                     47.5
                                                                                                                   UCF101
Table 4. Accuracy with parameter sharing over heads and layers                                           71.0
                                                                                                                                                                                                47.0
for ViT-B sized models on CIFAR-100.                                                                     70.5
                   Shared        Shared




                                                                                 CIFAR100 Accuracy (%)
FLOP All Shared                            RoPE-Mixed LieRE64 LieRE8                                                                                                                            46.5




                                                                                                                                                                                                  UCF101 Accuracy (%)
                Across Layers Across Heads                                                               70.0
5.684G               ✓            ✓          68.8      70.0   70.3                                                                                                                              46.0
5.684G               ✓                       68.7      69.5   69.8                                       69.5
5.613G                            ✓          69.5      69.7   69.7                                                                                                                              45.5
5.613G    ✓                                  68.3      69.4   69.5                                       69.0
                                                                                                                                                                                                45.0
                                                                                                         68.5
                                                                                                                                                                                                44.5
                                                                                                         68.0 RoPE-Mixed                         LieRE8                               LieRE64
resolutions (from 100% to 88.6%), both LieRE variants                                                           9,216                27,648      64,512      138,240     285,696      580,608
                                                                                                                                              LieRE Parameter
maintain near-perfect accuracy across all resolution scales,
with LieRE64 achieving 100% accuracy consistently. Quali-                  Figure 6. Performance varies with skew-symmetric basis learnable
tative analysis shows that absolute and visionllama position               dimensions, referred to as LieRE parameters. This is equivalent
encoding attend less clearly to the ”Y” token than RoPE-                   to increasing the tile size in the skew-symmetric basis (2 × 2, 4 ×
Mixed and LieRE. Please refer to the appendix for attention                4, 8 × 8, 16 × 16, 32 × 32, 48 × 48). For both 2D (CIFAR-100)
map examples, Figure 10 and Figure 12.                                     and 3D (UCF101) LieRE with tile size 8 × 8 performs superior.


5.3. 3D Classification
                                                                           Table 5. Relative accuracy drop for 2D image classification
To assess LieRE’s performance on 3D data, we use the                       (CIFAR-100) and Video recognition (UCF101) after patch shuf-
UCF101 video classification benchmark (Soomro et al.,                      fling
                                                                                                                                 CIFAR-100 (2D)                                     UCF101 (3D)
2012). All models use a ViT-style backbone with 3D patch                    Method                                       Before          After      Drop(%)             Before         After                Drop(%)
tokenization, trained from scratch with no hyperparameter                                                               Shuffling↑     Shuffling↓      ↑               Shuffling↑    Shuffling↓                ↑
                                                                            Abs. Pos. E.                                  63.9            19.6        69.3               40.9           39.5                             0.0
tuning and the dataloader from (Tong et al., 2022). The                     VisionLlama RoPE                              65.5            29.7        54.8               45.0           37.0                            17.7
                                                                            RoPE-Mixed                                    68.8            17.1        75.1               46.3           28.2                            39.1
full set of hyperparameters may be found in appendix B.1.                   LieRE8                                        70.3            12.3        82.5               47.0           27.8                            40.9
We observe a relative accuracy improvement of the LieRE-                    LieRE64                                       70.0            10.8        84.6               44.7           28.0                            37.4

based transformer of up to 15.1% compared to absolute
position embeddings and at least 1.5% compared to RoPE-
inspired position encodings (table 1).                                     Mixed.

5.4. LieRE Capacity and Parameter Efficiency                               5.5. Patch shuffling: Measuring Positional Dependency

LieRE introduces minimal overhead–—only 580k addi-                         Shuffling patches and frames allows us to see how much the
tional parameters (0.68% for ViT-B)–—yet offers a flex-                    model is able to use the positional information in its inputs.
ible mechanism for increasing representational capacity. To                A model whose architecture does not allow/encourage the
explore the impact of this marginal capacity, we vary the                  use of positional information will converge to a representa-
density of the skew-symmetric basis and examine parameter                  tion similar in spirit to a bag-of-words, where the relative
sharing across heads and layers.                                           locations of pixels/voxels do not matter. A greater drop-off
                                                                           in accuracy during shuffling is indicative that the model
We control capacity via imposing a block-diagonal structure                more heavily utilizes positional information.
on the basis matrices. Smaller blocks (e.g., 2 × 2) repli-
cate RoPE-Mixed, while larger blocks increase expressivity,                We evaluate models using the decline in accuracy when
with LieRE64 using a fully dense basis. We adopt RoPE-                     evaluating on shuffled patches. We observe the most sig-
Mixed’s initialization for fair comparison. We observe that                nificant decline LieRE-based transformers, leading to the
RoPE-Mixed is more sensitive to initialization than LieRE                  conclusion that LieRE models rely more on positional infor-
(Appendix Table 7).                                                        mation as expected. The complete results are displayed for
                                                                           CIFAR-100 and table for UCF101 (table 5).
Figure 6 shows performance as a function of block size.
Both in 2D (CIFAR-100) and 3D (UCF101), accuracy im-
                                                                           5.6. Multi-resolution Classification
proves with block size, peaking around 8 × 8—–suggesting
this is a sweet spot between capacity and regularization.                  In this section we compare the ability of methods to gen-
                                                                           eralize to image resolutions not seen during training. We
We also assess the effect of parameter sharing on CIFAR100
                                                                           evaluate two training recipes inspired by (Heo et al., 2024).
(Table 4). Learning LieRE parameters independently per
head and per layer yields the best results. Sharing across lay-            The first recipe matches the rest of the paper and consists
ers or heads reduces accuracy, but still outperforms RoPE-                 of training the models on images of size 224 × 224 for 200

                                                                       7
                                                                                   LieRE: Lie Rotational Positional Encodings

                                                                     ViT-B: Training at 224x224                          ViT-B: Pre-training at 224x224, Fine-tune at 256x256
                                    74

                                    72
          Validation Accuracy (%)



                                    70

                                    68
                                               Training Resolution




                                                                                                                                     Training Resolution
                                    66

                                    64
                                                                                                      LieRE 64                                                                     LieRE 64
                                                                                                      RoPE-Mixed                                                                   RoPE-Mixed
                                         2

                                              4

                                                                     6



                                                                                0



                                                                                                  4



                                                                                                            8

                                                                                                                      2

                                                                                                                            4

                                                                                                                                   6



                                                                                                                                                                  0



                                                                                                                                                                              4



                                                                                                                                                                                         8
                                         19

                                              22

                                                                 25



                                                                              32



                                                                                             38



                                                                                                          44

                                                                                                                    19

                                                                                                                          22

                                                                                                                                 25



                                                                                                                                                                32



                                                                                                                                                                              38



                                                                                                                                                                                       44
                                                                         Image Resolution                                                                  Image Resolution

Figure 7. Validation accuracy comparison between LieRE 64 and RoPE-Mixed positional embeddings across different image resolutions
on ImageNet. Left: Models trained at 224 × 224 resolution. Right: Models pre-trained at 224 × 224 and fine-tuned at 256 × 256
resolution. Both approaches show similar performance trends up to 320 × 320, after which LieRE 64 demonstrates significantly better
accuracy retention at higher resolutions, particularly in the fine-tuned scenario.


epochs. The second adds an additional fine-tuning step at                                                          coding methods. Our analysis indicates that LieRE-based
size 256 × 256 for 30 epochs. The full details can be found                                                        ViTs effectively leverage spatial reasoning capabilities un-
in appendix B.1.                                                                                                   available to transformers using only absolute positional en-
                                                                                                                   codings. In addition to accuracy gains, LieRE offers notable
We evaluate the accuracy on the ImageNet validation set
                                                                                                                   data and compute efficiency. Its simplicity, flexibility, and
with varying inference resolutions. Specifically, we scale
                                                                                                                   strong capacity to learn spatial structure make it broadly
the input images to resolutions of 196 × 196, 256 × 256,
                                                                                                                   applicable. With no tokenizer modifications beyond posi-
320 × 320, 384 × 384, and 448 × 448 pixels per dimension,
                                                                                                                   tion output and no additional architectural changes, LieRE
and present the resulting accuracies in figure 7.
                                                                                                                   provides simple approach for controlling the amount of
For position assignment, we adopt a sequential approach                                                            positional information into transformers.
where token positions are scaled proportionally to the image
dimensions. For example, doubling the length of an image                                                           7. Limitations
in each dimension doubles the range of positional indices.
This method outperforms rescaling positions to a fixed range,                                                      While LieRE shows promising results for 2D and 3D inputs,
as demonstrated by superior results for both RoPE-Mixed                                                            several limitations are worth noting. For 1D input, LieRE re-
and LieRE across the evaluated training recipes.                                                                   duces to RoPE with learnable phases (proof in appendix A).
                                                                                                                   Our method is designed to modify the inner product, making
6. Conclusion                                                                                                      it compatible with most attention mechanisms, including
                                                                                                                   standard softmax attention and linear attention. However,
We proposed Lie group Relative position Encodings                                                                  this may limit its applicability to other architectures—–such
(LieRE), a positional encoding method that modifies the                                                            as convolutional neural networks—–that do not rely on the
attention method via dense, learned, high-dimensional rota-                                                        attention mechanism. Future work could adapt the method
tion matrices. As compared to the more widely used block-                                                          to a broader range of architectures. The current formula-
2D rotation matrices typically used to encode positions,                                                           tion encodes vector positions in Rd . While sufficient for
dense rotation matrices can encode both relative and abso-                                                         many applications, it may not directly apply to tasks that
lute positional information and allow a greater portion of                                                         require pose encoding in SE(3) (e.g., robotics). Lastly, in
the model’s learnable capacity be allocated to spatial reason-                                                     its current implementation, LieRE relies on the accuracy
ing. Experiments on 2D image classification (CIFAR-100,                                                            and numerical stability of the matrix exponential in PyTorch.
ImageNet-1k) and 3D video classification (UCF101) show                                                             Future work may explore more efficient and robust imple-
that LieRE consistently outperforms existing positional en-                                                        mentations or approximations of this operation. Despite


                                                                                                              8
                                          LieRE: Lie Rotational Positional Encodings

these limitations, we believe our approach provides valuable          Devlin, J., Chang, M.-W., Lee, K., and Toutanova, K. Bert:
insight into improving model performance and reducing                   Pre-training of deep bidirectional transformers for lan-
training costs by encoding relative positional information              guage understanding. In Proceedings of the 2019 Confer-
across various input dimensionalities.                                  ence of the North American Chapter of the Association for
                                                                        Computational Linguistics: Human Language Technolo-
Impact statement                                                        gies, Volume 1 (Long and Short Papers), pp. 4171–4186,
                                                                        2019.
This paper presents work whose goal is to advance the field
of Machine Learning. There are many potential societal                Ding, Y., Zhang, L. L., Zhang, C., Xu, Y., Shang, N., Xu, J.,
consequences of our work, none which we feel must be                    Yang, F., and Yang, M. Longrope: Extending llm context
specifically highlighted here.                                          window beyond 2 million tokens. 2024.

                                                                      Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn,
Acknowledgements                                                        D., Zhai, X., Unterthiner, T., Dehghani, M., Minderer, M.,
                                                                        Heigold, G., Gelly, S., et al. An image is worth 16x16
We would like to thank Maksim Maydanskiy for helping us                words: Transformers for image recognition at scale. arXiv
understand Lie groups and Lie group representations. We                 preprint arXiv:2010.11929, 2020.
would like to thank Aradhana Sinha for suggesting the shuf-
fling experiment and providing early feedback on the paper.           Esteves, C., Allen-Blanchette, C., Zhou, X., and Dani-
Furthermore, lucidrains suggested adding the proof of that              ilidis, K. Polar transformer networks. arXiv preprint
in the 1D setting, LieRE has identical representational ca-             arXiv:1709.01889, 2017.
pacity to RoPE up to the learnable basis.Sophie Ostmeier
was supported by the German Research Foundation (DFG),                Esteves, C., Allen-Blanchette, C., Makadia, A., and Dani-
Walter-Benjamin fellowship (ID: 517316550). This project                ilidis, K. Learning so (3) equivariant representations with
was supported by Google Cloud and Azure credits. This                   spherical cnns. In Proceedings of the european confer-
work was also supported in part by the Medical Imaging and              ence on computer vision (ECCV), pp. 52–68, 2018.
Data Resource Center (MIDRC), which is funded by the Na-              Falcon, W. A. Pytorch lightning. GitHub, 3, 2019.
tional Institute of Biomedical Imaging and Bioengineering
(NIBIB) under contract 75N92020C00021 and through the                 Falorsi, L., de Haan, P., Davidson, T. R., and Forré, P. Repa-
Advanced Research Projects Agency for Health (ARPA-H).                  rameterizing distributions on lie groups. In The 22nd
                                                                        International Conference on Artificial Intelligence and
References                                                              Statistics, pp. 3244–3253. PMLR, 2019.

Chen, S., Wong, S., Chen, L., and Tian, Y. Extending                  Flanders, A. E., Prevedello, L. M., Shih, G., Halabi, S. S.,
  context window of large language models via positional                Kalpathy-Cramer, J., Ball, R., Mongan, J. T., Stein, A.,
  interpolation. arXiv preprint arXiv:2306.15595, 2023.                 Kitamura, F. C., Lungren, M. P., et al. Construction of
                                                                        a machine learning dataset through collaboration: the
Chowdhery, A., Narang, S., Devlin, J., Bosma, M., Mishra,               rsna 2019 brain ct hemorrhage challenge. Radiology:
  G., Roberts, A., Barham, P., Chung, H. W., Sutton, C.,                Artificial Intelligence, 2(3):e190211, 2020.
  Gehrmann, S., et al. Palm: Scaling language modeling
  with pathways. Journal of Machine Learning Research,                Forestano, R. T., Matchev, K. T., Matcheva, K., Roman, A.,
  24(240):1–113, 2023.                                                  Unlu, E. B., and Verner, S. Deep learning symmetries
                                                                        and their lie groups, algebras, and subalgebras from first
Chu, X., Su, J., Zhang, B., and Shen, C. Visionllama: A                 principles. Machine Learning: Science and Technology,
  unified llama interface for vision tasks. arXiv preprint              4(2):025027, 2023.
  arXiv:2403.00522, 2024.
                                                                      Fuchs, F., Worrall, D., Fischer, V., and Welling, M. Se
Cubuk, E. D., Zoph, B., Shlens, J., and Le, Q. V. Ran-                  (3)-transformers: 3d roto-translation equivariant attention
  daugment: Practical automated data augmentation with a                networks. Advances in neural information processing
  reduced search space. In Proceedings of the IEEE/CVF                  systems, 33:1970–1981, 2020.
  conference on computer vision and pattern recognition
  workshops, pp. 702–703, 2020.                                       Fulton, W. and Harris, J. Representation theory: a first
                                                                        course, volume 129. Springer Science & Business Media,
Deng, J., Dong, W., Socher, R., Li, L.-J., Li, K., and Fei-Fei,         2013.
  L. Imagenet: A large-scale hierarchical image database.
  In 2009 IEEE conference on computer vision and pattern              Gallier, J. Q. and Quaintance, J. Differential geometry and
  recognition, pp. 248–255. Ieee, 2009.                                 lie groups, volume 12. Springer, 2020.

                                                                  9
                                          LieRE: Lie Rotational Positional Encodings

Golovneva, O., Wang, T., Weston, J., and Sukhbaatar, S.                Peng, B., Quesnelle, J., Fan, H., and Shippole, E. Yarn:
  Contextual position encoding: Learning to count what’s                 Efficient context window extension of large language
  important. arXiv preprint arXiv:2405.18719, 2024.                      models. arXiv preprint arXiv:2309.00071, 2023.
Heo, B., Park, S., Han, D., and Yun, S. Rotary posi-                   Shah, K., Dikkala, N., Wang, X., and Panigrahy, R.
  tion embedding for vision transformer. arXiv preprint                  Causal language modeling can elicit search and rea-
  arXiv:2403.13298, 2024.                                                soning capabilities on logic puzzles. arXiv preprint
Ho, J., Kalchbrenner, N., Weissenborn, D., and Salimans, T.              arXiv:2409.10502, 2024.
  Axial attention in multidimensional transformers. arXiv
                                                                       Shaw, P., Uszkoreit, J., and Vaswani, A. Self-attention with
  preprint arXiv:1912.12180, 2019.
                                                                         relative position representations, 2018a. URL https:
Jaegle, A., Gimeno, F., Brock, A., Vinyals, O., Zisserman,               //arxiv.org/abs/1803.02155.
  A., and Carreira, J. Perceiver: General perception with it-
  erative attention. In International conference on machine            Shaw, P., Uszkoreit, J., and Vaswani, A. Self-attention
  learning, pp. 4651–4664. PMLR, 2021.                                   with relative position representations. arXiv preprint
                                                                         arXiv:1803.02155, 2018b.
Jiang, A. Q., Sablayrolles, A., Roux, A., Mensch, A., Savary,
   B., Bamford, C., Chaplot, D. S., Casas, D. d. l., Hanna,            Soomro, K., Zamir, A. R., and Shah, M. Ucf101: A dataset
   E. B., Bressand, F., et al. Mixtral of experts. arXiv                 of 101 human actions classes from videos in the wild.
   preprint arXiv:2401.04088, 2024.                                      arXiv preprint arXiv:1212.0402, 2012.
Krizhevsky, A., Hinton, G., et al. Learning multiple layers            Su, J., Ahmed, M., Lu, Y., Pan, S., Bo, W., and
  of features from tiny images. 2009.                                    Liu, Y.       Roformer: Enhanced transformer with
                                                                         rotary position embedding.           Neurocomputing,
Kumar, H., Parada-Mayorga, A., and Ribeiro, A. Lie group
                                                                         568:127063, 2024.          ISSN 0925-2312.      doi:
  algebra convolutional filters. IEEE Transactions on Sig-
                                                                         https://doi.org/10.1016/j.neucom.2023.127063.
  nal Processing, 2024.
                                                                         URL         https://www.sciencedirect.com/
Lee, S., Lee, S., and Song, B. C. Improving vision trans-                science/article/pii/S0925231223011864.
  formers to learn small-size dataset from scratch. IEEE
  Access, 10:123212–123224, 2022.                                      Tai, K. S., Bailis, P., and Valiant, G. Equivariant transformer
                                                                         networks. In International Conference on Machine Learn-
Liao, Y.-L. and Smidt, T. Equiformer: Equivariant graph                  ing, pp. 6086–6095. PMLR, 2019.
  attention transformer for 3d atomistic graphs. arXiv
  preprint arXiv:2206.11990, 2022.                                     Tong, Z., Song, Y., Wang, J., and Wang, L. Videomae:
                                                                         Masked autoencoders are data-efficient learners for self-
Liu, Y., Tian, Y., Zhao, Y., Yu, H., Xie, L., Wang, Y., Ye, Q.,
                                                                         supervised video pre-training. Advances in neural infor-
  Jiao, J., and Liu, Y. Vmamba: Visual state space model.
                                                                         mation processing systems, 35:10078–10093, 2022.
  Advances in neural information processing systems, 37:
  103031–103063, 2024.                                                 Touvron, H., Cord, M., and Jégou, H. Deit iii: Revenge of
Liu, Z., Lin, Y., Cao, Y., Hu, H., Wei, Y., Zhang, Z., Lin,              the vit. In European conference on computer vision, pp.
  S., and Guo, B. Swin transformer: Hierarchical vision                  516–533. Springer, 2022.
  transformer using shifted windows. In Proceedings of the
                                                                       Touvron, H., Lavril, T., Izacard, G., Martinet, X., Lachaux,
  IEEE/CVF international conference on computer vision,
                                                                         M.-A., Lacroix, T., Rozière, B., Goyal, N., Hambro, E.,
  pp. 10012–10022, 2021.
                                                                         Azhar, F., et al. Llama: Open and efficient foundation lan-
Liu, Z., Hu, H., Lin, Y., Yao, Z., Xie, Z., Wei, Y., Ning, J.,           guage models. arXiv preprint arXiv:2302.13971, 2023.
  Cao, Y., Zhang, Z., Dong, L., et al. Swin transformer v2:
  Scaling up capacity and resolution. In Proceedings of the            Tworkowski, S., Staniszewski, K., Pacek, M., Wu, Y.,
  IEEE/CVF conference on computer vision and pattern                     Michalewski, H., and Miłoś, P. Focused transformer:
  recognition, pp. 12009–12019, 2022.                                    Contrastive training for context scaling. Advances in
                                                                         Neural Information Processing Systems, 36, 2024.
McLeish, S., Bansal, A., Stein, A., Jain, N., Kirchenbauer,
 J., Bartoldson, B. R., Kailkhura, B., Bhatele, A., Geip-              Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones,
 ing, J., Schwarzschild, A., et al. Transformers can do                  L., Gomez, A. N., Kaiser, Ł., and Polosukhin, I. At-
 arithmetic with the right embeddings. arXiv preprint                    tention is all you need. Advances in neural information
 arXiv:2405.17399, 2024.                                                 processing systems, 30, 2017.

                                                                  10
                                     LieRE: Lie Rotational Positional Encodings

Worrall, D. E., Garbin, S. J., Turmukhambetov, D., and
 Brostow, G. J. Harmonic networks: Deep translation
 and rotation equivariance. In Proceedings of the IEEE
 conference on computer vision and pattern recognition,
 pp. 5028–5037, 2017.
Yoshioka, K.       vision-transformers-cifar10: Train-
  ing vision transformers (vit) and related models on
  cifar-10. https://github.com/kentaroy47/
  vision-transformers-cifar10, 2024.




                                                          11
                                        LieRE: Lie Rotational Positional Encodings

A. Equivalence of RoPE and LieRE in one Dimension
Though focused on higher dimensional inputs LieRE remains compatible with 1D tasks. It turns out that in the 1D setting,
LieRE has identical representational capacity to RoPE. This is not the case for higher dimensional inputs
Recall that in the 1D setting positions are scalars. The LieRE rotation is R = exp(tA) for a some learnable skew-symmetric
matrix A. Recall that skew-symmetric matrices can be written in the form S T ΛS where and is orthogonal Λ has the structure
denoted below.                                                                  
                                                      0    λ0
                                                  −λ0 0                         
                                                                                
                                                                0     λ         
                                             Λ=                         1       
                                                               −λ 1    0        
                                                                                
                                                                            ..
                                                                               .
We can then use an identity of the matrix exponential to break down the LieRE rotation matrix.

                                             R = exp(tS T ΛS) = S T exp(tΛ)S.

For two tokens in positions t, t′ we denote the embeddings for a specific attention head as xt , xt′ . If K, Q denote the
corresponding key and query linear transformation matrices we can write the attention inner product with LieRE explicitly.


                                xt KRtT Rt′ Qxt′ = xt KS T exp(tΛ)S T S exp(t′ Λ)SQxt′
                                                   = xt KS T exp(tΛ)T exp(t′ Λ)SQxt′
                                                   = xt KS T exp(tΛ)T exp(t′ Λ)SQxt′

We let K ′ = KS T and Q′ = SQ, since these matrices are all learnable we can fold the S matrix into parameters of the key
and query linear layers for the given head, allowing us to simplify the above expression.


                                               xt K ′ exp(tΛ)T exp(t′ Λ)Q′ xt′

Now we use the fact that each block is skew symmetric. In the case of two dimensions,
                                                                                   
                                                   0   λ       cos(λ)          sin(λ)
                                       exp                 =
                                                  −λ   0      − sin(λ)         cos(λ)

If we let Rλ denote a block diagonal rotation matrices with 2D rotations of angles λ0 , . . . , λn , we can rewrite the above
expression in a more familiar form.


                                                       xt K ′ RtT Rt′ Q′ xt′

This is exactly the formulation of the original RoPE position encoding. This also makes more clear how LieRE is different
from RoPE-Mixed in the high dimensional setting. The above proof depends on the fact that we can decompose every
rotation into a matrix of the form S T ΛS with S not dependent on the position, allowing us to fold the orthogonal S
matrices into the key and query matrices. This decomposition with constant S is guaranteed because the inputs to the matrix
exponential differ by only a scalar factor. This is no longer true once we switch to more than a one-dimensional basis of
skew symmetric matrices.

B. Experimental Details
B.1. Experimental Hyperparameters
The backbone for all experiments is configured as ViT-B, with 12 layers, a hidden dimension of 768, and an intermediate
dimension of 3096. We use a dropout of 0.1. We used CLS pooling in our implementation to facilitate comparability with

                                                                12
                                                     LieRE: Lie Rotational Positional Encodings

existing literature in the field. Further experiments revealed substantial performance improvement with mean pooling and
LieRE. We use the pytorch lightning framework for all experiments (Falcon, 2019).

B.2. 2D Image Classification
The CIFAR experiments where trained on 8xL4 GPUs with 24GB of VRAM each and all took under 30 minutes to complete.
The basis capacity scaling experiment was conducted using RTX6000 GPUs. The ImageNet experiments were trained on
8xL40 GPUs and all took less than 2 days and 5 hours of runtime including time lost due to preemption and resource sharing.
We use a cosine learning rate schedule with an initial learning rate of 1E − 4 and train for 200 epochs. We use an effective
batch size of 512. We use a patch size of 4 × 4 on the original 32 × 32 image for CIFAR-100 and a patch size of 16 × 16 on
the randomly cropped and resized 224 × 224 image. All vision experiments used RandAugment (Cubuk et al., 2020). We
use the ADAM optimizer with betas of 0.9 and 0.999 and ϵ = 1e − 8. The hyperparameters were tuned with RoPE-Mixed
and selected before conducting the LieRE trainers as to ensure a fair comparison.

B.3. 3D Video Classifications
The 3D classification experiments were conducted on either 8 × A100 40GB GPUs or 4 × A100 80GB GPUs with the
effective batch size held constant either by using a gradient accumulation or increasing the batch size. Similar to 2D
classification, we use an initial learning rate of 1E − 4 with a cosine decay, trained for 200 epochs, and had a total batch
size of 64 and a patch size of 2 × 16 × 16 on the randomly cropped and resized 8 × 224 × 224 video/image. We use the
ADAM optimizer with betas of 0.9 and 0.999 and ϵ = 1e − 8.

B.4. Multi-resolution Classification
The second training recipe consists of 30 epochs with an initial learning rate of 1E-5 with a cosine decay. This mirrors the
DEIT III training recipe that first pretrains at a lower resolution and finetunes at a higher resolution.

B.5. CIFAR-100 Performance Across Model Scales
We further evaluate the impact of incorporating LieRE across different model sizes on CIFAR-100, as shown in Table 6.
LieRE consistently outperforms the baseline with statistically significant gains across all three model scales. However,
these results may be sensitive to dataset size, as all models are trained from scratch in this study. This is reflected in the
performance drop observed with the ViT-Huge model.

                                                    75


                                                    70


                                                    65
                                     Accuracy (%)




                                                    60

                                                                                       Abs. Pos. E.
                                                    55                                 VisionLlaMA RoPE
                                                                                       RoPE-Mixed
                                                                                       LieRE
                                                                                       LieRE
                                                    50
                                                         ViT-Tiny        ViT-Base               ViT-Large
                                                                        Model Size


Figure 8. Performance behavior on CIFAR-100 (2D Image Classification) over ViT-Tiny (22M), ViT-Base (85M), ViT-Large (302M) for
LieRE RoPE-Mixed and Absolute Encoding (Appendix, table 6).



B.6. Initialization Sensitivity
In order to understand the sensitivity of the methods to the initialization, we explore several scaling factor for the weights
initialization. The results are presented in table 7.

                                                                        13
                                         LieRE: Lie Rotational Positional Encodings


Table 6. Comparison of Position Encoding Methods for Different ViT Models Sizes on CIFAR-100, Accuracy (bootstrapped 95%CI)
                     Position Enc.           ViT-Tiny (22M)        ViT-Base (86M)      ViT-Large (302M)
                     Abs. Pos. E.            57.2 (56.2-58.1)      63.9 (62.9-65.8)     60.5 (59.5-61.4)
                     VisionLlaMA RoPE        58.2 (57.2-59.2)      65.5 (64.6 -66.5)    62.3 (60.4-64.2)
                     RoPE-Mixed              65.4 (64.5-66.4)      68.8 (67.9-69.7)     68.8 (67.9-69.7)
                     LieRE8                  65.6 (64.7-66.6)      70.3 (69.4-71.2)     69.9 (68.9-70.8)
                     LieRE64                 65.3 (64.4-66.3)      70.0 (69.1-69.7)     68.9 (68.0-69.8)




                       Table 7. Paired Z-test between 2π and 1 initialization for LieRE8 and RoPE-Mixed.
Metric                       LieRE8 (2π vs 1 init) RoPE-Mixed (2π vs 1 init)
Z-statistic                          -1.81                     -3.85
P-value                              0.070                    0.00012
Difference between means            -0.0118                   -0.0255
95% Confidence Interval        [-0.0246, 0.0010]         [-0.0385, -0.0125]




C. Python implementation of LieRE rotation matrix computation

   basis_raw_params = nn.Parameter(
       torch.rand(
           input_dimensionality,
           head_dim,
           head_dim,
       ) * 2 * math.pi # optional, inspired from RoPE-Mixed paper
   )
   upper_triangle = (
       torch.triu(basis_raw_params, diagonal=1)
   )
   skew_bases = upper_triangle - torch.transpose(upper_triangle, -1, -2)
   in_basis_positions = (
       positions.reshape(list(positions.shape) + [1] * 2) * skew_bases
   )
   rotation_log = torch.sum(in_basis_positions, dim=-3)
   rotation = torch.matrix_exp(rotation_log.to(dtype=torch.float32))
   rotation = rotation.to(dtype=positions.dtype)




D. Compute efficiency
We demonstrate that LieRE-based transformer achieves comparable performance to the Absolute Position Embedding
baseline (DeiT III (Touvron et al., 2022)) on CIFAR-100 with fewer training epochs. This represents a notable advancement
over recent methods such as VisionLlama and RoPE-Mixed (Chu et al., 2024; Heo et al., 2024). Figure 9 illustrates that
LieRE enables a 3.9× reduction in training compute while maintaining the accuracy achieved by absolute position encodings
after 200 epochs.

                                                              14
                                                                                        LieRE: Lie Rotational Positional Encodings

                                                                                        4.0                                       DeiT Baseline

                                                                                        3.5




                                         Multiplicative Reduction in Training Compute
                                                                                        3.0

                                                                                        2.5

                                                                                        2.0

                                                                                        1.5

                                                                                        1.0

                                                                                        0.5

                                                                                        0.0




                                                                                                                              d
                                                                                                  64



                                                                                                            8




                                                                                                                                            A
                                                                                                                          ixe



                                                                                                                                       laM
                                                                                                           RE
                                                                                                 RE




                                                                                                                       e-M
                                                                                                         Lie




                                                                                                                                      nL
                                                                                               Lie




                                                                                                                                       io
                                                                                                                         p



                                                                                                                                   Vis
                                                                                                                      Ro
                                                                                                                 Method


Figure 9. The LieRE spatial encoding allows the model to match the performance of absolute position encodings with substantially less
training time.


D.1. FLOPS Comparison of methods
We find that since all methods we examine introduce a computational cost that is at most linear in the number of tokens, and
runtime is dominated by the quadratic attention component, there is no substantial difference in computational efficiency
between the methods. We list inference FLOP of the various methods in table 8.

                      Table 8. FLOP analysis with percentage increase compared to absolute position encodings
                   Position Enc.                                                              ViT-Tiny (22M)          ViT-Base (85M)                ViT-Large (302M)
                   Abs. Pos. E.  ∗
                                                                                              0.963G                      5.607G                        19.856G
                   VisionLlaMA RoPE                                                      0.963G (+0.001%)            5.607G (+0.002%)              19.856G (+0.000%)
                   RoPE-Mixed                                                            0.964G (+0.104%)            5.609G (+0.036%)              19.863G (+0.035%)
                   LieRE8                                                                0.968G (+0.519%)            5.617G (+0.178%)              19.882G (+0.065%)
                   LieRE64                                                               0.970G (+0.727%)            5.684G (+1.375%)              20.061G (+1.033%)



E. Validation Losses

Table 9. 2D image and 3D video classification Top-1 Validation loss (95% confidence intervals) results. All models use 85.1M parameters
for 2D tasks and 88.7M parameters for 3D task (Krizhevsky et al., 2009; Deng et al., 2009; Soomro et al., 2012; Flanders et al., 2020) ∗
equivalent to DeiT, ∗∗ equivalent to Vivit (spatio-temporal).
                           Method                                                                CIFAR-100            ImageNet-1k                 UCF101
                           Abs. Pos. E.∗,∗∗                                                      1.56 (1.47-1.56)     1.84 (1.81-1.86)            2.94 (2.92-2.96)
                           VisionLlama RoPE                                                      1.56 (1.51-1.61)     1.98 (1.94-2.01)            2.66 (2.63-2.69)
                           RoPE-Mixed                                                            1.38 (1.33-1.43)     1.72 (1.68-1.74)            2.52 (2.49-2.54)
                           LieRE8                                                                1.36 (1.31-1.41)     1.73 (1.70-1.76)            2.47 (2.44-2.49)
                           LieRE64                                                               1.37 (1.33-1.42)     1.73 (1.70-1.76)            2.64 (2.62-2.67)


                                                                                                                15
                                            LieRE: Lie Rotational Positional Encodings

F. Basis parameters scaling

             Table 10. Accuracy Results for Different LieREΘ Parameters, ∗ relative to the model size of 85.2M and 88.7M
   Dataset       LieREΘ Parameter Absolute        LieREΘ Parameter Relative ∗        Tile Size   Accuracy (%)        CI (95%)
 CIFAR100                     9216                            0.01 %                     2           68.84        (67.93-69.75)
 CIFAR100                    27648                            0.03 %                     4           69.28        (68.38-70.18)
 CIFAR100                    64512                            0.08 %                     8           70.32        (69.42-71.22)
 CIFAR100                   138240                            0.16 %                    16           69.85        (68.95-70.75)
 CIFAR100                   285696                            0.34 %                    32           69.65        (68.75-70.55)
 CIFAR100                   580608                            0.68 %                    64           69.99        (69.09-70.89)
  UCF101                     9216                             0.01 %                     2           46.30        (45.89-46.71)
  UCF101                    27648                             0.03 %                     4           45.67        (45.26-46.08)
  UCF101                    64512                             0.08 %                     8           47.03        (46.62-47.44)
  UCF101                    138240                            0.16 %                    16           46.86        (46.45-47.27)
  UCF101                    285696                            0.32 %                    32           46.42        (46.01-46.83)
  UCF101                    580608                            0.66 %                    64           44.68        (44.27-45.09)


G. Attention Maps
We show the attention scores to the CLS token, averaged across heads for every layer. Each grid is rescaled so that the
minimum element has value zero and maximum value of one. Red indicates the maximal score, and blue indicates the
minimal value.




                                                                 16
                                            LieRE: Lie Rotational Positional Encodings




Figure 10. Attention Maps (normalized), 108 × 108:
                                                                                Figure 11. Attention Maps (normalized), 276 × 276:
RoPE-Mixed, LieRE, Absolute and Visionllama posi-
                                                                                RoPE-Mixed, LieRE, Absolute and Visionllama posi-
tional encoding (x-axis), Layers 1-12 (y-axis). While
                                                                                tional encoding (x-axis), Layers 1-12 (y-axis). While
RoPE-Mixed and LieRE learn to look at the ”Y”, Ab-
                                                                                LieRE still learns to look at the ”Y”, Absolute and
solute and Visionllama less and concentrate on the
                                                                                Visionllama do less so.
arrows.
                 Figure 12. Comparison of attention maps across different resolutions and position encoding methods.
                                                                 17
