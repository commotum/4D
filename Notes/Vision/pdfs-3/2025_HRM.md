## 1. Basic Metadata
- Title: Hierarchical Reasoning Model
  Evidence:
  > "Hierarchical Reasoning Model"
  > (Title page)
- Authors: Guan Wang1,† , Jin Li1 , Yuhao Sun1 , Xing Chen1 , Changling Liu1 , Yue Wu1 , Meng Lu1,† , Sen Song2,† , Yasin Abbasi Yadkori1,†
  Evidence:
  > "Guan Wang1,† , Jin Li1 , Yuhao Sun1 , Xing Chen1 , Changling Liu1 ,"
  > "Yue Wu1 , Meng Lu1,† , Sen Song2,† , Yasin Abbasi Yadkori1,†"
  > (Title page)
- Year: 2025
  Evidence:
  > "arXiv:2506.21734v3 [cs.AI] 4 Aug 2025"
  > (Title page)
- Venue: arXiv (cs.AI)
  Evidence:
  > "arXiv:2506.21734v3 [cs.AI] 4 Aug 2025"
  > (Title page)

## 2. One-Sentence Contribution Summary
The paper proposes HRM, a hierarchical recurrent architecture aimed at enabling deep, stable latent reasoning for complex tasks without reliance on pre-training or CoT supervision.
Evidence:
> "we propose the Hierarchical Reasoning Model (HRM), a novel recurrent architecture that attains significant computational depth while maintaining both train-
> ing stability and efficiency." (Abstract)
> "The model operates without pre-training or CoT data, yet achieves nearly perfect performance on
> challenging tasks including complex Sudoku puzzles and optimal path finding in large mazes." (Abstract)

## 3. Tasks Evaluated
- ARC-AGI-1 (ARC-AGI Challenge)
  - Task type: Reasoning / relational
  - Dataset(s): ARC-AGI-1
  - Domain: synthetic grid puzzles (input-label grid pairs)
  - Evidence:
    > "ARC-AGI Challenge The ARC-AGI benchmark evaluates general fluid intelligence through IQ-
    > test-like puzzles that require inductive reasoning 27 ." (Section 3.1 Benchmarks)
    > "The initial version, ARC-AGI-1, presents chal-
    > lenges as input-label grid pairs that force AI systems to extract and generalize abstract rules from
    > just a few examples." (Section 3.1 Benchmarks)
- ARC-AGI-2
  - Task type: Reasoning / relational
  - Dataset(s): ARC-AGI-2
  - Domain: synthetic grid puzzles
  - Evidence:
    > "Addressing the limitations identified in ARC-AGI-1, ARC-AGI-2 significantly expands the bench-
    > mark by providing a more comprehensive and carefully refined collection of tasks. These new
    > tasks emphasize deeper compositional reasoning, multi-step logic, contextual rule application, and
    > symbolic abstraction." (Section 3.1 Benchmarks)
- Sudoku-Extreme (incl. Sudoku-Extreme-Full)
  - Task type: Reasoning / relational
  - Dataset(s): Sudoku-Extreme (1000 training examples), Sudoku-Extreme-Full (3 831 994 examples), compiled from Kaggle, 17-clue, Magictour 1465, Forum-Hard, Forum-Extreme
  - Domain: 9×9 logic puzzle grids
  - Evidence:
    > "Sudoku-Extreme Sudoku is a 9×9 logic puzzle, requiring each row, column, and 3×3 block to
    > contain the digits 1–9 exactly once." (Section 3.1 Benchmarks)
    > "Sudoku-Extreme is a down-sampled subset of this data containing 1000 training examples." (Section 3.1 Benchmarks)
    > "the complete training data, Sudoku-Extreme-Full, containing 3 831 994 examples." (Section 3.1 Benchmarks)
    > "Easy puzzles compiled from Kaggle, 17-clue, plus unbiased samples from the Sudoku puzzle
    >   distribution 67 : totaling 1 149 158 puzzles." (Section 3.1 Benchmarks)
    > "Challenging puzzles compiled from Magictour 1465, Forum-Hard and Forum-Extreme subsets:
    >   totaling 3 104 157 puzzles." (Section 3.1 Benchmarks)
- Maze-Hard
  - Task type: Reasoning / relational; Other (pathfinding/search)
  - Dataset(s): Maze-Hard (generated via Lehnert et al. procedure; filtered difficulty >110; 1000 train/test)
  - Domain: 30×30 maze grids
  - Evidence:
    > "Maze-Hard This task involves finding the optimal path in a 30×30 maze, making it interpretable
    > and frequently used for training LLMs in search tasks 69,70,71 ." (Section 3.1 Benchmarks)
    > "We adopt the instance generation procedure of Lehnert et al. 71 , but introduce an additional filter
    > to retain only those instances whose difficulty exceeds 110." (Section 3.1 Benchmarks)
    > "The training and test set both include 1000 examples." (Section 3.1 Benchmarks)

## 4. Domain and Modality Scope
- Scope: Multiple domains within the same modality (2D grid-based reasoning tasks: ARC-AGI grid puzzles, Sudoku 9×9 grids, Maze 30×30 grids).
  Evidence:
  > "The initial version, ARC-AGI-1, presents chal-
  > lenges as input-label grid pairs that force AI systems to extract and generalize abstract rules from
  > just a few examples." (Section 3.1 Benchmarks)
  > "Sudoku-Extreme Sudoku is a 9×9 logic puzzle, requiring each row, column, and 3×3 block to
  > contain the digits 1–9 exactly once." (Section 3.1 Benchmarks)
  > "Maze-Hard This task involves finding the optimal path in a 30×30 maze, making it interpretable
  > and frequently used for training LLMs in search tasks 69,70,71 ." (Section 3.1 Benchmarks)
- Domain generalization / cross-domain transfer: Not claimed in the paper.

## 5. Model Sharing Across Tasks
The paper describes training HRM per benchmark from random initialization, with no pretraining or CoT labels mentioned.
Evidence:
> "For all benchmarks, HRM models were initialized with random weights and trained in the sequence-
> to-sequence setup using the input-output pairs." (Section 3.2 Evaluation Details)
> "Remarkably, HRM attains these results with just ~1000 training examples per task—and
> without pretraining or CoT labels." (Section 3.2 Evaluation Details)
> "The HRM was randomly
> initialized, and it solved the tasks directly from inputs without chain of thoughts." (Figure 1 caption)

| Task | Shared Weights? | Fine-Tuned? | Separate Head? | Evidence |
| --- | --- | --- | --- | --- |
| ARC-AGI-1 | Not specified in the paper. | No (trained from random init; no pretraining). | Not specified in the paper. | "For all benchmarks, HRM models were initialized with random weights and trained in the sequence-to-sequence setup using the input-output pairs." (Section 3.2 Evaluation Details); "Remarkably, HRM attains these results with just ~1000 training examples per task—and without pretraining or CoT labels." (Section 3.2 Evaluation Details) |
| ARC-AGI-2 | Not specified in the paper. | No (trained from random init; no pretraining). | Not specified in the paper. | "For all benchmarks, HRM models were initialized with random weights and trained in the sequence-to-sequence setup using the input-output pairs." (Section 3.2 Evaluation Details); "Remarkably, HRM attains these results with just ~1000 training examples per task—and without pretraining or CoT labels." (Section 3.2 Evaluation Details) |
| Sudoku-Extreme / Sudoku-Extreme-Full | Not specified in the paper. | No (trained from random init; no pretraining). | Not specified in the paper. | "For all benchmarks, HRM models were initialized with random weights and trained in the sequence-to-sequence setup using the input-output pairs." (Section 3.2 Evaluation Details); "Remarkably, HRM attains these results with just ~1000 training examples per task—and without pretraining or CoT labels." (Section 3.2 Evaluation Details) |
| Maze-Hard | Not specified in the paper. | No (trained from random init; no pretraining). | Not specified in the paper. | "For all benchmarks, HRM models were initialized with random weights and trained in the sequence-to-sequence setup using the input-output pairs." (Section 3.2 Evaluation Details); "Remarkably, HRM attains these results with just ~1000 training examples per task—and without pretraining or CoT labels." (Section 3.2 Evaluation Details) |

## 6. Input and Representation Constraints
- Token sequence representation for inputs/outputs.
  Evidence:
  > "Both input and output are represented as token sequences: x = (x1 , . . . , xl ) and y = (y1 , . . . , yl′ ) respectively." (Architectural details)
- Explicit 2D grid assumption with flattening and padding to a max length.
  Evidence:
  > "The two-dimensional input and output grids were flattened and then padded to the maximum sequence length." (Section 3.2 Evaluation Details)
- Fixed grid sizes for specific tasks (9×9 Sudoku; 30×30 maze; 30x30 ARC context in experiments).
  Evidence:
  > "Sudoku-Extreme Sudoku is a 9×9 logic puzzle, requiring each row, column, and 3×3 block to
  > contain the digits 1–9 exactly once." (Section 3.1 Benchmarks)
  > "Maze-Hard This task involves finding the optimal path in a 30×30 maze, making it interpretable
  > and frequently used for training LLMs in search tasks 69,70,71 ." (Section 3.1 Benchmarks)
  > "with only 27M parameters and a 30x30 grid context (900 tokens)," (Section 1 Introduction)
- Patch size: Not specified in the paper.
- Fixed number of tokens: Not specified in general; ARC-AGI is described with a "30x30 grid context (900 tokens)" in the main results.
  Evidence:
  > "with only 27M parameters and a 30x30 grid context (900 tokens)," (Section 1 Introduction)

## 7. Context Window and Attention Structure
- Maximum sequence length: Not specified as a single global max; ARC-AGI results mention a 30x30 grid context (900 tokens).
  Evidence:
  > "with only 27M parameters and a 30x30 grid context (900 tokens)," (Section 1 Introduction)
- Sequence length fixed or variable: Inputs are flattened and padded to a maximum sequence length (variable before padding, fixed after padding).
  Evidence:
  > "The two-dimensional input and output grids were flattened and then padded to the maximum sequence length." (Section 3.2 Evaluation Details)
- Attention type: Global/full attention.
  Evidence:
  > "Since HRM focuses on reasoning, full attention is applied for simplicity." (Section 5 Related Work, Hierarchical memory)
- Mechanisms to manage computational cost: adaptive halting (ACT) and 1-step gradient approximation (constant memory vs BPTT).
  Evidence:
  > "we incorporate an adaptive halting strategy into HRM that en-
  > ables “thinking, fast and slow”." (Section 2, Adaptive computational time)
  > "we propose a one-step gradient approximation for training HRM, which offers im-
  > proved efficiency and eliminates the requirement for BPTT. This design maintains a constant mem-
  > ory footprint (O(1) compared to BPTT’s O(T ) for T timesteps)" (Section 1 Introduction)

## 8. Positional Encoding (Critical Section)
- Mechanism: Rotary Positional Encoding (RoPE).
  Evidence:
  > "These improvements include Rotary Positional Encoding 54 , Gated Linear Units 55 ,
  > RMSNorm 56 , and the removal of bias terms from linear layers." (Architectural details)
- Where applied: In Transformer blocks across HRM and baseline models (specific placement within blocks is not specified).
  Evidence:
  > "For all Transformer blocks in this work—including those in
  > the baseline models—we incorporate the enhancements found in modern LLMs (based on Llama 53
  > architectures). These improvements include Rotary Positional Encoding 54 , Gated Linear Units 55 ,
  > RMSNorm 56 , and the removal of bias terms from linear layers." (Architectural details)
- Fixed vs modified per task / ablations: Not specified in the paper.

## 9. Positional Encoding as a Variable
- Treated as a fixed architectural assumption (no explicit comparison or ablation).
  Evidence:
  > "For all Transformer blocks in this work—including those in
  > the baseline models—we incorporate the enhancements found in modern LLMs (based on Llama 53
  > architectures). These improvements include Rotary Positional Encoding 54 , Gated Linear Units 55 ,
  > RMSNorm 56 , and the removal of bias terms from linear layers." (Architectural details)
- Multiple positional encodings compared: Not specified in the paper.
- Claim that PE choice is not critical/secondary: Not specified in the paper.

## 10. Evidence of Constraint Masking (Scale vs Structure)
- Model size and data size are explicitly small for reported results.
  Evidence:
  > "With only 27 million parameters, HRM achieves
  > exceptional performance on complex reasoning tasks using only 1000 training samples." (Abstract)
  > "With only about 1000 training examples, the HRM (~27M parameters) surpasses state-of-the-art
  > CoT models on inductive benchmarks (ARC-AGI) and challenging symbolic tree-search puzzles
  > (Sudoku-Extreme, Maze-Hard) where CoT models failed completely." (Figure 1 caption)
  > "Sudoku-Extreme is a down-sampled subset of this data containing 1000 training examples." (Section 3.1 Benchmarks)
  > "the complete training data, Sudoku-Extreme-Full, containing 3 831 994 examples." (Section 3.1 Benchmarks)
  > "HRM, trained from scratch with only the official dataset (~1000 exam-
  > ples), with only 27M parameters and a 30x30 grid context (900 tokens)," (Section 1 Introduction)
- Performance gains are attributed to computational depth / hierarchical architecture rather than width scaling.
  Evidence:
  > "On Sudoku-Extreme Full, which
  > require extensive tree-search and backtracking, increasing a Transformer’s width yields no perfor-
  > mance gain, while increasing depth is critical." (Figure 2 caption)
  > "Standard architectures saturates, failing to
  > benefit from increased depth. HRM overcomes this fundamental limitation, effectively using its
  > computational depth to achieve near-perfect accuracy." (Figure 2 caption)
- The model is positioned as not relying on pretraining/CoT (i.e., not masking constraints via large pretraining).
  Evidence:
  > "The model operates without pre-training or CoT data, yet achieves nearly perfect performance on
  > challenging tasks including complex Sudoku puzzles and optimal path finding in large mazes." (Abstract)

## 11. Architectural Workarounds
- Two-level hierarchical recurrent architecture (slow H-module + fast L-module) to increase effective depth.
  Evidence:
  > "HRM executes sequential reasoning tasks in a single forward pass without explicit supervision of the intermediate process, through two interdependent recurrent
  > modules: a high-level module responsible for slow, abstract planning, and a low-level mod-
  > ule handling rapid, detailed computations." (Abstract)
- Hierarchical convergence to avoid premature convergence in recurrence.
  Evidence:
  > "avoids the rapid convergence of standard recurrent models through a process we term “hierarchi-
  > cal convergence.”" (Section 1 Introduction)
- One-step gradient approximation to avoid BPTT and reduce memory.
  Evidence:
  > "we propose a one-step gradient approximation for training HRM, which offers im-
  > proved efficiency and eliminates the requirement for BPTT. This design maintains a constant mem-
  > ory footprint (O(1) compared to BPTT’s O(T ) for T timesteps)" (Section 1 Introduction)
- Deep supervision to provide frequent feedback/stability.
  Evidence:
  > "we incorporate a deep supervision mechanism into HRM, as detailed next." (Section 2, Deep supervision)
- Adaptive computation time (ACT) to adapt compute per task complexity.
  Evidence:
  > "we incorporate an adaptive halting strategy into HRM that en-
  > ables “thinking, fast and slow”." (Section 2, Adaptive computational time)
- Post-Norm + RMSNorm/AdamW for stability of ACT/Q-learning.
  Evidence:
  > "Our
  > model satisfies these conditions through its Post-Norm architecture that employs RMSNorm (a
  > layer normalization variant) and the AdamW optimizer." (Section 2, Stability of Q-learning in ACT)
- Full attention (no windowing/pooling) explicitly chosen for simplicity.
  Evidence:
  > "Since HRM focuses on reasoning, full attention is applied for simplicity." (Section 5 Related Work, Hierarchical memory)

## 12. Explicit Limitations and Non-Claims
- Interpretability is limited; full algorithmic understanding is out of scope.
  Evidence:
  > "While a definitive answer lies beyond our current scope, we begin our investigation by analyzing
  > state trajectories and their corresponding solution evolution." (Section 3.3 Visualization of intermediate timesteps)
- Evidence for hierarchical dimensionality is correlational; causal necessity remains open.
  Evidence:
  > "We emphasize, however, that this evidence is correlational. While a causal link could be tested
  > via intervention (e.g., by constraining the H-module’s dimensionality), such methods are difficult
  > to interpret in deep learning due to potential confounding effects on the training process itself.
  > Thus, the causal necessity of this emergent hierarchy remains an important question for future
  > investigation." (Section 4 Brain Correspondence)
- Potential improvements left for future work (merging techniques).
  Evidence:
  > "These modules take mul-
  > tiple inputs, and we use straightforward element-wise addition to combine them, though more
  > sophisticated merging techniques such as gating mechanisms could potentially improve perfor-
  > mance and is left for future work." (Architectural details)
- Future direction: hierarchical memory integration.
  Evidence:
  > "Incorporating hierarchical memory into HRM could be a promising future direction." (Section 5 Related Work, Hierarchical memory)
- Explicit non-claims about open-world learning, unrestrained multi-task learning, or meta-learning: Not specified in the paper.
