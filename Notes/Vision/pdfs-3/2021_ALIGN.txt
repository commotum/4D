                                                      Scaling Up Visual and Vision-Language Representation Learning
                                                                        With Noisy Text Supervision


                                                     Chao Jia 1 Yinfei Yang 1 Ye Xia 1 Yi-Ting Chen 1 Zarana Parekh 1 Hieu Pham 1 Quoc V. Le 1
                                                                              Yunhsuan Sung 1 Zhen Li 1 Tom Duerig 1


                                                                    Abstract                            1. Introduction
arXiv:2102.05918v2 [cs.CV] 11 Jun 2021




                                                Pre-trained representations are becoming crucial        In the existing literature, visual and vision-language repre-
                                                for many NLP and perception tasks. While repre-         sentation learning are mostly studied separately with differ-
                                                sentation learning in NLP has transitioned to train-    ent training data sources. In the vision domain, pre-training
                                                ing on raw text without human annotations, vi-          on large-scale supervised data such as ImageNet (Deng
                                                sual and vision-language representations still rely     et al., 2009), OpenImages (Kuznetsova et al., 2020), and JFT-
                                                heavily on curated training datasets that are expen-    300M (Sun et al., 2017; Kolesnikov et al., 2020) has proven
                                                sive or require expert knowledge. For vision appli-     to be critical for improving performance on downstream
                                                cations, representations are mostly learned using       tasks via transfer learning. Curation of such pre-training
                                                datasets with explicit class labels such as Ima-        datasets requires heavy work on data gathering, sampling,
                                                geNet or OpenImages. For vision-language, popu-         and human annotation, and hence is difficult to scale.
                                                lar datasets like Conceptual Captions, MSCOCO,
                                                or CLIP all involve a non-trivial data collection       Pre-training has also become the de-facto approach
                                                (and cleaning) process. This costly curation pro-       in vision-language modeling (Lu et al., 2019; Chen
                                                cess limits the size of datasets and hence hinders      et al., 2020c; Li et al., 2020). However, vision-language
                                                the scaling of trained models. In this paper, we        pre-training datasets such as Conceptual Captions (Sharma
                                                leverage a noisy dataset of over one billion image      et al., 2018), Visual Genome Dense Captions (Krishna
                                                alt-text pairs, obtained without expensive filter-      et al., 2016), and ImageBERT (Qi et al., 2020) require
                                                ing or post-processing steps in the Conceptual          even heavier work on human annotation, semantic parsing,
                                                Captions dataset. A simple dual-encoder archi-          cleaning and balancing. As a result, the scales of these
                                                tecture learns to align visual and language rep-        datasets are only in the realm of ∼10M examples. This is at
                                                resentations of the image and text pairs using a        least an order of magnitude smaller than their counterparts
                                                contrastive loss. We show that the scale of our         in the vision domain, and much smaller than large corpora
                                                corpus can make up for its noise and leads to           of text from the internet for NLP pre-training (e.g., Devlin
                                                state-of-the-art representations even with such a       et al. (2019); Radford et al. (2019); Yang et al. (2019); Liu
                                                simple learning scheme. Our visual representation       et al. (2019b); Raffel et al. (2020)).
                                                achieves strong performance when transferred to         In this work, we leverage a dataset of over one billion noisy
                                                classification tasks such as ImageNet and VTAB.         image alt-text pairs to scale visual and vision-language rep-
                                                The aligned visual and language representations         resentation learning. We follow the procedures described
                                                enables zero-shot image classification and also         in the Conceptual Captions dataset (Sharma et al., 2018)
                                                set new state-of-the-art results on Flickr30K and       to have a large noisy dataset. But instead of applying the
                                                MSCOCO image-text retrieval benchmarks, even            complex filtering and post-processing steps as proposed
                                                when compared with more sophisticated cross-            by (Sharma et al., 2018) to clean the dataset, we only apply
                                                attention models. The representations also enable       simple frequency-based filtering. The resulting dataset is
                                                cross-modality search with complex text and text        noisy, but is two orders of magnitude larger than the Con-
                                                + image queries.                                        ceptual Captions dataset. We show that visual and vision-
                                                                                                        language representations pre-trained on our exascale dataset
                                                                                                        achieve very strong performance on a wide range of tasks.
                                            1
                                             Google Research. Correspondence to: Chao Jia <chao-
                                         jia@google.com>, Yinfei Yang <yinfeiy@google.com>.             To train our model, we use an objective that aligns the visual
                                                                                                        and language representations in a shared latent embedding
                                         Proceedings of the 38 th International Conference on Machine   space using a simple dual-encoder architecture. Similar
                                         Learning, PMLR 139, 2021. Copyright 2021 by the author(s).
                   Scaling Up Visual and Vision-Language Representation Learning With Noisy Text Supervision

                   Pre-training                          (Zero-shot) Visual Tasks

                 Contrastive Learning



                Text               Image
              Encoder             Encoder



                  Noisy Image-Text
                        Data
                                                     ImageNet (Deng et al. 2009)               Visual Task Adaptation Benchmark (VTAB)
                                                   figure credit to (Krizhevsky et al. 2012)                (Zhai et al. 2019)

        Fine-grained Image-Text Retrieval        Flickr30k (Plummer et al. 2015), MSCOCO(Chen et al. 2015), ...

        “Roppongi Hills Spider at night”                             “original picture of
                                                                     monet haystack”

                                                                     “monet haystack png”
                                                                                                         “snow”
                                                                 “haystack series
                                                                 monet art institute of
                                                                 chicago”
                                                                 ...
           (A) Text -> Image Retrieval              (B) Image -> Text Retrieval                             (C) Image + Text -> Image Retrieval

Figure 1. A summary of our method, ALIGN. Visual and language representations are jointly learned from noisy image alt-text data. The
representations can be used for vision-only or vision-language task transfer. Without any fine-tuning, ALIGN powers zero-shot visual
classification and cross-modal search including image-to-text search, text-to-image search and even search with joint image+text queries.

objectives has been applied to learning visual-semantic                          2. Related Work
embeddings (VSE) (Frome et al., 2013; Faghri et al., 2018).
We name our model ALIGN: A Large-scale ImaGe and                                 High-quality visual representations for classification or
Noisy-text embedding. Image and text encoders are learned                        retrieval are usually pre-trained on large-scale labeled
via a contrastive loss (formulated as normalized softmax)                        datasets (Mahajan et al., 2018; Kolesnikov et al., 2020;
that pushes the embeddings of matched image-text pair                            Dosovitskiy et al., 2021; Juan et al., 2020). Recently,
together while pushing those of non-matched image-text                           self-supervised (Chen et al., 2020b; Tian et al., 2020;
pair apart. This is one of the most effective loss functions                     He et al., 2020; Misra & Maaten, 2020; Li et al., 2021;
for both self-supervised (Chen et al., 2020b) and supervised                     Grill et al., 2020; Caron et al., 2020) and semi-supervised
(Zhai & Wu, 2019; Musgrave et al., 2020) representation                          learning (Yalniz et al., 2019; Xie et al., 2020; Pham et al.,
learning. Considering paired texts as fine-grained labels of                     2020) have been studied as alternative paradigms. However,
images, our image-to-text contrastive loss is analogous to                       models trained by these methods so far show limited
the conventional label-based classification objective; and                       transferability to downstream tasks (Zoph et al., 2020).
the key difference is that the text encoder generates the                        Leveraging images and natural language captions is another
“label” weights. The top-left of Figure 1 summarizes the                         direction of learning visual representations. Joulin et al.
method we use in ALIGN.                                                          (2015); Li et al. (2017); Desai & Johnson (2020); Sariyildiz
The aligned image and text representations are naturally                         et al. (2020); Zhang et al. (2020) show that a good visual
suited for cross-modality matching/retrieval tasks and                           representation can be learned by predicting the captions
achieve state-of-the-art (SOTA) results in corresponding                         from images, which inspires our work. These works are
benchmarks. For instance, ALIGN outperforms the previous                         however limited to small datasets such as Flickr (Joulin
SOTA method by over 7% in most zero-shot and fine-tuned                          et al., 2015; Li et al., 2017) and COCO Captions (Desai
R@1 metrics in Flickr30K and MSCOCO. Moreover, such                              & Johnson, 2020; Sariyildiz et al., 2020), and the resulting
cross-modality matching naturally enables zero-shot image                        models don’t produce a vision-language representation that
classification when feeding the classnames into the text en-                     is needed for tasks like cross-modal retrieval.
coder, achieving 76.4% top-1 accuracy in ImageNet without                        In the vision-language representation learning domain,
using any of its training samples. The image representa-                         visual-semantic embeddings (VSE) (Frome et al., 2013;
tion itself also achieves superior performance in various                        Faghri et al., 2018) and improved versions (e.g., leveraging
downstream visual tasks. For example, ALIGN achieves                             object detectors, dense feature maps, or multi-attention
88.64% top-1 accuracy in ImageNet. Figure 1-bottom shows                         layers) (Socher et al., 2014; Karpathy et al., 2014; Kiros
the cross-modal retrieval examples that come from a real                         et al.; Nam et al., 2017; Li et al., 2019; Messina et al., 2020;
retrieval system built by ALIGN.                                                 Chen et al., 2020a) have been proposed. Recently more
                        Scaling Up Visual and Vision-Language Representation Learning With Noisy Text Supervision

advanced models emerge with cross-modal attention layers                                    ratio is smaller than 3. Images with more than 1000 associ-
(Liu et al., 2019a; Lu et al., 2019; Chen et al., 2020c; Huang                              ated alt-texts are discarded. To ensure that we don’t train on
et al., 2020b) and show superior performance in image-text                                  test images, we also remove duplicates or near-duplicates
matching tasks. However, they are orders of magnitudes                                      of test images in all downstream evaluation datasets (e.g.,
slower and hence impractical for image-text retrieval                                       ILSVRC-2012, Flickr30K, and MSCOCO). See Appendix
systems in the real world. In contrast, our model inherits                                  A for more details.
the simplest VSE form, but still outperforms all previous
cross-attention models in image-text matching benchmarks.                                   Text-based filtering. We exclude alt-texts that are shared
Closely related to our work is CLIP (Radford et al., 2021),                                 by more than 10 images. These alt-texts are often irrelevant
which proposes visual representation learning via natural                                   to the content of the images (e.g., “1920x1080”, “alt img”,
language supervision in a similar contrastive learning                                      and “cristina”). We also discard alt-texts that contain any
setting. Besides using different vision and language encoder                                rare token (outside of 100 million most frequent unigrams
architectures, the key difference is on training data: ALIGN                                and bigrams from the raw dataset), and those that are ei-
follows the natural distribution of image-text pairs from the                               ther too short (<3 unigrams) or too long (>20 unigrams).
raw alt-text data, while CLIP collects the dataset by first                                 This removes noisy texts like “image tid 25&id mggqpuwe-
constructing an allowlist of high-frequency visual concepts                                 qdpd&cache 0&lan code 0”, or texts that are too generic to
from English Wikipedia. We demonstrate that strong visual                                   be useful.
and vision-language representations can be learned with
a dataset that doesn’t require expert knowledge to curate.
                                                                                            4. Pre-training and Task Transfer
                                                                                            4.1. Pre-training on Noisy Image-Text Pairs
3. A Large-Scale Noisy Image-Text Dataset
                                                                                            We pre-train ALIGN using a dual-encoder architecture. The
The focus of our work is to scale up visual and vision-                                     model consists of a pair of image and text encoders with a
language representation learning. For this purpose, we resort                               cosine-similarity combination function at the top. We use
to a much larger dataset than existing ones. Specifically,                                  EfficientNet with global pooling (without training the 1x1
we follow the methodology of constructing Conceptual                                        conv layer in the classification head) as the image encoder
Captions dataset (Sharma et al., 2018) to get a version of                                  and BERT with [CLS] token embedding as the text em-
raw English alt-text data (image and alt-text pairs). The                                   bedding encoder (we generate 100k wordpiece vocabulary
Conceptual Captions dataset was cleaned by heavy filtering                                  from our training dataset). A fully-connected layer with
and post-processing. Here, for the purpose of scaling, we                                   linear activation is added on top of BERT encoder to match
trade quality for scale by relaxing most of the cleaning                                    the dimension from the image tower. Both image and text
steps in the original work. Instead, we only apply minimal                                  encoders are trained from scratch.
frequency-based filtering as detailed below. The result is a
much larger (1.8B image-text pairs) but noisier dataset. Fig-                               The image and text encoders are optimized via normalized
ure 2 shows some sample image-text pairs from the dataset.                                  softmax loss (Zhai & Wu, 2019). In training, we treat
                                                                                            matched image-text pairs as positive and all other random
                                                                                            image-text pairs that can be formed in a training batch as
                                                                                            negative.
                                                                                            We minimize the sum of two losses: one for image-to-text
                                                                                            classification
“motorcycle front wheel”    “thumbnail for version as of 21       “file frankfurt airport                         N
                                  57 29 june 2010”                skyline 2017 05 jpg”                          1 X       exp(x>i yi /σ)
                                                                                                       Li2t = −     log PN                       (1)
                                                                                                                 N                  >
                                                                                                                                    exp(xi yj /σ)
                                                                                                                     i        j=1


                                                                                            and the other for text-to-image classification
                                                                                                                     N
“file london barge race 2 jpg”   “moustache seamless          “st oswalds way and shops”                         1 X       exp(yi> xi /σ)
                                   wallpaper design”                                                  Lt2i = −       log PN                             (2)
                                                                                                                 N            exp(yi> xj /σ)
                                                                                                                   i      j=1
Figure 2. Example image-text pairs randomly sampled from the
training dataset of ALIGN. One clearly noisy text annotation is                             Here, xi and yj are the normalized embedding of image in
marked in italics.                                                                          the i-th pair and that of text in the j-th pair, respectively. N
                                                                                            is the batch size, and σ is the temperature to scale the logits.
Image-based filtering. Following Sharma et al. (2018),                                      For in-batch negatives to be more effective, we concatenate
we remove pornographic images and keep only images                                          embeddings from all computing cores to form a much larger
whose shorter dimension is larger than 200 pixels and aspect                                batch. The temperature variable is crucial as both image
                Scaling Up Visual and Vision-Language Representation Learning With Noisy Text Supervision

and text embeddings are L2-normalized. Instead of man-           5. Experiments and Results
ually sweeping for the optimal temperature value, we find
that it can be effectively learned together with all the other   We train our ALIGN models from scratch, using the open-
parameters.                                                      sourced implementation of EfficientNet as the image en-
                                                                 coder and BERT as the text encoder. Unless in the ablation
4.2. Transferring to Image-Text Matching & Retrieval             study, we use the results of ALIGN where the image encoder
                                                                 is EfficientNet-L2 and the text encoder is BERT-Large. The
We evaluate ALIGN models on image-to-text and text-to-
                                                                 image encoder is trained at resolution of 289 × 289 pixels
image retrieval tasks, with and without finetuning. Two
                                                                 no matter what EfficientNet variant is used. We first resize
benchmark datasets are considered: Flickr30K (Plummer
                                                                 input images to 346 × 346 resolution and then perform ran-
et al., 2015) and MSCOCO (Chen et al., 2015). We also
                                                                 dom crop (with additional random horizontal flip) in training
evaluate ALIGN on Crisscrossed Captions (CxC) (Parekh
                                                                 and central crop in evaluation. For BERT we use wordpiece
et al., 2021), which is an extension of MSCOCO with
                                                                 sequence of maximum 64 tokens since the input texts are
additional human semantic similarity judgments for
                                                                 no longer than 20 unigrams. The softmax temperature vari-
caption-caption, image-image, and image-caption pairs.
                                                                 able is initialized as 1.0 (this temperature variable is shared
With extended annotations, CxC enables four intra- and
                                                                 between image-to-text loss and text-to-image loss) and we
inter-modal retrieval tasks including image-to-text, text-to-
                                                                 use 0.1 as label smoothing parameter in the softmax losses.
image, text-to-text, and image-to-image retrieval, and three
                                                                 We use LAMB optimizer (You et al., 2020)1 with weight
semantic similarity tasks including semantic textual sim-
                                                                 decay ratio 1e-5. The learning rate is warmed up linearly
ilarity (STS), semantic image similarity (SIS), and semantic
                                                                 to 1e-3 from zero in 10k steps, and then linearly decay to
image-text similarity (SITS). As the training set is identical
                                                                 zero in 1.2M steps (∼12 epochs). We train the model on
to the original MSCOCO, we can directly evaluate the
                                                                 1024 Cloud TPUv3 cores with 16 positive pairs on each
MSCOCO fine-tuned ALIGN model on CxC annotations.
                                                                 core. Therefore the total effective batch size is 16384.
4.3. Transferring to Visual Classification
                                                                 5.1. Image-Text Matching & Retrieval
We first apply zero-shot transfer of ALIGN to visual classifi-
                                                                 We evaluate ALIGN on Flickr30K and MSCOCO cross-
cation tasks on ImageNet ILSVRC-2012 benchmark (Deng
                                                                 modal retrieval benchmarks, in both zero-shot and fully
et al., 2009) and its variants including ImageNet-R(endition)
                                                                 fine-tuned settings. We follow (Karpathy & Fei-Fei, 2015)
(Hendrycks et al., 2020) (non-natural images such as art,
                                                                 and most existing works to obtain the train/test splits. Specif-
cartoons, sketches), ImageNet-A(dversarial) (Hendrycks
                                                                 ically, for Flickr30K, we evaluate on the standard 1K test
et al., 2021) (more challenging images for ML models), and
                                                                 set, and finetune on the 30k training set. For MSCOCO, we
ImageNet-V2 (Recht et al., 2019). All of these variants
                                                                 evaluate on the 5K test set, and finetune on 82K training
follow the same set (or a subset) of ImageNet classes, while
                                                                 plus 30K additional validation images that are not in the 5K
the images in ImageNet-R and ImageNet-A are sampled
                                                                 validation or 5K test sets.
from drastically different distributions from ImageNet.
                                                                 During fine-tuning, the same loss function is used. But there
We also transfer the image encoder to downstream visual
                                                                 can be false negatives when the batch size is comparable
classification tasks. For this purpose, we use the ImageNet
                                                                 to the total number of training samples. So we reduce the
as well as a handful of smaller fine-grained classifica-
                                                                 global batch size from 16384 to 2048. We also reduce the ini-
tion datasets such as Oxford Flowers-102 (Nilsback &
                                                                 tial learning rate to 1e-5 and train for 3K and 6K steps (with
Zisserman, 2008), Oxford-IIIT Pets (Parkhi et al., 2012),
                                                                 linear decay) respectively on Flickr30K and MSCOCO. All
Stanford Cars (Krause et al., 2013), and Food101 (Bossard
                                                                 the other hyper-parameters are kept the same as pre-training.
et al., 2014). For ImageNet, results from two settings are
reported: training the top classification layer only (with       Table 1 shows that, compared to previous works, ALIGN
frozen ALIGN image encoder) and fully fine-tuned. Only           achieves SOTA results in all metrics of Flickr30K and
the latter setting is reported for fine-grained classification   MSCOCO benchmarks. In the zero-shot setting, ALIGN
benchmarks. Following Kolesnikov et al. (2020), we               gets more than 7% improvement in image retrieval task
also evaluate the robustness of our model on Visual Task         compared to the previous SOTA, CLIP (Radford et al.,
Adaptation Benchmark (VTAB) (Zhai et al., 2019) which            2021). With fine-tuning, ALIGN outperforms all existing
consists of 19 diverse (covering subgroups of natural,           methods by a large margin, including those that employ
specialized and structured image classification tasks) visual    more complex cross-modal attention layers such as
classification tasks with 1000 training samples each.            ImageBERT (Qi et al., 2020), UNITER (Chen et al., 2020c),
                                                                    1
                                                                      We tried SGD with momentum and ADAM which are known
                                                                 to work well for CNNs and BERT respectively. LAMB appears to
                                                                 be a better choice for training both image and text encoders.
                  Scaling Up Visual and Vision-Language Representation Learning With Noisy Text Supervision


Table 1. Image-text retrieval results on Flickr30K and MSCOCO datasets (zero-shot and fine-tuned). ALIGN is compared with Image-
BERT (Qi et al., 2020), UNITER (Chen et al., 2020c), CLIP (Radford et al., 2021), GPO (Chen et al., 2020a), ERNIE-ViL (Yu et al.,
2020), VILLA (Gan et al., 2020), and Oscar (Li et al., 2020).

                                               Flickr30K (1K test set)                                 MSCOCO (5K test set)
                                        image → text            text → image                    image → text         text → image
                                R@1        R@5     R@10       R@1     R@5      R@10       R@1       R@5       R@10      R@1          R@5       R@10
                  ImageBERT      70.7       90.2     94.0      54.3    79.6     87.5       44.0      71.2      80.4      32.3         59.0      70.2
                  UNITER         83.6       95.7     97.7      68.7    89.2     93.9          -         -         -         -            -         -
   Zero-shot
                  CLIP           88.0       98.7     99.4      68.7    90.6     95.2       58.4      81.5      88.1      37.8         62.4      72.2
                  ALIGN          88.6       98.7     99.7      75.7    93.8     96.8       58.6      83.0      89.7      45.6         69.8      78.6
                  GPO            88.7       98.9     99.8      76.1    94.5     97.1       68.1      90.2         -      52.7         80.2         -
                  UNITER         87.3       98.0     99.2      75.6    94.1     96.8       65.7      88.6      93.8      52.9         79.9      88.0
                  ERNIE-ViL      88.1       98.0     99.2      76.7    93.6     96.4          -         -         -         -            -         -
   Fine-tuned
                  VILLA          87.9       97.5     98.8      76.3    94.2     96.8          -         -         -         -            -         -
                  Oscar             -          -        -         -       -        -       73.5      92.2      96.0      57.5         82.8      89.8
                  ALIGN          95.3       99.8    100.0      84.9    97.4     98.6       77.0      93.5      96.9      59.9         83.3      89.8


Table 2. Multimodal retrieval performance on Crisscrossed Captions (CxC) dataset. ALIGN is compared with VSE++ (Faghri et al.,
2018), VSRN (Li et al., 2019), DEI2T (Parekh et al., 2021), and DET2T+I2T (Parekh et al., 2021).

                            image → text                  text → image                   text → text                  image → image
                       R@1     R@5       R@10         R@1     R@5     R@10       R@1       R@5       R@10       R@1          R@5       R@10
          VSE++         43.1    74.3      84.2         32.5    62.7    75.4       38.7      62.3      72.2       36.4         70.4      81.3
          VSRN          52.4    81.9      90.0         40.1    71.1    81.5       41.0      64.8      74.5       44.2         76.7      86.2
          DEI2T         53.9    82.7      91.2         39.8    70.2    80.9       26.0      47.1      57.5       38.3         74.1      85.0
          DET2T+I2T     55.9    84.2      91.8         41.7    72.3    83.0       42.4      64.9      74.0       38.5         73.6      84.9
          ALIGN         78.1    94.3      97.4         61.8    84.9    91.1       45.4      66.8      75.2       49.4         81.4      89.1

                                                                          DEI2T . We suspect it is because the training objective of
Table 3. Spearman’s R Bootstrap Correlation (×100) on Criss-
                                                                          ALIGN focuses on cross-modal (image-text) matching in-
crossed Captions (CxC) dataset. ALIGN is compared with
VSE++ (Faghri et al., 2018), VSRN (Li et al., 2019), DEI2T (Parekh        stead of intra-modal matching. Parekh et al. (2021) suggest
et al., 2021), and DET2T+I2T (Parekh et al., 2021).                       multitask learning could produce more balanced representa-
                                                                          tions. We leave it to the future work.
                  STS          SIS          SITS        Mean Avg
  Model
                avg ± std   avg ± std     avg ± std                       5.2. Zero-shot Visual Classification
  VSE++         74.4±0.4    73.3±0.9      55.2±1.5            67.6
  VSRN          73.0±0.4    70.1±1.0      60.4±1.3            67.8        If we directly feed the texts of classnames into the text
  DEI2T         50.9±0.6    81.3±0.7      61.6±1.4            64.6        encoder, ALIGN is able to classify images into candidate
  DET2T+I2T     74.2±0.4    74.5±0.9      61.9±1.3            70.2
  ALIGN         72.9±0.4    77.2±0.8      67.6±1.2            72.6
                                                                          classes via image-text retrieval. Table 4 compares ALIGN
                                                                          with CLIP on Imagenet and its variants. Similar to CLIP,
                                                                          ALIGN shows great robustness on classification tasks
ERNIE-ViL (Yu et al., 2020), VILLA (Gan et al., 2020) and                 with different image distributions. In order to make a
Oscar (Li et al., 2020).                                                  fair comparison, we use the same prompt ensembling
                                                                          method as CLIP. Each classname is expanded with a set
Table 2 reports the performance of ALIGN on Crisscrossed                  of prompt templates defined by CLIP such as “A photo
Captions (CxC) retrieval tasks. Again, ALIGN achieves                     of a {classname}”. The class embedding is computed by
SOTA results in all metrics, especially by a large margin                 averaging the embeddings of all templates followed by an
on image-to-text (+22.2% R@1) and text-to-image (20.1%                    L2-normalization. We find that such ensembling gives 2.9%
R@1) tasks. Table 3 shows that ALIGN also outperforms                     improvement on ImageNet top-1 accuracy.
the previous SOTA on SITS task with an improvement of
5.7%. One interesting observation is that, despite being
much better on inter-modal tasks, ALIGN is not as impres-                Table 4. Top-1 Accuracy of zero-shot transfer of ALIGN to image
sive on intra-modal tasks. For instance, the improvements                classification on ImageNet and its variants.
on text-to-text and image-to-image retrieval tasks (in partic-                Model      ImageNet      ImageNet-R     ImageNet-A        ImageNet-V2
ular the former) are less significant compared to those on                    CLIP       76.2          88.9           77.2              70.1
image-to-text and text-to-image tasks. The performance on                     ALIGN      76.4          92.2           75.8              70.1
STS and SIS tasks is also slightly worse than VSE++ and
                 Scaling Up Visual and Vision-Language Representation Learning With Noisy Text Supervision

Table 5. ImageNet classification results. ALIGN is compared with WSL (Mahajan et al., 2018), CLIP (Radford et al., 2021),
BiT (Kolesnikov et al., 2020), ViT (Dosovitskiy et al., 2021), NoisyStudent (Xie et al., 2020), and Meta-Pseudo-Labels (Pham et al.,
2020).
                      Model (backbone)                          Acc@1 w/ frozen features Acc@1 Acc@5
                      WSL (ResNeXt-101 32x48d)                 83.6                             85.4        97.6
                      CLIP (ViT-L/14)                          85.4                             -           -
                      BiT (ResNet152 x 4)                      -                                87.54       98.46
                      NoisyStudent (EfficientNet-L2)           -                                88.4        98.7
                      ViT (ViT-H/14)                           -                                88.55       -
                      Meta-Pseudo-Labels (EfficientNet-L2)     -                                90.2        98.8
                      ALIGN (EfficientNet-L2)                  85.5                             88.64       98.67

                                                                      Table 6. VTAB (19 tasks) comparison between ALIGN and BiT-L.
5.3. Visual Classification w/ Image Encoder Only
                                                                        Model       All tasks       Natural     Specialized    Structured
On the ImageNet benchmark, we first freeze the learned                  Bit-L       78.72           -           -              -
visual features and only train the classification head.                 ALIGN       79.99±0.15      83.38       87.56          73.25
Afterwards we fine-tune all layers. We use basic data aug-
mentations including random cropping (same as in Szegedy
                                                                      To evaluate on smaller fine-grained classification bench-
et al. (2015)) and horizontal flip. In evaluation we apply a
                                                                      marks, we adopt a simple fine-tuning strategy for all tasks.
single central crop with ratio of 0.875. Following Touvron
                                                                      We use the same data augmentation and optimizer as in Ima-
et al. (2019), we use 0.8 scale ratio between training and
                                                                      geNet fine-tuning. Similarly, we first train the classification
evaluation to mitigate the resolution discrepancy introduced
                                                                      head and then fine-tune all layers, except with batch norm
by random crop. Specifically, train/eval resolution is
                                                                      statistics frozen. The train/eval resolution is fixed at 289/360.
289/360 with frozen visual features, and is 475/600 when
                                                                      We use batch size 256 and weight decay 1e-5. The initial
fine-tuning all variables.
                                                                      learning rate is set to 1e-2 and 1e-3 respectively, with cosine
In both stages of training, we use a global batch size of             learning rate decay in 20k steps. Table 7 compares ALIGN
1024, SGD optimizer with momentum 0.9, and learning                   with BiT-L (Kolesnikov et al., 2020) and SAM (Foret et al.,
rate decayed every 30 epochs with ratio 0.2 (100 epochs               2021) which both apply same fine-tuning hyper-parameters
in total). Weight decay is set to zero. With frozen visual            for all tasks.2 For small tasks like these, details in fine-
features, we use the initial learning rate of 0.1. When               tuning matter. So we list the baseline results in (Foret et al.,
fine-tuning all layers with use the initial learning rate of          2021) without using SAM optimization for a fairer compari-
0.01, and use 10x smaller learning rate on the backbone               son. Our result (average of three runs) is comparable to the
network compared to the classification head.                          SOTA results without tweaking on optimization algorithms.
Table 5 compares ALIGN with previous methods on the Im-               Table 7. Transfer learning results on Fine-grained Classifica-
ageNet benchmark. With frozen features, ALIGN slightly                tion Tasks. BiT-L (Kolesnikov et al., 2020) was trained with
outperforms CLIP and achieves SOTA result of 85.5% top-1              ResNet152 x 4 whereas SAM-baseline, SAM-final (Foret et al.,
accuracy. After fine-tuning ALIGN achieves higher accu-               2021) and ALIGN were trained with EfficientNet-L2.
racy than BiT and ViT models, and is only worse than Meta                                  Oxford       Oxford      Stanford
                                                                        Model                                                   Food101
Pseudo Labels which requires deeper interaction between                                    Flowers       Pets         Cars
ImageNet training and large-scale unlabeled data. Com-                  BiT-L               99.63       96.62          -           -
pared to NoisyStudent and Meta-Pseudeo-Labels which also                SAM-baseline        99.60       96.92        95.07       96.03
use EfficientNet-L2, ALIGN saves 44% FLOPS by using                     SAM-final           99.65       97.10        95.96       96.18
smaller test resolution (600 instead of 800).                           ALIGN               99.65       96.19        96.13       95.88

In VTAB eval, we follow a hyper-parameter sweep as shown
in the Appendix I in (Zhai et al., 2019) with 50 trials for each      6. Ablation Study
task. Each task is trained on 800 images and the hyperpa-             In the ablation study, we compare model performance
rameters are selected using the validation set of 200 images.         mostly on MSCOCO zero-shot retrieval and ImageNet K-
After the sweep, the selected hyperparameters are used to             Nearest-neighbor (KNN) tasks.3 We find these two met-
train on the combined training and validation splits of 1000
                                                                          2
images for each task. Table 6 reports the mean accuracy                     ViT (Dosovitskiy et al., 2021) uses different hyper-parameters
(including the breakdown results on each subgroup) with               for different tasks and hence is not included in comparison.
                                                                          3
standard deviation from three fine-tuning runs and shows                    For each image in the validation set of ImageNet, we retrieve
                                                                      its nearest neighbors from the training set w/ pre-trained image
that ALIGN outperforms BiT-L (Kolesnikov et al., 2020)                encoder. Recall@K metric is calculated based on if the groundtruth
with similar hyper-parameter selection method applied.                label of the query image appears in the top-K retrieved images.
                Scaling Up Visual and Vision-Language Representation Learning With Noisy Text Supervision

rics are representative and correlate well with other metrics   Table 8. Ablation study of key architecture parameters. Baseline
reported in the section above. If not mentioned, hyper-         model (first row) is trained with embedding dimension 640, using
parameters other than the ablated factor are kept the same      all negatives in the batch, and a learnable softmax temperature.
as in the baseline model.                                                                         MSCOCO         ImangeNet KNN
                                                                  Model
                                                                                             I2T R@1 T2I R@1               R@1
                                                                  B5 + BERT-base                  51.7    37.5              64.6
6.1. Model Architectures                                            w/ embedding dim=320          50.3    34.1              64.0
                                                                    w/ embedding dim=160          47.0    34.4              63.7
We first study the performance of ALIGN models using                w/ embedding dim=80           42.0    29.3              61.9
different image and text backbones. We train EfficientNet           w/ 50% in-batch negs          50.2    37.0              63.8
                                                                    w/ 25% in-batch negs          48.7    35.8              63.3
from B1 to L2 for the image encoder and BERT-Mini to
                                                                    w/ softmax temp=1/128         52.2    36.5              64.8
BERT-Large for the text encoder. We add an additional               w/ softmax temp=1/64          52.2    37.3              64.8
fully-connected layer with linear activation on top of B1,          w/ softmax temp=1/32          39.6    26.9              61.2
B3, B5 and L2 globally-pooled features to match the output
dimension of B7 (640). A similar linear layer is added to
all text encoders. We reduce the training steps to 1M in        6.2. Pre-training Datasets
ablation to save some runtime.
                                                                It’s also important to understand how the model performs
Figures 3 shows MSCOCO zero-shot retrieval and Ima-             when trained on different datasets with varying size. For
geNet KNN results with different combinations of image          this purpose, we train two models: EfficientNet-B7 + BERT-
and text backbones. Model quality improves nicely with          base and EfficientNet-B3 + BERT-mini on three different
larger backbones except that the ImageNet KNN metric            datasets: full ALIGN training data, 10% randomly sampled
starts to saturate from BERT-Base to BERT-Large with            ALIGN training data, and Conceptual Captions (CC-3M,
EfficientNet-B7 and EfficientNet-L2. As expected, scaling       around 3M images). CC-3M is much smaller so we train
up image encoder capacity is more important for vision          the model with 1/10 of the default number of steps. All
tasks (e.g., even with BERT-Mini text tower, L2 performs        models are trained from scratch. As shown in Table 9, a
better than B7 with BERT-Large). In image-text retrieval        large scale training set is essential to allow scaling up of
tasks the image and text encoder capacities are equally         our models and to achieve better performance. For instance,
important. Based on the nice scaling property shown in          models trained on ALIGN data clearly outperform those
Figure 3, we only fine-tune the model with EfficientNet-L2      trained on CC-3M data. On CC-3M, B7+BERT-base starts
+ BERT-Large as reported in Section 5.                          to overfit and performs even worse than B3+BERT-mini.
We then study key architecture hyperparameters including        Conversely, a larger model is required to fully utilize the
embedding dimensions, number of random negatives in the         larger dataset – the smaller B3+BERT-mini almost saturate
batch, and the softmax temperature. Table 8 compares a          at 10% of ALIGN data, while with the larger B7+BERT-
number of model variants to a baseline model (first row)        base, there is a clear improvement with full ALIGN data.
trained with the following settings: EfficientNet-B5 image
encoder, BERT-Base text encoder, embedding dimension                  Table 9. Ablation study of different training datasets.
640, all negatives in the batch, and a learnable softmax                                         MSCOCO          ImangeNet KNN
temperature.                                                      Model + Data
                                                                                            I2T R@1 T2I R@1                R@1
                                                                  B7 + BERT-base
Rows 2-4 of Table 8 show that model performance improves            + ALIGN full data          55.4      41.7               69.3
with higher embedding dimensions. Hence, we let the                 + ALIGN 10% data           52.0      39.2               68.8
                                                                    + CC-3M data               18.9      15.5               48.7
dimension scale with larger EfficientNet backbone (L2 uses
                                                                  B3 + BERT-mini
1376). Rows 5 and 6 show that using fewer in-batch neg-             + ALIGN full data          37.4      24.5               56.5
atives (50% and 25%) in the softmax loss will degrade the           + ALIGN 10% data           36.7      24.4               55.8
                                                                    + CC-3M data               22.1      17.3               48.9
performance. Rows 7-9 study the effect of the temperature
parameter in the softmax loss. Compared to the baseline
model that learns the temperature parameter (converged to       To understand better how data size scaling wins over the
about 1/64), some hand-selected, fixed temperatures could       increased noise, we further randomly sample 3M, 6M, and
be slightly better. However, we choose to use the learnable     12M ALIGN training data and compare them with the
temperature as it performs competitively and makes              cleaned CC-3M data on B7+BERT-base model. Table 10
learning easier. We also notice that the temperature usually    shows that while the ALIGN data performs much worse
quickly decrease to only around 1.2x of the converged           than CC data with the same size (3M), the model quality
values in the first 100k steps, and then slowly converges       trained on 6M and 12M ALIGN data rapidly catches up.
until the end of training.                                      Despite being noisy, ALIGN data outperforms Conceptual
                                                                Captions with only 4x size.
                      Scaling Up Visual and Vision-Language Representation Learning With Noisy Text Supervision

                 MSCOCO image-to-text retrieval R@1                     MSCOCO text-to-image retrieval R@1                           ImageNet NN accuracy
                                                                   45

                                                                   40                                                  70
           50
                                                                   35

                                                                   30                                                  60
           40
                                                                   25
                                                                                                                     50
           BERT-Mini BERT-Medium BERT-Base          BERT-Large BERT-Mini BERT-Medium BERT-Base            BERT-Large BERT-Mini BERT-Medium BERT-Base            BERT-Large

                                EfficientNet-B1              EfficientNet-B3           EfficientNet-B5      EfficientNet-B7           EfficientNet-L2

          Figure 3. Zero-shot image-text retrieval and ImageNet KNN accuracy@1 with different image and text encoder sizes.
    Table 10. Tradeoff between training data size and quality.                                                                  + “red”             + “Australia”      + “Madagascar”
                                    MSCOCO                      ImangeNet KNN
  Model + Data
                               I2T R@1 T2I R@1                            R@1
  B7 + BERT-base
    + ALIGN 12M data                 23.8             17.5                      51.4
    + ALIGN 6M data                  15.8             11.9                      47.9                                            + “forest”               + “desert”          + “orange”
    + ALIGN 3M data                   8.1              6.3                      41.3
    + CC-3M data                     18.9             15.5                      48.7

7. Analysis of Learned Embeddings
We build a simple image retrieval system to study the                                                                           + “blue”          + “purple”          + “from distance”

behaviors of embeddings trained by ALIGN. For demon-
stration purposes, we use an index consisting of 160M
CC-BY licensed images that are separate from our training
set. Figure 4 shows the top 1 text-to-image retrieval results                                                                 + “beige”             + “red”             + “purple”
for a handful of text queries not existing in the training
data. ALIGN can retrieve precise images given detailed
descriptions of a scene, or fine-grained or instance-level
concepts like landmarks and artworks. These examples                                                                            - “cars”            - “trees”            - “houses”
demonstrate that our ALIGN model can align images
and texts with similar semantics, and that ALIGN can
generalize to novel complex concepts.
                             “Van Gogh Starry Night ...”                                                                      - “flowers”          - “orange”             + “rose”
     “details”       “in black and white”     “on a canvas”         “in dark wood frame”




                                                                                                                               - “bridge”           - “waterfall”        - “mountain”
                                “Lombard street ...”
“view from bottom”     “view from top”    “bird’s eye view”             “in heavy rain”




                                                                                                  Figure 5. Image retrieval with image±text queries. We add (or
                                                                                                  subtract) text query embedding to (or from) the image query em-
                              “seagull in front of ...”
  “Golden Gate           “London Tower
                                                                                                  bedding, and then use the resulting embedding to retrieve relevant
                                                   “Sydney Harbour             “Rialto
     Bridge”                Bridge”                    Bridge”                 Bridge”            images using cosine similarity.

                                                                                                  image and text embeddings also emerge in ALIGN. We
                                                                                                  perform image retrieval using a combined image+text query.
Figure 4. Image retrieval with fine-grained text queries using                                    Specifically, given a query image and a text string, we add
ALIGN’s embeddings.                                                                               their ALIGN embeddings together and use it to retrieve
                                                                                                  relevant images.4 Figure 5 shows results for a variety of
Previously word2vec (Mikolov et al., 2013a;b) shows that                                              4
                                                                                                        We normalize the text and image embeddings before adding
linear relationships between word vectors emerge as a re-                                         them. We also tried various scale factor and found that a scale of 2
sult of training them to predict adjacent words in sentences                                      for the text embedding and 1 for the image embedding give best
and paragraphs. We show that linear relationships between                                         results as shown in the figure, although 1:1 also works well.
                 Scaling Up Visual and Vision-Language Representation Learning With Noisy Text Supervision

image+text queries. These examples not only demonstrate
                                                                    Table 11. Multimodal retrieval performance on Multi30K dataset.
great compositionality of ALIGN embeddings across vision
                                                                    The metric is the mean Recall (mR).
and language domains, but also show the feasibility of a new
                                                                              Model              en      de     fr     cs
paradigm of “search with multi-modal query” that would
otherwise be hard using only text query or image query. For                  zero-shot
instance, one could now look for the “Australia” or “Mada-                     M3 P           57.9    36.8   27.1    20.4
                                                                               ALIGNEN        92.2       -      -       -
gascar” equivalence of pandas, or turn a pair of black shoes                   ALIGNmling     90.2    84.1   84.9    63.2
into identically-looking shoes with the color of “beige”. Fi-                w/ fine-tuning
nally, as shown in the last three rows of Figure 5, removing                   M3 P           87.7    82.7   73.9    72.2
objects/attributes from a scene is possible by performing                      UC2            88.2    84.5   83.9    81.2
subtraction in the embedding space.

8. Multilingual ALIGN Model                                         dual-encoder model using a contrastive loss. The result-
                                                                    ing model, named ALIGN, is capable of cross-modal re-
One advantage of ALIGN is that the model is trained on              trieval and significantly outperforms SOTA VSE and cross-
noisy web image text data with very simple filters, and none        attention vision-language models. In visual-only down-
of the filters are language specific. Given that, we further lift   stream tasks, ALIGN is also comparable to or outperforms
the language constraint of the conceptual caption data pro-         SOTA models trained with large-scale labeled data.
cessing pipeline to extend the dataset to multilingual (cover-
ing 100+ languages) and match its size to the English dataset
(1.8B image-text pairs). A multilingual model ALIGNmling            10. Social Impacts and Future Work
is trained using this data. We created a new mutlilingual           While this work shows promising results from a method-
wordpiece vocabulary with size 250k to cover all languages.         ology perspective with a simple data collection method,
Model training follows the exact English configuration.             additional analysis of the data and the resulting model is
We test the multilingual model on Multi30k, a multilin-             necessary before the use of the model in practice. For in-
gual image text retrieval dataset extends Flickr30K (Plum-          stance, considerations should be made towards the potential
mer et al., 2015) to German (de) (Elliott et al., 2016),            for the use of harmful text data in alt-texts to reinforce such
French (fr) (Elliott et al., 2017) and Czech (cs) (Barrault         harms. On the fairness front, data balancing efforts may be
et al., 2018). The dataset consists of 31,783 images with           required to prevent reinforcing stereotypes from the web
5 captions per image in English and German and 1 cap-               data. Additional testing and training around sensitive reli-
tion per image in French and Czech. The train/dev/test              gious or cultural items should be taken to understand and
splits are defined in Young et al. (2014). We evaluate the          mitigate the impact from possibly mislabeled data.
zero-shot model performance of ALIGN and compare it                 Further analysis should also be taken to ensure that the de-
with M3 P (Huang et al., 2020a) and UC2 (Zhou et al., 2021).        mographic distribution of humans and related cultural items
The evaluation metric is mean Recall (mR), which computes           like clothing, food, and art do not cause model performance
the average score of Recall@1, Recall@5 and Recall@10               to be skewed. Analysis and balancing would be required if
on image-to-text retrieval and text-to-image retrieval tasks.       such models will be used in production.
Table 11 shows that the zero-shot performance of                    Finally, unintended misuse of such models for surveillance
ALIGNmling outperforms M3 P on all languages by a large             or other nefarious purposes should be prohibited.
margin, with the largest +57.8 absolution mR improvement
on fr. The zero-shot performance of ALIGNmling is even
comparable to the fine-tuned (w/ training splits) M3 P and          Acknowledgements
UC2 except on cs. On en, ALIGNmling performs slightly               This work was done with invaluable help from colleagues
worse on its counterpart ALIGNEN (trained on EN-only                from Google. We would like to thank Jan Dlabal and Zhe
data.)                                                              Li for continuous support in training infrastructure, Simon
                                                                    Kornblith for building the zero-shot & robustness model
9. Conclusion                                                       evaluation on ImageNet variants, Xiaohua Zhai for help
                                                                    on conducting VTAB evaluation, Mingxing Tan and Max
We present a simple method of leveraging large-scale noisy          Moroz for suggestions on EfficientNet training, Aleksei Tim-
image-text data to scale up visual and vision-language rep-         ofeev for the early idea of multimodal query retrieval, Aaron
resentation learning. Our method avoids heavy work on               Michelony and Kaushal Patel for their early work on data
data curation and annotation, and only requires minimal             generation, and Sergey Ioffe, Jason Baldridge and Krishna
frequency-based cleaning. On this dataset, we train a simple        Srinivasan for the insightful feedback and discussion.
                Scaling Up Visual and Vision-Language Representation Learning With Noisy Text Supervision

References                                                        Elliott, D., Frank, S., Sima’an, K., and Specia, L. Multi30k:
                                                                    Multilingual english-german image descriptions. In Pro-
Barrault, L., Bougares, F., Specia, L., Lala, C., Elliott, D.,
                                                                    ceedings of the 5th Workshop on Vision and Language,
  and Frank, S. Findings of the third shared task on multi-
                                                                    2016.
  modal machine translation. In Proceedings of the Third
  Conference on Machine Translation: Shared Task Papers,          Elliott, D., Frank, S., Barrault, L., Bougares, F., and Specia,
  pp. 304–323, 2018.                                                L. Findings of the second shared task on multimodal
                                                                    machine translation and multilingual image description.
Bossard, L., Guillaumin, M., and Van Gool, L. Food-101 –
                                                                    In Proceedings of the Second Conference on Machine
  mining discriminative components with random forests.
                                                                    Translation, Volume 2: Shared Task Papers, September
  In European Conference on Computer Vision, 2014.
                                                                    2017.
Caron, M., Misra, I., Mairal, J., Goyal, P., Bojanowski, P.,
  and Joulin, A. Unsupervised learning of visual features         Faghri, F., Fleet, D. J., Kiros, J. R., and Fidler, S. Vse++: Im-
  by contrasting cluster assignments. In Advances in Neural         proving visual-semantic embeddings with hard negatives.
  Information Processing Systems, 2020.                             In Proceedings of the British Machine Vision Conference,
                                                                    2018.
Chen, J., Hu, H., Wu, H., Jiang, Y., and Wang, C. Learning
  the best pooling strategy for visual semantic embedding.        Foret, P., Kleiner, A., Mobahi, H., and Neyshabur, B.
  In arXiv preprint arXiv:2011.04305, 2020a.                        Sharpness-aware minimization for efficiently improving
                                                                    generalization. In International Conference on Learning
Chen, T., Kornblith, S., Norouzi, M., and Hinton, G. A              Representations, 2021.
  simple framework for contrastive learning of visual rep-
  resentations. In Proceedings of International Conference        Frome, A., Corrado, G. S., Shlens, J., Bengio, S., Dean, J.,
  on Machine Learning, 2020b.                                       Ranzato, M. A., and Mikolov, T. Devise: A deep visual-
                                                                    semantic embedding model. In Proceedings of Neural
Chen, X., Fang, H., Lin, T.-Y., Vedantam, R., Gupta, S.,            Information Processing Systems, 2013.
  Dollar, P., and Zitnick, C. L. Microsoft coco captions:
  Data collection and evaluation server. In arXiv preprint        Gan, Z., Chen, Y.-C., Li, L., Zhu, C., Cheng, Y., and Liu, J.
  arXiv:1504.00325, 2015.                                           Large-scale adversarial training for vision-and-language
                                                                    representation learning. In Proceedings of Neural Infor-
Chen, Y.-C., Li, L., Yu, L., Kholy, A. E., Ahmed, F., Gan,          mation Processing Systems, 2020.
  Z., Cheng, Y., and Liu, J. Uniter: Universal image-text
  representation learning. In Proceedings of European             Grill, J.-B., Strub, F., Altché, F., Tallec, C., Richemond,
  Conference on Computer Vision, 2020c.                             P. H., Buchatskaya, E., Doersch, C., Pires, B. A., Guo,
                                                                    Z. D., Azar, M. G., Piot, B., Kavukcuoglu, K., Munos,
Deng, J., Dong, W., Socher, R., Li, L.-J., Li, K., and Fei-Fei,     R., and Valko, M. Bootstrap your own latent: A new
  L. Imagenet: A large-scale hierarchical image database.           approach to self-supervised learning. arXiv preprint
  In Proceedings of Conference on Computer Vision and               arXiv:2006.07733, 2020.
  Pattern Recognition, 2009.
                                                                  He, K., Fan, H., Wu, Y., Xie, S., and Girshick, R. Mo-
Desai, K. and Johnson, J. Virtex: Learning visual repre-            mentum contrast for unsupervised visual representation
  sentations from textual annotations. In arXiv preprint            learning. In Proceedings of the IEEE/CVF Conference
  arXiv:2006.06666, 2020.                                           on Computer Vision and Pattern Recognition, June 2020.

Devlin, J., Chang, M.-W., Lee, K., and Toutanova, K. Bert:        Hendrycks, D., Basart, S., Mu, N., Kadavath, S., Wang, F.,
  Pre-training of deep bidirectional transformers for lan-          Dorundo, E., Desai, R., Zhu, T., Parajuli, S., Guo, M.,
  guage understanding. In Proceedings of Conference of              Song, D., Steinhardt, J., and Gilmer, J. The many faces
  the North American Chapter of the Association for Com-            of robustness: A critical analysis of out-of-distribution
  putational Linguistics, 2019.                                     generalization. arXiv preprint arXiv:2006.16241, 2020.

Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn,          Hendrycks, D., Zhao, K., Basart, S., Steinhardt, J., and
  D., Zhai, X., Unterthiner, T., Dehghani, M., Minderer,            Song, D. Natural adversarial examples. CVPR, 2021.
  M., Heigold, G., Gelly, S., Uszkoreit, J., and Houlsby, N.
 An image is worth 16x16 words: Transformers for image            Hill, F., Reichart, R., and Korhonen, A. Simlex-999: Evalu-
  recognition at scale. In Proceedings of International             ating semantic models with (genuine) similarity estima-
 Conference on Learning Representations, 2021.                      tion. Computational Linguistics, 2015.
                Scaling Up Visual and Vision-Language Representation Learning With Noisy Text Supervision

Huang, H., Su, L., Qi, D., Duan, N., Cui, E., Bharti, T.,          Kolesnikov, A., Duerig, T., and Ferrari, V. The open
  Zhang, L., Wang, L., Gao, J., Liu, B., Fu, J., Zhang,            images dataset v4: Unified image classification, object
  D., Liu, X., and Zhou, M. M3p: Learning universal                detection, and visual relationship detection at scale. In-
  representations via multitask multilingual multimodal            ternational Journal of Computer Vision, 2020.
  pre-training. arXiv, abs/2006.02635, 2020a.
                                                                 Li, A., Jabri, A., Joulin, A., and van der Maaten, L. Learning
Huang, Z., Zeng, Z., Liu, B., Fu, D., and Fu, J. Pixel-bert:       visual n-grams from web data. In Proceedings of IEEE
 Aligning image pixels with text by deep multi-modal               International Conference on Computer Vision, 2017.
  transformers. arXiv preprint arXiv:2004.00849, 2020b.
                                                                 Li, J., Zhou, P., Xiong, C., and Hoi, S. Prototypical con-
Joulin, A., van der Maaten, L., Jabri, A., and Vasilache, N.       trastive learning of unsupervised representations. In Inter-
  Learning visual features from large weakly supervised            national Conference on Learning Representations, 2021.
  data. In European Conference on Computer Vision, 2015.
                                                                 Li, K., Zhang, Y., Li, K., Li, Y., and Fu, Y. Visual semantic
Juan, D.-C., Lu, C.-T., Li, Z., Peng, F., Timofeev, A., Chen,      reasoning for image-text matching. In Proceedings of
  Y.-T., Gao, Y., Duerig, T., Tomkins, A., and Ravi, S.            International Conference on Computer Vision, 2019.
  Graph-rise: Graph-regularized image semantic embed-
  ding. In Proceedings of ACM International Conference           Li, X., Yin, X., Li, C., Zhang, P., Hu, X., Zhang, L., Wang,
  on Web Search and Data Mining, 2020.                             L., Hu, H., Dong, L., Wei, F., Choi, Y., and Gao, J. Oscar:
                                                                   Object-semantics aligned pre-training for vision-language
Karpathy, A. and Fei-Fei, L. Deep visual-semantic align-           tasks. In Proceedings of European Conference on Com-
  ments for generating image descriptions. In Proceedings          puter Vision, 2020.
  of Conference on Computer Vision and Pattern Recogni-
  tion, 2015.                                                    Liu, F., Liu, Y., Ren, X., He, X., and Sun, X. Aligning
                                                                   visual regions and textual concepts for semantic-grounded
Karpathy, A., Joulin, A., and Li, F. Deep fragment embed-          image representations. In Advances in Neural Information
  dings for bidirectional image sentence mapping. In Ad-           Processing Systems, 2019a.
  vances in Neural Information Processing Systems, 2014.
                                                                 Liu, Y., Ott, M., Goyal, N., Du, J., Joshi, M., Chen, D.,
Kiros, J., Chan, W., and Hinton, G. Illustrative language          Levy, O., Lewis, M., Zettlemoyer, L., and Stoyanov, V.
  understanding: Large-scale visual grounding with image           Roberta: A robustly optimized bert pretraining approach.
  search. In Proceedings of the 56th Annual Meeting of             arXiv preprint arXiv:1907.11692, 2019b.
  the Association for Computational Linguistics (Volume 1:
  Long Papers), 2018.                                            Lu, J., Batra, D., Parikh, D., and Lee, S. Vilbert: Pre-
                                                                   training task-agnostic visiolinguistic representations for
Kiros, R., Salakhutdinov, R., and Zemel, R. S. Unifying            vision-and-language tasks. In Proceedings of Neural In-
  visual-semantic embeddings with multimodal neural lan-           formation Processing Systems, 2019.
  guage models. arXiv preprint arXiv:1411.2539.
                                                                 Mahajan, D., Girshick, R., Ramanathan, V., He, K., Paluri,
Kolesnikov, A., Beyer, L., Zhai, X., Puigcerver, J., Yung, J.,    M., Li, Y., Bharambe, A., and van der Maaten, L. Ex-
  Gelly, S., and Houlsby, N. Big transfer (bit): General vi-      ploring the limits of weakly supervised pretraining. In
  sual representation learning. In Proceedings of European        Proceedings of European Conference on Computer Vi-
  Conference on Computer Vision, 2020.                            sion, 2018.

Krause, J., Stark, M., Deng, J., and Fei-Fei, L. 3d object       Messina, N., Amato, G., Esuli, A., Falchi, F., Gennaro, C.,
  representations for fine-grained categorization. In Pro-        and Marchand-Maillet, S. Fine-grained visual textual
  ceedings of ICCV Workshop on 3D Representation and              alignment for cross-modal retrieval using transformer
  Recognition, 2013.                                              encoders. ACM Transactions on Multimedia Computing,
                                                                  Communications, and Applications, 2020.
Krishna, R., Zhu, Y., Groth, O., Johnson, J., Hata, K.,
  Kravitz, J., Chen, S., Kalantidis, Y., Li, L.-J., Shamma,      Mikolov, T., Chen, K., Corrado, G., and Dean, J. Efficient
  D. A., Bernstein, M., and Fei-Fei, L. Visual genome: Con-       estimation of word representations in vector space. arXiv
  necting language and vision using crowdsourced dense            preprint arXiv:1301.3781, 2013a.
  image annotations. International Journal of Computer
  Vision, 2016.                                                  Mikolov, T., Sutskever, I., Chen, K., Corrado, G., and Dean,
                                                                  J. Distributed representations of words and phrases and
Kuznetsova, A., Rom, H., Alldrin, N., Uijlings, J., Krasin,       their compositionality. In Advances in Neural Information
  I., Pont-Tuset, J., Kamali, S., Popov, S., Malloci, M.,         Processing Systems, 2013b.
               Scaling Up Visual and Vision-Language Representation Learning With Noisy Text Supervision

Misra, I. and Maaten, L. v. d. Self-supervised learning of    Raffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S.,
 pretext-invariant representations. In Proceedings of the       Matena, M., Zhou, Y., Li, W., and Liu, P. J. Exploring
 IEEE/CVF Conference on Computer Vision and Pattern             the limits of transfer learning with a unified text-to-text
 Recognition, June 2020.                                        transformer. Journal of Machine Learning Research,
                                                                2020.
Musgrave, K., Belongie, S., and Lim, S.-N. A metric learn-
 ing reality check. In Proceedings of European Conference     Recht, B., Roelofs, R., Schmidt, L., and Shankar, V. Do
 on Computer Vision, 2020.                                      imagenet classifiers generalize to imagenet? In Interna-
                                                                tional Conference on Machine Learning, pp. 5389–5400,
Nam, H., Ha, J.-W., and Kim, J. Dual attention networks for     2019.
  multimodal reasoning and matching. In Proceedings of
  Conference on Computer Vision and Pattern Recognition,      Sariyildiz, M. B., Perez, J., and Larlus, D. Learning visual
  2017.                                                         representations with caption annotations. arXiv preprint
                                                                arXiv:2008.01392, 2020.
Nilsback, M.-E. and Zisserman, A. Automated flower clas-
                                                              Sharma, P., Ding, N., Goodman, S., and Soricut, R. Con-
  sification over a large number of classes. In Indian Con-
                                                                ceptual captions: A cleaned, hypernymed, image alt-text
  ference on Computer Vision, Graphics and Image Pro-
                                                                dataset for automatic image captioning. In Proceedings
  cessing, Dec 2008.
                                                                of Annual Meeting of the Association for Computational
Parekh, Z., Baldridge, J., Cer, D., Waters, A., and Yang,       Linguistics, 2018.
  Y. Crisscrossed captions: Extended intramodal and in-       Socher, R., Karpathy, A., Le, Q. V., Manning, C. D., and
  termodal semantic similarity judgments for ms-coco. In        Ng, A. Y. Grounded compositional semantics for finding
  Proceedings of Conference of the European Chapter of          and describing images with sentences. Transactions of
  the Association for Computational Linguistics, 2021.          the Association for Computational Linguistics, 2014.
Parkhi, O. M., Vedaldi, A., Zisserman, A., and Jawahar,       Sun, C., Shrivastava, A., Sigh, S., and Gupta, A. Revisiting
  C. V. Cats and dogs. In IEEE Conference on Computer           unreasonable effectiveness of data in deep learning era. In
  Vision and Pattern Recognition, 2012.                         Proceedings of the International Conference on Computer
                                                                Vision, 2017.
Pennington, J., Socher, R., and Manning, C. GloVe: Global
  vectors for word representation. In Proceedings of the      Szegedy, C., Liu, W., Jia, Y., Sermanet, P., Reed, S.,
  2014 Conference on Empirical Methods in Natural Lan-          Anguelov, D., Erhan, D., Vanhoucke, V., and Rabinovich,
  guage Processing (EMNLP), 2014.                               A. Going deeper with convolutions. In Proceedings of
                                                                Conference on Computer Vision and Pattern Recognition,
Pham, H., Dai, Z., Xie, Q., Luong, M.-T., and Le, Q. V.         2015.
  Meta pseudo labels. In arXiv preprint arXiv:2003.10580,
  2020.                                                       Tian, Y., Krishnan, D., and Isola, P. Contrastive multiview
                                                                coding. In European Conference on Computer Vision,
Plummer, B. A., Wang, L., Cervantes, C. M., Caicedo, J. C.,     2020.
  Hockenmaier, J., and Lazebnik, S. Flickr30k entities:
                                                              Touvron, H., Vedaldi, A., Douze, M., and Jégou, H. Fix-
  Collecting region-to-phrase correspondences for richer
                                                                ing the train-test resolution discrepancy. In Advances in
  image-to-sentence models. In Proceedings of the Interna-
                                                                Neural Information Processing Systems, 2019.
  tional Conference on Computer Vision, 2015.
                                                              Wang, J., Song, Y., Leung, T., Rosenberg, C., Wang, J.,
Qi, D., Su, L., Song, J., Cui, E., Bharti, T., and Sacheti,    Philbin, J., Chen, B., and Wu, Y. Learning fine-grained
  A. Imagebert: Cross-modal pre-training with large-           image similarity with deep ranking. In Proceedings of
  scale weak-supervised image-text data. arXiv preprint-       Conference on Computer Vision and Pattern Recognition,
  arXiv:2001.07966, 2020.                                      2014.
Radford, A., Wu, J., Child, R., Luan, D., Amodei, D., and     Xie, Q., Luong, M.-T., Hovy, E., and Le, Q. V. Self-training
  Sutskever, I. Language models are unsupervised multitask      with noisy student improves imagenet classification. In
  learners. 2019.                                               Proceedings of Conference on Computer Vision and Pat-
                                                                tern Recognition, 2020.
Radford, A., Kim, J. W., Hallacy, C., Ramesh, A., Goh, G.,
  Agarawl, S., Sastry, G., Askell, A., Mishkin, P., Clark,    Yalniz, I. Z., Jégou, H., Chen, K., Paluri, M., and Maha-
  J., Krueger, G., and Sutskever, I. Learning transferable      jan, D. Billion-scale semi-supervised learning for image
  visual models from natural language supervision. 2021.        classification. arXiv preprint arXiv:1905.00546, 2019.
                Scaling Up Visual and Vision-Language Representation Learning With Noisy Text Supervision

Yang, Z., Dai, Z., Yang, Y., Carbonell, J. G., Salakhutdinov,
  R., and Le, Q. V. Xlnet: Generalized autoregressive
  pretraining for language understanding. In Advances in
  Neural Information Processing Systems, 2019.
You, Y., Li, J., Reddi, S., Hseu, J., Kumar, S., Bhojana-
  palli, S., Song, X., Demmel, J., Keutzer, K., and Hsieh,
  C.-J. Large batch optimization for deep learning: Train-
  ing bert in 76 minutes. In Proceedings of International
  Conference on Learning Representations, 2020.
Young, P., Lai, A., Hodosh, M., and Hockenmaier, J. From
  image descriptions to visual denotations: New similarity
  metrics for semantic inference over event descriptions.
  Transactions of the Association for Computational Lin-
  guistics, 2014.
Yu, F., Tang, J., Yin, W., Sun, Y., Tian, H., Wu, H.,
  and Wang, H. Ernie-vil: Knowledge enhanced vision-
  language representations through scene graph. arXiv
  preprint arXiv:2006.16934, 2020.
Zhai, A. and Wu, H.-Y. Classification is a strong baseline
  for deep metric learning. In Proceedings of the British
  Machine Vision Conference, 2019.
Zhai, X., Puigcerver, J., Kolesnikov, A., Ruyssen, P.,
  Riquelme, C., Lucic, M., Djolonga, J., Pinto, A. S.,
  Neumann, M., Dosovitskiy, A., Beyer, L., Bachem, O.,
  Tschannen, M., Michalski, M., Bousquet, O., Gelly, S.,
  and Houlsby, N. A large-scale study of representation
  learning with the visual task adaptation benchmark. arXiv
  preprint arXiv:1910.04867, 2019.

Zhang, Y., Jiang, H., Miura, Y., Manning, C. D., and Lan-
  glotz, C. P. Contrastive learning of medical visual repre-
  sentations from paired images and text. arXiv preprint
  arXiv:2010.00747, 2020.

Zhou, M., Zhou, L., Wang, S., Cheng, Y., Li, L., Yu,
  Z., and Liu, J. UC2: Universal cross-lingual cross-
  modal vision-and-language pre-training. arXiv preprint
  arXiv:2104.00332, 2021.
Zoph, B., Ghiasi, G., Lin, T.-Y., Cui, Y., Liu, H., Cubuk,
  E. D., and Le, Q. V. Rethinking pre-training and self-
  training. In Advances in Neural Information Processing
  Systems, 2020.
                  Scaling Up Visual and Vision-Language Representation Learning With Noisy Text Supervision

A. Remove Near-Duplicate Test Images from                         GloVe embeddings. ALIGN word embedding achieves the
   Training Data                                                  highest performance on the hard category, which similarity
                                                                  is difficult to distinguish from relatedness. This observa-
To detect near-duplicate images, we first train a separate        tion confirmed the hypothesis from Kiros et al. (2018) that
high-quality image embedding model following (Wang et al.,        image-based word embeddings are less likely to confuse
2014) with a large-scale labeled dataset as in (Juan et al.,      similarity with relatedness than text learned distributional-
2020), and then generate 4K clusters via k-means based on         based methods.
all training images of the embedding model. For each query
image (from the ALIGN dataset) and index image (from
test sets of downstream tasks), we find their top-10 nearest
clusters based on the  embedding distance. Each image is
then assigned to 10 3 buckets (all possible combinations of
3 clusters out of 10). For any query-index image pair that
falls into the same bucket, we mark it as near-duplicated if
their embedding cosine similarity is larger than 0.975. This
threshold is trained on a large-scale dataset built with human
rated data and synthesized data with random augmentation.

B. Evaluation on SimLex-999
The image-text co-training could also help the natural lan-
guage understanding as shown in Kiros et al. (2018). For in-
stance, with language only, it is very hard to learn antonyms.
In order to test this capability of ALIGN model, we also
evaluate the word representation from ALIGN model5 on
SimLex-999 (Hill et al., 2015), which is a task to com-
pare word similarity for 999 word pairs. We follow Kiros
et al. (2018) to report the results on 9 sub-tasks each con-
tains a subset of word pairs: all, adjectives, nouns, verbs,
concreteness quartiles (1-4), and hard.

           Table 12. SimLex-999 results (Spearman’s ρ).
                       GloVe    Picturebook    ALIGN
            all          40.8           37.3       39.8
            adjs         62.2           11.7       49.8
            nouns        42.8           48.2       45.9
       .    verbs        19.6           17.3       16.6
            conc-q1      43.3           14.4       23.9
            conc-q2      41.6           27.5       41.7
            conc-q3      42.3           46.2       47.6
            conc-q4      40.2           60.7       57.8
            hard         27.2           28.8       31.7

The results are listed in the Table 12 compared to Picture-
book (Kiros et al., 2018) and GloVe (Pennington et al., 2014)
embeddings. Overall the learned ALIGN perform better
than Picturebook but slightly worse than GloVe embeddings.
What is interesting is that the ALIGN word embeddings
has a similar trend of Picturebook embeddings, with bet-
ter performance on nouns and most concrete categories but
worse on adjs and less concrete categories compared to
    5
      As ALIGN uses the wordpiece tokens, one word can be split
into multiple pieces. We feed the wordpieces of a word into
ALIGN model and use the [CLS] token representation before the
project layers as the word embeddings.
