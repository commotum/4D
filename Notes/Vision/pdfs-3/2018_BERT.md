## 1. Basic Metadata
Title: BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding.
Evidence (Title page):
> "BERT: Pre-training of Deep Bidirectional Transformers for
> Language Understanding"

Authors: Jacob Devlin; Ming-Wei Chang; Kenton Lee; Kristina Toutanova.
Evidence (Title page):
> "Jacob Devlin"
> "Ming-Wei Chang Kenton Lee Kristina Toutanova"

Year: 2019.
Evidence (Title page):
> "arXiv:1810.04805v2 [cs.CL] 24 May 2019"

Venue: arXiv (cs.CL).
Evidence (Title page):
> "arXiv:1810.04805v2 [cs.CL] 24 May 2019"

## 2. One-Sentence Contribution Summary
BERT introduces a bidirectional Transformer pre-training approach (masked LM plus next sentence prediction) that can be fine-tuned with minimal task-specific changes to achieve state-of-the-art performance across many NLP tasks.

## 3. Tasks Evaluated
Task: MNLI (Multi-Genre Natural Language Inference)
Task type: Classification (entailment)
Dataset(s): MNLI (GLUE)
Domain: Natural language text, multi-genre sentence pairs
Evidence (Appendix B.1):
> "MNLI Multi-Genre Natural Language Inference
> is a large-scale, crowdsourced entailment classifi-
> cation task (Williams et al., 2018). Given a pair of
> sentences, the goal is to predict whether the sec-
> ond sentence is an entailment, contradiction, or
> neutral with respect to the first one."

Task: QQP (Quora Question Pairs)
Task type: Classification
Dataset(s): QQP (GLUE)
Domain: Natural language questions from Quora
Evidence (Appendix B.1):
> "QQP Quora Question Pairs is a binary classifi-
> cation task where the goal is to determine if two
> questions asked on Quora are semantically equiv-
> alent (Chen et al., 2018)."

Task: QNLI (Question Natural Language Inference)
Task type: Classification
Dataset(s): QNLI (GLUE; derived from SQuAD)
Domain: Natural language text (question, sentence pairs from the same paragraph)
Evidence (Appendix B.1):
> "QNLI Question Natural Language Inference is
> a version of the Stanford Question Answering
> Dataset (Rajpurkar et al., 2016) which has been
> converted to a binary classification task (Wang
> et al., 2018a). The positive examples are (ques-
> tion, sentence) pairs which do contain the correct
> answer, and the negative examples are (question,
> sentence) from the same paragraph which do not
> contain the answer."

Task: SST-2 (Stanford Sentiment Treebank)
Task type: Classification
Dataset(s): SST-2 (GLUE)
Domain: Movie review sentences
Evidence (Appendix B.1):
> "SST-2 The Stanford Sentiment Treebank is a
> binary single-sentence classification task consist-
> ing of sentences extracted from movie reviews
> with human annotations of their sentiment (Socher
> et al., 2013)."

Task: CoLA (Corpus of Linguistic Acceptability)
Task type: Classification
Dataset(s): CoLA (GLUE)
Domain: English sentences (linguistic acceptability judgments)
Evidence (Appendix B.1):
> "CoLA The Corpus of Linguistic Acceptability is
> a binary single-sentence classification task, where
> the goal is to predict whether an English sentence"

Task: STS-B (Semantic Textual Similarity Benchmark)
Task type: Other (similarity regression)
Dataset(s): STS-B (GLUE)
Domain: Sentence pairs from news headlines and other sources
Evidence (Appendix B.1):
> "STS-B The Semantic Textual Similarity Bench-
> mark is a collection of sentence pairs drawn from
> news headlines and other sources (Cer et al.,
> 2017). They were annotated with a score from 1
> to 5 denoting how similar the two sentences are in
> terms of semantic meaning."

Task: MRPC (Microsoft Research Paraphrase Corpus)
Task type: Classification
Dataset(s): MRPC (GLUE)
Domain: Sentence pairs from online news sources
Evidence (Appendix B.1):
> "MRPC Microsoft Research Paraphrase Corpus
> consists of sentence pairs automatically extracted
> from online news sources, with human annotations
> for whether the sentences in the pair are semanti-
> cally equivalent (Dolan and Brockett, 2005)."

Task: RTE (Recognizing Textual Entailment)
Task type: Classification (entailment)
Dataset(s): RTE (GLUE)
Domain: Natural language inference sentence pairs
Evidence (Appendix B.1):
> "RTE Recognizing Textual Entailment is a bi-
> nary entailment task similar to MNLI, but with
> much less training data (Bentivogli et al., 2009)."

Task: WNLI (Winograd NLI)
Task type: Classification (natural language inference)
Dataset(s): WNLI (GLUE)
Domain: Natural language inference dataset
Evidence (Appendix B.1):
> "WNLI Winograd NLI is a small natural lan-
> guage inference dataset (Levesque et al., 2011)."
Evidence on WNLI treatment (Table 1 note):
> "we exclude the problematic WNLI set."

Task: SQuAD v1.1 (Stanford Question Answering Dataset)
Task type: Other (extractive question answering / span prediction)
Dataset(s): SQuAD v1.1
Domain: Wikipedia passages
Evidence (Section 4.2 SQuAD v1.1):
> "The Stanford Question Answering Dataset
> (SQuAD v1.1) is a collection of 100k crowd-
> sourced question/answer pairs (Rajpurkar et al.,
> 2016). Given a question and a passage from
> Wikipedia containing the answer, the task is to
> predict the answer text span in the passage."

Task: SQuAD v2.0
Task type: Other (extractive question answering with unanswerable questions)
Dataset(s): SQuAD v2.0
Domain: Wikipedia passages
Evidence (Section 4.3 SQuAD v2.0):
> "The SQuAD 2.0 task extends the SQuAD 1.1
> problem definition by allowing for the possibility
> that no short answer exists in the provided para-
> graph, making the problem more realistic."

Task: SWAG (Situations With Adversarial Generations)
Task type: Classification; Reasoning / relational (commonsense inference)
Dataset(s): SWAG
Domain: Natural language sentence-pair completion
Evidence (Section 4.4 SWAG):
> "The Situations With Adversarial Generations
> (SWAG) dataset contains 113k sentence-pair com-
> pletion examples that evaluate grounded common-
> sense inference (Zellers et al., 2018). Given a sen-
> tence, the task is to choose the most plausible con-
> tinuation among four choices."

Task: CoNLL-2003 Named Entity Recognition (NER)
Task type: Classification (token-level tagging)
Dataset(s): CoNLL-2003 NER
Domain: Not specified in the paper.
Evidence (Section 5.3 Feature-based Approach with BERT):
> "by applying BERT to the CoNLL-2003 Named
> Entity Recognition (NER) task (Tjong Kim Sang
> and De Meulder, 2003)."
> "Following standard practice, we for-
> mulate this as a tagging task but do not use a CRF
> layer in the output."

## 4. Domain and Modality Scope
Is evaluation performed on a single domain, multiple domains within the same modality, or multiple modalities?
Answer: Multiple domains within the same modality (text). Evidence includes:
Evidence (Section 4.1 GLUE):
> "The General Language Understanding Evaluation
> (GLUE) benchmark (Wang et al., 2018a) is a col-
> lection of diverse natural language understanding
> tasks."
Evidence (Appendix B.1 and Section 4.2 for domain variety):
> "SST-2 The Stanford Sentiment Treebank is a
> binary single-sentence classification task consist-
> ing of sentences extracted from movie reviews"
> "MRPC Microsoft Research Paraphrase Corpus
> consists of sentence pairs automatically extracted
> from online news sources"
> "Given a question and a passage from
> Wikipedia containing the answer, the task is to
> predict the answer text span in the passage."

Does the paper claim domain generalization or cross-domain transfer?
Answer: Not claimed in the paper.

## 5. Model Sharing Across Tasks
Evidence for shared initialization and separate fine-tuned models (Section 3 and Figure 1 caption):
> "For fine-tuning, the BERT model is first initialized with
> the pre-trained parameters, and all of the param-
> eters are fine-tuned using labeled data from the
> downstream tasks. Each downstream task has sep-
> arate fine-tuned models, even though they are ini-
> tialized with the same pre-trained parameters."
> "Apart from output layers, the same architec-
> tures are used in both pre-training and fine-tuning.
> The same pre-trained model parameters are used to initialize
> models for different down-stream tasks. During fine-tuning,
> all parameters are fine-tuned."

Evidence for task-specific outputs (Section 3.2):
> "For each task, we simply plug in the task-
> specific inputs and outputs into BERT and fine-
> tune all the parameters end-to-end."
> "At the output, the token rep-
> resentations are fed into an output layer for token-
> level tasks, such as sequence tagging or question
> answering, and the [CLS] representation is fed
> into an output layer for classification, such as en-
> tailment or sentiment analysis."

| Task | Shared Weights? | Fine-Tuned? | Separate Head? | Evidence |
| --- | --- | --- | --- | --- |
| MNLI | Shared pre-trained init; separate fine-tuned model | Yes | Yes | "Each downstream task has sep- arate fine-tuned models..." (Section 3); "At the output, ... output layer for classification..." (Section 3.2) |
| QQP | Shared pre-trained init; separate fine-tuned model | Yes | Yes | Same as MNLI evidence (Section 3; Section 3.2) |
| QNLI | Shared pre-trained init; separate fine-tuned model | Yes | Yes | Same as MNLI evidence (Section 3; Section 3.2) |
| SST-2 | Shared pre-trained init; separate fine-tuned model | Yes | Yes | Same as MNLI evidence (Section 3; Section 3.2) |
| CoLA | Shared pre-trained init; separate fine-tuned model | Yes | Yes | Same as MNLI evidence (Section 3; Section 3.2) |
| STS-B | Shared pre-trained init; separate fine-tuned model | Yes | Yes | Same as MNLI evidence (Section 3; Section 3.2) |
| MRPC | Shared pre-trained init; separate fine-tuned model | Yes | Yes | Same as MNLI evidence (Section 3; Section 3.2) |
| RTE | Shared pre-trained init; separate fine-tuned model | Yes | Yes | Same as MNLI evidence (Section 3; Section 3.2) |
| WNLI | Shared pre-trained init; separate fine-tuned model | Yes | Yes | Same as MNLI evidence (Section 3; Section 3.2) |
| SQuAD v1.1 | Shared pre-trained init; separate fine-tuned model | Yes | Yes (token-level output layer for QA) | "Each downstream task has sep- arate fine-tuned models..." (Section 3); "token rep- resentations are fed into an output layer for token- level tasks, such as ... question answering" (Section 3.2) |
| SQuAD v2.0 | Shared pre-trained init; separate fine-tuned model | Yes | Yes (token-level output layer for QA) | Same as SQuAD v1.1 evidence (Section 3; Section 3.2) |
| SWAG | Shared pre-trained init; separate fine-tuned model | Yes | Yes | "Each downstream task has sep- arate fine-tuned models..." (Section 3); "The only task-specific parameters introduced is a vec- tor whose dot product with the [CLS] token rep- resentation C denotes a score for each choice which is normalized with a softmax layer." (Section 4.4 SWAG) |
| CoNLL-2003 NER | Shared pre-trained init; separate fine-tuned model | Yes (fine-tuning) | Yes (token-level tagging head) | "Each downstream task has sep- arate fine-tuned models..." (Section 3); "we for- mulate this as a tagging task" (Section 5.3) |

## 6. Input and Representation Constraints
Input structure (single or paired sequences):
Evidence (Input/Output Representations):
> "the in-
> put token sequence to BERT, which may be a sin-
> gle sentence or two sentences packed together."

Tokenization and vocabulary:
Evidence (Input/Output Representations):
> "We use WordPiece embeddings (Wu et al.,
> 2016) with a 30,000 token vocabulary."

Special tokens and segment embeddings:
Evidence (Input/Output Representations):
> "The first token of every sequence is always a special clas-
> sification token ([CLS])."
> "Sentence pairs are packed together into a
> single sequence. We differentiate the sentences in
> two ways. First, we separate them with a special
> token ([SEP]). Second, we add a learned embed-
> ding to every token indicating whether it belongs
> to sentence A or sentence B."

Input embedding composition:
Evidence (Input/Output Representations and Figure 2):
> "For a given token, its input representation is
> constructed by summing the corresponding token,
> segment, and position embeddings."
> "The input embeddings are the sum of the token embeddings, the segmenta-
> tion embeddings and the position embeddings."

Sequence length / context constraint:
Evidence (Appendix A.2 Pre-training Procedure):
> "we pre-train the model with sequence length of
> 128 for 90% of the steps. Then, we train the rest
> 10% of the steps of sequence of 512 to learn the
> positional embeddings."

Fixed or variable input resolution: Variable sequence length; pre-training uses length 128 for 90% of steps and length 512 for 10% of steps. Evidence above.

Fixed patch size: Not specified in the paper.

Fixed number of tokens: Not fixed; sequences vary in length (evidence above).

Fixed dimensionality (e.g., strictly 2D): Not specified in the paper (model is described as a token sequence).

Padding or resizing requirements: Not specified in the paper.

## 7. Context Window and Attention Structure
Maximum sequence length: Not specified in the paper; pre-training includes sequence length 512.
Evidence (Appendix A.2):
> "we pre-train the model with sequence length of
> 128 for 90% of the steps. Then, we train the rest
> 10% of the steps of sequence of 512 to learn the
> positional embeddings."

Sequence length fixed or variable: Variable (128 for most steps, 512 for the rest). Evidence same as above.

Attention type: Not specified in the paper (described as bidirectional self-attention in a Transformer).
Evidence (Section 3):
> "the BERT Transformer uses
> bidirectional self-attention"

Mechanisms to manage computational cost:
Evidence (Appendix A.2):
> "Longer sequences are disproportionately expen-
> sive because attention is quadratic to the sequence
> length. To speed up pretraing in our experiments,
> we pre-train the model with sequence length of
> 128 for 90% of the steps. Then, we train the rest
> 10% of the steps of sequence of 512 to learn the
> positional embeddings."

## 8. Positional Encoding (Critical Section)
Positional encoding mechanism: Position embeddings (type not specified: absolute vs relative not stated).
Evidence (Input/Output Representations and Figure 2):
> "For a given token, its input representation is
> constructed by summing the corresponding token,
> segment, and position embeddings."
> "The input embeddings are the sum of the token embeddings, the segmenta-
> tion embeddings and the position embeddings."

Where applied: Input embeddings only.
Evidence (Input/Output Representations and Figure 2):
> "For a given token, its input representation is
> constructed by summing the corresponding token,
> segment, and position embeddings."
> "The input embeddings are the sum of the token embeddings, the segmenta-
> tion embeddings and the position embeddings."

Whether positional encoding is fixed across experiments, modified per task, or ablated:
Not specified in the paper. The architecture is shared across pre-training and fine-tuning except for output layers.
Evidence (Figure 1 caption):
> "Apart from output layers, the same architec-
> tures are used in both pre-training and fine-tuning."

## 9. Positional Encoding as a Variable
Core research variable: Not specified in the paper.
Multiple positional encodings compared: Not specified in the paper.
Claim that PE choice is not critical or secondary: Not specified in the paper.

## 10. Evidence of Constraint Masking (Scale vs Structure)
Model sizes:
Evidence (Section 3):
> "We primarily report results on two model sizes:
> BERTBASE (L=12, H=768, A=12, Total Param-
> eters=110M) and BERTLARGE (L=24, H=1024,
> A=16, Total Parameters=340M)."

Dataset sizes (pre-training corpora):
Evidence (Pre-training data):
> "use the BooksCorpus (800M words) (Zhu et al.,
> 2015) and English Wikipedia (2,500M words)."

Scaling model size:
Evidence (Section 5.2):
> "We can see that larger models lead to a strict ac-
> curacy improvement across all four datasets"
> "scaling to extreme model sizes also
> leads to large improvements on very small scale
> tasks, provided that the model has been suffi-
> ciently pre-trained."

Scaling training steps:
Evidence (Appendix C.1):
> "BERTBASE achieves almost
> 1.0% additional accuracy on MNLI when
> trained on 1M steps compared to 500k steps."

Attribution to pre-training objectives / bidirectionality:
Evidence (Appendix A.4):
> "The core argument of this
> work is that the bi-directionality and the two pre-
> training tasks presented in Section 3.1 account for
> the majority of the empirical improvements"

## 11. Architectural Workarounds
Task-specific heads (classification and token-level outputs):
Evidence (Section 3.2):
> "At the output, the token rep-
> resentations are fed into an output layer for token-
> level tasks, such as sequence tagging or question
> answering, and the [CLS] representation is fed
> into an output layer for classification, such as en-
> tailment or sentiment analysis."

Special tokens / segment embeddings to support single or paired sequences:
Evidence (Input/Output Representations):
> "The first token of every sequence is always a special clas-
> sification token ([CLS])."
> "Sentence pairs are packed together into a
> single sequence. We differentiate the sentences in
> two ways. First, we separate them with a special
> token ([SEP]). Second, we add a learned embed-
> ding to every token indicating whether it belongs
> to sentence A or sentence B."

Compute management via shorter sequences:
Evidence (Appendix A.2):
> "Longer sequences are disproportionately expen-
> sive because attention is quadratic to the sequence
> length. To speed up pretraing in our experiments,
> we pre-train the model with sequence length of
> 128 for 90% of the steps. Then, we train the rest
> 10% of the steps of sequence of 512 to learn the
> positional embeddings."

SWAG-specific head:
Evidence (Section 4.4 SWAG):
> "The only task-specific parameters introduced is a vec-
> tor whose dot product with the [CLS] token rep-
> resentation C denotes a score for each choice
> which is normalized with a softmax layer."

## 12. Explicit Limitations and Non-Claims
Pre-training / fine-tuning mismatch due to masking:
Evidence (Section 3.1):
> "a downside is that we
> are creating a mismatch between pre-training and
> fine-tuning, since the [MASK] token does not ap-
> pear during fine-tuning."

[CLS] representation limitation:
Evidence (Section 3.1 footnote):
> "The vector C is not a meaningful sentence representation
> without fine-tuning, since it was trained with NSP."

Single-task fine-tuning only (non-claim of multitask results):
Evidence (Appendix B.1 footnote):
> "Note that we only report single-task fine-tuning results
> in this paper."

Stated limitations / future work beyond the above: Not specified in the paper.
