Length Generalization of Causal Transformers without Position Encoding
                                   Jie Wang1 * , Tao Ji2 * , Yuanbin Wu1 ,
                 Hang Yan5 , Tao Gui3 , Qi Zhang2 , Xuanjing Huang2,4 , Xiaoling Wang1
             1
                School of Computer Science, East China Normal University, Shanghai, China
                     2
                       School of Computer Science, Fudan University, Shanghai, China
         3
           Institute of Modern Languages and Linguistics, Fudan University, Shanghai, China
            4
               International Human Phenome Institutes, Shanghai, China 5 Shanghai AI Lab
         jiewang.cs@stu.ecnu.edu.cn, taoji@fudan.edu.cn, ybwu@cs.ecnu.edu.cn

                          Abstract                                      8         NoPE
                                                                                           1.2
                                                                                  NoPE λ = √ d                              6

        Generalizing to longer sentences is important                             RoPE




                                                                                                                                log Perplexity
                                                                        6




                                                                Entropy H̄i
                                                                                  RoPE NTK                                  5
        for recent Transformer-based language mod-
                                                                        4
        els. Besides algorithms manipulating explicit                                                                       4
        position features, the success of Transform-                    2
        ers without position encodings (NoPE) pro-                                                                          3

        vides a new way to overcome the challenge.                      0
                                                                                                                            2
        In this paper, we study the length generaliza-                        0          1000     2000
                                                                                                 Position i
                                                                                                              3000   4000

        tion property of NoPE. We find that although
        NoPE can extend to longer sequences than                Figure 1: Length generalization from 2K to 4K. For
        the commonly used explicit position encod-              different testing lengths (or, positions of sequences),
        ings, it still has a limited context length. We         dashed lines draw the log-perplexity of models (mea-
        identify a connection between the failure of            sured on validation set of the pre-training dataset), and
        NoPE’s generalization and the distraction of at-        solid lines represent the entropy of attention heads (av-
        tention distributions. We propose a parameter-          eraged on all heads).
        efficient tuning for searching attention heads’
        best temperature hyper-parameters, which sub-
        stantially expands NoPE’s context size. Experi-         time. Generalizing to unseen sentence length is cru-
        ments on long sequence language modeling, the           cial in many language model applications like re-
        synthetic passkey retrieval task and real-world         trieval augmented language models (Izacard et al.,
        long context tasks show that NoPE can achieve           2023), personalized language models (Wang et al.,
        competitive performances with state-of-the-art          2023), language-model-based agents (Park et al.,
        length generalization algorithms. The source
                                                                2023).
        code is publicly accessible1 .
                                                                   Departing from the standard ways of encoding
1       Introduction                                            positions, one may ask (following the principle
                                                                of parsimony) that are the explicit position fea-
Causal Transformer has been widely applied in                   tures necessary? The answer is no. Both empir-
modern language models. To help models recog-                   ically (Haviv et al., 2022) and theoretically (Chi
nize the correct ordering of words, it is common to             et al., 2023; Kazemnejad et al., 2023), the casu-
configure Transformers with explicit position en-               ally masked Transformers are shown to be able
codings (e.g., the sinusoidal embeddings in the orig-           to successfully model languages without any prior
inal development of Transformer (Vaswani et al.,                position encoding (NoPE). The finding calls for a
2017), the relative position encoding in T5 (Raffel             deeper understanding of implicit position informa-
et al., 2020), and the rotary position encoding in              tion in Transformer-based language models, and
GPT series (Su et al., 2021)). The setup of posi-               also inspires a new direction for length generaliza-
tion features provides flexibility to include prior             tion: without explicit position features, can NoPE
knowledge structure on describing distance, but it              generalize?
also brings the problem of length generalization:                  In this paper, we study the length generalization
language models trained with in-domain position                 property of NoPE. Our main findings are,
features can not handle longer sentences (i.e., those
with out-of-domain position features) in testing                • When extending to unseen sentence length, NoPE
    *   Equal contribution.                                       has less performance loss. However, beyond a
    1
        https://github.com/AntNLP/nope_head_scale                 certain range, NoPE also fails to extend, with
                                                             14024
                                                               1 Linguistics: ACL 2024, pages 14024–14040
                    Findings of the Association for Computational
                             August 11-16, 2024 ©2024 Association for Computational Linguistics
                           0.9
                  NoPE λ = √                                 8                            0.8
                                                                                 RoPE λ = √
        8                    d                                                              d                                  7
                                                                 8                        1.0
                           1.0                                                   RoPE λ = √
                  NoPE λ = √ d                               7                              d
                                                                                                                               6




                                                                                                                                   log Perplexity
        6                  1.1                                                            1.2
                  NoPE λ = √                                                     RoPE λ = √
                                                             6 6
Entropy H̄i



                             d                                                              d
                           1.2                                                            1.4
                  NoPE λ = √                                                     RoPE λ = √ d                                  5
                             d
        4                                                    5 4

                                                             4                                                                 4
        2                                                        2
                                                             3                                                                 3
        0                                                        0
                                                             2
              0          1000     2000        3000   4000                0              1000     2000        3000       4000
                                 Position i                                                     Position i


Figure 2: UniformScale modifies the temperature hyper-parameter of the SoftMax operator in self-attention layers
(Left, NoPE; Right, RoPE). NoPE can generalize to longer context by merely scaling the softmax scores. However,
this exact technique does not directly apply to RoPE models.


     no substantial difference observed when com-                    alization methods for explicit position encodings
     pared to explicit position encodings. For exam-                 (e.g., PI (Chen et al., 2023), YaRN (Peng et al.,
     ple, NoPE can effectively extend the training                   2024)).
     length by 20% (from 2K to 2.4K, Figure 1) with-
     out a significant increase in perplexity. In con-               2           Length Generalization of NoPE
     trast, the rotary position encoding (RoPE) is only              2.1          Language Modeling with NoPE
     capable of extending by 10%.
                                                                     Before diving into the length generalization prob-
• We analyze the failure cases of NoPE’s general-                    lem, we first briefly describe the NoPE models used
  ization and find that they always co-occur with                    in this paper. 2 Our default NoPE has 1.1B parame-
  the distraction of attention distributions: the at-                ters. It is trained from the TinyLlama (Zhang et al.,
  tention heads begin to allocate their weights to                   2024b) code base 3 , with training sequence length
  tokens evenly when NoPE’s extension perfor-                        L = 2048 and 50K steps (≈ 100B tokens). More
  mance begins to collapse. The connection be-                       details can be found in Section 4.1.
  tween NoPE’s generalization and concentration                         We also include the original TinyLlama model
  of attention heads suggests controlling the behav-                 which uses rotary position encoding (RoPE) for
  iors of attention heads during length extension.                   comparison. By default, both models are trained
                                                                     with identical settings.
• We show that by simply searching one tempera-
  ture hyper-parameter, NoPE’s length generaliza-                    2.2          Length Generalization
  tion can be significantly improved. For example,
                                                                     Given a language model (LM) with pre-trained
  by scaling the attention score by a factor of 1.2,
                                                                     maximal sequence length L, the goal of length
  NoPE can immediately generalize to over 4K
                                                                     generalization is to expand it to length L′ > L.
  tokens (Figure 1).
                                                                     Length generalization can be tested in a zero-shot
• Moreover, we developed an advanced version of                      manner (“train short, test long”) or with some fine-
  this strategy by searching temperature parame-                     tuning.
  ters for each head, in the light that different layers                Figure 1 depicts language modeling perfor-
  and heads exhibit varied behaviors. The proce-                     mances of NoPE (and RoPE). We can observe that,
  dure resembles a parameter-efficient fine-tuning,                  within the pre-training length (L = 2048), NoPE
  with an extremely small number of tunable pa-                      has a similar performance as RoPE, which agrees
  rameters (704 delta parameters over 1B model                       with existing works: casual masking can implicitly
  parameters). We show that the proposed method                      encode the positions of a sequence (Haviv et al.,
  can help NoPE to generalize further (Figure 4).                    2022; Chi et al., 2023).
                                                                        When the testing sequence length exceeds the
   We conduct length generalization experiments                      training length, we see that 1) NoPE’s length gen-
on long sequence language modeling, synthetic
                                                                             2
tasks (passkey retrieval), and LongBench. The re-                         For simplicity, we refer NoPE to both the implicit way of
                                                                     encoding positions and the language model trained without
sults show that NoPE enjoys a competitive exten-                     position encoding.
sion performances to state-of-the-art length gener-                     3
                                                                          https://github.com/jzhang38/TinyLlama

                                                            14025
                                                              2
eralization error (light blue dashed line, measured          Unlike explicit position encodings, NoPE has no
with log-perplexity) is lower than RoPE (light red        clear target objects to manipulate, thus it is quite
dashed line). 2) vanilla NoPE still has an increased      challenging to perform length generalization with-
perplexity than in-domain tests. Therefore, though        out fine-tuning on longer sequences. However, the
it is not a perfect solution, removing explicit po-       strong correlation between length extension and
sition encoding can effectively reduce the length         attention pattern transition suggests such an object,
generalization error. Next, we will try to find the       the entropy of attention heads.
reason for the failure of NoPE’s length generaliza-
tion, and also develop algorithms for improving           2.4   Uniform Attention Scale
it.                                                       We write the general scaled dot-product attention
                                                          as
2.3 Extension? Attention!                                                             (h) (h)
                                                                         (h)     eλqi ·kj
To analyze NoPE’s generalization failure, we first                     αij = P           (h) (h)
                                                                                                         (3)
                                                                                     λqi ·kk
                                                                                    e
see that since explicit position encodings have been                              k

dropped, the casual Transformer block is only left        where the scaling factor λ is the temperature hyper-
with three core modules, the embedding layer, feed-       parameter of the SoftMax operator. The prevalent
forward layers, and self-attention layers. The out-       setting is λ = √1d .
puts of the former two modules are independent of            Based on observations in Section 2.3, we know
their inputs’ position in sequence (i.e., no matter       that NoPE’s failure of length generalization might
which position, they always have the same output).        be correlated with distracted attention, hence we
Therefore, multi-head attention layers become our         can try to gradually increase the scale factor λ to
main target.                                              reconcentrate attention, and see whether the gener-
   We visualize the attention pattern of NoPE at          alization error can be reduced. Figure 2 visualizes
different lengths. Specifically, given a validation       the average entropy under different scale values
set with a size n and a target position i, we define      and the corresponding perplexity curves.
the average attention entropy Hi at position i, as           We first find that when increasing the scale factor
                                                          during length generalization evaluation (e.g., the
                      1 X (h)
              Hi =       Hi (x)                     (1)   pre-training scale λ = √1d is increased to λ = √  1.2
                                                                                                              d
                                                                                                                ),
                     n×m
                            x,h                           the inflection points of entropy curves are shifted
                      i
                      X                                   to longer lengths, at the same time, NoPE all gen-
        (h)                  (h)          (h)
     Hi (x) = −             αij (x) · log αij (x)   (2)   eralize to further positions (L=2k → L′ =4k). That
                      j=1                                 is, with all NoPE’s parameters frozen and only uni-
                        (h)                               formly increasing the softmax’s temperature, NoPE
where x is a sample, αij (x) is the attention prob-       can successfully generalize to unseen lengths.
ability of token i focusing on token j in the h-th           The same conclusion doesn’t hold for RoPE (Fig-
                                         (h)
attention head (h ∈ {1, 2, ..., m}), Hi (x) is the        ure 2 Right): no matter what value the scale takes
                                         (h)
entropy of the attention distribution αij (x) evalu-      (from λ=0.8 to λ=1.4), the inflection points of en-
ated at position i.                                       tropy curves remain almost unchanged, meaning
   The light solid lines in Figure 1 show the average     that it fails to generalize to longer lengths. On
entropy for NoPE (light blue) and RoPE (light red).       the other side, successful RoPE extension algo-
We can observe that, the inflection point of Hi is        rithms (e.g., RoPE-NTK in Figure 1) can control
highly consistent with the inflection point of per-       the distraction of entropy by explicitly manipulate
plexity. It implies that failed length generalization     position encodings. Therefore, though attention
of NoPE (and RoPE) might be connected to the              scaling has been used for RoPE (Su, 2021; Chiang
distraction of attention: attention heads begin to        and Cholak, 2022), it may contribute marginally to
allocate attention to more tokens. To further verify      RoPE’s generation.
the connection, we also draw a successful extension          We also find that extending NoPE to more dis-
algorithm for RoPE (RoPE-NTK (bloc97, 2023b)              tant positions generally requires a larger scale (i.e.,
which interpolates out-of-domain encodings to in-         a more concentrated attention distribution). As the
domain encodings). Its length generalization loss         position becomes further, the number of tokens in-
curve is flat, while its entropy curve also has no        volved in the attention calculation increases, the
steeply increasing point.                                 attention is more easily scattered, and therefore, a
                                                     14026
                                                       3
                Layer 0                             NoPE - Layer 10                            Layer 20
                                                                                                                                                     1.6
10                                      10                                                                                                  NoPE λ = √ d                                    4.0
                                                                                5                                               6           NoPE λ(h)




                                                                                                                                                                                                  log Perplexity
 0                                       0                                      0
                                                                                                                                                                                            3.5




                                                                                                                        Entropy H̄i
     0   1000    2000     3000   4000        0      1000   2000   3000   4000       0   1000    2000      3000   4000

                Layer 0
                                                          1.2
                                                 NoPE λ = √   - Layer 10                       Layer 20                         4
                                                            d
                                        10
10                                                                              5
                                                                                                                                                                                            3.0
                                                                                                                                2
 0                                       0                                      0
     0   1000    2000     3000   4000        0      1000   2000   3000   4000       0   1000    2000      3000   4000
                                                                                                                                                                                            2.5
                Layer 0                           NoPE λ(h) - Layer 10                         Layer 20
                                        10                                                                                      0
10
                                                                                5
                                                                                                                                      0            2000     4000              6000   8000
                                                                                                                                                           Position i
 0                                       0                                      0
     0   1000    2000     3000   4000        0      1000   2000   3000   4000       0   1000    2000      3000   4000

                                                                                                                        Figure 4: Comparing uniform and head-based scale
Figure 3: The attention entropy across all heads for the                                                                (denoted as λ(h) ). UniformScale fails eventually as the
original NoPE, head-based scaled NoPE and uniform-                                                                      perplexity increases with longer sequences. HeadScale
scaled NoPE, with each model represented in a separate                                                                  is capable of handling much longer context by assigning
row. The attention heads exhibit divergent patterns.                                                                    different scale factors to each attention head.


larger scaling factor is needed to concentrate the                                                                      through automated hyperparameter search, consid-
attention. In particular, for our NoPE model, gen-                                                                      ering both parameter efficiency and data efficiency.
eralizing to twice the pre-training length requires                                                                     As a result, head-based scaling generalizes better
about 1.2 times the scale, four times the length re-                                                                    than uniform scaling. Moreover, correlation anal-
quires about 1.5 times the scale, and eight times                                                                       ysis shows that within each layer, the smaller the
the length requires about 1.8 times the scale. Ap-                                                                      converged entropy (i.e., the more concentrated at-
pendix B reports the fitted function of the scaling                                                                     tention), the larger the required scaling factor to
factor with respect to the generalization length L′ .                                                                   maintain that concentration.
   Finally, we note remark that the attention scaling
factor in this section takes the same value for all                                                                     3.1               Visual Analysis
positions, including the pre-training length (uni-                                                                      The entropy values span a broad spectrum, with
form scaling). We experimented with a piecewise                                                                         each attention head demonstrating a distinct atten-
function which use the original scale within the                                                                        tion pattern. In Figure 3, certain attention heads
pre-training positions, and a more concentrated at-                                                                     show a highly concentrated pattern, with entropy
tention scale for the extrapolated positions. We also                                                                   values converging to ≈ 1, while others exhibit a
try position-dependent functions, where the scale                                                                       highly dispersed pattern, with entropy values con-
increases with position. However, none of these                                                                         verging to ≈ 10. The full head visualization of
methods could further improve generalization. We                                                                        Figure 3 is located in Appendix D.
speculate that if the attention at earlier positions is                                                                    This phenomenon casts doubt on uniform scal-
not highly concentrated, the learned token represen-                                                                    ing — how can a single scaling factor cater to di-
tations may hinder the concentration of attention at                                                                    verse attention heads? Inspired by this, we further
latter positions. We leave a deeper discussion and                                                                      propose a head-based scale method.
analysis of this observation in future work.
                                                                                                                        3.2               Head-based Scale
3        Head-based Attention Scale                                                                                     We reformulate the uniform attention scale as head-
After verifying that the attention scaling can help                                                                     base attention scales
NoPE generalizing, we delved deeper into the                                                                                                               (h)          (h)   (h)

multi-head attention mechanism and posed a new                                                                                                     (h)   eλ qi ·kj
                                                                                                                                                  αij = P         (h) (h)
                                                                                                                                                                                             (4)
question, “Does each attention head require a                                                                                                            ke
                                                                                                                                                            λ(h) qi ·kk

unique scaling factor?”
   In this section, we first visualize the average en-                                                                  where λ(h) is a unique attention scaling factor for
tropy curves for each head and find that they have                                                                      each head, totaling 704. Compared to a uniform
different attention patterns. Hence we propose to                                                                       attention scale, 704 head-based scales make it diffi-
replace the uniform scaling with head-based scal-                                                                       cult to determine the optimal values by grid search.
ing (from one factor to 22 × 32 = 704 factors). To                                                                      Similar to AutoML (He et al., 2021), we model
address the issue of an exploding search space, we                                                                      the scales’ optimal search as a parameter-efficient
efficiently determine the values of scaling factors                                                                     fine-tuning task. Given a NoPE model M and a set
                                                                                                                   14027
                                                                                                                     4
                                Layers (0 ∼ 21)                             the pre-training phase. However, its length general-
                       1   4     7     10     13         16   19            ization results are quite unstable, with most being
                                                                            subpar, as the optimal scale often deviates signifi-
          12                                                                cantly from the default value. We propose another
                                                                            approach to utilize the best uniform scale from the
          10
                                                                            grid search as the initial value. The ablation study
 Entropy H8192
(h)




                 8                                                          for the initialization approach is in Section 4.5.
                                                                               Figure 4 compares the two generalization meth-
                 6
                                                                            ods of NoPE, uniform scale versus head-based
                 4                                                          scales. Head-based scale exhibits better general-
                                                                            ization than the uniform scale, achieving a lower
                 2
                                                                            log-PPL by 0.2 at 4K positions (2×L) and by 0.8
                 0                                                          at 8K positions (4×L). The average entropy Hi
                     1.0          1.5              2.0             2.5
                     √
                       d
                                  √
                                    d
                                                   √
                                                     d
                                                                   √
                                                                     d      of the head-based scale is higher than that of the
                                        λ(h)                                uniform scale, suggesting that the uniform scale
Figure 5: Correlation analysis for head-based scale                         method over-concentrates attention, particularly for
when extended to 8K context. The analysis was con-                          some heads that inherently have more distracted
ducted on the converged entropy values at 8K position,                      patterns.
in relation to the scale searched. Each data point repre-                      Figure 5 shows the correlation between the con-
sents a unique attention head.                                              verged entropy and the searched scale. To save
                                                                            space, we uniformly sampled 7 layers and all their
                                                                            respective heads. We observed that the correlation
of head-based scales {λ(1) , λ(2) , . . . , λ(m) }, we fix
                                                                            is layer-dependent, within each layer, heads with
the model M and define the head-based scales as
                                                                            more concentrated attention (i.e., lower entropy)
trainable parameters θ = {λ(1) , λ(2) , . . . , λ(m) }.
                                                                            searched for larger scales, while heads with more
We aim to find an optimal set of values θ∗ =
                                                                            dispersed attention (i.e., higher entropy) searched
{λ∗(1) , λ∗(2) , . . . , λ∗(m) }, that allows the model
                                                                            for smaller scales. The result is as expected, the
M(θ∗ ) to successfully extend to the target length
                                                                            more concentrated the attention pattern, the larger
L′ . To this end, we optimize the language model-
                                                                            the scaling factor needed to maintain its focus. Fur-
ing loss function LLM on the pre-training dataset
                                                                            thermore, we observed that attention heads in lower
D with length L′ and size n′ , n′ ≪ n.
                                                                            layers are generally more dispersed, whereas heads
                                                                            in higher layers are generally more concentrated
                      θ∗ = minimize LLM (M(θ, x))                  (5)
                               x∈D                                          (note that this is not strictly observed).

The search process is highly efficient. (1) The num-                        4        Experiment
ber of tunable parameters is extremely small, only
704 delta parameters over 1B model parameters;                              We train a NoPE base model from scratch and
2) The amount of training tokens for fine-tuning is                         investigate its capability in length generalization.
extremely small too, only 0.03% of the pre-training                         We conduct length generalization experiments on
data.                                                                       long sequence language modeling, synthetic tasks
   In addition, to ensure that the attention is recon-                      (passkey retrieval), and real-world long context
centrated instead of distracted by the scaling fac-                         tasks (LongBench). Detailed experiment setup can
tors, we apply a focus constraint during the opti-                          be found in Appendix A.
mization of Equation 5
                                                                            4.1       NoPE pre-trained model
                                         1                                  For a fair comparison with RoPE, we train a NoPE
                                 λ(∗) ≥ √                          (6)
                                          d                                 model with 1.1B parameters from the TinyLlama
                                                                            (Zhang et al., 2024b) code base4 . The NoPE model
                                                                            has 22 layers of Transformer blocks, 32 attention
Initializing HeadScale In practice, we found
                                                                            heads per layer, 2048 embedding size. The model
that the initial value of head-based scales has a
                                                                            is trained on Slimpajama (Soboleva et al., 2023)
significant impact on the search of θ∗ . An obvious
approach is to use the default value λ(∗) = √1d from                             4
                                                                                     https://github.com/jzhang38/TinyLlama

                                                                         14028
                                                                           5
        Model        Avg.   arc_challenge        arc_easy    boolq       hellaswag     openbookqa         piqa    winogrande
        RoPE         46.1          24.3            44.9          59.7         43.5           29.8         67.3       53.3
        NoPE         46.2          24.0            44.9          58.1         43.4           31.8         68.4       52.9

                        Table 1: Commonsense reasoning ability of the pre-trained base models.

                                   FT                            PG19                                Proof-pile
            Model
                             L ′
                                    Tokens        2K        4K          8K       16K   2K           4K      8K      16K
                                                          Original LMs
            RoPE               -             -   14.5  491.4     488.5    599.5        3.5     303.0      432.1   759.5
            NoPE               -             -   14.6  326.9 > 103 > 103               3.5     117.4      > 103   > 103
            BLOOM              -             -   27.7  158.0     264.6    403.4        6.9      74.1      176.2   334.5
            MPT                -             -   10.6  103.6     361.6    345.1        2.8      70.1      > 103   > 103
                                                      Generalization for RoPE
            NTKzero            -            -    14.5   14.9      22.8     80.4        3.5          3.3     4.1     13.3
            YaRNzero           -            -    14.5   14.5      15.0     17.1        3.5          3.3     3.2      3.6
                             4K           6M     16.0   15.9     551.9 > 103           3.8          3.4   307.9    633.8
            PIfair           8K          13M     17.4   17.1      17.1    752.8        4.0          3.6     3.4    406.3
                            16K          30M     18.7   18.4      18.3     18.2        4.3          3.9     3.6      3.6
                             4K           6M     15.5   15.4     545.2 > 103           3.7          3.4   351.5    698.2
            YaRNfair         8K          13M     15.7   15.4      15.5    794.6        3.8          3.4     3.2    492.8
                            16K          30M     15.9   15.6      15.4     15.5        3.8          3.5     3.2      3.2
                             4K          33M     15.2   15.0     623.8    951.7        3.6          3.3   334.4    595.5
            PIraw            8K          66M     15.4   15.1      15.0    909.6        3.6          3.3     3.0    463.0
                            16K         131M     15.6   15.3      15.0     14.9        3.7          3.3     3.0      3.0
                             4K          33M     15.1   15.0     573.3    951.4        3.6          3.3   358.8    656.8
            YaRNraw          8K          66M     15.1   14.8      14.8    816.0        3.6          3.3     3.1    501.5
                            16K         131M     15.0   14.8      14.5     14.5        3.6          3.3     3.0      3.0
                                                      Generalization for NoPE
               1.2
            λ= √ d
                               -             -   15.0   16.0     513.7 > 103           3.6          3.3   175.3   > 103
               1.5
            λ= √ d
                               -             -   19.0   20.2      45.3    224.1        3.9          3.7     4.9    99.2
               1.8
            λ= √ d
                               -             -   30.4   42.4      69.1    198.8        5.1          5.6     8.5    38.2
                             4K            6M    14.8   15.3     404.5 > 103           3.5          3.2   153.4   > 103
            λ(h)             8K           13M    15.7   15.3      21.1    721.7        3.6          3.3     3.2   318.5
                            18K           30M    18.3   19.0      18.8     30.4        4.0          3.7     3.3     4.1

Table 2: Sliding window perplexity of different context window extension methods tested on PG19 and ProofPile.
The “fair” and “raw” versions of PI and YaRN differ from the training data, as detailed in Appendix A. The notation
λ = ∗ denotes uniform attention scale by the given number, and λ(h) represents head-based scale.


joint with Starcoderdata (Li et al., 2023) by 50K                       4.2     Long Sequence Language Modeling
steps (≈ 100B tokens) with sequence length L =                          Success on long sequence language modeling tasks
2048.                                                                   is essential for length generalization. A method that
                                                                        does not perform well in language modeling proba-
  All settings are kept identical to those of TinyL-                    bly won’t handle real-world long-context tasks.
lama, including the model architecture, training                        Settings. To evaluate the long sequence language
data, training procedure, and hyper-parameters, ex-                     modeling performances, we test our NoPE-based
cept that the rotary position embedding (RoPE) in                       methods and RoPE-based baselines on PG19 (Rae
TinyLlama is removed, making it a NoPE model,                           et al., 2020) and proof-pile (Azerbayev et al., 2022)
and the learning rate is set to 3.5 × 10−4 .                            datasets. For each dataset, we sample a subset of
                                                                        the test set and evaluate on 2M tokens using sliding
                                                                        window evaluation (S = 256) suggested by Press
  Following TinyLlama, we evaluate the common-
                                                                        et al. (2022). We report the perplexity (PPL) of the
sense reasoning ability of the NoPE model and
                                                                        models in Table 2.
report acc_norm in Table 1. We compare with the
TinyLlama checkpoint that is trained on 100B to-                        Main results. Firstly, by comparing the origi-
kens. The purpose of this experiment is to prove                        nal language models, NoPE’s perplexity (PPL) is
the NoPE base model performs on par with RoPE.                          comparable to RoPE’s for lengths within the train-
                                                                  14029
                                                                    6
                                           RoPE acc 1.00                                                                     YaRNfair 8k acc 0.90                                                                     YaRNfair 16k acc 0.81
                  0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0                    0 1.0 1.0 1.0 0.9 0.9 1.0 0.2 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0                         0 1.0 0.0 1.0 0.9 1.0 0.3 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0
                 10 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0                   10 1.0 1.0 1.0 1.0 1.0 1.0 0.9 0.1 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0                        10 0.5 0.4 1.0 1.0 0.9 0.5 0.2 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0
                 20 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0                   20 1.0 1.0 1.0 1.0 1.0 1.0 1.0 0.7 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0                        20 0.6 0.8 1.0 1.0 1.0 1.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0
 Passkey Depth




                                                                                      Passkey Depth




                                                                                                                                                                                Passkey Depth
                 30 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0                   30 1.0 1.0 1.0 1.0 1.0 1.0 1.0 0.9 0.7 0.0 0.0 0.0 0.0 0.0 0.0 0.0                        30 1.0 1.0 1.0 1.0 0.9 0.9 0.9 0.6 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0
                 40 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0                   40 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 0.9 0.0 0.0 0.0 0.0 0.0 0.0 0.0                        40 0.9 1.0 0.9 1.0 0.9 0.9 1.0 0.6 0.1 0.0 0.0 0.0 0.0 0.0 0.0 0.0
                 50 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0                   50 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0                        50 0.9 0.9 1.0 0.9 1.0 1.0 0.9 0.9 0.6 0.0 0.0 0.0 0.0 0.0 0.0 0.0
                 60 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0                   60 1.0 1.0 1.0 1.0 1.0 1.0 1.0 0.8 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0                        60 1.0 1.0 1.0 1.0 1.0 1.0 1.0 0.9 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0
                 70 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0                   70 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0                        70 1.0 0.9 1.0 1.0 0.9 1.0 1.0 1.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0
                 80 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0                   80 1.0 0.9 1.0 0.6 0.9 1.0 1.0 1.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0                        80 0.3 1.0 1.0 1.0 1.0 1.0 1.0 0.9 0.9 0.0 0.0 0.0 0.0 0.0 0.0 0.0
                 90 1.0 1.0 0.8 1.0 1.0 1.0 0.9 1.0 0.6 0.0 0.0 0.0 0.0 0.0 0.0 0.0                   90 0.0 0.9 0.8 0.3 0.2 0.4 0.7 0.9 0.8 0.2 0.0 0.0 0.0 0.0 0.0 0.0                        90 0.0 0.7 0.4 0.8 0.8 0.8 0.6 0.9 0.5 0.0 0.0 0.0 0.0 0.0 0.0 0.0
                        512    1024    1536  2048   2560       3072    3584    4096                          2048   4096    6144  8192 10240            12288   14336   16384                          4096   8192    12288 16384 20480           24576   28672   32768
                                          Context Length                                                                       Context Length                                                                             Context Length
                                                                                                                                    (h)                                                                                      (h)
                                           NoPE acc 1.00                                                                        λ         8k acc 0.93                                                                    λ         18k acc 0.81
                  0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 0.5 0.0 0.2 0.1 0.0                    0 0.9 1.0 1.0 0.9 1.0 0.9 0.9 0.8 0.7 0.5 0.5 0.3 0.4 0.1 0.3 0.0                         0 1.0 0.9 0.8 0.5 0.6 0.5 0.5 0.4 0.6 0.4 0.1 0.2 0.4 0.1 0.1 0.1
                 10 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 0.9 0.4 0.0 0.0 0.0 0.0                   10 1.0 1.0 1.0 0.9 0.8 0.8 0.5 0.9 0.8 0.9 1.0 0.8 0.8 0.7 0.7 0.2                        10 0.8 0.6 0.6 0.5 0.7 0.6 0.5 0.7 0.6 0.7 0.3 0.1 0.4 0.5 0.1 0.1
                 20 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 0.9 0.6 0.6 0.3                   20 1.0 1.0 1.0 0.9 0.8 0.9 0.9 1.0 0.9 0.9 0.9 0.9 0.8 0.7 0.9 0.8                        20 0.8 0.7 0.8 0.6 0.9 0.9 0.9 0.9 0.8 1.0 0.9 0.7 0.6 0.6 0.8 0.4
 Passkey Depth




                                                                                      Passkey Depth




                                                                                                                                                                                Passkey Depth
                 30 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 0.9 0.9                   30 0.9 1.0 1.0 0.8 1.0 1.0 0.9 0.9 0.9 0.8 0.9 0.9 0.9 0.7 1.0 0.6                        30 0.8 0.8 0.9 1.0 1.0 1.0 0.9 0.7 0.9 0.8 0.8 1.0 0.6 0.9 0.7 0.8
                 40 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0                   40 0.9 0.9 0.9 1.0 1.0 0.9 0.9 0.9 0.8 0.8 0.8 0.7 0.6 0.8 0.7 0.8                        40 0.9 0.9 0.6 0.9 0.9 0.8 1.0 0.7 0.5 0.5 0.4 0.7 0.8 0.8 0.7 0.9
                 50 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 0.9                   50 1.0 0.9 0.9 1.0 0.9 1.0 1.0 1.0 0.7 0.8 0.4 0.4 0.4 0.6 0.4 0.6                        50 0.9 0.9 0.9 0.7 0.8 0.6 0.8 0.6 0.6 0.5 0.9 0.7 0.4 0.8 0.9 0.9
                 60 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0                   60 0.9 1.0 0.9 1.0 1.0 0.9 0.8 0.9 0.5 0.4 0.5 0.5 0.6 0.3 0.1 0.2                        60 0.7 1.0 0.7 1.0 0.9 0.7 0.4 0.5 0.5 0.3 0.3 0.6 0.6 0.6 0.3 0.2
                 70 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 0.7 0.5                   70 0.8 1.0 0.9 0.9 1.0 0.9 0.9 0.8 0.9 0.8 0.8 0.8 0.8 0.6 0.2 0.2                        70 0.8 0.9 1.0 1.0 0.7 1.0 0.9 0.8 0.3 0.7 0.2 0.1 0.3 0.1 0.0 0.0
                 80 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 0.6 0.2 0.0 0.0                   80 1.0 0.8 1.0 1.0 1.0 0.9 1.0 0.8 0.8 0.7 1.0 0.5 0.5 0.3 0.0 0.0                        80 1.0 1.0 1.0 1.0 0.9 1.0 0.9 1.0 0.8 0.6 0.5 0.2 0.2 0.0 0.0 0.0
                 90 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 0.7 0.6 0.0 0.0 0.0 0.0 0.0                   90 0.9 1.0 0.9 1.0 1.0 1.0 0.9 1.0 1.0 0.8 0.9 0.6 0.0 0.0 0.0 0.0                        90 0.9 0.8 1.0 0.9 1.0 0.7 0.9 0.8 0.6 0.0 0.0 0.0 0.0 0.0 0.0 0.0
                        512    1024    1536  2048   2560       3072    3584    4096                          2048   4096    6144  8192 10240            12288   14336   16384                          4096   8192    12288 16384 20480           24576   28672   32768
                                          Context Length                                                                       Context Length                                                                             Context Length



Figure 6: The figures illustrate the passkey retrieval accuracy for both RoPE and NoPE methods. The vertical dashed
line represents the context length of the models, which could be either the pre-training length or the fine-tuning
length. The title of each sub-figure indicates the average accuracy within the model’s context length. Notably, NoPE
demonstrates robust performance even beyond the model’s context window, indicating significant potential for
generalization.


ing distribution, confirming the findings of Haviv                                                                                            scale method is competitive with these RoPE base-
et al. (2022); Chi et al. (2023). However, all LMs,                                                                                           lines, especially the Proof-pile dataset. However
including ALiBi models, fail to generalize out-of-                                                                                            RoPE baselines (PI, YaRN) still benefit from more
the-distribution, indicating that explicit positional                                                                                         training tokens, and the head-based scale on NoPE
encoding is not the main reason for their failure in                                                                                          reaches its upper limit.
generalization. Current work on length generaliza-                                                                                               In summary, the head-based scale generaliza-
tion still focuses mainly on manipulating positional                                                                                          tion method for NoPE slightly outperforms RoPE’s
encoding. Therefore, the length generalization is-                                                                                            early generalization method NTK, but still lags be-
sue within causal Transformer networks warrants a                                                                                             hind the recently introduced YaRN, particularly in
reanalysis and reinterpretation.                                                                                                              near-distance PPL performance. Considering the
   Secondly, by comparing the two generalization                                                                                              significant challenge of generalizing NoPE com-
methods for NoPE proposed in this paper, the uni-                                                                                             pared to RoPE (due to the lack of explicit positional
form scale method has significant limitations. Al-                                                                                            encoding to manipulate), this work, as the first to
though using a larger scale can reduce the PPL at                                                                                             tackle length generalization for NoPE, has achieved
greater positions, it significantly affects the PPL                                                                                           its set goals.
at closer ranges. For instance, with a scale value                                                                                               The observed gap may imply that constraining
of 1.8, the PPL on 2K@PG19 rises from 14.6 to                                                                                                 the NoPE model to focus on fewer tokens could
30.4, and on 2K@Proof-pile, it rises from 3.5 to                                                                                              detrimentally affect its efficacy. Future efforts will
5.1. On the contrary, the head-based scale method                                                                                             be directed at enhancing the head-based scaling
not only successfully extrapolates to 16k but also                                                                                            method to regain the level of performance seen in
has minimal impact on the PPL at closer distances                                                                                             pretraining.
(for 18K, increases only +3.7 on 2K@PG19, +0.5
on 2K@Proof-pile), proving that attention heads                                                                                               4.3         Synthetic Long Context Tasks
with different patterns indeed require distinct scale                                                                                         A synthetic task is constructed in Landmark At-
values.                                                                                                                                       tention (Mohtashami and Jaggi, 2023b) called
   Third, a full comparison with RoPE LM’s gener-                                                                                             "Passkey Retrieval". It aims to test the effective
alization method. Comparing the zero-shot gener-                                                                                              context window size of the model. The task is to
alization methods, the head-based scale has better                                                                                            retrieve a randomly placed passkey from a long se-
generalization than NTK, but weaker than YaRN.                                                                                                quence of tokens, where the passkey is a randomly
In a fair comparison with the RoPE generalization                                                                                             sampled number of 5 digits and the sequence is
methods which require fine-tuning, the head-based                                                                                             built by concatenating irrelevant sentences.
                                                                                                                                14030
                                                                                                                                  7
                                 Singl-Doc QA                    Multi-Doc QA              Summarization          Few-shot Learning       Synthetic       Code
 Model      Ctx.   Avg.
                           NQA           Qsp    MulF       HpQA       2WQA    Musq.    GRpt     QSum     MulN    TREC    TrQA    SSum    PsgC   PsgR   Lcc    Re-P
                                                                                  Original LMs
 RoPE        2K    16.5          3.5      4.7       17.5        3.4      8.8     2.8     26.9      8.4    25.9    33.5    18.8    15.7    1.9    2.5   49.5   40.1
 NoPE        2K    18.3          6.1      7.9       22.4        6.6     10.3     3.1     28.9      8.8    25.1    41.5    30.0     3.5    1.0    3.0   48.4   46.6
                                                                             Generalization for RoPE
             4K    16.7          5.4      8.6       18.6        4.5      9.1     3.9     26.4      9.9    18.5    21.5    21.2    22.2    2.7    1.5   48.5   44.6
 PIraw       8K    16.7          4.7      9.6       16.3        5.4      9.3     4.0     14.6      9.4    20.7    27.0    23.1    23.5    2.1    3.4   50.0   44.7
            16K    17.2          4.8      8.1       18.6        5.4      9.4     3.8     22.9      9.9    21.3    24.0    23.9    25.4    1.6    1.8   50.5   43.8
             4K    16.2          6.4      8.7       18.2        4.0     11.0     3.0     17.5      9.0    15.6    27.5    21.5    20.3    1.6    0.5   49.8   45.2
 YaRNraw     8K    16.4          6.0     11.4       16.0        5.0      8.3     3.5     16.3     10.3    19.6    21.0    24.9    22.1    1.3    2.0   49.6   45.3
            16K    17.7          4.5     10.5       17.1        5.2      8.9     4.7     18.9      9.2    19.5    38.0    24.4    25.2    1.7    1.8   49.8   44.6
                                                                             Generalization for NoPE
             4K    18.5          6.3     11.1       23.1        5.7     10.1     4.2     27.7      8.9    23.4    25.5    35.7    13.7    0.6    4.5   47.9   46.9
 λ(h)        8K    17.2          5.8     11.7       21.4        6.1     10.8     3.9     24.1      8.9    18.3    31.0    31.4     4.5    0.6    3.1   47.3   46.5
            18K    17.0          6.0     12.8       20.3        7.0     12.9     4.1     17.2      8.4    16.1    41.0    32.9     5.1    0.3    2.1   44.5   41.0


Table 3: Real-world Long-Context performance of NoPE-extension methods and various RoPE baselines. The “Ctx.”
column represents testing context length during evaluation, which corresponds to either the pre-training length for
base models or the extended length for length generalization methods.


 Model
                           PPL@16K (↓)
                                                      Passkey (↑)     LongBench (↑)         model is better than its RoPE counterpart. Con-
                        PG19           Proof-pile
                                                                                            cluding better information utilization in the orig-
 λ(h) 18K                 30.4            4.1              81             17.0
 w/o focus constraint     25.9            4.2              53             16.7              inal length. Moreover, the head-based scale at a
 w/o initialization       31.4            4.3              26             15.8              4k extension length performs the best among all
                                                                                            baselines. We attribute it to the capability of the
Table 4: Ablation study on the two variants of Head-
                                                                                            NoPE base model and the successful length gener-
Scale. Passkey results are listed as average accuracy,
and LongBench results are averaged score among all                                          alization of the head-based attention scale method.
sub-tasks.                                                                                  While the head-based model still suffers from per-
                                                                                            formance degradation when extending to a longer
                                                                                            context, as it is stated in Section 4.2.
Settings. We evaluate the performance of
passkey retrieval across various context lengths.                                           4.5     Ablation Study
For each specified context length, we conduct tests                                         We have introduced two key components of Head-
on 10 distinct passkey depths, each associated with                                         Scale in Section 3.2, a concentration constraint
10 randomly selected passkeys. We report the re-                                            and an initializing technique. The ablation study
trieval accuracy in this task.                                                              in Table 4 depicts that although occasionally per-
   It is observed in Figure 6 that both the NoPE                                            form better in language modeling, the two variants
base model and head-based scale perform well                                                are less preferment in passkey retrieval and Long-
even when evaluating on 2× the pretraining or                                               Bench, indicating their inability to utilize long con-
fine-tuning context window, while RoPE strictly                                             text information.
operates within the pre-trained sequence length                                                Detailed results of the passkey retrieval task can
and immediately fails outside of it. The result indi-                                       be found in Figure 9 in the Appendix C. They are
cates that NoPE possesses significant potential for                                         completely unable to answer the passkey except
generalization.                                                                             when it is at the beginning of the context window.

4.4 Real-World Long Context Tasks                                                           5     Related Work
LongBench (Bai et al., 2023) is a comprehensive                                             Transformers without position encoding Ha-
assessment of the long context understanding ca-                                            viv et al. (2022) was the first to discover that
pabilities of large language models. We test all                                            causal Transformer networks could perform lan-
models using beam search decoding with beam                                                 guage modeling tasks successfully even without
size 5. The evaluation context size is set to the                                           explicit PE. Chi et al. (2023) provided a theoretical
model context window accordingly in order to test                                           explanation for NoPE, demonstrating that for an
the model’s capability to utilize a longer context.                                         initialized NoPE LM, the variance of the hidden
We only include raw PI and YaRN as the baseline                                             representations in each layer is position-dependent,
in this task.                                                                               with variance decreasing for larger positions. Both
   We find that the performance of the NoPE base                                            works demonstrate that the NoPE hidden layer rep-
                                                                                      14031
                                                                                        8
resentation implies positional information through         NoPE provides a new perspective to understand-
the probing task. Kazemnejad et al. (2023) proved       ing the role of positional information by isolating
through constructive methods that NoPE can learn        and eliminating the effects of explicit positional
absolute PE from the first layer and relative PE        encoding. Our work demonstrates the correlation
from the second layer. They also showed that            between length generation failures and distraction
NoPE has an extremely weak length generalization        of attention in NoPE models, thus the proposed
ability (train ∼20, test ∼40), but is slightly better   method concentrates the attention by adjusting the
than LM with explicit PE. This paper first proposes     scaling factor. While current works on length gen-
length generalization methods for NoPE with uni-        eralization mainly focus on manipulating positional
form scale and head-based scale. For the first time     encoding, our work suggests a new key component
verifies the effectiveness of NoPE generalization in    to generalization.
real LLM settings.
                                                        Limitation
Length generalization Due to high computa-
tional and memory requirements, LLM training            The length generalization algorithms discussed in
is usually limited to short inputs. Directly applying   this paper exhibit competitive performances, but
LLMs to long inputs faces the challenge of out-         the NoPE model itself still underperforms with
of-distribution (OOD) issues. Research to enable        state-of-the-art RoPE models, which makes the re-
LLMs to process long inputs has been extensive          sults over long sequence language modeling tasks
(Huang et al., 2023; Dong et al., 2023). The earliest   and LongBench tasks are less competitive. NoPE
methods involved designing new relative PE mech-        still faces the challenges of considerable memory
anisms during pre-training (Press et al., 2021; Sun     usage and computational complexity due to the
et al., 2023). Subsequent studies focused primar-       quadratic nature of attention computation when
ily on the widely used RoPE (Su et al., 2024) and       processing extremely long contexts. Hardware lim-
proposed length extension by mitigating RoPE’s          itations are likely to become a constraining factor
OOD issues through interpolated positions (Chen         for length generalization soon. We plan to further
et al., 2023; kaiokendev, 2023; Peng et al., 2023;      improve the NoPE’s performances for a fairer com-
emozilla, 2023; bloc97, 2023b,a). Other works em-       parison. This paper is also most an empirical one,
ployed sliding window attention mechanisms to           which requires a deeper theoretical understanding
prevent relative positions from exceeding the max-      of NoPE’s length generalization in the future.
imum distance seen in pre-training (Mohtashami
and Jaggi, 2023a; Han et al., 2023; Xiao et al.,        Acknowledgement
2023; Jin et al., 2024; Zhang et al., 2024a). How-
                                                        The authors wish to thank all reviewers for their
ever, these models ignore information from distant
                                                        helpful comments and suggestions. The corre-
tokens, thus failing to capture long-distance context
                                                        sponding authors are Tao Ji, Yuanbin Wu and Xiaol-
dependencies. All existing methods rely on spe-
                                                        ing Wang. This research was (partially) supported
cific explicit PEs. However, the NoPE architecture
                                                        by NSFC(62076097), National Key R&D Program
is more streamlined and more aligned to the form
                                                        of China (2021YFC3340700), the Open Research
of human language modeling. Exploring NoPE’s
                                                        Fund of Key Laboratory of Advanced Theory and
length generalization is therefore more intriguing
                                                        Application in Statistics and Data Science (East
and attractive.
                                                        China Normal University), Ministry of Education.
6   Discussion
We studied the length generalization of Casual          References
Transformer without explicit position encoding.
                                                        Zhangir Azerbayev, Edward Ayers, , and Bartosz Pi-
We developed a parameter-efficient tuning algo-           otrowski. 2022. Proof-pile.
rithm which aims to search for the best temperature
hyper-parameters for attention heads. Through em-       Yushi Bai, Xin Lv, Jiajie Zhang, Hongchang Lyu,
pirical evaluation, we saw that NoPE can achieve          Jiankai Tang, Zhidian Huang, Zhengxiao Du, Xiao
                                                          Liu, Aohan Zeng, Lei Hou, Yuxiao Dong, Jie Tang,
competitive length generalization and might be a          and Juanzi Li. 2023. Longbench: A bilingual, mul-
promising alternative for long-context language           titask benchmark for long context understanding.
modeling.                                                 arXiv preprint arXiv:2308.14508.
                                                   14032
                                                     9
BigScience Workshop. 2022.           Bloom (revision       Gautier Izacard, Patrick S. H. Lewis, Maria Lomeli,
  4ab0472).                                                  Lucas Hosseini, Fabio Petroni, Timo Schick, Jane
                                                             Dwivedi-Yu, Armand Joulin, Sebastian Riedel, and
bloc97. 2023a. Add NTK-Aware interpolation "by               Edouard Grave. 2023. Atlas: Few-shot learning
   parts" correction.                                        with retrieval augmented language models. J. Mach.
                                                             Learn. Res., 24:251:1–251:43.
bloc97. 2023b. NTK-Aware Scaled RoPE allows
   LLaMA models to have extended (8k+) context size        Hongye Jin, Xiaotian Han, Jingfeng Yang, Zhimeng
   without any fine-tuning and minimal perplexity degra-     Jiang, Zirui Liu, Chia-Yuan Chang, Huiyuan Chen,
   dation.                                                   and Xia Hu. 2024. Llm maybe longlm: Self-extend
                                                             llm context window without tuning.
bloc97. 2023c. NTK-Aware Scaled RoPE allows
   LLaMA models to have extended (8k+) context size        kaiokendev. 2023. Things iḿ learning while training
   without any fine-tuning and minimal perplexity degra-     superhot.
   dation.
                                                           Amirhossein Kazemnejad, Inkit Padhi, Karthikeyan
Shouyuan Chen, Sherman Wong, Liangjian Chen, and            Natesan, Payel Das, and Siva Reddy. 2023. The
  Yuandong Tian. 2023. Extending context window of          impact of positional encoding on length generaliza-
  large language models via positional interpolation.       tion in transformers. In Thirty-seventh Conference
                                                            on Neural Information Processing Systems.
Ta-Chung Chi, Ting-Han Fan, Li-Wei Chen, Alexander
  Rudnicky, and Peter Ramadge. 2023. Latent posi-          Raymond Li, Loubna Ben Allal, Yangtian Zi, Niklas
  tional information is in the self-attention variance       Muennighoff, Denis Kocetkov, Chenghao Mou, Marc
  of transformer language models without positional          Marone, Christopher Akiki, Jia Li, Jenny Chim,
  embeddings. In Proceedings of the 61st Annual Meet-        Qian Liu, Evgenii Zheltonozhskii, Terry Yue Zhuo,
  ing of the Association for Computational Linguistics       Thomas Wang, Olivier Dehaene, Mishig Davaadorj,
  (Volume 2: Short Papers), pages 1183–1193, Toronto,        Joel Lamy-Poirier, João Monteiro, Oleh Shliazhko,
  Canada. Association for Computational Linguistics.         Nicolas Gontier, Nicholas Meade, Armel Zebaze,
David Chiang and Peter Cholak. 2022. Overcoming a            Ming-Ho Yee, Logesh Kumar Umapathi, Jian Zhu,
  theoretical limitation of self-attention. In Proceed-      Benjamin Lipkin, Muhtasham Oblokulov, Zhiruo
  ings of the 60th Annual Meeting of the Association for     Wang, Rudra Murthy, Jason Stillerman, Siva Sankalp
  Computational Linguistics (Volume 1: Long Papers),         Patel, Dmitry Abulkhanov, Marco Zocca, Manan Dey,
  pages 7654–7664.                                           Zhihan Zhang, Nour Fahmy, Urvashi Bhattacharyya,
                                                             Wenhao Yu, Swayam Singh, Sasha Luccioni, Paulo
Zican Dong, Tianyi Tang, Lunyi Li, and Wayne Xin             Villegas, Maxim Kunakov, Fedor Zhdanov, Manuel
  Zhao. 2023. A survey on long text modeling with            Romero, Tony Lee, Nadav Timor, Jennifer Ding,
  transformers. arXiv preprint arXiv:2302.14502.             Claire Schlesinger, Hailey Schoelkopf, Jan Ebert, Tri
                                                             Dao, Mayank Mishra, Alex Gu, Jennifer Robinson,
emozilla. 2023. Dynamically Scaled RoPE further in-          Carolyn Jane Anderson, Brendan Dolan-Gavitt, Dan-
  creases performance of long context LLaMA with             ish Contractor, Siva Reddy, Daniel Fried, Dzmitry
  zero fine-tuning.                                          Bahdanau, Yacine Jernite, Carlos Muñoz Ferrandis,
                                                             Sean Hughes, Thomas Wolf, Arjun Guha, Leandro
Chi Han, Qifan Wang, Wenhan Xiong, Yu Chen, Heng             von Werra, and Harm de Vries. 2023. Starcoder: may
  Ji, and Sinong Wang. 2023. Lm-infinite: Simple             the source be with you!
  on-the-fly length generalization for large language
  models.                                                  Ilya Loshchilov and Frank Hutter. 2017. Decoupled
                                                              weight decay regularization. In International Confer-
Adi Haviv, Ori Ram, Ofir Press, Peter Izsak, and Omer         ence on Learning Representations.
  Levy. 2022. Transformer language models without
  positional encodings still learn positional informa-     Amirkeivan Mohtashami and Martin Jaggi. 2023a.
  tion. In Findings of the Association for Computa-         Landmark attention: Random-access infinite context
  tional Linguistics: EMNLP 2022, pages 1382–1390,          length for transformers.
  Abu Dhabi, United Arab Emirates. Association for
  Computational Linguistics.                               Amirkeivan Mohtashami and Martin Jaggi. 2023b.
                                                            Random-access infinite context length for transform-
Xin He, Kaiyong Zhao, and Xiaowen Chu. 2021. Au-            ers. In Thirty-seventh Conference on Neural Infor-
  toml: A survey of the state-of-the-art. Knowledge-        mation Processing Systems.
  Based Systems, 212:106622.
                                                           MosaicML NLP Team. 2023. Introducing mpt-7b: A
Yunpeng Huang, Jingwei Xu, Zixu Jiang, Junyu Lai,           new standard for open-source, commercially usable
  Zenan Li, Yuan Yao, Taolue Chen, Lijuan Yang,             llms. Accessed: 2023-05-05.
  Zhou Xin, and Xiaoxing Ma. 2023. Advancing trans-
  former architecture in long-context large language       Joon Sung Park, Joseph O’Brien, Carrie Jun Cai, Mered-
  models: A comprehensive survey. arXiv preprint             ith Ringel Morris, Percy Liang, and Michael S. Bern-
  arXiv:2311.12351.                                          stein. 2023. Generative agents: Interactive simulacra
                                                      14033
                                                       10
  of human behavior. In Proceedings of the 36th An-         Hugo Touvron, Louis Martin, Kevin Stone, Peter Al-
  nual ACM Symposium on User Interface Software               bert, Amjad Almahairi, Yasmine Babaei, Nikolay
  and Technology, UIST ’23, New York, NY, USA.                Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti
  Association for Computing Machinery.                        Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton
                                                              Ferrer, Moya Chen, Guillem Cucurull, David Esiobu,
Bowen Peng, Jeffrey Quesnelle, Honglu Fan, and En-            Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller,
  rico Shippole. 2023. Yarn: Efficient context window         Cynthia Gao, Vedanuj Goswami, Naman Goyal, An-
  extension of large language models.                         thony Hartshorn, Saghar Hosseini, Rui Hou, Hakan
                                                              Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa,
Bowen Peng, Jeffrey Quesnelle, Honglu Fan, and Enrico         Isabel Kloumann, Artem Korenev, Punit Singh Koura,
  Shippole. 2024. YaRN: Efficient context window ex-          Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Di-
  tension of large language models. In The Twelfth            ana Liskovich, Yinghai Lu, Yuning Mao, Xavier Mar-
  International Conference on Learning Representa-            tinet, Todor Mihaylov, Pushkar Mishra, Igor Moly-
  tions.                                                      bog, Yixin Nie, Andrew Poulton, Jeremy Reizen-
                                                              stein, Rashi Rungta, Kalyan Saladi, Alan Schelten,
Ofir Press, Noah Smith, and Mike Lewis. 2021. Train           Ruan Silva, Eric Michael Smith, Ranjan Subrama-
  short, test long: Attention with linear biases enables      nian, Xiaoqing Ellen Tan, Binh Tang, Ross Tay-
  input length extrapolation. In International Confer-        lor, Adina Williams, Jian Xiang Kuan, Puxin Xu,
  ence on Learning Representations.                           Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan,
                                                              Melanie Kambadur, Sharan Narang, Aurelien Ro-
Ofir Press, Noah A. Smith, and Mike Lewis. 2022. Train        driguez, Robert Stojnic, Sergey Edunov, and Thomas
  short, test long: Attention with linear biases enables      Scialom. 2023. Llama 2: Open foundation and fine-
  input length extrapolation.                                 tuned chat models.

                                                            Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob
JackW. Rae, Anna Potapenko, SiddhantM. Jayakumar,             Uszkoreit, Llion Jones, Aidan N Gomez, Ł ukasz
   Chloe Hillier, and TimothyP. Lillicrap. 2020. Com-         Kaiser, and Illia Polosukhin. 2017. Attention is all
   pressive transformers for long-range sequence mod-         you need. In Advances in Neural Information Pro-
   elling. International Conference on Learning Rep-          cessing Systems, volume 30.
   resentations,International Conference on Learning
  Representations.                                          Zekun Moore Wang, Zhongyuan Peng, Haoran Que,
                                                              Jiaheng Liu, Wangchunshu Zhou, Yuhan Wu,
Colin Raffel, Noam Shazeer, Adam Roberts, Kather-             Hongcheng Guo, Ruitong Gan, Zehao Ni, Man
  ine Lee, Sharan Narang, Michael Matena, Yanqi               Zhang, et al. 2023. Rolellm: Benchmarking, elic-
  Zhou, Wei Li, and Peter J. Liu. 2020. Exploring the         iting, and enhancing role-playing abilities of large
  limits of transfer learning with a unified text-to-text     language models. arXiv preprint arXiv:2310.00746.
  transformer. Journal of Machine Learning Research,
  21(140):1–67.                                             Guangxuan Xiao, Yuandong Tian, Beidi Chen, Song
                                                              Han, and Mike Lewis. 2023. Efficient streaming
Soboleva, Daria, Al-Khateeb, Faisal, Myers, Robert,           language models with attention sinks.
  Steeves, Jacob R, Hestness, Joel, Dey, and Nolan.
  2023. SlimPajama: A 627B token cleaned and dedu-          Peitian Zhang, Zheng Liu, Shitao Xiao, Ninglu Shao,
  plicated version of RedPajama.                              Qiwei Ye, and Zhicheng Dou. 2024a. Soaring from
                                                              4k to 400k: Extending llm’s context with activation
Jianlin Su. 2021. Attentionś scale operation from en-        beacon.
   tropy invariance.
                                                            Peiyuan Zhang, Guangtao Zeng, Tianduo Wang, and
Jianlin Su, Murtadha Ahmed, Yu Lu, Shengfeng Pan,             Wei Lu. 2024b. Tinyllama: An open-source small
   Wen Bo, and Yunfeng Liu. 2024. Roformer: En-               language model.
   hanced transformer with rotary position embedding.
   Neurocomputing, 568:127063.                              A    Experiment Setup
Jianlin Su, Yu Lu, Shengfeng Pan, Bo Wen, and Yunfeng       Searching scales. We approach the search for op-
   Liu. 2021. Roformer: Enhanced transformer with           timal head-based scales λ(h) by parameter-efficient
   rotary position embedding. CoRR, abs/2104.09864.         fine-tuning. We use a large learning rate (LR, =0.05
                                                            or =0.1) for fine-tuning, as λ spans a wide range,
Yutao Sun, Li Dong, Barun Patra, Shuming Ma, Shao-
  han Huang, Alon Benhaim, Vishrav Chaudhary, Xia           (e.g., [ √1d , √3d ], shown in Figure 5). The fine-tuning
  Song, and Furu Wei. 2023. A length-extrapolatable         data comes from the pretraining dataset (Slimpa-
  transformer. In Proceedings of the 61st Annual Meet-      jama (Soboleva et al., 2023) and Starcoderdata (Li
  ing of the Association for Computational Linguis-
  tics (Volume 1: Long Papers), pages 14590–14604,
                                                            et al., 2023)) with a different data fetching seed
  Toronto, Canada. Association for Computational Lin-       from the pretraining. We set the batch size to 8
  guistics.                                                 and set the optimizer to the AdamW (β1 = 0.9,
                                                       14034
                                                        11
β2 = 0.95) without weight decay (Loshchilov and                       Based on the search results, we guess a func-
Hutter, 2017). We use a cosine LR decay from                      tion form that best fits the data points. We then
LR to 0.1LR for 200 fine-tuning steps and a linear                fit this function over the range i ∈ [2048, 16384].
warmup for the first 20 steps. We found that the                  The fitted function, along with its corresponding
head-based scale searching on 16K suffers from a                  coefficient of determination, is presented below:
minor PPL degradation at the end of the context
                                                                  • For NoPE at 10k steps, the coefficient of deter-
window. We simply expanded the length L′ to 18K
                                                                    mination R2 = 0.9954. The fitted function is
and then solved it.
                                                                                       1 + 0.3010 ln s
Length generalization baselines. To compare                                       λ=         √
                                                                                               d
with mainstream length generalization research, we
reproduced three generalization baselines on RoPE,                • For NoPE at 50k steps, the coefficient of deter-
including:                                                          mination R2 = 0.9773. The fitted function is
• NTK (2023c), zero-shot generalization;                                               1 + 0.3973 ln s
                                                                                  λ=         √
                                                                                               d
• PI (2023), efficiently train long, test long;
• YaRN (2024), supports both settings 5 .                            In these functions, s is defined as Li for each
                                                                  position i, representing the model’s extension ratio
   For the zero-shot setting, we grid-searched the                relative to its pre-training length.
baseline hyper-parameters and reported their best                    Furthermore, it is also found by Peng et al.
results. For the baselines that need fine-tuning,                 (2024) that the YaRN method benefits from a sim-
we propose two settings, one for a fair compari-                  ilar uniform scale on LLaMA2 (Touvron et al.,
son, with the same number of fine-tuned tokens                    2023), although the scale does not have a direct
(0.3‰ of pre-trained data) as the head-based scales               impact on the RoPE extension capability (refer to
searching, and the other follows their original pa-               Figure 2). The scale proposed by the YaRN method
per, which is 1.3‰ of pre-trained data. Specifically,             can be formulated as follows, which is quite similar
we fine-tune the RoPE model for 200 steps in the                  to our result.
“fair” version, and 1000 steps for the “raw” version.
   In addition, we incorporate open-source AL-                                         (1 + 0.1 ln s)2
                                                                                 λ=         √
iBi models (Press et al., 2022) into our baselines,                                           d
which include BLOOM 1.1B (BigScience Work-                           In conclusion, the optimal uniform scale varies
shop, 2022) and MPT 7B Base (MosaicML NLP                         across different models. It is also observed from
Team, 2023), both of which are trained on a con-                  Figure 7 that uniform scale, despite being optimal,
text length of 2K. We test a zero-shot generalization             cannot flatten the NoPE model’s perplexity within
of the ALiBi models following the original paper                  a large context window. This finding underscores
(Press et al., 2022).                                             the importance of employing a head-based scaling
                                                                  method for managing model perplexity effectively
B       Fitted Function of the Uniform Scale                      across larger context windows, thereby enhancing
In the study depicted in Figure 7, a hyper-parameter              the model’s performance.
search was conducted for the uniform scale λ with                 C   Additional Passkey Results
an interval of 0.01
                √ . This search was applied to two
                  d
checkpoints of the pre-trained NoPE model, to fit                 In Section 4.2, we note that the ALiBi baselines do
the optimal λ at the extension length. We note                    not exhibit competitive performance in terms of per-
remark that the scaling factor takes the same value               plexity when applied to longer contexts. We also
for all positions during a single test. The output of             conduct Passkey Retrieval tests on these models,
a single test is the perplexity across all positions.             with the results depicted in Figure 8. These mod-
We run multiple tests with different scales and find              els yield expected results within their pre-trained
the best one for each position.                                   sequence length, but they are unable to complete
                                                                  the task when it exceeds this length.
    5
     The YaRN paper also proposes a “train short, test long”         In Section 4.5, we conducted an ablation study
setting with lower training costs. However, for a fair com-
parison, we relax this setting to “train long, test long” which   on HeadScale. Figure 9 shows the passkey retrieval
generalizes better.                                               task of the two variations of HeadScale.
                                                             14035
                                                              12
                 2.0
                                              NoPE 10k steps                                                      2.0
                                                                                                                                              NoPE 50k steps
                 √                                                                 6.0                            √                                                                6.0
                   d                                                                                                d

                                                                                   5.5                                                                                             5.5
                 1.8
                 √                                                                                                1.8
                                                                                                                  √
                   d                                                                                                d
                                                                                   5.0                                                                                             5.0




                                                                                         log Perplexity




                                                                                                                                                                                         log Perplexity
                 1.6                                                                                              1.6
                 √
                   d
                                                                                   4.5                            √
                                                                                                                    d
                                                                                                                                                                                   4.5

                                                                                   4.0                                                                                             4.0
λ




                                                                                               λ
                 1.4
                 √                                                                                                1.4
                                                                                                                  √
                   d                                                                                                d
                                                                                   3.5                                                                                             3.5
                 1.2                                                                                              1.2
                 √
                   d
                                                                                   3.0                            √
                                                                                                                    d
                                                                                                                                                                                   3.0

                                                                                   2.5                                                                                             2.5
                 1.0
                 √                                                                                                1.0
                                                                                                                  √
                   d                                                                                                d
                                                                                   2.0                                                                                             2.0
                        0         2500      5000    7500 10000   12500   15000                                          0         2500      5000    7500 10000   12500   15000
                                                   Position i                                                                                      Position i

Figure 7: Fitted optimal uniform scale for each position. The red line indicates best log perplexity found at each
position, the blue line plots the corresponding optimal uniform λ for that position, the black curve is the fitted
function and the vertical dotted line is pre-training length.

                                              BLOOM 1.1B acc 0.90                                                                             MPT 7B Base acc 0.86
                  0 1.0 1.0 1.0 1.0 0.9 1.0 0.8 0.9 0.9 0.0 0.0 0.5 0.0 0.0 0.0 0.0                                0 1.0 0.8 1.0 0.7 0.9 1.0 0.8 0.9 1.0 0.9 0.8 0.9 0.0 0.0 0.0 0.0
                 10 1.0 0.9 0.9 0.9 1.0 1.0 0.8 1.0 1.0 0.0 0.0 0.5 0.0 0.0 0.0 0.0                               10 0.9 0.7 0.9 1.0 0.8 1.0 0.7 0.6 0.9 0.9 0.7 0.8 0.0 0.0 0.0 0.0
                 20 0.9 1.0 0.8 0.7 0.9 0.8 0.8 1.0 1.0 0.0 0.0 0.8 0.0 0.0 0.0 0.0                               20 1.0 1.0 0.9 0.7 0.8 0.9 1.0 0.9 0.8 1.0 0.7 0.0 0.0 0.0 0.0 0.0
 Passkey Depth




                                                                                                  Passkey Depth




                 30 0.9 1.0 0.8 1.0 0.8 1.0 0.8 1.0 0.9 0.0 0.0 0.0 0.0 0.0 0.0 0.0                               30 0.9 0.9 0.9 0.8 0.9 0.9 0.7 0.9 0.9 0.8 0.0 0.0 0.0 0.0 0.0 0.0
                 40 1.0 0.9 0.7 0.9 1.0 1.0 0.9 1.0 0.8 0.0 0.0 0.0 0.0 0.0 0.0 0.0                               40 0.9 0.8 0.8 0.8 1.0 0.9 0.9 1.0 0.7 0.5 0.0 0.0 0.0 0.0 0.0 0.0
                 50 0.8 1.0 1.0 0.9 0.7 0.9 0.5 1.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0                               50 0.7 1.0 0.9 0.8 0.9 0.8 1.0 0.9 0.8 0.5 0.0 0.0 0.0 0.0 0.0 0.0
                 60 0.9 1.0 1.0 0.8 1.0 1.0 0.8 0.9 0.9 0.0 0.0 0.0 0.0 0.0 0.0 0.0                               60 0.9 1.0 1.0 0.9 0.8 0.8 0.8 1.0 1.0 0.9 0.0 0.0 0.0 0.0 0.0 0.0
                 70 1.0 1.0 0.9 1.0 0.9 0.9 0.6 0.9 0.9 0.0 0.0 0.0 0.0 0.0 0.0 0.0                               70 0.6 0.8 0.7 1.0 0.9 0.8 0.8 0.8 0.9 0.9 0.0 0.0 0.0 0.0 0.0 0.0
                 80 0.9 1.0 1.0 1.0 1.0 0.9 0.8 0.9 0.8 0.1 0.0 0.1 0.0 0.0 0.0 0.0                               80 0.9 0.9 1.0 0.5 0.9 0.6 1.0 0.7 0.7 0.9 0.7 0.9 0.0 0.0 0.0 0.0
                 90 0.0 0.7 1.0 1.0 1.0 0.9 0.8 1.0 0.7 0.0 0.1 0.2 0.1 0.0 0.0 0.0                               90 0.8 0.9 1.0 0.8 0.7 0.7 0.9 0.9 0.8 0.9 0.8 1.0 0.2 0.0 0.0 0.0
                            512      1024      1536  2048   2560    3072    3584   4096                                     512      1024      1536  2048   2560    3072    3584   4096
                                                  Context Length                                                                                  Context Length


Figure 8: The results of passkey retrieval for ALiBi baselines. The vertical dashed line represents the pre-training
length. While ALiBi models do exhibit performance beyond the pre-trained length, their expansion is not substantial.


D                      Entropy Visualization of All Heads
Figures 10 to 12 show attention entropy across all
layers and all heads of the 8k extension head-based
scale method, UniformScale and the original NoPE.
An additional theoretical upper bound of entropy
is also plotted in the figures. We note that for each
position i, the maximum entropy is achieved when
       (h)
∀j, αij = 1i is satisfied in Equation 2. The maxi-
                                                           (h)
mum value is then given by Hi = log i.
   It is observed in Figure 10 that the lower lay-
ers have high entropy, closely approaching the up-
per bound. Most heads exhibit constant entropy
for all positions. And the attention values span
a broad spectrum, ranging from 0 to theoretical
upper-bound.




                                                                                     14036
                                                                                      13
                                λ(h) 18k w/o initialization acc 0.26                                                 λ(h) 18k w/o focus constraint acc 0.53
                  0 0.7 0.8 0.8 0.7 0.7 0.9 0.7 0.7 0.9 0.8 0.8 0.6 0.8 0.7 0.7 0.5                     0 0.7 0.8 0.7 0.4 0.3 0.3 0.3 0.4 0.3 0.2 0.6 0.3 0.7 0.8 0.6 0.6
                 10 0.9 0.9 1.0 0.8 0.9 0.7 0.6 0.8 0.3 0.1 0.3 0.5 0.1 0.3 0.3 0.0                    10 0.8 0.5 0.8 0.8 0.9 0.7 0.9 0.9 1.0 1.0 0.8 0.9 1.0 1.0 0.7 1.0
                 20 1.0 0.8 0.9 0.5 0.8 0.3 0.1 0.1 0.1 0.0 0.0 0.0 0.0 0.0 0.0 0.0                    20 0.7 0.8 0.9 1.0 1.0 1.0 0.9 1.0 1.0 0.9 0.9 0.9 0.5 0.3 0.1 0.4
 Passkey Depth




                                                                                       Passkey Depth




                 30 0.9 0.8 0.8 0.5 0.4 0.0 0.2 0.1 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0                    30 0.6 0.9 0.9 1.0 0.8 1.0 1.0 0.8 0.8 0.8 0.7 0.2 0.1 0.0 0.1 0.0
                 40 0.8 0.7 0.6 0.4 0.1 0.3 0.0 0.1 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0                    40 0.9 1.0 1.0 0.9 1.0 0.8 0.9 0.7 0.6 0.1 0.3 0.1 0.0 0.0 0.0 0.0
                 50 0.9 0.9 0.6 0.0 0.2 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0                    50 1.0 1.0 1.0 0.9 0.7 0.5 0.8 0.8 0.3 0.0 0.1 0.0 0.0 0.0 0.0 0.1
                 60 0.9 0.9 0.1 0.2 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0                    60 0.7 1.0 0.9 1.0 0.8 0.5 0.8 0.1 0.1 0.1 0.1 0.0 0.0 0.1 0.0 0.0
                 70 0.8 0.8 0.2 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0                    70 0.8 1.0 0.9 1.0 0.9 0.7 0.5 0.2 0.2 0.3 0.0 0.1 0.0 0.1 0.0 0.0
                 80 0.7 0.6 0.0 0.1 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0                    80 0.9 1.0 0.9 0.9 0.9 0.8 0.5 0.3 0.2 0.1 0.0 0.0 0.0 0.0 0.0 0.0
                 90 0.7 0.6 0.1 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0                    90 1.0 0.9 0.7 0.6 1.0 0.4 0.1 0.2 0.1 0.1 0.3 0.0 0.0 0.0 0.0 0.0
                        2048   4096    6144  8192 10240        12288   14336   16384                          2048    4096   6144  8192 10240        12288   14336   16384
                                          Context Length                                                                        Context Length


Figure 9: The results of passkey retrieval for HeadScale variations. These results are anticipated to apply to a context
length of 16K, but they fail to retrieve the passkey unless it is positioned at the beginning of the context window.




                                                                                 14037
                                                                                  14
                                                     λ(h) 8K
           Layer 0                      Layer 1                       Layer 2                       Layer 3

10                            10                           10                            10

 5                             5                             5                            5

 0                             0                             0                            0
     0          5000               0         5000                0         5000               0         5000

           Layer 4                      Layer 5                       Layer 6                       Layer 7

10                            10                           10                            10

 5                             5                             5                            5

 0                             0                             0                            0
     0          5000               0         5000                0         5000               0         5000

           Layer 8                      Layer 9                      Layer 10                      Layer 11

10                            10                           10                            10

 5                             5                             5                            5

 0                             0                             0                            0
     0          5000               0         5000                0         5000               0         5000

          Layer 12                      Layer 13                     Layer 14                      Layer 15

10                            10                           10                            10

 5                             5                             5                            5

 0                             0                             0                            0
     0          5000               0         5000                0         5000               0         5000

          Layer 16                      Layer 17                     Layer 18                      Layer 19

10                            10                           10                            10

 5                             5                             5                            5

 0                             0                             0                            0
     0          5000               0         5000                0         5000               0         5000

          Layer 20                      Layer 21

10                            10

 5                             5

 0                             0
     0          5000               0         5000

Figure 10: Entropy across all layers and all head of 8k extension head-based scale method. The x-axis is the position
of extension and the y-axis is entropy averaged over all test samples. The black dashed curve is the theoretical
upper-bound of entropy.

                                                       14038
                                                        15
                                                    1.6
                                                 λ= √ d

         Layer 0                    Layer 1                     Layer 2                        Layer 3

10                        10                          10                          10

 5                         5                           5                           5

 0                         0                           0                           0
     0      5000               0        5000               0         5000              0          5000

         Layer 4                    Layer 5                     Layer 6                        Layer 7

10                        10                          10                          10

 5                         5                           5                           5

 0                         0                           0                           0
     0      5000               0        5000               0         5000              0          5000

         Layer 8                    Layer 9                    Layer 10                        Layer 11

10                        10                          10                          10

 5                         5                           5                           5

 0                         0                           0                           0
     0      5000               0        5000               0         5000              0          5000

         Layer 12                  Layer 13                    Layer 14                        Layer 15

10                        10                          10                          10

 5                         5                           5                           5

 0                         0                           0                           0
     0      5000               0        5000               0         5000              0          5000

         Layer 16                  Layer 17                    Layer 18                        Layer 19

10                        10                          10                          10

 5                         5                           5                           5

 0                         0                           0                           0
     0      5000               0        5000               0         5000              0          5000

         Layer 20                  Layer 21

10                        10

 5                         5

 0                         0
     0      5000               0        5000

              Figure 11: Entropy across all layers and all head of UniformScale with λ = √
                                                                                         1.6
                                                                                           d




                                                  14039
                                                   16
                                                   NoPE
         Layer 0                      Layer 1                      Layer 2                    Layer 3

10                         10                           10                           10

 5                          5                            5                            5

 0                          0                            0                            0
     0      5000                0         5000               0         5000               0      5000

         Layer 4                      Layer 5                      Layer 6                    Layer 7

10                         10                           10                           10

 5                          5                            5                            5

 0                          0                            0                            0
     0      5000                0         5000               0         5000               0      5000

         Layer 8                      Layer 9                     Layer 10                    Layer 11

10                         10                           10                           10

 5                          5                            5                            5

 0                          0                            0                            0
     0      5000                0         5000               0         5000               0      5000

         Layer 12                    Layer 13                     Layer 14                    Layer 15

10                         10                           10                           10

 5                          5                            5                            5

 0                          0                            0                            0
     0      5000                0         5000               0         5000               0      5000

         Layer 16                    Layer 17                     Layer 18                    Layer 19

10                         10                           10                           10

 5                          5                            5                            5

 0                          0                            0                            0
     0      5000                0         5000               0         5000               0      5000

         Layer 20                    Layer 21

10                         10

 5                          5

 0                          0
     0      5000                0         5000

                    Figure 12: Entropy across all layers and all head of the original NoPE.



                                                    14040
                                                     17
