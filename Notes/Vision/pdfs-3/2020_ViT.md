## 1. Basic Metadata
- Title: An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale.
- Authors: Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, Neil Houlsby.
- Year: 2021.
  - Evidence: "Published as a conference paper at ICLR 2021" (Front matter, p.1)
- Venue (conference/journal/arXiv): ICLR 2021; arXiv.
  - Evidence: "Published as a conference paper at ICLR 2021" (Front matter, p.1); "arXiv:2010.11929v2 [cs.CV] 3 Jun 2021" (Front matter, p.1)

## 2. One-Sentence Contribution Summary
The paper claims that a pure Transformer operating on sequences of image patches can achieve strong image classification performance when pre-trained on large-scale data and transferred to smaller image recognition benchmarks.

## 3. Tasks Evaluated
- ImageNet classification (original validation labels)
  - Task type: Classification.
  - Dataset(s): ImageNet (ILSVRC-2012).
  - Domain: Not specified in the paper.
  - Evidence: "We transfer the models trained on these dataset to several benchmark tasks: ImageNet on the original validation labels and the cleaned-up ReaL labels (Beyer et al., 2020), CIFAR-10/100 (Krizhevsky, 2009), Oxford-IIIT Pets (Parkhi et al., 2012), and Oxford Flowers-102 (Nilsback & Zisserman, 2008)." (Section 4.1, p.4)

- ImageNet ReaL classification (cleaned-up labels)
  - Task type: Classification.
  - Dataset(s): ImageNet ReaL.
  - Domain: Not specified in the paper.
  - Evidence: "ImageNet on the original validation labels and the cleaned-up ReaL labels (Beyer et al., 2020)" (Section 4.1, p.4)

- CIFAR-10 classification
  - Task type: Classification.
  - Dataset(s): CIFAR-10.
  - Domain: Not specified in the paper.
  - Evidence: "CIFAR-10/100 (Krizhevsky, 2009)" (Section 4.1, p.4)

- CIFAR-100 classification
  - Task type: Classification.
  - Dataset(s): CIFAR-100.
  - Domain: Not specified in the paper.
  - Evidence: "CIFAR-10/100 (Krizhevsky, 2009)" (Section 4.1, p.4)

- Oxford-IIIT Pets classification
  - Task type: Classification.
  - Dataset(s): Oxford-IIIT Pets.
  - Domain: Not specified in the paper.
  - Evidence: "Oxford-IIIT Pets (Parkhi et al., 2012)" (Section 4.1, p.4)

- Oxford Flowers-102 classification
  - Task type: Classification.
  - Dataset(s): Oxford Flowers-102.
  - Domain: Not specified in the paper.
  - Evidence: "Oxford Flowers-102 (Nilsback & Zisserman, 2008)." (Section 4.1, p.4)

- VTAB-1k classification suite (19 tasks)
  - Task type: Classification.
  - Dataset(s): Caltech101; CIFAR-100; DTD; Flowers102; Pets; Sun397; SVHN; Camelyon; EuroSAT; Resisc45; Retinopathy; Clevr-Count; Clevr-Dist; DMLab; dSpr-Loc; dSpr-Ori; KITTI-Dist; sNORB-Azim; sNORB-Elev.
  - Domain: Includes specialized imagery and structured tasks (explicitly stated for VTAB; see Section 4).
  - Evidence: "We also evaluate on the 19-task VTAB classification suite (Zhai et al., 2019b)." (Section 4.1, p.4); "Table 9: Breakdown of VTAB-1k performance across tasks." (Appendix D.10, p.22); task names as listed in Table 9: "Caltech101", "CIFAR-100", "DTD", "Flowers102", "Pets", "Sun397", "SVHN", "Camelyon", "EuroSAT", "Resisc45", "Retinopathy", "Clevr-Count", "Clevr-Dist", "DMLab", "dSpr-Loc", "dSpr-Ori", "KITTI-Dist", "sNORB-Azim", "sNORB-Elev". (Appendix D.10, p.22)

- ObjectNet classification
  - Task type: Classification.
  - Dataset(s): ObjectNet benchmark.
  - Domain: Not specified in the paper.
  - Evidence: "We also evaluate our flagship ViT-H/14 model on the ObjectNet benchmark following the evaluation setup in Kolesnikov et al. (2020), resulting in 82.1% top-5 accuracy and 61.7% top-1 accuracy." (Appendix D.9, p.20)

- Masked patch prediction (self-supervised pre-training task)
  - Task type: Reconstruction / prediction (masked patch prediction).
  - Dataset(s): JFT (self-supervised pre-training dataset used).
  - Domain: Not specified in the paper.
  - Evidence: "We employ the masked patch prediction objective for preliminary self-supervision experiments." (Appendix B.1.2, p.14); "We trained our self-supervised model for 1M steps (ca. 14 epochs) with batch size 4096 on JFT." (Appendix B.1.2, p.14)

## 4. Domain and Modality Scope
- Evaluation modality: Image recognition / image classification.
  - Evidence: "a pure transformer applied directly to sequences of image patches can perform very well on image classification tasks." (Abstract, p.1)
- Domain scope: Multiple domains within the same modality (images), including specialized imagery and structured tasks.
  - Evidence: "medical and satellite imagery" (Section 4.1, p.4); "tasks that require geometric understanding like localization." (Section 4.1, p.4)
- Multiple modalities: Not specified in the paper.
- Domain generalization / cross-domain transfer:
  - Claimed transfer across benchmarks: "When pre-trained on large amounts of data and transferred to multiple mid-sized or small image recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.), Vision Transformer (ViT) attains excellent results compared to state-of-the-art convolutional networks while requiring substantially fewer computational resources to train." (Abstract, p.1)
  - Domain generalization claim: Not claimed.

## 5. Model Sharing Across Tasks
| Task | Shared Weights? | Fine-Tuned? | Separate Head? | Evidence |
| --- | --- | --- | --- | --- |
| ImageNet (orig labels) | Pretrained shared init, then task-specific fine-tune | Yes | Yes | "Typically, we pre-train ViT on large datasets, and fine-tune to (smaller) downstream tasks." (Section 3.2, p.3); "When transferring ViT models to another dataset, we remove the whole head (two linear layers) and replace it by a single, zero-initialized linear layer outputting the number of classes required by the target dataset." (Appendix B.1.1, p.13) |
| ImageNet ReaL | Pretrained shared init, then task-specific fine-tune | Yes | Yes | "Typically, we pre-train ViT on large datasets, and fine-tune to (smaller) downstream tasks." (Section 3.2, p.3); "When transferring ViT models to another dataset, we remove the whole head (two linear layers) and replace it by a single, zero-initialized linear layer outputting the number of classes required by the target dataset." (Appendix B.1.1, p.13) |
| CIFAR-10 | Pretrained shared init, then task-specific fine-tune | Yes | Yes | "Typically, we pre-train ViT on large datasets, and fine-tune to (smaller) downstream tasks." (Section 3.2, p.3); "When transferring ViT models to another dataset, we remove the whole head (two linear layers) and replace it by a single, zero-initialized linear layer outputting the number of classes required by the target dataset." (Appendix B.1.1, p.13) |
| CIFAR-100 | Pretrained shared init, then task-specific fine-tune | Yes | Yes | "Typically, we pre-train ViT on large datasets, and fine-tune to (smaller) downstream tasks." (Section 3.2, p.3); "When transferring ViT models to another dataset, we remove the whole head (two linear layers) and replace it by a single, zero-initialized linear layer outputting the number of classes required by the target dataset." (Appendix B.1.1, p.13) |
| Oxford-IIIT Pets | Pretrained shared init, then task-specific fine-tune | Yes | Yes | "Typically, we pre-train ViT on large datasets, and fine-tune to (smaller) downstream tasks." (Section 3.2, p.3); "When transferring ViT models to another dataset, we remove the whole head (two linear layers) and replace it by a single, zero-initialized linear layer outputting the number of classes required by the target dataset." (Appendix B.1.1, p.13) |
| Oxford Flowers-102 | Pretrained shared init, then task-specific fine-tune | Yes | Yes | "Typically, we pre-train ViT on large datasets, and fine-tune to (smaller) downstream tasks." (Section 3.2, p.3); "When transferring ViT models to another dataset, we remove the whole head (two linear layers) and replace it by a single, zero-initialized linear layer outputting the number of classes required by the target dataset." (Appendix B.1.1, p.13) |
| VTAB-1k suite (19 tasks) | Pretrained shared init, then task-specific fine-tune | Yes | Yes | "Typically, we pre-train ViT on large datasets, and fine-tune to (smaller) downstream tasks." (Section 3.2, p.3); "When transferring ViT models to another dataset, we remove the whole head (two linear layers) and replace it by a single, zero-initialized linear layer outputting the number of classes required by the target dataset." (Appendix B.1.1, p.13) |
| ObjectNet | Not specified in the paper | Not specified in the paper | Not specified in the paper | "We also evaluate our flagship ViT-H/14 model on the ObjectNet benchmark following the evaluation setup in Kolesnikov et al. (2020), resulting in 82.1% top-5 accuracy and 61.7% top-1 accuracy." (Appendix D.9, p.20) |
| Masked patch prediction (self-supervised pre-training) | Not specified in the paper (pre-training objective) | Not applicable (pre-training objective) | Not specified in the paper | "We employ the masked patch prediction objective for preliminary self-supervision experiments." (Appendix B.1.2, p.14) |

## 6. Input and Representation Constraints
- Fixed-size patches: "We split an image into fixed-size patches, linearly embed each of them, add position embeddings, and feed the resulting sequence of vectors to a standard Transformer encoder." (Figure 1 caption, p.3)
- Patch size fixed when changing resolution: "When feeding images of higher resolution, we keep the patch size the same, which results in a larger effective sequence length." (Section 3.2, p.3)
- Sequence length tied to patch grid: "N = HW/P 2 is the resulting number of patches, which also serves as the effective input sequence length for the Transformer." (Section 3.1, p.3)
- 1D token sequence input: "The standard Transformer receives as input a 1D sequence of token embeddings." (Section 3.1, p.3)
- Constant latent dimensionality: "The Transformer uses constant latent vector size D through all of its layers, so we flatten the patches and map to D dimensions with a trainable linear projection (Eq. 1)." (Section 3.1, p.3)
- Training resolution: "Training resolution is 224." (Table 3, Appendix B.1, p.13)
- Fine-tuning resolution (default): "If not mentioned otherwise, fine-tuning resolution is 384." (Table 4, Appendix B.1.1, p.13)
- Position embedding resizing for higher resolution: "We therefore perform 2D interpolation of the pre-trained position embeddings, according to their location in the original image." (Section 3.2, p.3)
- Padding / resizing requirements: Not specified in the paper.

## 7. Context Window and Attention Structure
- Maximum sequence length: Not specified; the paper states "The Vision Transformer can handle arbitrary sequence lengths (up to memory constraints)," (Section 3.2, p.3)
- Fixed or variable length: Variable with resolution; "When feeding images of higher resolution, we keep the patch size the same, which results in a larger effective sequence length." (Section 3.2, p.3)
- Attention type: Global self-attention. Evidence: "the self-attention layers are global." (Section 3.1, p.3)
- Compute management mechanisms: Patching controls sequence length; "N = HW/P 2 is the resulting number of patches, which also serves as the effective input sequence length for the Transformer." (Section 3.1, p.3); "Note that the Transformer’s sequence length is inversely proportional to the square of the patch size, thus models with smaller patch size are computationally more expensive." (Section 4.1, p.4)

## 8. Positional Encoding (Critical Section)
- Mechanism: Learnable absolute 1D position embeddings added to patch embeddings at input.
  - Evidence: "Position embeddings are added to the patch embeddings to retain positional information. We use standard learnable 1D position embeddings, since we have not observed significant performance gains from using more advanced 2D-aware position embeddings (Appendix D.4)." (Section 3.1, p.3)
- Where applied: Input only by default; alternative placements ablated.
  - Evidence: "add positional embeddings to the inputs right after the stem of them model and before feeding the inputs to the Transformer encoder (default across all other experiments in this paper); (2) learn and add positional embeddings to the inputs at the beginning of each layer; (3) add a learned positional embeddings to the inputs at the beginning of each layer (shared between layers)." (Appendix D.4, p.17)
- Alternatives compared: No PE, 1-D PE, 2-D PE, Relative PE.
  - Evidence: "No Pos. Emb.", "1-D Pos. Emb.", "2-D Pos. Emb.", "Rel. Pos. Emb." (Table 8, Appendix D.4, p.17)
- Modified per task/resolution: Yes, for higher-resolution fine-tuning via interpolation.
  - Evidence: "We therefore perform 2D interpolation of the pre-trained position embeddings, according to their location in the original image." (Section 3.2, p.3)

## 9. Positional Encoding as a Variable
- Role: Fixed architectural choice in main experiments, with ablations showing limited impact.
  - Evidence: "We use standard learnable 1D position embeddings, since we have not observed significant performance gains from using more advanced 2D-aware position embeddings (Appendix D.4)." (Section 3.1, p.3)
- Multiple positional encodings compared: Yes.
  - Evidence: "No Pos. Emb.", "1-D Pos. Emb.", "2-D Pos. Emb.", "Rel. Pos. Emb." (Table 8, Appendix D.4, p.17)
- PE choice described as not critical: Yes.
  - Evidence: "there is little to no difference between different ways of encoding positional information." (Appendix D.4, p.17)

## 10. Evidence of Constraint Masking (Scale vs. Structure)
- Model sizes:
  - Evidence: "ViT-Base       12          768           3072        12      86M", "ViT-Large      24          1024          4096        16      307M", "ViT-Huge       32          1280          5120        16      632M" (Table 1, Section 4.1, p.4)
- Dataset sizes:
  - Evidence: "Datasets. To explore model scalability, we use the ILSVRC-2012 ImageNet dataset with 1k classes and 1.3M images (we refer to it as ImageNet in what follows), its superset ImageNet-21k with 21k classes and 14M images (Deng et al., 2009), and JFT (Sun et al., 2017) with 18k classes and 303M high-resolution images." (Section 4.1, p.4)
- Scaling data as primary driver: "However, the picture changes if the models are trained on larger datasets (14M-300M images). We find that large scale training trumps inductive bias." (Introduction, p.2)
- Scaling model size benefit at large data: "Only with JFT-300M, do we see the full benefit of larger models." (Section 4.3, p.6)
- Training tricks / regularization: "To boost the performance on the smaller datasets, we optimize three basic regularization parameters – weight decay, dropout, and label smoothing." (Section 4.3, p.6)
- Additional training tricks for results: "For ImageNet results in Table 2, we fine-tuned at higher resolution: 512 for ViT-L/16 and 518 for ViT-H/14, and also used Polyak & Juditsky (1992) averaging with a factor of 0.9999 (Ramachandran et al., 2019; Wang et al., 2020b)." (Section 4.1, p.5)

## 11. Architectural Workarounds
- Fixed-size patching to convert images into token sequences: "We split an image into fixed-size patches, linearly embed each of them, add position embeddings, and feed the resulting sequence of vectors to a standard Transformer encoder." (Figure 1 caption, p.3)
- Classification token for image-level prediction: "we use the standard approach of adding an extra learnable “classification token” to the sequence." (Figure 1 caption, p.3)
- Hybrid CNN+ViT variant (CNN feature map as input): "As an alternative to raw image patches, the input sequence can be formed from feature maps of a CNN (LeCun et al., 1989)." (Section 3.1, p.3)
- Patch-size control of sequence length / compute: "Note that the Transformer’s sequence length is inversely proportional to the square of the patch size, thus models with smaller patch size are computationally more expensive." (Section 4.1, p.4)
- Resolution change via positional embedding interpolation: "We therefore perform 2D interpolation of the pre-trained position embeddings, according to their location in the original image." (Section 3.2, p.3)
- Task-specific heads on transfer: "When transferring ViT models to another dataset, we remove the whole head (two linear layers) and replace it by a single, zero-initialized linear layer outputting the number of classes required by the target dataset." (Appendix B.1.1, p.13)

## 12. Explicit Limitations and Non-Claims
- Other tasks (detection/segmentation) not addressed: "While these initial results are encouraging, many challenges remain. One is to apply ViT to other computer vision tasks, such as detection and segmentation." (Conclusion, p.9)
- Self-supervised pre-training gap: "Our initial experiments show improvement from self-supervised pre-training, but there is still large gap between self-supervised and large-scale supervised pre-training." (Conclusion, p.9)
- Contrastive pre-training left for future work: "We leave exploration of contrastive pre-training (Chen et al., 2020b; He et al., 2020; Bachman et al., 2019; Hénaff et al., 2020) to future work." (Section 4.6, p.8)
- Open-world learning / unrestrained multi-task learning / meta-learning: Not specified in the paper.
