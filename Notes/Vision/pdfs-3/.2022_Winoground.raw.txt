Winoground: Probing Vision and Language Models
for Visio-Linguistic Compositionality

arXiv:2204.03162v2 [cs.CV] 22 Apr 2022

Tristan Thrush¶ *, Ryan Jiang‡ , Max Bartolo§ ,
Amanpreet Singh¶ , Adina Williams† , Douwe Kiela¶ , Candace Ross† *
¶
Hugging Face; † Facebook AI Research; ‡ University of Waterloo; § University College London
tristan@huggingface.co, ccross@fb.com

Abstract
We present a novel task and dataset for evaluating
the ability of vision and language models to conduct
visio-linguistic compositional reasoning, which we call
Winoground. Given two images and two captions, the goal
is to match them correctly—but crucially, both captions
contain a completely identical set of words, only in a different order. The dataset was carefully hand-curated by expert annotators and is labeled with a rich set of fine-grained
tags to assist in analyzing model performance. We probe a
diverse range of state-of-the-art vision and language models and find that, surprisingly, none of them do much better
than chance. Evidently, these models are not as skilled at
visio-linguistic compositional reasoning as we might have
hoped. We perform an extensive analysis to obtain insights
into how future work might try to mitigate these models’
shortcomings. We aim for Winoground to serve as a useful
evaluation set for advancing the state of the art and driving further progress in the field. The dataset is available at
https://huggingface.co/datasets/facebook/winoground.

1. Introduction
Despite the impressive performance of pretrained vision
and language transformers on a wide variety of multimodal
tasks [47, 51, 56], they remain poorly understood [8, 19, 46,
67]. One important question is to what extent such models are able to conduct unimodal and multimodal compositional reasoning. For humans, the visual differences between images depicting “the tree is in the shopping cart”
and “the shopping cart is in the tree” will be blatantly obvious, even when the words in the captions are identical—but
is the same true for machines?
While matching simple images and captions may seem
almost too trivial a task, recent work in NLP has shown
* Equal contribution. TT, AS, and DK conducted most of the work for
this paper when they were at Facebook AI Research.

(a) some plants
surrounding a
lightbulb

(b) a lightbulb surrounding some plants

Figure 1. An example from Winoground. The two sentences contain the same words but in a different order. The task of understanding which image and caption match is trivial for humans but
much harder for vision and language models. Every model that we
tested (UNITER, ViLLA, VinVL, VisualBERT, ViLT, LXMERT,
ViLBERT, UniT, FLAVA, CLIP, VSE++, and VSRN) fails to correctly pair the images and captions, except the large checkpoint of
ViLLA by a very thin margin (0.00013 confidence).

that transformers are often remarkably insensitive to word
order [70]. Understanding the relationship between text in
captions and corresponding visual content is a fundamental
goal of computer vision, and the fact that different word orders correspond to wildly different visual depictions should
be reflected in the capabilities of our models.
Motivated by this, we propose a novel task, called
Winoground, for measuring visio-linguistic compositional
reasoning, whereby two images and two captions have to be
matched correctly; both captions contain exactly the same
set of words, ordered in such a way that each describes primarily one of the images. To perform well on Winoground,
models must not only encode text and images well (i.e.,
be sensitive to the compositional structure present in each
modality), but they also must be able to synthesize information across the two modalities.
We draw inspiration from the Winograd Schema Challenge [44], which tests the commonsense capabilities of
models. In the challenge, a model is given two sentences

that minimally differ and is tasked with performing coreference resolution. The Winograd twin sentence format has
been used for a variety of language-related tasks [59,60,91].
In this work, we study the image-grounding of twin sentences with identical but differently ordered words.
Winoground was hand-crafted by expert annotators and
is labeled with a rich set of fine-grained tags to assist in analyzing model performance. In efforts to shed better light
on what exactly models learn, the NLP community has designed a wide variety of “probing tasks”: specialized, targeted tasks meant specifically for evaluation. The primary
purpose of Winoground is to serve as a probing task for vision and language models. See Fig. 1 for an example.
We evaluate a variety of state-of-the-art vision and language (V&L) transformers [12, 23, 35, 40, 47, 51, 56, 68,
76, 90] and RNN-based models [21, 45]. Surprisingly, all
of the models rarely—and if so only barely—outperform
chance. Our findings indicate that the visio-linguistic compositional reasoning capabilities of these models fall dramatically short of what we might have hoped.
In what follows, we introduce the Winoground task and
dataset. We then describe the models we tested and discuss
our findings. Next, we conduct an analysis of the performance of different models. We hope that insights from this
work will lead to more robust vision and language models.

2. Related Work
Visio-linguistic stress testing. There are a number of
existing multimodal stress tests about correctly understanding implausible scenes [13], exploitation of language and
vision priors [11, 27], single word mismatches [64], hate
speech detection [26, 32, 41, 92], memes [39, 75], ablation of one modality to probe the other [22], distracting
models with visual similarity between images [7, 33], distracting models with textual similarity between many suitable captions [1,17], collecting more diverse image-caption
pairs beyond the predominately English and North American/Western European datasets [50], probing for an understanding of verb-argument relationships [30], counting [53],
or specific model failure modes [65, 69]. Many of these
stress tests rely only on synthetically generated images, often with minimal visual differences, but no correspondingly
minimal textual changes [80]. Other datasets test models with a single caption [74] or a single image [6, 37].
There are also purely visual stress tests with naturalistic images: ImageNet-C/ImageNet-P [31] tests models on perturbations for a variety of image features. Unlike Winoground,
these stress tests tend to come from existing datasets that
have images and text from typical training domains, such as
Conceptual Captions [63], COCO [48], Visual7W [93] and
VQA [3, 27]. None of them hold the set of words constant
in the captions, which is what allows us to carefully test for
compositional reasoning without any biases stemming from

the presence of altogether different words. While it is theoretically possible for unstructured bag of words models to
do well on these previous datasets, that is not possible on
Winoground.
Probing. Measuring what exactly a model knows about
word order and linguistic structure has been explored in natural language processing. Sinha et al. [70] found that word
order information does not have a large impact on performance when pretraining large transformer language models,
across a variety of metrics. This suggests that transformers
use high-level word co-occurence statistics, which gives the
illusion of an understanding of word order. Other work in
this space has tried to understand what models know about
syntax [24, 28, 34, 49, 54, 71, 83] or the complex interaction
between syntactic and semantic categories [38, 78, 81, 82].
Winograd schemas. The Winograd Schema Challenge [44] was named after a coreference resolution problem presented by Terry Winograd [85]. The goal is to
correctly resolve (an) ambiguous referent(s) in two English sentences. The sentences have a minor difference
that changes how a human resolves the referent. Winograd schema examples are easily handled by humans, and
commonsense reasoning is said to be required [4]. For example, in the sentence “The city councilmen refused the
demonstrators a permit because they [feared/advocated] violence”, the pronoun they can either refer to the councilmen
or to the demonstrators depending on which word is chosen. The format has been used in a variety of other tasks
and datasets. For instance, Sakaguchi et al. [60] introduce
WinoGrande: a large-scale approach to building a Winograd Schema dataset that uses Amazon Mechanical Turk
to generate sentences instead of expert annotators like the
original work of Levesque et al. [44]. Other approaches use
ambiguous pronouns in sentences to probe for gender biases
in models [59, 91]. See Kotcijan et al. [42] for an in-depth
review. Winoground is the first work to apply these ideas
to the vision and language domain, by using twin captions
with identical word content and two images that are each
associated with one caption over the other.

3. Winoground
In this section, we describe how the dataset was constructed and how performance on the task is to be measured.

3.1. Dataset
The Winoground dataset was hand-curated by four expert annotators with extensive experience in vision and
language research as well as computational linguistics.
Let (C0 , I0 ) and (C1 , I1 ) be two image-caption pairs. An
example satisfies the Winoground schema if and only if:
• (C0 , I0 ) and (C1 , I1 ) are preferred by the annotator
over (C1 , I0 ) and (C0 , I1 ); and

Category
Linguisticswap-dep.
(a) there is [a mug] in (c) a person [sits] and a
[some grass]
dog [stands]

(e) it’s a [truck] [fire]

Linguisticswap-indep.

Visual
(b) there is [some (d) a person [stands]
grass] in [a mug]
and a dog [sits]

Object

Relation

(f) it’s a [fire] [truck]

Both

(a) the kid [with the (c) the person with the (e) there are [three]
magnifying
glass] ponytail [packs] stuff people and [two] winlooks at them []
and other [buys] it
dows

(b) the kid [] looks at (d) the person with the (f) there are [two] peothem [with the magni- ponytail [buys] stuff ple and [three] winfying glass]
and other [packs] it
dows

Pragmatics

Series

Symbolic

Figure 3. Examples from our dataset for the swap-dependent linguistic tags (top) and visual tags (bottom). The visual examples
are additionally tagged with the Relation tag, and 1, 2, and 1 main
predicates from left to right. The linguistic examples are additionally tagged with 2, 1, and 1 main predicates from left to right.

• C0 and C1 have the same words and/or morphemes but
the order differs.
We have secured a license from Getty Images to distribute images for research purposes. Thus, the expert annotators were given access to the Getty Images API [25],
and tasked with jointly creating captions and finding images
to compose examples. We encouraged them to be as creative as possible, and to mark each of their examples with
fine-grained linguistic tags. If applicable, annotators also
marked examples with one or more visual reasoning tags.
The annotators created a total of 70 linguistic tags for
the swaps that make caption pairs different. This set of tags
can be split into three broad groups: objects, relations, and
swaps involving both relations and objects. Object swaps
reorder elements such as noun phrases that tend to refer

Tag

Count

Object
Relation
Both

141
233
26

1 Main Pred
2 Main Preds

293
108

Symbolic
Series
Pragmatics

41
31
24

Table 1. Linguistic and visual tag counts in the Winoground
dataset. Every example has a linguistic tag; only examples that
contain the visual phenomena have visual tags.

to objects in the real world. Relation swaps reorder elements such as verbs, adjectives, prepositions, and/or adverbs, which tend to take nouns referring to objects as semantic arguments [2]. Swaps of both relations and objects
can involve two separate swaps, or can involve a single swap
that changes parts of speech (e.g., “it’s a [fire] [truck]” vs.
“it’s a [truck] [fire]”). Examples of each broad tag group
can be seen in Fig. 3. For examples for each fine-grained
linguistic tag, see Appendix C.
Separately, the annotators tagged examples for how
many main predicates were in the captions, which is not
dependent on the specific swap happening between the two
captions. For example, “left is blue and right is red” has
two main predicates and “water is in a bottle” has one main
predicate. It turned out that all examples in Winoground
have either one main predicate or two.
Finally, examples were tagged from a set of three nonmutually exclusive visual reasoning tags, which are tied in
some way to the images in an example, and not necessarily the captions. The “Pragmatics” tag comprises examples
where the images need to be interpreted non-literally due to
idiomatic uses of language in a caption (e.g. “it starts with Z
and ends with A” describing an image of a Zebra) or due to
attachment preferences of prepositional phrases in the captions (e.g. “the kid looks at them with the magnifying glass”
describing an image of a child looking at someone through
a magnifying glass with greater confidence than an image
of a child looking at someone while holding a magnifying
glass at their side). The “Symbolic” tag represents whether
a symbolic depiction of something must be understood to
make a correct prediction (e.g., objects in a child’s drawing). Lastly, the “Series” tag is given to examples where
both images come from the same photo series on Getty,
which typically means that the same people occur in both
images, with a similar background and in similar lighting.
See Fig. 3 for representative examples of the tags, and
Tab. 1 for tag counts. As noted, Winoground is a probing

dataset and so we prioritize clean, expert annotations over
mere size. Our dataset has 1600 image-text pairs in total,
with 800 correct and 800 incorrect pairings. These comprise
400 examples, with 800 unique captions and images.

3.2. Metrics
Performance on Winoground is computed according to
three different metrics that evaluate different aspects of the
models’ visio-linguistic reasoning abilities. The first metric
is the text score, which measures whether a model can select the correct caption, given an image. Given images I0
and I1 and captions C0 and C1 , the text score for an example (C0 , I0 , C1 , I1 ) is computed according to:


1 if s(C0 , I0 ) > s(C1 , I0 )
f (C0 , I0 , C1 , I1 ) =
and s(C1 , I1 ) > s(C0 , I1 )


0 otherwise
(1)
where s(·) is the model’s score for the image/caption pair.
This metric tests whether the ground truth caption for a
given image in our dataset is scored higher than the alternative caption and whether this holds for the other image/caption pair in the example too.
The second metric is the image score, which measures
whether a model can select the correct image, given a caption. Given images I0 and I1 and captions C0 and C1 , the
image score for an example is computed according to:


1 if s(C0 , I0 ) > s(C0 , I1 )
g(C0 , I0 , C1 , I1 ) =
and s(C1 , I1 ) > s(C1 , I0 )


0 otherwise
(2)
This metric tests whether the ground truth image for a given
caption is scored higher than the image corresponding to the
alternative caption and whether this holds vice versa.
Our final metric combines the previous two. In their
analysis of the Winograd Schema Challenge, Elazar et
al. [20] find that evaluation metrics tend to overestimate
model performance by computing scores for the twin sentences individually instead of as a set. So, we also evaluate using the group score, where every combination for a
given example {(C0 , I0 ), (C0 , I1 ), (C1 , I0 ), (C1 , I1 )} must
be correctly scored by the model in order for the example to
be considered correct. The group score in our framework is
computed according to:


1 if f (C0 , I0 , C1 , I1 )
h(C0 , I0 , C1 , I1 ) =
(3)
and g(C0 , I0 , C1 , I1 )


0 otherwise

4. Experimental Setup

We evaluate various configurations of the following multimodal transformers: CLIP [56], FLAVA [68], LXMERT

[76], UniT [35], UNITER [12], VILLA [23], VinVL [90],
ViLT [40], VisualBERT [47] and ViLBERT [51]. We also
evaluate several configurations of two types of RNN-based
models: VSE++ [21] and VSRN [45]. We detail differences
between these models and provide a high-level overview in
Tab. 2. We also establish a human baseline using crowdworkers, as described in Sec. 4.3.

4.1. Vision & Language Transformers
Image and language embedding. All transformer
models we evaluate use a pretrained BERT tokenizer
[16], except CLIP, which uses a Byte-Pair Encoding tokenizer [62] trained from scratch. For the image embedding, five transformers (VisualBERT, ViLBERT, LXMERT,
UNITER, ViLLA) [12,23,47,51,76] use region features extracted from the fc6 layer of a Faster R-CNN [58] trained
on Visual Genome [43]. VinVL trains its own feature extractor on a large combined dataset from public sources with
a unified object vocabulary [90]. The CLIP, FLAVA, and
ViLT that we test all use Vision Transformer (ViT) [18]. In
ViT, images are flattened into patches that are linearly projected and combined with a position encoding. UniT [35]
alternatively uses a transformer network [79] on top of a
convolutional network following Carion et al. [9].
Single-stream vs. dual-stream encoders. Vision and
language transformers are mainly single- or dual-stream
models: the embeddings for the image and text modalities
are either concatenated and then jointly encoded (singlestream), or encoded by two separate modality-specific encoders with optional cross-modality fusion (dual-stream).
Five of our transformers are single-stream [12, 23, 40, 47,
90]. VinVL additionally concatenates object tags, which
are the set of objects detected by the X152-C4 model during feature extraction, to the language tokens before encoding. All single-stream models use merged attention,
where the language and visual input attend to both themselves and the other modality. The dual-stream transformers
we evaluate are CLIP, FLAVA, UniT, LXMERT and ViLBERT [35, 51, 56, 68, 76]. CLIP and the contrastive configuration of FLAVA lack cross-modal attention. ViLBERT
has language-only transformer layers that are then fused by
cross-modal transformer layers. LXMERT, the ITM configuration of FLAVA, and UniT each use language-only and
vision-only layers that are also fused by cross-modal transformer layers, which perform a combo of modality-specific
attention and co-attention across modalities.
Pretraining objectives. V&L transformers use a number of pretraining objectives including but not limited to
masked language modeling, masked region modeling (classification of object classes and regression over image features) and image-text matching. As we are evaluating a
model’s ability to determine if an image and a corresponding caption match, we select V&L transformers that are pre-

Model

Datasets

VinVL [90]
UNITER [12]
ViLLA [23]
VisualBERT [47]
ViLT [40]
LXMERT [76]
ViLBERT [51]
UniT [35]
FLAVA IT M [68]
FLAVA Contrastive [68]
CLIP [56]
VSE++ and VSRN COCO
VSE++ and VSRN F lickr30k

VQA, GQA, VG-QA, COCO, Flickr30k, CC, SBU
COCO, VG, CC, SBU
COCO, VG, CC, SBU
COCO, NVLR2
COCO, VG, SBU, CC
COCO, VG
CC
COCO detect., VG detect., VQAv2, SNLI-VE QNLI, MNLI-mm, QQP, SST-2
COCO, SBU, LN, CC, VG, WIT, CC 12M, RC, YFCC100M
COCO, SBU, LN, CC, VG, WIT, CC 12M, RC, YFCC100M
−
COCO
Flickr30k

# Images, Captions (Millions)

Architecture

Attention

1.89, 4.87
4.20, 9.58
4.20, 9.58
0.30, 0.52
4.10, 9.85
0.18, 9.18
3.30, 3.30
0.69, 1.91
70.00, 70.00
70.00, 70.00
400.00, 400.00
0.11, 0.57
0.03, 0.16

single-stream
single-stream
single-stream
single-stream
single-stream
dual-stream
dual-stream
dual-stream
dual-stream
dual-stream
dual-stream
dual-stream
dual-stream

merged
merged
merged
merged
merged
modality-specific, co-attn, merged
modality-specific, co-attn, merged
modality-specific, merged
modality-specific, merged
modality-specific
modality-specific
−
−

Table 2. A high-level overview of the differences between the models we evaluate by the pretraining datasets, architecture, and attention
mechanisms between the modalities. We omit datasets that were only used to train backbones. We exclude the language embedding from
this table as every model uses a pretrained BERT tokenizer, except CLIP, VSE++, and VSRN. The pretraining datasets include COCO [48],
Visual Genome (VG) [43], Conceptual Captions (CC) [63], SBU Captions [52], Flickr30k [88], VQA 2.0 [27], VCR [89], NLVR2 [74],
SNLI-VE [87], QNLI [57], MLNI-mm [84], QQP [36], Localized Narratives (LN) [55], Wikipedia Image Text (WIT) [73], Conceptual
Captions 12M (CC 12M) [10], Red Caps (RC) [15], YFCC100M [77], and SST-2 [72]. CLIP uses their own dataset for pretraining.

trained with an image-text matching classification head or
that produce a similarity score between the two modalities1 .

4.2. Multimodal RNNs
To determine whether low performance on Winoground
is unique to transformer-based models, we include results
for two sequence-based models, which are VSRN [45] and
VSE++ [21]. Both VSE++ and VSRN have a loss function that prioritizes minimizing the hardest negative’s score.
The hardest negative is the highest-scoring image-caption
pair that is not correct. Intuitively, this type of loss function
could enable models to get higher scores on Winoground in
particular and may be useful in future work. Although we
show later in the paper that VSRN and VSE++ do not do
well, perhaps due to issues besides the loss function. Both
models use a GRU [14] to get language embeddings and
a separate pipeline to get image embeddings. Scores for
image-caption pairs are found by taking an inner-product of
the embeddings. VSE’s image encoder is a linear projection
of the embedding from a backbone (either ResNet152 [29]
or VGG19 [66]). In VSRN, a ResNet101-based Faster RCNN with graph convolutions on top is used to get a sequence of features which are fed into a GRU. The GRU’s
last hidden state is then used as the image embedding.

4.3. Human Performance
We employed crowd workers on the Amazon Mechanical Turk platform to establish a more conservative human
baseline than the expert annotator upper bound of a perfect
score. Like the models, annotators are shown one image
and one caption at a time. Annotators are asked the binary
choice question “Does the caption match the image?”. All
1600 combinations of images and captions are labeled by at
1 UniT is the only model we selected that was not pretrained on imagetext matching. To get image-text alignment scores, we finetuned UniT on
image-text matching loss using MS-COCO [48]

least ten annotators. We compute the human image-caption
score as the ratio of annotators who said the image/caption
pair match over the total number of annotators for the pair.
More details about the human labelling interface, onboarding criteria, and quality control are provided in Appendix E.

5. Results
5.1. Compared to humans
As shown in Tab. 3, the models struggle across the board
on Winoground, often performing close to or below random chance. Comparatively, as expected, the human performance is high across the full range of linguistic and visual phenomena. For the text score, we observe ∼50%
absolute difference between humans and the best performing models—UNITER, VILLA VinVL, ViLT, FLAVA, and
CLIP—with the remaining models below chance.
The human performance is only slightly lower for the
image score, whereas all models perform much worse.
Even the highest performing model, FLAVAIT M , has a
∼70% performance gap compared to humans. This gap is
not unique to our dataset: in prior work [21] [56], models also tend to perform significantly better on caption retrieval compared to image retrieval. More investigation is
required to pinpoint the reasons: perhaps textual encoders
are stronger, or the text modality has different biases.
Lastly, we consider the group score. For humans, it is
not appreciably lower than their text and image scores. All
of the models are below random chance here as well. We
report confidence intervals for these results in Appendix A.

5.2. Results by Tags
For the swap-dependent linguistic tags, human performance is highest on object, followed by the relation and
then both. For the swap-independent linguistic tags, humans do better on examples with two main predicates,

Model

Text

Image

Group

MTurk Human
Random Chance

89.50
25.00

88.50
25.00

85.50
16.67

VinVL
UNITERlarge
UNITERbase
ViLLAlarge
ViLLAbase
VisualBERTbase
ViLT (ViT-B/32)
LXMERT
ViLBERTbase
UniTIT M f inetuned
FLAVAIT M
FLAVAContrastive
CLIP (ViT-B/32)
VSE++COCO (ResNet)
VSE++COCO (VGG)
VSE++F lickr30k (ResNet)
VSE++F lickr30k (VGG)
VSRNCOCO
VSRNF lickr30k

37.75
38.00
32.25
37.00
30.00
15.50
34.75
19.25
23.75
19.50
32.25
25.25
30.75
22.75
18.75
20.00
19.75
17.50
20.00

17.75
14.00
13.25
13.25
12.00
2.50
14.00
7.00
7.25
6.25
20.50
13.50
10.50
8.00
5.50
5.00
6.25
7.00
5.00

14.50
10.50
10.00
11.00
8.00
1.50
9.25
4.00
4.75
4.00
14.25
9.00
8.00
4.00
3.50
2.75
4.50
3.75
3.50

Table 3. Results on the Winoground dataset across the text, image
and group score metrics. Results above random chance in bold.

which tend to contain longer and more complicated sentences. The models perform poorly on every category, but
they largely show the opposite pattern. They perform better on examples with simpler and shorter sentences which
more often have swaps at the morpheme level (see Tab. 4).
One exception to the low model performance is that CLIP
performs comparably to the humans on the both tag text
score—the 26 examples with the both tag have some of the
shortest and least compositional captions in our dataset (e.g.
“presenting the watch” vs “watching the present”).
We also evaluate performance for the visual reasoning
tags as shown in Tab. 5. Models and humans are particularly good at the symbolic examples, but the models are
poor comparatively. On the pragmatics tag, humans have
the lowest performance. Ten crowdworkers probably didn’t
capture slight pragmatics preferences that our expert linguist annotators agreed on. One example that the crowdworkers failed is Fig. 3(a): “the kid [with the magnifying
glass] looks at them []”. All ten annotators said that “the
kid with the magnifying glass looks at them” was acceptable for both images, but captured the correct preference
for the second caption. This reveals a limitation in how the
task was presented to humans: our hypothesis is that if we
gave humans both images and both captions at the same
time, or if significantly more human annotators gave their

judgements, then the human scores would be substantially
higher. Finally, models do worst on the series tag where
most get a 0% group score, which indicates that they are
always choosing one image over the other regardless of the
caption (or vice versa).

6. Discussion
Despite the fact that every model struggled on
Winoground compared to humans, we hope to gain further
insights by analyzing which aspects of these models could
contribute to their performance differences.

6.1. Capabilities of Encoders
Richer features. UNITER, VILLA, VinVL, ViLT,
FLAVA, and CLIP are the only models that get above random chance performance in Tab. 3, and only for the text
score. We hypothesize that these models perform better
than others due to their richer features (unimodal features
for CLIP and FLAVAContrastive , multimodal features for
the others). A potential explanation could be the large-scale
pretraining used by CLIP and FLAVA, the large training
dataset used to train the object detector for VinVL, or the
ViT approach for image features used by ViLT, FLAVA, and
CLIP that encodes every portion of the image.
Common failure modes. We highlight again that most
of the models fail with 0% group score on the same image
series tag. One explanation is that the models’ visual encoders might be too weak to correctly discriminate between
substantially similar images. This could cause the models
to fall back on their unimodal priors, picking one caption
or image over the other in the majority of the four potential
caption-image pairings.
Heat maps. We show a heatmap in Fig. 4 of the wordregion alignment between ViLT’s vision and language features as a visualization for a model with some of the better
performance on our dataset. ViLLA and UNITER are also
trained with word-region alignment and we provide their
heatmaps in Appendix D.
Complicated captions. The above-chance models do
worse on examples with longer captions, possibly due to
weak language encoding abilities. As shown in Tab. 6, caption length and lower model performance significantly correlate for the best models, even though the correlation is reversed for humans. The examples with the shortest captions
are also the least compositional; they are primarily the examples where the parts of speech change between swapped
words, or where there is a morpheme-level swap. Finally,
we show in Tab. 6 correlations between caption perplexity2
and model scores. We found that there is typically a weak
correlation between models assigning an image-caption pair
a higher score and a caption having low perplexity.
2 We used the standard size GPT2 checkpoint from Hugging Face transformers to get perplexity [86].

Model

Text

Object
Image Group

Text

Relation
Image Group

Text

Both
Image

Group

Text

MTurk Human
VinVL
UNITERlarge
UNITERbase
ViLLAlarge
ViLLAbase
VisualBERTbase
ViLT (ViT-B/32)
LXMERT
ViLBERTbase
UniTIT M f inetuned
FLAVAIT M
FLAVAContrastive
CLIP (ViT-B/32)
VSE++COCO (ResNet)
VSE++COCO (VGG)
VSE++F lickr30k (ResNet)
VSE++F lickr30k (VGG)
VSRNCOCO
VSRNF lickr30k

92.20
36.88
39.01
34.04
36.88
33.33
19.15
31.91
22.70
29.08
17.73
31.91
23.40
34.75
21.99
17.73
20.57
17.73
15.60
16.31

90.78
17.73
12.77
11.35
14.89
15.60
2.13
15.60
9.22
10.64
5.67
23.40
19.15
7.80
6.38
2.13
6.38
4.96
4.96
4.96

89.27
37.77
36.05
30.04
37.34
27.04
12.88
36.91
17.60
19.31
18.03
30.04
23.61
22.75
23.61
18.45
18.88
19.74
18.88
21.03

90.56
17.60
14.16
14.16
12.88
9.01
2.15
11.59
5.58
3.00
4.72
16.31
8.58
8.58
9.01
7.30
4.29
6.87
7.73
4.29

76.92
42.31
50.00
42.31
34.62
38.46
19.23
30.77
15.38
34.62
42.31
53.85
50.00
80.77
19.23
26.92
26.92
30.77
15.38
30.77

57.69
19.23
19.23
15.38
7.69
19.23
7.69
26.92
7.69
26.92
23.08
42.31
26.92
42.31
7.69
7.69
3.85
7.69
11.54
11.54

57.69
19.23
19.23
11.54
7.69
15.38
3.85
19.23
3.85
19.23
19.23
30.77
26.92
38.46
3.85
7.69
3.85
7.69
3.85
7.69

87.33
39.38
40.07
35.27
39.73
33.22
16.44
35.27
19.18
23.97
21.58
36.30
26.37
35.27
25.00
18.49
21.58
20.55
17.12
20.89

88.65
14.18
9.93
9.22
11.35
9.93
0.71
9.22
6.38
7.09
2.13
14.89
11.35
6.38
1.42
2.13
3.55
2.84
2.13
2.13

86.70
14.16
9.87
10.30
11.16
6.01
1.72
8.15
2.58
1.72
3.43
12.02
5.58
5.58
5.58
3.86
2.15
5.15
4.72
3.86

1 Main Pred
Image Group
85.62
21.23
16.44
14.73
17.12
14.04
2.74
17.12
8.56
8.90
6.85
24.66
16.44
13.01
9.59
4.79
6.51
6.16
7.19
5.82

82.53
17.47
13.36
11.99
14.38
10.27
1.71
11.64
5.14
5.82
4.11
17.81
10.62
10.27
4.79
2.74
3.42
4.79
3.77
3.77

2 Main Preds
Text Image Group
95.37
33.33
32.41
24.07
29.63
21.30
12.96
33.33
19.44
23.15
13.89
21.30
22.22
18.52
16.67
19.44
15.74
17.59
18.52
17.59

96.30
8.33
7.41
9.26
2.78
6.48
1.85
5.56
2.78
2.78
4.63
9.26
5.56
3.70
3.70
7.41
0.93
6.48
6.48
2.78

93.52
6.48
2.78
4.63
1.85
1.85
0.93
2.78
0.93
1.85
3.70
4.63
4.63
1.85
1.85
5.56
0.93
3.70
3.70
2.78

Table 4. The results by linguistic tag. Results above chance are in bold.
Model

Text

Symbolic
Image Group

Text

MTurk Human
VinVL
UNITERlarge
UNITERbase
ViLLAlarge
ViLLAbase
VisualBERTbase
ViLT (ViT-B/32)
LXMERT
ViLBERTbase
UniTIT M f inetuned
FLAVAIT M
FLAVAContrastive
CLIP (ViT-B/32)
VSE++COCO (ResNet)
VSE++COCO (VGG)
VSE++F lickr30k (ResNet)
VSE++F lickr30k (VGG)
VSRNCOCO
VSRNF lickr30k

96.43
25.00
39.29
46.43
39.29
42.86
28.57
28.57
28.57
28.57
14.29
25.00
17.86
39.29
32.14
17.86
21.43
28.57
7.14
21.43

92.86
17.86
28.57
14.29
14.29
17.86
0.00
17.86
3.57
10.71
10.71
28.57
10.71
3.57
10.71
14.29
3.57
10.71
3.57
3.57

58.82
29.41
35.29
29.41
17.65
29.41
5.88
35.29
17.65
29.41
17.65
17.65
11.76
35.29
23.53
17.65
23.53
11.76
11.76
35.29

92.86
14.29
17.86
14.29
10.71
14.29
0.00
10.71
3.57
7.14
7.14
17.86
10.71
3.57
10.71
7.14
0.00
10.71
0.00
3.57

Pragmatics
Image Group
41.18
5.88
0.00
17.65
0.00
5.88
0.00
0.00
5.88
5.88
5.88
29.41
23.53
5.88
11.76
0.00
0.00
0.00
0.00
11.76

41.18
5.88
0.00
11.76
0.00
5.88
0.00
0.00
0.00
5.88
5.88
11.76
5.88
5.88
0.00
0.00
0.00
0.00
0.00
5.88

Same Image Series
Text Image Group
95.65
34.78
4.35
8.70
17.39
13.04
13.04
26.09
8.70
13.04
21.74
17.39
17.39
8.70
13.04
13.04
17.39
13.04
13.04
8.70

91.30
17.39
8.70
8.70
4.35
8.70
0.00
0.00
4.35
0.00
4.35
8.70
4.35
0.00
4.35
4.35
4.35
4.35
0.00
4.35

91.30
13.04
0.00
0.00
0.00
4.35
0.00
0.00
0.00
0.00
4.35
0.00
4.35
0.00
4.35
4.35
0.00
0.00
0.00
4.35

Table 5. The results by visual tag. Results above chance are in bold.

6.2. By Architecture & Type of Attention
As shown in Tabs. 3 to 5, both single-stream and dualstream models perform significantly worse than humans on
the text, image and group scores. We find at least one
single-stream model and at least one dual-stream model
are above chance for most of our experiments, suggesting
there is not a distinct performance difference by architecture. Although, six single-stream model checkpoints do
above chance overall, compared to only the very large dualstream models (CLIP and FLAVA). CLIP and FLAVA were
trained on an order of magnitude more data than the other
models. Across all types of attention, models struggled
compared to humans. But neither of the two models using co-attention, in conjunction with single-modality and/or
merged attention, performed above chance.

6.3. By Multimodal Pretraining Dataset Size
We find highly significant correlations between the size
of the multimodal pretraining dataset and the scores, if we
remove CLIP and FLAVA as outliers. Tab. 7 shows these
correlations, and Appendix B has graphs showing each
model’s score versus the pretraining data size. The unimodal training data (for image backbones or pre-initialized
text encoders) is not included in these calculations.

7. Conclusion
We introduced a novel task and dataset, Winoground,
aimed at measuring visio-linguistic compositional reasoning in state of the art vision and language models. We
demonstrate that models fall short, in most cases performing no better than chance. Our findings highlight that there

a brown dog is on a white couch

a white dog

is on a

circular food on heart-shaped wood

brown couch

heart-shaped food on circular wood

.png

Figure 4. Word-region alignment scores between the image and text features for ViLT [40] on examples from Winoground. In this case
study, ViLT appears to disregard the information from adjectives. E.g., the heatmaps highlight the brown dog just as strongly regardless of
whether the text was “brown dog” or “white dog”.

Model

Perplexity
Corr. p-value

Caption Length
Corr. p-value

Pretraining Modality

Score

Corr.

p-value

MTurk Human
VinVL
UNITERlarge
UNITERbase
ViLLAlarge
ViLLAbase
VisualBERTbase
ViLT (ViT-B/32)
LXMERT
ViLBERTbase
UniTIT M f inetuned
FLAVAIT M
FLAVAContrastive
CLIP (ViT-B/32)
VSE++COCO (ResNet)
VSE++COCO (VGG)
VSE++F lickr30k (ResNet)
VSE++F lickr30k (VGG)
VSRNCOCO
VSRNF lickr30k

0.05
-0.05
-0.01
-0.03
-0.02
-0.04
-0.04
-0.04
-0.04
-0.04
-0.01
-0.03
-0.06
-0.04
-0.05
-0.04
-0.02
0.01
-0.07
-0.02

0.20
-0.20
-0.16
-0.14
-0.12
-0.11
-0.06
-0.16
-0.11
-0.14
-0.02
-0.23
-0.19
-0.22
0.01
0.03
0.02
-0.10
-0.05
-0.05

Image

Text
Image
Group

0.84
0.76
0.75

0.00
0.00
0.00

Caption

Text
Image
Group

0.77
0.75
0.71

0.00
0.00
0.00

0.07
0.04
0.57
0.22
0.39
0.13
0.15
0.16
0.12
0.11
0.73
0.22
0.01
0.09
0.04
0.08
0.43
0.74
0.01
0.32

0.00
0.00
0.00
0.00
0.01
0.03
0.22
0.00
0.02
0.00
0.73
0.00
0.00
0.00
0.90
0.56
0.67
0.04
0.36
0.29

Table 6. (left) The correlation between model image-caption
scores and the caption perplexity from GPT2. (right) The correlation between the model group scores and the caption length.

is more work to be done. Particularly, the field could investigate possible strengths of single-stream models, the compilation of more pretraining data, improving image-encoding
capabilities, and pretraining objectives that emphasize similar but wrong images. We hope that our task and dataset
will help guide research in this important direction.

Table 7. Correlations between the number of pretraining images
and captions and the model text, image, and group scores. CLIP
and FLAVA are excluded as outliers.

Broader Impact & Limitations.
Winoground is
English-only and translation to other languages may be nontrivial [50]. Expert curation is time-consuming and our
dataset is limited in size. Multimodal datasets containing
images of people require thoughtful consideration of how
people are represented (see [5] for a detailed analysis of the
stereotypes present in many multimodal datasets). We used
gender underspecified human denoting terms (e.g., person,
child) to avoid issues with inferring gender identity from
images [61]. Our annotators disproportionately come from
the USA and the same could be true for our crowdworkers.
Getty Acknowledgement. Images in the paper are a
compilation of assets, including ©Getty Images/Natasha
Breen, Maki Nakamura, Jessica Peterson, Kundanlall
Sharma, lacaosa, Alberto Bogo, Vu Le, Toson Rueangsuksut, Nisian Hughes, Tanja Walter, Douglas Sacha, PBNJ
Productions, Glow Images, 10’000 Hours, zoranm, Marlene
Ford, Westend61.

References
[1] Arjun Akula, Spandana Gella, Yaser Al-Onaizan, SongChun Zhu, and Siva Reddy. Words aren’t enough, their order
matters: On the robustness of grounding visual referring expressions. In ACL, 2020. 2
[2] Daniel Altshuler,
Terence Parsons,
and Roger
Schwarzschild. A Course in Semantics. MIT Press,
2019. 3
[3] Stanislaw Antol, Aishwarya Agrawal, Jiasen Lu, Margaret
Mitchell, Dhruv Batra, C Lawrence Zitnick, and Devi Parikh.
Vqa: Visual question answering. In ICCV, 2015. 2
[4] David Bender. Establishing a human baseline for the winograd schema challenge. In Modern Artificial Intelligence and
Cognitive Science, 2015. 2
[5] Abeba Birhane, Vinay Uday Prabhu, and Emmanuel Kahembwe. Multimodal datasets: misogyny, pornography, and
malignant stereotypes. In arXiv preprint arXiv:2110.01963,
2021. 8
[6] Yonatan Bitton, Gabriel Stanovsky, Roy Schwartz, and
Michael Elhadad. Automatic generation of contrast sets
from scene graphs: Probing the compositional consistency
of GQA. In NAACL: Human Language Technologies, 2021.
2
[7] Ben Bogin, Shivanshu Gupta, Matt Gardner, and Jonathan
Berant. Covr: A test-bed for visually grounded compositional generalization with real images. In EMNLP, 2021. 2
[8] Jize Cao, Zhe Gan, Yu Cheng, Licheng Yu, Yen-Chun Chen,
and Jingjing Liu. Behind the scene: Revealing the secrets of
pre-trained vision-and-language models. In ECCV, 2020. 1
[9] Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas
Usunier, Alexander Kirillov, and Sergey Zagoruyko. End-toend object detection with transformers. In ECCV, 2020. 4
[10] Soravit Changpinyo, Piyush Sharma, Nan Ding, and Radu
Soricut. Conceptual 12m: Pushing web-scale image-text pretraining to recognize long-tail visual concepts. In CVPR,
2021. 5
[11] Wei-Lun Chao, Hexiang Hu, and Fei Sha. Being negative but constructively: Lessons learnt from creating better visual question answering datasets. In arXiv preprint
arXiv:1704.07121, 2017. 2
[12] Yen-Chun Chen, Linjie Li, Licheng Yu, Ahmed El Kholy,
Faisal Ahmed, Zhe Gan, Yu Cheng, and Jingjing Liu. Uniter:
Universal image-text representation learning. In ECCV,
2020. 2, 4, 5
[13] Myung Jin Choi, Antonio Torralba, and Alan S. Willsky.
Context models and out-of-context objects. In Pattern
Recognition Letters, 2012. 2
[14] Junyoung Chung, Caglar Gulcehr, KyungHyun Cho, and
Yoshua Bengio. Empirical evaluation of gated recurrent neural networks on sequence modeling. In NeurIPS, 2014. 5
[15] Karan Desai, Gaurav Kaul, Zubin Aysola, and Justin Johnson. Redcaps: Web-curated image-text data created by the
people. In NeurIPS Datasets and Benchmarks, 2021. 5
[16] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina
Toutanova. BERT: Pre-training of deep bidirectional transformers for language understanding. In NAACL: Human
Language Technologies, 2019. 4

[17] Nan Ding, Sebastian Goodman, Fei Sha, and Radu Soricut.
Understanding image and text simultaneously: a dual visionlanguage machine comprehension task. In arXiv preprint
arXiv:1612.07833, 2016. 2
[18] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,
Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,
Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. An Image is
Worth 16x16 Words: Transformers for Image Recognition at
Scale. In ICLR, 2021. 4
[19] Zi-Yi Dou, Yichong Xu, Zhe Gan, Jianfeng Wang, Shuohang Wang, Lijuan Wang, Chenguang Zhu, Zicheng Liu,
Michael Zeng, et al. An empirical study of training endto-end vision-and-language transformers. In arXiv preprint
arXiv:2111.02387, 2021. 1
[20] Yanai Elazar, Hongming Zhang, Yoav Goldberg, and Dan
Roth. Back to square one: Artifact detection, training and
commonsense disentanglement in the winograd schema. In
EMNLP, 2021. 4
[21] Fartash Faghri, David J. Fleet, Jamie Ryan Kiros, and Sanja
Fidler. Vse++: Improving visual-semantic embeddings with
hard negatives. In BMVC, 2018. 2, 4, 5
[22] Stella Frank, Emanuele Bugliarello, and Desmond Elliott.
Vision-and-language or vision-for-language? on crossmodal influence in multimodal transformers. In EMNLP,
2021. 2
[23] Zhe Gan, Yen-Chun Chen, Linjie Li, Chen Zhu, Yu Cheng,
and Jingjing Liu. Large-scale adversarial training for visionand-language representation learning. In NeurIPS, 2020. 2,
4, 5
[24] Jon Gauthier, Jennifer Hu, Ethan Wilcox, Peng Qian, and
Roger Levy. SyntaxGym: An online platform for targeted
evaluation of language models. In ACL: System Demonstrations, 2020. 2
[25] https://www.gettyimages.com/. 3
[26] Raul Gomez, Jaume Gibert, Lluis Gomez, and Dimosthenis Karatzas. Exploring hate speech detection in multimodal
publications. In ICCV, 2020. 2
[27] Yash Goyal, Tejas Khot, Douglas Summers-Stay, Dhruv Batra, and Devi Parikh. Making the v in vqa matter: Elevating
the role of image understanding in visual question answering. In CVPR, 2017. 2, 5
[28] Kristina Gulordava, Piotr Bojanowski, Edouard Grave, Tal
Linzen, and Marco Baroni. Colorless green recurrent networks dream hierarchically. In NAACL: Human Language
Technologies, 2018. 2
[29] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
Deep residual learning for image recognition. In CVPR,
2016. 5
[30] Lisa Anne Hendricks and Aida Nematzadeh. Probing imagelanguage transformers for verb understanding. In ACLIJCNLP, 2021. 2
[31] Dan Hendrycks and Thomas Dietterich. Benchmarking neural network robustness to common corruptions and perturbations. In ICLR, 2019. 2
[32] Homa Hosseinmardi, Sabrina Arredondo Mattson, Rahat Ibn
Rafiq, Richard Han, Qin Lv, and Shivakant Mishra. Detec-

tion of cyberbullying incidents on the instagram social network. In arXiv preprint arXiv:1503.03909, 2015. 2
[33] Hexiang Hu, Ishan Misra, and Laurens van der Maaten. Evaluating text-to-image matching using binary image selection
(bison). In ICCV, 2019. 2
[34] Jennifer Hu, Jon Gauthier, Peng Qian, Ethan Wilcox, and
Roger Levy. A systematic assessment of syntactic generalization in neural language models. In ACL, 2020. 2
[35] Ronghang Hu and Amanpreet Singh. Unit: Multimodal multitask learning with a unified transformer. In arXiv preprint
arXiv:2102.10772, 2021. 2, 4, 5
[36] Shankar Iyer, Nikhil Dandekar, and Kornel Csernai. First
quora dataset release: Question pairs, 2017. 5
[37] Justin Johnson, Bharath Hariharan, Laurens Van
Der Maaten, Li Fei-Fei, C Lawrence Zitnick, and Ross
Girshick. Clevr: A diagnostic dataset for compositional
language and elementary visual reasoning. In CVPR, 2017.
2
[38] Katharina Kann, Alex Warstadt, Adina Williams, and
Samuel R. Bowman. Verb argument structure alternations
in word and sentence embeddings. In SCiL, 2019. 2
[39] Douwe Kiela, Hamed Firooz, Aravind Mohan, Vedanuj
Goswami, Amanpreet Singh, Pratik Ringshia, and Davide Testuggine. The hateful memes challenge: Detecting hate speech in multimodal memes. In arXiv preprint
arXiv:2005.04790, 2020. 2
[40] Wonjae Kim, Bokyung Son, and Ildoo Kim. Vilt: Visionand-language transformer without convolution or region supervision. In ICML, 2021. 2, 4, 5, 8
[41] Hannah Rose Kirk, Bertram Vidgen, Paul Röttger, Tristan Thrush, and Scott A Hale.
Hatemoji: A test
suite and adversarially-generated dataset for benchmarking and detecting emoji-based hate. In arXiv preprint
arXiv:2108.05921, 2021. 2
[42] Vid Kocijan, Thomas Lukasiewicz, Ernest Davis, Gary
Marcus, and Leora Morgenstern. A review of winograd
schema challenge datasets and approaches. In arXiv preprint
arXiv:2004.13831, 2020. 2
[43] Ranjay Krishna, Yuke Zhu, Oliver Groth, Justin Johnson,
Kenji Hata, Joshua Kravitz, Stephanie Chen, Yannis Kalantidis, Li-Jia Li, David A Shamma, et al. Visual genome: Connecting language and vision using crowdsourced dense image annotations. In arXiv preprint arXiv:1602.07332, 2016.
4, 5
[44] Hector Levesque, Ernest Davis, and Leora Morgenstern. The
winograd schema challenge. In Conference on the Principles
of Knowledge Representation and Reasoning, 2012. 1, 2
[45] Kunpeng Li, Yulun Zhang, Kai Li, Yuanyuan Li, and Yun
Fu. Visual semantic reasoning for image-text matching. In
ICCV, 2019. 2, 4, 5
[46] Linjie Li, Zhe Gan, and Jingjing Liu. A closer look at the
robustness of vision-and-language pre-trained models. In
arXiv preprint arXiv:2012.08673, 2020. 1
[47] Liunian Harold Li, Mark Yatskar, Da Yin, Cho-Jui Hsieh,
and Kai-Wei Chang. VisualBERT: A Simple and Performant Baseline for Vision and Language. In arXiv preprint
arXiv:1908.03557, 2019. 1, 2, 4, 5

[48] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays,
Pietro Perona, Deva Ramanan, Piotr Dollár, and C Lawrence
Zitnick. Microsoft coco: Common objects in context. In
ECCV, 2014. 2, 5
[49] Tal Linzen, Emmanuel Dupoux, and Yoav Goldberg. Assessing the ability of lstms to learn syntax-sensitive dependencies. In TACL, 2015. 2
[50] Fangyu Liu, Emanuele Bugliarello, Edoardo Maria Ponti,
Siva Reddy, Nigel Collier, and Desmond Elliott. Visually grounded reasoning across languages and cultures. In
EMNLP, 2021. 2, 8
[51] Jiasen Lu, Dhruv Batra, Devi Parikh, and Stefan Lee. ViLBERT: Pretraining Task-Agnostic Visiolinguistic Representations for Vision-and-Language Tasks. In NeurIPS, 2019.
1, 2, 4, 5
[52] Vicente Ordonez, Girish Kulkarni, and Tamara Berg.
Im2text: Describing images using 1 million captioned photographs. In NIPS, 2011. 5
[53] Letitia Parcalabescu, Albert Gatt, Anette Frank, and Iacer
Calixto. Seeing past words: Testing the cross-modal capabilities of pretrained v&l models on counting tasks. In ACL,
2021. 2
[54] Prasanna Parthasarathi, Koustuv Sinha, Joelle Pineau, and
Adina Williams. Sometimes we want ungrammatical translations. In Findings of the Association for Computational
Linguistics: EMNLP, 2021. 2
[55] Jordi Pont-Tuset, Jasper Uijlings, Soravit Changpinyo, Radu
Soricut, and Vittorio Ferrari. Connecting vision and language with localized narratives. In ECCV, 2020. 5
[56] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya
Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,
Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen
Krueger, and Ilya Sutskever. Learning transferable visual
models from natural language supervision. In ICML, 2021.
1, 2, 4, 5
[57] Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and
Percy Liang. Squad: 100,000+ questions for machine comprehension of text. In arXiv preprint arXiv:1606.05250,
2016. 5
[58] Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun.
Faster r-cnn: Towards real-time object detection with region
proposal networks. In NeurIPS, 2015. 4
[59] Rachel Rudinger, Jason Naradowsky, Brian Leonard, and
Benjamin Van Durme. Gender bias in coreference resolution. In arXiv preprint arXiv:1804.09301, 2018. 2
[60] Keisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula,
and Yejin Choi. Winogrande: An adversarial winograd
schema challenge at scale. In AAAI, 2020. 2
[61] Morgan Klaus Scheuerman, Jacob M. Paul, and Jed R.
Brubaker. How computers see gender: An evaluation of gender classification in commercial facial analysis services. In
ACM: Human Computer Interaction, 2019. 8
[62] Rico Sennrich, Barry Haddow, and Alexandra Birch. Neural
machine translation of rare words with subword units. In
arXiv preprint arXiv:1508.07909, 2015. 4
[63] Piyush Sharma, Nan Ding, Sebastian Goodman, and Radu
Soricut. Conceptual captions: A cleaned, hypernymed, im-

age alt-text dataset for automatic image captioning. In ACL,
2018. 2, 5
[64] Ravi Shekhar, Sandro Pezzelle, Yauhen Klimovich, Aurelie Herbelot, Moin Nabi, Enver Sangineto, and Raffaella
Bernardi. ”foil it! find one mismatch between image and
language caption”. In ACL, 2017. 2
[65] Oleksii Sidorov, Ronghang Hu, Marcus Rohrbach, and
Amanpreet Singh. Textcaps: a dataset for image captioning
with reading comprehension. In ECCV, 2020. 2
[66] Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for largescale image recognition. In CVPR,
2015. 5
[67] Amanpreet Singh, Vedanuj Goswami, and Devi Parikh. Are
we pretraining it right? digging deeper into visio-linguistic
pretraining. In arXiv preprint arXiv:2004.08744, 2020. 1
[68] Amanpreet Singh, Ronghang Hu, Vedanuj Goswami, Guillaume Couairon, Wojciech Galuba, Marcus Rohrbach, and
Douwe Kiela. Flava: A foundational language and vision
alignment model. In CVPR, 2022. 2, 4, 5
[69] Amanpreet Singh, Vivek Natarajan, Meet Shah, Yu Jiang,
Xinlei Chen, Dhruv Batra, Devi Parikh, and Marcus
Rohrbach. Towards vqa models that can read. In CVPR,
2019. 2
[70] Koustuv Sinha, Robin Jia, Dieuwke Hupkes, Joelle Pineau,
Adina Williams, and Douwe Kiela. Masked language modeling and the distributional hypothesis: Order word matters
pre-training for little. In EMNLP, 2021. 1, 2
[71] Koustuv Sinha, Prasanna Parthasarathi, Joelle Pineau, and
Adina Williams. UnNatural Language Inference. In ACLIJCNLP, 2021. 2
[72] Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang,
Christopher D. Manning, A. Ng, and Christopher Potts. Recursive deep models for semantic compositionality over a
sentiment treebank. In EMNLP, 2013. 5
[73] Krishna Srinivasan, Karthik Raman, Jiecao Chen, Michael
Bendersky, and Marc Najork. Wit: Wikipedia-based image
text dataset for multimodal multilingual machine learning. In
arXiv preprint arXiv:2103.01913, 2021. 5
[74] Alane Suhr, Mike Lewis, James Yeh, and Yoav Artzi. A corpus of natural language for visual reasoning. In ACL, 2017.
2, 5
[75] Shardul Suryawanshi and Bharathi Raja Chakravarthi. Findings of the shared task on troll meme classification in Tamil.
In Proceedings of the First Workshop on Speech and Language Technologies for Dravidian Languages, 2021. 2
[76] Hao Tan and Mohit Bansal. Lxmert: Learning crossmodality encoder representations from transformers. In
EMNLP-IJCNLP, 2020. 2, 4, 5
[77] Bart Thomee, David A Shamma, Gerald Friedland, Benjamin Elizalde, Karl Ni, Douglas Poland, Damian Borth, and
Li-Jia Li. Yfcc100m: The new data in multimedia research.
In Communications of the ACM, 2016. 5
[78] Tristan Thrush, Ethan Wilcox, and Roger Levy. Investigating
novel verb learning in BERT: Selectional preference classes
and alternation-based syntactic generalization. In Proceedings of the Third BlackboxNLP Workshop on Analyzing and
Interpreting Neural Networks for NLP, 2020. 2

[79] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia
Polosukhin. Attention is all you need. In NeurIPS, 2017. 4
[80] Ramakrishna Vedantam, Arthur Szlam, Maximillian Nickel,
Ari Morcos, and Brenden M Lake. Curi: A benchmark for
productive concept learning under uncertainty. In ICML,
2021. 2
[81] Alex Warstadt, Yu Cao, Ioana Grosu, Wei Peng, Hagen Blix,
Yining Nie, Anna Alsop, Shikha Bordia, Haokun Liu, Alicia
Parrish, Sheng-Fu Wang, Jason Phang, Anhad Mohananey,
Phu Mon Htut, Paloma Jeretic, and Samuel R. Bowman. Investigating BERT’s knowledge of language: Five analysis
methods with NPIs. In EMNLP-IJCNLP, 2019. 2
[82] Alex Warstadt, Alicia Parrish, Haokun Liu, Anhad Mohananey, Wei Peng, Sheng-Fu Wang, and Samuel R. Bowman. BLiMP: The benchmark of linguistic minimal pairs for
English. In TACL, 2020. 2
[83] Adina Williams, Andrew Drozdov, and Samuel R. Bowman.
Do latent tree learning models identify meaningful structure
in sentences? In TACL, 2018. 2
[84] Adina Williams, Nikita Nangia, and Samuel R Bowman. A
broad-coverage challenge corpus for sentence understanding through inference. In arXiv preprint arXiv:1704.05426,
2017. 5
[85] Terry Winograd. Understanding natural language. In Cognitive psychology, 1972. 2
[86] Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim
Rault, Rémi Louf, Morgan Funtowicz, Joe Davison, Sam
Shleifer, Patrick von Platen, Clara Ma, Yacine Jernite, Julien
Plu, Canwen Xu, Teven Le Scao, Sylvain Gugger, Mariama
Drame, Quentin Lhoest, and Alexander M. Rush. Transformers: State-of-the-art natural language processing. In
EMNLP: System Demonstrations, 2020. 6
[87] Ning Xie, Farley Lai, Derek Doran, and Asim Kadav. Visual
entailment task for visually-grounded language learning. In
arXiv preprint arXiv:1811.10582, 2018. 5
[88] Peter Young, Alice Lai, Micah Hodosh, and Julia Hockenmaier. From image descriptions to visual denotations: New
similarity metrics for semantic inference over event descriptions. In TACL, 2014. 5
[89] Rowan Zellers, Yonatan Bisk, Ali Farhadi, and Yejin Choi.
From recognition to cognition: Visual commonsense reasoning. In CVPR, 2019. 5
[90] Pengchuan Zhang, Xiujun Li, Xiaowei Hu, Jianwei Yang,
Lei Zhang, Lijuan Wang, Yejin Choi, and Jianfeng Gao.
Vinvl: Revisiting visual representations in vision-language
models. In CVPR, 2021. 2, 4, 5
[91] Jieyu Zhao, Tianlu Wang, Mark Yatskar, Vicente Ordonez,
and Kai-Wei Chang. Gender bias in coreference resolution: Evaluation and debiasing methods. In arXiv preprint
arXiv:1804.06876, 2018. 2
[92] Haoti Zhong, Hao Li, Anna Cinzia Squicciarini,
Sarah Michele Rajtmajer, Christopher Griffin, David J
Miller, and Cornelia Caragea. Content-driven detection of
cyberbullying on the instagram social network. In IJCAI,
2016. 2

[93] Yuke Zhu, Oliver Groth, Michael Bernstein, and Li FeiFei. Visual7w: Grounded question answering in images. In
CVPR, 2016. 2

A. Confidence Intervals

arXiv:2204.03162v2 [cs.CV] 22 Apr 2022

We provide confidence intervals for the overall model results on Winoground. We divided the dataset into 4 groups of
equal size to get 4 scores for each model and score-type, and used Student’s t-distribution to compute the confidence intervals.
Model

Text

Image

Group

MTurk Human

89.50

[80.83,98.17]

88.50

[79.00,98.00]

85.50

[73.84,97.16]

VinVL
UNITERlarge
UNITERbase
ViLLAlarge
ViLLAbase
VisualBERTbase
ViLT (ViT-B/32)
LXMERT
ViLBERTbase
UniTIT M F inetuned
FLAVAIT M
FLAVAContrastive
CLIP (ViT-B/32)
VSE++COCO (ResNet)
VSE++COCO (VGG)
VSE++F lickr30k (ResNet)
VSE++F lickr30k (VGG)
VSRNCOCO
VSRNF lickr30k

37.75
38.00
32.25
37.00
30.00
15.50
34.75
19.25
23.75
19.50
32.25
25.25
30.75
22.75
18.75
20.00
19.75
17.50
20.00

[28.71,46.79]
[33.32,42.68]
[25.84,38.66]
[31.05,42.95]
[25.32,34.68]
[9.34,21.66]
[29.03,40.47]
[16.53,21.97]
[18.03,29.47]
[14.73,24.27]
[20.04,44.46]
[19.99,30.51]
[25.03,36.47]
[19.22,26.28]
[17.23,20.27]
[12.77,27.23]
[14.49,25.01]
[9.54,25.46]
[13.25,26.75]

17.75
14.00
13.25
13.25
12.00
2.50
14.00
7.00
7.25
6.25
20.50
13.50
10.50
8.00
5.50
5.00
6.25
7.00
5.00

[11.21,24.29]
[6.77,21.23]
[7.68,18.82]
[7.83,18.67]
[8.33,15.67]
[0.00,6.29]
[8.49,19.51]
[3.10,10.90]
[3.97,10.53]
[0.53,11.97]
[14.34,26.66]
[8.55,18.45]
[6.29,14.71]
[6.70,9.30]
[3.45,7.55]
[0.89,9.11]
[2.27,10.23]
[1.19,12.81]
[2.09,7.91]

14.50
10.50
10.00
11.00
8.00
1.50
9.25
4.00
4.75
4.00
14.25
9.00
8.00
4.00
3.50
2.75
4.50
3.75
3.50

[6.65,22.35]
[8.45,12.55]
[7.75,12.25]
[7.10,14.90]
[5.75,10.25]
[0.00,3.55]
[6.53,11.97]
[2.70,5.30]
[1.47,8.03]
[2.70,5.30]
[8.53,19.97]
[5.10,12.90]
[4.56,11.44]
[1.40,6.60]
[2.58,4.42]
[0.75,4.75]
[2.91,6.09]
[0.00,8.50]
[2.58,4.42]

Table 1. 95% confidence intervals for the aggregate results on Winoground. Results above chance are shown in bold.

B. Impact of Pretraining Data Size and Model Type on Model Performance

30
random chance (text, image)

20

random chance (group)

10

0

Group Score, Image Score, Text Score

Group Score, Image Score, Text Score

Correlations between pretraining data size and model performance are highly significant in every case and the numbers
are shown in the main paper. We show plots in the figures below. Most of the single-stream models perform slightly above
chance on the text score. CLIP and FLAVA are the only dual-stream models which perform above chance, and they have
drastically more training data than all other models.

30
random chance (text, image)

20

random chance (group)

10

1
2
3
4
# Pretraining Images (Millions)

0

2
4
6
8
# Pretraining Captions (Millions)

10

Figure 1. Graphs of the model performance on Winoground for each model by the number of pretraining images (left) and pretraining
captions (right). ◇ = dual-stream RNNs, ◻ = dual-stream transformers, ◯ = single-stream transformers. CLIP and FLAVA are removed
as outliers. Backbone pretraining data is not included.
VinVLbase

UNITERlarge
VILLAlarge
UNITERbase
VILLAbase

30 VSE++COCO

ViLT

VSRNF 30k

ViLBERT

random chance

LXMERT

20

UniT
VisualBERT

10

VSE++COCO
UniT
VSRN++F 30k

VinVLbase

ViLT

UNITERlarge

UNITERbase /VILLAlarge
VILLAbase
LXMERT
ViLBERT

Image Score, Text Score

Image Score, Text Score

VinVLbase

30

VSE++COCO

random chance
ViLBERT
VSRNF 30k
UniT

20

VisualBERT

1
2
3
4
# Pretraining Images (Millions)

VinVLbase

10

UniT

ViLBERT
VSRNF 30k VisualBERT

5

0

LXMERT
UNITERlarge ViLT

UNITERbase /VILLAlarge

VSE++COCO

VisualBERT

0

UNITERlarge
VILLAlarge
ViLT
UNITERbase
VILLAbase

VILLAbase
LXMERT

2
4
6
8
# Pretraining Captions (Millions)

10

Figure 2. Graphs of the model performance on Winoground for each model by the number of pretraining images (left) and pretraining
captions (right). This is a finer-grained version of Tab. 1, with model names instead of grouping by architecture; we again exclude CLIP
and FLAVA as their pretraining dataset sizes are outliers. We only show the best VSE++ and VSRN configurations and do not show group
scores due to clutter issues.

C. Linguistic Tag Breakdown
This section reports every different swap-dependent linguistic tag that our annotators gave examples. Many of these
fine-grained linguistic tags are used for multiple examples, although some tags are only used once in the dataset.
Tag

Object

Relation

Both

Fine-Grained Tag

Example

Noun Phrase, Determiner-Numeral
Noun Phrase
Determiner-Numeral, Noun Phrase
Noun Phrase, Determiner-Possessive
Noun Phrase, Adjective-Color
Pronoun, Noun Phrase
Determiner-Numeral Phrase
Pronoun, Verb-Intransitive
Noun

[a person] carrying [more than one flotation device]
[a person] holding up [books]
[a lightbulb] surrounding [some plants]
[a deer’s nose] is resting on [a child’s hand]
aerial view of a green tree in [the brown freshly turned soil] next to [a green field]
[the person] wears a hat but [it] doesn’t
[one] is in a boat and [almost everyone] is swimming
[it] ran away while [they] pursued
more [bicycles] than [cars]

Adjective-Age
Scope, Preposition
Verb-Intransitive, Verb-Transitive Phrase
Verb-Intransitive, Adjective-Manner
Negation, Noun Phrase, Preposition Phrase
Scope, Preposition, Verb-Intransitive
Noun Phrase, Adjective-Animate
Adjective-Size
Determiner-Possessive
Adjective-Texture
Adjective-Color
Scope
Preposition Phrase
Relative Clause, Scope
Adjective-Height
Verb-Intransitive Phrase, Preposition
Verb-Intransitive, Determiner-Numeral
Determiner-Numeral
Adjective-Weight
Verb-Intransitive, Noun
Verb-Intransitive Phrase, Adverb-Animate
Scope, Relative Clause
Adjective-Speed
Adverb-Temporal
Adverb-Spatial
Adjective-Shape
Noun, Adjective-Color
Verb-Transitive
Scope, Verb-Transitive
Scope, Preposition Phrase
Adjective-Temperature
Adjective-Temporal
Scope, Conjunction
Scope, Conjunction Phrase
Preposition Phrase, Scope
Adjective-Manner Phrase
Verb-Intransitive
Adjective-Animate
Adverb-Spatial Phrase
Scope, Adjective-Texture
Adverb-Animate
Adjective-Manner
Verb-Transitive Phrase
Adjective-Color (3-way swap)
Scope, Adjective-Manner
Preposition
Verb-Intransitive Phrase
Sentence
Adjective-Speed Phrase, Verb-Intransitive
Adjective-Spatial
Negation, Scope
Verb-Transitive Phrase, Verb-Intransitive, Preposition Phrase

[an older] person blocking [a younger] person
racing [over] it []
a kid [threw a basketball] then [jumped]
the younger person is [making noise] while the other is [silent]
a person [with long braids] is exercising in front of a person [without braids]
[out]1[swam]2 the person in the red swimcap []2[]1
the one on the left is [sad] and the other is [happy]
the [taller] person hugs the [shorter] person
the [person’s] leg is on the [dog’s] torso
[smooth] shoes are on a [soft] floor
painting the [white] wall [red]
[getting] a horse [] wet
flat [at the bottom] and pointy [on top]
the person [who is wearing a crown] is kissing a frog []
a [taller] person wearing blue standing next to a [shorter] person
the gesture of the person [sitting down] is supporting the understanding of the person [standing up]
some people are [standing] but more are [sitting]
[one]1 person[]2 wearing [two]1 scarf[s]2
the larger ball is [lighter] and the smaller one is [heavier]
the dog is [standing] and the person is [swimming]
the person on the left is [crying sadly] while the one on the right is [smiling happily]
a fencer [who is wearing black pants] having a point scored against them by another fencer [] using a wheelchair
the train is [still] while the person is [moving fast]
a person is drinking [now] and eating [later]
the car is sitting [upside down] while the person is standing [rightside up]
the [round] table has a [square] base
Young person playing baseball with a [blue] bat and [green] ball
the person with the ponytail [buys] stuff and other [packs] it
[] gears for [moving] something
[] child in [front facing] row of yellow rubber ducks
a [hot] drink on a [cold] day
the [first] vowel is E and the [last] consonant is N
a person spraying water on [someone else]1 [and]2 a person on a bike []2 []1
A child [] riding a bike [and an adult]
someone [with an apple] is hurt by a tree []
two people wearing clothes of [different] colors are on [the same] side of the tennis net
a person [stands] and a dog [sits]
[toy] cat with [real] baby
the sailboat sails [close] but the beach is [far away]
A [] small animal with [curled] hair
someone talks on the phone [angrily] while another person sits [happily]
[poor] [unfortunate] people
they [drank water] then they [worked out]
The [red]→[yellow] book is above the [yellow]→[blue] book and below the [blue]→[red] book
[] living things [drinking]
seat numbers increasing from [right] to [left]
a cat is [stretching] and a person is [lying down]
[the coffee is poured] before [it is ground]
the person with green legs is running [quite slowly] and the red legged one runs [faster]
A [left] hand pulls a glove onto a [right] hand
The [un]caged bird has an []opened cage door
the dog [bite]1s []2 what someone would normally [wear]1 [as a hat]2

Altered POS
Verb-Transitive, Noun
Scope, Altered POS, Verb-Intransitive, Verb-Transitive
Noun, Adjective-Size
Noun, Verb-Intransitive
Scope, Noun, Preposition
Noun, Preposition Phrase, Scope
Scope, Preposition Phrase, Adjective-Color
Altered POS, Determiner-Numeral

[watch]ing the [present]
someone []1 on [the ground]2 [is]1 spraying water towards [a vehicle]2
[walking]1 someone []1 [cut]2 [lines]2 into green plants
the [person]1 is too [big]2 for the [small]2 [door]1
a [dog sitting] on a couch with a [person lying] on the floor
[]1 a person [near]1 [water]2 using a []2 lasso
a person wearing a [bear]1 mask []2 in blue on the left hand side of a person wearing a [panda]1 mask [with glasses]2 in pink
[darker]1 things []2 become [light]1 [in stripes]2
[one] ear that some [donkey] is whispering a secret into

Table 2. Examples showcasing the full linguistic (swap-dependent) tag breakdown.

D. Heatmaps for the Word-Region Alignment Models
We provide heatmaps for models that were trained with a word-region alignment objective: UNITER, ViLLA and ViLT.
See the main text for ViLT heatmaps.
a brown dog is on a white couch

a white dog

is on a

brown couch

circular food on heart-shaped wood

heart-shaped food on circular wood

Figure 3. Word-region alignment scores between the image and text features for ViLLAbase on examples from Winoground.

a brown dog is on a white couch

a white dog

is on a

brown couch

circular food on heart-shaped wood

heart-shaped food on circular wood

Figure 4. Word-region alignment scores between the image and text features for UNITERbase on examples from Winoground.

E. Mechanical Turk Interface
In order to participate, crowdworkers needed to satisfy several criteria: be an English speaker, have 98% previous HIT
approval, have completed 1000 previous HITs, and pass the onboarding test. The onboarding test used the same interface as
the actual task. It consisted of ten image-caption match questions, with images and captions that are independent from the
actual Winoground dataset. If they made one mistake, a pop-up asked them if they were sure, and they would be allowed to
select whether there was a match or not again. If they made any additional mistakes during onboarding, they were disqualified.

Figure 5. The Amazon Mechanical Turk validation interface.

F. Ethical Considerations
A key consideration while designing Winoground centered on how the expert annotators would describe the people contained in the images. We avoided using gendered terms (e.g. using “person” in place of “woman” or “man”) in our captions
and did not include any swaps between pairs of captions based on gender, race or ethnicity (e.g. “[the man] hands a water to
[the woman]”). We recognize that, barring direct access to the people in the images, we would be merely making a guess at
a person’s identity based on our own cultural norms and experiences.
In addition, we encouraged the expert annotators to find images that represent a variety of people across the dimensions of
perceived race, gender, disability, etc.. We gathered the Getty Images metadata (title and short alt text-like description) and
searched them for specific words as a rough proxy for gender representation. The relevant words are either words referring
to women (e.g. girl, her), words referring to men (e.g. boy, him) or words that are gender-neutral (e.g. them, themself).
Using the Getty Images metadata corresponding to the 800 images in Winoground, 371 images have corresponding metadata
that contained at least one word from the lists we created. Using this metadata for these 371 images, we estimate that 152
images only contain women, 123 images only contain men, 22 images only contain people without gender descriptors, and
the remaining 74 images contain people described by multiple genders. This serves only as a rough estimate as much of the
metadata contain words referring to people that are inherently non-gendered (e.g. scuba diver, friend, etc.) and because the
relevant gendered words we found are themselves subject to the assumptions of those who wrote the titles and captions.

