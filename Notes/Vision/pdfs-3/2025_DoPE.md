## 1. Basic Metadata
- Title: DoPE: Denoising Rotary Position Embedding
- Authors: Jing Xiong; Liyang Fan; Hui Shen; Zunhai Su; Min Yang; Lingpeng Kong; Ngai Wong
- Year: 2025
- Venue: arXiv (arXiv:2511.09146v1 [cs.CL], 12 Nov 2025)

## 2. One-Sentence Contribution Summary
The paper proposes a training-free denoising positional encoding method (DoPE) based on truncated matrix entropy and Gaussian reparameterization to improve RoPE length extrapolation and mitigate attention sinks.

## 3. Tasks Evaluated
Task: Needle-in-a-haystack (NIH) synthesis task
- Task type: Other (information retrieval / synthesis)
- Dataset(s): Not specified in the paper.
- Domain: Natural language processing / information retrieval.
- Evidence: 'The "needle-in-a-haystack" synthesis task presents a particularly challenging problem in the field of natural language processing and information retrieval.' (Section 5.1 Experimental Setup)
- Evidence: "Experiments on needle-in-a-haystack and many-shot in-context learning tasks demonstrate that D O PE significantly improves retrieval accuracy and reasoning stability across extended contexts (up to 64K tokens)." (Abstract)

Task: Many-Shot In-Context Learning (MICL)
- Task type: Other (in-context learning)
- Dataset(s): nlile/hendrycks-MATH-benchmark dataset; MATH problems (100 sampled).
- Domain: MATH problems.
- Evidence: "Table 2: Summary of experimental configurations and results for denoising strategies on Qwen2.5-Math-7B extrapolation in the Many-Shot In-Context Learning task." (Table 2 caption)
- Evidence: "Experiments utilize In-Context Learning (ICL) constructed from the nlile/hendrycks-MATH-benchmark dataset." (Table 2 caption)
- Evidence: "Results are accuracy scores on 100 sampled MATH problems (400 total configurations across 4 insertion positions)." (Table 2 caption)

## 4. Domain and Modality Scope
- Single domain vs multiple: The NIH task is explicitly framed as NLP/information retrieval, and MICL uses MATH problems; the paper does not state evaluation across multiple domains or modalities. Evidence: 'The "needle-in-a-haystack" synthesis task presents a particularly challenging problem in the field of natural language processing and information retrieval.' (Section 5.1 Experimental Setup) and "Results are accuracy scores on 100 sampled MATH problems (400 total configurations across 4 insertion positions)." (Table 2 caption)
- Multiple domains within the same modality: Not specified in the paper.
- Multiple modalities: Not specified in the paper.
- Domain generalization or cross-domain transfer: Not claimed; the only related statement is cross-task transferability: "head selection using entropy scores computed on both the MATH dataset and the NIH dataset to evaluate cross-task transferability (Table 4)." (Section 5.1 Experimental Setup)

## 5. Model Sharing Across Tasks
| Task | Shared Weights? | Fine-Tuned? | Separate Head? | Evidence |
| --- | --- | --- | --- | --- |
| Needle-in-a-haystack (NIH) | No (different base model) | No (training-free) | Not specified in the paper. | "For the needle-in-a-haystack (NIH) task on LLaMA-3-8B-Instruct, we set max_new_tokens to" (Appendix A.2 Experimental Setup); "a training-free method" (Abstract) |
| Many-Shot In-Context Learning (MICL) | No (different base model) | No (training-free) | Not specified in the paper. | "Table 2: Summary of experimental configurations and results for denoising strategies on Qwen2.5-Math-7B extrapolation in the Many-Shot In-Context Learning task." (Table 2 caption); "a training-free method" (Abstract) |

## 6. Input and Representation Constraints
- Fixed or variable input resolution: Not specified in the paper.
- Fixed patch size: Not specified in the paper.
- Fixed number of tokens / context length: "context length of 32K tokens, while LLaMA-3-8B is trained with a 8K-token context window." (Appendix A.2 Experimental Setup) and "extended contexts (up to 64K tokens)." (Abstract)
- Fixed dimensionality (e.g., strictly 2D): Not specified in the paper.
- Padding or resizing requirements: Not specified in the paper.

## 7. Context Window and Attention Structure
- Maximum sequence length: "extended contexts (up to 64K tokens)." (Abstract)
- Fixed or variable length: "Results are reported for two extrapolation lengths: 24,756 (24k) and 65,536 (64k)." (Table 1 caption)
- Attention type: Causal decoder-only self-attention. Evidence: "We consider a causal language model implemented as a decoder-only Transformer." (Section 2.1 Multi-Head Self-Attention)
- Mechanisms to manage computational cost: Implementation uses efficient attention/inference tooling rather than architectural windowing; "All experiments are conducted using SGLang (Zheng et al., 2023) (v0.5.3rc0) with the FlashAttention-3 backend (Shah et al., 2024). Tensor parallelism is enabled for multi-GPU inference when necessary. CUDA graphs are disabled to support dynamic context lengths." (Section 5.1 Experimental Setup)
- Windowed/hierarchical/sparse attention or token pruning: Not specified in the paper.

## 8. Positional Encoding (Critical Section)
- Mechanism: RoPE. Evidence: "Most LLMs adopt Rotary Position Embedding (RoPE) (Su et al., 2024) as their default positional encoding mechanism, which has become the de facto standard in contemporary architectures." (Section 2.2 Rotary Position Embedding)
- Where applied: "RoPE encodes token positions by rotating each query/key vector on a sequence of two-dimensional planes." (Section 2.2 Rotary Position Embedding)
- Modified/compared in experiments: The paper introduces DoPE variants and uses RoPE extrapolation scaling. Evidence: "We reinterpret the attention map with positional encoding as a noisy feature map, and propose Denoising Positional Encoding (D O PE), a training-free method based on truncated matrix entropy to detect outlier frequency bands in the feature map." (Abstract); "DoPE-by-all. In this variant, denoising is performed by applying the head-level mask to the entire positional encoding of each head, rather than completely zeroing out the head." (Section 4.3 Denoising via Truncated Matrix Entropy); "DoPE-by-Gaussian. In this variant, denoising is performed by applying the head-level mask to the positional encoding and replacing the removed parts with Gaussian noise." (Section 4.3 Denoising via Truncated Matrix Entropy); "To support longer contexts beyond their pre-training limits, we apply RoPE-based extrapolation (e.g., Dynamic-NTK), which rescales RoPE frequencies to improve stability and retrieval performance in extended-context settings." (Appendix A.2 Experimental Setup)
- Fixed across experiments vs modified: Modified; multiple denoising variants and RoPE extrapolation scaling are evaluated (see evidence above).

## 9. Positional Encoding as a Variable
- Core research variable vs fixed assumption: Core research variable. Evidence: "We reinterpret the attention map with positional encoding as a noisy feature map, and propose Denoising Positional Encoding (D O PE), a training-free method based on truncated matrix entropy to detect outlier frequency bands in the feature map." (Abstract)
- Multiple positional encodings compared: The paper compares multiple PE modifications (DoPE variants and Dynamic-NTK scaling). Evidence: "DoPE-by-all. In this variant, denoising is performed by applying the head-level mask to the entire positional encoding of each head, rather than completely zeroing out the head." (Section 4.3 Denoising via Truncated Matrix Entropy); "DoPE-by-Gaussian. In this variant, denoising is performed by applying the head-level mask to the positional encoding and replacing the removed parts with Gaussian noise." (Section 4.3 Denoising via Truncated Matrix Entropy); "To support longer contexts beyond their pre-training limits, we apply RoPE-based extrapolation (e.g., Dynamic-NTK), which rescales RoPE frequencies to improve stability and retrieval performance in extended-context settings." (Appendix A.2 Experimental Setup)
- Claim that PE choice is not critical or secondary: Not specified in the paper.

## 10. Evidence of Constraint Masking
- Model size(s): "Models. Qwen-1.5-7B (Li, 2023), Qwen2.5-Math-7B (Yang et al., 2024) and LLaMA-3-8B-Instruct (Grattafiori et al., 2024) are decoder-only transformer models that employ Rotary Positional Embeddings (RoPE) for encoding positional information." (Appendix A.2 Experimental Setup)
- Dataset size(s): "Results are accuracy scores on 100 sampled MATH problems (400 total configurations across 4 insertion positions)." (Table 2 caption)
- Attribution of gains: Improvements are attributed to DoPE/denoising, not scaling. Evidence: "Experiments on needle-in-a-haystack and many-shot in-context learning tasks demonstrate that D O PE significantly improves retrieval accuracy and reasoning stability across extended contexts (up to 64K tokens)." (Abstract)
- Claims about scaling model size, data size, architectural hierarchy, or training tricks: Not specified in the paper.

## 11. Architectural Workarounds
- Head selection via truncated matrix entropy: "Head Selection. To avoid uniformly modifying all attention heads, we perform selection at the head level based on the truncated matrix entropy." (Section 4.3 Denoising via Truncated Matrix Entropy)
- Head-level masking of positional encoding (DoPE-by-all): "DoPE-by-all. In this variant, denoising is performed by applying the head-level mask to the entire positional encoding of each head, rather than completely zeroing out the head." (Section 4.3 Denoising via Truncated Matrix Entropy)
- Gaussian replacement for masked positional encoding (DoPE-by-Gaussian): "DoPE-by-Gaussian. In this variant, denoising is performed by applying the head-level mask to the positional encoding and replacing the removed parts with Gaussian noise." (Section 4.3 Denoising via Truncated Matrix Entropy)
- RoPE extrapolation scaling: "To support longer contexts beyond their pre-training limits, we apply RoPE-based extrapolation (e.g., Dynamic-NTK), which rescales RoPE frequencies to improve stability and retrieval performance in extended-context settings." (Appendix A.2 Experimental Setup)
- Stated purpose of denoising: "The results show that the denoising strategy for positional embeddings effectively mitigates attention sinks and restores balanced attention patterns, providing a simple yet powerful solution for improving length generalization." (Abstract)

## 12. Explicit Limitations and Non-Claims
- Not specified in the paper.
