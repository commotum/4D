Number of distinct tasks evaluated: 3 (copy memory, Penn Treebank word-level language modeling, WikiText-103 word-level language modeling). (2019_DEQ.pdf p16)

Number of trained model instances required to cover all tasks: 3. The paper evaluates separate models per task/dataset (copy memory uses DEQ-Transformer; PTB uses DEQ-TrellisNet; WT103 uses DEQ-TrellisNet/DEQ-Transformer), with no single jointly trained model across all tasks. (2019_DEQ.pdf p7, p8, p16)

$$
\boxed{
\frac{3\ \text{tasks}}{3\ \text{models}} = 1
}
$$
