### 1. Basic Metadata
- Title: VG4D: Vision-Language Model Goes 4D Video Recognition.
  Evidence: "VG4D: Vision-Language Model Goes 4D Video Recognition" (Title)
- Authors: Zhichao Deng; Xiangtai Li; Xia Li; Yunhai Tong; Shen Zhao; Mengyuan Liu.
- Year: 2024.
  Evidence: "arXiv:2404.11605v1 [cs.CV] 17 Apr 2024" (Title page)
- Venue: arXiv.
  Evidence: "arXiv:2404.11605v1 [cs.CV] 17 Apr 2024" (Title page)

### 2. One-Sentence Contribution Summary
The paper proposes VG4D to transfer vision-language model knowledge to 4D point cloud action recognition to improve recognition performance.
Evidence: "we propose the Vision-Language Models Goes 4D (VG4D) framework to transfer VLM knowledge from visual-text pretrained models to a 4D point cloud network." (Abstract)

### 3. Tasks Evaluated
Task 1: 4D point cloud action recognition (classification)
- Task type: Classification.
- Dataset(s): NTU RGB+D 60; NTU RGB+D 120.
- Domain: 4D point cloud videos derived from depth maps of RGB-D action videos.
- Evidence:
  - "Experiments demonstrate that our method achieves state-ofthe-art performance for action recognition on both the NTU RGB+D 60 dataset and the NTU RGB+D 120 dataset." (Abstract)
  - "Dataset. NTU RGB+D [56] is a large-scale benchmark dataset for action recognition, which contains 56,880 videos collected from 40 subjects performing 60 different actions in 80 camera views." (IV. Experiments)
  - "The videos are captured using Kinect V2 to collect four data modalities: RGB frames, depth maps, 3D joint information, and IR sequences." (IV. Experiments)
  - "we follow PSTNet to convert depth maps to point cloud sequences, in which we sample 2048 points in each frame." (IV. Experiments)
  - "understanding point cloud videos in 4D, encompassing three spatial dimensions and one temporal dimension." (I. Introduction)

Task 2: Multi-modal action recognition (RGB + point cloud + text) (classification)
- Task type: Classification.
- Dataset(s): NTU RGB+D 60; NTU RGB+D 120.
- Domain: Multi-modal action recognition using RGB video, 4D point cloud, and text labels.
- Evidence:
  - "we synergize the exceptional capabilities of Vision-Language Models (VLMs) in video understanding with 4D point cloud representation to enhance multi-modal action recognition." (I. Introduction)
  - "we achieve robust multi-modal action recognition by integrating multi-modal prediction scores and utilizing text information as classifiers." (I. Introduction)
  - "We use languageRGB-4D point cloud triplets to train the framework." (III. Method)
  - "Dataset. NTU RGB+D [56] is a large-scale benchmark dataset for action recognition, which contains 56,880 videos collected from 40 subjects performing 60 different actions in 80 camera views." (IV. Experiments)

### 4. Domain and Modality Scope
- Single domain vs multiple domains: Evaluation is within a single domain of RGB-D human action recognition datasets (NTU RGB+D 60/120). Evidence: "Dataset. NTU RGB+D [56] is a large-scale benchmark dataset for action recognition..." (IV. Experiments)
- Multiple domains within the same modality: Not specified in the paper.
- Multiple modalities: Yes. Evidence: "We use languageRGB-4D point cloud triplets to train the framework." (III. Method); "The videos are captured using Kinect V2 to collect four data modalities: RGB frames, depth maps, 3D joint information, and IR sequences." (IV. Experiments)
- Domain generalization or cross-domain transfer claims: Not specified in the paper.

### 5. Model Sharing Across Tasks
| Task | Shared Weights? | Fine-Tuned? | Separate Head? | Evidence |
| --- | --- | --- | --- | --- |
| 4D point cloud action recognition | Not specified in the paper | Yes; im-PSTNet is fine-tuned after pre-training | Yes | "During cross-modal learning, We use Lcl as supervision to fine-tune the im-PSTNet model that has been pre-trained to learn 4D representations." (III. Method); "Our VG4D also includes two classification heads to classify the 4D features and RGB video features extracted by im-PSTNet and Video encoder, respectively." (III. Method) |
| Multi-modal action recognition | Not specified in the paper | Yes; video encoder is fine-tuned; VLM frozen during cross-modal learning | Yes | "consists of 3 networks: 4D point cloud encoder EP , video encoder EV and text encoder ET from VLM." (III. Method); "We use the pre-trained X-CLIP-B/16 model on Kinetics600 [58] to fine-tune for 30 epochs on the NTU RGB+D dataset." (IV. Experiments); "Note that the VLM is frozen at this stage." (III. Method); "Our VG4D also includes two classification heads to classify the 4D features and RGB video features extracted by im-PSTNet and Video encoder, respectively." (III. Method) |

### 6. Input and Representation Constraints
- Fixed number of points per frame: "we follow PSTNet to convert depth maps to point cloud sequences, in which we sample 2048 points in each frame." (IV. Experiments)
- Clip length and frame sampling stride: "we set the clip length and frame sampling stride to 23 and 2, respectively." (IV. Experiments)
- RGB input frames: "For the RGB modality, we set the number of input frames to 8, using the same frame sampling method in the point cloud video." (IV. Experiments)
- Neighbor and radius settings: "The number of neighboring points K and the spatial search radius r at the grouping module are set as 9 and 0.1, respectively." (IV. Experiments)
- Fixed/variable input resolution: Not specified in the paper.
- Fixed patch size: Not specified in the paper.
- Fixed number of tokens: Not specified in the paper.
- Fixed dimensionality: "understanding point cloud videos in 4D, encompassing three spatial dimensions and one temporal dimension." (I. Introduction)
- Padding/resizing requirements: Not specified in the paper.

### 7. Context Window and Attention Structure
- Maximum sequence length / context window:
  - Point cloud clip length: "we set the clip length and frame sampling stride to 23 and 2, respectively." (IV. Experiments)
  - RGB frames per clip: "For the RGB modality, we set the number of input frames to 8..." (IV. Experiments)
- Fixed or variable: The reported setup uses fixed clip length and frame counts: "we set the clip length and frame sampling stride to 23 and 2, respectively." and "For the RGB modality, we set the number of input frames to 8..." (IV. Experiments)
- Attention type:
  - Video encoder: "X-CLIP builds upon CLIP by incorporating cross-frame attention mechanisms and video-specific hinting techniques." (III. Method)
  - 4D point cloud encoder: Not specified in the paper.
- Mechanisms to manage computational cost:
  - Frame sampling: "we employ a data augmentation strategy for frame sampling, significantly reducing both training and testing durations." (III. Method)
  - Point subsampling: "In the point sampling layer, given a spatial subsampling rate Ss , the iterative farthest point sampling(FPS) method is used to subsample" (III. Method)

### 8. Positional Encoding (Critical Section)
- Positional encoding mechanism used: Not specified in the paper.
- Where it is applied: Not specified in the paper.
- Fixed/modified/ablated across experiments: Not specified in the paper.

### 9. Positional Encoding as a Variable
- Treated as a core research variable or fixed assumption: Not specified in the paper.
- Multiple positional encodings compared: Not specified in the paper.
- Claims that PE choice is not critical or secondary: Not specified in the paper.

### 10. Evidence of Constraint Masking (Scale vs Structure)
- Model size(s): Not specified in the paper.
- Dataset size(s): "NTU RGB+D [56]... contains 56,880 videos..." and "NTU RGB+D 120 [57] is an extension of NTU60, with 120 action classes and 114,480 videos." (IV. Experiments)
- Performance gains attributed to training strategies vs architecture:
  - Training strategy: "We observe that a significant portion of the performance improvement achieved by the state-of-the-art approach compared to PSTNet can be attributed to the enhanced training strategy employed." (I. Introduction)
  - Training/architecture modernization focus: "Our exploration mainly focuses on training strategy modernization and network architecture modernization." (III. Method)
- Scaling model size or data claims: Not specified in the paper.

### 11. Architectural Workarounds
- Frame sampling to reduce cost: "we employ a data augmentation strategy for frame sampling, significantly reducing both training and testing durations." (III. Method)
- Point subsampling and local grouping: "point sampling, grouping, MLP layers, and max-pooling" and "In the point sampling layer, given a spatial subsampling rate Ss , the iterative farthest point sampling(FPS) method is used to subsample" (III. Method)
- Spatio-temporal cross-grouping: "im-PSTConv will group spatiotemporal points by building point pipes. It searches for spatio-temporal neighbors across frames, so this module is called a cross-grouping module." (III. Method)
- Score fusion at test time: "we fuse four 4D-text, RGB-text, 4D, and RGB scores as the final classification result." (III. Method)
- Modality-specific classification heads: "Our VG4D also includes two classification heads to classify the 4D features and RGB video features extracted by im-PSTNet and Video encoder, respectively." (III. Method)

### 12. Explicit Limitations and Non-Claims
Not specified in the paper.

### 13. Constraint Profile (Synthesis)
- Domain scope: Single domain (NTU RGB+D action recognition) with multiple modalities (RGB, depth/point cloud, text).
- Task structure: Classification-only action recognition on NTU RGB+D 60/120.
- Representation rigidity: Fixed sampling choices (2048 points per frame; clip length 23; 8 RGB frames; fixed K and r in grouping).
- Model sharing vs specialization: Separate encoders for point cloud/video/text and separate classification heads; VLM frozen during cross-modal learning while im-PSTNet is fine-tuned.
- Role of positional encoding: Not specified in the paper.

### 14. Final Classification
Single-task, single-domain.
Justification: The paper evaluates action recognition on NTU RGB+D 60/120, which is a single domain of RGB-D human action datasets, and does not present multiple distinct tasks. It uses multiple modalities (RGB, point cloud, text) within the same task, but the evaluation remains action recognition on the same dataset family.
