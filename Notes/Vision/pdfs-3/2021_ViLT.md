# ViLT (2021) Survey Responses

## 1. Basic Metadata
- Title: ViLT: Vision-and-Language Transformer Without Convolution or Region Supervision
  > ViLT: Vision-and-Language Transformer
  > Without Convolution or Region Supervision
  > (Front matter, title block)
- Authors: Wonjae Kim; Bokyung Son; Ildoo Kim
  > Wonjae Kim * 1 † Bokyung Son * 1 Ildoo Kim 2
  > (Front matter, author block)
- Year: 2021
  > arXiv:2102.03334v2 [stat.ML] 10 Jun 2021
  > (Front matter)
- Venue: ICML (PMLR 139)
  > Proceedings of the 38 th International Conference on Machine
  > Learning, PMLR 139, 2021. Copyright 2021 by the author(s).
  > (Front matter)

## 2. One-Sentence Contribution Summary
ViLT proposes a minimal vision-and-language pre-training model that removes convolutional or region-supervised visual embedders to improve efficiency while maintaining competitive downstream performance.

## 3. Tasks Evaluated

### Task: Visual Question Answering (VQAv2)
- Task type: Classification
- Dataset(s): VQAv2
- Domain: Images paired with natural language questions; exact image domain not specified in the paper.
- Evidence:
  > Visual Question Answering. The VQAv2 task asks for
  > answers given pairs of an image and a question in natural
  > language. The annotated answers are originally in free-form
  > natural language, but it is a common practice to convert the
  > task to a classification task with 3,129 answer classes.
  > (Section 4.3, Visual Question Answering)

### Task: Natural Language for Visual Reasoning (NLVR2)
- Task type: Classification
- Dataset(s): NLVR2
- Domain: Two images paired with a natural language question; exact image domain not specified in the paper.
- Evidence:
  > Natural Language for Visual Reasoning. The NLVR2
  > task is a binary classification task given triplets of two images and a question in natural language.
  > (Section 4.3, Natural Language for Visual Reasoning)

### Task: Image-to-Text Retrieval
- Task type: Other (retrieval)
- Dataset(s): MSCOCO, Flickr30K (F30K)
- Domain: Images with text; exact image domain not specified in the paper.
- Evidence:
  > We fine-tune ViLT-B/32 on the Karpathy & Fei-Fei (2015)
  > split of MSCOCO and F30K. For image-to-text and text-toimage retrieval, we measure both zero-shot and fine-tuned
  > performance8 .
  > (Section 4.4, Retrieval Tasks)

### Task: Text-to-Image Retrieval
- Task type: Other (retrieval)
- Dataset(s): MSCOCO, Flickr30K (F30K)
- Domain: Images with text; exact image domain not specified in the paper.
- Evidence:
  > We fine-tune ViLT-B/32 on the Karpathy & Fei-Fei (2015)
  > split of MSCOCO and F30K. For image-to-text and text-toimage retrieval, we measure both zero-shot and fine-tuned
  > performance8 .
  > (Section 4.4, Retrieval Tasks)

### Task coverage statement
- Evidence that these are the evaluated downstream tasks:
  > We evaluate ViLT on two widely explored types of visionand-language downstream tasks: for classification, we use
  > VQAv2 (Goyal et al., 2017) and NLVR2 (Suhr et al., 2018),
  > and for retrieval, we use MSCOCO and Flickr30K (F30K)
  > (Plummer et al., 2015) re-splited by Karpathy & Fei-Fei
  > (2015).
  > (Section 4.1, Overview)

## 4. Domain and Modality Scope
- Modalities used in evaluation: Vision + language (multimodal)
  > fine-tuned on vision-and-language downstream tasks where
  > the inputs involve two modalities.
  > (Section 1, Introduction)
- Domain scope: Multiple datasets within the same modality (vision-and-language); the paper does not specify that the image domain changes across datasets.
  > We evaluate ViLT on two widely explored types of visionand-language downstream tasks: for classification, we use
  > VQAv2 (Goyal et al., 2017) and NLVR2 (Suhr et al., 2018),
  > and for retrieval, we use MSCOCO and Flickr30K (F30K)
  > (Plummer et al., 2015) re-splited by Karpathy & Fei-Fei
  > (2015).
  > (Section 4.1, Overview)
- Domain generalization or cross-domain transfer claim: Not claimed.

## 5. Model Sharing Across Tasks
- Evidence for shared pre-training and per-task fine-tuning:
  > We pre-train ViLT-B/32 for 100K or 200K steps on 64
  > NVIDIA V100 GPUs with a batch size of 4,096.
  > (Section 4.2, Implementation Details)
  > We evaluate ViLT-B/32 on two commonly used datasets:
  > VQAv2 and NLVR2. We use a two-layer MLP of hidden
  > size 1,536 as the fine-tuned downstream head.
  > (Section 4.3, Classification Tasks)
  > Following this practice, we fine-tune ViLT-B/32 on the VQAv2
  > train and validation sets while reserving 1,000 validation
  > images and their related questions for internal validation.
  > (Section 4.3, Visual Question Answering)
  > We fine-tune ViLT-B/32 on the Karpathy & Fei-Fei (2015)
  > split of MSCOCO and F30K.
  > (Section 4.4, Retrieval Tasks)

| Task | Shared Weights? | Fine-Tuned? | Separate Head? | Evidence |
| --- | --- | --- | --- | --- |
| VQAv2 (VQA) | Yes, same ViLT-B/32 base is pre-trained then fine-tuned per task | Yes | Yes (two-layer MLP head) | "We evaluate ViLT-B/32 on two commonly used datasets: VQAv2 and NLVR2. We use a two-layer MLP of hidden size 1,536 as the fine-tuned downstream head." (Section 4.3, Classification Tasks); "Following this practice, we fine-tune ViLT-B/32 on the VQAv2 train and validation sets while reserving 1,000 validation images and their related questions for internal validation." (Section 4.3, Visual Question Answering) |
| NLVR2 | Yes, same ViLT-B/32 base is pre-trained then fine-tuned per task | Yes | Yes (two-layer MLP head) | "We evaluate ViLT-B/32 on two commonly used datasets: VQAv2 and NLVR2. We use a two-layer MLP of hidden size 1,536 as the fine-tuned downstream head." (Section 4.3, Classification Tasks) |
| Image-to-text retrieval | Yes, same ViLT-B/32 base is pre-trained then fine-tuned per task | Yes | Yes (similarity score head initialized from ITM head) | "We fine-tune ViLT-B/32 on the Karpathy & Fei-Fei (2015) split of MSCOCO and F30K." (Section 4.4, Retrieval Tasks); "We initialize the similarity score head from the pre-trained ITM head," (Section 4.4, Retrieval Tasks) |
| Text-to-image retrieval | Yes, same ViLT-B/32 base is pre-trained then fine-tuned per task | Yes | Yes (similarity score head initialized from ITM head) | "We fine-tune ViLT-B/32 on the Karpathy & Fei-Fei (2015) split of MSCOCO and F30K." (Section 4.4, Retrieval Tasks); "We initialize the similarity score head from the pre-trained ITM head," (Section 4.4, Retrieval Tasks) |

## 6. Input and Representation Constraints
- Image resizing constraints:
  > We resize the shorter edge of input images to 384 and limit
  > the longer edge to under 640 while preserving the aspect
  > ratio.
  > (Section 4.2, Implementation Details)
- Patch size and patch projection:
  > We use a 32 × 32 patch projection which only requires 2.4M
  > parameters.
  > (Section 2.3, Patch Projection)
  > Hidden
  > size H is 768, layer depth D is 12, patch size P is 32, MLP
  > size is 3,072, and the number of attention heads is 12.
  > (Section 3.1, Model Overview)
- Token/patch count and padding:
  > Patch projection of ViLT-B/32 yields 12
  > × 20 = 240 patches for an image with a resolution of 384 ×
  > 640. As this is a rarely reached upper limit, we sample 200
  > patches at maximum during pre-training. We interpolate
  > V pos of ViT-B/32 to fit the size of each image and pad the
  > patches for batch training.
  > (Section 4.2, Implementation Details)
- Text tokenization and text positional embeddings:
  > We use the bert-base-uncased tokenizer to tokenize
  > text inputs. Instead of fine-tuning from pre-trained BERT,
  > we learn the textual embedding-related parameters tclass , T ,
  > and T pos from scratch.
  > (Section 4.2, Implementation Details)
  > text t ∈ RL×|V | is embedded to t̄ ∈ RL×H with a word
  > embedding matrix T ∈ R|V |×H and a position embedding
  > matrix T pos ∈ R(L+1)×H .
  > (Section 3.1, Model Overview)
- Fixed dimensionality: Not specified in the paper beyond defining images as I ∈ RC×H×W.
  > The input image I ∈ RC×H×W is sliced into patches
  > (Section 3.1, Model Overview)

## 7. Context Window and Attention Structure
- Maximum sequence length (visual tokens): 240 image tokens (with 200 sampled during pre-training); text length not specified in the paper.
  > Patch projection of ViLT-B/32 yields 12
  > × 20 = 240 patches for an image with a resolution of 384 ×
  > 640. As this is a rarely reached upper limit, we sample 200
  > patches at maximum during pre-training.
  > (Section 4.2, Implementation Details)
  > We observe that the runtime of BERT-base-like transformers
  > varies only by < 1 ms for input sequences of length under
  > 300. Since patch projection of ViLT-B/32 generates at most
  > 240 image tokens, our model can still be efficient even
  > though it receives a combination of image and text tokens.
  > (Section 4.6, Complexity Analysis of VLP Models)
- Sequence length fixed or variable: Variable (image size preserved and patches are padded).
  > We resize the shorter edge of input images to 384 and limit
  > the longer edge to under 640 while preserving the aspect
  > ratio.
  > (Section 4.2, Implementation Details)
  > We interpolate
  > V pos of ViT-B/32 to fit the size of each image and pad the
  > patches for batch training.
  > (Section 4.2, Implementation Details)
- Attention type: Global self-attention (standard MSA); no windowed/hierarchical/sparse attention described.
  > ViT consists of stacked blocks that include a multiheaded
  > self-attention (MSA) layer and an MLP layer.
  > (Section 3.1, Model Overview)
- Mechanisms to manage computational cost: Patch projection and sampling patches during pre-training.
  > Patch Projection. To minimize overhead, we adopt the
  > simplest visual embedding scheme: linear projection that
  > operates on image patches.
  > (Section 2.3, Patch Projection)
  > As this is a rarely reached upper limit, we sample 200
  > patches at maximum during pre-training.
  > (Section 4.2, Implementation Details)

## 8. Positional Encoding (Critical Section)
- Mechanism: Absolute learned position embeddings for text and image tokens.
  > text t ∈ RL×|V | is embedded to t̄ ∈ RL×H with a word
  > embedding matrix T ∈ R|V |×H and a position embedding
  > matrix T pos ∈ R(L+1)×H .
  > (Section 3.1, Model Overview)
  > Followed by lin2
  > ear projection V ∈ R(P ·C)×H and position embedding
  > V pos ∈ R(N +1)×H , v is embedded into v̄ ∈ RN ×H .
  > (Section 3.1, Model Overview)
- Where applied: Added at the input embedding stage for text and image tokens.
  > text t ∈ RL×|V | is embedded to t̄ ∈ RL×H with a word
  > embedding matrix T ∈ R|V |×H and a position embedding
  > matrix T pos ∈ R(L+1)×H .
  > (Section 3.1, Model Overview)
  > Followed by lin2
  > ear projection V ∈ R(P ·C)×H and position embedding
  > V pos ∈ R(N +1)×H , v is embedded into v̄ ∈ RN ×H .
  > (Section 3.1, Model Overview)
- Fixed or modified per task: Visual position embeddings are interpolated to fit image size; text position embeddings are learned from scratch.
  > We interpolate
  > V pos of ViT-B/32 to fit the size of each image and pad the
  > patches for batch training.
  > (Section 4.2, Implementation Details)
  > we learn the textual embedding-related parameters tclass , T ,
  > and T pos from scratch.
  > (Section 4.2, Implementation Details)
- Ablation or comparison of alternatives: Not specified in the paper.

## 9. Positional Encoding as a Variable
- Treated as: Fixed architectural assumption (position embeddings are specified but not varied).
- Multiple positional encodings compared: Not specified in the paper.
- Claim that PE choice is "not critical" or secondary: Not specified in the paper.

## 10. Evidence of Constraint Masking (Scale vs. Structure)
- Model size / capacity details:
  > Hidden
  > size H is 768, layer depth D is 12, patch size P is 32, MLP
  > size is 3,072, and the number of attention heads is 12.
  > (Section 3.1, Model Overview)
  > Table 6. Comparison of VLP models in terms of parameter size,
  > FLOPs, and inference latency.
  > (Section 4.6, Complexity Analysis of VLP Models)
  > #Params
  > (M)
  > (Section 4.6, Complexity Analysis of VLP Models)
  > ViLT-B/32200+40
  >
  > 87.4
  > (Section 4.6, Complexity Analysis of VLP Models)
- Dataset sizes (pre-training):
  > Table 1. Pre-training dataset statistics.
  > (Section 4.1, Overview)
  > Dataset
  > MSCOCO
  > VG
  > GCC†
  > SBU†
  >
  > # Images
  > 113K
  > 108K
  > 3.01M
  > 867K
  >
  > # Captions
  > 567K
  > 5.41M
  > 3.01M
  > 867K
  > (Section 4.1, Overview)
- Training scale and observed gains from longer training:
  > We pre-train ViLT-B/32 for 100K or 200K steps on 64
  > NVIDIA V100 GPUs with a batch size of 4,096.
  > (Section 4.2, Implementation Details)
  > As expected, the performance constantly
  > increases as we train the model for longer training steps
  > (rows 1~3).
  > (Section 4.5, Ablation Study)
- Evidence of scale or training tricks driving gains:
  > In Table 5, we perform various ablations. More training
  > steps, whole word masking, and image augmentation come
  > to be beneficial, whereas an additional training objective
  > does not help.
  > (Section 4.5, Ablation Study)
- Evidence about scaling data/model size (forward-looking, not an empirical claim in this paper):
  > the per-
  > formance of pre-trained transformers scale well given an
  > appropriate amount of data.
  > (Section 5, Conclusion and Future Work)
  > We leave training larger models for future work because aligned vision-and-language
  > datasets are yet scarce.
  > (Section 5, Conclusion and Future Work)

## 11. Architectural Workarounds
- Convolution-free visual embedding to reduce overhead:
  > the processing of visual
  > inputs is drastically simplified to just the same
  > convolution-free manner that we process textual
  > inputs.
  > (Abstract)
- Patch projection (linear) as lightweight visual embedder:
  > Patch Projection. To minimize overhead, we adopt the
  > simplest visual embedding scheme: linear projection that
  > operates on image patches.
  > (Section 2.3, Patch Projection)
- Single-stream interaction to avoid extra parameters:
  > We
  > follow the single-stream approach for our interaction transformer module because the dual-stream approach introduces
  > additional parameters.
  > (Section 2.2, Modality Interaction Schema)
- Patch sampling and padding to manage input size:
  > As this is a rarely reached upper limit, we sample 200
  > patches at maximum during pre-training. We interpolate
  > V pos of ViT-B/32 to fit the size of each image and pad the
  > patches for batch training.
  > (Section 4.2, Implementation Details)

## 12. Explicit Limitations and Non-Claims
- Proof-of-concept limitation:
  > Although remarkable as it is, ViLT-B/32 is more of a proof
  > of concept that efficient VLP models free of convolution
  > and region supervision can still be competent.
  > (Section 5, Conclusion and Future Work)
- Data scarcity / larger models deferred to future work:
  > We leave training larger models for future work because aligned vision-and-language
  > datasets are yet scarce.
  > (Section 5, Conclusion and Future Work)
- Additional limitations or explicit non-claims about open-world learning, unrestrained multi-task learning, or meta-learning: Not specified in the paper.
