Number of distinct tasks evaluated: 23.
- MMBench defines 20 distinct L-3 (leaf) abilities in the benchmark. (p.6)
- The paper additionally validates the evaluation paradigm on three external VQA tasks: GQA, OK-VQA, and Text-VQA. (p.19)

Number of trained model instances required to cover all tasks: 1.
- MMBench evaluation uses a zero-shot setting with the same prompt across questions/abilities, and the paper states results are presented under a zero-shot setting unless noted, indicating a single model instance is used across tasks rather than task-specific training. (p.8; p.21)

$$
\boxed{
\frac{23\ \text{tasks}}{1\ \text{model}} = 23
}
$$
