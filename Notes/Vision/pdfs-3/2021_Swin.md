## 1. Basic Metadata
- Title: Swin Transformer: Hierarchical Vision Transformer using Shifted Windows.
- Authors: Ze Liu; Yutong Lin; Yue Cao; Han Hu; Yixuan Wei; Zheng Zhang; Stephen Lin; Baining Guo.
- Year: 2021.
- Venue: arXiv preprint (arXiv:2103.14030v2 [cs.CV], 17 Aug 2021).

## 2. One-Sentence Contribution Summary
Swin Transformer is a hierarchical vision Transformer with shifted-window attention proposed to serve as a general-purpose backbone for vision while addressing scale variation and high-resolution image efficiency (Abstract, p1: "This paper presents a new vision Transformer, called Swin Transformer, that capably serves as a general-purpose backbone for computer vision." and "To address these differences, we propose a hierarchical Transformer whose representation is computed with Shifted windows.").

## 3. Tasks Evaluated
- Image classification
  - Task type: Classification.
  - Dataset(s): ImageNet-1K (and ImageNet-22K for pre-training).
  - Domain: RGB images (dataset domain not explicitly stated).
  - Evidence (Section 4, p5): "We conduct experiments on ImageNet-1K image classification [19], COCO object detection [43], and ADE20K semantic segmentation [83]."; (Section 4.1, p5): "For image classification, we benchmark the proposed Swin Transformer on ImageNet-1K [19], which contains 1.28M training images and 50K validation images from 1,000 classes."; (Section 4.1, p6): "We also pre-train on the larger ImageNet-22K dataset, which contains 14.2 million images and 22K classes."; modality evidence (Section 3.1, p3): "It first splits an input RGB image into non-overlapping patches by a patch splitting module, like ViT.".

- Object detection
  - Task type: Detection.
  - Dataset(s): COCO 2017.
  - Domain: RGB images (dataset domain not explicitly stated).
  - Evidence (Section 4.2, p6): "Object detection and instance segmentation experiments are conducted on COCO 2017, which contains 118K training, 5K validation and 20K test-dev images."; modality evidence (Section 3.1, p3): "input RGB image".

- Instance segmentation
  - Task type: Segmentation (instance).
  - Dataset(s): COCO 2017.
  - Domain: RGB images (dataset domain not explicitly stated).
  - Evidence (Section 4.2, p6): "Object detection and instance segmentation experiments are conducted on COCO 2017, which contains 118K training, 5K validation and 20K test-dev images.".

- Semantic segmentation
  - Task type: Segmentation (semantic).
  - Dataset(s): ADE20K.
  - Domain: RGB images (dataset domain not explicitly stated).
  - Evidence (Section 4, p5): "We conduct experiments on ImageNet-1K image classification [19], COCO object detection [43], and ADE20K semantic segmentation [83]."; (Section 4.3, p8): "ADE20K [83] is a widely-used semantic segmentation dataset, covering a broad range of 150 semantic categories. It has 25K images in total, with 20K for training, 2K for validation, and another 3K for testing.".

## 4. Domain and Modality Scope
- Modality: Single modality (RGB images). Evidence (Section 3.1, p3): "It first splits an input RGB image into non-overlapping patches by a patch splitting module, like ViT.".
- Domain scope: Multiple datasets within the same modality (RGB image datasets: ImageNet-1K, COCO, ADE20K). Evidence (Section 4, p5): "We conduct experiments on ImageNet-1K image classification [19], COCO object detection [43], and ADE20K semantic segmentation [83].".
- Domain generalization / cross-domain transfer: Not claimed in the paper.

## 5. Model Sharing Across Tasks
| Task | Shared Weights? | Fine-Tuned? | Separate Head? | Evidence |
| --- | --- | --- | --- | --- |
| Image classification | Not specified in the paper. | Yes (ImageNet-22K pre-training then ImageNet-1K fine-tuning). | Yes. | Fine-tuning evidence (Section 4.1, p6): "In ImageNet-1K fine-tuning, we train the models for 30 epochs with a batch size of 1024, a constant learning rate of 10−5 , and a weight decay of 10−8 ."; classification head (Appendix A2.1, p9): "The image classification is performed by applying a global average pooling layer on the output feature map of the last stage, followed by a linear classifier." |
| Object detection | Not specified in the paper. | Not specified in the paper (uses ImageNet-22K pre-trained initialization). | Yes. | Initialization evidence (Section 4.2, p6): "ImageNet-22K pre-trained model as initialization."; task head/frameworks (Section 4.2, p6): "For the ablation study, we consider four typical object detection frameworks: Cascade Mask R-CNN [29, 6], ATSS [79], RepPoints v2 [12], and Sparse RCNN [56] in mmdetection [10]." |
| Instance segmentation | Not specified in the paper. | Not specified in the paper (uses ImageNet-22K pre-trained initialization). | Yes. | Evidence (Section 4.2, p6): "Object detection and instance segmentation experiments are conducted on COCO 2017, which contains 118K training, 5K validation and 20K test-dev images." and "ImageNet-22K pre-trained model as initialization.". |
| Semantic segmentation | Not specified in the paper. | Not specified in the paper (ImageNet-22K pre-training noted for Swin-B/L). | Yes. | Pre-training evidence (Appendix A2.3, p10): "Swin-B and Swin-L with ‡ indicate that these two models are pre-trained on ImageNet-22K"; segmentation head/framework (Section 4.3, p8): "We utilize UperNet [69] in mmseg [16] as our base framework for its high efficiency." |

## 6. Input and Representation Constraints
- Input modality / dimensionality: The paper specifies RGB images (Section 3.1, p3: "It first splits an input RGB image into non-overlapping patches by a patch splitting module, like ViT."). No explicit statement about other dimensionalities.
- Input resolution: Default 224×224 for architectures (Appendix A1, p9: "an input image size of 224×224 is assumed for all architectures.") with other resolutions via fine-tuning (Appendix A2.1, p9: "For other resolutions such as 3842 , we fine-tune the models trained at 2242 resolution, instead of training from scratch, to reduce GPU consumption."); detection uses multi-scale resizing (Section 4.2, p6: "multi-scale training [8, 56] (resizing the input such that the shorter side is between 480 and 800 while the longer side is at most 1333)"); segmentation uses fixed inputs of 512×512 or 640×640 (Appendix A2.3, p10: "Swin-T, Swin-S are trained on the standard setting as the previous approaches with an input of 512×512. Swin-B and Swin-L with ‡ indicate that these two models are pre-trained on ImageNet-22K, and trained with the input of 640×640.").
- Patch size: "In our implementation, we use a patch size of 4 × 4 and thus the feature dimension of each patch is 4 × 4 × 3 = 48." (Section 3.1, p3).
- Number of tokens: Not specified as fixed; token counts are reduced by patch merging (Section 3.1, p3: "This reduces the number of tokens by a multiple of 2 × 2 = 4 (2× downsampling of resolution).").
- Window/attention granularity: "The window size is set to M = 7 by default." (Section 3.3, p5).
- Padding/resizing requirements: "To make the window size (M, M ) divisible by the feature map size of (h, w), bottom-right padding is employed on the feature map if needed." (Section 3.2, p5); segmentation also uses multi-scale test (Appendix A2.3, p10: "In inference, a multi-scale test using resolutions that are [0.5, 0.75, 1.0, 1.25, 1.5, 1.75]× of that in training is employed.").

## 7. Context Window and Attention Structure
- Attention type: Windowed + shifted, with hierarchical stages.
  - Windowed attention: "we propose to compute self-attention within local windows." (Section 3.2, p4) and "The windows are arranged to evenly partition the image in a non-overlapping manner." (Section 3.2, p4).
  - Shifted windows: "To introduce cross-window connections while maintaining the efficient computation of non-overlapping windows, we propose a shifted window partitioning approach which alternates between two partitioning configurations in consecutive Swin Transformer blocks." (Section 3.2, p4).
  - Hierarchical structure: "Swin Transformer constructs a hierarchical representation by starting from small-sized patches (outlined in gray) and gradually merging neighboring patches in deeper Transformer layers." (Section 1, p2).
- Maximum sequence length: Not specified as a single global sequence length; attention is computed within windows of M × M patches (Section 3.2, p4: "Supposing each window contains M × M patches") and M is fixed by default (Section 3.3, p5: "The window size is set to M = 7 by default.").
- Fixed vs variable length: Window size is fixed (M = 7 by default), but input resolution varies across experiments (Appendix A2.1, p9: "For other resolutions such as 3842 , we fine-tune the models trained at 2242 resolution, instead of training from scratch, to reduce GPU consumption.").
- Mechanisms to manage computational cost:
  - Local windows and linear complexity: "The number of patches in each window is fixed, and thus the complexity becomes linear to image size." (Section 1, p2) and "The global computation leads to quadratic complexity with respect to the number of tokens, making it unsuitable for many vision problems requiring an immense set of tokens for dense prediction or to represent a high-resolution image." (Section 3.2, p4).
  - Token reduction via patch merging: "This reduces the number of tokens by a multiple of 2 × 2 = 4 (2× downsampling of resolution)." (Section 3.1, p3).
  - Shifted windows for cross-window connections while maintaining efficiency (Section 3.2, p4 quote above).

## 8. Positional Encoding (Critical Section)
- Mechanism: Relative position bias in attention (bias-based, relative). Evidence (Section 3.2, p5): "Relative position bias In computing self-attention, we follow [49, 1, 32, 33] by including a relative position bias".
- Where applied: As an additive bias in the attention similarity (QK^T / d + B) inside each attention computation (Section 3.2, p5 quote above).
- Fixed across experiments vs modified: Relative position bias is the default; absolute position embedding is tested but not adopted. Evidence (Section 3.2, p5): "We observe significant improvements over counterparts without this bias term or that use absolute position embedding, as shown in Table 4. Further adding absolute position embedding to the input as in [20] drops performance slightly, thus it is not adopted in our implementation.".
- Ablations/comparisons: "Table 4 shows comparisons of different position embedding approaches." (Section 4.4, p8).

## 9. Positional Encoding as a Variable
- Core variable vs fixed assumption: The paper treats position encoding as an ablated design element (not explicitly stated as a core research variable). Evidence (Section 4.4, p8): "Table 4 shows comparisons of different position embedding approaches.".
- Multiple positional encodings compared: Yes (explicitly compared in Table 4). Evidence (Section 4.4, p8 quote above).
- Claim that PE choice is “not critical” or secondary: Not specified in the paper.

## 10. Evidence of Constraint Masking (Scale vs Structure)
- Model sizes / scaling:
  - "We build our base model, called Swin-B, to have of model size and computation complexity similar to ViT-B/DeiT-B. We also introduce Swin-T, Swin-S and Swin-L, which are versions of about 0.25×, 0.5× and 2× the model size and computational complexity, respectively." (Section 3.3, p5).
- Dataset sizes / scaling data:
  - ImageNet-1K size: "ImageNet-1K [19], which contains 1.28M training images and 50K validation images from 1,000 classes." (Section 4.1, p5).
  - ImageNet-22K size: "the larger ImageNet-22K dataset, which contains 14.2 million images and 22K classes." (Section 4.1, p6).
- Evidence of gains from scaling data:
  - "ImageNet-22K pre-training brings 1.8%∼1.9% gains over training on ImageNet-1K from scratch." (Section 4.1, p6).
- Evidence of gains from architectural hierarchy / windowing:
  - Hierarchy: "Swin Transformer constructs a hierarchical representation by starting from small-sized patches (outlined in gray) and gradually merging neighboring patches in deeper Transformer layers." (Section 1, p2).
  - Shifted window ablation gains: "Swin-T with the shifted window partitioning outperforms the counterpart built on a single window partitioning at each stage by +1.1% top-1 accuracy on ImageNet-1K, +2.8 box AP/+2.2 mask AP on COCO, and +2.8 mIoU on ADE20K." (Section 4.4, p8).
- Training tricks: Not explicitly claimed as primary drivers of gains beyond reporting settings; no direct attribution statement found.

## 11. Architectural Workarounds
- Windowed self-attention to control compute: "we propose to compute self-attention within local windows." (Section 3.2, p4) and "The number of patches in each window is fixed, and thus the complexity becomes linear to image size." (Section 1, p2).
- Shifted windows to enable cross-window connections efficiently: "To introduce cross-window connections while maintaining the efficient computation of non-overlapping windows, we propose a shifted window partitioning approach which alternates between two partitioning configurations in consecutive Swin Transformer blocks." (Section 3.2, p4).
- Hierarchical feature maps via patch merging: "To produce a hierarchical representation, the number of tokens is reduced by patch merging layers as the network gets deeper." (Section 3.1, p3).
- Efficient batch computation via cyclic shift and masking: "Here, we propose a more efficient batch computation approach by cyclic-shifting toward the top-left direction, as illustrated in Figure 4. After this shift, a batched window may be composed of several sub-windows that are not adjacent in the feature map, so a masking mechanism is employed to limit self-attention computation to within each sub-window." (Section 3.2, p5).

## 12. Explicit Limitations and Non-Claims
- Limitation (scope): "A thorough kernel optimization is beyond the scope of this paper." (Section 4.2, p7).
- Future work: "we look forward to investigating its use in natural language processing as well." (Conclusion, p9).
- Non-claims / scope statements: "our work focuses on general-purpose performance rather than specifically on classification." (Related Work, p3).
- Other limitations or explicit non-claims about open-world/multi-task learning: Not specified in the paper.
