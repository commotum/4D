## 1. Basic Metadata
- Title: "BLIP: Bootstrapping Language-Image Pre-training for Unified Vision-Language Understanding and Generation" (page 1)
- Authors: "Junnan Li Dongxu Li Caiming Xiong Steven Hoi" (page 1)
- Year: 2022 (from "arXiv:2201.12086v2 [cs.CV] 15 Feb 2022", page 1)
- Venue: arXiv (from "arXiv:2201.12086v2 [cs.CV] 15 Feb 2022", page 1)

## 2. One-Sentence Contribution Summary
BLIP is proposed as a VLP framework that "transfers flexibly to both vision-language understanding and generation tasks" by "bootstrapping the captions, where a captioner generates synthetic captions and a filter removes the noisy ones." (page 1, Abstract)

## 3. Tasks Evaluated
- Image-text retrieval. Task type: Other (retrieval). Dataset(s): COCO, Flickr30K. Domain: Not specified in the paper. Evidence: "We evaluate BLIP for both image-to-text retrieval (TR) and text-to-image retrieval (IR) on COCO and Flickr30K (Plummer et al., 2015) datasets." (page 6, Section 5.1)
- Image captioning. Task type: Generation. Dataset(s): No-Caps, COCO. Domain: Not specified in the paper. Evidence: "We consider two datasets for image captioning: No-Caps (Agrawal et al., 2019) and COCO, both evaluated using the model finetuned on COCO with the LM loss." (page 6, Section 5.2)
- Visual Question Answering (VQA). Task type: Other (question answering). Dataset(s): VQA2.0 (with additional Visual Genome samples for training). Domain: Not specified in the paper. Evidence: "VQA (Antol et al., 2015) requires the model to predict an answer given an image and a question." (page 7, Section 5.3); "We experiment with the VQA2.0 dataset (Goyal et al., 2017), which contains 83k/41k/81k images for training/validation/test. Following Li et al. (2021a), we use both training and validation splits for training, and include additional training samples from Visual Genome." (page 12, Appendix A)
- Natural Language Visual Reasoning (NLVR2). Task type: Reasoning / relational. Dataset(s): NLVR2 official split. Domain: Not specified in the paper. Evidence: "NLVR2 (Suhr et al., 2019) asks the model to predict whether a sentence describes a pair of images." (page 7, Section 5.4); "NLVR2 . We conduct experiment on the official split (Suhr et al., 2019)." (page 12, Appendix A)
- Visual Dialog (VisDial). Task type: Other (dialog/answer ranking). Dataset(s): VisDial v1.0 train/validation. Domain: Not specified in the paper. Evidence: "VisDial (Das et al., 2017) extends VQA in a natural conversational setting, where the model needs to predict an answer not only based on the image-question pair, but also considering the dialog history and the image’s caption." (page 8, Section 5.5); "We finetune on the training split of VisDial v1.0 and evaluate on its validation set." (page 12, Appendix A)
- Text-to-video retrieval (zero-shot). Task type: Other (retrieval). Dataset(s): MSRVTT (1k test split). Domain: Video. Evidence: "we perform zero-shot transfer to text-to-video retrieval" (page 8, Section 5.6); "text-to-video retrieval on the 1k test split of the MSRVTT dataset." (page 8, Table 10 caption)
- Video question answering (zero-shot). Task type: Other (question answering). Dataset(s): MSRVTT-QA, MSVD-QA. Domain: Video. Evidence: "we perform zero-shot transfer to text-to-video retrieval and video question answering" (page 8, Section 5.6); "Method                         MSRVTT-QA         MSVD-QA" (page 8, Table 11); "video question answering. We report top-1 test accuracy on two datasets." (page 8, Table 11 caption)

## 4. Domain and Modality Scope
- Multiple datasets within the image modality are used (COCO, Flickr30K, No-Caps, VQA2.0, NLVR2, VisDial), but the paper does not characterize them as a single domain. Evidence: "We evaluate BLIP for both image-to-text retrieval (TR) and text-to-image retrieval (IR) on COCO and Flickr30K" (page 6, Section 5.1); "We consider two datasets for image captioning: No-Caps (Agrawal et al., 2019) and COCO, both evaluated using the model finetuned on COCO with the LM loss." (page 6, Section 5.2); "We experiment with the VQA2.0 dataset (Goyal et al., 2017), which contains 83k/41k/81k images for training/validation/test." (page 12, Appendix A)
- Multiple modalities are evaluated: image tasks plus video-language tasks. Evidence: image retrieval on COCO/Flickr30K (page 6, Section 5.1) and "we perform zero-shot transfer to text-to-video retrieval and video question answering" (page 8, Section 5.6).
- Domain generalization / cross-domain transfer is claimed: "BLIP also demonstrates strong generalization ability when directly transferred to video-language tasks in a zero-shot manner." (page 1, Abstract)

## 5. Model Sharing Across Tasks
Pre-training is multi-task: "We jointly optimize three objectives during pre-training, with two understanding-based objectives and one generation-based objective." (page 3, Section 3.2)

| Task | Shared Weights? | Fine-Tuned? | Separate Head? | Evidence |
| --- | --- | --- | --- | --- |
| Image-text retrieval | Yes (pre-trained BLIP weights) | Yes | Yes (ITM head) | "We finetune the pre-trained model using ITC and ITM losses." (page 6, Section 5.1); "ITM is a binary classification task, where the model uses an ITM head (a linear layer)" (page 3, Section 3.2) |
| Image captioning | Yes (pre-trained BLIP weights) | Yes | Yes (image-grounded text decoder / LM) | "We consider two datasets for image captioning: No-Caps (Agrawal et al., 2019) and COCO, both evaluated using the model finetuned on COCO with the LM loss." (page 6, Section 5.2); "Language Modeling Loss (LM) activates the image-grounded text decoder" (page 3, Section 3.2) |
| VQA | Yes (pre-trained BLIP weights) | Yes | Yes (answer decoder) | "The VQA model is finetuned with the LM loss using ground-truth answers as targets." (page 7, Section 5.3); "an image-question is first encoded into multimodal embeddings and then given to an answer decoder." (page 7, Section 5.3) |
| NLVR2 | Yes (pre-trained BLIP weights) | Yes | Yes (MLP classifier) | "Table 14 shows the hyperparameters that we use for fine-tuning on the downstream vision-language tasks." (page 12, Appendix A); "An MLP classifier is applied on the output embedding of the [Encode] token." (page 8, Section 5.4) |
| VisDial | Yes (pre-trained BLIP weights) | Yes | Yes (ITM-based discriminative head) | "We finetune on the training split of VisDial v1.0 and evaluate on its validation set." (page 12, Appendix A); "The dialog encoder is trained with the ITM loss to discriminate whether the answer is true or false" (page 8, Section 5.5); "ITM is a binary classification task, where the model uses an ITM head (a linear layer)" (page 3, Section 3.2) |
| Text-to-video retrieval | Yes (uses COCO-retrieval model weights) | No (zero-shot) | Not specified beyond reuse of retrieval model | "we perform zero-shot transfer to text-to-video retrieval and video question answering, where we directly evaluate the models trained on COCO-retrieval and VQA, respectively." (page 8, Section 5.6) |
| Video question answering | Yes (uses VQA model weights) | No (zero-shot) | Not specified beyond reuse of VQA model | "we perform zero-shot transfer to text-to-video retrieval and video question answering, where we directly evaluate the models trained on COCO-retrieval and VQA, respectively." (page 8, Section 5.6) |

## 6. Input and Representation Constraints
- Image and text tokenization details: "divides an input image into patches and encodes them as a sequence of embeddings, with an additional [CLS] token" and "a [CLS] token is appended to the beginning of the text input"; plus task tokens: "A task-specific [Encode] token is appended" and "A [Decode] token is used to signal the beginning of a sequence, and an end-of-sequence token is used to signal its end." (page 3, Section 3.1)
- Input image resolution constraints: "We take random image crops of resolution 224 × 224 during pre-training, and increase the image resolution to 384 × 384 during finetuning." (page 4, Section 4.1); "We use an image resolution of 384 × 384, except for VQA where we follow Wang et al. (2021) and use 480 × 480 images." (page 12, Appendix A); dataset filter: "We only download images whose shorter edge is larger than 256 pixels" (page 4, footnote)
- Fixed patch size implied by backbone choice: "We explore two variants of ViTs: ViT-B/16 and ViT-L/16." (page 4, Section 4.1)
- Video input constraint: "we uniformly sample n frames per video (n = 8 for retrieval and n = 16 for QA), and concatenate the frame features into a single sequence." (page 8, Section 5.6)
- Output/inference constraints: captioning uses "the maximum generation length as 20" and VQA uses "the decoder to rank the 3,128 candidate answers." (page 12, Appendix A)
- Fixed number of tokens, padding, or fixed dimensionality beyond the above: Not specified in the paper.

## 7. Context Window and Attention Structure
- Maximum sequence length: Not specified for input sequences; only captioning inference specifies "the maximum generation length as 20." (page 12, Appendix A)
- Sequence length fixed or variable: Not specified for text/image token sequences; video processing fixes frame count "n = 8 for retrieval and n = 16 for QA." (page 8, Section 5.6)
- Attention type: the paper describes "bi-directional self-attention" for the encoder and "causal self-attention" for the decoder, with "cross-attention" layers; it does not characterize attention as global/windowed/hierarchical/sparse. (page 3, Section 3.1)
- Computational cost mechanisms: parameter sharing "share all parameters except for the SA layers" (page 3, Section 3.1); efficiency in pre-training with "one forward pass through the computational-heavier visual transformer" (page 3, Section 3.2); and retrieval speedups via candidate preselection and reranking: "To enable faster inference speed, we follow Li et al. (2021a) and first select k candidates based on the image-text feature similarity, and then rerank the selected candidates based on their pairwise ITM scores. We set k = 256 for COCO and k = 128 for Flickr30K." (page 6, Section 5.1)

## 8. Positional Encoding (Critical Section)
- Positional encoding mechanism: Not specified in the paper.
- Where applied / comparisons / ablations: Not specified in the paper.

## 9. Positional Encoding as a Variable
- Treated as research variable vs fixed assumption: Not specified in the paper.
- Multiple positional encodings compared or claims of non-criticality: Not specified in the paper.

## 10. Evidence of Constraint Masking (Scale)
- Model size/backbone scale: "We explore two variants of ViTs: ViT-B/16 and ViT-L/16." (page 4, Section 4.1)
- Dataset size scale: "We use the same pre-training dataset as Li et al. (2021a) with 14M images in total, including two human-annotated datasets (COCO and Visual Genome (Krishna et al., 2017)), and three web datasets (Conceptual Captions (Changpinyo et al., 2021), Conceptual 12M (Changpinyo et al., 2021), SBU captions (Ordonez et al., 2011)). We also experimented with an additional web dataset, LAION (Schuhmann et al., 2021), which contains 115M images with more noisy texts1 ." (page 4, Section 4.1)
- Data scaling as a driver: "performance improvement has been largely achieved by scaling up the dataset with noisy image-text pairs collected from the web" (page 1, Abstract).
- Explicit scaling benefit claim: "CapFilt can further boost performance with a larger dataset and a larger vision backbone, which verifies its scalability in both the data size and the model size." (page 4, Section 4.1)
- Training/data-generation trick: "In CapFilt, we employ nucleus sampling (Holtzman et al., 2020) to generate synthetic captions." (page 5, Section 4.3)

## 11. Architectural Workarounds
- Multi-mode architecture for multi-tasking: "we propose multimodal mixture of encoder-decoder (MED), a multi-task model which can operate in one of the three functionalities" (unimodal encoder, image-grounded text encoder, image-grounded text decoder). (page 3, Section 3.1)
- Cross-attention insertion and task tokens: "Image-grounded text encoder, which injects visual information by inserting one additional cross-attention (CA) layer between the self-attention (SA) layer and the feed forward network (FFN) for each transformer block of the text encoder. A task-specific [Encode] token is appended to the text, and the output embedding of [Encode] is used as the multimodal representation of the image-text pair." (page 3, Section 3.1); "A [Decode] token is used to signal the beginning of a sequence, and an end-of-sequence token is used to signal its end." (page 3, Section 3.1)
- Parameter sharing for efficiency: "the text encoder and text decoder share all parameters except for the SA layers" to improve training efficiency. (page 3, Section 3.1)
- NLVR2 two-image handling: "for each transformer block in the image-grounded text encoder, there exist two cross-attention layers to process the two input images, and their outputs are merged and fed to the FFN." (page 8, Section 5.4); "The merge layer performs simple average pooling in the first 6 layers of the encoder, and performs concatenation followed by a linear projection in layer 6-12. An MLP classifier is applied on the output embedding of the [Encode] token." (page 8, Section 5.4)
- Retrieval inference speedup: "To enable faster inference speed, we follow Li et al. (2021a) and first select k candidates based on the image-text feature similarity, and then rerank the selected candidates based on their pairwise ITM scores. We set k = 256 for COCO and k = 128 for Flickr30K." (page 6, Section 5.1)
- Video simplification: "we uniformly sample n frames per video (n = 8 for retrieval and n = 16 for QA), and concatenate the frame features into a single sequence." and "Note that this simple approach ignores all temporal information." (page 8, Section 5.6)

## 12. Explicit Limitations and Non-Claims
- Video temporal limitation: "Note that this simple approach ignores all temporal information." (page 8, Section 5.6)
- Limited benefit of more web data on NLVR2: "performance on NLVR2 does not benefit much from additional web images, possibly due to the domain gap between web data and downstream data." (page 8, Section 5.4)
- Future work directions: "There are a few potential directions that can further enhance the performance of BLIP: (1) Multiple rounds of dataset bootstrapping; (2) Generate multiple synthetic captions per image to further enlarge the pre-training corpus; (3) Model ensemble by training multiple different captioners and filters and combining their forces in CapFilt." (page 9, Conclusion)
- Benchmark omission: "we omit SNLI-VE from the benchmark because its test data has been reported to be noisy" (page 6, Section 5)
