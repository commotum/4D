1. Basic Metadata
- Title: "Deep Equilibrium Models" (page 1)
- Authors: "Shaojie Bai                          J. Zico Kolter                 Vladlen Koltun" (page 1)
- Year: 2019. Evidence: "arXiv:1909.01377v2 [cs.LG] 28 Oct 2019" (page 1)
- Venue: "33rd Conference on Neural Information Processing Systems (NeurIPS 2019), Vancouver, Canada." (page 1)

2. One-Sentence Contribution Summary
- The paper proposes the deep equilibrium model (DEQ) as a new approach to modeling sequential data that directly finds equilibrium points via root-finding (page 1: "We present a new approach to modeling sequential data: the deep equilibrium model (DEQ)." and "we propose the DEQ approach that directly finds these equilibrium points via root-finding.").

3. Tasks Evaluated
- Copy memory task
  - Task type: Other (sequence prediction / memory recall)
  - Dataset(s): synthetic copy-memory sequences
  - Domain: synthetic 1D sequences
  - Evidence: "Copy memory task. The copy memory task is a small but challenging synthetic stress test that has been frequently used in prior work to test a sequence model’s memory retention ability [53, 4, 7]." (Appendix F, page 16)
  - Evidence: "In this task, each sequence x1:(T +20) is 1-dimensional and has length T + 20, with x1:10 randomly selected from integers 1, 2, . . . , 8 (with repetition)." (Appendix F, page 16)
  - Evidence: "The goal of this task is to produce y1:(T +20) such that y1:T +10 = 0 and yT +11:T +20 = x1:10 ." (Appendix F, page 16)

- Word-level language modeling (Penn Treebank)
  - Task type: Generation (language modeling)
  - Dataset(s): Penn Treebank (PTB)
  - Domain: text (natural language)
  - Evidence: "Penn Treebank. The Penn Treebank (PTB) corpus [31] is a commonly used dataset for character- and word-level language modeling." (Appendix F, page 16)
  - Evidence: "When used for word-level language modeling, PTB contains about 888K words at training, with a vocabulary size of 10,000." (Appendix F, page 16)

- Word-level language modeling (WikiText-103)
  - Task type: Generation (language modeling)
  - Dataset(s): WikiText-103 (WT103)
  - Domain: text (natural language)
  - Evidence: "WikiText-103. The training corpus of WikiText-103 (WT103) [35] is about 110 times larger than PTB, with a vocabulary size over 260K." (Appendix F, page 16)
  - Evidence: "In general, this dataset is considered much more realistic than many others because it contains many rare words and retains punctuation, numbers, and capitalization from the original Wikipedia articles." (Appendix F, page 16)

4. Domain and Modality Scope
- Evaluation spans synthetic sequences and natural language text: "We evaluate DEQ on both synthetic stress tests and realistic large-scale language modeling (where complex long-term temporal dependencies are involved)." (page 6)
- Single domain vs multiple: Multiple domains within the same modality (sequence data); no multimodal evaluation stated (page 6).
- Domain generalization / cross-domain transfer: Not claimed in the paper.

5. Model Sharing Across Tasks

| Task | Shared Weights? | Fine-Tuned? | Separate Head? | Evidence |
| --- | --- | --- | --- | --- |
| Copy memory task | Not specified in the paper. | Not specified in the paper. | Not specified in the paper. | "The goal of the copy memory task is simple: to explicitly test a sequence model’s ability to exactly memorize elements across a long period of time (see Appendix F)." (page 7) |
| Word-level language modeling (PTB) | Not specified in the paper. | Not specified in the paper. | Not specified in the paper. | "we evaluate the DEQ-TrellisNet instantiation on word-level language modeling with the PTB corpus." (page 7) |
| Word-level language modeling (WT103) | Not specified in the paper. | Not specified in the paper. | Not specified in the paper. | "Performance on WikiText-103. On the much larger scale WT103 corpus (about 100x larger than PTB), the DEQ-TrellisNet achieves better test perplexity than the original deep TrellisNet." (page 7) |

6. Input and Representation Constraints
- Copy memory input dimensionality and length: "In this task, each sequence x1:(T +20) is 1-dimensional and has length T + 20, with x1:10 randomly selected from integers 1, 2, . . . , 8 (with repetition)." (Appendix F, page 16)
- Fixed sequence length for WT103 experiments: "For DEQ-Transformers, we employ the relative positional embedding [16], with sequences of length 150 at both training and inference on the WikiText-103 dataset." (page 7)
- Padding/sequence handling in TrellisNet: "u−(k−1)s: is typically: 1) the last (k − 1)s elements of the previous sequence’s output (if using history padding [8]); or 2) simply zero-padding." (page 6)
- Transformer history padding and relative PE: "we feed [z?−T 0 : , z1:T ] (i.e., with history padding of length T 0 ) and relative positional embedding PE−T 0 :T to the self-attention operation." (page 6)
- General input representation: "Deep sequence models. Given an input sequence x1:T = [x1 , . . . , xT ] ∈ RT ×p , where xi ∈ Rp (e.g., a word embedding) and T ∈ N is the sequence length, we define a sequence model as any function G that produces output G(x1:T ) = y1:T =∈ RT ×q that satisfies the causality constraint: yt depends only on x1:t and not on any element of xt+1:T ." (page 2)
- Fixed patch size / image resolution / 2D grid constraints: Not specified in the paper.

7. Context Window and Attention Structure
- Sequence length used in WT103 experiments: "sequences of length 150 at both training and inference on the WikiText-103 dataset." (page 7)
- Copy memory evaluation length: "DEQ demonstrates good memory retention over relatively long sequences (T = 400), with substantially better results than recurrent architectures such as LSTM/GRU (consistent with the findings in [7])." (page 7) and "each sequence x1:(T +20) is 1-dimensional and has length T + 20" (Appendix F, page 16)
- Attention type (global self-attention across time steps): "a self-attention layer maps the input into Q (query), K (key), and V (value) and computes the attention score between time-steps ti and tj as [QK > ]i,j . This attention score is then normalized via softmax and multiplied with the V sequence to produce the output." (page 6)
- Context extension via memory: "we feed [z?−T 0 : , z1:T ] (i.e., with history padding of length T 0 ) and relative positional embedding PE−T 0 :T to the self-attention operation." (page 6)
- Mechanisms to manage computation: "Both instantiations of DEQ use Broyden’s method [10] to avoid direct computation of the inverse Jacobian, as described in Section 3.1.3." (page 7)
- Windowed/hierarchical/sparse attention or token pruning: Not specified in the paper.

8. Positional Encoding (Critical Section)
- Mechanism: positional embeddings, specifically relative positional embeddings for the Transformer-XL-style memory.
  - Evidence: "Since the transformer is order-invariant, prior work proposed to add positional embeddings (PE) [48, 16] to the self-attention operation." (page 6)
  - Evidence: "we feed [z?−T 0 : , z1:T ] (i.e., with history padding of length T 0 ) and relative positional embedding PE−T 0 :T to the self-attention operation." (page 6)
  - Evidence: "For DEQ-Transformers, we employ the relative positional embedding [16]" (page 7)
- Where applied: self-attention operation (page 6 quotes above).
- Fixed vs modified/ablated: Only relative positional embedding is described; no ablation or alternatives are stated (not specified in the paper).

9. Positional Encoding as a Variable
- Treated as a fixed architectural choice, not a research variable: "For DEQ-Transformers, we employ the relative positional embedding [16]" (page 7)
- Multiple positional encodings compared: Not specified in the paper.
- Claims that PE choice is not critical: Not specified in the paper.

10. Evidence of Constraint Masking
- Dataset scale: "On both WikiText-103 [35] (which contains >100M words and a vocabulary size of >260K) and the smaller Penn Treebank corpus" (page 6); "PTB contains about 888K words at training, with a vocabulary size of 10,000." (Appendix F, page 16)
- Model sizes (examples): "DEQ-TrellisNet (ours)                    24M               20M             57.1         1.2GB" (Table 2, page 7); "DEQ-Transformer (medium, ours).                                                                                     172M                  43M                 24.2                2.7GB" (Table 3, page 8)
- Performance and memory claims (not explicitly tied to scaling model size/data): "we show that DEQs 1) often improve performance over these state-of-the-art models (for similar parameter counts); 2) have similar computational requirements to existing models; and 3) vastly reduce memory consumption (often the bottleneck for training large sequence models), demonstrating an up-to 88% memory reduction in our experiments." (Abstract, page 1)
- Attribution to scaling model size/data or training tricks: Not explicitly claimed beyond the DEQ approach and memory efficiency (not specified in the paper).

11. Architectural Workarounds
- Weight tying in TrellisNet: "the convolutional kernel weights are tied across the depth of the network (i.e., TrellisNet is a weight-tied TCN)." (page 6)
- Input injection in TrellisNet: "a linear transformation of the original input sequence x1:T is added to the convolutional outputs at all layers." (page 6)
- Positional embeddings for order invariance: "Since the transformer is order-invariant, prior work proposed to add positional embeddings (PE) [48, 16] to the self-attention operation." (page 6)
- Memory-augmented transformer with history padding: "we feed [z?−T 0 : , z1:T ] (i.e., with history padding of length T 0 ) and relative positional embedding PE−T 0 :T to the self-attention operation." (page 6)
- Root-finding efficiency: "Both instantiations of DEQ use Broyden’s method [10] to avoid direct computation of the inverse Jacobian, as described in Section 3.1.3." (page 7)
- Long-sequence workaround: "Training DEQs with subsequences. On extremely long sequences (e.g., T > 1000), the forward-pass fixed points can be challenging to solve accurately (especially at the start of the training) even with the help of the root-finding methods. Therefore, in practice, we suggest breaking these long sequences into a few subsequences when needed (recall that the forward pass can be any black-box root-finder)." (Appendix E, page 15)

12. Explicit Limitations and Non-Claims
- Regularization limitation: "it is not clear how certain regularizations such as auxiliary losses [46, 8] could be applied on DEQ, since there are no more “layers”." (Appendix E, page 15)
- Minibatch imbalance: "Not all sequences in a minibatch converge to the equilibrium with the same number of iterations. However, with standard batched CUDA operations, the sequences that converge faster essentially need to “wait” for the slower ones. Though we empirically find such imbalance to be relatively small in scale, it could mean an inefficient GPU utilization at times." (Appendix E, page 15)
- Long-sequence difficulty: "On extremely long sequences (e.g., T > 1000), the forward-pass fixed points can be challenging to solve accurately (especially at the start of the training) even with the help of the root-finding methods." (Appendix E, page 15)
- Future work: "combination of these approaches with the DEQ model is a promising avenue for future work." (page 3)
- Explicit non-claims about unrestrained multi-task or open-world learning: Not specified in the paper.

13. Constraint Profile (Synthesis)
- Domain scope: synthetic sequence stress tests and natural language language modeling (page 6; Appendix F, page 16).
- Task structure: sequence prediction (copy memory) and word-level language modeling tasks (Appendix F, page 16).
- Representation rigidity: fixed sequence length 150 for WT103 experiments; copy-memory sequences are 1-dimensional with length T + 20 (page 7; Appendix F, page 16).
- Model sharing vs specialization: separate evaluations are described; shared weights or joint training are not specified (page 6-7).
- Positional encoding: relative positional embedding used in self-attention; no alternatives or ablations described (page 6-7).

14. Final Classification
- Classification: Multi-task, single-domain.
- Justification: The paper evaluates "both synthetic stress tests and realistic large-scale language modeling" (page 6), covering multiple tasks (copy memory, PTB, WT103) within sequence modeling rather than multiple modalities. There is no claim of cross-domain transfer or multimodal evaluation.
