## 1. Basic Metadata
- Title: On the Measure of Intelligence.
  - Evidence (Title page): "On the Measure of Intelligence"
- Authors: François Chollet.
  - Evidence (Title page): "François Chollet ∗"
- Year: 2019.
  - Evidence (Title page): "arXiv:1911.01547v2 [cs.AI] 25 Nov 2019"
  - Evidence (Title page): "November 5, 2019"
- Venue: arXiv (cs.AI).
  - Evidence (Title page): "arXiv:1911.01547v2 [cs.AI] 25 Nov 2019"

## 2. One-Sentence Contribution Summary
The paper defines intelligence as skill-acquisition efficiency via Algorithmic Information Theory and proposes the ARC benchmark to measure human-like general intelligence under explicit priors.
- Evidence (Abstract): "We then articulate a new formal definition of intelligence based on Algorithmic Information Theory, describing intelligence as skill-acquisition efficiency and highlighting the concepts of scope, generalization difficulty, priors, and experience, as critical pieces to be accounted for in characterizing intelligent systems."
- Evidence (Abstract): "Finally, we present a new benchmark closely following these guidelines, the Abstraction and Reasoning Corpus (ARC), built upon an explicit set of priors designed to be as close as possible to innate human priors."

## 3. Tasks Evaluated
The paper does not report model evaluations; it defines ARC as the benchmark and describes ARC tasks.
- Evidence (Section: III A benchmark proposal: the ARC dataset): "In this last part, we introduce the Abstraction and Reasoning Corpus (ARC), a dataset intended to serve as a benchmark for the kind of general intelligence defined in II.2."

Task: ARC grid-to-grid abstraction and reasoning tasks
- Task type: Reasoning / relational; Reconstruction; Other (program synthesis / grid-to-grid transformation)
- Dataset(s): ARC training set (400 tasks) and evaluation set (600 tasks)
- Domain: Synthetic grids of discrete symbols/colors (2D grid)
- Evidence (Section: III.1.1 What is ARC?): "ARC can be seen as a general artificial intelligence benchmark, as a program synthesis benchmark, or as a psychometric intelligence test."
- Evidence (Section: III.1.1 What is ARC?): "ARC comprises a training set and an evaluation set. The training set features 400 tasks, while the evaluation set features 600 tasks."
- Evidence (Section: III.1.1 What is ARC?): "Each task consists of a small number of demonstration examples (3.3 on average), and a small number of test examples (generally 1, although it may be 2 or 3 in rare cases)."
- Evidence (Section: III.1.1 What is ARC?): "The test-taker must construct on its own the output grid corresponding to the input grid of each test example."
- Evidence (Section: III.1.1 What is ARC?): "Constructing the output grid is done entirely from scratch, meaning that the test-taker must decide what the height and width of the output grid should be, what symbols it should place on the grid, and where."
- Evidence (Section: III.1.1 What is ARC?): "There are 10 unique symbols (or colors)."
- Evidence (Section: III.1.1 What is ARC?): "A grid can be any height or width between 1x1 and 30x30, inclusive (the median height is 9 and the median width is 10)."
- Evidence (Section: III.1.1 What is ARC?): "The task is successfully solved if the testtaker can produce the exact correct answer on all test examples for the task (binary measure of success)."

## 4. Domain and Modality Scope
- Single domain? Yes. ARC tasks are defined over symbolic grids with bounded sizes and colors.
  - Evidence (Section: III.1.1 What is ARC?): "There are 10 unique symbols (or colors)."
  - Evidence (Section: III.1.1 What is ARC?): "A grid can be any height or width between 1x1 and 30x30, inclusive (the median height is 9 and the median width is 10)."
- Multiple domains within the same modality? Not specified in the paper.
- Multiple modalities? Not specified in the paper.
- Domain generalization or cross-domain transfer claimed? The paper frames ARC around developer-aware generalization, not cross-domain transfer.
  - Evidence (Section: III.1.1 What is ARC?): "Focus on measuring developer-aware generalization, rather than task-specific skill, by only featuring novel tasks in the evaluation set (assumed unknown to the developer of a test-taker)."

## 5. Model Sharing Across Tasks
| Task | Shared Weights? | Fine-Tuned? | Separate Head? | Evidence |
| --- | --- | --- | --- | --- |
| ARC grid-to-grid tasks | Not specified in the paper. | Not specified in the paper. | Not specified in the paper. | "ARC can be seen as a general artificial intelligence benchmark, as a program synthesis benchmark, or as a psychometric intelligence test." (Section: III.1.1 What is ARC?) |

## 6. Input and Representation Constraints
- Variable input resolution (grid size varies):
  - Evidence (Section: III.1.1 What is ARC?): "A grid can be any height or width between 1x1 and 30x30, inclusive (the median height is 9 and the median width is 10)."
- Discrete symbol/color vocabulary size:
  - Evidence (Section: III.1.1 What is ARC?): "There are 10 unique symbols (or colors)."
- Input/output format is grid-to-grid transformation with explicit output construction:
  - Evidence (Section: III.1.1 What is ARC?): "The test-taker must construct on its own the output grid corresponding to the input grid of each test example."
  - Evidence (Section: III.1.1 What is ARC?): "Constructing the output grid is done entirely from scratch, meaning that the test-taker must decide what the height and width of the output grid should be, what symbols it should place on the grid, and where."
- Fixed patch size? Not specified in the paper.
- Fixed number of tokens? Not specified in the paper.
- Fixed dimensionality (e.g., strictly 2D)? Not specified in the paper (the paper describes grids but does not formalize dimensionality constraints beyond grid size).
- Padding/resizing requirements? Not specified in the paper.

## 7. Context Window and Attention Structure
- Maximum sequence length: Not specified in the paper (only grid size bounds are specified).
- Fixed or variable sequence length: Not specified in the paper.
- Attention type (global/windowed/hierarchical/sparse): Not specified in the paper.
- Mechanisms to manage computational cost (windowing/pooling/pruning): Not specified in the paper.

## 8. Positional Encoding (Critical Section)
Not specified in the paper.

## 9. Positional Encoding as a Variable
Not specified in the paper.

## 10. Evidence of Constraint Masking
- Model size(s): Not specified in the paper.
- Dataset size(s): ARC totals 1,000 tasks.
  - Evidence (Section: III.2 Weaknesses and future refinements): "Dataset size and diversity may be limited. ARC only features 1,000 tasks in total, and there may be some amount of conceptual overlap across many tasks."
  - Evidence (Section: III.1.1 What is ARC?): "ARC comprises a training set and an evaluation set. The training set features 400 tasks, while the evaluation set features 600 tasks."
- Performance gains attributed to scaling model size, scaling data, architectural hierarchy, or training tricks: Not specified in the paper (no experimental results are reported).

## 11. Architectural Workarounds
The paper does not introduce a concrete model architecture; it suggests a hypothetical program-synthesis-based solver for ARC.
- Evidence (Section: III.1.4 What a solution to ARC may look like, and what it would imply for AI applications): "For a researcher setting out to solve it, ARC is perhaps best understood as a program synthesis benchmark."
- Evidence (Section: III.1.4 What a solution to ARC may look like, and what it would imply for AI applications): "A hypothetical ARC solver may take the form of a program synthesis engine that uses the demonstration examples of a task to generate candidates that transform input grids into corresponding output grids."
- Evidence (Section: III.1.4 What a solution to ARC may look like, and what it would imply for AI applications): "Start by developing a domain-specific language (DSL) capable of expressing all possible solution programs for any ARC task."

## 12. Explicit Limitations and Non-Claims
- The paper explicitly labels ARC as incomplete and lists weaknesses:
  - Evidence (Section: III.2 Weaknesses and future refinements): "It is important to note that ARC is a work in progress, not a definitive solution; it does not fit all of the requirements listed in II.3.2, and it features a number of key weaknesses:"
  - Evidence (Section: III.2 Weaknesses and future refinements): "Generalization is not quantified."
  - Evidence (Section: III.2 Weaknesses and future refinements): "Test validity is not established."
  - Evidence (Section: III.2 Weaknesses and future refinements): "Dataset size and diversity may be limited. ARC only features 1,000 tasks in total, and there may be some amount of conceptual overlap across many tasks."
  - Evidence (Section: III.2 Weaknesses and future refinements): "Core Knowledge priors may not be well understood and may not be well captured in ARC."
- Explicit non-claims about what ARC does not attempt to use or assess:
  - Evidence (Section: III.1.3 Key differences with psychometric intelligence tests): "Unlike some psychometric intelligence tests, ARC is not interested in assessing crystallized intelligence or crystallized cognitive abilities. ARC only assesses a general form of fluid intelligence, with a focus on reasoning and abstraction. ARC does not involve language, pictures of real-world objects, or real-world common sense."
