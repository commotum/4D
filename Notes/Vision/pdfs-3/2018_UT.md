## 1. Basic Metadata
Title: Universal Transformers
Authors: Mostafa Dehghani; Stephan Gouws; Oriol Vinyals; Jakob Uszkoreit; Łukasz Kaiser
Year: 2019
Venue: ICLR (conference paper)
Evidence: "Published as a conference paper at ICLR 2019" (Front matter).

## 2. One-Sentence Contribution Summary
The paper introduces the Universal Transformer, a parallel-in-time self-attentive recurrent sequence model with dynamic halting that generalizes the Transformer and improves results across algorithmic and language understanding sequence tasks.

## 3. Tasks Evaluated
### bAbI question answering
Task name: bAbI question answering
Task type: Reasoning / relational; Other (question answering)
Dataset(s): bAbI question answering dataset (Weston et al., 2015)
Domain: English sentences; synthetic tasks
Evidence: "The bAbi question answering dataset (Weston et al., 2015) consists of 20 different synthetic tasks7." (Appendix D.1) "The bAbi question answering dataset (Weston et al., 2015) consists of 20 different tasks, where the goal is to answer a question given a number of English sentences that encode potentially multiple supporting facts." (Section 3.1)

### Subject-verb agreement
Task name: Subject-verb agreement (number agreement)
Task type: Classification; Other (language modeling ranking)
Dataset(s): Linzen et al. (2016) dataset
Domain: English sentences (natural language)
Evidence: "Subject-verb agreement is the task of predicting number agreement between subject and verb in English sentences." (Appendix D.2) "We use the dataset provided by (Linzen et al., 2016)" (Section 3.2).

### LAMBADA language modeling / reading comprehension
Task name: LAMBADA (language modeling; reading comprehension setting)
Task type: Generation; Other (reading comprehension)
Dataset(s): LAMBADA (Paperno et al., 2016)
Domain: Narrative passages (natural language)
Evidence: "The LAMBADA task (Paperno et al., 2016) is a broad context language modeling task. In this task, given a narrative passage, the goal is to predict the last word (target word) of the last sentence (target sentence) in the passage." (Appendix D.3) "The task is evaluated in two settings: as language modeling (the standard setup) and as reading comprehension." (Appendix D.3)

### Algorithmic tasks (Copy, Reverse, Addition)
Task name: Copy; Reverse; (integer) Addition
Task type: Generation; Reconstruction
Dataset(s): Sequences of decimal symbols
Domain: Decimal symbol strings
Evidence: "We trained UTs on three algorithmic tasks, namely Copy, Reverse, and (integer) Addition, all on strings composed of decimal symbols (‘0’-‘9’)." (Section 3.4)

### Learning to Execute (LTE)
Task name: Learning to Execute (program evaluation; memorization)
Task type: Generation; Reasoning / relational
Dataset(s): LTE (Zaremba & Sutskever, 2015)
Domain: Program text (computer programs)
Evidence: "LTE is a set of tasks indicating the ability of a model to learn to execute computer programs and was proposed by Zaremba & Sutskever (2015)." (Appendix D.4) "These tasks include two subsets: 1) program evaluation tasks (program, control, and addition) that are designed to assess the ability of models for understanding numerical operations, if-statements, variable assignments, the compositionality of operations, and more, as well as 2) memorization tasks (copy, double, and reverse)." (Appendix D.4)

### Machine translation
Task name: WMT14 English-German machine translation
Task type: Generation
Dataset(s): WMT 2014 English-German
Domain: Natural language translation
Evidence: "We trained a UT on the WMT 2014 English-German translation task" (Section 3.6).

## 4. Domain and Modality Scope
Single vs multiple domains: Multiple domains are evaluated (algorithmic tasks, language understanding tasks, machine translation).
Evidence: "We evaluated the Universal Transformer on a range of algorithmic and language understanding tasks, as well as on machine translation." (Section 3)
Multiple domains within the same modality: Not specified in the paper.
Multiple modalities: Not specified in the paper.
Domain generalization / cross-domain transfer: Not claimed.

## 5. Model Sharing Across Tasks
| Task | Shared Weights? | Fine-Tuned? | Separate Head? | Evidence |
| --- | --- | --- | --- | --- |
| bAbI question answering | Both reported: train single (no sharing across tasks) and train joint (shared across tasks) | Not specified in the paper. | Not specified in the paper. | "models can either be trained on each task separately (“train single”) or jointly on all tasks (“train joint”)." (Section 3.1) "the original idea is that a single model should be evaluated across all the tasks (not tuning per task), which is the train joint setup" (Appendix D.1). |
| Subject-verb agreement | Not specified in the paper. | Not specified in the paper. | Not specified in the paper. | "Next, we consider the task of predicting number-agreement between subjects and verbs in English sentences" (Section 3.2). |
| LAMBADA | Not specified in the paper. | Not specified in the paper. | Not specified in the paper. | "The LAMBADA task (Paperno et al., 2016) is a broad context language modeling task." (Appendix D.3) |
| Algorithmic tasks (Copy/Reverse/Addition) | Not specified in the paper. | Not specified in the paper. | Not specified in the paper. | "We trained UTs on three algorithmic tasks, namely Copy, Reverse, and (integer) Addition" (Section 3.4). |
| LTE (program evaluation, memorization) | Not specified in the paper. | Not specified in the paper. | Not specified in the paper. | "we also evaluate UTs on tasks indicating the ability of a model to learn to execute computer programs" (Section 3.5). |
| Machine translation (WMT14 En-De) | Not specified in the paper. | Not specified in the paper. | Not specified in the paper. | "We trained a UT on the WMT 2014 English-German translation task" (Section 3.6). |

## 6. Input and Representation Constraints
Input length / embedding dimensionality: "Given an input sequence of length m, we start with a matrix whose rows are initialized as the d-dimensional embeddings of the symbols at each position of the sequence H 0 ∈ Rm×d ." (Section 2.1)
Token/sequence constraints (task-specific): "In all the experiments, we train the models on sequences of length 40 and evaluated on sequences of length 400" (Section 3.4). "with maximum length of 55." (Table 5 caption) "with maximum nesting of 2 and length of 5." (Table 6 caption)
Input symbol constraints (task-specific): "all on strings composed of decimal symbols (‘0’-‘9’)." (Section 3.4)
Decoder input constraint: "During training, the decoder input is the target output, shifted to the right by one position." (Section 2.1)
Fixed/variable input resolution: Not specified in the paper.
Fixed patch size: Not specified in the paper.
Fixed number of tokens (global): Not specified in the paper.
Padding or resizing requirements: Not specified in the paper.

## 7. Context Window and Attention Structure
Maximum sequence length: Not specified globally; task-specific lengths are given (e.g., "In all the experiments, we train the models on sequences of length 40 and evaluated on sequences of length 400" (Section 3.4); "with maximum length of 55." (Table 5 caption); "with maximum nesting of 2 and length of 5." (Table 6 caption)).
Sequence length fixed or variable: Not specified in the paper; the model is described for an "input sequence of length m" (Section 2.1).
Attention type: Global self-attention across the full sequence.
Evidence: "using a self-attention mechanism to exchange information across all positions in the sequence" (Section 2.1).
Causal masking in decoder: "The decoder self-attention distributions are further masked so that the model can only attend to positions to the left of any predicted symbol." (Section 2.1)
Computation management: Dynamic halting/ACT.
Evidence: "Adaptive Computation Time (ACT) (Graves, 2016) is a mechanism for dynamically modulating the number of computational steps needed to process each input symbol" (Section 2.2).

## 8. Positional Encoding (Critical Section)
Mechanism: Fixed, absolute sinusoidal position + time embeddings.
Evidence: "P t ∈ Rm×d above are fixed, constant, two-dimensional (position, time) coordinate embeddings, obtained by computing the sinusoidal position embedding vectors as defined in (Vaswani et al., 2017)" (Section 2.1).
Where applied: Added to the representations before self-attention at each recurrent step.
Evidence: "where A = L AYER N ORM((H              +P )+M ULTI H EAD S ELFATTENTION(H               +P )),    (5)" (Section 2.1).
Fixed vs modified / ablated: The embeddings are described as fixed; no ablations or alternatives are reported.
Evidence: "fixed, constant" (Section 2.1).

## 9. Positional Encoding as a Variable
PE treated as variable? Fixed architectural assumption.
Evidence: "P t ∈ Rm×d above are fixed, constant" (Section 2.1).
Multiple positional encodings compared: Not specified in the paper.
Claim that PE choice is not critical/secondary: Not specified in the paper.

## 10. Evidence of Constraint Masking
Model size(s): Explicit model sizes are not given; comparisons indicate equal parameter counts for some settings.
Evidence: "All base results have the same number of parameters." (Table 7 caption)
Dataset size(s): "There are two versions of the dataset, one with 1k training examples and the other with 10k examples." (Appendix D.1)
Performance gains attributed to architectural/conditional computation changes: "We also add a dynamic per-position halting mechanism (Graves, 2016), allowing the model to choose the required number of refinement steps for each symbol dynamically, and show for the first time that such a conditional computation mechanism can in fact improve accuracy on several smaller, structured algorithmic and linguistic inference tasks (although it marginally degraded results on MT)." (Section 1) "The added recurrence yields improved results in machine translation" (Section 1).
Scaling model size or data as primary driver: Not specified in the paper.

## 11. Architectural Workarounds
Weight sharing across depth: "Weight sharing: Following intuitions behind weight sharing found in CNNs and RNNs, we extend the Transformer with a simple form of weight sharing" (Conclusion).
Conditional computation / dynamic halting: "Conditional computation: In our goal to build a computationally universal machine, we equipped the Universal Transformer with the ability to halt or continue computation through a recently introduced mechanism, which shows stronger results compared to the fixed-depth Universal Transformer." (Conclusion).
Shared transition function: "followed by a transformation (shared across all positions and time-steps) consisting of a depth-wise separable convolution (Chollet, 2016; Kaiser et al., 2017) or a position-wise fully-connected layer (see Fig 1)." (Section 1).
Decoder causal masking: "The decoder self-attention distributions are further masked so that the model can only attend to positions to the left of any predicted symbol." (Section 2.1)
Windowed attention / hierarchical stages / token pooling / task-specific heads / fixed grid assumptions: Not specified in the paper.

## 12. Explicit Limitations and Non-Claims
Limitation: "We also add a dynamic per-position halting mechanism (Graves, 2016), allowing the model to choose the required number of refinement steps for each symbol dynamically, and show for the first time that such a conditional computation mechanism can in fact improve accuracy on several smaller, structured algorithmic and linguistic inference tasks (although it marginally degraded results on MT)." (Section 1)
Future work direction: "we hope that further improvements beyond the basic Universal Transformer presented here will help us build learning algorithms that are both more powerful, data efficient, and generalize beyond the current state-of-the-art." (Conclusion)
Explicit non-claims about open-world learning, unrestrained multi-task learning, or meta-learning: Not specified in the paper.
