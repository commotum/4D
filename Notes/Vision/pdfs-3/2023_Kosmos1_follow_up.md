Number of distinct tasks evaluated: 21
- Task 1: StoryCloze (commonsense reasoning cloze/completion). (Table 1, p.3)
- Task 2: HellaSwag (commonsense NLI cloze/completion). (Table 1, p.3)
- Task 3: Winograd (word ambiguity / Winograd-style). (Table 1, p.3)
- Task 4: Winogrande (word ambiguity / Winograd-style). (Table 1, p.3)
- Task 5: PIQA (physical commonsense reasoning). (Table 1, p.3)
- Task 6: BoolQ (question answering). (Table 1, p.3)
- Task 7: CB (textual entailment). (Table 1, p.3)
- Task 8: COPA (causal reasoning). (Table 1, p.3)
- Task 9: Rendered SST-2 (OCR-free sentiment classification). (Table 1, p.3)
- Task 10: HatefulMemes (OCR-free meme classification). (Table 1, p.3)
- Task 11: RelativeSize (object size commonsense reasoning). (Table 1, p.3)
- Task 12: MemoryColor (object color commonsense reasoning). (Table 1, p.3)
- Task 13: ColorTerms (object color commonsense reasoning). (Table 1, p.3)
- Task 14: Raven IQ Test / Raven's Progressive Matrices (nonverbal reasoning). (Table 1, p.3)
- Task 15: COCO Caption (image captioning). (Table 1, p.3)
- Task 16: Flickr30k (image captioning). (Table 1, p.3)
- Task 17: VQAv2 (visual question answering). (Table 1, p.3)
- Task 18: VizWiz (visual question answering). (Table 1, p.3)
- Task 19: WebSRC (web page question answering). (Table 1, p.3)
- Task 20: ImageNet (zero-shot image classification). (Table 1, p.3)
- Task 21: CUB (zero-shot image classification with descriptions). (Table 1, p.3)

Number of trained model instances required to cover all tasks: 1
- KOSMOS-1 is evaluated across language, perception-language, and vision tasks in zero-shot/few-shot settings without gradient updates or finetuning, indicating a single trained model instance covers all tasks. (Abstract, p.2; Sec. 2, p.5)

$$
\boxed{
\frac{21\ \text{tasks}}{1\ \text{model}} = 21
}
$$
