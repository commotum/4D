<!DOCTYPE html>
<html lang="en" class="no-js">
    <head>
        <meta charset="UTF-8">
        <meta http-equiv="X-UA-Compatible" content="IE=edge">
        <meta name="applicable-device" content="pc,mobile">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        
        
        
            <meta name="robots" content="max-image-preview:large">
            <meta name="access" content="Yes">

        
        <meta name="360-site-verification" content="1268d79b5e96aecf3ff2a7dac04ad990" />

        <title>Adaptive patch selection to improve Vision Transformers through Reinforcement Learning | Applied Intelligence</title>

        
            
    
    <meta name="twitter:site" content="@SpringerLink"/>
    <meta name="twitter:card" content="summary_large_image"/>
    <meta name="twitter:image:alt" content="Content cover image"/>
    <meta name="twitter:title" content="Adaptive patch selection to improve Vision Transformers through Reinforcement Learning"/>
    <meta name="twitter:description" content="Applied Intelligence - In recent years, Transformers have revolutionized the management of Natural Language Processing tasks, and Vision Transformers (ViTs) promise to do the same for Computer..."/>
    <meta name="twitter:image" content="https://static-content.springer.com/image/art%3A10.1007%2Fs10489-025-06516-z/MediaObjects/10489_2025_6516_Fig1_HTML.png"/>
    <meta name="journal_id" content="10489"/>
    <meta name="dc.title" content="Adaptive patch selection to improve Vision Transformers through Reinforcement Learning"/>
    <meta name="dc.source" content="Applied Intelligence 2025 55:7"/>
    <meta name="dc.format" content="text/html"/>
    <meta name="dc.publisher" content="Springer"/>
    <meta name="dc.date" content="2025-04-01"/>
    <meta name="dc.type" content="OriginalPaper"/>
    <meta name="dc.language" content="En"/>
    <meta name="dc.copyright" content="2025 The Author(s)"/>
    <meta name="dc.rights" content="2025 The Author(s)"/>
    <meta name="dc.rightsAgent" content="journalpermissions@springernature.com"/>
    <meta name="dc.description" content="In recent years, Transformers have revolutionized the management of Natural Language Processing tasks, and Vision Transformers (ViTs) promise to do the same for Computer Vision ones. However, the adoption of ViTs is hampered by their computational cost. Indeed, given an image divided into patches, it is necessary to compute for each layer the attention of each patch with respect to all the others. Researchers have proposed many solutions to reduce the computational cost of attention layers by adopting techniques such as quantization, knowledge distillation and manipulation of input images. In this paper, we aim to contribute to the solution of this problem. In particular, we propose a new framework, called AgentViT, which uses Reinforcement Learning to train an agent that selects the most important patches to improve the learning of a ViT. The goal of AgentViT is to reduce the number of patches processed by a ViT, and thus its computational load, while still maintaining competitive performance. We tested AgentViT on CIFAR10, FashionMNIST, and Imagenette $$^+$$ (which is a subset of ImageNet) in the image classification task and obtained promising performance when compared to baseline ViTs and other related approaches available in the literature."/>
    <meta name="prism.issn" content="1573-7497"/>
    <meta name="prism.publicationName" content="Applied Intelligence"/>
    <meta name="prism.publicationDate" content="2025-04-01"/>
    <meta name="prism.volume" content="55"/>
    <meta name="prism.number" content="7"/>
    <meta name="prism.section" content="OriginalPaper"/>
    <meta name="prism.startingPage" content="607"/>
    <meta name="prism.endingPage" content=""/>
    <meta name="prism.copyright" content="2025 The Author(s)"/>
    <meta name="prism.rightsAgent" content="journalpermissions@springernature.com"/>
    <meta name="prism.url" content="https://link.springer.com/article/10.1007/s10489-025-06516-z"/>
    <meta name="prism.doi" content="doi:10.1007/s10489-025-06516-z"/>
    <meta name="citation_pdf_url" content="https://link.springer.com/content/pdf/10.1007/s10489-025-06516-z.pdf"/>
    <meta name="citation_fulltext_html_url" content="https://link.springer.com/article/10.1007/s10489-025-06516-z"/>
    <meta name="citation_journal_title" content="Applied Intelligence"/>
    <meta name="citation_journal_abbrev" content="Appl Intell"/>
    <meta name="citation_publisher" content="Springer US"/>
    <meta name="citation_issn" content="1573-7497"/>
    <meta name="citation_title" content="Adaptive patch selection to improve Vision Transformers through Reinforcement Learning"/>
    <meta name="citation_volume" content="55"/>
    <meta name="citation_issue" content="7"/>
    <meta name="citation_publication_date" content="2025/05"/>
    <meta name="citation_online_date" content="2025/04/01"/>
    <meta name="citation_firstpage" content="607"/>
    <meta name="citation_lastpage" content=""/>
    <meta name="citation_article_type" content="Article"/>
    <meta name="citation_fulltext_world_readable" content=""/>
    <meta name="citation_language" content="en"/>
    <meta name="dc.identifier" content="doi:10.1007/s10489-025-06516-z"/>
    <meta name="DOI" content="10.1007/s10489-025-06516-z"/>
    <meta name="size" content="454770"/>
    <meta name="citation_doi" content="10.1007/s10489-025-06516-z"/>
    <meta name="citation_springer_api_url" content="http://api.springer.com/xmldata/jats?q=doi:10.1007/s10489-025-06516-z&amp;api_key="/>
    <meta name="description" content="In recent years, Transformers have revolutionized the management of Natural Language Processing tasks, and Vision Transformers (ViTs) promise to do the sam"/>
    <meta name="dc.creator" content="Cauteruccio, Francesco"/>
    <meta name="dc.creator" content="Marchetti, Michele"/>
    <meta name="dc.creator" content="Traini, Davide"/>
    <meta name="dc.creator" content="Ursino, Domenico"/>
    <meta name="dc.creator" content="Virgili, Luca"/>
    <meta name="dc.subject" content="Artificial Intelligence"/>
    <meta name="dc.subject" content="Mechanical Engineering"/>
    <meta name="dc.subject" content="Manufacturing, Machines, Tools, Processes"/>
    <meta name="citation_reference" content="citation_journal_title=ACM Comput Surv; citation_title=Recent advances in natural language processing via large pre-trained language models: a survey; citation_author=B Min, H Ross, E Sulem, APB Veyseh, TH Nguyen, O Sainz, E Agirre, I Heintz, D Roth; citation_volume=56; citation_issue=2; citation_publication_date=2023; citation_pages=1-40; citation_doi=10.1145/3605943; citation_id=CR1"/>
    <meta name="citation_reference" content="citation_journal_title=Mach Learn Appl; citation_title=Deep learning in computer vision: a critical review of emerging techniques and application scenarios; citation_author=J Chai, H Zeng, A Li, E Ngai; citation_volume=6; citation_publication_date=2021; citation_pages=100134; citation_id=CR2"/>
    <meta name="citation_reference" content="Vaswani A, Shazeer N, Parmar N, Uszkoreit J, Jones L, Gomez AN, Kaiser &#321;, Polosukhin I (2017) Attention is All you Need. In: Proceedings of the international conference on advances in neural information processing systems (NIPS&#8217;17), Long Beach, CA, USA. Curran Associates, p 30"/>
    <meta name="citation_reference" content="Dosovitskiy A, Beyer L, Kolesnikov A, Weissenborn D, Zhai X, Unterthiner T, Dehghani M, Minderer M, Heigold G, Gelly S, Uszkoreit J, Houlsby N (2021) An image is worth 16x16 words: transformers for image recognition at scale. In: Proceedings of the international conference on learning representations (ICLR&#8217;21), Virtual event. OpenReview.net"/>
    <meta name="citation_reference" content="Waqas M, Tahir MA, Danish M, Al-Maadeed S, Bouridane A, Wu J (2024) Simultaneous instance pooling and bag representation selection approach for multiple-instance learning (MIL) using vision transformer. Neural Comput Appl 1&#8211;22. Springer"/>
    <meta name="citation_reference" content="Poornam S, Angelina J (2024) VITALT: a robust and efficient brain tumor detection system using vision transformer with attention and linear transformation. Neural Comput Appl 1&#8211;17. Springer"/>
    <meta name="citation_reference" content="Child R, Gray S, Radford A, Sutskever I (2019) Generating long sequences with sparse transformers. 
                  arXiv:1904.10509
                  
                "/>
    <meta name="citation_reference" content="Fayyaz M, Koohpayegani SA, Jafari FR, Sengupta S, Joze HRV, Sommerlade E, Pirsiavash H, Gall J (2022) Adaptive token sampling for efficient vision transformers. In: Proceedings of the european conference on computer vision (ECCV&#8217;22), Tel Aviv, Israel. Springer, pp 396&#8211;414"/>
    <meta name="citation_reference" content="Yin H, Vahdat A, Alvarez JM, Mallya A, Kautz J, Molchanov P (2022) A-vit: adaptive tokens for efficient vision transformer. In: Proceedings of the international conference on computer vision and pattern recognition (CVPR&#8217;22), New Orleans, LA, USA, pp 10809&#8211;10818"/>
    <meta name="citation_reference" content="Renggli C, Pinto AS, Houlsby N, Mustafa B, Puigcerver J, Riquelme C (2022) Learning to merge tokens in vision transformers. 
                  arXiv:2202.12015
                  
                "/>
    <meta name="citation_reference" content="Liang Y, Ge C, Tong Z, Song Y, Wang J, Xie P (2022) EViT: expediting vision transformers via token reorganizations. In: Proceedings of the international conference on learning representations (ICLR&#8217;22), Virtual event. OpenReview.net"/>
    <meta name="citation_reference" content="citation_journal_title=J Intell Robot Syst; citation_title=Survey of model-based reinforcement learning: applications on robotics; citation_author=AS Polydoros, L Nalpantidis; citation_volume=86; citation_issue=2; citation_publication_date=2017; citation_pages=153-173; citation_doi=10.1007/s10846-017-0468-y; citation_id=CR12"/>
    <meta name="citation_reference" content="citation_journal_title=Artif Intell Med; citation_title=Reinforcement learning for intelligent healthcare applications: a survey; citation_author=A Coronato, M Naeem, GD Pietro, G Paragliola; citation_volume=109; citation_publication_date=2020; citation_pages=101964; citation_doi=10.1016/j.artmed.2020.101964; citation_id=CR13"/>
    <meta name="citation_reference" content="citation_journal_title=Comput Chem Eng; citation_title=A review on reinforcement learning: introduction and applications in industrial process control; citation_author=R Nian, J Liu, B Huang; citation_volume=139; citation_publication_date=2020; citation_pages=106886; citation_doi=10.1016/j.compchemeng.2020.106886; citation_id=CR14"/>
    <meta name="citation_reference" content="citation_journal_title=IEEE Commun Surv Tutor; citation_title=Applications of deep reinforcement learning in communications and networking: a survey; citation_author=NC Luong, DT Hoang, S Gong, D Niyato, P Wang, YC Liang, DI Kim; citation_volume=21; citation_issue=4; citation_publication_date=2019; citation_pages=3133-3174; citation_doi=10.1109/COMST.2019.2916583; citation_id=CR15"/>
    <meta name="citation_reference" content="citation_journal_title=IEEE Trans Intell Trans Syst; citation_title=Deep reinforcement learning for intelligent transportation systems: a survey; citation_author=A Haydari, Y Y&#305;lmaz; citation_volume=23; citation_issue=1; citation_publication_date=2020; citation_pages=11-32; citation_doi=10.1109/TITS.2020.3008612; citation_id=CR16"/>
    <meta name="citation_reference" content="citation_journal_title=J Artif Intell; citation_title=Survey on the application of deep reinforcement learning in image processing; citation_author=W Fang, L Pang, W Yi; citation_volume=2; citation_issue=1; citation_publication_date=2020; citation_pages=39-58; citation_doi=10.32604/jai.2020.09789; citation_id=CR17"/>
    <meta name="citation_reference" content="Yuan X, Fei H, Baek J (2024) Efficient transformer adaptation with soft token merging. In: Proceedings of the IEEE/CVF conference on computer vision and pattern recognition (CVPR&#8217;24), Seattle, WA, USA, pp 3658&#8211;3668"/>
    <meta name="citation_reference" content="Bolya D, Hoffman J (2023) Token merging for fast stable diffusion. In: Proceedings of the IEEE/CVF conference on computer vision and pattern recognition (CVPR&#8217;23), Vancouver, British Columbia, Canada, pp 4598&#8211;4602"/>
    <meta name="citation_reference" content="Tang Y, Han K, Wang Y, Xu C, Guo J, Xu C, Tao D (2022) Patch slimming for efficient vision transformers. In: Proceedings of the international conference on computer vision and pattern recognition (CVPR&#8217;22), New Orleans, LA, USA, pp 12165&#8211;12174"/>
    <meta name="citation_reference" content="citation_journal_title=Adv Neural Inf Process Syst; citation_title=Dynamicvit: efficient vision transformers with dynamic token sparsification; citation_author=Y Rao, W Zhao, B Liu, J Lu, J Zhou, CJ Hsieh; citation_volume=34; citation_publication_date=2021; citation_pages=13937-13949; citation_id=CR21"/>
    <meta name="citation_reference" content="Rao Y, Liu Z, Zhao W, Zhou J, Lu J (2023) Dynamic spatial sparsification for efficient vision transformers and convolutional neural networks. IEEE Trans Pattern Anal Mach Intell. IEEE"/>
    <meta name="citation_reference" content="Hasselt HV, Guez A, Silver D (2016) Deep reinforcement learning with double q-learning. In: Proceedings of the AAAI conference on artificial intelligence (AAAI&#8217;16), vol. 30. Phoenix, AZ, USA"/>
    <meta name="citation_reference" content="Krizhevsky A, Nair V, Hinton G (2010) CIFAR-10 (Canadian Institute for Advanced Research). 
                  https://www.cs.toronto.edu/~kriz/cifar.html
                  
                "/>
    <meta name="citation_reference" content="Xiao H, Rasul K, Vollgraf R (2017) Fashion-MNIST: a novel image dataset for benchmarking machine learning algorithms. 
                  arXiv:1708.07747
                  
                "/>
    <meta name="citation_reference" content="Deng J, Dong W, Socher R, Li LJ, Kai L, Li FF (2009) ImageNet: a large-scale hierarchical image database. In: Proceedings of the International IEEE Conference on Computer Vision and Pattern Recognition (CVPR&#8217;09), Miami, FL, USA. IEEE, pp 248&#8211;255"/>
    <meta name="citation_reference" content="citation_journal_title=Renew Sustain Energy Rev; citation_title=Applications of reinforcement learning in energy systems; citation_author=ATD Perera, P Kamalaruban; citation_volume=137; citation_publication_date=2021; citation_pages=110618; citation_doi=10.1016/j.rser.2020.110618; citation_id=CR27"/>
    <meta name="citation_reference" content="citation_journal_title=Artif Intell Rev; citation_title=Reinforcement learning applications in environmental sustainability: a review; citation_author=M Zuccotto, A Castellini, DL Torre, L Mola, A Farinelli; citation_volume=57; citation_issue=4; citation_publication_date=2024; citation_pages=88; citation_doi=10.1007/s10462-024-10706-5; citation_id=CR28"/>
    <meta name="citation_reference" content="citation_journal_title=Mach Learn; citation_title=Multi-agent reinforcement learning for fast-timescale demand response of residential loads; citation_author=V Mai, P Maisonneuve, T Zhang, H Nekoei, L Paull, A Lesage-Landry; citation_volume=113; citation_issue=5; citation_publication_date=2024; citation_pages=3355-3355; citation_doi=10.1007/s10994-024-06514-1; citation_id=CR29"/>
    <meta name="citation_reference" content="citation_journal_title=Appl Intell; citation_title=Applications of asynchronous deep reinforcement learning based on dynamic updating weights; citation_author=X Zhao, S Ding, Y An, W Jia; citation_volume=49; citation_publication_date=2019; citation_pages=581-591; citation_doi=10.1007/s10489-018-1296-x; citation_id=CR30"/>
    <meta name="citation_reference" content="citation_journal_title=Appl Intell; citation_title=An effective asynchronous framework for small scale reinforcement learning problems; citation_author=S Ding, X Zhao, X Xu, T Sun, W Jia; citation_volume=49; citation_publication_date=2019; citation_pages=4303-4318; citation_doi=10.1007/s10489-019-01501-9; citation_id=CR31"/>
    <meta name="citation_reference" content="Fan Y, Watkins O, Du Y, Liu H, Ryu M, Boutilier C, Abbeel P, Ghavamzadeh M, Lee K, Lee K (2023) Reinforcement learning for fine-tuning text-to-image diffusion models. In: Proceedings of the annual conference on neural information processing systems (NeurIPS&#8217;23), New Orleans, LA, USA"/>
    <meta name="citation_reference" content="Alrebdi N, Alrumiah S, Almansour A, Rassam M (2022) Reinforcement learning in image classification: a review. In: Proceedings of the international conference on computing and information technology (ICCIT&#8217;22), Tabuk, Saudi Arabia. IEEE, pp 79&#8211;86"/>
    <meta name="citation_reference" content="Jiu M, Song X, Sahbi H, Li S, Chen Y, Guo W, Guo L, Xu M (2024) Image classification with deep reinforcement active learning. 
                  arXiv:2412.19877
                  
                "/>
    <meta name="citation_reference" content="citation_journal_title=Med Image Anal; citation_title=Deep reinforcement learning in medical imaging: a literature review; citation_author=SK Zhou, HN Le, K Luu, HV Nguyen, N Ayache; citation_volume=73; citation_publication_date=2021; citation_pages=102193; citation_doi=10.1016/j.media.2021.102193; citation_id=CR35"/>
    <meta name="citation_reference" content="Gupta SK (2020) Reinforcement based learning on classification task could yield better generalization and adversarial accuracy. In: Proceedings of the annual conference on neural information processing systems workshop on shared visual representations in human and machine intelligence (SVRHM@NeurIPS&#8217;20), Virtual event"/>
    <meta name="citation_reference" content="Uzkent B, Yeh C, Ermon S (2020) Efficient object detection in large images using deep reinforcement learning. In: Proceedings of the IEEE/CVF winter conference on applications of computer vision (WACV&#8217;20), Snowmass Village, CO, USA, pp 1824&#8211;1833"/>
    <meta name="citation_reference" content="citation_journal_title=Eng Appl Artif Intell; citation_title=A tutorial-based survey on feature selection: recent advancements on feature selection; citation_author=A Moslemi; citation_volume=126; citation_publication_date=2023; citation_pages=107136; citation_doi=10.1016/j.engappai.2023.107136; citation_id=CR38"/>
    <meta name="citation_reference" content="citation_journal_title=Swarm Evol Comput; citation_title=Ant-TD: ant colony optimization plus temporal difference reinforcement learning for multi-label feature selection; citation_author=M Paniri, MB Dowlatshahi, H Nezamabadi-pour; citation_volume=64; citation_publication_date=2021; citation_pages=100892; citation_doi=10.1016/j.swevo.2021.100892; citation_id=CR39"/>
    <meta name="citation_reference" content="citation_journal_title=Int J Mach Learn Cybern; citation_title=Reinforced feature selection using Q-learning based on collaborative agents; citation_author=L Zhang, L Jin, M Gan, L Zhao, H Yin; citation_volume=14; citation_issue=11; citation_publication_date=2023; citation_pages=3867-3882; citation_doi=10.1007/s13042-023-01869-8; citation_id=CR40"/>
    <meta name="citation_reference" content="citation_journal_title=IEEE Trans Knowl Data Eng; citation_title=Automated feature selection: a reinforcement learning perspective; citation_author=K Liu, Y Fu, L Wu, X Li, C Aggarwal, H Xiong; citation_volume=35; citation_issue=3; citation_publication_date=2021; citation_pages=2272-2284; citation_id=CR41"/>
    <meta name="citation_reference" content="Wang K, Liu Z, Lin Y, Lin J, Han S (2019) Haq: hardware-aware automated quantization with mixed precision. In: Proceedings of the international conference on computer vision and pattern recognition (CVPR&#8217;19), Long Beach, CA, USA, pp 8612&#8211;8620"/>
    <meta name="citation_reference" content="Gong Y, Liu Y, Yang M, Bourdev L (2014) Compressing deep convolutional networks using vector quantization. 
                  arXiv:1412.6115
                  
                "/>
    <meta name="citation_reference" content="Yuan Z, Xue C, Chen Y, Wu Q, Sun G (2022) Ptq4vit: Post-training quantization for vision transformers with twin uniform quantization. In: Proc. of the European Conference on Computer Vision (ECCV&#8217;22), Tel Aviv, Israel. Springer, pp 191&#8211;207"/>
    <meta name="citation_reference" content="Lin Y, Zhang T, Sun P, Li Z, Zhou S (2022) FQ-ViT: post-training quantization for fully quantized vision transformer. In: Proceedings of the international joint conference on artificial intelligence (IJCAI&#8217;22), Vienna, Austria, pp 1173&#8211;1179"/>
    <meta name="citation_reference" content="Ding Y, Qin H, Yan Q, Chai Z, Liu J, Wei X, Liu X (2022) Towards accurate post-training quantization for vision transformer. In: Proceedings of the international conference on multimedia (MM&#8217;22), Lisbon, Portugal, pp 5380&#8211;5388"/>
    <meta name="citation_reference" content="Li Z, Yang T, Wang P, Cheng J (2022) Q-vit: fully differentiable quantization for vision transformer. 
                  arXiv:2201.07703
                  
                "/>
    <meta name="citation_reference" content="citation_journal_title=Adv Neural Inf Process Syst; citation_title=Post-training quantization for vision transformer; citation_author=Z Liu, Y Wang, K Han, W Zhang, S Ma, W Gao; citation_volume=34; citation_publication_date=2021; citation_pages=28092-28103; citation_id=CR48"/>
    <meta name="citation_reference" content="He Y, Zhang X, Sun J (2017) Channel pruning for accelerating very deep neural networks. In: Proceedings of the international conference on computer vision (ICCV&#8217;17), Venice, Italy, pp 1389&#8211;1397"/>
    <meta name="citation_reference" content="citation_journal_title=IEEE Trans Pattern Anal Mach Intell; citation_title=Runtime network routing for efficient image classification; citation_author=Y Rao, J Lu, J Lin, J Zhou; citation_volume=41; citation_issue=10; citation_publication_date=2018; citation_pages=2291-2304; citation_doi=10.1109/TPAMI.2018.2878258; citation_id=CR50"/>
    <meta name="citation_reference" content="Zhu M, Tang Y, Han K (2021) Vision transformer pruning. 
                  arXiv:2104.08500
                  
                "/>
    <meta name="citation_reference" content="Yu F, Huang K, Wang M, Cheng Y, Chu W, Cui L (2022) Width &amp; depth pruning for vision transformers. In: Proceedings of the international conference on artificial intelligence (AAAI&#8217;22), vol. 36. Virtual event, pp 3143&#8211;3151"/>
    <meta name="citation_reference" content="Yu X, Liu T, Wang X, Tao D (2017) On compressing deep models by low rank and sparse decomposition. In: Proceedings of the international conference on computer vision and pattern recognition (CVPR&#8217;17), Honolulu, HI, USA, pp 7370&#8211;7379"/>
    <meta name="citation_reference" content="Jaderberg M, Vedaldi A, Zisserman A (2014) Speeding up convolutional neural networks with low rank expansions. 
                  arXiv:1405.3866
                  
                "/>
    <meta name="citation_reference" content="Hinton G, Vinyals O, Dean J (2015) Distilling the knowledge in a neural network. 
                  arXiv:1503.02531
                  
                "/>
    <meta name="citation_reference" content="Liu B, Rao Y, Lu J, Zhou J, Hsieh CJ (2020) Metadistiller: network self-boosting via meta-learned top-down distillation. In: Proceedings of the european conference on computer vision (ECCV&#8217;20), Glasgow, Scotland, UK. Springer, pp 694&#8211;709"/>
    <meta name="citation_reference" content="citation_journal_title=Adv Neural Inf Process Syst; citation_title=Minilm: deep self-attention distillation for task-agnostic compression of pre-trained transformers; citation_author=W Wang, F Wei, L Dong, H Bao, N Yang, M Zhou; citation_volume=33; citation_publication_date=2020; citation_pages=5776-5788; citation_id=CR57"/>
    <meta name="citation_reference" content="citation_journal_title=eLife; citation_title=Cell class-specific long-range axonal projections of neurons in mouse whisker-related somatosensory cortices; citation_author=Y Liu, P Bech, K Tamura, LT D&#233;lez, S Crochet, CCH Petersen; citation_volume=13; citation_publication_date=2024; citation_pages=97602; citation_doi=10.7554/eLife.97602.3; citation_id=CR58"/>
    <meta name="citation_reference" content="Chen X, Cao Q, Zhong Y, Zhang J, Gao S, Tao D (2022) Dearkd: data-efficient early knowledge distillation for vision transformers. In: Proceedings of the international conference on computer vision and pattern recognition (CVPR&#8217;22), New Orleans, LA, USA, pp 12052&#8211;12062"/>
    <meta name="citation_reference" content="Touvron H, Cord M, Douze M, Massa F, Sablayrolles A, J&#233;go H (2021) Training data-efficient image transformers &amp; distillation through attention. In: Proceedings of the International Conference on Machine Learning (ICML&#8217;21), Virtual event. PMLR, pp 10347&#8211;10357"/>
    <meta name="citation_reference" content="Jiao X, Yin Y, Shang L, Jiang X, Chen X, Li L, Wang F, Liu Q (2020) TinyBERT: distilling BERT for natural language understanding. In: Findings of the association for computational linguistics: EMNLP&#8217;20. Virtual event. Association for Computational Linguistics, pp 4163&#8211;4174"/>
    <meta name="citation_reference" content="Guo Q, Qiu X, Liu P, Shao Y, Xue X, Zhang Z (2019) Star-transformer. 
                  arXiv:1902.09113
                  
                "/>
    <meta name="citation_reference" content="citation_journal_title=Trans Assoc Comput Ling; citation_title=Efficient content-based sparse attention with routing transformers; citation_author=A Roy, M Saffar, A Vaswani, D Grangier; citation_volume=9; citation_publication_date=2021; citation_pages=53-68; citation_id=CR63"/>
    <meta name="citation_reference" content="Gregor K, Danihelka I, Graves A, Rezende D, Wierstra D (2015) Draw: a recurrent neural network for image generation. In: Proceedings of the international conference on machine learning (ICML&#8217;15), Lille, France. PMLR, pp 1462&#8211;1471"/>
    <meta name="citation_reference" content="citation_journal_title=Comput Biol Med; citation_title=Atlas-guided parcellation: individualized functionally-homogenous parcellation in cerebral cortex; citation_author=Y Li, A Liu, X Fu, MJ Mckeown, ZJ Wang, X Chen; citation_volume=150; citation_publication_date=2022; citation_pages=106078; citation_doi=10.1016/j.compbiomed.2022.106078; citation_id=CR65"/>
    <meta name="citation_reference" content="Grainger R, Paniagua T, Song X, Cuntoor N, Lee MW, Wu T (2023) PaCa-ViT: learning patch-to-cluster attention in vision transformers. In: Proceedings of the IEEE/CVF conference on computer vision and pattern recognition (CVPR&#8217;23), Vancouver, British Columbia, Canada, pp 18568&#8211;18578"/>
    <meta name="citation_reference" content="Kim M, Gao S, Hsu YC, Shen Y, Jin H (2024) Token fusion: bridging the gap between token pruning and token merging. In: Proceedings of the IEEE/CVF winter conference on applications of computer vision (WCAV&#8217;24), Waikoloa, HI, USA, pp 1383&#8211;1392"/>
    <meta name="citation_reference" content="Meng L, Li H, Chen BC, Lan S, Wu Z, Jiang YG, Lim SN (2022) Adavit: adaptive vision transformers for efficient image recognition. In: Proceedings of the international conference on computer vision and pattern recognition (CVPR&#8217;22), New Orleans, LA, USA, pp 12309&#8211;12318"/>
    <meta name="citation_reference" content="Le Lan C, Tu S, Oberman A, Agarwal R, Bellemare MG (2022) On the generalization of representations in reinforcement learning. In: Proceedings of the international conference on artificial intelligence and statistics (AISTATS&#8217;22). Virtual event. PMLR, vol 151, pp 4132&#8211;4157"/>
    <meta name="citation_reference" content="citation_journal_title=IEEE Signal Process Mag; citation_title=Deep reinforcement learning: a brief survey; citation_author=K Arulkumaran, MP Deisenroth, M Brundage, AA Bharath; citation_volume=34; citation_issue=6; citation_publication_date=2017; citation_pages=26-38; citation_doi=10.1109/MSP.2017.2743240; citation_id=CR70"/>
    <meta name="citation_reference" content="citation_journal_title=Mach Learn; citation_title=Q-learning; citation_author=CJCH Watkins, P Dayan; citation_volume=8; citation_publication_date=1992; citation_pages=279-292; citation_id=CR71"/>
    <meta name="citation_reference" content="Fan J, Wang Z, Xie Y, Yang Z (2020) A theoretical analysis of deep Q-learning. In: Proceedings of the international conference on learning for dynamics and control (L4DC&#8217;20), Berkeley, CA, USA. PMLR, pp 486&#8211;489"/>
    <meta name="citation_reference" content="Schulman J, Wolski F, Dhariwal P, Radford A, Klimov O (2017) Proximal policy optimization algorithms. 
                  arXiv:1707.06347
                  
                "/>
    <meta name="citation_reference" content="Mnih V, Badia AP, Mirza M, Graves A, Lillicrap T, Harley T, Silver D, Kavukcuoglu K (2016) Asynchronous methods for deep reinforcement learning. In: Proceedings of the international conference on machine learning and computer application (ICML&#8217;2016), New York, NY, USA. PmLR, pp 1928&#8211;1937"/>
    <meta name="citation_reference" content="De La Fuente N, Guerra DAV (2024) A comparative study of deep reinforcement learning models: DQN vs PPO vs A2C . In: Proceedings of the ACM knowledge discovery and data mining (ACM KDD&#8217;24), Barcelona, Spain"/>
    <meta name="citation_reference" content="Sutton RS, Barto AG (2018) Reinforcement learning: an introduction. MIT Press"/>
    <meta name="citation_reference" content="Huber PJ (1992) Robust estimation of a location parameter. In: Breakthroughs in statistics: methodology and distribution. Springer, pp 492&#8211;518"/>
    <meta name="citation_reference" content="Liu R, Zou J (2018) The effects of memory replay in reinforcement learning. In: Proceedings of the annual allerton conference on communication, control, and computing (Allerton&#8217;18), Monticello, IL, USA. IEEE, pp 478&#8211;485"/>
    <meta name="citation_reference" content="citation_journal_title=Mach Learn; citation_title=Self-improving reactive agents based on reinforcement learning, planning and teaching; citation_author=LJ Lin; citation_volume=8; citation_publication_date=1992; citation_pages=293-321; citation_doi=10.1023/A:1022628806385; citation_id=CR79"/>
    <meta name="citation_reference" content="Even-Dar E, Mansour Y, Bartlett P (2003) Learning rates for Q-learning. J Mach Learn Res 5(1). MIT Press"/>
    <meta name="citation_reference" content="Guo J (2023) Online influence maximization: concept and algorithm. 
                  arXiv:2312.00099
                  
                "/>
    <meta name="citation_reference" content="Chen W, Wang Y, Yuan Y (2013) Combinatorial multi-armed bandit: general framework and applications. In: Proceedings of the international conference on machine learning (ICML 2013). PMLR, pp 151&#8211;159"/>
    <meta name="citation_reference" content="Chen W, Hu W, Li F, Li J, Liu Y, Lu P (2016) Combinatorial multi-armed bandit with general reward functions. Adv Neural Inf Process Syst 29. NeurIPS"/>
    <meta name="citation_reference" content="Beyer L, Zhai X, Kolesnikov A (2022) Better plain ViT baselines for ImageNet-1k. 
                  arXiv:2205.01580
                  
                "/>
    <meta name="citation_reference" content="Haurum JB, Escalera S, Taylor GW, Moeslund TB (2023) Which tokens to use? investigating token reduction in vision transformers. In: Proceedings of the IEEE/CVF international conference on computer vision (ICCV&#8217;23), Paris, France, pp 773&#8211;783"/>
    <meta name="citation_reference" content="Zhang S, Sutton RS (2017) A deeper look at experience replay. 
                  arXiv:1712.01275
                  
                "/>
    <meta name="citation_reference" content="citation_journal_title=IEEE Trans Autom Control; citation_title=A new look at the statistical model identification; citation_author=H Akaike; citation_volume=19; citation_issue=6; citation_publication_date=1974; citation_pages=716-723; citation_doi=10.1109/TAC.1974.1100705; citation_id=CR87"/>
    <meta name="citation_author" content="Cauteruccio, Francesco"/>
    <meta name="citation_author_email" content="fcauteruccio@unisa.it"/>
    <meta name="citation_author_institution" content="DIEM, University of Salerno, Fisciano, Italy"/>
    <meta name="citation_author" content="Marchetti, Michele"/>
    <meta name="citation_author_email" content="m.marchetti@pm.univpm.it"/>
    <meta name="citation_author_institution" content="DII, Polytechnic University of Marche, Ancona, Italy"/>
    <meta name="citation_author" content="Traini, Davide"/>
    <meta name="citation_author_email" content="davide.traini@unimore.it"/>
    <meta name="citation_author_institution" content="CHIMOMO, University of Modena and Reggio Emilia, Modena, Italy"/>
    <meta name="citation_author" content="Ursino, Domenico"/>
    <meta name="citation_author_email" content="d.ursino@univpm.it"/>
    <meta name="citation_author_institution" content="DII, Polytechnic University of Marche, Ancona, Italy"/>
    <meta name="citation_author" content="Virgili, Luca"/>
    <meta name="citation_author_email" content="luca.virgili@univpm.it"/>
    <meta name="citation_author_institution" content="DII, Polytechnic University of Marche, Ancona, Italy"/>
    <meta name="format-detection" content="telephone=no"/>
    <meta name="citation_cover_date" content="2025/05/01"/>
    

            
    
    <meta property="og:url" content="https://link.springer.com/article/10.1007/s10489-025-06516-z"/>
    <meta property="og:type" content="article"/>
    <meta property="og:site_name" content="SpringerLink"/>
    <meta property="og:title" content="Adaptive patch selection to improve Vision Transformers through Reinforcement Learning - Applied Intelligence"/>
    <meta property="og:description" content="In recent years, Transformers have revolutionized the management of Natural Language Processing tasks, and Vision Transformers (ViTs) promise to do the same for Computer Vision ones. However, the adoption of ViTs is hampered by their computational cost. Indeed, given an image divided into patches, it is necessary to compute for each layer the attention of each patch with respect to all the others. Researchers have proposed many solutions to reduce the computational cost of attention layers by adopting techniques such as quantization, knowledge distillation and manipulation of input images. In this paper, we aim to contribute to the solution of this problem. In particular, we propose a new framework, called AgentViT, which uses Reinforcement Learning to train an agent that selects the most important patches to improve the learning of a ViT. The goal of AgentViT is to reduce the number of patches processed by a ViT, and thus its computational load, while still maintaining competitive performance. We tested AgentViT on CIFAR10, FashionMNIST, and Imagenette $$^+$$ + (which is a subset of ImageNet) in the image classification task and obtained promising performance when compared to baseline ViTs and other related approaches available in the literature."/>
    <meta property="og:image" content="https://static-content.springer.com/image/art%3A10.1007%2Fs10489-025-06516-z/MediaObjects/10489_2025_6516_Fig1_HTML.png"/>
    

        

        <meta name="format-detection" content="telephone=no">

        
    
        
    
    
    

    


        <link rel="apple-touch-icon" sizes="180x180" href=/oscar-static/img/favicons/darwin/apple-touch-icon-6ef0829b9c.png>
<link rel="icon" type="image/png" sizes="192x192" href=/oscar-static/img/favicons/darwin/android-chrome-192x192.png>
<link rel="icon" type="image/png" sizes="32x32" href=/oscar-static/img/favicons/darwin/favicon-32x32.png>
<link rel="icon" type="image/png" sizes="16x16" href=/oscar-static/img/favicons/darwin/favicon-16x16.png>
<link rel="shortcut icon" data-test="shortcut-icon" href=/oscar-static/img/favicons/darwin/favicon-de0c289efe.ico>

<meta name="theme-color" content="#e6e6e6">


        



<link rel="stylesheet" media="print" href=/oscar-static/app-springerlink/css/print-b8af42253b.css>



    
        
            
    <style> html{line-height:1.15;text-size-adjust:100%}body{font-family:Merriweather Sans,Helvetica Neue,Helvetica,Arial,sans-serif;line-height:1.8;margin:0}details,main{display:block}h1{font-size:2em;margin:.67em 0}a{background-color:transparent;color:#025e8d}b{font-weight:bolder}sub{bottom:-.25em;font-size:75%;line-height:0;position:relative;vertical-align:baseline}img{border:0;height:auto;max-width:100%;vertical-align:middle}button,input{font-family:inherit;font-size:100%;line-height:1.15;margin:0;overflow:visible}button{text-transform:none}[type=button],[type=submit],button{-webkit-appearance:button}[type=search]{-webkit-appearance:textfield;outline-offset:-2px}summary{display:list-item}[hidden]{display:none}button{cursor:pointer}svg{height:1rem;width:1rem}.eds-c-header__brand img{max-width:100%}@media only screen and (min-width:768px){.eds-c-header__brand img{max-width:340px}} </style>
    <style>@media only print, only all and (prefers-color-scheme: no-preference), only all and (prefers-color-scheme: light), only all and (prefers-color-scheme: dark) {  body{background:#fff;color:#222;font-family:Merriweather Sans,Helvetica Neue,Helvetica,Arial,sans-serif;line-height:1.8;min-height:100%}a{color:#025e8d;padding:initial;text-decoration:underline;text-decoration-skip-ink:auto}button{cursor:pointer}img{border:0;height:auto;max-width:100%;vertical-align:middle}html{box-sizing:border-box;font-size:100%;height:100%;overflow-y:scroll}h1{font-size:2.25rem}h2{font-size:1.75rem}h1,h2,h3{font-weight:700;line-height:1.2}h3{font-size:1.5rem}body{font-size:1.125rem}*{box-sizing:inherit}p{margin-bottom:2rem;margin-top:0}p:last-of-type{margin-bottom:0}.c-ad{text-align:center}@media only screen and (min-width:480px){.c-ad{padding:8px}}.c-ad--728x90{display:none}.c-ad--728x90 .c-ad__inner{min-height:calc(1.5em + 94px)}@media only screen and (min-width:876px){.js .c-ad--728x90{display:none}}.c-ad__label{color:#333;font-family:Merriweather Sans,Helvetica Neue,Helvetica,Arial,sans-serif;font-size:.875rem;font-weight:400;line-height:1.5;margin-bottom:4px}.c-status-message{align-items:center;box-sizing:border-box;display:flex;width:100%}.c-status-message__heading{font-family:Merriweather Sans,Helvetica Neue,Helvetica,Arial,sans-serif;font-size:1rem;font-weight:700}.c-status-message__icon{fill:currentcolor;display:inline-block;flex:0 0 auto;transform:translate(0);vertical-align:text-top}.c-status-message--boxed.c-status-message--info{border-bottom:4px solid #003f8d}.c-status-message--boxed.c-status-message--error{border-bottom:4px solid #c40606}.c-status-message--boxed.c-status-message--success{border-bottom:4px solid #00b8b0}.c-status-message--boxed.c-status-message--warning{border-bottom:4px solid #edbc53}.eds-c-button{border-radius:32px;cursor:pointer;display:inline-block;font-family:Merriweather Sans,Helvetica Neue,Helvetica,Arial,sans-serif;font-size:1rem;font-weight:700;line-height:1.5;margin:0;padding:.5rem 1.5rem;position:relative;text-align:center;text-decoration:none;transition:all .2s ease 0s;width:100%}.eds-c-button span,.eds-c-button svg{vertical-align:middle}.eds-c-button svg{height:1.5rem;width:1.5rem}.eds-c-button svg:last-child{margin-left:8px}@media only screen and (min-width:480px){.eds-c-button{width:auto}}.eds-c-button--primary{background-color:#025e8d;background-image:none;border:2px solid transparent;box-shadow:none;color:#fff;text-decoration:none}.eds-c-button--primary svg,.eds-c-button--secondary svg{fill:currentcolor}.eds-c-button--secondary{background-color:#fff;background-image:none;border:2px solid #025e8d;box-shadow:none;color:#025e8d;text-decoration:none}.eds-c-header{background-color:#fff;border-bottom:2px solid #01324b;font-family:Merriweather Sans,Helvetica Neue,Helvetica,Arial,sans-serif;font-size:1rem;line-height:1.5;padding:8px 0 0}.eds-c-header__container{align-items:center;display:flex;flex-wrap:nowrap;gap:8px 16px;justify-content:space-between;margin:0 auto 8px;max-width:1280px;padding:0 8px;position:relative}.eds-c-header__nav{border-top:2px solid #c5e0f4;padding-top:4px;position:relative}.eds-c-header__nav-container{align-items:center;display:flex;flex-wrap:wrap;margin:0 auto 4px;max-width:1280px;padding:0 8px;position:relative}.eds-c-header__nav-container>:not(:last-child){margin-right:32px}.eds-c-header__link-container{align-items:center;display:flex;flex:1 0 auto;gap:8px 16px;justify-content:space-between}.eds-c-header__list{list-style:none;margin:0;padding:0}.eds-c-header__list-item{font-weight:700;margin:0 auto;max-width:1280px;padding:8px}.eds-c-header__list-item:not(:last-child){border-bottom:2px solid #c5e0f4}.eds-c-header__item{color:inherit}@media only screen and (min-width:768px){.eds-c-header__item--menu{display:none;visibility:hidden}.eds-c-header__item--menu:first-child+*{margin-block-start:0}}.eds-c-header__item--inline-links{display:none;visibility:hidden}@media only screen and (min-width:768px){.eds-c-header__item--inline-links{display:flex;gap:16px 16px;visibility:visible}}.eds-c-header__item--divider:before{border-left:2px solid #c5e0f4;content:"";height:calc(100% - 16px);margin-left:-15px;position:absolute;top:8px}.eds-c-header__brand{padding:16px 8px}.eds-c-header__brand a{display:block;line-height:1;text-decoration:none}.eds-c-header__brand img{height:1.5rem;width:auto}.eds-c-header__link{color:inherit;display:inline-block;font-weight:700;padding:16px 8px;position:relative;text-decoration-color:transparent;white-space:nowrap;word-break:normal}.eds-c-header__icon{fill:currentcolor;display:inline-block;font-size:1.5rem;height:1em;transform:translate(0);vertical-align:bottom;width:1em}.eds-c-header__icon+*{margin-left:8px}.eds-c-header__expander{background-color:#f0f7fc}.eds-c-header__search{display:block;padding:24px 0}@media only screen and (min-width:768px){.eds-c-header__search{max-width:70%}}.eds-c-header__search-container{position:relative}.eds-c-header__search-label{color:inherit;display:inline-block;font-weight:700;margin-bottom:8px}.eds-c-header__search-input{background-color:#fff;border:1px solid #000;padding:8px 48px 8px 8px;width:100%}.eds-c-header__search-button{background-color:transparent;border:0;color:inherit;height:100%;padding:0 8px;position:absolute;right:0}.has-tethered.eds-c-header__expander{border-bottom:2px solid #01324b;left:0;margin-top:-2px;top:100%;width:100%;z-index:10}@media only screen and (min-width:768px){.has-tethered.eds-c-header__expander--menu{display:none;visibility:hidden}}.has-tethered .eds-c-header__heading{display:none;visibility:hidden}.has-tethered .eds-c-header__heading:first-child+*{margin-block-start:0}.has-tethered .eds-c-header__search{margin:auto}.eds-c-header__heading{margin:0 auto;max-width:1280px;padding:16px 16px 0}.eds-c-pagination,.js .eds-c-modal--open{align-items:center;display:flex;justify-content:center}.eds-c-pagination{flex-wrap:wrap;font-family:Merriweather Sans,Helvetica Neue,Helvetica,Arial,sans-serif;font-size:.875rem;gap:16px 0;line-height:1.4;list-style:none;margin:0;padding:32px 0}@media only screen and (min-width:480px){.eds-c-pagination{padding:32px 16px}}.eds-c-pagination__item{margin-right:8px}.eds-c-pagination__item--prev{margin-right:16px}.eds-c-pagination__item--next .eds-c-pagination__link,.eds-c-pagination__item--prev .eds-c-pagination__link{padding:16px 8px}.eds-c-pagination__item--next{margin-left:8px}.eds-c-pagination__item:last-child{margin-right:0}.eds-c-pagination__link{align-items:center;color:#222;cursor:pointer;display:inline-block;font-size:1rem;margin:0;padding:16px 24px;position:relative;text-align:center;transition:all .2s ease 0s}.eds-c-pagination__link:visited{color:#222}.eds-c-pagination__link--disabled{border-color:#555;color:#555;cursor:default}.eds-c-pagination__link--active{background-color:#01324b;background-image:none;border-radius:8px;color:#fff}.eds-c-pagination__link--active:focus,.eds-c-pagination__link--active:hover,.eds-c-pagination__link--active:visited{color:#fff}.eds-c-pagination__link-container{align-items:center;display:flex}.eds-c-pagination__icon{fill:#222;height:1.5rem;width:1.5rem}.eds-c-pagination__icon--disabled{fill:#555}.eds-c-pagination__visually-hidden{border:0;clip:rect(0,0,0,0);clip-path:inset(50%);height:1px;overflow:hidden;padding:0;position:absolute!important;white-space:nowrap;width:1px}.c-breadcrumbs{color:#333;font-family:Merriweather Sans,Helvetica Neue,Helvetica,Arial,sans-serif;font-size:1rem;list-style:none;margin:0;padding:0}.c-breadcrumbs>li{display:inline}svg.c-breadcrumbs__chevron{margin:0 .25rem;fill:#333;height:10px;width:10px}.c-breadcrumbs--contrast,.c-breadcrumbs--contrast .c-breadcrumbs__link,.c-breadcrumbs--contrast .c-breadcrumbs__link.hover,.c-breadcrumbs--contrast .c-breadcrumbs__link.visited,.c-breadcrumbs--contrast .c-breadcrumbs__link:hover,.c-breadcrumbs--contrast .c-breadcrumbs__link:visited{color:#fff}.c-breadcrumbs--contrast svg.c-breadcrumbs__chevron{fill:#fff}@media only screen and (max-width:479px){.c-breadcrumbs .c-breadcrumbs__item{display:none}.c-breadcrumbs .c-breadcrumbs__item:last-child,.c-breadcrumbs .c-breadcrumbs__item:nth-last-child(2){display:inline}}.c-header__link{color:inherit;display:inline-block;font-weight:700;padding:16px 8px;position:relative;text-decoration-color:transparent;white-space:nowrap;word-break:normal}.l-with-sidebar{display:flex;flex-wrap:wrap}.l-with-sidebar>*{margin:0}.l-with-sidebar__sidebar{flex-basis:var(--with-sidebar--basis,400px);flex-grow:1}.l-with-sidebar>:not(.l-with-sidebar__sidebar){flex-basis:0px;flex-grow:999;min-width:var(--with-sidebar--min,52%)}.l-with-sidebar>:first-child{padding-right:4rem}@supports (gap:1em){.l-with-sidebar>:first-child{padding-right:0}.l-with-sidebar{gap:var(--with-sidebar--gap,4rem)}}.c-meta__link:visited,.c-status-message a{color:#000}.c-skip-link,.js .c-popup{font-family:Merriweather Sans,Helvetica Neue,Helvetica,Arial,sans-serif;position:absolute}.c-skip-link{background:#01324b;bottom:auto;color:#fff;font-size:1rem;padding:8px;text-align:center;transform:translateY(-100%);width:100%;z-index:9999}@media (prefers-reduced-motion:reduce){.c-skip-link{transition:top .3s ease-in-out 0s}}@media print{.c-skip-link{display:none}}.c-skip-link:active,.c-skip-link:hover,.c-skip-link:link,.c-skip-link:visited{color:#fff}.c-skip-link:focus{transform:translateY(0)}.c-status-message{font-family:Merriweather Sans,Helvetica Neue,Helvetica,Arial,sans-serif;font-weight:400;position:relative}.c-status-message :last-child{margin-bottom:0}.c-status-message--bold{font-weight:700}.c-status-message--boxed{background-color:#fff;border:1px solid #dadada;box-shadow:0 0 5px 0 rgba(51,51,51,.1);line-height:1.4;overflow:hidden;padding:16px}.c-status-message--banner{background-color:#fff;line-height:1.4;padding:16px 0}.c-status-message--banner .c-status-message__container{justify-content:center;padding:0 16px}.c-status-message--sticky{position:sticky;top:0;z-index:999}.c-status-message__container{align-items:center;display:flex;justify-content:flex-start}.c-status-message__icon{align-self:flex-start;flex-shrink:0;height:21px;margin-right:8px;width:21px}.c-status-message__message :first-child,.c-status-message__message :last-child{margin-top:0}.c-status-message__icon--top{align-self:flex-start}.c-status-message--info .c-status-message__icon{color:#0070a8}.c-status-message--banner.c-status-message--info{border-bottom:4px solid #0070a8}.c-status-message--boxed.c-status-message--info .c-status-message__bottom-border{background:#0070a8;bottom:0;content:"";height:4px;left:0;position:absolute;width:100%}.c-status-message--info .c-status-message__icon{fill:#0070a8}.c-status-message--error .c-status-message__icon{color:#be1818}.c-status-message--banner.c-status-message--error{border-bottom:4px solid #be1818}.c-status-message--boxed.c-status-message--error .c-status-message__bottom-border{background:#be1818;bottom:0;content:"";height:4px;left:0;position:absolute;width:100%}.c-status-message--error .c-status-message__icon{fill:#be1818}.c-status-message--success .c-status-message__icon{color:#00a69d}.c-status-message--banner.c-status-message--success{border-bottom:4px solid #00a69d}.c-status-message--boxed.c-status-message--success .c-status-message__bottom-border{background:#00a69d;bottom:0;content:"";height:4px;left:0;position:absolute;width:100%}.c-status-message--success .c-status-message__icon{fill:#00a69d}.c-status-message--warning .c-status-message__icon{color:#f58220}.c-status-message--banner.c-status-message--warning{border-bottom:4px solid #f58220}.c-status-message--boxed.c-status-message--warning .c-status-message__bottom-border{background:#f58220;bottom:0;content:"";height:4px;left:0;position:absolute;width:100%}.c-status-message--warning .c-status-message__icon{fill:#f58220}:root{--header-height:58px}.app-masthead__colour-4{--background-color:#ff9500;--gradient-light:rgba(0,0,0,.5);--gradient-dark:rgba(0,0,0,.8)}.app-masthead--pastel{--gradient-light:hsla(0,0%,100%,.9);--gradient-dark:hsla(0,0%,100%,.75);--masthead-color:#000}.app-masthead{background:var(--background-color,#0070a8);position:relative}.app-masthead:after{background:radial-gradient(circle at top right,var(--gradient-light,rgba(0,0,0,.4)),var(--gradient-dark,rgba(0,0,0,.7)));bottom:0;content:"";left:0;position:absolute;right:0;top:0}.app-masthead--pastel .app-masthead{background:var(--background-color,#6ac)}@media only screen and (max-width:479px){.app-masthead:after{background:linear-gradient(225deg,var(--gradient-light,rgba(0,0,0,.4)),var(--gradient-dark,rgba(0,0,0,.7)))}}.app-masthead__container{color:var(--masthead-color,#fff);margin:0 auto;max-width:1280px;padding:0 16px;position:relative;z-index:1}.u-clear-both{clear:both}.u-container{margin:0 auto;max-width:1280px;padding:0 16px}.u-justify-content-space-between{justify-content:space-between}.u-display-none{display:none}.js .u-js-hide,.u-hide{display:none;visibility:hidden}.u-visually-hidden{border:0;clip:rect(0,0,0,0);clip-path:inset(50%);height:1px;overflow:hidden;padding:0;position:absolute!important;white-space:nowrap;width:1px}.u-icon{fill:currentcolor;display:inline-block;height:1em;transform:translate(0);vertical-align:text-top;width:1em}.u-list-reset{list-style:none;margin:0;padding:0}.u-ma-16{margin:16px}.u-mt-0{margin-top:0}.u-mt-24{margin-top:24px}.u-mt-32{margin-top:32px}.u-mb-8{margin-bottom:8px}.u-mb-32{margin-bottom:32px}.c-article-extras .c-pdf-container .c-pdf-download+.c-pdf-download,.u-ml-0{margin-left:0}.u-sans-serif{font-family:Merriweather Sans,Helvetica Neue,Helvetica,Arial,sans-serif}.u-serif{font-family:Merriweather,serif}h1,h2,h3{-webkit-font-smoothing:antialiased}p{overflow-wrap:break-word;word-break:break-word}.u-h4{font-size:1.25rem;font-weight:700;line-height:1.2}.u-mbs-0{margin-block-start:0!important}.c-article-header{font-family:Merriweather Sans,Helvetica Neue,Helvetica,Arial,sans-serif}.c-article-identifiers{color:#6f6f6f;display:flex;flex-wrap:wrap;font-size:1rem;line-height:1.3;list-style:none;padding:0}.c-article-identifiers__item{list-style:none;margin-right:8px;padding-right:8px}.c-article-identifiers__item:last-child{margin-right:0;padding-right:0}@media only screen and (min-width:876px){.c-article-title{font-size:1.875rem;line-height:1.2}}.c-article-author-list{display:inline;font-size:1rem;list-style:none;margin:0 8px 0 0;padding:0;width:100%}.c-article-author-list__item{display:inline;padding-right:0}.c-article-author-list__show-more{display:none;margin-right:4px}.c-article-author-list__button,.js .c-article-author-list__item--hide,.js .c-article-author-list__show-more{display:none}.js .c-article-author-list--long .c-article-author-list__show-more,.js .c-article-author-list--long+.c-article-author-list__button{display:inline}@media only screen and (max-width:767px){.js .c-article-author-list__item--hide-small-screen{display:none}.js .c-article-author-list--short .c-article-author-list__show-more,.js .c-article-author-list--short+.c-article-author-list__button{display:inline}}#uptodate-client,.js .c-article-author-list--expanded .c-article-author-list__show-more{display:none!important}.js .c-article-author-list--expanded .c-article-author-list__item--hide-small-screen{display:inline!important}.c-article-author-list__button,.c-button-author-list{background:#ebf1f5;border:4px solid #ebf1f5;border-radius:20px;color:#666;font-size:.875rem;line-height:1.4;padding:2px 11px 2px 8px;text-decoration:none}.c-article-author-list__button svg,.c-button-author-list svg{margin:1px 4px 0 0}.c-article-author-list__button:hover,.c-button-author-list:hover{background:#025e8d;border-color:transparent;color:#fff}.c-article-body .c-article-access-provider{padding:8px 16px}.c-article-body .c-article-access-provider,.c-notes{border:1px solid #d5d5d5;border-image:initial;border-left:none;border-right:none;margin:24px 0}.c-article-body .c-article-access-provider__text{color:#555}.c-article-body .c-article-access-provider__text,.c-notes__text{font-size:1rem;margin-bottom:0;padding-bottom:2px;padding-top:2px;text-align:center}.c-article-body .c-article-author-affiliation__address{color:inherit;font-weight:700;margin:0}.c-article-body .c-article-author-affiliation__authors-list{list-style:none;margin:0;padding:0}.c-article-body .c-article-author-affiliation__authors-item{display:inline;margin-left:0}.c-article-authors-search{margin-bottom:24px;margin-top:0}.c-article-authors-search__item,.c-article-authors-search__title{font-family:Merriweather Sans,Helvetica Neue,Helvetica,Arial,sans-serif}.c-article-authors-search__title{color:#626262;font-size:1.05rem;font-weight:700;margin:0;padding:0}.c-article-authors-search__item{font-size:1rem}.c-article-authors-search__text{margin:0}.c-article-body .c-article-subject-list--no-mb{margin-bottom:0}.c-code-block{border:1px solid #fff;font-family:monospace;margin:0 0 24px;padding:20px}.c-code-block__heading{font-weight:400;margin-bottom:16px}.c-code-block__line{display:block;overflow-wrap:break-word;white-space:pre-wrap}.c-article-share-box{font-family:Merriweather Sans,Helvetica Neue,Helvetica,Arial,sans-serif;margin-bottom:24px}.c-article-share-box__description{font-size:1rem;margin-bottom:8px}.c-article-share-box__no-sharelink-info{font-size:.813rem;font-weight:700;margin-bottom:24px;padding-top:4px}.c-article-share-box__only-read-input{border:1px solid #d5d5d5;box-sizing:content-box;display:inline-block;font-size:.875rem;font-weight:700;height:24px;margin-bottom:8px;padding:8px 10px}.c-article-share-box__additional-info{color:#626262;font-size:.813rem}.c-article-share-box__button{background:#fff;box-sizing:content-box;text-align:center}.c-article-share-box__button--link-like{background-color:transparent;border:0;color:#025e8d;cursor:pointer;font-size:.875rem;margin-bottom:8px;margin-left:10px}.c-article-associated-content__container .c-article-associated-content__collection-label{font-size:.875rem;line-height:1.4}.c-article-associated-content__container .c-article-associated-content__collection-title{line-height:1.3}.c-reading-companion{clear:both;min-height:389px}.c-reading-companion__figures-list,.c-reading-companion__references-list{list-style:none;min-height:389px;padding:0}.c-reading-companion__references-list--numeric{list-style:decimal inside}.c-reading-companion__figure-item{border-top:1px solid #d5d5d5;font-size:1rem;padding:16px 8px 16px 0}.c-reading-companion__figure-item:first-child{border-top:none;padding-top:8px}.c-reading-companion__reference-item{font-size:1rem}.c-reading-companion__reference-item:first-child{border-top:none}.c-reading-companion__reference-item a{word-break:break-word}.c-reading-companion__reference-citation{display:inline}.c-reading-companion__reference-links{font-size:.813rem;font-weight:700;list-style:none;margin:8px 0 0;padding:0;text-align:right}.c-reading-companion__reference-links>a{display:inline-block;padding-left:8px}.c-reading-companion__reference-links>a:first-child{display:inline-block;padding-left:0}.c-reading-companion__figure-title{display:block;font-size:1.25rem;font-weight:700;line-height:1.2;margin:0 0 8px}.c-reading-companion__figure-links{display:flex;justify-content:space-between;margin:8px 0 0}.c-reading-companion__figure-links>a{align-items:center;display:flex}.c-article-section__figure-caption{display:block;margin-bottom:8px;word-break:break-word}.c-article-section__figure .video,p.app-article-masthead__access--above-download{margin:0 0 16px}.c-cod{display:block;font-size:1rem;width:100%}.c-cod__form{background:#ebf0f3}.c-cod__prompt{font-size:1.125rem;line-height:1.3;margin:0 0 24px}.c-cod__label{display:block;margin:0 0 4px}.c-cod__row{display:flex;margin:0 0 16px}.c-cod__row:last-child{margin:0}.c-cod__input{border:1px solid #d5d5d5;border-radius:2px;flex:1 1 auto;margin:0;padding:13px}.c-cod__input--submit{background-color:#025e8d;border:1px solid #025e8d;color:#fff;flex-shrink:1;margin-left:8px;transition:background-color .2s ease-out 0s,color .2s ease-out 0s}.c-cod__input--submit-single{flex-basis:100%;flex-shrink:0;margin:0}.c-cod__input--submit:focus,.c-cod__input--submit:hover{background-color:#fff;color:#025e8d}.save-data .c-article-author-institutional-author__sub-division,.save-data .c-article-equation__number,.save-data .c-article-figure-description,.save-data .c-article-fullwidth-content,.save-data .c-article-main-column,.save-data .c-article-satellite-article-link,.save-data .c-article-satellite-subtitle,.save-data .c-article-table-container,.save-data .c-blockquote__body,.save-data .c-code-block__heading,.save-data .c-reading-companion__figure-title,.save-data .c-reading-companion__reference-citation,.save-data .c-site-messages--nature-briefing-email-variant .serif,.save-data .c-site-messages--nature-briefing-email-variant.serif,.save-data .serif,.save-data .u-serif,.save-data h1,.save-data h2,.save-data h3{font-family:Merriweather Sans,Helvetica Neue,Helvetica,Arial,sans-serif}.c-pdf-download__link{display:flex;flex:1 1 0%;padding:13px 24px}.c-pdf-download__link:hover{text-decoration:none}.c-pdf-container{display:flex;justify-content:flex-end}@media only screen and (max-width:767px){.c-pdf-container .c-pdf-download{display:flex;flex-basis:100%}}.c-pdf-container--flex-start{justify-content:flex-start}.c-pdf-container .c-pdf-download+.c-pdf-download{margin-left:16px}.c-article-extras .c-pdf-container{flex-wrap:wrap;width:100%}.c-article-extras .c-pdf-container .c-pdf-download{width:100%}@media only screen and (min-width:768px){.c-context-bar--sticky .c-pdf-download__link{align-items:center;flex:1 1 183px}}@media only screen and (max-width:320px){.c-context-bar--sticky .c-pdf-download__link{padding:16px}}.article-page--commercial .c-pdf-container{display:block}.c-article-body .c-article-recommendations-list,.c-book-body .c-article-recommendations-list{display:flex;flex-direction:row;gap:16px 16px;margin:0;max-width:100%;padding:16px 0 0}.c-article-body .c-article-recommendations-list__item,.c-book-body .c-article-recommendations-list__item{flex:1 1 0%}@media only screen and (max-width:767px){.c-article-body .c-article-recommendations-list,.c-book-body .c-article-recommendations-list{flex-direction:column}}.c-article-body .c-article-recommendations-card__authors{display:none;font-family:Merriweather Sans,Helvetica Neue,Helvetica,Arial,sans-serif;font-size:.875rem;line-height:1.5;margin:0 0 8px}@media only screen and (max-width:767px){.c-article-body .c-article-recommendations-card__authors{display:block;margin:0}}.c-article-body .c-article-history{margin-top:24px}.app-article-metrics-bar p{margin:0}.app-article-masthead{display:flex;flex-direction:column;gap:16px 16px;padding:16px 0 24px}.app-article-masthead__info{display:flex;flex-direction:column;flex-grow:1}.app-article-masthead__brand{border-top:1px solid hsla(0,0%,100%,.8);display:flex;flex-direction:column;flex-shrink:0;gap:8px 8px;min-height:96px;padding:16px 0 0}.app-article-masthead__brand img{border:1px solid #fff;border-radius:8px;box-shadow:0 4px 15px 0 hsla(0,0%,50%,.25);height:auto;left:0;position:absolute;width:72px}.app-article-masthead__journal-link{display:block;font-size:1.125rem;font-weight:700;margin:0 0 8px;max-width:400px;padding:0 0 0 88px;position:relative}.app-article-masthead__journal-title{display:-webkit-box;overflow:hidden;-webkit-box-orient:vertical;-webkit-line-clamp:3}.app-article-masthead__submission-link{align-items:center;display:flex;font-size:1rem;gap:4px 4px;margin:0 0 0 88px}.app-article-masthead__access{align-items:center;display:flex;flex-wrap:wrap;font-size:.875rem;font-weight:300;gap:4px 4px;line-height:1.4;margin:0}.app-article-masthead__buttons{display:flex;flex-flow:column wrap;gap:16px 16px}.app-article-masthead__access svg{fill:currentcolor}.app-article-masthead a{color:#fff}.app-article-masthead a.c-pdf-download__link,.app-article-masthead__syndicated-card a,.app-article-masthead__syndicated-card a:visited,.app-masthead--pastel .app-article-masthead .c-article-identifiers *,.app-masthead--pastel .app-article-masthead .c-article-identifiers a:focus,.app-masthead--pastel .app-article-masthead .c-article-identifiers a:hover,.app-masthead--pastel .app-article-masthead a,.app-masthead--pastel .app-article-masthead a:visited{color:#000}.app-masthead--pastel .app-article-masthead .c-article-identifiers__item{border-left:1px solid #000}.app-masthead--pastel .c-pdf-download a.u-button--primary,.c-context-bar--sticky .c-context-bar__container .c-pdf-download a.u-button--primary{background-color:#025e8d;border:2px solid transparent;box-shadow:none;color:#fff;font-weight:700}.app-masthead--pastel .c-pdf-download a.u-button--primary:focus,.app-masthead--pastel .c-pdf-download a.u-button--primary:hover,.c-context-bar--sticky .c-context-bar__container .c-pdf-download a.u-button--primary:focus,.c-context-bar--sticky .c-context-bar__container .c-pdf-download a.u-button--primary:hover{background:0 0;border:2px solid #025e8d;box-shadow:none;color:#025e8d}.app-masthead--pastel .c-pdf-download a.u-button--secondary,.c-context-bar--sticky .c-context-bar__container .c-pdf-download a.u-button--secondary{background:0 0;border:2px solid #025e8d;color:#025e8d}.app-masthead--pastel .c-pdf-download a.u-button--secondary:focus,.app-masthead--pastel .c-pdf-download a.u-button--secondary:hover,.c-context-bar--sticky .c-context-bar__container .c-pdf-download a.u-button--secondary:focus,.c-context-bar--sticky .c-context-bar__container .c-pdf-download a.u-button--secondary:hover{background-color:#025e8d;border:2px solid transparent;color:#fff}@media only screen and (min-width:768px){.app-article-masthead{flex-direction:row;gap:64px 64px;padding:24px 0}.app-article-masthead__brand{border:0;padding:0}.app-article-masthead__brand img{height:auto;position:static;width:auto}.app-article-masthead__buttons{align-items:normal;margin-top:auto}.app-article-masthead__journal-link{display:flex;flex-direction:column;gap:24px 24px;margin:0 0 8px;padding:0}.app-article-masthead__submission-link{margin:0}.app-article-masthead .c-pdf-container{flex-grow:0}}@media only screen and (min-width:1024px){.app-article-masthead__brand{flex-basis:400px}}.app-article-masthead__buttons .c-pdf-container{justify-content:flex-start}.app-article-masthead .c-article-identifiers{font-size:.875rem;font-weight:300;line-height:1;margin:0 0 8px;overflow:hidden;padding:0}.app-article-masthead .c-article-identifiers--cite-list{margin:0 0 16px}.app-article-masthead .c-article-identifiers *{color:#fff}.app-article-masthead .c-cod{display:none}.app-article-masthead .c-article-identifiers__item{border-left:1px solid #fff;border-right:0;margin:0 17px 8px -9px;padding:0 0 0 8px}.app-article-masthead .c-article-identifiers__item--cite{border-left:0}.app-article-masthead__access-container{align-items:center;display:flex;flex-wrap:wrap;gap:16px 36px;justify-content:center}@media only screen and (min-width:480px){.app-article-masthead__access-container{justify-content:normal}}.app-article-masthead__access-container>*{flex:1 1 auto}@media only screen and (min-width:480px){.app-article-masthead__access-container>*{flex:0 1 auto}}.app-article-metrics-bar{display:flex;flex-wrap:wrap;font-size:1rem;padding:16px 0 0;row-gap:24px}.app-article-metrics-bar__item{padding:0 16px 0 0}.app-article-metrics-bar__count{font-weight:700}.app-article-metrics-bar__label{font-weight:400;padding-left:4px}.app-article-metrics-bar__icon{height:auto;margin-right:4px;margin-top:-4px;width:auto}.app-article-metrics-bar__arrow-icon{margin:4px 0 0 4px}.app-article-metrics-bar a{color:#000}.app-article-metrics-bar .app-article-metrics-bar__item--metrics{padding-right:0}.app-overview-section .c-article-author-list,.app-overview-section__authors{line-height:2}.app-article-metrics-bar{margin-top:8px}.c-book-toc-pagination+.c-book-section__back-to-top{margin-top:0}.c-article-body .c-article-access-provider__text--chapter{color:#222;font-family:Merriweather Sans,Helvetica Neue,Helvetica,Arial,sans-serif;padding:20px 0}.c-article-body .c-article-access-provider__text--chapter svg.c-status-message__icon{fill:#003f8d;vertical-align:middle}.c-article-body-section__content--separator{padding-top:40px}.c-pdf-download__link{max-height:44px}@media only screen and (max-width:767px){.app-article-masthead--book__info .c-pdf-container,.app-article-masthead__info .c-pdf-container{flex-direction:column;gap:12px 12px}.app-article-masthead--book__info .c-pdf-container .c-pdf-download+.c-pdf-download,.app-article-masthead__info .c-pdf-container .c-pdf-download+.c-pdf-download{margin:0}}.app-article-access .u-button--primary,.app-article-access .u-button--primary:visited{color:#fff}.c-article-authors-search__list{align-items:center;display:flex;flex-wrap:wrap;gap:16px 16px;justify-content:center}@media only screen and (min-width:480px){.c-article-authors-search__list{justify-content:normal}}.c-article-authors-search__text{align-items:center;display:flex;flex-flow:column wrap;font-size:14px;justify-content:center}@media only screen and (min-width:480px){.c-article-authors-search__text{flex-direction:row;font-size:16px}}.c-article-authors-search__links-text{font-weight:700;margin-right:8px;text-align:center}@media only screen and (min-width:480px){.c-article-authors-search__links-text{text-align:left}}.c-article-authors-search__list-item--left{flex:1 1 100%}@media only screen and (min-width:480px){.c-article-authors-search__list-item--left{flex-basis:auto}}.c-article-authors-search__list-item--right{flex:1 1 auto}.c-article-identifiers{margin:0}.c-article-identifiers__item{border-right:2px solid #cedbe0;color:#222;font-size:14px}@media only screen and (min-width:480px){.c-article-identifiers__item{font-size:16px}}.c-article-identifiers__item:last-child{border-right:none}.c-article-body .app-article-access p,.c-article-body .app-explore-related-subjects__list--no-mb{margin-bottom:0}.c-spp-access-message .c-status-message__icon{color:#00a69d;margin-top:8px}.c-article-sidebar{display:none}@media only screen and (min-width:1024px){.c-article-sidebar{display:block}}.c-cod__form{border-radius:12px}.c-cod__label{font-size:.875rem}.c-cod .c-status-message{align-items:center;justify-content:center;margin-bottom:16px;padding-bottom:16px}@media only screen and (min-width:1024px){.c-cod .c-status-message{align-items:inherit}}.c-cod .c-status-message__icon{margin-top:4px}.c-cod .c-cod__prompt{font-size:1rem;margin-bottom:16px}.c-article-body .app-article-access,.c-book-body .app-article-access{display:block}@media only screen and (min-width:1024px){.c-article-body .app-article-access,.c-book-body .app-article-access{display:none}}.c-article-body .app-card-service{margin-bottom:32px}@media only screen and (min-width:1024px){.c-article-body .app-card-service{display:none}}.app-article-access .buybox__buy .u-button--secondary,.app-article-access .u-button--primary,.c-cod__row .u-button--primary{background-color:#025e8d;border:2px solid #025e8d;box-shadow:none;font-size:1rem;font-weight:700;gap:8px 8px;justify-content:center;line-height:1.4;padding:8px 24px}.app-article-access .buybox__buy .u-button--secondary,.app-article-access .u-button--primary:hover,.c-cod__row .u-button--primary:hover{background-color:#fff;color:#025e8d}.app-article-access .buybox__buy .u-button--secondary:hover{background-color:#025e8d;color:#fff}.buybox__buy .c-notes__text{color:#666;font-size:.875rem;padding:0 16px 8px}.c-cod__input{flex-basis:auto;width:100%}.c-article-title{font-family:Merriweather Sans,Helvetica Neue,Helvetica,Arial,sans-serif;font-size:2.25rem;font-weight:700;line-height:1.2;margin:12px 0}.c-reading-companion__figure-item figure{margin:0}@media only screen and (min-width:768px){.c-article-title{margin:16px 0}}.app-article-access{border:1px solid #cedbe0;border-radius:12px;margin:0 0 32px}.app-article-access__heading{border-bottom:1px solid #cedbe0;font-family:Merriweather Sans,Helvetica Neue,Helvetica,Arial,sans-serif;font-size:1.125rem;font-weight:700;margin:0;padding:16px;text-align:center}@media only screen and (min-width:1024px){.app-article-access{margin:0 0 24px}}.c-status-message{font-size:1rem}.c-article-body{font-size:1.125rem}.c-article-body dl,.c-article-body ol,.c-article-body p,.c-article-body ul{margin-bottom:32px;margin-top:0}.c-article-access-provider__text:last-of-type,.c-article-body .c-notes__text:last-of-type{margin-bottom:0}.c-article-body ol p,.c-article-body ul p{margin-bottom:16px}.c-article-section__figure-caption{font-family:Merriweather Sans,Helvetica Neue,Helvetica,Arial,sans-serif}.c-reading-companion__figure-item{border-top-color:#cedbe0}.c-reading-companion__sticky{max-width:400px}.c-reading-companion__reference-item{border-top:1px solid #d5d5d5;padding:16px 0}.c-reading-companion__reference-item:first-child{padding-top:0}.c-article-share-box__button,.js .c-article-authors-search__item .c-article-button{background:0 0;border:2px solid #025e8d;border-radius:32px;box-shadow:none;color:#025e8d;font-size:1rem;font-weight:700;line-height:1.4;margin:0;padding:8px 24px;transition:all .2s ease 0s}.c-article-authors-search__item .c-article-button{width:100%}.c-pdf-download .c-pdf-download__link{align-items:center;background-color:#fff;border:2px solid #fff;border-radius:32px;box-shadow:none;color:#01324b;cursor:pointer;font-family:Merriweather Sans,Helvetica Neue,Helvetica,Arial,sans-serif;font-size:1rem;font-weight:700;justify-content:center;line-height:1.4;padding:8px 16px;text-decoration:none}.c-context-bar__container .c-pdf-download .c-pdf-download__link{background-color:#025e8d;background-image:none;border:2px solid #025e8d;box-shadow:none;color:#fff;font-size:1rem;font-weight:700;line-height:1.4;padding:8px 16px}@media only screen and (min-width:768px){.c-context-bar__container .c-pdf-download .c-pdf-download__link,.c-pdf-download .c-pdf-download__link{padding:8px 24px}}.c-pdf-download .c-pdf-download__link:hover{background:0 0;border:2px solid #fff;box-shadow:none;color:#fff}.c-pdf-download .c-pdf-download__link:focus{background:0 0;box-shadow:none;color:#fff}.c-context-bar__container .c-pdf-download .c-pdf-download__link:hover{border:2px solid #025e8d;box-shadow:none;color:#025e8d}.c-context-bar__container .c-pdf-download .c-pdf-download__link:focus,.c-pdf-download .c-pdf-download__link:focus{border:2px solid #025e8d}.c-article-share-box__button:focus:focus,.c-article__pill-button:focus:focus,.c-context-bar__container .c-pdf-download .c-pdf-download__link:focus:focus,.c-pdf-download .c-pdf-download__link:focus:focus{outline:3px solid #08c;will-change:transform}.c-pdf-download__link .u-icon{padding-top:0}.c-bibliographic-information__column button{margin-bottom:16px}.c-article-body .c-article-author-affiliation__list p,.c-article-body .c-article-author-information__list p,figure{margin:0}.c-article-share-box__button{margin-right:16px}.c-status-message--boxed{border-radius:12px}.c-article-associated-content__collection-title{font-size:1rem}.app-card-service__description,.c-article-body .app-card-service__description{color:#222;margin-bottom:0;margin-top:8px}.app-article-access__subscriptions a,.app-article-access__subscriptions a:visited,.app-book-series-listing__item a,.app-book-series-listing__item a:hover,.app-book-series-listing__item a:visited,.c-article-author-list a,.c-article-author-list a:visited,.c-article-buy-box a,.c-article-buy-box a:visited,.c-article-peer-review a,.c-article-peer-review a:visited,.c-article-satellite-subtitle a,.c-article-satellite-subtitle a:visited,.c-breadcrumbs__link,.c-breadcrumbs__link:hover,.c-breadcrumbs__link:visited{color:#000}.c-article-author-list svg{height:24px;margin:0 0 0 6px;width:24px}.c-article-header{margin-bottom:32px}@media only screen and (min-width:876px){.js .c-ad--conditional{display:block}}.u-lazy-ad-wrapper{background-color:#fff;display:none;min-height:149px}@media only screen and (min-width:876px){.u-lazy-ad-wrapper{display:block}}p.c-ad__label{margin-bottom:4px}.c-ad--728x90{background-color:#fff;border-bottom:2px solid #cedbe0} } </style>
    <style>@media only print, only all and (prefers-color-scheme: no-preference), only all and (prefers-color-scheme: light), only all and (prefers-color-scheme: dark) {  .eds-c-header__brand img{height:24px;width:203px}.app-article-masthead__journal-link img{height:93px;width:72px}@media only screen and (min-width:769px){.app-article-masthead__journal-link img{height:161px;width:122px}} } </style>

        
        <link rel="stylesheet" data-test="critical-css-handler" data-inline-css-source="critical-css" href=/oscar-static/app-springerlink/css/core-darwin-9fe647df8f.css media="print" onload="this.media='all';this.onload=null">
        <link rel="stylesheet" data-test="critical-css-handler" data-inline-css-source="critical-css"
              href="/oscar-static/app-springerlink/css/enhanced-darwin-article-02295df069.css" media="print"
              onload="this.media='only print, only all and (prefers-color-scheme: no-preference), only all and (prefers-color-scheme: light), only all and (prefers-color-scheme: dark)';this.onload=null">
    

        

        
        
        
    <script type="text/javascript">
        config = {
            env: 'live',
            site: '10489.springer.com',
            siteWithPath: '10489.springer.com' + window.location.pathname,
            twitterHashtag: '10489',
            cmsPrefix: 'https://studio-cms.springernature.com/studio/',
            
            
            
            
            publisherBrand: 'Springer',
            mustardcut: false
        };
    </script>

        




    <script>
        window.dataLayer = [{"GA Key":"UA-26408784-1","DOI":"10.1007/s10489-025-06516-z","Page":"article","springerJournal":true,"Publishing Model":"Hybrid Access","Country":"US","japan":false,"doi":"10.1007-s10489-025-06516-z","Journal Id":10489,"Journal Title":"Applied Intelligence","imprint":"Springer","Keywords":"Vision transformers, Training time reduction, Reinforcement learning, Computer vision","kwrd":["Vision_transformers","Training_time_reduction","Reinforcement_learning","Computer_vision"],"Labs":"Y","ksg":"Krux.segments","kuid":"Krux.uid","Has Body":"Y","Features":[],"Open Access":"Y","hasAccess":"Y","bypassPaywall":"N","user":{"license":{"businessPartnerID":[],"businessPartnerIDString":""}},"Access Type":"open","Bpids":"","Bpnames":"","BPID":["1"],"VG Wort Identifier":"vgzm.415900-10.1007-s10489-025-06516-z","Full HTML":"Y","Subject Codes":["SCI","SCI21000","SCT17004","SCT22050"],"pmc":["I","I21000","T17004","T22050"],"session":{"authentication":{"loginStatus":"N"},"attributes":{"edition":"academic"}},"entitlement":{"accessDecision":"OpenAccess"},"content":{"serial":{"eissn":"1573-7497","pissn":"0924-669X"},"type":"Article","category":{"pmc":{"primarySubject":"Computer Science","primarySubjectCode":"I","secondarySubjects":{"1":"Artificial Intelligence","2":"Mechanical Engineering","3":"Manufacturing, Machines, Tools, Processes"},"secondarySubjectCodes":{"1":"I21000","2":"T17004","3":"T22050"}},"sucode":"SC6","articleType":"Article","snt":["Artificial Intelligence","Machine Learning","Computer Vision","Agent-based Economics","Computational Intelligence","Learning Theory"]},"attributes":{"deliveryPlatform":"oscar"}},"page":{"attributes":{"environment":"live"},"category":{"pageType":"article"}},"Event Category":"Article"}];
    </script>











    <script data-test="springer-link-article-datalayer">
        window.dataLayer = window.dataLayer || [];
        window.dataLayer.push({
            ga4MeasurementId: 'G-B3E4QL2TPR',
            ga360TrackingId: 'UA-26408784-1',
            twitterId: 'o47a7',
            baiduId: 'aef3043f025ccf2305af8a194652d70b',
            ga4ServerUrl: 'https://collect.springer.com',
            imprint: 'springerlink',
                page: {
                    attributes:{
                        featureFlags: [
                            
                                { name: 'darwin-orion', active: true },
                            
                                { name: 'show-profile-page-links', active: true },
                            
                                { name: 'download-collection-test', active: false },
                            
                                { name: 'download-issue-test', active: false },
                            
                        ],
                        darwinAvailable: true
                    }
                }
            
        });
    </script>



        <script>
    (function(w, d) {
        w.config = w.config || {};
        w.config.mustardcut = false;

        
        if (w.matchMedia && w.matchMedia('only print, only all and (prefers-color-scheme: no-preference), only all and (prefers-color-scheme: light), only all and (prefers-color-scheme: dark)').matches) {
            w.config.mustardcut = true;
            d.classList.add('js');
            d.classList.remove('grade-c');
            d.classList.remove('no-js');
        }
    })(window, document.documentElement);
</script>


        <script class="js-entry">
    if (window.config.mustardcut) {
        (function(w, d) {
            
            
            
                window.Component = {};
                window.suppressShareButton = false;
                window.onArticlePage = true;
            

            var currentScript = d.currentScript || d.head.querySelector('script.js-entry');

            
            function catchNoModuleSupport() {
                var scriptEl = d.createElement('script');
                return (!('noModule' in scriptEl) && 'onbeforeload' in scriptEl)
            }

            var headScripts = [
                {'src': '/oscar-static/js/polyfill-es5-bundle-b4356fa7f5.js', 'async': false}
            ];

            var bodyScripts = [
                
                    
                    {'src': '/oscar-static/js/global-article-es5-bundle-f45c6eaf2d.js', 'async': false, 'module': false},
                    {'src': '/oscar-static/js/global-article-es6-bundle-09cde44cd7.js', 'async': false, 'module': true}
                    
                
                
                    
                
                
                
            ];

            function createScript(script) {
                var scriptEl = d.createElement('script');
                scriptEl.src = script.src;
                scriptEl.async = script.async;
                if (script.module === true) {
                    scriptEl.type = "module";
                    if (catchNoModuleSupport()) {
                        scriptEl.src = '';
                    }
                } else if (script.module === false) {
                    scriptEl.setAttribute('nomodule', true)
                }
                if (script.charset) {
                    scriptEl.setAttribute('charset', script.charset);
                }

                return scriptEl;
            }

            for (var i = 0; i < headScripts.length; ++i) {
                var scriptEl = createScript(headScripts[i]);
                currentScript.parentNode.insertBefore(scriptEl, currentScript.nextSibling);
            }

            d.addEventListener('DOMContentLoaded', function() {
                for (var i = 0; i < bodyScripts.length; ++i) {
                    var scriptEl = createScript(bodyScripts[i]);
                    d.body.appendChild(scriptEl);
                }
            });

            // Webfont repeat view
            var config = w.config;
            if (config && config.publisherBrand && sessionStorage.fontsLoaded === 'true') {
                d.documentElement.className += ' webfonts-loaded';
            }
        })(window, document);
    }
</script>



        
        
            <script>
    (function (w, d, s) {
        var urlParams = new URLSearchParams(w.location.search);
        if (urlParams.get('gptAdsTest') !== null) {
            d.addEventListener('sncc:initialise', function (e) {
                var t = d.createElement(s);
                var h = d.getElementsByTagName(s)[0];
                t.src = 'https://' + (e.detail.C03 ? 'securepubads.g.doubleclick' : 'pagead2.googlesyndication') + '.net/tag/js/gpt.js';
                t.async = false;
                t.onload = function () {
                    var n = d.createElement(s);
                    n.src = 'https://fed-libs.springer.com/production/gpt-ads-gtm.min.js';
                    n.async = false;
                    h.insertAdjacentElement('afterend', n);
                };
                h.insertAdjacentElement('afterend', t);
            })
        }
    })(window, document, 'script');
</script>
        
        
        

        
            
            
                
    <script data-test="gtm-head">
        window.initGTM = function () {
            if (window.config.mustardcut) {
                (function (w, d, s, l, i) {
                    w[l] = w[l] || [];
                    w[l].push({'gtm.start': new Date().getTime(), event: 'gtm.js'});
                    var f = d.getElementsByTagName(s)[0],
                        j = d.createElement(s),
                        dl = l != 'dataLayer' ? '&l=' + l : '';
                    j.async = true;
                    j.src = 'https://sgtm.springer.com/gtm.js?id=' + i + dl;
                    f.parentNode.insertBefore(j, f);
                })(window, document, 'script', 'dataLayer', 'GTM-MRVXSHQ');
            }
        }
    </script>

            
            
            
        

        <script>
(function (w, d, t) {
    function cc() {
        var h = w.location.hostname;
        var e = d.createElement(t),
        s = d.getElementsByTagName(t)[0];

        
        if (h.indexOf('springer.com') > -1 && h.indexOf('biomedcentral.com') === -1 && h.indexOf('springeropen.com') === -1) {
            e.src = 'https://cmp.springer.com/production_live/en/consent-bundle-17-71.js';
            e.setAttribute('onload', "initGTM(window,document,'script','dataLayer','GTM-MRVXSHQ')");
        } else if (h.indexOf('biomedcentral.com') > -1) {
            e.src = 'https://cmp.biomedcentral.com/production_live/en/consent-bundle-15-46.js';
            e.setAttribute('onload', "initGTM(window,document,'script','dataLayer','GTM-MRVXSHQ')");
        } else if (h.indexOf('springeropen.com') > -1) {
            e.src = 'https://cmp.springernature.com/production_live/en/consent-bundle-16-42.js';
            e.setAttribute('onload', "initGTM(window,document,'script','dataLayer','GTM-MRVXSHQ')");
        } else if (h.indexOf('springernature.com') > -1) {
            e.src = 'https://cmp.springernature.com/production_live/en/consent-bundle-49-65.js';
            e.setAttribute('onload', "initGTM(window,document,'script','dataLayer','GTM-NK22KLS')");
        } else {
            e.src = '/oscar-static/js/cookie-consent-es5-bundle-8d962b73c2.js';
            e.setAttribute('data-consent', h);
        }
        s.insertAdjacentElement('afterend', e);
    }

    cc();
})(window, document, 'script');
</script>


        
        
        
    
        
    

        
    
    <link rel="canonical" href="https://link.springer.com/article/10.1007/s10489-025-06516-z"/>
    

        
        
        
        
        
    <script type="application/ld+json">{"mainEntity":{"headline":"Adaptive patch selection to improve Vision Transformers through Reinforcement Learning","description":"In recent years, Transformers have revolutionized the management of Natural Language Processing tasks, and Vision Transformers (ViTs) promise to do the same for Computer Vision ones. However, the adoption of ViTs is hampered by their computational cost. Indeed, given an image divided into patches, it is necessary to compute for each layer the attention of each patch with respect to all the others. Researchers have proposed many solutions to reduce the computational cost of attention layers by adopting techniques such as quantization, knowledge distillation and manipulation of input images. In this paper, we aim to contribute to the solution of this problem. In particular, we propose a new framework, called AgentViT, which uses Reinforcement Learning to train an agent that selects the most important patches to improve the learning of a ViT. The goal of AgentViT is to reduce the number of patches processed by a ViT, and thus its computational load, while still maintaining competitive performance. We tested AgentViT on CIFAR10, FashionMNIST, and Imagenette\n                \n                  \n                \n                $$^+$$\n                \n               (which is a subset of ImageNet) in the image classification task and obtained promising performance when compared to baseline ViTs and other related approaches available in the literature.","datePublished":"2025-04-01T00:00:00Z","dateModified":"2025-04-01T00:00:00Z","pageStart":"1","pageEnd":"26","license":"http://creativecommons.org/licenses/by/4.0/","sameAs":"https://doi.org/10.1007/s10489-025-06516-z","keywords":["Vision transformers","Training time reduction","Reinforcement learning","Computer vision","Artificial Intelligence","Mechanical Engineering","Manufacturing","Machines","Tools","Processes"],"image":["https://media.springernature.com/lw1200/springer-static/image/art%3A10.1007%2Fs10489-025-06516-z/MediaObjects/10489_2025_6516_Fig1_HTML.png","https://media.springernature.com/lw1200/springer-static/image/art%3A10.1007%2Fs10489-025-06516-z/MediaObjects/10489_2025_6516_Fig2_HTML.png","https://media.springernature.com/lw1200/springer-static/image/art%3A10.1007%2Fs10489-025-06516-z/MediaObjects/10489_2025_6516_Fig3_HTML.png","https://media.springernature.com/lw1200/springer-static/image/art%3A10.1007%2Fs10489-025-06516-z/MediaObjects/10489_2025_6516_Fig4_HTML.png","https://media.springernature.com/lw1200/springer-static/image/art%3A10.1007%2Fs10489-025-06516-z/MediaObjects/10489_2025_6516_Fig5_HTML.png","https://media.springernature.com/lw1200/springer-static/image/art%3A10.1007%2Fs10489-025-06516-z/MediaObjects/10489_2025_6516_Fig6_HTML.png","https://media.springernature.com/lw1200/springer-static/image/art%3A10.1007%2Fs10489-025-06516-z/MediaObjects/10489_2025_6516_Fig7_HTML.png","https://media.springernature.com/lw1200/springer-static/image/art%3A10.1007%2Fs10489-025-06516-z/MediaObjects/10489_2025_6516_Fig8_HTML.png"],"isPartOf":{"name":"Applied Intelligence","issn":["1573-7497","0924-669X"],"volumeNumber":"55","@type":["Periodical","PublicationVolume"]},"publisher":{"name":"Springer US","logo":{"url":"https://www.springernature.com/app-sn/public/images/logo-springernature.png","@type":"ImageObject"},"@type":"Organization"},"author":[{"name":"Francesco Cauteruccio","affiliation":[{"name":"University of Salerno","address":{"name":"DIEM, University of Salerno, Fisciano, Italy","@type":"PostalAddress"},"@type":"Organization"}],"@type":"Person"},{"name":"Michele Marchetti","affiliation":[{"name":"Polytechnic University of Marche","address":{"name":"DII, Polytechnic University of Marche, Ancona, Italy","@type":"PostalAddress"},"@type":"Organization"}],"@type":"Person"},{"name":"Davide Traini","affiliation":[{"name":"University of Modena and Reggio Emilia","address":{"name":"CHIMOMO, University of Modena and Reggio Emilia, Modena, Italy","@type":"PostalAddress"},"@type":"Organization"}],"@type":"Person"},{"name":"Domenico Ursino","affiliation":[{"name":"Polytechnic University of Marche","address":{"name":"DII, Polytechnic University of Marche, Ancona, Italy","@type":"PostalAddress"},"@type":"Organization"}],"@type":"Person"},{"name":"Luca Virgili","affiliation":[{"name":"Polytechnic University of Marche","address":{"name":"DII, Polytechnic University of Marche, Ancona, Italy","@type":"PostalAddress"},"@type":"Organization"}],"email":"luca.virgili@univpm.it","@type":"Person"}],"isAccessibleForFree":true,"@type":"ScholarlyArticle"},"@context":"https://schema.org","@type":"WebPage"}</script>

        
        
    </head>

    <body class=""
    
          >
        <div class="u-visually-hidden" aria-hidden="true" data-test="darwin-icons">
    <svg xmlns="http://www.w3.org/2000/svg"><defs><clipPath id="a"><path d="M.5 0h24v24H.5z"/></clipPath><clipPath id="youtube-icon"><rect width="24" height="24"/></clipPath></defs><symbol id="icon-eds-i-accesses-medium" viewBox="0 0 24 24"><path d="M15.59 1a1 1 0 0 1 .706.291l5.41 5.385a1 1 0 0 1 .294.709v13.077c0 .674-.269 1.32-.747 1.796a2.549 2.549 0 0 1-1.798.742H15a1 1 0 0 1 0-2h4.455a.549.549 0 0 0 .387-.16.535.535 0 0 0 .158-.378V7.8L15.178 3H5.545a.543.543 0 0 0-.538.451L5 3.538v8.607a1 1 0 0 1-2 0V3.538A2.542 2.542 0 0 1 5.545 1h10.046ZM8 13c2.052 0 4.66 1.61 6.36 3.4l.124.141c.333.41.516.925.516 1.459 0 .6-.232 1.178-.64 1.599C12.666 21.388 10.054 23 8 23c-2.052 0-4.66-1.61-6.353-3.393A2.31 2.31 0 0 1 1 18c0-.6.232-1.178.64-1.6C3.34 14.61 5.948 13 8 13Zm0 2c-1.369 0-3.552 1.348-4.917 2.785A.31.31 0 0 0 3 18c0 .083.031.161.09.222C4.447 19.652 6.631 21 8 21c1.37 0 3.556-1.35 4.917-2.785A.31.31 0 0 0 13 18a.32.32 0 0 0-.048-.17l-.042-.052C11.553 16.348 9.369 15 8 15Zm0 1a2 2 0 1 1 0 4 2 2 0 0 1 0-4Z"/></symbol><symbol id="icon-eds-i-altmetric-medium" viewBox="0 0 24 24"><path d="M12 1c5.978 0 10.843 4.77 10.996 10.712l.004.306-.002.022-.002.248C22.843 18.23 17.978 23 12 23 5.925 23 1 18.075 1 12S5.925 1 12 1Zm-1.726 9.246L8.848 12.53a1 1 0 0 1-.718.461L8.003 13l-4.947.014a9.001 9.001 0 0 0 17.887-.001L16.553 13l-2.205 3.53a1 1 0 0 1-1.735-.068l-.05-.11-2.289-6.106ZM12 3a9.001 9.001 0 0 0-8.947 8.013l4.391-.012L9.652 7.47a1 1 0 0 1 1.784.179l2.288 6.104 1.428-2.283a1 1 0 0 1 .722-.462l.129-.008 4.943.012A9.001 9.001 0 0 0 12 3Z"/></symbol><symbol id="icon-eds-i-arrow-bend-down-medium" viewBox="0 0 24 24"><path d="m11.852 20.989.058.007L12 21l.075-.003.126-.017.111-.03.111-.044.098-.052.104-.074.082-.073 6-6a1 1 0 0 0-1.414-1.414L13 17.585v-12.2C13 4.075 11.964 3 10.667 3H4a1 1 0 1 0 0 2h6.667c.175 0 .333.164.333.385v12.2l-4.293-4.292a1 1 0 0 0-1.32-.083l-.094.083a1 1 0 0 0 0 1.414l6 6c.035.036.073.068.112.097l.11.071.114.054.105.035.118.025Z"/></symbol><symbol id="icon-eds-i-arrow-bend-down-small" viewBox="0 0 16 16"><path d="M1 2a1 1 0 0 0 1 1h5v8.585L3.707 8.293a1 1 0 0 0-1.32-.083l-.094.083a1 1 0 0 0 0 1.414l5 5 .063.059.093.069.081.048.105.048.104.035.105.022.096.01h.136l.122-.018.113-.03.103-.04.1-.053.102-.07.052-.043 5.04-5.037a1 1 0 1 0-1.415-1.414L9 11.583V3a2 2 0 0 0-2-2H2a1 1 0 0 0-1 1Z"/></symbol><symbol id="icon-eds-i-arrow-bend-up-medium" viewBox="0 0 24 24"><path d="m11.852 3.011.058-.007L12 3l.075.003.126.017.111.03.111.044.098.052.104.074.082.073 6 6a1 1 0 1 1-1.414 1.414L13 6.415v12.2C13 19.925 11.964 21 10.667 21H4a1 1 0 0 1 0-2h6.667c.175 0 .333-.164.333-.385v-12.2l-4.293 4.292a1 1 0 0 1-1.32.083l-.094-.083a1 1 0 0 1 0-1.414l6-6c.035-.036.073-.068.112-.097l.11-.071.114-.054.105-.035.118-.025Z"/></symbol><symbol id="icon-eds-i-arrow-bend-up-small" viewBox="0 0 16 16"><path d="M1 13.998a1 1 0 0 1 1-1h5V4.413L3.707 7.705a1 1 0 0 1-1.32.084l-.094-.084a1 1 0 0 1 0-1.414l5-5 .063-.059.093-.068.081-.05.105-.047.104-.035.105-.022L7.94 1l.136.001.122.017.113.03.103.04.1.053.102.07.052.043 5.04 5.037a1 1 0 1 1-1.415 1.414L9 4.415v8.583a2 2 0 0 1-2 2H2a1 1 0 0 1-1-1Z"/></symbol><symbol id="icon-eds-i-arrow-diagonal-medium" viewBox="0 0 24 24"><path d="M14 3h6l.075.003.126.017.111.03.111.044.098.052.096.067.09.08c.036.035.068.073.097.112l.071.11.054.114.035.105.03.148L21 4v6a1 1 0 0 1-2 0V6.414l-4.293 4.293a1 1 0 0 1-1.414-1.414L17.584 5H14a1 1 0 0 1-.993-.883L13 4a1 1 0 0 1 1-1ZM4 13a1 1 0 0 1 1 1v3.584l4.293-4.291a1 1 0 1 1 1.414 1.414L6.414 19H10a1 1 0 0 1 .993.883L11 20a1 1 0 0 1-1 1l-6.075-.003-.126-.017-.111-.03-.111-.044-.098-.052-.096-.067-.09-.08a1.01 1.01 0 0 1-.097-.112l-.071-.11-.054-.114-.035-.105-.025-.118-.007-.058L3 20v-6a1 1 0 0 1 1-1Z"/></symbol><symbol id="icon-eds-i-arrow-diagonal-small" viewBox="0 0 16 16"><path d="m2 15-.082-.004-.119-.016-.111-.03-.111-.044-.098-.052-.096-.067-.09-.08a1.008 1.008 0 0 1-.097-.112l-.071-.11-.031-.062-.034-.081-.024-.076-.025-.118-.007-.058L1 14.02V9a1 1 0 1 1 2 0v2.584l2.793-2.791a1 1 0 1 1 1.414 1.414L4.414 13H7a1 1 0 0 1 .993.883L8 14a1 1 0 0 1-1 1H2ZM14 1l.081.003.12.017.111.03.111.044.098.052.096.067.09.08c.036.035.068.073.097.112l.071.11.031.062.034.081.024.076.03.148L15 2v5a1 1 0 0 1-2 0V4.414l-2.96 2.96A1 1 0 1 1 8.626 5.96L11.584 3H9a1 1 0 0 1-.993-.883L8 2a1 1 0 0 1 1-1h5Z"/></symbol><symbol id="icon-eds-i-arrow-down-medium" viewBox="0 0 24 24"><path d="m20.707 12.728-7.99 7.98a.996.996 0 0 1-.561.281l-.157.011a.998.998 0 0 1-.788-.384l-7.918-7.908a1 1 0 0 1 1.414-1.416L11 17.576V4a1 1 0 0 1 2 0v13.598l6.293-6.285a1 1 0 0 1 1.32-.082l.095.083a1 1 0 0 1-.001 1.414Z"/></symbol><symbol id="icon-eds-i-arrow-down-small" viewBox="0 0 16 16"><path d="m1.293 8.707 6 6 .063.059.093.069.081.048.105.049.104.034.056.013.118.017L8 15l.076-.003.122-.017.113-.03.085-.032.063-.03.098-.058.06-.043.05-.043 6.04-6.037a1 1 0 0 0-1.414-1.414L9 11.583V2a1 1 0 1 0-2 0v9.585L2.707 7.293a1 1 0 0 0-1.32-.083l-.094.083a1 1 0 0 0 0 1.414Z"/></symbol><symbol id="icon-eds-i-arrow-left-medium" viewBox="0 0 24 24"><path d="m11.272 3.293-7.98 7.99a.996.996 0 0 0-.281.561L3 12.001c0 .32.15.605.384.788l7.908 7.918a1 1 0 0 0 1.416-1.414L6.424 13H20a1 1 0 0 0 0-2H6.402l6.285-6.293a1 1 0 0 0 .082-1.32l-.083-.095a1 1 0 0 0-1.414.001Z"/></symbol><symbol id="icon-eds-i-arrow-left-small" viewBox="0 0 16 16"><path d="m7.293 1.293-6 6-.059.063-.069.093-.048.081-.049.105-.034.104-.013.056-.017.118L1 8l.003.076.017.122.03.113.032.085.03.063.058.098.043.06.043.05 6.037 6.04a1 1 0 0 0 1.414-1.414L4.417 9H14a1 1 0 0 0 0-2H4.415l4.292-4.293a1 1 0 0 0 .083-1.32l-.083-.094a1 1 0 0 0-1.414 0Z"/></symbol><symbol id="icon-eds-i-arrow-right-medium" viewBox="0 0 24 24"><path d="m12.728 3.293 7.98 7.99a.996.996 0 0 1 .281.561l.011.157c0 .32-.15.605-.384.788l-7.908 7.918a1 1 0 0 1-1.416-1.414L17.576 13H4a1 1 0 0 1 0-2h13.598l-6.285-6.293a1 1 0 0 1-.082-1.32l.083-.095a1 1 0 0 1 1.414.001Z"/></symbol><symbol id="icon-eds-i-arrow-right-small" viewBox="0 0 16 16"><path d="m8.707 1.293 6 6 .059.063.069.093.048.081.049.105.034.104.013.056.017.118L15 8l-.003.076-.017.122-.03.113-.032.085-.03.063-.058.098-.043.06-.043.05-6.037 6.04a1 1 0 0 1-1.414-1.414L11.583 9H2a1 1 0 1 1 0-2h9.585L7.293 2.707a1 1 0 0 1-.083-1.32l.083-.094a1 1 0 0 1 1.414 0Z"/></symbol><symbol id="icon-eds-i-arrow-up-medium" viewBox="0 0 24 24"><path d="m3.293 11.272 7.99-7.98a.996.996 0 0 1 .561-.281L12.001 3c.32 0 .605.15.788.384l7.918 7.908a1 1 0 0 1-1.414 1.416L13 6.424V20a1 1 0 0 1-2 0V6.402l-6.293 6.285a1 1 0 0 1-1.32.082l-.095-.083a1 1 0 0 1 .001-1.414Z"/></symbol><symbol id="icon-eds-i-arrow-up-small" viewBox="0 0 16 16"><path d="m1.293 7.293 6-6 .063-.059.093-.069.081-.048.105-.049.104-.034.056-.013.118-.017L8 1l.076.003.122.017.113.03.085.032.063.03.098.058.06.043.05.043 6.04 6.037a1 1 0 0 1-1.414 1.414L9 4.417V14a1 1 0 0 1-2 0V4.415L2.707 8.707a1 1 0 0 1-1.32.083l-.094-.083a1 1 0 0 1 0-1.414Z"/></symbol><symbol id="icon-eds-i-article-medium" viewBox="0 0 24 24"><path d="M8 7a1 1 0 0 0 0 2h4a1 1 0 1 0 0-2H8ZM8 11a1 1 0 1 0 0 2h8a1 1 0 1 0 0-2H8ZM7 16a1 1 0 0 1 1-1h8a1 1 0 1 1 0 2H8a1 1 0 0 1-1-1Z"/><path d="M5.545 1A2.542 2.542 0 0 0 3 3.538v16.924A2.542 2.542 0 0 0 5.545 23h12.91A2.542 2.542 0 0 0 21 20.462V3.5A2.5 2.5 0 0 0 18.5 1H5.545ZM5 3.538C5 3.245 5.24 3 5.545 3H18.5a.5.5 0 0 1 .5.5v16.962c0 .293-.24.538-.546.538H5.545A.542.542 0 0 1 5 20.462V3.538Z" clip-rule="evenodd"/></symbol><symbol id="icon-eds-i-book-medium" viewBox="0 0 24 24"><path d="M18.5 1A2.5 2.5 0 0 1 21 3.5v12c0 1.16-.79 2.135-1.86 2.418l-.14.031V21h1a1 1 0 0 1 .993.883L21 22a1 1 0 0 1-1 1H6.5A3.5 3.5 0 0 1 3 19.5v-15A3.5 3.5 0 0 1 6.5 1h12ZM17 18H6.5a1.5 1.5 0 0 0-1.493 1.356L5 19.5A1.5 1.5 0 0 0 6.5 21H17v-3Zm1.5-15h-12A1.5 1.5 0 0 0 5 4.5v11.837l.054-.025a3.481 3.481 0 0 1 1.254-.307L6.5 16h12a.5.5 0 0 0 .492-.41L19 15.5v-12a.5.5 0 0 0-.5-.5ZM15 6a1 1 0 0 1 0 2H9a1 1 0 1 1 0-2h6Z"/></symbol><symbol id="icon-eds-i-book-series-medium" viewBox="0 0 24 24"><path fill-rule="evenodd" d="M1 3.786C1 2.759 1.857 2 2.82 2H6.18c.964 0 1.82.759 1.82 1.786V4h3.168c.668 0 1.298.364 1.616.938.158-.109.333-.195.523-.252l3.216-.965c.923-.277 1.962.204 2.257 1.187l4.146 13.82c.296.984-.307 1.957-1.23 2.234l-3.217.965c-.923.277-1.962-.203-2.257-1.187L13 10.005v10.21c0 1.04-.878 1.785-1.834 1.785H7.833c-.291 0-.575-.07-.83-.195A1.849 1.849 0 0 1 6.18 22H2.821C1.857 22 1 21.241 1 20.214V3.786ZM3 4v11h3V4H3Zm0 16v-3h3v3H3Zm15.075-.04-.814-2.712 2.874-.862.813 2.712-2.873.862Zm1.485-5.49-2.874.862-2.634-8.782 2.873-.862 2.635 8.782ZM8 20V6h3v14H8Z" clip-rule="evenodd"/></symbol><symbol id="icon-eds-i-calendar-acceptance-medium" viewBox="0 0 24 24"><path d="M17 2a1 1 0 0 1 1 1v1h1.5C20.817 4 22 5.183 22 6.5v13c0 1.317-1.183 2.5-2.5 2.5h-15C3.183 22 2 20.817 2 19.5v-13C2 5.183 3.183 4 4.5 4a1 1 0 1 1 0 2c-.212 0-.5.288-.5.5v13c0 .212.288.5.5.5h15c.212 0 .5-.288.5-.5v-13c0-.212-.288-.5-.5-.5H18v1a1 1 0 0 1-2 0V3a1 1 0 0 1 1-1Zm-.534 7.747a1 1 0 0 1 .094 1.412l-4.846 5.538a1 1 0 0 1-1.352.141l-2.77-2.076a1 1 0 0 1 1.2-1.6l2.027 1.519 4.236-4.84a1 1 0 0 1 1.411-.094ZM7.5 2a1 1 0 0 1 1 1v1H14a1 1 0 0 1 0 2H8.5v1a1 1 0 1 1-2 0V3a1 1 0 0 1 1-1Z"/></symbol><symbol id="icon-eds-i-calendar-date-medium" viewBox="0 0 24 24"><path d="M17 2a1 1 0 0 1 1 1v1h1.5C20.817 4 22 5.183 22 6.5v13c0 1.317-1.183 2.5-2.5 2.5h-15C3.183 22 2 20.817 2 19.5v-13C2 5.183 3.183 4 4.5 4a1 1 0 1 1 0 2c-.212 0-.5.288-.5.5v13c0 .212.288.5.5.5h15c.212 0 .5-.288.5-.5v-13c0-.212-.288-.5-.5-.5H18v1a1 1 0 0 1-2 0V3a1 1 0 0 1 1-1ZM8 15a1 1 0 1 1 0 2 1 1 0 0 1 0-2Zm4 0a1 1 0 1 1 0 2 1 1 0 0 1 0-2Zm-4-4a1 1 0 1 1 0 2 1 1 0 0 1 0-2Zm4 0a1 1 0 1 1 0 2 1 1 0 0 1 0-2Zm4 0a1 1 0 1 1 0 2 1 1 0 0 1 0-2ZM7.5 2a1 1 0 0 1 1 1v1H14a1 1 0 0 1 0 2H8.5v1a1 1 0 1 1-2 0V3a1 1 0 0 1 1-1Z"/></symbol><symbol id="icon-eds-i-calendar-decision-medium" viewBox="0 0 24 24"><path d="M17 2a1 1 0 0 1 1 1v1h1.5C20.817 4 22 5.183 22 6.5v13c0 1.317-1.183 2.5-2.5 2.5h-15C3.183 22 2 20.817 2 19.5v-13C2 5.183 3.183 4 4.5 4a1 1 0 1 1 0 2c-.212 0-.5.288-.5.5v13c0 .212.288.5.5.5h15c.212 0 .5-.288.5-.5v-13c0-.212-.288-.5-.5-.5H18v1a1 1 0 0 1-2 0V3a1 1 0 0 1 1-1Zm-2.935 8.246 2.686 2.645c.34.335.34.883 0 1.218l-2.686 2.645a.858.858 0 0 1-1.213-.009.854.854 0 0 1 .009-1.21l1.05-1.035H7.984a.992.992 0 0 1-.984-1c0-.552.44-1 .984-1h5.928l-1.051-1.036a.854.854 0 0 1-.085-1.121l.076-.088a.858.858 0 0 1 1.213-.009ZM7.5 2a1 1 0 0 1 1 1v1H14a1 1 0 0 1 0 2H8.5v1a1 1 0 1 1-2 0V3a1 1 0 0 1 1-1Z"/></symbol><symbol id="icon-eds-i-calendar-impact-factor-medium" viewBox="0 0 24 24"><path d="M17 2a1 1 0 0 1 1 1v1h1.5C20.817 4 22 5.183 22 6.5v13c0 1.317-1.183 2.5-2.5 2.5h-15C3.183 22 2 20.817 2 19.5v-13C2 5.183 3.183 4 4.5 4a1 1 0 1 1 0 2c-.212 0-.5.288-.5.5v13c0 .212.288.5.5.5h15c.212 0 .5-.288.5-.5v-13c0-.212-.288-.5-.5-.5H18v1a1 1 0 0 1-2 0V3a1 1 0 0 1 1-1Zm-3.2 6.924a.48.48 0 0 1 .125.544l-1.52 3.283h2.304c.27 0 .491.215.491.483a.477.477 0 0 1-.13.327l-4.18 4.484a.498.498 0 0 1-.69.031.48.48 0 0 1-.125-.544l1.52-3.284H9.291a.487.487 0 0 1-.491-.482c0-.121.047-.238.13-.327l4.18-4.484a.498.498 0 0 1 .69-.031ZM7.5 2a1 1 0 0 1 1 1v1H14a1 1 0 0 1 0 2H8.5v1a1 1 0 1 1-2 0V3a1 1 0 0 1 1-1Z"/></symbol><symbol id="icon-eds-i-call-papers-medium" viewBox="0 0 24 24"><g><path d="m20.707 2.883-1.414 1.414a1 1 0 0 0 1.414 1.414l1.414-1.414a1 1 0 0 0-1.414-1.414Z"/><path d="M6 16.054c0 2.026 1.052 2.943 3 2.943a1 1 0 1 1 0 2c-2.996 0-5-1.746-5-4.943v-1.227a4.068 4.068 0 0 1-1.83-1.189 4.553 4.553 0 0 1-.87-1.455 4.868 4.868 0 0 1-.3-1.686c0-1.17.417-2.298 1.17-3.14.38-.426.834-.767 1.338-1 .51-.237 1.06-.36 1.617-.36L6.632 6H7l7.932-2.895A2.363 2.363 0 0 1 18 5.36v9.28a2.36 2.36 0 0 1-3.069 2.25l.084.03L7 14.997H6v1.057Zm9.637-11.057a.415.415 0 0 0-.083.008L8 7.638v5.536l7.424 1.786.104.02c.035.01.072.02.109.02.2 0 .363-.16.363-.36V5.36c0-.2-.163-.363-.363-.363Zm-9.638 3h-.874a1.82 1.82 0 0 0-.625.111l-.15.063a2.128 2.128 0 0 0-.689.517c-.42.47-.661 1.123-.661 1.81 0 .34.06.678.176.992.114.308.28.585.485.816.4.447.925.691 1.464.691h.874v-5Z" clip-rule="evenodd"/><path d="M20 8.997h2a1 1 0 1 1 0 2h-2a1 1 0 1 1 0-2ZM20.707 14.293l1.414 1.414a1 1 0 0 1-1.414 1.414l-1.414-1.414a1 1 0 0 1 1.414-1.414Z"/></g></symbol><symbol id="icon-eds-i-card-medium" viewBox="0 0 24 24"><path d="M19.615 2c.315 0 .716.067 1.14.279.76.38 1.245 1.107 1.245 2.106v15.23c0 .315-.067.716-.279 1.14-.38.76-1.107 1.245-2.106 1.245H4.385a2.56 2.56 0 0 1-1.14-.279C2.485 21.341 2 20.614 2 19.615V4.385c0-.315.067-.716.279-1.14C2.659 2.485 3.386 2 4.385 2h15.23Zm0 2H4.385c-.213 0-.265.034-.317.14A.71.71 0 0 0 4 4.385v15.23c0 .213.034.265.14.317a.71.71 0 0 0 .245.068h15.23c.213 0 .265-.034.317-.14a.71.71 0 0 0 .068-.245V4.385c0-.213-.034-.265-.14-.317A.71.71 0 0 0 19.615 4ZM17 16a1 1 0 0 1 0 2H7a1 1 0 0 1 0-2h10Zm0-3a1 1 0 0 1 0 2H7a1 1 0 0 1 0-2h10Zm-.5-7A1.5 1.5 0 0 1 18 7.5v3a1.5 1.5 0 0 1-1.5 1.5h-9A1.5 1.5 0 0 1 6 10.5v-3A1.5 1.5 0 0 1 7.5 6h9ZM16 8H8v2h8V8Z"/></symbol><symbol id="icon-eds-i-cart-medium" viewBox="0 0 24 24"><path d="M5.76 1a1 1 0 0 1 .994.902L7.155 6h13.34c.18 0 .358.02.532.057l.174.045a2.5 2.5 0 0 1 1.693 3.103l-2.069 7.03c-.36 1.099-1.398 1.823-2.49 1.763H8.65c-1.272.015-2.352-.927-2.546-2.244L4.852 3H2a1 1 0 0 1-.993-.883L1 2a1 1 0 0 1 1-1h3.76Zm2.328 14.51a.555.555 0 0 0 .55.488l9.751.001a.533.533 0 0 0 .527-.357l2.059-7a.5.5 0 0 0-.48-.642H7.351l.737 7.51ZM18 19a2 2 0 1 1 0 4 2 2 0 0 1 0-4ZM8 19a2 2 0 1 1 0 4 2 2 0 0 1 0-4Z"/></symbol><symbol id="icon-eds-i-check-circle-medium" viewBox="0 0 24 24"><path d="M12 1c6.075 0 11 4.925 11 11s-4.925 11-11 11S1 18.075 1 12 5.925 1 12 1Zm0 2a9 9 0 1 0 0 18 9 9 0 0 0 0-18Zm5.125 4.72a1 1 0 0 1 .156 1.405l-6 7.5a1 1 0 0 1-1.421.143l-3-2.5a1 1 0 0 1 1.28-1.536l2.217 1.846 5.362-6.703a1 1 0 0 1 1.406-.156Z"/></symbol><symbol id="icon-eds-i-check-filled-medium" viewBox="0 0 24 24"><path d="M12 1c6.075 0 11 4.925 11 11s-4.925 11-11 11S1 18.075 1 12 5.925 1 12 1Zm5.125 6.72a1 1 0 0 0-1.406.155l-5.362 6.703-2.217-1.846a1 1 0 1 0-1.28 1.536l3 2.5a1 1 0 0 0 1.42-.143l6-7.5a1 1 0 0 0-.155-1.406Z"/></symbol><symbol id="icon-eds-i-chevron-down-medium" viewBox="0 0 24 24"><path d="M3.305 8.28a1 1 0 0 0-.024 1.415l7.495 7.762c.314.345.757.543 1.224.543.467 0 .91-.198 1.204-.522l7.515-7.783a1 1 0 1 0-1.438-1.39L12 15.845l-7.28-7.54A1 1 0 0 0 3.4 8.2l-.096.082Z"/></symbol><symbol id="icon-eds-i-chevron-down-small" viewBox="0 0 16 16"><path d="M13.692 5.278a1 1 0 0 1 .03 1.414L9.103 11.51a1.491 1.491 0 0 1-2.188.019L2.278 6.692a1 1 0 0 1 1.444-1.384L8 9.771l4.278-4.463a1 1 0 0 1 1.318-.111l.096.081Z"/></symbol><symbol id="icon-eds-i-chevron-left-medium" viewBox="0 0 24 24"><path d="M15.72 3.305a1 1 0 0 0-1.415-.024l-7.762 7.495A1.655 1.655 0 0 0 6 12c0 .467.198.91.522 1.204l7.783 7.515a1 1 0 1 0 1.39-1.438L8.155 12l7.54-7.28A1 1 0 0 0 15.8 3.4l-.082-.096Z"/></symbol><symbol id="icon-eds-i-chevron-left-small" viewBox="0 0 16 16"><path d="M10.722 2.308a1 1 0 0 0-1.414-.03L4.49 6.897a1.491 1.491 0 0 0-.019 2.188l4.838 4.637a1 1 0 1 0 1.384-1.444L6.229 8l4.463-4.278a1 1 0 0 0 .111-1.318l-.081-.096Z"/></symbol><symbol id="icon-eds-i-chevron-right-medium" viewBox="0 0 24 24"><path d="M8.28 3.305a1 1 0 0 1 1.415-.024l7.762 7.495c.345.314.543.757.543 1.224 0 .467-.198.91-.522 1.204l-7.783 7.515a1 1 0 1 1-1.39-1.438L15.845 12l-7.54-7.28A1 1 0 0 1 8.2 3.4l.082-.096Z"/></symbol><symbol id="icon-eds-i-chevron-right-small" viewBox="0 0 16 16"><path d="M5.278 2.308a1 1 0 0 1 1.414-.03l4.819 4.619a1.491 1.491 0 0 1 .019 2.188l-4.838 4.637a1 1 0 1 1-1.384-1.444L9.771 8 5.308 3.722a1 1 0 0 1-.111-1.318l.081-.096Z"/></symbol><symbol id="icon-eds-i-chevron-up-medium" viewBox="0 0 24 24"><path d="M20.695 15.72a1 1 0 0 0 .024-1.415l-7.495-7.762A1.655 1.655 0 0 0 12 6c-.467 0-.91.198-1.204.522l-7.515 7.783a1 1 0 1 0 1.438 1.39L12 8.155l7.28 7.54a1 1 0 0 0 1.319.106l.096-.082Z"/></symbol><symbol id="icon-eds-i-chevron-up-small" viewBox="0 0 16 16"><path d="M13.692 10.722a1 1 0 0 0 .03-1.414L9.103 4.49a1.491 1.491 0 0 0-2.188-.019L2.278 9.308a1 1 0 0 0 1.444 1.384L8 6.229l4.278 4.463a1 1 0 0 0 1.318.111l.096-.081Z"/></symbol><symbol id="icon-eds-i-citations-medium" viewBox="0 0 24 24"><path d="M15.59 1a1 1 0 0 1 .706.291l5.41 5.385a1 1 0 0 1 .294.709v13.077c0 .674-.269 1.32-.747 1.796a2.549 2.549 0 0 1-1.798.742h-5.843a1 1 0 1 1 0-2h5.843a.549.549 0 0 0 .387-.16.535.535 0 0 0 .158-.378V7.8L15.178 3H5.545a.543.543 0 0 0-.538.451L5 3.538v8.607a1 1 0 0 1-2 0V3.538A2.542 2.542 0 0 1 5.545 1h10.046ZM5.483 14.35c.197.26.17.62-.049.848l-.095.083-.016.011c-.36.24-.628.45-.804.634-.393.409-.59.93-.59 1.562.077-.019.192-.028.345-.028.442 0 .84.158 1.195.474.355.316.532.716.532 1.2 0 .501-.173.9-.518 1.198-.345.298-.767.446-1.266.446-.672 0-1.209-.195-1.612-.585-.403-.39-.604-.976-.604-1.757 0-.744.11-1.39.33-1.938.222-.549.49-1.009.807-1.38a4.28 4.28 0 0 1 .992-.88c.07-.043.148-.087.232-.133a.881.881 0 0 1 1.121.245Zm5 0c.197.26.17.62-.049.848l-.095.083-.016.011c-.36.24-.628.45-.804.634-.393.409-.59.93-.59 1.562.077-.019.192-.028.345-.028.442 0 .84.158 1.195.474.355.316.532.716.532 1.2 0 .501-.173.9-.518 1.198-.345.298-.767.446-1.266.446-.672 0-1.209-.195-1.612-.585-.403-.39-.604-.976-.604-1.757 0-.744.11-1.39.33-1.938.222-.549.49-1.009.807-1.38a4.28 4.28 0 0 1 .992-.88c.07-.043.148-.087.232-.133a.881.881 0 0 1 1.121.245Z"/></symbol><symbol id="icon-eds-i-clipboard-check-medium" viewBox="0 0 24 24"><path d="M14.4 1c1.238 0 2.274.865 2.536 2.024L18.5 3C19.886 3 21 4.14 21 5.535v14.93C21 21.86 19.886 23 18.5 23h-13C4.114 23 3 21.86 3 20.465V5.535C3 4.14 4.114 3 5.5 3h1.57c.27-1.147 1.3-2 2.53-2h4.8Zm4.115 4-1.59.024A2.601 2.601 0 0 1 14.4 7H9.6c-1.23 0-2.26-.853-2.53-2H5.5c-.27 0-.5.234-.5.535v14.93c0 .3.23.535.5.535h13c.27 0 .5-.234.5-.535V5.535c0-.3-.23-.535-.485-.535Zm-1.909 4.205a1 1 0 0 1 .19 1.401l-5.334 7a1 1 0 0 1-1.344.23l-2.667-1.75a1 1 0 1 1 1.098-1.672l1.887 1.238 4.769-6.258a1 1 0 0 1 1.401-.19ZM14.4 3H9.6a.6.6 0 0 0-.6.6v.8a.6.6 0 0 0 .6.6h4.8a.6.6 0 0 0 .6-.6v-.8a.6.6 0 0 0-.6-.6Z"/></symbol><symbol id="icon-eds-i-clipboard-report-medium" viewBox="0 0 24 24"><path d="M14.4 1c1.238 0 2.274.865 2.536 2.024L18.5 3C19.886 3 21 4.14 21 5.535v14.93C21 21.86 19.886 23 18.5 23h-13C4.114 23 3 21.86 3 20.465V5.535C3 4.14 4.114 3 5.5 3h1.57c.27-1.147 1.3-2 2.53-2h4.8Zm4.115 4-1.59.024A2.601 2.601 0 0 1 14.4 7H9.6c-1.23 0-2.26-.853-2.53-2H5.5c-.27 0-.5.234-.5.535v14.93c0 .3.23.535.5.535h13c.27 0 .5-.234.5-.535V5.535c0-.3-.23-.535-.485-.535Zm-2.658 10.929a1 1 0 0 1 0 2H8a1 1 0 0 1 0-2h7.857Zm0-3.929a1 1 0 0 1 0 2H8a1 1 0 0 1 0-2h7.857ZM14.4 3H9.6a.6.6 0 0 0-.6.6v.8a.6.6 0 0 0 .6.6h4.8a.6.6 0 0 0 .6-.6v-.8a.6.6 0 0 0-.6-.6Z"/></symbol><symbol id="icon-eds-i-close-medium" viewBox="0 0 24 24"><path d="M12 1c6.075 0 11 4.925 11 11s-4.925 11-11 11S1 18.075 1 12 5.925 1 12 1Zm0 2a9 9 0 1 0 0 18 9 9 0 0 0 0-18ZM8.707 7.293 12 10.585l3.293-3.292a1 1 0 0 1 1.414 1.414L13.415 12l3.292 3.293a1 1 0 0 1-1.414 1.414L12 13.415l-3.293 3.292a1 1 0 1 1-1.414-1.414L10.585 12 7.293 8.707a1 1 0 0 1 1.414-1.414Z"/></symbol><symbol id="icon-eds-i-cloud-upload-medium" viewBox="0 0 24 24"><path d="m12.852 10.011.028-.004L13 10l.075.003.126.017.086.022.136.052.098.052.104.074.082.073 3 3a1 1 0 0 1 0 1.414l-.094.083a1 1 0 0 1-1.32-.083L14 13.416V20a1 1 0 0 1-2 0v-6.586l-1.293 1.293a1 1 0 0 1-1.32.083l-.094-.083a1 1 0 0 1 0-1.414l3-3 .112-.097.11-.071.114-.054.105-.035.118-.025Zm.587-7.962c3.065.362 5.497 2.662 5.992 5.562l.013.085.207.073c2.117.782 3.496 2.845 3.337 5.097l-.022.226c-.297 2.561-2.503 4.491-5.124 4.502a1 1 0 1 1-.009-2c1.619-.007 2.967-1.186 3.147-2.733.179-1.542-.86-2.979-2.487-3.353-.512-.149-.894-.579-.981-1.165-.21-2.237-2-4.035-4.308-4.308-2.31-.273-4.497 1.06-5.25 3.19l-.049.113c-.234.468-.718.756-1.176.743-1.418.057-2.689.857-3.32 2.084a3.668 3.668 0 0 0 .262 3.798c.796 1.136 2.169 1.764 3.583 1.635a1 1 0 1 1 .182 1.992c-2.125.194-4.193-.753-5.403-2.48a5.668 5.668 0 0 1-.403-5.86c.85-1.652 2.449-2.79 4.323-3.092l.287-.039.013-.028c1.207-2.741 4.125-4.404 7.186-4.042Z"/></symbol><symbol id="icon-eds-i-collection-medium" viewBox="0 0 24 24"><path d="M21 7a1 1 0 0 1 1 1v12.5a2.5 2.5 0 0 1-2.5 2.5H8a1 1 0 0 1 0-2h11.5a.5.5 0 0 0 .5-.5V8a1 1 0 0 1 1-1Zm-5.5-5A2.5 2.5 0 0 1 18 4.5v12a2.5 2.5 0 0 1-2.5 2.5h-11A2.5 2.5 0 0 1 2 16.5v-12A2.5 2.5 0 0 1 4.5 2h11Zm0 2h-11a.5.5 0 0 0-.5.5v12a.5.5 0 0 0 .5.5h11a.5.5 0 0 0 .5-.5v-12a.5.5 0 0 0-.5-.5ZM13 13a1 1 0 0 1 0 2H7a1 1 0 0 1 0-2h6Zm0-3.5a1 1 0 0 1 0 2H7a1 1 0 0 1 0-2h6ZM13 6a1 1 0 0 1 0 2H7a1 1 0 1 1 0-2h6Z"/></symbol><symbol id="icon-eds-i-conference-series-medium" viewBox="0 0 24 24"><path fill-rule="evenodd" d="M4.5 2A2.5 2.5 0 0 0 2 4.5v11A2.5 2.5 0 0 0 4.5 18h2.37l-2.534 2.253a1 1 0 0 0 1.328 1.494L9.88 18H11v3a1 1 0 1 0 2 0v-3h1.12l4.216 3.747a1 1 0 0 0 1.328-1.494L17.13 18h2.37a2.5 2.5 0 0 0 2.5-2.5v-11A2.5 2.5 0 0 0 19.5 2h-15ZM20 6V4.5a.5.5 0 0 0-.5-.5h-15a.5.5 0 0 0-.5.5V6h16ZM4 8v7.5a.5.5 0 0 0 .5.5h15a.5.5 0 0 0 .5-.5V8H4Z" clip-rule="evenodd"/></symbol><symbol id="icon-eds-i-delivery-medium" viewBox="0 0 24 24"><path d="M8.51 20.598a3.037 3.037 0 0 1-3.02 0A2.968 2.968 0 0 1 4.161 19L3.5 19A2.5 2.5 0 0 1 1 16.5v-11A2.5 2.5 0 0 1 3.5 3h10a2.5 2.5 0 0 1 2.45 2.004L16 5h2.527c.976 0 1.855.585 2.27 1.49l2.112 4.62a1 1 0 0 1 .091.416v4.856C23 17.814 21.889 19 20.484 19h-.523a1.01 1.01 0 0 1-.121-.007 2.96 2.96 0 0 1-1.33 1.605 3.037 3.037 0 0 1-3.02 0A2.968 2.968 0 0 1 14.161 19H9.838a2.968 2.968 0 0 1-1.327 1.597Zm-2.024-3.462a.955.955 0 0 0-.481.73L5.999 18l.001.022a.944.944 0 0 0 .388.777l.098.065c.316.181.712.181 1.028 0A.97.97 0 0 0 8 17.978a.95.95 0 0 0-.486-.842 1.037 1.037 0 0 0-1.028 0Zm10 0a.955.955 0 0 0-.481.73l-.005.156a.944.944 0 0 0 .388.777l.098.065c.316.181.712.181 1.028 0a.97.97 0 0 0 .486-.886.95.95 0 0 0-.486-.842 1.037 1.037 0 0 0-1.028 0ZM21 12h-5v3.17a3.038 3.038 0 0 1 2.51.232 2.993 2.993 0 0 1 1.277 1.45l.058.155.058-.005.581-.002c.27 0 .516-.263.516-.618V12Zm-7.5-7h-10a.5.5 0 0 0-.5.5v11a.5.5 0 0 0 .5.5h.662a2.964 2.964 0 0 1 1.155-1.491l.172-.107a3.037 3.037 0 0 1 3.022 0A2.987 2.987 0 0 1 9.843 17H13.5a.5.5 0 0 0 .5-.5v-11a.5.5 0 0 0-.5-.5Zm5.027 2H16v3h4.203l-1.224-2.677a.532.532 0 0 0-.375-.316L18.527 7Z"/></symbol><symbol id="icon-eds-i-download-medium" viewBox="0 0 24 24"><path d="M22 18.5a3.5 3.5 0 0 1-3.5 3.5h-13A3.5 3.5 0 0 1 2 18.5V18a1 1 0 0 1 2 0v.5A1.5 1.5 0 0 0 5.5 20h13a1.5 1.5 0 0 0 1.5-1.5V18a1 1 0 0 1 2 0v.5Zm-3.293-7.793-6 6-.063.059-.093.069-.081.048-.105.049-.104.034-.056.013-.118.017L12 17l-.076-.003-.122-.017-.113-.03-.085-.032-.063-.03-.098-.058-.06-.043-.05-.043-6.04-6.037a1 1 0 0 1 1.414-1.414l4.294 4.29L11 3a1 1 0 0 1 2 0l.001 10.585 4.292-4.292a1 1 0 0 1 1.32-.083l.094.083a1 1 0 0 1 0 1.414Z"/></symbol><symbol id="icon-eds-i-edit-medium" viewBox="0 0 24 24"><path d="M17.149 2a2.38 2.38 0 0 1 1.699.711l2.446 2.46a2.384 2.384 0 0 1 .005 3.38L10.01 19.906a1 1 0 0 1-.434.257l-6.3 1.8a1 1 0 0 1-1.237-1.237l1.8-6.3a1 1 0 0 1 .257-.434L15.443 2.718A2.385 2.385 0 0 1 17.15 2Zm-3.874 5.689-7.586 7.536-1.234 4.319 4.318-1.234 7.54-7.582-3.038-3.039ZM17.149 4a.395.395 0 0 0-.286.126L14.695 6.28l3.029 3.029 2.162-2.173a.384.384 0 0 0 .106-.197L20 6.864c0-.103-.04-.2-.119-.278l-2.457-2.47A.385.385 0 0 0 17.149 4Z"/></symbol><symbol id="icon-eds-i-education-medium" viewBox="0 0 24 24"><path fill-rule="evenodd" d="M12.41 2.088a1 1 0 0 0-.82 0l-10 4.5a1 1 0 0 0 0 1.824L3 9.047v7.124A3.001 3.001 0 0 0 4 22a3 3 0 0 0 1-5.83V9.948l1 .45V14.5a1 1 0 0 0 .087.408L7 14.5c-.913.408-.912.41-.912.41l.001.003.003.006.007.015a1.988 1.988 0 0 0 .083.16c.054.097.131.225.236.373.21.297.53.68.993 1.057C8.351 17.292 9.824 18 12 18c2.176 0 3.65-.707 4.589-1.476.463-.378.783-.76.993-1.057a4.162 4.162 0 0 0 .319-.533l.007-.015.003-.006v-.003h.002s0-.002-.913-.41l.913.408A1 1 0 0 0 18 14.5v-4.103l4.41-1.985a1 1 0 0 0 0-1.824l-10-4.5ZM16 11.297l-3.59 1.615a1 1 0 0 1-.82 0L8 11.297v2.94a3.388 3.388 0 0 0 .677.739C9.267 15.457 10.294 16 12 16s2.734-.543 3.323-1.024a3.388 3.388 0 0 0 .677-.739v-2.94ZM4.437 7.5 12 4.097 19.563 7.5 12 10.903 4.437 7.5ZM3 19a1 1 0 1 1 2 0 1 1 0 0 1-2 0Z" clip-rule="evenodd"/></symbol><symbol id="icon-eds-i-error-diamond-medium" viewBox="0 0 24 24"><path d="M12.002 1c.702 0 1.375.279 1.871.775l8.35 8.353a2.646 2.646 0 0 1 .001 3.744l-8.353 8.353a2.646 2.646 0 0 1-3.742 0l-8.353-8.353a2.646 2.646 0 0 1 0-3.744l8.353-8.353.156-.142c.424-.362.952-.58 1.507-.625l.21-.008Zm0 2a.646.646 0 0 0-.38.123l-.093.08-8.34 8.34a.646.646 0 0 0-.18.355L3 12c0 .171.068.336.19.457l8.353 8.354a.646.646 0 0 0 .914 0l8.354-8.354a.646.646 0 0 0-.001-.914l-8.351-8.354A.646.646 0 0 0 12.002 3ZM12 14.5a1.5 1.5 0 0 1 .144 2.993L12 17.5a1.5 1.5 0 0 1 0-3ZM12 6a1 1 0 0 1 1 1v5a1 1 0 0 1-2 0V7a1 1 0 0 1 1-1Z"/></symbol><symbol id="icon-eds-i-error-filled-medium" viewBox="0 0 24 24"><path d="M12.002 1c.702 0 1.375.279 1.871.775l8.35 8.353a2.646 2.646 0 0 1 .001 3.744l-8.353 8.353a2.646 2.646 0 0 1-3.742 0l-8.353-8.353a2.646 2.646 0 0 1 0-3.744l8.353-8.353.156-.142c.424-.362.952-.58 1.507-.625l.21-.008ZM12 14.5a1.5 1.5 0 0 0 0 3l.144-.007A1.5 1.5 0 0 0 12 14.5ZM12 6a1 1 0 0 0-1 1v5a1 1 0 0 0 2 0V7a1 1 0 0 0-1-1Z"/></symbol><symbol id="icon-eds-i-external-link-medium" viewBox="0 0 24 24"><path d="M9 2a1 1 0 1 1 0 2H4.6c-.371 0-.6.209-.6.5v15c0 .291.229.5.6.5h14.8c.371 0 .6-.209.6-.5V15a1 1 0 0 1 2 0v4.5c0 1.438-1.162 2.5-2.6 2.5H4.6C3.162 22 2 20.938 2 19.5v-15C2 3.062 3.162 2 4.6 2H9Zm6 0h6l.075.003.126.017.111.03.111.044.098.052.096.067.09.08c.036.035.068.073.097.112l.071.11.054.114.035.105.03.148L22 3v6a1 1 0 0 1-2 0V5.414l-6.693 6.693a1 1 0 0 1-1.414-1.414L18.584 4H15a1 1 0 0 1-.993-.883L14 3a1 1 0 0 1 1-1Z"/></symbol><symbol id="icon-eds-i-external-link-small" viewBox="0 0 16 16"><path d="M5 1a1 1 0 1 1 0 2l-2-.001V13L13 13v-2a1 1 0 0 1 2 0v2c0 1.15-.93 2-2.067 2H3.067C1.93 15 1 14.15 1 13V3c0-1.15.93-2 2.067-2H5Zm4 0h5l.075.003.126.017.111.03.111.044.098.052.096.067.09.08.044.047.073.093.051.083.054.113.035.105.03.148L15 2v5a1 1 0 0 1-2 0V4.414L9.107 8.307a1 1 0 0 1-1.414-1.414L11.584 3H9a1 1 0 0 1-.993-.883L8 2a1 1 0 0 1 1-1Z"/></symbol><symbol id="icon-eds-i-file-download-medium" viewBox="0 0 24 24"><path d="M14.5 1a1 1 0 0 1 .707.293l5.5 5.5A1 1 0 0 1 21 7.5v12.962A2.542 2.542 0 0 1 18.455 23H5.545A2.542 2.542 0 0 1 3 20.462V3.538A2.542 2.542 0 0 1 5.545 1H14.5Zm-.415 2h-8.54A.542.542 0 0 0 5 3.538v16.924c0 .296.243.538.545.538h12.91a.542.542 0 0 0 .545-.538V7.915L14.085 3ZM12 7a1 1 0 0 1 1 1v6.585l2.293-2.292a1 1 0 0 1 1.32-.083l.094.083a1 1 0 0 1 0 1.414l-4 4a1.008 1.008 0 0 1-.112.097l-.11.071-.114.054-.105.035-.149.03L12 18l-.075-.003-.126-.017-.111-.03-.111-.044-.098-.052-.096-.067-.09-.08-4-4a1 1 0 0 1 1.414-1.414L11 14.585V8a1 1 0 0 1 1-1Z"/></symbol><symbol id="icon-eds-i-file-report-medium" viewBox="0 0 24 24"><path d="M14.5 1a1 1 0 0 1 .707.293l5.5 5.5A1 1 0 0 1 21 7.5v12.962c0 .674-.269 1.32-.747 1.796a2.549 2.549 0 0 1-1.798.742H5.545c-.674 0-1.32-.267-1.798-.742A2.535 2.535 0 0 1 3 20.462V3.538A2.542 2.542 0 0 1 5.545 1H14.5Zm-.415 2h-8.54A.542.542 0 0 0 5 3.538v16.924c0 .142.057.278.158.379.102.102.242.159.387.159h12.91a.549.549 0 0 0 .387-.16.535.535 0 0 0 .158-.378V7.915L14.085 3ZM16 17a1 1 0 0 1 0 2H8a1 1 0 0 1 0-2h8Zm0-3a1 1 0 0 1 0 2H8a1 1 0 0 1 0-2h8Zm-4.793-6.207L13 9.585l1.793-1.792a1 1 0 0 1 1.32-.083l.094.083a1 1 0 0 1 0 1.414l-2.5 2.5a1 1 0 0 1-1.414 0L10.5 9.915l-1.793 1.792a1 1 0 0 1-1.32.083l-.094-.083a1 1 0 0 1 0-1.414l2.5-2.5a1 1 0 0 1 1.414 0Z"/></symbol><symbol id="icon-eds-i-file-text-medium" viewBox="0 0 24 24"><path d="M14.5 1a1 1 0 0 1 .707.293l5.5 5.5A1 1 0 0 1 21 7.5v12.962A2.542 2.542 0 0 1 18.455 23H5.545A2.542 2.542 0 0 1 3 20.462V3.538A2.542 2.542 0 0 1 5.545 1H14.5Zm-.415 2h-8.54A.542.542 0 0 0 5 3.538v16.924c0 .296.243.538.545.538h12.91a.542.542 0 0 0 .545-.538V7.915L14.085 3ZM16 15a1 1 0 0 1 0 2H8a1 1 0 0 1 0-2h8Zm0-4a1 1 0 0 1 0 2H8a1 1 0 0 1 0-2h8Zm-5-4a1 1 0 0 1 0 2H8a1 1 0 1 1 0-2h3Z"/></symbol><symbol id="icon-eds-i-file-upload-medium" viewBox="0 0 24 24"><path d="M14.5 1a1 1 0 0 1 .707.293l5.5 5.5A1 1 0 0 1 21 7.5v12.962A2.542 2.542 0 0 1 18.455 23H5.545A2.542 2.542 0 0 1 3 20.462V3.538A2.542 2.542 0 0 1 5.545 1H14.5Zm-.415 2h-8.54A.542.542 0 0 0 5 3.538v16.924c0 .296.243.538.545.538h12.91a.542.542 0 0 0 .545-.538V7.915L14.085 3Zm-2.233 4.011.058-.007L12 7l.075.003.126.017.111.03.111.044.098.052.104.074.082.073 4 4a1 1 0 0 1 0 1.414l-.094.083a1 1 0 0 1-1.32-.083L13 10.415V17a1 1 0 0 1-2 0v-6.585l-2.293 2.292a1 1 0 0 1-1.32.083l-.094-.083a1 1 0 0 1 0-1.414l4-4 .112-.097.11-.071.114-.054.105-.035.118-.025Z"/></symbol><symbol id="icon-eds-i-filter-medium" viewBox="0 0 24 24"><path d="M21 2a1 1 0 0 1 .82 1.573L15 13.314V18a1 1 0 0 1-.31.724l-.09.076-4 3A1 1 0 0 1 9 21v-7.684L2.18 3.573a1 1 0 0 1 .707-1.567L3 2h18Zm-1.921 2H4.92l5.9 8.427a1 1 0 0 1 .172.45L11 13v6l2-1.5V13a1 1 0 0 1 .117-.469l.064-.104L19.079 4Z"/></symbol><symbol id="icon-eds-i-funding-medium" viewBox="0 0 24 24"><path fill-rule="evenodd" d="M23 8A7 7 0 1 0 9 8a7 7 0 0 0 14 0ZM9.006 12.225A4.07 4.07 0 0 0 6.12 11.02H2a.979.979 0 1 0 0 1.958h4.12c.558 0 1.094.222 1.489.617l2.207 2.288c.27.27.27.687.012.944a.656.656 0 0 1-.928 0L7.744 15.67a.98.98 0 0 0-1.386 1.384l1.157 1.158c.535.536 1.244.791 1.946.765l.041.002h6.922c.874 0 1.597.748 1.597 1.688 0 .203-.146.354-.309.354H7.755c-.487 0-.96-.178-1.339-.504L2.64 17.259a.979.979 0 0 0-1.28 1.482L5.137 22c.733.631 1.66.979 2.618.979h9.957c1.26 0 2.267-1.043 2.267-2.312 0-2.006-1.584-3.646-3.555-3.646h-4.529a2.617 2.617 0 0 0-.681-2.509l-2.208-2.287ZM16 3a5 5 0 1 0 0 10 5 5 0 0 0 0-10Zm.979 3.5a.979.979 0 1 0-1.958 0v3a.979.979 0 1 0 1.958 0v-3Z" clip-rule="evenodd"/></symbol><symbol id="icon-eds-i-hashtag-medium" viewBox="0 0 24 24"><path d="M12 1c6.075 0 11 4.925 11 11s-4.925 11-11 11S1 18.075 1 12 5.925 1 12 1Zm0 2a9 9 0 1 0 0 18 9 9 0 0 0 0-18ZM9.52 18.189a1 1 0 1 1-1.964-.378l.437-2.274H6a1 1 0 1 1 0-2h2.378l.592-3.076H6a1 1 0 0 1 0-2h3.354l.51-2.65a1 1 0 1 1 1.964.378l-.437 2.272h3.04l.51-2.65a1 1 0 1 1 1.964.378l-.438 2.272H18a1 1 0 0 1 0 2h-1.917l-.592 3.076H18a1 1 0 0 1 0 2h-2.893l-.51 2.652a1 1 0 1 1-1.964-.378l.437-2.274h-3.04l-.51 2.652Zm.895-4.652h3.04l.591-3.076h-3.04l-.591 3.076Z"/></symbol><symbol id="icon-eds-i-home-medium" viewBox="0 0 24 24"><path d="M5 22a1 1 0 0 1-1-1v-8.586l-1.293 1.293a1 1 0 0 1-1.32.083l-.094-.083a1 1 0 0 1 0-1.414l10-10a1 1 0 0 1 1.414 0l10 10a1 1 0 0 1-1.414 1.414L20 12.415V21a1 1 0 0 1-1 1H5Zm7-17.585-6 5.999V20h5v-4a1 1 0 0 1 2 0v4h5v-9.585l-6-6Z"/></symbol><symbol id="icon-eds-i-image-medium" viewBox="0 0 24 24"><path d="M19.615 2A2.385 2.385 0 0 1 22 4.385v15.23A2.385 2.385 0 0 1 19.615 22H4.385A2.385 2.385 0 0 1 2 19.615V4.385A2.385 2.385 0 0 1 4.385 2h15.23Zm0 2H4.385A.385.385 0 0 0 4 4.385v15.23c0 .213.172.385.385.385h1.244l10.228-8.76a1 1 0 0 1 1.254-.037L20 13.392V4.385A.385.385 0 0 0 19.615 4Zm-3.07 9.283L8.703 20h10.912a.385.385 0 0 0 .385-.385v-3.713l-3.455-2.619ZM9.5 6a3.5 3.5 0 1 1 0 7 3.5 3.5 0 0 1 0-7Zm0 2a1.5 1.5 0 1 0 0 3 1.5 1.5 0 0 0 0-3Z"/></symbol><symbol id="icon-eds-i-impact-factor-medium" viewBox="0 0 24 24"><path d="M16.49 2.672c.74.694.986 1.765.632 2.712l-.04.1-1.549 3.54h1.477a2.496 2.496 0 0 1 2.485 2.34l.005.163c0 .618-.23 1.21-.642 1.675l-7.147 7.961a2.48 2.48 0 0 1-3.554.165 2.512 2.512 0 0 1-.633-2.712l.042-.103L9.108 15H7.46c-1.393 0-2.379-1.11-2.455-2.369L5 12.473c0-.593.142-1.145.628-1.692l7.307-7.944a2.48 2.48 0 0 1 3.555-.165ZM14.43 4.164l-7.33 7.97c-.083.093-.101.214-.101.34 0 .277.19.526.46.526h4.163l.097-.009c.015 0 .03.003.046.009.181.078.264.32.186.5l-2.554 5.817a.512.512 0 0 0 .127.552.48.48 0 0 0 .69-.033l7.155-7.97a.513.513 0 0 0 .13-.34.497.497 0 0 0-.49-.502h-3.988a.355.355 0 0 1-.328-.497l2.555-5.844a.512.512 0 0 0-.127-.552.48.48 0 0 0-.69.033Z"/></symbol><symbol id="icon-eds-i-info-circle-medium" viewBox="0 0 24 24"><path d="M12 1c6.075 0 11 4.925 11 11s-4.925 11-11 11S1 18.075 1 12 5.925 1 12 1Zm0 2a9 9 0 1 0 0 18 9 9 0 0 0 0-18Zm0 7a1 1 0 0 1 1 1v5h1.5a1 1 0 0 1 0 2h-5a1 1 0 0 1 0-2H11v-4h-.5a1 1 0 0 1-.993-.883L9.5 11a1 1 0 0 1 1-1H12Zm0-4.5a1.5 1.5 0 0 1 .144 2.993L12 8.5a1.5 1.5 0 0 1 0-3Z"/></symbol><symbol id="icon-eds-i-info-filled-medium" viewBox="0 0 24 24"><path d="M12 1c6.075 0 11 4.925 11 11s-4.925 11-11 11S1 18.075 1 12 5.925 1 12 1Zm0 9h-1.5a1 1 0 0 0-1 1l.007.117A1 1 0 0 0 10.5 12h.5v4H9.5a1 1 0 0 0 0 2h5a1 1 0 0 0 0-2H13v-5a1 1 0 0 0-1-1Zm0-4.5a1.5 1.5 0 0 0 0 3l.144-.007A1.5 1.5 0 0 0 12 5.5Z"/></symbol><symbol id="icon-eds-i-journal-medium" viewBox="0 0 24 24"><path d="M18.5 1A2.5 2.5 0 0 1 21 3.5v14a2.5 2.5 0 0 1-2.5 2.5h-13a.5.5 0 1 0 0 1H20a1 1 0 0 1 0 2H5.5A2.5 2.5 0 0 1 3 20.5v-17A2.5 2.5 0 0 1 5.5 1h13ZM7 3H5.5a.5.5 0 0 0-.5.5v14.549l.016-.002c.104-.02.211-.035.32-.042L5.5 18H7V3Zm11.5 0H9v15h9.5a.5.5 0 0 0 .5-.5v-14a.5.5 0 0 0-.5-.5ZM16 5a1 1 0 0 1 1 1v4a1 1 0 0 1-1 1h-5a1 1 0 0 1-1-1V6a1 1 0 0 1 1-1h5Zm-1 2h-3v2h3V7Z"/></symbol><symbol id="icon-eds-i-mail-medium" viewBox="0 0 24 24"><path d="M20.462 3C21.875 3 23 4.184 23 5.619v12.762C23 19.816 21.875 21 20.462 21H3.538C2.125 21 1 19.816 1 18.381V5.619C1 4.184 2.125 3 3.538 3h16.924ZM21 8.158l-7.378 6.258a2.549 2.549 0 0 1-3.253-.008L3 8.16v10.222c0 .353.253.619.538.619h16.924c.285 0 .538-.266.538-.619V8.158ZM20.462 5H3.538c-.264 0-.5.228-.534.542l8.65 7.334c.2.165.492.165.684.007l8.656-7.342-.001-.025c-.044-.3-.274-.516-.531-.516Z"/></symbol><symbol id="icon-eds-i-mail-send-medium" viewBox="0 0 24 24"><path d="M20.444 5a2.562 2.562 0 0 1 2.548 2.37l.007.078.001.123v7.858A2.564 2.564 0 0 1 20.444 18H9.556A2.564 2.564 0 0 1 7 15.429l.001-7.977.007-.082A2.561 2.561 0 0 1 9.556 5h10.888ZM21 9.331l-5.46 3.51a1 1 0 0 1-1.08 0L9 9.332v6.097c0 .317.251.571.556.571h10.888a.564.564 0 0 0 .556-.571V9.33ZM20.444 7H9.556a.543.543 0 0 0-.32.105l5.763 3.706 5.766-3.706a.543.543 0 0 0-.32-.105ZM4.308 5a1 1 0 1 1 0 2H2a1 1 0 1 1 0-2h2.308Zm0 5.5a1 1 0 0 1 0 2H2a1 1 0 0 1 0-2h2.308Zm0 5.5a1 1 0 0 1 0 2H2a1 1 0 0 1 0-2h2.308Z"/></symbol><symbol id="icon-eds-i-mentions-medium" viewBox="0 0 24 24"><path d="m9.452 1.293 5.92 5.92 2.92-2.92a1 1 0 0 1 1.415 1.414l-2.92 2.92 5.92 5.92a1 1 0 0 1 0 1.415 10.371 10.371 0 0 1-10.378 2.584l.652 3.258A1 1 0 0 1 12 23H2a1 1 0 0 1-.874-1.486l4.789-8.62C4.194 9.074 4.9 4.43 8.038 1.292a1 1 0 0 1 1.414 0Zm-2.355 13.59L3.699 21h7.081l-.689-3.442a10.392 10.392 0 0 1-2.775-2.396l-.22-.28Zm1.69-11.427-.07.09a8.374 8.374 0 0 0 11.737 11.737l.089-.071L8.787 3.456Z"/></symbol><symbol id="icon-eds-i-menu-medium" viewBox="0 0 24 24"><path d="M21 4a1 1 0 0 1 0 2H3a1 1 0 1 1 0-2h18Zm-4 7a1 1 0 0 1 0 2H3a1 1 0 0 1 0-2h14Zm4 7a1 1 0 0 1 0 2H3a1 1 0 0 1 0-2h18Z"/></symbol><symbol id="icon-eds-i-metrics-medium" viewBox="0 0 24 24"><path d="M3 22a1 1 0 0 1-1-1V3a1 1 0 0 1 1-1h6a1 1 0 0 1 1 1v7h4V8a1 1 0 0 1 1-1h6a1 1 0 0 1 1 1v13a1 1 0 0 1-.883.993L21 22H3Zm17-2V9h-4v11h4Zm-6-8h-4v8h4v-8ZM8 4H4v16h4V4Z"/></symbol><symbol id="icon-eds-i-news-medium" viewBox="0 0 24 24"><path d="M17.384 3c.975 0 1.77.787 1.77 1.762v13.333c0 .462.354.846.815.899l.107.006.109-.006a.915.915 0 0 0 .809-.794l.006-.105V8.19a1 1 0 0 1 2 0v9.905A2.914 2.914 0 0 1 20.077 21H3.538a2.547 2.547 0 0 1-1.644-.601l-.147-.135A2.516 2.516 0 0 1 1 18.476V4.762C1 3.787 1.794 3 2.77 3h14.614Zm-.231 2H3v13.476c0 .11.035.216.1.304l.054.063c.101.1.24.157.384.157l13.761-.001-.026-.078a2.88 2.88 0 0 1-.115-.655l-.004-.17L17.153 5ZM14 15.021a.979.979 0 1 1 0 1.958H6a.979.979 0 1 1 0-1.958h8Zm0-8c.54 0 .979.438.979.979v4c0 .54-.438.979-.979.979H6A.979.979 0 0 1 5.021 12V8c0-.54.438-.979.979-.979h8Zm-.98 1.958H6.979v2.041h6.041V8.979Z"/></symbol><symbol id="icon-eds-i-newsletter-medium" viewBox="0 0 24 24"><path d="M21 10a1 1 0 0 1 1 1v9.5a2.5 2.5 0 0 1-2.5 2.5h-15A2.5 2.5 0 0 1 2 20.5V11a1 1 0 0 1 2 0v.439l8 4.888 8-4.889V11a1 1 0 0 1 1-1Zm-1 3.783-7.479 4.57a1 1 0 0 1-1.042 0l-7.48-4.57V20.5a.5.5 0 0 0 .501.5h15a.5.5 0 0 0 .5-.5v-6.717ZM15 9a1 1 0 0 1 0 2H9a1 1 0 0 1 0-2h6Zm2.5-8A2.5 2.5 0 0 1 20 3.5V9a1 1 0 0 1-2 0V3.5a.5.5 0 0 0-.5-.5h-11a.5.5 0 0 0-.5.5V9a1 1 0 1 1-2 0V3.5A2.5 2.5 0 0 1 6.5 1h11ZM15 5a1 1 0 0 1 0 2H9a1 1 0 1 1 0-2h6Z"/></symbol><symbol id="icon-eds-i-notifcation-medium" viewBox="0 0 24 24"><path d="M14 20a1 1 0 0 1 0 2h-4a1 1 0 0 1 0-2h4ZM3 18l-.133-.007c-1.156-.124-1.156-1.862 0-1.986l.3-.012C4.32 15.923 5 15.107 5 14V9.5C5 5.368 8.014 2 12 2s7 3.368 7 7.5V14c0 1.107.68 1.923 1.832 1.995l.301.012c1.156.124 1.156 1.862 0 1.986L21 18H3Zm9-14C9.17 4 7 6.426 7 9.5V14c0 .671-.146 1.303-.416 1.858L6.51 16h10.979l-.073-.142a4.192 4.192 0 0 1-.412-1.658L17 14V9.5C17 6.426 14.83 4 12 4Z"/></symbol><symbol id="icon-eds-i-publish-medium" viewBox="0 0 24 24"><g><path d="M16.296 1.291A1 1 0 0 0 15.591 1H5.545A2.542 2.542 0 0 0 3 3.538V13a1 1 0 1 0 2 0V3.538l.007-.087A.543.543 0 0 1 5.545 3h9.633L20 7.8v12.662a.534.534 0 0 1-.158.379.548.548 0 0 1-.387.159H11a1 1 0 1 0 0 2h8.455c.674 0 1.32-.267 1.798-.742A2.534 2.534 0 0 0 22 20.462V7.385a1 1 0 0 0-.294-.709l-5.41-5.385Z"/><path d="M10.762 16.647a1 1 0 0 0-1.525-1.294l-4.472 5.271-2.153-1.665a1 1 0 1 0-1.224 1.582l2.91 2.25a1 1 0 0 0 1.374-.144l5.09-6ZM16 10a1 1 0 1 1 0 2H8a1 1 0 1 1 0-2h8ZM12 7a1 1 0 0 0-1-1H8a1 1 0 1 0 0 2h3a1 1 0 0 0 1-1Z"/></g></symbol><symbol id="icon-eds-i-refresh-medium" viewBox="0 0 24 24"><g><path d="M7.831 5.636H6.032A8.76 8.76 0 0 1 9 3.631 8.549 8.549 0 0 1 12.232 3c.603 0 1.192.063 1.76.182C17.979 4.017 21 7.632 21 12a1 1 0 1 0 2 0c0-5.296-3.674-9.746-8.591-10.776A10.61 10.61 0 0 0 5 3.851V2.805a1 1 0 0 0-.987-1H4a1 1 0 0 0-1 1v3.831a1 1 0 0 0 1 1h3.831a1 1 0 0 0 .013-2h-.013ZM17.968 18.364c-1.59 1.632-3.784 2.636-6.2 2.636C6.948 21 3 16.993 3 12a1 1 0 1 0-2 0c0 6.053 4.799 11 10.768 11 2.788 0 5.324-1.082 7.232-2.85v1.045a1 1 0 1 0 2 0v-3.831a1 1 0 0 0-1-1h-3.831a1 1 0 0 0 0 2h1.799Z"/></g></symbol><symbol id="icon-eds-i-search-medium" viewBox="0 0 24 24"><path d="M11 1c5.523 0 10 4.477 10 10 0 2.4-.846 4.604-2.256 6.328l3.963 3.965a1 1 0 0 1-1.414 1.414l-3.965-3.963A9.959 9.959 0 0 1 11 21C5.477 21 1 16.523 1 11S5.477 1 11 1Zm0 2a8 8 0 1 0 0 16 8 8 0 0 0 0-16Z"/></symbol><symbol id="icon-eds-i-settings-medium" viewBox="0 0 24 24"><path d="M11.382 1h1.24a2.508 2.508 0 0 1 2.334 1.63l.523 1.378 1.59.933 1.444-.224c.954-.132 1.89.3 2.422 1.101l.095.155.598 1.066a2.56 2.56 0 0 1-.195 2.848l-.894 1.161v1.896l.92 1.163c.6.768.707 1.812.295 2.674l-.09.17-.606 1.08a2.504 2.504 0 0 1-2.531 1.25l-1.428-.223-1.589.932-.523 1.378a2.512 2.512 0 0 1-2.155 1.625L12.65 23h-1.27a2.508 2.508 0 0 1-2.334-1.63l-.524-1.379-1.59-.933-1.443.225c-.954.132-1.89-.3-2.422-1.101l-.095-.155-.598-1.066a2.56 2.56 0 0 1 .195-2.847l.891-1.161v-1.898l-.919-1.162a2.562 2.562 0 0 1-.295-2.674l.09-.17.606-1.08a2.504 2.504 0 0 1 2.531-1.25l1.43.223 1.618-.938.524-1.375.07-.167A2.507 2.507 0 0 1 11.382 1Zm.003 2a.509.509 0 0 0-.47.338l-.65 1.71a1 1 0 0 1-.434.51L7.6 6.85a1 1 0 0 1-.655.123l-1.762-.275a.497.497 0 0 0-.498.252l-.61 1.088a.562.562 0 0 0 .04.619l1.13 1.43a1 1 0 0 1 .216.62v2.585a1 1 0 0 1-.207.61L4.15 15.339a.568.568 0 0 0-.036.634l.601 1.072a.494.494 0 0 0 .484.26l1.78-.278a1 1 0 0 1 .66.126l2.2 1.292a1 1 0 0 1 .43.507l.648 1.71a.508.508 0 0 0 .467.338h1.263a.51.51 0 0 0 .47-.34l.65-1.708a1 1 0 0 1 .428-.507l2.201-1.292a1 1 0 0 1 .66-.126l1.763.275a.497.497 0 0 0 .498-.252l.61-1.088a.562.562 0 0 0-.04-.619l-1.13-1.43a1 1 0 0 1-.216-.62v-2.585a1 1 0 0 1 .207-.61l1.105-1.437a.568.568 0 0 0 .037-.634l-.601-1.072a.494.494 0 0 0-.484-.26l-1.78.278a1 1 0 0 1-.66-.126l-2.2-1.292a1 1 0 0 1-.43-.507l-.649-1.71A.508.508 0 0 0 12.62 3h-1.234ZM12 8a4 4 0 1 1 0 8 4 4 0 0 1 0-8Zm0 2a2 2 0 1 0 0 4 2 2 0 0 0 0-4Z"/></symbol><symbol id="icon-eds-i-shipping-medium" viewBox="0 0 24 24"><path d="M16.515 2c1.406 0 2.706.728 3.352 1.902l2.02 3.635.02.042.036.089.031.105.012.058.01.073.004.075v11.577c0 .64-.244 1.255-.683 1.713a2.356 2.356 0 0 1-1.701.731H4.386a2.356 2.356 0 0 1-1.702-.731 2.476 2.476 0 0 1-.683-1.713V7.948c.01-.217.083-.43.22-.6L4.2 3.905C4.833 2.755 6.089 2.032 7.486 2h9.029ZM20 9H4v10.556a.49.49 0 0 0 .075.26l.053.07a.356.356 0 0 0 .257.114h15.23c.094 0 .186-.04.258-.115a.477.477 0 0 0 .127-.33V9Zm-2 7.5a1 1 0 0 1 0 2h-4a1 1 0 0 1 0-2h4ZM16.514 4H13v3h6.3l-1.183-2.13c-.288-.522-.908-.87-1.603-.87ZM11 3.999H7.51c-.679.017-1.277.36-1.566.887L4.728 7H11V3.999Z"/></symbol><symbol id="icon-eds-i-step-guide-medium" viewBox="0 0 24 24"><path d="M11.394 9.447a1 1 0 1 0-1.788-.894l-.88 1.759-.019-.02a1 1 0 1 0-1.414 1.415l1 1a1 1 0 0 0 1.601-.26l1.5-3ZM12 11a1 1 0 0 1 1-1h3a1 1 0 1 1 0 2h-3a1 1 0 0 1-1-1ZM12 17a1 1 0 0 1 1-1h3a1 1 0 1 1 0 2h-3a1 1 0 0 1-1-1ZM10.947 14.105a1 1 0 0 1 .447 1.342l-1.5 3a1 1 0 0 1-1.601.26l-1-1a1 1 0 1 1 1.414-1.414l.02.019.879-1.76a1 1 0 0 1 1.341-.447Z"/><path d="M5.545 1A2.542 2.542 0 0 0 3 3.538v16.924A2.542 2.542 0 0 0 5.545 23h12.91A2.542 2.542 0 0 0 21 20.462V7.5a1 1 0 0 0-.293-.707l-5.5-5.5A1 1 0 0 0 14.5 1H5.545ZM5 3.538C5 3.245 5.24 3 5.545 3h8.54L19 7.914v12.547c0 .294-.24.539-.546.539H5.545A.542.542 0 0 1 5 20.462V3.538Z" clip-rule="evenodd"/></symbol><symbol id="icon-eds-i-submission-medium" viewBox="0 0 24 24"><g><path d="M5 3.538C5 3.245 5.24 3 5.545 3h9.633L20 7.8v12.662a.535.535 0 0 1-.158.379.549.549 0 0 1-.387.159H6a1 1 0 0 1-1-1v-2.5a1 1 0 1 0-2 0V20a3 3 0 0 0 3 3h13.455c.673 0 1.32-.266 1.798-.742A2.535 2.535 0 0 0 22 20.462V7.385a1 1 0 0 0-.294-.709l-5.41-5.385A1 1 0 0 0 15.591 1H5.545A2.542 2.542 0 0 0 3 3.538V7a1 1 0 0 0 2 0V3.538Z"/><path d="m13.707 13.707-4 4a1 1 0 0 1-1.414 0l-.083-.094a1 1 0 0 1 .083-1.32L10.585 14 2 14a1 1 0 1 1 0-2l8.583.001-2.29-2.294a1 1 0 0 1 1.414-1.414l4.037 4.04.043.05.043.06.059.098.03.063.031.085.03.113.017.122L14 13l-.004.087-.017.118-.013.056-.034.104-.049.105-.048.081-.07.093-.058.063Z"/></g></symbol><symbol id="icon-eds-i-table-1-medium" viewBox="0 0 24 24"><path d="M4.385 22a2.56 2.56 0 0 1-1.14-.279C2.485 21.341 2 20.614 2 19.615V4.385c0-.315.067-.716.279-1.14C2.659 2.485 3.386 2 4.385 2h15.23c.315 0 .716.067 1.14.279.76.38 1.245 1.107 1.245 2.106v15.23c0 .315-.067.716-.279 1.14-.38.76-1.107 1.245-2.106 1.245H4.385ZM4 19.615c0 .213.034.265.14.317a.71.71 0 0 0 .245.068H8v-4H4v3.615ZM20 16H10v4h9.615c.213 0 .265-.034.317-.14a.71.71 0 0 0 .068-.245V16Zm0-2v-4H10v4h10ZM4 14h4v-4H4v4ZM19.615 4H10v4h10V4.385c0-.213-.034-.265-.14-.317A.71.71 0 0 0 19.615 4ZM8 4H4.385l-.082.002c-.146.01-.19.047-.235.138A.71.71 0 0 0 4 4.385V8h4V4Z"/></symbol><symbol id="icon-eds-i-table-2-medium" viewBox="0 0 24 24"><path d="M4.384 22A2.384 2.384 0 0 1 2 19.616V4.384A2.384 2.384 0 0 1 4.384 2h15.232A2.384 2.384 0 0 1 22 4.384v15.232A2.384 2.384 0 0 1 19.616 22H4.384ZM10 15H4v4.616c0 .212.172.384.384.384H10v-5Zm5 0h-3v5h3v-5Zm5 0h-3v5h2.616a.384.384 0 0 0 .384-.384V15ZM10 9H4v4h6V9Zm5 0h-3v4h3V9Zm5 0h-3v4h3V9Zm-.384-5H4.384A.384.384 0 0 0 4 4.384V7h16V4.384A.384.384 0 0 0 19.616 4Z"/></symbol><symbol id="icon-eds-i-tag-medium" viewBox="0 0 24 24"><path d="m12.621 1.998.127.004L20.496 2a1.5 1.5 0 0 1 1.497 1.355L22 3.5l-.005 7.669c.038.456-.133.905-.447 1.206l-9.02 9.018a2.075 2.075 0 0 1-2.932 0l-6.99-6.99a2.075 2.075 0 0 1 .001-2.933L11.61 2.47c.246-.258.573-.418.881-.46l.131-.011Zm.286 2-8.885 8.886a.075.075 0 0 0 0 .106l6.987 6.988c.03.03.077.03.106 0l8.883-8.883L19.999 4l-7.092-.002ZM16 6.5a1.5 1.5 0 0 1 .144 2.993L16 9.5a1.5 1.5 0 0 1 0-3Z"/></symbol><symbol id="icon-eds-i-trash-medium" viewBox="0 0 24 24"><path d="M12 1c2.717 0 4.913 2.232 4.997 5H21a1 1 0 0 1 0 2h-1v12.5c0 1.389-1.152 2.5-2.556 2.5H6.556C5.152 23 4 21.889 4 20.5V8H3a1 1 0 1 1 0-2h4.003l.001-.051C7.114 3.205 9.3 1 12 1Zm6 7H6v12.5c0 .238.19.448.454.492l.102.008h10.888c.315 0 .556-.232.556-.5V8Zm-4 3a1 1 0 0 1 1 1v6.005a1 1 0 0 1-2 0V12a1 1 0 0 1 1-1Zm-4 0a1 1 0 0 1 1 1v6a1 1 0 0 1-2 0v-6a1 1 0 0 1 1-1Zm2-8c-1.595 0-2.914 1.32-2.996 3h5.991v-.02C14.903 4.31 13.589 3 12 3Z"/></symbol><symbol id="icon-eds-i-user-account-medium" viewBox="0 0 24 24"><path d="M12 1c6.075 0 11 4.925 11 11s-4.925 11-11 11S1 18.075 1 12 5.925 1 12 1Zm0 16c-1.806 0-3.52.994-4.664 2.698A8.947 8.947 0 0 0 12 21a8.958 8.958 0 0 0 4.664-1.301C15.52 17.994 13.806 17 12 17Zm0-14a9 9 0 0 0-6.25 15.476C7.253 16.304 9.54 15 12 15s4.747 1.304 6.25 3.475A9 9 0 0 0 12 3Zm0 3a4 4 0 1 1 0 8 4 4 0 0 1 0-8Zm0 2a2 2 0 1 0 0 4 2 2 0 0 0 0-4Z"/></symbol><symbol id="icon-eds-i-user-add-medium" viewBox="0 0 24 24"><path d="M9 1a5 5 0 1 1 0 10A5 5 0 0 1 9 1Zm0 2a3 3 0 1 0 0 6 3 3 0 0 0 0-6Zm9 10a1 1 0 0 1 1 1v3h3a1 1 0 0 1 0 2h-3v3a1 1 0 0 1-2 0v-3h-3a1 1 0 0 1 0-2h3v-3a1 1 0 0 1 1-1Zm-5.545-.15a1 1 0 1 1-.91 1.78 5.713 5.713 0 0 0-5.705.282c-1.67 1.068-2.728 2.927-2.832 4.956L3.004 20 11.5 20a1 1 0 0 1 .993.883L12.5 21a1 1 0 0 1-1 1H2a1 1 0 0 1-1-1v-.876c.028-2.812 1.446-5.416 3.763-6.897a7.713 7.713 0 0 1 7.692-.378Z"/></symbol><symbol id="icon-eds-i-user-assign-medium" viewBox="0 0 24 24"><path d="M16.226 13.298a1 1 0 0 1 1.414-.01l.084.093a1 1 0 0 1-.073 1.32L15.39 17H22a1 1 0 0 1 0 2h-6.611l2.262 2.298a1 1 0 0 1-1.425 1.404l-3.939-4a1 1 0 0 1 0-1.404l3.94-4Zm-3.771-.449a1 1 0 1 1-.91 1.781 5.713 5.713 0 0 0-5.705.282c-1.67 1.068-2.728 2.927-2.832 4.956L3.004 20 10.5 20a1 1 0 0 1 .993.883L11.5 21a1 1 0 0 1-1 1H2a1 1 0 0 1-1-1v-.876c.028-2.812 1.446-5.416 3.763-6.897a7.713 7.713 0 0 1 7.692-.378ZM9 1a5 5 0 1 1 0 10A5 5 0 0 1 9 1Zm0 2a3 3 0 1 0 0 6 3 3 0 0 0 0-6Z"/></symbol><symbol id="icon-eds-i-user-block-medium" viewBox="0 0 24 24"><path d="M9 1a5 5 0 1 1 0 10A5 5 0 0 1 9 1Zm0 2a3 3 0 1 0 0 6 3 3 0 0 0 0-6Zm9 10a5 5 0 1 1 0 10 5 5 0 0 1 0-10Zm-5.545-.15a1 1 0 1 1-.91 1.78 5.713 5.713 0 0 0-5.705.282c-1.67 1.068-2.728 2.927-2.832 4.956L3.004 20 11.5 20a1 1 0 0 1 .993.883L12.5 21a1 1 0 0 1-1 1H2a1 1 0 0 1-1-1v-.876c.028-2.812 1.446-5.416 3.763-6.897a7.713 7.713 0 0 1 7.692-.378ZM15 18a3 3 0 0 0 4.294 2.707l-4.001-4c-.188.391-.293.83-.293 1.293Zm3-3c-.463 0-.902.105-1.294.293l4.001 4A3 3 0 0 0 18 15Z"/></symbol><symbol id="icon-eds-i-user-check-medium" viewBox="0 0 24 24"><path d="M9 1a5 5 0 1 1 0 10A5 5 0 0 1 9 1Zm0 2a3 3 0 1 0 0 6 3 3 0 0 0 0-6Zm13.647 12.237a1 1 0 0 1 .116 1.41l-5.091 6a1 1 0 0 1-1.375.144l-2.909-2.25a1 1 0 1 1 1.224-1.582l2.153 1.665 4.472-5.271a1 1 0 0 1 1.41-.116Zm-8.139-.977c.22.214.428.44.622.678a1 1 0 1 1-1.548 1.266 6.025 6.025 0 0 0-1.795-1.49.86.86 0 0 1-.163-.048l-.079-.036a5.721 5.721 0 0 0-2.62-.63l-.194.006c-2.76.134-5.022 2.177-5.592 4.864l-.035.175-.035.213c-.03.201-.05.405-.06.61L3.003 20 10 20a1 1 0 0 1 .993.883L11 21a1 1 0 0 1-1 1H2a1 1 0 0 1-1-1v-.876l.005-.223.02-.356.02-.222.03-.248.022-.15c.02-.133.044-.265.071-.397.44-2.178 1.725-4.105 3.595-5.301a7.75 7.75 0 0 1 3.755-1.215l.12-.004a7.908 7.908 0 0 1 5.87 2.252Z"/></symbol><symbol id="icon-eds-i-user-delete-medium" viewBox="0 0 24 24"><path d="M9 1a5 5 0 1 1 0 10A5 5 0 0 1 9 1Zm0 2a3 3 0 1 0 0 6 3 3 0 0 0 0-6ZM4.763 13.227a7.713 7.713 0 0 1 7.692-.378 1 1 0 1 1-.91 1.781 5.713 5.713 0 0 0-5.705.282c-1.67 1.068-2.728 2.927-2.832 4.956L3.004 20H11.5a1 1 0 0 1 .993.883L12.5 21a1 1 0 0 1-1 1H2a1 1 0 0 1-1-1v-.876c.028-2.812 1.446-5.416 3.763-6.897Zm11.421 1.543 2.554 2.553 2.555-2.553a1 1 0 0 1 1.414 1.414l-2.554 2.554 2.554 2.555a1 1 0 0 1-1.414 1.414l-2.555-2.554-2.554 2.554a1 1 0 0 1-1.414-1.414l2.553-2.555-2.553-2.554a1 1 0 0 1 1.414-1.414Z"/></symbol><symbol id="icon-eds-i-user-edit-medium" viewBox="0 0 24 24"><path d="m19.876 10.77 2.831 2.83a1 1 0 0 1 0 1.415l-7.246 7.246a1 1 0 0 1-.572.284l-3.277.446a1 1 0 0 1-1.125-1.13l.461-3.277a1 1 0 0 1 .283-.567l7.23-7.246a1 1 0 0 1 1.415-.001Zm-7.421 2.08a1 1 0 1 1-.91 1.78 5.713 5.713 0 0 0-5.705.282c-1.67 1.068-2.728 2.927-2.832 4.956L3.004 20 7.5 20a1 1 0 0 1 .993.883L8.5 21a1 1 0 0 1-1 1H2a1 1 0 0 1-1-1v-.876c.028-2.812 1.446-5.416 3.763-6.897a7.713 7.713 0 0 1 7.692-.378Zm6.715.042-6.29 6.3-.23 1.639 1.633-.222 6.302-6.302-1.415-1.415ZM9 1a5 5 0 1 1 0 10A5 5 0 0 1 9 1Zm0 2a3 3 0 1 0 0 6 3 3 0 0 0 0-6Z"/></symbol><symbol id="icon-eds-i-user-linked-medium" viewBox="0 0 24 24"><path d="M15.65 6c.31 0 .706.066 1.122.274C17.522 6.65 18 7.366 18 8.35v12.3c0 .31-.066.706-.274 1.122-.375.75-1.092 1.228-2.076 1.228H3.35a2.52 2.52 0 0 1-1.122-.274C1.478 22.35 1 21.634 1 20.65V8.35c0-.31.066-.706.274-1.122C1.65 6.478 2.366 6 3.35 6h12.3Zm0 2-12.376.002c-.134.007-.17.04-.21.12A.672.672 0 0 0 3 8.35v12.3c0 .198.028.24.122.287.09.044.2.063.228.063h.887c.788-2.269 2.814-3.5 5.263-3.5 2.45 0 4.475 1.231 5.263 3.5h.887c.198 0 .24-.028.287-.122.044-.09.063-.2.063-.228V8.35c0-.198-.028-.24-.122-.287A.672.672 0 0 0 15.65 8ZM9.5 19.5c-1.36 0-2.447.51-3.06 1.5h6.12c-.613-.99-1.7-1.5-3.06-1.5ZM20.65 1A2.35 2.35 0 0 1 23 3.348V15.65A2.35 2.35 0 0 1 20.65 18H20a1 1 0 0 1 0-2h.65a.35.35 0 0 0 .35-.35V3.348A.35.35 0 0 0 20.65 3H8.35a.35.35 0 0 0-.35.348V4a1 1 0 1 1-2 0v-.652A2.35 2.35 0 0 1 8.35 1h12.3ZM9.5 10a3.5 3.5 0 1 1 0 7 3.5 3.5 0 0 1 0-7Zm0 2a1.5 1.5 0 1 0 0 3 1.5 1.5 0 0 0 0-3Z"/></symbol><symbol id="icon-eds-i-user-multiple-medium" viewBox="0 0 24 24"><path d="M9 1a5 5 0 1 1 0 10A5 5 0 0 1 9 1Zm6 0a5 5 0 0 1 0 10 1 1 0 0 1-.117-1.993L15 9a3 3 0 0 0 0-6 1 1 0 0 1 0-2ZM9 3a3 3 0 1 0 0 6 3 3 0 0 0 0-6Zm8.857 9.545a7.99 7.99 0 0 1 2.651 1.715A8.31 8.31 0 0 1 23 20.134V21a1 1 0 0 1-1 1h-3a1 1 0 0 1 0-2h1.995l-.005-.153a6.307 6.307 0 0 0-1.673-3.945l-.204-.209a5.99 5.99 0 0 0-1.988-1.287 1 1 0 1 1 .732-1.861Zm-3.349 1.715A8.31 8.31 0 0 1 17 20.134V21a1 1 0 0 1-1 1H2a1 1 0 0 1-1-1v-.877c.044-4.343 3.387-7.908 7.638-8.115a7.908 7.908 0 0 1 5.87 2.252ZM9.016 14l-.285.006c-3.104.15-5.58 2.718-5.725 5.9L3.004 20h11.991l-.005-.153a6.307 6.307 0 0 0-1.673-3.945l-.204-.209A5.924 5.924 0 0 0 9.3 14.008L9.016 14Z"/></symbol><symbol id="icon-eds-i-user-notify-medium" viewBox="0 0 24 24"><path d="M9 1a5 5 0 1 1 0 10A5 5 0 0 1 9 1Zm0 2a3 3 0 1 0 0 6 3 3 0 0 0 0-6Zm10 18v1a1 1 0 0 1-2 0v-1h-3a1 1 0 0 1 0-2v-2.818C14 13.885 15.777 12 18 12s4 1.885 4 4.182V19a1 1 0 0 1 0 2h-3Zm-6.545-8.15a1 1 0 1 1-.91 1.78 5.713 5.713 0 0 0-5.705.282c-1.67 1.068-2.728 2.927-2.832 4.956L3.004 20 11.5 20a1 1 0 0 1 .993.883L12.5 21a1 1 0 0 1-1 1H2a1 1 0 0 1-1-1v-.876c.028-2.812 1.446-5.416 3.763-6.897a7.713 7.713 0 0 1 7.692-.378ZM18 14c-1.091 0-2 .964-2 2.182V19h4v-2.818c0-1.165-.832-2.098-1.859-2.177L18 14Z"/></symbol><symbol id="icon-eds-i-user-remove-medium" viewBox="0 0 24 24"><path d="M9 1a5 5 0 1 1 0 10A5 5 0 0 1 9 1Zm0 2a3 3 0 1 0 0 6 3 3 0 0 0 0-6Zm3.455 9.85a1 1 0 1 1-.91 1.78 5.713 5.713 0 0 0-5.705.282c-1.67 1.068-2.728 2.927-2.832 4.956L3.004 20 11.5 20a1 1 0 0 1 .993.883L12.5 21a1 1 0 0 1-1 1H2a1 1 0 0 1-1-1v-.876c.028-2.812 1.446-5.416 3.763-6.897a7.713 7.713 0 0 1 7.692-.378ZM22 17a1 1 0 0 1 0 2h-8a1 1 0 0 1 0-2h8Z"/></symbol><symbol id="icon-eds-i-user-single-medium" viewBox="0 0 24 24"><path d="M12 1a5 5 0 1 1 0 10 5 5 0 0 1 0-10Zm0 2a3 3 0 1 0 0 6 3 3 0 0 0 0-6Zm-.406 9.008a8.965 8.965 0 0 1 6.596 2.494A9.161 9.161 0 0 1 21 21.025V22a1 1 0 0 1-1 1H4a1 1 0 0 1-1-1v-.985c.05-4.825 3.815-8.777 8.594-9.007Zm.39 1.992-.299.006c-3.63.175-6.518 3.127-6.678 6.775L5 21h13.998l-.009-.268a7.157 7.157 0 0 0-1.97-4.573l-.214-.213A6.967 6.967 0 0 0 11.984 14Z"/></symbol><symbol id="icon-eds-i-warning-circle-medium" viewBox="0 0 24 24"><path d="M12 1c6.075 0 11 4.925 11 11s-4.925 11-11 11S1 18.075 1 12 5.925 1 12 1Zm0 2a9 9 0 1 0 0 18 9 9 0 0 0 0-18Zm0 11.5a1.5 1.5 0 0 1 .144 2.993L12 17.5a1.5 1.5 0 0 1 0-3ZM12 6a1 1 0 0 1 1 1v5a1 1 0 0 1-2 0V7a1 1 0 0 1 1-1Z"/></symbol><symbol id="icon-eds-i-warning-filled-medium" viewBox="0 0 24 24"><path d="M12 1c6.075 0 11 4.925 11 11s-4.925 11-11 11S1 18.075 1 12 5.925 1 12 1Zm0 13.5a1.5 1.5 0 0 0 0 3l.144-.007A1.5 1.5 0 0 0 12 14.5ZM12 6a1 1 0 0 0-1 1v5a1 1 0 0 0 2 0V7a1 1 0 0 0-1-1Z"/></symbol><symbol id="icon-eds-i-work-medium" viewBox="0 0 24 24"><path d="M4 3.53808C4 3.24519 4.23975 3 4.5451 3H14.1778L19 7.80031V8C19 8.55228 19.4477 9 20 9C20.5523 9 21 8.55228 21 8V7.38477C21 7.11876 20.894 6.86372 20.7055 6.67605L15.2962 1.29129C15.1088 1.10473 14.8551 1 14.5907 1H4.5451C3.14377 1 2 2.13206 2 3.53808V20.5007C2 21.882 3.11988 23 4.5 23H8C8.55228 23 9 22.5523 9 22C9 21.4477 8.55228 21 8 21H4.5C4.22327 21 4 20.7762 4 20.5007V3.53808Z"/><path fill-rule="evenodd" clip-rule="evenodd" d="M19.8764 10.7698C19.6887 10.5822 19.4341 10.4768 19.1687 10.4769C18.9033 10.4771 18.6489 10.5827 18.4614 10.7706L11.2306 18.0167C11.0776 18.1701 10.9785 18.3691 10.9483 18.5836L10.4867 21.8605C10.443 22.1707 10.5472 22.4835 10.7682 22.7055C10.9892 22.9275 11.3015 23.0331 11.6118 22.9909L14.8888 22.5447C15.1054 22.5152 15.3064 22.4155 15.461 22.261L22.7071 15.0148C22.8947 14.8273 23 14.5729 23 14.3077C23 14.0425 22.8947 13.7881 22.7071 13.6006L19.8764 10.7698ZM12.8821 19.1931L19.17 12.8919L20.5858 14.3077L14.285 20.6085L12.6515 20.8309L12.8821 19.1931Z"/><path d="M11.0812 4.68628C11.5307 5.00729 11.6347 5.63184 11.3137 6.08125L8.81373 9.58125C8.64288 9.82045 8.37543 9.97236 8.08248 9.99661C7.78953 10.0209 7.50075 9.91498 7.29289 9.70712L5.79289 8.20712C5.40237 7.8166 5.40237 7.18343 5.79289 6.79291C6.18342 6.40239 6.81658 6.40239 7.20711 6.79291L7.8724 7.4582L9.68627 4.91878C10.0073 4.46937 10.6318 4.36527 11.0812 4.68628Z"/><path d="M11.3137 12.0813C11.6347 11.6318 11.5307 11.0073 11.0812 10.6863C10.6318 10.3653 10.0073 10.4694 9.68627 10.9188L7.8724 13.4582L7.20711 12.7929C6.81658 12.4024 6.18342 12.4024 5.79289 12.7929C5.40237 13.1834 5.40237 13.8166 5.79289 14.2071L7.29289 15.7071C7.50075 15.915 7.78953 16.0209 8.08248 15.9966C8.37543 15.9724 8.64288 15.8205 8.81373 15.5813L11.3137 12.0813Z"/></symbol><symbol id="icon-ai-stars"><path d="M22.294 13.39c.941.536.941 1.945 0 2.482l-3.613 2.061c-.228.13-.415.325-.54.563l-1.976 3.768a1.33 1.33 0 0 1-2.38 0l-1.977-3.768a1.4 1.4 0 0 0-.539-.563l-3.614-2.061c-.94-.537-.94-1.946 0-2.482l3.614-2.061c.228-.13.415-.325.54-.563l1.976-3.768a1.33 1.33 0 0 1 2.38 0l1.977 3.768c.124.238.311.433.539.563zM10.08 4.861c1.044.508 1.044 2.056 0 2.564l-1.543.751c-.29.14-.521.383-.656.684l-.72 1.61a1.334 1.334 0 0 1-2.459 0l-.72-1.61a1.4 1.4 0 0 0-.656-.684l-1.543-.751c-1.044-.508-1.044-2.056 0-2.564l1.543-.751c.29-.14.521-.383.656-.684l.72-1.61a1.334 1.334 0 0 1 2.459 0l.72 1.61c.135.301.367.543.656.684z"/></symbol><symbol id="icon-chevron-left-medium" viewBox="0 0 24 24"><path d="M15.7194 3.3054C15.3358 2.90809 14.7027 2.89699 14.3054 3.28061L6.54342 10.7757C6.19804 11.09 6 11.5335 6 12C6 12.4665 6.19804 12.91 6.5218 13.204L14.3054 20.7194C14.7027 21.103 15.3358 21.0919 15.7194 20.6946C16.103 20.2973 16.0919 19.6642 15.6946 19.2806L8.155 12L15.6946 4.71939C16.0614 4.36528 16.099 3.79863 15.8009 3.40105L15.7194 3.3054Z"/></symbol><symbol id="icon-chevron-right-medium" viewBox="0 0 24 24"><path d="M8.28061 3.3054C8.66423 2.90809 9.29729 2.89699 9.6946 3.28061L17.4566 10.7757C17.802 11.09 18 11.5335 18 12C18 12.4665 17.802 12.91 17.4782 13.204L9.6946 20.7194C9.29729 21.103 8.66423 21.0919 8.28061 20.6946C7.89699 20.2973 7.90809 19.6642 8.3054 19.2806L15.845 12L8.3054 4.71939C7.93865 4.36528 7.90098 3.79863 8.19908 3.40105L8.28061 3.3054Z"/></symbol><symbol id="icon-citations-medium-green-dot" viewBox="0 0 32 32"><path fill-rule="evenodd" clip-rule="evenodd" d="M19.7876 1.33301C20.1402 1.33301 20.4784 1.47265 20.7282 1.72139L27.9407 8.90108C28.192 9.1513 28.3333 9.49136 28.3333 9.84603V26.2822C28.3333 27.181 27.9748 28.0426 27.3373 28.6773C26.7009 29.3108 25.8386 29.6663 24.9399 29.6663H17.1491C16.4128 29.6663 15.8158 29.0694 15.8158 28.333C15.8158 27.5966 16.4128 26.9997 17.1491 26.9997H24.9399C25.1339 26.9997 25.3196 26.9231 25.4559 26.7874C25.5911 26.6529 25.6667 26.4712 25.6667 26.2822V10.3997L19.2373 3.99967H8.39346C8.03099 3.99967 7.73226 4.26096 7.67614 4.60108L7.66667 4.71712V14.1929C7.66667 14.9293 7.06971 15.5262 6.33333 15.5262C5.59695 15.5262 5 14.9293 5 14.1929V4.71712C5 2.84677 6.52066 1.33301 8.39346 1.33301H19.7876Z"/><path style="fill:#00A69D" d="M7 19C3.68629 19 1 21.6863 1 25C1 28.3137 3.68629 31 7 31C10.3137 31 13 28.3137 13 25C13 21.6863 10.3137 19 7 19Z"/></symbol><symbol id="icon-eds-alerts" viewBox="0 0 32 32"><path d="M28 12.667c.736 0 1.333.597 1.333 1.333v13.333A3.333 3.333 0 0 1 26 30.667H6a3.333 3.333 0 0 1-3.333-3.334V14a1.333 1.333 0 1 1 2.666 0v1.252L16 21.769l10.667-6.518V14c0-.736.597-1.333 1.333-1.333Zm-1.333 5.71-9.972 6.094c-.427.26-.963.26-1.39 0l-9.972-6.094v8.956c0 .368.299.667.667.667h20a.667.667 0 0 0 .667-.667v-8.956ZM19.333 12a1.333 1.333 0 1 1 0 2.667h-6.666a1.333 1.333 0 1 1 0-2.667h6.666Zm4-10.667a3.333 3.333 0 0 1 3.334 3.334v6.666a1.333 1.333 0 1 1-2.667 0V4.667A.667.667 0 0 0 23.333 4H8.667A.667.667 0 0 0 8 4.667v6.666a1.333 1.333 0 1 1-2.667 0V4.667a3.333 3.333 0 0 1 3.334-3.334h14.666Zm-4 5.334a1.333 1.333 0 0 1 0 2.666h-6.666a1.333 1.333 0 1 1 0-2.666h6.666Z"/></symbol><symbol id="icon-eds-arrow-up" viewBox="0 0 24 24"><path fill-rule="evenodd" d="m13.002 7.408 4.88 4.88a.99.99 0 0 0 1.32.08l.09-.08c.39-.39.39-1.03 0-1.42l-6.58-6.58a1.01 1.01 0 0 0-1.42 0l-6.58 6.58a1 1 0 0 0-.09 1.32l.08.1a1 1 0 0 0 1.42-.01l4.88-4.87v11.59a.99.99 0 0 0 .88.99l.12.01c.55 0 1-.45 1-1V7.408z" class="layer"/></symbol><symbol id="icon-eds-checklist" viewBox="0 0 32 32"><path d="M19.2 1.333a3.468 3.468 0 0 1 3.381 2.699L24.667 4C26.515 4 28 5.52 28 7.38v19.906c0 1.86-1.485 3.38-3.333 3.38H7.333c-1.848 0-3.333-1.52-3.333-3.38V7.38C4 5.52 5.485 4 7.333 4h2.093A3.468 3.468 0 0 1 12.8 1.333h6.4ZM9.426 6.667H7.333c-.36 0-.666.312-.666.713v19.906c0 .401.305.714.666.714h17.334c.36 0 .666-.313.666-.714V7.38c0-.4-.305-.713-.646-.714l-2.121.033A3.468 3.468 0 0 1 19.2 9.333h-6.4a3.468 3.468 0 0 1-3.374-2.666Zm12.715 5.606c.586.446.7 1.283.253 1.868l-7.111 9.334a1.333 1.333 0 0 1-1.792.306l-3.556-2.333a1.333 1.333 0 1 1 1.463-2.23l2.517 1.651 6.358-8.344a1.333 1.333 0 0 1 1.868-.252ZM19.2 4h-6.4a.8.8 0 0 0-.8.8v1.067a.8.8 0 0 0 .8.8h6.4a.8.8 0 0 0 .8-.8V4.8a.8.8 0 0 0-.8-.8Z"/></symbol><symbol id="icon-eds-citation" viewBox="0 0 36 36"><path d="M23.25 1.5a1.5 1.5 0 0 1 1.06.44l8.25 8.25a1.5 1.5 0 0 1 .44 1.06v19.5c0 2.105-1.645 3.75-3.75 3.75H18a1.5 1.5 0 0 1 0-3h11.25c.448 0 .75-.302.75-.75V11.873L22.628 4.5H8.31a.811.811 0 0 0-.8.68l-.011.13V16.5a1.5 1.5 0 0 1-3 0V5.31A3.81 3.81 0 0 1 8.31 1.5h14.94ZM8.223 20.358a.984.984 0 0 1-.192 1.378l-.048.034c-.54.36-.942.676-1.206.951-.59.614-.885 1.395-.885 2.343.115-.028.288-.042.518-.042.662 0 1.26.237 1.791.711.533.474.799 1.074.799 1.799 0 .753-.259 1.352-.777 1.799-.518.446-1.151.669-1.9.669-1.006 0-1.812-.293-2.417-.878C3.302 28.536 3 27.657 3 26.486c0-1.115.165-2.085.496-2.907.331-.823.734-1.513 1.209-2.071.475-.558.971-.997 1.49-1.318a6.01 6.01 0 0 1 .347-.2 1.321 1.321 0 0 1 1.681.368Zm7.5 0a.984.984 0 0 1-.192 1.378l-.048.034c-.54.36-.942.676-1.206.951-.59.614-.885 1.395-.885 2.343.115-.028.288-.042.518-.042.662 0 1.26.237 1.791.711.533.474.799 1.074.799 1.799 0 .753-.259 1.352-.777 1.799-.518.446-1.151.669-1.9.669-1.006 0-1.812-.293-2.417-.878-.604-.586-.906-1.465-.906-2.636 0-1.115.165-2.085.496-2.907.331-.823.734-1.513 1.209-2.071.475-.558.971-.997 1.49-1.318a6.01 6.01 0 0 1 .347-.2 1.321 1.321 0 0 1 1.681.368Z"/></symbol><symbol id="icon-eds-i-access-indicator" viewBox="0 0 16 16"><circle cx="4.5" cy="11.5" r="3.5" style="fill:currentColor"/><path fill-rule="evenodd" d="M4 3v3a1 1 0 0 1-2 0V2.923C2 1.875 2.84 1 3.909 1h5.909a1 1 0 0 1 .713.298l3.181 3.231a1 1 0 0 1 .288.702v7.846c0 .505-.197.993-.554 1.354a1.902 1.902 0 0 1-1.355.569H10a1 1 0 1 1 0-2h2V5.64L9.4 3H4Z" clip-rule="evenodd" style="fill:#222"/></symbol><symbol id="icon-eds-i-accessibility-medium" viewBox="0 0 24 24"><path d="M17 10.5C17.5523 10.5 18 10.9477 18 11.5C18 12.0523 17.5523 12.5 17 12.5H13V13C13 13.4952 13.2735 14.3106 13.7695 15.3027C14.1249 16.0135 14.551 16.7321 14.9483 17.3564L15.332 17.9453L15.3848 18.0332C15.6218 18.4812 15.4855 19.0448 15.0547 19.332C14.6238 19.6193 14.0508 19.5282 13.7285 19.1367L13.668 19.0547L13.2627 18.4326C12.8412 17.7703 12.3781 16.9924 11.9844 16.2061C11.4619 17.2978 10.8292 18.309 10.332 19.0547C10.0257 19.5142 9.40486 19.6384 8.94533 19.332C8.4858 19.0257 8.36163 18.4048 8.66798 17.9453C9.15666 17.2123 9.75032 16.2592 10.2197 15.2617C10.6385 14.372 10.9221 13.5218 10.9863 12.8008L11 12.5H7.00002C6.44774 12.5 6.00002 12.0523 6.00002 11.5C6.00002 10.9477 6.44774 10.5 7.00002 10.5H17Z"/><path fill-rule="evenodd" clip-rule="evenodd" d="M12 4C12.7957 4 13.5585 4.3163 14.1211 4.87891C14.6837 5.44152 15 6.20435 15 7C15 7.79565 14.6837 8.55848 14.1211 9.12109C13.5585 9.6837 12.7957 10 12 10C11.2044 10 10.4415 9.6837 9.87892 9.12109C9.31631 8.55848 9.00002 7.79565 9.00002 7C9.00002 6.20435 9.31631 5.44152 9.87892 4.87891C10.4415 4.3163 11.2044 4 12 4ZM12 6C11.7348 6 11.4805 6.10543 11.293 6.29297C11.1054 6.4805 11 6.73478 11 7C11 7.26522 11.1054 7.5195 11.293 7.70703C11.4805 7.89457 11.7348 8 12 8C12.2652 8 12.5195 7.89457 12.707 7.70703C12.8946 7.5195 13 7.26522 13 7C13 6.73478 12.8946 6.48051 12.707 6.29297C12.5195 6.10543 12.2652 6 12 6Z"/><path fill-rule="evenodd" clip-rule="evenodd" d="M12 1C18.0751 1 23 5.92487 23 12C23 18.0751 18.0751 23 12 23C5.92488 23 1.00002 18.0751 1.00002 12C1.00002 5.92487 5.92488 1 12 1ZM12 3C7.02945 3 3.00002 7.02944 3.00002 12C3.00002 16.9706 7.02945 21 12 21C16.9706 21 21 16.9706 21 12C21 7.02944 16.9706 3 12 3Z"/></symbol><symbol id="icon-eds-i-book-research-medium"><path fill-rule="evenodd" d="M9.99.952c.922 0 1.822.273 2.589.784l2.42 1.614 2.421-1.614a4.667 4.667 0 0 1 2.283-.774l.306-.01h6.324a3.333 3.333 0 0 1 3.333 3.334v8.666a1.333 1.333 0 0 1-2.666 0V4.286a.667.667 0 0 0-.667-.667h-6.324a2 2 0 0 0-1.11.336l-2.566 1.71v5.954a1.333 1.333 0 1 1-2.667 0V5.666L11.1 3.955a2 2 0 0 0-1.11-.336H3.666A.667.667 0 0 0 3 4.286v17.333c0 .368.298.667.666.667h10a1.333 1.333 0 1 1 0 2.666h-10A3.333 3.333 0 0 1 .333 21.62V4.286A3.333 3.333 0 0 1 3.666.952H9.99Zm12.343 13.334a6 6 0 0 1 5.08 9.193l1.863 1.864a1.333 1.333 0 1 1-1.886 1.886l-1.864-1.863a6 6 0 1 1-3.193-11.08Zm-3.333 6a3.333 3.333 0 1 1 6.666 0 3.333 3.333 0 0 1-6.666 0Z" clip-rule="evenodd"/></symbol><symbol id="icon-eds-i-circle-bluesky" viewBox="0 0 25 24"><path d="M12.5 0c6.627 0 12 5.373 12 12s-5.373 12-12 12-12-5.373-12-12 5.373-12 12-12m7 7.44c0-2.158-1.877-1.48-3.035-.604-1.605 1.214-3.331 3.676-3.965 4.997-.634-1.321-2.36-3.783-3.965-4.997C7.377 5.96 5.5 5.282 5.5 7.439c0 .432.245 3.62.389 4.137.5 1.8 2.32 2.258 3.94 1.98-2.831.486-3.551 2.095-1.996 3.703 2.954 3.054 4.246-.766 4.577-1.745.061-.181.09-.265.09-.19 0-.075.029.009.09.19.33.98 1.623 4.8 4.577 1.745 1.555-1.608.835-3.217-1.996-3.702 1.62.277 3.44-.182 3.94-1.98.144-.518.389-3.706.389-4.138"/></symbol><symbol id="icon-eds-i-circle-facebook" viewBox="0 0 25 24"><path d="M12.5 0C5.872 0 .5 5.372.5 12s5.372 12 12 12 12-5.372 12-12-5.372-12-12-12m3.356 7.417h-1.62s-.858-.023-.93 1v1.836h2.186l-.017 2.454h-2.168l-.003 6.409-2.571-.004v-6.405H8.612V10.23h2.12V8.012s.096-2.574 2.598-2.884h2.526z"/></symbol><symbol id="icon-eds-i-circle-github" viewBox="0 0 24 24"><path fill-rule="evenodd" d="M11.964 0C5.348 0 0 5.5 0 12.304c0 5.44 3.427 10.043 8.18 11.673.595.122.813-.265.813-.59 0-.286-.02-1.264-.02-2.282-3.328.733-4.02-1.467-4.02-1.467-.536-1.426-1.328-1.793-1.328-1.793-1.09-.753.08-.753.08-.753 1.207.081 1.841 1.263 1.841 1.263 1.07 1.874 2.793 1.344 3.487 1.018.099-.795.416-1.344.752-1.65-2.654-.285-5.447-1.344-5.447-6.07 0-1.345.475-2.445 1.228-3.3-.119-.306-.535-1.57.12-3.26 0 0 1.01-.326 3.287 1.263.975-.27 1.981-.407 2.991-.408 1.01 0 2.04.143 2.991.408 2.278-1.59 3.288-1.263 3.288-1.263.654 1.69.238 2.954.12 3.26.772.855 1.227 1.955 1.227 3.3 0 4.725-2.792 5.764-5.467 6.07.436.387.812 1.12.812 2.282 0 1.65-.02 2.974-.02 3.381 0 .326.219.713.813.591 4.754-1.63 8.18-6.234 8.18-11.673C23.929 5.5 18.56 0 11.965 0" clip-rule="evenodd"/></symbol><symbol id="icon-eds-i-circle-instagram" viewBox="0 0 25 24"><g clip-path="url(#a)"><path d="M12.5 0C5.872 0 .5 5.372.5 12s5.372 12 12 12 12-5.372 12-12-5.372-12-12-12m6.687 15.313a3.377 3.377 0 0 1-3.374 3.373H9.188a3.377 3.377 0 0 1-3.373-3.373V8.687a3.377 3.377 0 0 1 3.373-3.374h6.627a3.377 3.377 0 0 1 3.373 3.374zm-3.374-8.544H9.188A1.92 1.92 0 0 0 7.27 8.687v6.627a1.92 1.92 0 0 0 1.918 1.918h6.627a1.92 1.92 0 0 0 1.918-1.918V8.686a1.92 1.92 0 0 0-1.918-1.918M12.5 15.444A3.45 3.45 0 0 1 9.056 12c0-1.9 1.545-3.444 3.444-3.444S15.944 10.1 15.944 12 14.4 15.444 12.5 15.444m3.444-6.073a.816.816 0 1 1 .002-1.632.816.816 0 0 1-.002 1.632m-3.444.64c-1.096 0-1.99.893-1.99 1.989s.894 1.99 1.99 1.99A1.99 1.99 0 0 0 14.489 12a1.99 1.99 0 0 0-1.989-1.989"/></g></symbol><symbol id="icon-eds-i-circle-linkedin" viewBox="0 0 25 24"><path d="M12.5 0C5.872 0 .5 5.373.5 12s5.372 12 12 12 12-5.373 12-12-5.373-12-12-12m-2.288 16.662h-2.24V9.765h2.24zM9.056 8.844a1.253 1.253 0 1 1 0-2.507 1.253 1.253 0 0 1 0 2.507m9.03 7.81h-2.119v-3.998s-.034-1.321-1.22-1.17c0 0-1.016-.05-1.135 1.237v3.914h-2.151v-6.86h2.032v.914s.576-1.135 2.067-1.101c0 0 2.405-.254 2.525 2.49z"/></symbol><symbol id="icon-eds-i-circle-x" viewBox="0 0 25 24"><path d="m12.59 11.233-2.397-3.427H8.915l2.97 4.247.373.534 2.542 3.636h1.278l-3.115-4.455zM12.5 0C5.872 0 .5 5.372.5 12s5.372 12 12 12 12-5.372 12-12-5.372-12-12-12m1.908 16.82-2.572-3.743-3.221 3.744h-.832l3.683-4.281-3.683-5.36h2.809l2.435 3.544 3.05-3.545h.833l-3.513 4.083 3.82 5.56z"/></symbol><symbol id="icon-eds-i-circle-youtube" viewBox="0 0 24 24"><g clip-path="url(#youtube-icon)"><path d="M10.568 9.79053V13.9189L12.0275 13.1403L14.4376 11.855L12.0275 10.5686L10.568 9.79053Z"/><path d="M12 0C5.37302 0 0 5.37247 0 12C0 18.6275 5.37302 24 12 24C18.627 24 24 18.6275 24 12C24 5.37247 18.6275 0 12 0ZM18.2984 14.5486C18.2539 14.9382 18.1418 15.286 17.9637 15.5855C17.7483 15.9471 17.4071 16.1702 16.9768 16.2323C16.6015 16.2856 12.0275 16.4301 12.0275 16.4301H12.0258C12.0258 16.4301 9.15523 16.363 8.22658 16.3317C7.84222 16.3197 7.45882 16.2865 7.07812 16.2323C6.8686 16.2052 6.66815 16.1302 6.49235 16.013L6.48961 16.0114C6.34964 15.9169 6.22913 15.7963 6.13463 15.6564L6.13298 15.6536C6.11833 15.6317 6.10386 15.6088 6.08957 15.5849C5.90594 15.267 5.79223 14.9135 5.75602 14.548C5.66366 13.7017 5.62054 12.8508 5.62689 11.9995C5.62054 11.1481 5.66366 10.2972 5.75602 9.45087C5.79943 9.06127 5.91208 8.71289 6.08957 8.41451C6.30497 8.05348 6.64731 7.82984 7.07757 7.76774C7.45123 7.71499 7.83863 7.68147 8.22603 7.66828C9.13106 7.63861 12.0269 7.57047 12.0269 7.57047C12.0269 7.57047 14.9228 7.63861 15.8273 7.66828C16.2152 7.68092 16.6015 7.71499 16.9763 7.76774C17.2554 7.80786 17.4972 7.91611 17.6906 8.08645C17.734 8.12382 17.9181 8.34032 17.9632 8.41506C18.1418 8.71399 18.2539 9.06182 18.2978 9.45142C18.3874 10.2614 18.4292 11.0944 18.4264 12.0011C18.4317 12.852 18.3893 13.7025 18.2984 14.5486Z"/></g></symbol><symbol id="icon-eds-i-copy-link" viewBox="0 0 24 24"><path fill-rule="evenodd" clip-rule="evenodd" d="M19.4594 8.57015C19.0689 8.17963 19.0689 7.54646 19.4594 7.15594L20.2927 6.32261C20.2927 6.32261 20.2927 6.32261 20.2927 6.32261C21.0528 5.56252 21.0528 4.33019 20.2928 3.57014C19.5327 2.81007 18.3004 2.81007 17.5404 3.57014L16.7071 4.40347C16.3165 4.794 15.6834 4.794 15.2928 4.40348C14.9023 4.01296 14.9023 3.3798 15.2928 2.98927L16.1262 2.15594C17.6673 0.614803 20.1659 0.614803 21.707 2.15593C23.2481 3.69705 23.248 6.19569 21.707 7.7368L20.8737 8.57014C20.4831 8.96067 19.85 8.96067 19.4594 8.57015Z"/><path fill-rule="evenodd" clip-rule="evenodd" d="M18.0944 5.90592C18.4849 6.29643 18.4849 6.9296 18.0944 7.32013L16.4278 8.9868C16.0373 9.37733 15.4041 9.37734 15.0136 8.98682C14.6231 8.59631 14.6231 7.96314 15.0136 7.57261L16.6802 5.90594C17.0707 5.51541 17.7039 5.5154 18.0944 5.90592Z"/><path fill-rule="evenodd" clip-rule="evenodd" d="M13.5113 6.32243C13.9018 6.71295 13.9018 7.34611 13.5113 7.73664L12.678 8.56997C12.678 8.56997 12.678 8.56997 12.678 8.56997C11.9179 9.33006 11.9179 10.5624 12.6779 11.3224C13.438 12.0825 14.6703 12.0825 15.4303 11.3224L16.2636 10.4891C16.6542 10.0986 17.2873 10.0986 17.6779 10.4891C18.0684 10.8796 18.0684 11.5128 17.6779 11.9033L16.8445 12.7366C15.3034 14.2778 12.8048 14.2778 11.2637 12.7366C9.72262 11.1955 9.72266 8.69689 11.2637 7.15578L12.097 6.32244C12.4876 5.93191 13.1207 5.93191 13.5113 6.32243Z"/><path d="M8 20V22H19.4619C20.136 22 20.7822 21.7311 21.2582 21.2529C21.7333 20.7757 22 20.1289 22 19.4549V15C22 14.4477 21.5523 14 21 14C20.4477 14 20 14.4477 20 15V19.4549C20 19.6004 19.9426 19.7397 19.8408 19.842C19.7399 19.9433 19.6037 20 19.4619 20H8Z"/><path d="M4 13H2V19.4619C2 20.136 2.26889 20.7822 2.74705 21.2582C3.22434 21.7333 3.87105 22 4.5451 22H9C9.55228 22 10 21.5523 10 21C10 20.4477 9.55228 20 9 20H4.5451C4.39957 20 4.26028 19.9426 4.15804 19.8408C4.05668 19.7399 4 19.6037 4 19.4619V13Z"/><path d="M4 13H2V4.53808C2 3.86398 2.26889 3.21777 2.74705 2.74178C3.22434 2.26666 3.87105 2 4.5451 2H9C9.55228 2 10 2.44772 10 3C10 3.55228 9.55228 4 9 4H4.5451C4.39957 4 4.26028 4.05743 4.15804 4.15921C4.05668 4.26011 4 4.39633 4 4.53808V13Z"/></symbol><symbol id="icon-eds-i-funding-dollar" viewBox="0 0 32 32"><path d="M17.333 7.79549V9.21808C18.3681 9.32469 19.2889 9.82002 19.9444 10.5523C20.2938 10.9427 20.5697 11.4022 20.7488 11.9089C20.9942 12.6031 20.6303 13.3649 19.936 13.6103C19.2418 13.8558 18.48 13.4919 18.2346 12.7976C18.1735 12.6249 18.0788 12.4665 17.9574 12.3308C17.6988 12.0419 17.3272 11.8632 16.9122 11.8632H16.042C16.028 11.8636 16.0139 11.8639 15.9997 11.8639C15.9907 11.8639 15.9817 11.8638 15.9727 11.8636C15.9676 11.8635 15.9624 11.8634 15.9573 11.8632H14.7952C14.1833 11.8632 13.6872 12.3593 13.6872 12.9713C13.6872 13.492 14.0498 13.9424 14.5584 14.0537L17.7816 14.7588C19.6498 15.1675 20.9806 16.8226 20.9806 18.734C20.9806 20.8383 19.3827 22.5712 17.333 22.7819V24.2051C17.333 24.9415 16.7361 25.5384 15.9997 25.5384C15.2633 25.5384 14.6663 24.9415 14.6663 24.2051V22.7817C13.0793 22.618 11.7653 21.5424 11.2524 20.091C11.007 19.3967 11.3709 18.635 12.0651 18.3896C12.7594 18.1442 13.5212 18.5081 13.7666 19.2024C13.9597 19.7486 14.4807 20.1367 15.0889 20.1367H15.9849C15.9898 20.1367 15.9947 20.1366 15.9997 20.1366C16.0046 20.1366 16.0095 20.1367 16.0144 20.1367H16.9122C17.6857 20.1367 18.3139 19.5088 18.3139 18.734C18.3139 18.0748 17.8548 17.5045 17.2118 17.3639L13.9886 16.6588C12.2557 16.2797 11.0205 14.7451 11.0205 12.9713C11.0205 10.9297 12.6413 9.26664 14.6663 9.19869V7.79549C14.6663 7.05911 15.2633 6.46216 15.9997 6.46216C16.7361 6.46216 17.333 7.05911 17.333 7.79549Z"/><path fill-rule="evenodd" clip-rule="evenodd" d="M15.9997 1.33325C7.8995 1.33325 1.33301 7.89974 1.33301 15.9999C1.33301 24.1002 7.89951 30.6666 15.9997 30.6666C24.1 30.6666 30.6663 24.1002 30.6663 15.9999C30.6663 7.89975 24.1 1.33325 15.9997 1.33325ZM3.99967 15.9999C3.99967 9.3725 9.37226 3.99992 15.9997 3.99992C22.6272 3.99992 27.9997 9.3725 27.9997 15.9999C27.9997 22.6274 22.6272 27.9999 15.9997 27.9999C9.37225 27.9999 3.99967 22.6274 3.99967 15.9999Z"/></symbol><symbol id="icon-eds-i-github-medium" viewBox="0 0 24 24"><path d="M 11.964844 0 C 5.347656 0 0 5.269531 0 11.792969 C 0 17.003906 3.425781 21.417969 8.179688 22.976562 C 8.773438 23.09375 8.992188 22.722656 8.992188 22.410156 C 8.992188 22.136719 8.972656 21.203125 8.972656 20.226562 C 5.644531 20.929688 4.953125 18.820312 4.953125 18.820312 C 4.417969 17.453125 3.625 17.101562 3.625 17.101562 C 2.535156 16.378906 3.703125 16.378906 3.703125 16.378906 C 4.914062 16.457031 5.546875 17.589844 5.546875 17.589844 C 6.617188 19.386719 8.339844 18.878906 9.03125 18.566406 C 9.132812 17.804688 9.449219 17.277344 9.785156 16.984375 C 7.132812 16.710938 4.339844 15.695312 4.339844 11.167969 C 4.339844 9.878906 4.8125 8.824219 5.566406 8.003906 C 5.445312 7.710938 5.03125 6.5 5.683594 4.878906 C 5.683594 4.878906 6.695312 4.566406 8.972656 6.089844 C 9.949219 5.832031 10.953125 5.703125 11.964844 5.699219 C 12.972656 5.699219 14.003906 5.835938 14.957031 6.089844 C 17.234375 4.566406 18.242188 4.878906 18.242188 4.878906 C 18.898438 6.5 18.480469 7.710938 18.363281 8.003906 C 19.136719 8.824219 19.589844 9.878906 19.589844 11.167969 C 19.589844 15.695312 16.796875 16.691406 14.125 16.984375 C 14.558594 17.355469 14.933594 18.058594 14.933594 19.171875 C 14.933594 20.753906 14.914062 22.019531 14.914062 22.410156 C 14.914062 22.722656 15.132812 23.09375 15.726562 22.976562 C 20.480469 21.414062 23.910156 17.003906 23.910156 11.792969 C 23.929688 5.269531 18.558594 0 11.964844 0 Z M 11.964844 0 "/></symbol><symbol id="icon-eds-i-institution-medium" viewBox="0 0 24 24"><g><path fill-rule="evenodd" clip-rule="evenodd" d="M11.9967 1C11.6364 1 11.279 1.0898 10.961 1.2646C10.9318 1.28061 10.9035 1.29806 10.8761 1.31689L2.79765 6.87C2.46776 7.08001 2.20618 7.38466 2.07836 7.76668C1.94823 8.15561 1.98027 8.55648 2.12665 8.90067C2.42086 9.59246 3.12798 10 3.90107 10H4.99994V16H4.49994C3.11923 16 1.99994 17.1193 1.99994 18.5V19.5C1.99994 20.8807 3.11923 22 4.49994 22H19.4999C20.8807 22 21.9999 20.8807 21.9999 19.5V18.5C21.9999 17.1193 20.8807 16 19.4999 16H18.9999V10H20.0922C20.8653 10 21.5725 9.59252 21.8667 8.90065C22.0131 8.55642 22.0451 8.15553 21.9149 7.7666C21.7871 7.38459 21.5255 7.07997 21.1956 6.86998L13.1172 1.31689C13.0898 1.29806 13.0615 1.28061 13.0324 1.2646C12.7143 1.0898 12.357 1 11.9967 1ZM4.6844 8L11.9472 3.00755C11.9616 3.00295 11.9783 3 11.9967 3C12.015 3 12.0318 3.00295 12.0461 3.00755L19.3089 8H4.6844ZM16.9999 16V10H14.9999V16H16.9999ZM12.9999 16V10H10.9999V16H12.9999ZM8.99994 16V10H6.99994V16H8.99994ZM3.99994 18.5C3.99994 18.2239 4.2238 18 4.49994 18H19.4999C19.7761 18 19.9999 18.2239 19.9999 18.5V19.5C19.9999 19.7761 19.7761 20 19.4999 20H4.49994C4.2238 20 3.99994 19.7761 3.99994 19.5V18.5Z"/></g></symbol><symbol id="icon-eds-i-limited-access" viewBox="0 0 16 16"><path fill-rule="evenodd" d="M4 3v3a1 1 0 0 1-2 0V2.923C2 1.875 2.84 1 3.909 1h5.909a1 1 0 0 1 .713.298l3.181 3.231a1 1 0 0 1 .288.702V6a1 1 0 1 1-2 0v-.36L9.4 3H4ZM3 8a1 1 0 0 1 1 1v1a1 1 0 1 1-2 0V9a1 1 0 0 1 1-1Zm10 0a1 1 0 0 1 1 1v1a1 1 0 1 1-2 0V9a1 1 0 0 1 1-1Zm-3.5 6a1 1 0 0 1-1 1h-1a1 1 0 1 1 0-2h1a1 1 0 0 1 1 1Zm2.441-1a1 1 0 0 1 2 0c0 .73-.246 1.306-.706 1.664a1.61 1.61 0 0 1-.876.334l-.032.002H11.5a1 1 0 1 1 0-2h.441ZM4 13a1 1 0 0 0-2 0c0 .73.247 1.306.706 1.664a1.609 1.609 0 0 0 .876.334l.032.002H4.5a1 1 0 1 0 0-2H4Z" clip-rule="evenodd"/></symbol><symbol id="icon-eds-i-marker-filled"><path d="M19.7998 21.1049C19.7998 21.7661 19.4529 22.3363 18.9053 22.6263C18.3697 22.9099 17.7222 22.8847 17.1855 22.5629C17.1651 22.5506 17.1445 22.5375 17.125 22.5238L11.999 18.9261L6.87402 22.5238C6.85458 22.5374 6.83481 22.5506 6.81445 22.5629C6.25918 22.896 5.60347 22.8929 5.08398 22.6469C4.54665 22.3923 4.09961 21.8431 4.09961 21.1049V3.57751C4.09961 2.22526 5.14769 1.17712 6.5 1.17712H17.3994C18.7517 1.17712 19.7998 2.22526 19.7998 3.57751V21.1049Z"/></symbol><symbol id="icon-eds-i-marker-unfilled"><path d="M17.7998 3.57751C17.7998 3.32977 17.6471 3.17712 17.3994 3.17712H6.5C6.25231 3.17712 6.09961 3.32977 6.09961 3.57751V20.6244L11.4248 16.8871L11.5596 16.807C11.8831 16.6484 12.2726 16.6755 12.5742 16.8871L17.7998 20.5531V3.57751ZM19.7998 21.1049C19.7998 21.7661 19.4529 22.3363 18.9053 22.6263C18.3697 22.9099 17.7222 22.8847 17.1855 22.5629C17.1651 22.5506 17.1445 22.5375 17.125 22.5238L11.999 18.9261L6.87402 22.5238C6.85458 22.5374 6.83481 22.5506 6.81445 22.5629C6.25918 22.896 5.60347 22.8929 5.08398 22.6469C4.54665 22.3923 4.09961 21.8431 4.09961 21.1049V3.57751C4.09961 2.22526 5.14769 1.17712 6.5 1.17712H17.3994C18.7517 1.17712 19.7998 2.22526 19.7998 3.57751V21.1049Z"/></symbol><symbol id="icon-eds-i-quotation-mark-large" viewBox="0 0 48 48"><path d="M25.749 28.9958C25.749 19.5707 32.5601 9.711 43.2492 7.5L44.9712 11.3977C41.0949 12.9285 37.6859 18.258 37.3061 21.932C41.6654 22.6507 44.9992 26.6015 44.9992 31.3718C44.9992 37.2055 40.4772 40.5 35.9009 40.5C30.6246 40.5 25.749 36.2742 25.749 28.9958Z"/><path d="M3 28.9958C3 19.5707 9.81107 9.711 20.5002 7.5L22.2222 11.3977C18.3459 12.9285 14.9369 18.258 14.5571 21.932C18.9164 22.6507 22.2502 26.6015 22.2502 31.3718C22.2502 37.2055 17.7282 40.5 13.1519 40.5C7.87555 40.5 3 36.2742 3 28.9958Z"/></symbol><symbol id="icon-eds-i-rss" viewBox="0 0 22 22"><path d="M1.96094 1C1.96094 0.447715 2.40865 0 2.96094 0C5.46109 0 7.93678 0.492038 10.2467 1.44806C12.5565 2.40407 14.6554 3.80534 16.4234 5.57189C18.1913 7.33843 19.5939 9.4357 20.5508 11.744C21.5077 14.0522 22.0001 16.5263 22.0001 19.0247C22.0001 19.577 21.5524 20.0247 21.0001 20.0247C20.4478 20.0247 20.0001 19.577 20.0001 19.0247C20.0001 16.7891 19.5595 14.5753 18.7033 12.5098C17.8471 10.4444 16.5919 8.56762 15.0097 6.98666C13.4275 5.40575 11.5492 4.15167 9.48182 3.29604C7.41447 2.4404 5.19868 2 2.96094 2C2.40865 2 1.96094 1.55228 1.96094 1Z"/><path fill-rule="evenodd" clip-rule="evenodd" d="M0 18.649C0 16.7974 1.50196 15.298 3.35294 15.298C5.20392 15.298 6.70588 16.7974 6.70588 18.649C6.70588 20.5003 5.20397 22 3.35294 22C1.50191 22 0 20.5003 0 18.649ZM3.35294 17.298C2.60493 17.298 2 17.9036 2 18.649C2 19.3943 2.60498 20 3.35294 20C4.1009 20 4.70588 19.3943 4.70588 18.649C4.70588 17.9036 4.10095 17.298 3.35294 17.298Z"/><path d="M3.3374 7.46115C2.78512 7.46115 2.3374 7.90887 2.3374 8.46115C2.3374 9.01344 2.78512 9.46115 3.3374 9.46115C4.54515 9.46115 5.74107 9.69885 6.85684 10.1606C7.97262 10.6224 8.98639 11.2993 9.84028 12.1525C10.6942 13.0057 11.3715 14.0185 11.8336 15.1332C12.2956 16.2478 12.5335 17.4424 12.5335 18.649C12.5335 19.2013 12.9812 19.649 13.5335 19.649C14.0858 19.649 14.5335 19.2013 14.5335 18.649C14.5335 17.1796 14.2438 15.7247 13.6811 14.3673C13.1184 13.0099 12.2936 11.7765 11.2539 10.7377C10.2142 9.69885 8.97999 8.87484 7.62168 8.31266C6.26337 7.75049 4.80757 7.46115 3.3374 7.46115Z"/></symbol><symbol id="icon-eds-i-search-category-medium" viewBox="0 0 32 32"><path fill-rule="evenodd" d="M2 5.306A3.306 3.306 0 0 1 5.306 2h5.833a3.306 3.306 0 0 1 3.306 3.306v5.833a3.306 3.306 0 0 1-3.306 3.305H5.306A3.306 3.306 0 0 1 2 11.14V5.306Zm3.306-.584a.583.583 0 0 0-.584.584v5.833c0 .322.261.583.584.583h5.833a.583.583 0 0 0 .583-.583V5.306a.583.583 0 0 0-.583-.584H5.306Zm15.555 8.945a7.194 7.194 0 1 0 4.034 13.153l2.781 2.781a1.361 1.361 0 1 0 1.925-1.925l-2.781-2.781a7.194 7.194 0 0 0-5.958-11.228Zm3.173 10.346a4.472 4.472 0 1 0-.021.021l.01-.01.011-.011Zm-5.117-19.29a.583.583 0 0 0-.584.583v5.833a1.361 1.361 0 0 1-2.722 0V5.306A3.306 3.306 0 0 1 18.917 2h5.833a3.306 3.306 0 0 1 3.306 3.306v5.833c0 .6-.161 1.166-.443 1.654a1.361 1.361 0 1 1-2.357-1.363.575.575 0 0 0 .078-.291V5.306a.583.583 0 0 0-.584-.584h-5.833ZM2 18.916a3.306 3.306 0 0 1 3.306-3.306h5.833a1.361 1.361 0 1 1 0 2.722H5.306a.583.583 0 0 0-.584.584v5.833c0 .322.261.583.584.583h5.833a.574.574 0 0 0 .29-.077 1.361 1.361 0 1 1 1.364 2.356 3.296 3.296 0 0 1-1.654.444H5.306A3.306 3.306 0 0 1 2 24.75v-5.833Z" clip-rule="evenodd"/></symbol><symbol id="icon-eds-i-search-magic" viewBox="0 0 20 20"><path d="M8.695 1.667a9.1 9.1 0 0 1 1.756.17A3.098 3.098 0 0 0 9.436 3.37a7.333 7.333 0 0 0-.738-.038c-3.841 0-6.956 2.986-6.956 6.668 0 3.681 3.115 6.665 6.956 6.665 3.642 0 6.627-2.681 6.928-6.096a3.19 3.19 0 0 0 1.763-.548 8.091 8.091 0 0 1-1.961 5.25l3.446 3.306a.81.81 0 0 1 0 1.178.897.897 0 0 1-1.23 0l-3.447-3.303a8.892 8.892 0 0 1-5.502 1.88C3.893 18.334 0 14.603 0 10c0-4.603 3.893-8.333 8.695-8.334Z"/><path d="M20 4.166a.663.663 0 0 1-.128.395.709.709 0 0 1-.342.251l-2.341.827-.863 2.244a.693.693 0 0 1-.263.326.74.74 0 0 1-.821 0 .693.693 0 0 1-.264-.326l-.862-2.244-2.341-.827a.715.715 0 0 1-.341-.252.669.669 0 0 1 0-.787.715.715 0 0 1 .34-.252l2.342-.827.862-2.244a.693.693 0 0 1 .264-.327.74.74 0 0 1 .821 0 .7.7 0 0 1 .263.327l.863 2.244 2.34.827a.709.709 0 0 1 .343.251.663.663 0 0 1 .128.394Z"/></symbol><symbol id="icon-eds-i-subjects-medium" viewBox="0 0 24 24"><g id="icon-subjects-copy" stroke="none" stroke-width="1" fill-rule="evenodd"><path d="M13.3846154,2 C14.7015971,2 15.7692308,3.06762994 15.7692308,4.38461538 L15.7692308,7.15384615 C15.7692308,8.47082629 14.7015955,9.53846154 13.3846154,9.53846154 L13.1038388,9.53925278 C13.2061091,9.85347965 13.3815528,10.1423885 13.6195822,10.3804178 C13.9722182,10.7330539 14.436524,10.9483278 14.9293854,10.9918129 L15.1153846,11 C16.2068332,11 17.2535347,11.433562 18.0254647,12.2054189 C18.6411944,12.8212361 19.0416785,13.6120766 19.1784166,14.4609738 L19.6153846,14.4615385 C20.932386,14.4615385 22,15.5291672 22,16.8461538 L22,19.6153846 C22,20.9323924 20.9323924,22 19.6153846,22 L16.8461538,22 C15.5291672,22 14.4615385,20.932386 14.4615385,19.6153846 L14.4615385,16.8461538 C14.4615385,15.5291737 15.5291737,14.4615385 16.8461538,14.4615385 L17.126925,14.460779 C17.0246537,14.1465537 16.8492179,13.857633 16.6112344,13.6196157 C16.2144418,13.2228606 15.6764136,13 15.1153846,13 C14.0239122,13 12.9771569,12.5664197 12.2053686,11.7946314 C12.1335167,11.7227795 12.0645962,11.6485444 11.9986839,11.5721119 C11.9354038,11.6485444 11.8664833,11.7227795 11.7946314,11.7946314 C11.0228431,12.5664197 9.97608778,13 8.88461538,13 C8.323576,13 7.78552852,13.2228666 7.38881294,13.6195822 C7.15078359,13.8576115 6.97533988,14.1465203 6.8730696,14.4607472 L7.15384615,14.4615385 C8.47082629,14.4615385 9.53846154,15.5291737 9.53846154,16.8461538 L9.53846154,19.6153846 C9.53846154,20.932386 8.47083276,22 7.15384615,22 L4.38461538,22 C3.06762347,22 2,20.9323876 2,19.6153846 L2,16.8461538 C2,15.5291721 3.06762994,14.4615385 4.38461538,14.4615385 L4.8215823,14.4609378 C4.95831893,13.6120029 5.3588057,12.8211623 5.97459937,12.2053686 C6.69125996,11.488708 7.64500941,11.0636656 8.6514968,11.0066017 L8.88461538,11 C9.44565477,11 9.98370225,10.7771334 10.3804178,10.3804178 C10.6184472,10.1423885 10.7938909,9.85347965 10.8961612,9.53925278 L10.6153846,9.53846154 C9.29840448,9.53846154 8.23076923,8.47082629 8.23076923,7.15384615 L8.23076923,4.38461538 C8.23076923,3.06762994 9.29840286,2 10.6153846,2 L13.3846154,2 Z M7.15384615,16.4615385 L4.38461538,16.4615385 C4.17220099,16.4615385 4,16.63374 4,16.8461538 L4,19.6153846 C4,19.8278134 4.17218833,20 4.38461538,20 L7.15384615,20 C7.36626945,20 7.53846154,19.8278103 7.53846154,19.6153846 L7.53846154,16.8461538 C7.53846154,16.6337432 7.36625679,16.4615385 7.15384615,16.4615385 Z M19.6153846,16.4615385 L16.8461538,16.4615385 C16.6337432,16.4615385 16.4615385,16.6337432 16.4615385,16.8461538 L16.4615385,19.6153846 C16.4615385,19.8278103 16.6337306,20 16.8461538,20 L19.6153846,20 C19.8278229,20 20,19.8278229 20,19.6153846 L20,16.8461538 C20,16.6337306 19.8278103,16.4615385 19.6153846,16.4615385 Z M13.3846154,4 L10.6153846,4 C10.4029708,4 10.2307692,4.17220099 10.2307692,4.38461538 L10.2307692,7.15384615 C10.2307692,7.36625679 10.402974,7.53846154 10.6153846,7.53846154 L13.3846154,7.53846154 C13.597026,7.53846154 13.7692308,7.36625679 13.7692308,7.15384615 L13.7692308,4.38461538 C13.7692308,4.17220099 13.5970292,4 13.3846154,4 Z" id="Shape" fill-rule="nonzero"/></g></symbol><symbol id="icon-eds-small-arrow-left" viewBox="0 0 16 17"><path stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M14 8.092H2m0 0L8 2M2 8.092l6 6.035"/></symbol><symbol id="icon-eds-small-arrow-right" viewBox="0 0 16 16"><g fill-rule="evenodd" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2"><path d="M2 8.092h12M8 2l6 6.092M8 14.127l6-6.035"/></g></symbol><symbol id="icon-funding-dollar-green-dot" viewBox="0 0 33 33"><path d="M16.25 0C25.2246 0 32.5 7.27537 32.5 16.25C32.5 25.2246 25.2246 32.5 16.25 32.5C15.5596 32.5 15 31.9404 15 31.25C15 30.5596 15.5596 30 16.25 30C23.8439 30 30 23.8439 30 16.25C30 8.65608 23.8439 2.5 16.25 2.5C8.65608 2.5 2.5 8.65608 2.5 16.25C2.5 16.9404 1.94036 17.5 1.25 17.5C0.559644 17.5 0 16.9404 0 16.25C0 7.27537 7.27537 0 16.25 0ZM16.3428 6.25C17.0956 6.25024 17.7059 6.86048 17.7061 7.61328V9.06836C18.7647 9.1774 19.7066 9.68466 20.377 10.4336C20.7341 10.8327 21.0161 11.3024 21.1992 11.8203C21.4502 12.5304 21.0782 13.3095 20.3682 13.5605C19.6581 13.8115 18.8789 13.4395 18.6279 12.7295C18.5655 12.5529 18.4689 12.3907 18.3447 12.252C18.0802 11.9565 17.6998 11.7734 17.2754 11.7734H16.3857C16.3715 11.7739 16.3571 11.7744 16.3428 11.7744C16.3336 11.7744 16.3236 11.7746 16.3145 11.7744C16.3093 11.7743 16.304 11.7736 16.2988 11.7734H15.1104C14.4846 11.7735 13.9775 12.2814 13.9775 12.9072C13.9776 13.4396 14.3481 13.8998 14.8682 14.0137L18.165 14.7354C20.0755 15.1534 21.4365 16.846 21.4365 18.8008C21.4365 20.9528 19.8023 22.7249 17.7061 22.9404V24.3965C17.7059 25.1493 17.0956 25.7595 16.3428 25.7598C15.5898 25.7598 14.9787 25.1495 14.9785 24.3965V22.9404C13.3555 22.7729 12.0119 21.6727 11.4873 20.1885C11.2364 19.4784 11.6083 18.6992 12.3184 18.4482C13.0283 18.1974 13.8075 18.5695 14.0586 19.2793C14.256 19.8379 14.7891 20.2354 15.4111 20.2354H17.2754C18.0665 20.2354 18.709 19.5932 18.709 18.8008C18.709 18.1266 18.2397 17.5433 17.582 17.3994L14.2852 16.6787C12.5131 16.2909 11.2501 14.7212 11.25 12.9072C11.25 10.8193 12.9075 9.11841 14.9785 9.04883V7.61328C14.9787 6.86033 15.5898 6.25 16.3428 6.25Z"/><path style="fill:#00A69D" d="M6.25 20.25C2.93629 20.25 0.25 22.9363 0.25 26.25C0.25 29.5637 2.93629 32.25 6.25 32.25C9.56371 32.25 12.25 29.5637 12.25 26.25C12.25 22.9363 9.56371 20.25 6.25 20.25Z"/></symbol><symbol id="icon-globe-with-star" viewBox="0 0 32 32"><path style="fill:none;stroke:#fff" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M27.282 17.026c0 6.797-5.51 12.307-12.307 12.307-6.798 0-12.308-5.51-12.308-12.307 0-6.798 5.51-12.308 12.308-12.308M2.667 17.026h14.201"/><path style="fill:none;stroke:#fff" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M14.975 4.718a21.244 21.244 0 0 0-4.734 12.308c.233 4.5 1.89 8.81 4.734 12.307a21.245 21.245 0 0 0 4.394-9.467M17.045 9.72c-.709-.126-.709-1.16 0-1.286 2.568-.454 4.61-2.443 5.168-5.032l.043-.199c.153-.712 1.15-.716 1.31-.006l.052.232c.578 2.577 2.621 4.549 5.182 5.002.712.126.712 1.166 0 1.292-2.561.453-4.604 2.425-5.182 5.002l-.052.231c-.16.711-1.157.707-1.31-.005l-.043-.199c-.557-2.59-2.6-4.578-5.168-5.032Z"/></symbol><symbol id="icon-journal-medium-green-dot" viewBox="0 0 32 32"><path style="stroke:#222222;stroke-width:2.5;stroke-linecap:round;fill:transparent" d="M11 2L8.51877 2.00121C7.56562 2.00168 7.08905 2.00191 6.73603 2.21084C6.51866 2.33949 6.3373 2.52093 6.20876 2.73836C6 3.09149 6 3.56806 6 4.52121L6 13.2518V15.5M11 2H24.6133C25.5669 2 26.0437 2 26.3969 2.20889C26.6143 2.3375 26.7958 2.51896 26.9244 2.73644C27.1333 3.08965 27.1333 3.56547 27.1333 4.51712C27.1333 10.3583 27.1333 19.2745 27.1333 20.9995C27.1333 22.0923 27.0835 22.9368 26.984 23.5324C26.9756 23.5832 26.9713 23.6085 26.7521 23.9008C26.6766 24.0015 26.3534 24.2752 26.2416 24.3331C25.9172 24.501 25.8079 24.501 25.5893 24.501L18 24.5M11 2V12V16M27.5 28.5011H17.5M18.6271 6.50113H19.6071C20.5607 6.50113 21.0375 6.50113 21.3907 6.71001C21.6082 6.83863 21.7896 7.02009 21.9183 7.23757C22.1271 7.59077 22.1271 8.06756 22.1271 9.02113V10.4811C22.1271 11.4347 22.1271 11.9115 21.9183 12.2647C21.7896 12.4822 21.6082 12.6636 21.3907 12.7922C21.0375 13.0011 20.5607 13.0011 19.6071 13.0011H17.6471C16.6936 13.0011 16.2168 13.0011 15.8636 12.7922C15.6461 12.6636 15.4646 12.4822 15.336 12.2647C15.1271 11.9115 15.1271 11.4347 15.1271 10.4811V9.02113C15.1271 8.06756 15.1271 7.59077 15.336 7.23757C15.4646 7.02009 15.6461 6.83863 15.8636 6.71001C16.2168 6.50113 16.6936 6.50113 17.6471 6.50113H18.6271Z"/><path style="fill:#00A69D" d="M8 19C4.68629 19 2 21.6863 2 25C2 28.3137 4.68629 31 8 31C11.3137 31 14 28.3137 14 25C14 21.6863 11.3137 19 8 19Z"/></symbol><symbol id="icon-orcid-logo" viewBox="0 0 40 40"><path fill-rule="evenodd" d="M12.281 10.453c.875 0 1.578-.719 1.578-1.578 0-.86-.703-1.578-1.578-1.578-.875 0-1.578.703-1.578 1.578 0 .86.703 1.578 1.578 1.578Zm-1.203 18.641h2.406V12.359h-2.406v16.735Z"/><path fill-rule="evenodd" d="M17.016 12.36h6.5c6.187 0 8.906 4.421 8.906 8.374 0 4.297-3.36 8.375-8.875 8.375h-6.531V12.36Zm6.234 14.578h-3.828V14.53h3.703c4.688 0 6.828 2.844 6.828 6.203 0 2.063-1.25 6.203-6.703 6.203Z" clip-rule="evenodd"/></symbol><symbol id="icon-thumbs-down" viewBox="41 0 33 33"><path stroke="currentcolor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M61 17.611h-1l-2.085 4.436a1 1 0 0 1-.366.332 1.04 1.04 0 0 1-1.197-.159.95.95 0 0 1-.298-.678v-3.32h-3.93c-.61 0-.674-1.066-.599-1.55l.726-4.301a.98.98 0 0 1 .341-.622 1.06 1.06 0 0 1 .685-.249H61M63.5 11.5v6"/></symbol><symbol id="icon-thumbs-up-medium" viewBox="0 0 24 24"><path d="M6.75 9.33333H8.25L11.3778 2.67913C11.5138 2.47128 11.7025 2.29994 11.9263 2.18116C12.1502 2.06239 12.4018 2.00005 12.6576 2C13.056 1.99999 13.4384 2.15092 13.7214 2.41998C14.0044 2.68903 14.1652 3.05442 14.1688 3.43663V8.41667C16.1342 8.41667 18.1116 8.41667 20.0648 8.41667C20.9795 8.41667 21.0744 10.0164 20.9625 10.7422L19.8733 17.194C19.8269 17.5542 19.6449 17.8856 19.3616 18.1261C19.0783 18.3667 18.7132 18.4996 18.3349 18.5H6.75" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"/><path d="M3 18.5V9.5" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"/></symbol><symbol id="icon-thumbs-up"><path stroke="currentcolor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M13 15.389h1l2.085-4.436a1 1 0 0 1 .366-.332 1.04 1.04 0 0 1 1.197.159.95.95 0 0 1 .298.678v3.32h3.93c.61 0 .674 1.066.599 1.55l-.726 4.301a.98.98 0 0 1-.341.622 1.06 1.06 0 0 1-.685.249H13m-2.5 0v-6"/></symbol><symbol id="icon-tick-with-curly-circle" viewBox="0 0 40 40"><path d="M17.923.757a3.227 3.227 0 0 1 4.401.231l2.168 2.25 3.001-.866.33-.076a3.23 3.23 0 0 1 3.598 2.076l.099.325.75 3.03 3.033.753a3.229 3.229 0 0 1 2.4 3.697l-.075.33-.865 3.001 2.249 2.168.231.247a3.227 3.227 0 0 1-.231 4.401l-2.25 2.166.866 3.003.076.33a3.228 3.228 0 0 1-2.401 3.697l-3.033.75-.75 3.033a3.229 3.229 0 0 1-4.027 2.325l-3.001-.867-2.168 2.25a3.227 3.227 0 0 1-4.648 0l-2.17-2.25-2.999.867a3.229 3.229 0 0 1-4.027-2.325l-.752-3.033-3.03-.75a3.229 3.229 0 0 1-2.326-4.027l.865-3.001-2.249-2.168a3.227 3.227 0 0 1 0-4.648l2.25-2.17-.866-2.999A3.229 3.229 0 0 1 4.697 8.48l3.031-.752.752-3.03a3.229 3.229 0 0 1 4.027-2.326l3 .865L17.675.988l.247-.231Zm.027 5.156a3.23 3.23 0 0 1-3.219.864l-2.838-.82-.712 2.87a3.227 3.227 0 0 1-2.355 2.354l-2.868.712.819 2.838a3.23 3.23 0 0 1-.864 3.219L3.786 20l2.127 2.05a3.23 3.23 0 0 1 .864 3.219l-.82 2.837 2.87.713.215.062a3.228 3.228 0 0 1 2.14 2.293l.71 2.867 2.84-.818a3.229 3.229 0 0 1 3.218.864L20 36.214l2.05-2.127.16-.156a3.228 3.228 0 0 1 2.841-.76l.218.052 2.837.818.713-2.867.062-.214a3.227 3.227 0 0 1 2.293-2.141l2.867-.713-.816-2.837c-.331-1.15 0-2.389.861-3.219L36.212 20l-2.126-2.05a3.229 3.229 0 0 1-.861-3.219l.816-2.838-2.867-.712a3.228 3.228 0 0 1-2.355-2.355l-.713-2.868-2.837.819a3.23 3.23 0 0 1-3.219-.864L20 3.786l-2.05 2.127Z" stroke="transparent" stroke-width=".9"/><path d="M25.723 13.406a1.807 1.807 0 0 1 2.699 2.397l-9.658 12.074a1.808 1.808 0 0 1-2.497.316L11.44 24.57a1.806 1.806 0 1 1 2.168-2.892l3.427 2.57 8.565-10.704.124-.138Z" stroke="transparent" stroke-width=".9"/></symbol></svg>
</div>


        

        
        
    <a class="c-skip-link" href="#main">Skip to main content</a>

    
        
    <aside class="u-lazy-ad-wrapper u-mbs-0" aria-label="Advertisement">
        <div class="c-ad c-ad--728x90 c-ad--conditional" data-test="springer-doubleclick-ad">
            <div class="c-ad c-ad__inner" >
                <p class="c-ad__label">Advertisement</p>
                <div id="div-gpt-ad-LB1"
                     class="div-gpt-ad grade-c-hide"
                     data-gpt
                     data-gpt-unitpath="/270604982/springerlink/10489/article"
                     data-gpt-sizes="728x90"
                     data-gpt-targeting="pos=top;articleid=s10489-025-06516-z;"
                     data-ad-type="top"
                     style="min-width:728px;min-height:90px">
                    
                    <script>
                        window.SN = window.SN || {};
                        window.SN.libs = window.SN.libs || {};
                        window.SN.libs.ads = window.SN.libs.ads || {};
                        window.SN.libs.ads.slotConfig = window.SN.libs.ads.slotConfig || {};
                        window.SN.libs.ads.slotConfig['LB1'] = {
                            'pos': 'top',
                            'type': 'top',
                        };
                        window.SN.libs.ads.slotConfig['unitPath'] = '/270604982/springerlink/10489/article';
                    </script>
                    <noscript>
                        <a href="//pubads.g.doubleclick.net/gampad/jump?iu=/270604982/springerlink/10489/article&amp;sz=728x90&amp;pos=top&amp;articleid=s10489-025-06516-z">
                            <img data-test="gpt-advert-fallback-img"
                                 src="//pubads.g.doubleclick.net/gampad/ad?iu=/270604982/springerlink/10489/article&amp;sz=728x90&amp;pos=top&amp;articleid=s10489-025-06516-z"
                                 alt="Advertisement"
                                 width="728"
                                 height="90">
                        </a>
                    </noscript>
                </div>
            </div>
        </div>
    </aside>

    

    <header class="eds-c-header" data-eds-c-header>
    <div class="eds-c-header__container" data-eds-c-header-expander-anchor>
        <div class="eds-c-header__brand">
            
                
                    <a href="https://link.springer.com"
                    	 data-test=springerlink-logo
                        
                            data-track="click_imprint_logo"
                        
                            data-track-context="unified header"
                        
                            data-track-action="click logo link"
                        
                            data-track-category="unified header"
                        
                            data-track-label="link"
                        
					>
                        <img src="/oscar-static/images/darwin/header/img/logo-springer-nature-link-3149409f62.svg" alt="Springer Nature Link">
                    </a>
                
            
        </div>

        
            
                
    
        <a class="c-header__link eds-c-header__link" id="identity-account-widget" data-track="click_login" data-track-context="header" href='https://idp.springer.com/auth/personal/springernature?redirect_uri=https://link.springer.com/article/10.1007/s10489-025-06516-z?'><span class="eds-c-header__widget-fragment-title">Log in</span></a>
    


            
        
    </div>

    
        <nav class="eds-c-header__nav" aria-label="header navigation">
            <div class="eds-c-header__nav-container">
                <div class="eds-c-header__item eds-c-header__item--menu">
                   <a href="#eds-c-header-nav" class="eds-c-header__link" data-eds-c-header-expander>
                        <svg class="eds-c-header__icon" width="24" height="24" aria-hidden="true" focusable="false">
                            <use xlink:href="#icon-eds-i-menu-medium"></use>
                        </svg><span>Menu</span>
                    </a>
                </div>

                <div class="eds-c-header__item eds-c-header__item--inline-links">
                    
                        <a class="eds-c-header__link" href="https://link.springer.com/journals/"
                            
                                data-track="nav_find_a_journal"
                            
                                data-track-context="unified header"
                            
                                data-track-action="click find a journal"
                            
                                data-track-category="unified header"
                            
                                data-track-label="link"
                            
						>
                            Find a journal
                        </a>
                    
                        <a class="eds-c-header__link" href="https://www.springernature.com/gp/authors"
                            
                                data-track="nav_how_to_publish"
                            
                                data-track-context="unified header"
                            
                                data-track-action="click publish with us link"
                            
                                data-track-category="unified header"
                            
                                data-track-label="link"
                            
						>
                            Publish with us
                        </a>
                    
                        <a class="eds-c-header__link" href="https://link.springernature.com/home/"
                            
                                data-track="nav_track_your_research"
                            
                                data-track-context="unified header"
                            
                                data-track-action="click track your research"
                            
                                data-track-category="unified header"
                            
                                data-track-label="link"
                            
						>
                            Track your research
                        </a>
                    
                </div>

                <div class="eds-c-header__link-container">
                    
                        <div class="eds-c-header__item eds-c-header__item--divider">
                            <a href="#eds-c-header-popup-search" class="eds-c-header__link" data-eds-c-header-expander data-eds-c-header-test-search-btn>
                                <svg class="eds-c-header__icon" width="24" height="24" aria-hidden="true" focusable="false">
                                    <use xlink:href="#icon-eds-i-search-medium"></use>
                                </svg><span>Search</span>
                            </a>
                        </div>
                    
                    
                        
                            <div id="ecommerce-header-cart-icon-link" class="eds-c-header__item ecommerce-cart" style="display:inline-block">
 <a class="eds-c-header__link" href="https://order.springer.com/public/cart" style="appearance:none;border:none;background:none;color:inherit;position:relative">
  <svg id="eds-i-cart" class="eds-c-header__icon" xmlns="http://www.w3.org/2000/svg" height="24" width="24" viewBox="0 0 24 24" aria-hidden="true" focusable="false">
   <path fill="currentColor" fill-rule="nonzero" d="M2 1a1 1 0 0 0 0 2l1.659.001 2.257 12.808a2.599 2.599 0 0 0 2.435 2.185l.167.004 9.976-.001a2.613 2.613 0 0 0 2.61-1.748l.03-.106 1.755-7.82.032-.107a2.546 2.546 0 0 0-.311-1.986l-.108-.157a2.604 2.604 0 0 0-2.197-1.076L6.042 5l-.56-3.17a1 1 0 0 0-.864-.82l-.12-.007L2.001 1ZM20.35 6.996a.63.63 0 0 1 .54.26.55.55 0 0 1 .082.505l-.028.1L19.2 15.63l-.022.05c-.094.177-.282.299-.526.317l-10.145.002a.61.61 0 0 1-.618-.515L6.394 6.999l13.955-.003ZM18 19a2 2 0 1 0 0 4 2 2 0 0 0 0-4ZM8 19a2 2 0 1 0 0 4 2 2 0 0 0 0-4Z"></path>
  </svg>
  <span>Cart</span><span class="cart-info" style="display:none;position:absolute;top:10px;right:45px;background-color:#C65301;color:#fff;width:18px;height:18px;font-size:11px;border-radius:50%;line-height:17.5px;text-align:center"></span>
 </a>
 <script>(function () { var exports = {}; if (window.fetch) {
            
            "use strict";
Object.defineProperty(exports, "__esModule", { value: true });
exports.headerWidgetClientInit = void 0;
var headerWidgetClientInit = function (getCartInfo) {
    document.body.addEventListener("updatedCart", function () {
        updateCartIcon();
    }, false);
    return updateCartIcon();
    function updateCartIcon() {
        return getCartInfo()
            .then(function (res) { return res.json(); })
            .then(refreshCartState)
            .catch(function (_) { });
    }
    function refreshCartState(json) {
        var indicator = document.querySelector("#ecommerce-header-cart-icon-link .cart-info");
        /* istanbul ignore else */
        if (indicator && json.itemCount) {
            indicator.style.display = 'block';
            indicator.textContent = json.itemCount > 9 ? '9+' : json.itemCount.toString();
            var moreThanOneItem = json.itemCount > 1;
            indicator.setAttribute('title', "there ".concat(moreThanOneItem ? "are" : "is", " ").concat(json.itemCount, " item").concat(moreThanOneItem ? "s" : "", " in your cart"));
        }
        return json;
    }
};
exports.headerWidgetClientInit = headerWidgetClientInit;

            
            headerWidgetClientInit(
              function () {
                return window.fetch("https://cart.springer.com/cart-info", {
                  credentials: "include",
                  headers: { Accept: "application/json" }
                })
              }
            )
        }})()</script>
</div>
                        
                    
                </div>
            </div>
        </nav>
    
</header>



    <article lang="en" id="main" class="app-masthead__colour-22">
        <section class="app-masthead " aria-label="article masthead">
    <div class="app-masthead__container">
        
            <div class="app-article-masthead u-sans-serif js-context-bar-sticky-point-masthead" data-track-component="article" data-test="masthead-component">
                <div class="app-article-masthead__info">
                    
    
        <nav aria-label="breadcrumbs" data-test="breadcrumbs">
            <ol class="c-breadcrumbs c-breadcrumbs--contrast" itemscope itemtype="https://schema.org/BreadcrumbList">
                
                    <li class="c-breadcrumbs__item" id="breadcrumb0" itemprop="itemListElement" itemscope="" itemtype="https://schema.org/ListItem">
                        <a href="/" class="c-breadcrumbs__link" itemprop="item" data-track="click_breadcrumb" data-track-context="article page"  data-track-category="article" data-track-action="breadcrumbs" data-track-label="breadcrumb1"><span itemprop="name">Home</span></a><meta itemprop="position" content="1">
                            <svg class="c-breadcrumbs__chevron" role="img" aria-hidden="true" focusable="false" width="10" height="10" viewBox="0 0 10 10">
                                <path d="m5.96738168 4.70639573 2.39518594-2.41447274c.37913917-.38219212.98637524-.38972225 1.35419292-.01894278.37750606.38054586.37784436.99719163-.00013556 1.37821513l-4.03074001 4.06319683c-.37758093.38062133-.98937525.38100976-1.367372-.00003075l-4.03091981-4.06337806c-.37759778-.38063832-.38381821-.99150444-.01600053-1.3622839.37750607-.38054587.98772445-.38240057 1.37006824.00302197l2.39538588 2.4146743.96295325.98624457z" fill-rule="evenodd" transform="matrix(0 -1 1 0 0 10)"/>
                            </svg>
                    </li>
                
                    <li class="c-breadcrumbs__item" id="breadcrumb1" itemprop="itemListElement" itemscope="" itemtype="https://schema.org/ListItem">
                        <a href="/journal/10489" class="c-breadcrumbs__link" itemprop="item" data-track="click_breadcrumb" data-track-context="article page"  data-track-category="article" data-track-action="breadcrumbs" data-track-label="breadcrumb2"><span itemprop="name">Applied Intelligence</span></a><meta itemprop="position" content="2">
                            <svg class="c-breadcrumbs__chevron" role="img" aria-hidden="true" focusable="false" width="10" height="10" viewBox="0 0 10 10">
                                <path d="m5.96738168 4.70639573 2.39518594-2.41447274c.37913917-.38219212.98637524-.38972225 1.35419292-.01894278.37750606.38054586.37784436.99719163-.00013556 1.37821513l-4.03074001 4.06319683c-.37758093.38062133-.98937525.38100976-1.367372-.00003075l-4.03091981-4.06337806c-.37759778-.38063832-.38381821-.99150444-.01600053-1.3622839.37750607-.38054587.98772445-.38240057 1.37006824.00302197l2.39538588 2.4146743.96295325.98624457z" fill-rule="evenodd" transform="matrix(0 -1 1 0 0 10)"/>
                            </svg>
                    </li>
                
                    <li class="c-breadcrumbs__item" id="breadcrumb2" itemprop="itemListElement" itemscope="" itemtype="https://schema.org/ListItem">
                        <span itemprop="name">Article</span><meta itemprop="position" content="3">
                    </li>
                
            </ol>
        </nav>
    

                    <h1 class="c-article-title" data-test="article-title" data-article-title="">Adaptive patch selection to improve Vision Transformers through Reinforcement Learning</h1>

                    <ul class="c-article-identifiers">
                        
    
        <li class="c-article-identifiers__item">
            <a href="https://www.springernature.com/gp/open-science/about/the-fundamentals-of-open-access-and-open-research" data-track="click" data-track-action="open access" data-track-label="link" class="u-color-open-access" data-test="open-access">Open access</a>
        </li>
    
    

                        <li class="c-article-identifiers__item">
                            Published: <time datetime="2025-04-01">01 April 2025</time>
                        </li>
                    </ul>
                    <ul class="c-article-identifiers c-article-identifiers--cite-list">
                        <li class="c-article-identifiers__item">
                            <span data-test="journal-volume">Volume55</span>, articlenumber<span data-test="article-number">607</span>, (<span data-test="article-publication-year">2025</span>)
            
                        </li>
                        <li class="c-article-identifiers__item c-article-identifiers__item--cite">
                            <a href="#citeas" data-track="click" data-track-action="cite this article" data-track-category="article body" data-track-label="link">Cite this article</a>
                        </li>
                    </ul>

                    <div class="app-article-masthead__buttons" data-test="download-article-link-wrapper" data-track-context="masthead">
                        <p class="app-article-masthead__access">
                            <svg width="16" height="16" focusable="false" role="img" aria-hidden="true"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#icon-eds-i-check-filled-medium"></use></svg>
                            You have full access to this <a href="https://www.springernature.com/gp/open-science/about/the-fundamentals-of-open-access-and-open-research" data-track="click" data-track-action="open access" data-track-label="link">open access</a> article</p>
                        
                        <div class="app-article-masthead__access-container">
                            
        <div class="c-pdf-container">
            <div class="c-pdf-download u-clear-both u-mb-16">
                <a href="/content/pdf/10.1007/s10489-025-06516-z.pdf" class="c-pdf-download__link" data-article-pdf="true" data-readcube-pdf-url="true" data-test="pdf-link" data-draft-ignore="true" data-track="content_download" data-track-type="article pdf download" data-track-action="download pdf" data-track-label="button" data-track-external download>
                    
                        <span class="c-pdf-download__text">Download PDF</span>
                        <svg aria-hidden="true" focusable="false" width="16" height="16" class="u-icon"><use xlink:href="#icon-eds-i-download-medium"/></svg>
                    
                </a>
            </div>
        </div>
    

                            
                            
                        </div>
                    </div>
                </div>
                <div class="app-article-masthead__brand">
                    
                        
                            <a href="/journal/10489"
                        
                           class="app-article-masthead__journal-link"
                           data-track="click_journal_home"
                           data-track-action="journal homepage"
                           data-track-context="article page"
                           data-track-label="link">
                            <picture>
                                <source type="image/webp" media="(min-width: 768px)" width="120" height="159"
                                        srcset="https://media.springernature.com/w120/springer-static/cover-hires/journal/10489?as=webp,
                                                    https://media.springernature.com/w316/springer-static/cover-hires/journal/10489?as=webp 2x">
                                <img width="72" height="95"
                                     src="https://media.springernature.com/w72/springer-static/cover-hires/journal/10489?as=webp"
                                     srcset="https://media.springernature.com/w144/springer-static/cover-hires/journal/10489?as=webp 2x" alt="">
                            </picture>
                            <span class="app-article-masthead__journal-title">Applied Intelligence</span>
                        </a>
                        
                            <a href="/journal/10489/aims-and-scope" class="app-article-masthead__submission-link"
                               data-track="click_aims_and_scope"
                               data-track-action="aims and scope"
                               data-track-context="article page"
                               data-track-label="link">
                                Aims and scope
                                <svg width="16" height="16" focusable="false" role="img" aria-hidden="true" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#icon-eds-i-arrow-right-medium"></use></svg>
                            </a>
                        
                        
                            <a href="https://www.editorialmanager.com/apin" class="app-article-masthead__submission-link"
                               data-track="click_submit_manuscript"
                               data-track-context="article masthead on springerlink article page"
                               data-track-action="submit manuscript"
                               data-track-label="link">
                                Submit manuscript
                                <svg width="16" height="16" focusable="false" role="img" aria-hidden="true" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#icon-eds-i-arrow-right-medium"></use></svg>
                            </a>
                        
                    
                </div>
            </div>
        
    </div>
</section>

        <div class="c-article-main u-container u-mt-24 u-mb-32 l-with-sidebar" id="main-content"
             data-component="article-container">
            <main class="u-serif js-main-column" data-track-component="article body">
                
                
                    <div class="c-context-bar u-hide"
                         data-test="context-bar"
                         data-context-bar
                         aria-hidden="true">
                        <div class="c-context-bar__container">
                            <div class="c-context-bar__title">
                                Adaptive patch selection to improve Vision Transformers through Reinforcement Learning
                            </div>
                            
                                <div class="c-context-bar__cta-container" data-test="inCoD" data-track-context="sticky banner">
                                    
        <div class="c-pdf-container">
            <div class="c-pdf-download u-clear-both u-mb-16">
                <a href="/content/pdf/10.1007/s10489-025-06516-z.pdf" class="c-pdf-download__link" data-article-pdf="true" data-readcube-pdf-url="true" data-test="pdf-link" data-draft-ignore="true" data-track="content_download" data-track-type="article pdf download" data-track-action="download pdf" data-track-label="button" data-track-external download>
                    
                        <span class="c-pdf-download__text">Download PDF</span>
                        <svg aria-hidden="true" focusable="false" width="16" height="16" class="u-icon"><use xlink:href="#icon-eds-i-download-medium"/></svg>
                    
                </a>
            </div>
        </div>
    


                                    
                                </div>
                            
                        </div>
                    </div>
                

                <div class="c-article-header">
                    <header>
                        <ul class="c-article-author-list c-article-author-list--short" data-test="authors-list" data-component-authors-activator="authors-list"><li class="c-article-author-list__item"><a data-test="author-name" data-track="click" data-track-action="open author" data-track-label="link" data-track-index="1_5" data-track-context="researcher popup with no profile" href="#auth-Francesco-Cauteruccio-Aff1" data-author-popup="auth-Francesco-Cauteruccio-Aff1" data-author-search="Cauteruccio, Francesco">Francesco Cauteruccio</a><sup class="u-js-hide"><a href="#Aff1">1</a></sup>, </li><li class="c-article-author-list__item"><a data-test="author-name" data-track="click" data-track-action="open author" data-track-label="link" data-track-index="2_5" data-track-context="researcher popup with no profile" href="#auth-Michele-Marchetti-Aff2" data-author-popup="auth-Michele-Marchetti-Aff2" data-author-search="Marchetti, Michele">Michele Marchetti</a><sup class="u-js-hide"><a href="#Aff2">2</a></sup>, </li><li class="c-article-author-list__item c-article-author-list__item--hide-small-screen"><a data-test="author-name" data-track="click" data-track-action="open author" data-track-label="link" data-track-index="3_5" data-track-context="researcher popup with no profile" href="#auth-Davide-Traini-Aff3" data-author-popup="auth-Davide-Traini-Aff3" data-author-search="Traini, Davide">Davide Traini</a><sup class="u-js-hide"><a href="#Aff3">3</a></sup>, </li><li class="c-article-author-list__item c-article-author-list__item--hide-small-screen"><a data-test="author-name" data-track="click" data-track-action="open author" data-track-label="link" data-track-index="4_5" data-track-context="researcher popup with no profile" href="#auth-Domenico-Ursino-Aff2" data-author-popup="auth-Domenico-Ursino-Aff2" data-author-search="Ursino, Domenico">Domenico Ursino</a><sup class="u-js-hide"><a href="#Aff2">2</a></sup> &amp; </li><li class="c-article-author-list__show-more" aria-label="Show all 5 authors for this article" title="Show all 5 authors for this article"></li><li class="c-article-author-list__item"><a data-test="author-name" data-track="click" data-track-action="open author" data-track-label="link" data-track-index="5_5" data-track-context="researcher popup with no profile" href="#auth-Luca-Virgili-Aff2" data-author-popup="auth-Luca-Virgili-Aff2" data-author-search="Virgili, Luca" data-corresp-id="c1">Luca Virgili<svg width="16" height="16" focusable="false" role="img" aria-hidden="true" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#icon-eds-i-mail-medium"></use></svg></a><sup class="u-js-hide"><a href="#Aff2">2</a></sup></li></ul><button aria-expanded="false" class="c-article-author-list__button"><svg width="16" height="16" focusable="false" role="img" aria-hidden="true" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#icon-eds-i-chevron-down-medium"></use></svg><span>Show authors</span></button>
                        
    


                        <div data-test="article-metrics">
                            
        <ul class="app-article-metrics-bar u-list-reset">
            
                <li class="app-article-metrics-bar__item" data-test="access-count">
                    <p class="app-article-metrics-bar__count"><svg class="u-icon app-article-metrics-bar__icon" width="24" height="24" aria-hidden="true" focusable="false">
                        <use xlink:href="#icon-eds-i-accesses-medium"></use>
                    </svg>2924 <span class="app-article-metrics-bar__label">Accesses</span></p>
                </li>
            
            
                <li class="app-article-metrics-bar__item" data-test=citation-count>
                    <p class="app-article-metrics-bar__count"><svg class="u-icon app-article-metrics-bar__icon" width="24" height="24" aria-hidden="true" focusable="false">
                        <use xlink:href="#icon-eds-i-citations-medium"></use>
                    </svg>4 <span class="app-article-metrics-bar__label">Citations</span></p>
                </li>
            
            
            
            
                
                    <li class="app-article-metrics-bar__item app-article-metrics-bar__item--metrics">
                        <p class="app-article-metrics-bar__details"><a href="/article/10.1007/s10489-025-06516-z/metrics" data-track="click" data-track-action="view metrics" data-track-label="link" rel="nofollow">Explore all metrics <svg class="u-icon app-article-metrics-bar__arrow-icon" width="24" height="24" aria-hidden="true" focusable="false">
                            <use xlink:href="#icon-eds-i-arrow-right-medium"></use>
                        </svg></a></p>
                    </li>
                
            
        </ul>
    
                        </div>
                        
                        
    <div class="u-mt-32">
    

    
    </div>

                        
                    </header>
                </div>

                <div data-article-body="true" data-track-component="article body" class="c-article-body">

                    
                    <section aria-labelledby="Abs1" data-title="Abstract" lang="en"><div class="c-article-section" id="Abs1-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Abs1">Abstract</h2><div class="c-article-section__content" id="Abs1-content"><p>In recent years, Transformers have revolutionized the management of Natural Language Processing tasks, and Vision Transformers (ViTs) promise to do the same for Computer Vision ones. However, the adoption of ViTs is hampered by their computational cost. Indeed, given an image divided into patches, it is necessary to compute for each layer the attention of each patch with respect to all the others. Researchers have proposed many solutions to reduce the computational cost of attention layers by adopting techniques such as quantization, knowledge distillation and manipulation of input images. In this paper, we aim to contribute to the solution of this problem. In particular, we propose a new framework, called AgentViT, which uses Reinforcement Learning to train an agent that selects the most important patches to improve the learning of a ViT. The goal of AgentViT is to reduce the number of patches processed by a ViT, and thus its computational load, while still maintaining competitive performance. We tested AgentViT on CIFAR10, FashionMNIST, and Imagenette<span class="mathjax-tex">\(^+\)</span> (which is a subset of ImageNet) in the image classification task and obtained promising performance when compared to baseline ViTs and other related approaches available in the literature.</p></div></div></section>

                    

                    
    


                    

                    <div data-test="cobranding-download">
                        
                    </div>

                    
                        
        
            <section aria-labelledby="inline-recommendations" data-title="Inline Recommendations" class="c-article-recommendations" data-track-component="inline-recommendations">
                <h3 class="c-article-recommendations-title" id="inline-recommendations">Similar content being viewed by others</h3>
                <div class="c-article-recommendations-list">
                    
                        <div class="c-article-recommendations-list__item">
                            <article class="c-article-recommendations-card" itemscope itemtype="http://schema.org/ScholarlyArticle">
                                
                                    <div class="c-article-recommendations-card__img"><img src="https://media.springernature.com/w215h120/springer-static/image/art%3A10.1007%2Fs11760-025-04020-y/MediaObjects/11760_2025_4020_Fig1_HTML.png" loading="lazy" alt=""></div>
                                
                                <div class="c-article-recommendations-card__main">
                                    <h3 class="c-article-recommendations-card__heading" itemprop="name headline">
                                        <a class="c-article-recommendations-card__link"
                                           itemprop="url"
                                           href="https://link.springer.com/10.1007/s11760-025-04020-y?fromPaywallRec=false"
                                           data-track="select_recommendations_1"
                                           data-track-context="inline recommendations"
                                           data-track-action="click recommendations inline - 1"
                                           data-track-label="10.1007/s11760-025-04020-y">Patch-wise self-supervised visual representation learning: a fine-grained approach
                                        </a>
                                    </h3>
                                    <div class="c-article-meta-recommendations" data-test="recommendation-info">
                                        <span class="c-article-meta-recommendations__item-type">Article</span>
                                        
                                         <span class="c-article-meta-recommendations__date">03 April 2025</span>
                                    </div>
                                </div>
                            </article>
                        </div>
                    
                        <div class="c-article-recommendations-list__item">
                            <article class="c-article-recommendations-card" itemscope itemtype="http://schema.org/ScholarlyArticle">
                                
                                    <div class="c-article-recommendations-card__img"><img src="https://media.springernature.com/w92h120/springer-static/cover-hires/book/978-3-031-16014-1?as&#x3D;webp" loading="lazy" alt=""></div>
                                
                                <div class="c-article-recommendations-card__main">
                                    <h3 class="c-article-recommendations-card__heading" itemprop="name headline">
                                        <a class="c-article-recommendations-card__link"
                                           itemprop="url"
                                           href="https://link.springer.com/10.1007/978-3-031-16014-1_59?fromPaywallRec=false"
                                           data-track="select_recommendations_2"
                                           data-track-context="inline recommendations"
                                           data-track-action="click recommendations inline - 2"
                                           data-track-label="10.1007/978-3-031-16014-1_59">TT-ViT: Vision Transformer Compression Using Tensor-Train Decomposition
                                        </a>
                                    </h3>
                                    <div class="c-article-meta-recommendations" data-test="recommendation-info">
                                        <span class="c-article-meta-recommendations__item-type">Chapter</span>
                                        
                                         <span class="c-article-meta-recommendations__date"> 2022</span>
                                    </div>
                                </div>
                            </article>
                        </div>
                    
                        <div class="c-article-recommendations-list__item">
                            <article class="c-article-recommendations-card" itemscope itemtype="http://schema.org/ScholarlyArticle">
                                
                                    <div class="c-article-recommendations-card__img"><img src="https://media.springernature.com/w92h120/springer-static/cover-hires/book/978-3-031-19806-9?as&#x3D;webp" loading="lazy" alt=""></div>
                                
                                <div class="c-article-recommendations-card__main">
                                    <h3 class="c-article-recommendations-card__heading" itemprop="name headline">
                                        <a class="c-article-recommendations-card__link"
                                           itemprop="url"
                                           href="https://link.springer.com/10.1007/978-3-031-19806-9_13?fromPaywallRec=false"
                                           data-track="select_recommendations_3"
                                           data-track-context="inline recommendations"
                                           data-track-action="click recommendations inline - 3"
                                           data-track-label="10.1007/978-3-031-19806-9_13">Training Vision Transformers withonly2040 Images
                                        </a>
                                    </h3>
                                    <div class="c-article-meta-recommendations" data-test="recommendation-info">
                                        <span class="c-article-meta-recommendations__item-type">Chapter</span>
                                        
                                         <span class="c-article-meta-recommendations__date"> 2022</span>
                                    </div>
                                </div>
                            </article>
                        </div>
                    
                </div>
            </section>
        
            <script>
                window.dataLayer = window.dataLayer || [];
                window.dataLayer.push({
                    recommendations: {
                        recommender: 'semantic',
                        model: 'specter',
                        policy_id: 'NA',
                        timestamp: 1766855373,
                        embedded_user: 'null'
                    }
                });
            </script>
        
    
                    

                    
                        
    <section class="app-explore-related-subjects" aria-labelledby="content-related-subjects" data-test="subject-content">
        <h3 id="content-related-subjects" class="app-explore-related-subjects__title">Explore related subjects</h3>
        <span class="u-sans-serif u-text-xs u-display-block u-mb-16">Discover the latest articles, books and news in related subjects, suggested using machine learning.</span>
        <ul class="app-explore-related-subjects__list app-explore-related-subjects__list--no-mb" role="list">
        
            <li class="app-explore-related-subjects__items" data-test="related-subject-item">
                <a href="/subjects/computational-intelligence"  data-track="select_related_subject_1" data-track-context="related subjects from content page" data-track-label="Computational Intelligence">Computational Intelligence</a>
            </li>
        
            <li class="app-explore-related-subjects__items" data-test="related-subject-item">
                <a href="/subjects/computer-vision"  data-track="select_related_subject_2" data-track-context="related subjects from content page" data-track-label="Computer Vision">Computer Vision</a>
            </li>
        
            <li class="app-explore-related-subjects__items" data-test="related-subject-item">
                <a href="/subjects/learning-theory"  data-track="select_related_subject_3" data-track-context="related subjects from content page" data-track-label="Learning Theory">Learning Theory</a>
            </li>
        
            <li class="app-explore-related-subjects__items" data-test="related-subject-item">
                <a href="/subjects/machine-learning"  data-track="select_related_subject_4" data-track-context="related subjects from content page" data-track-label="Machine Learning">Machine Learning</a>
            </li>
        
            <li class="app-explore-related-subjects__items" data-test="related-subject-item">
                <a href="/subjects/artificial-intelligence"  data-track="select_related_subject_5" data-track-context="related subjects from content page" data-track-label="Artificial Intelligence">Artificial Intelligence</a>
            </li>
        
            <li class="app-explore-related-subjects__items" data-test="related-subject-item">
                <a href="/subjects/agent-based-economics"  data-track="select_related_subject_6" data-track-context="related subjects from content page" data-track-label="Agent-based Economics">Agent-based Economics</a>
            </li>
        
        </ul>
    </section>

                    

                    
                        
    <div class="app-card-service" data-test="article-checklist-banner">
        <div>
            <a class="app-card-service__link" data-track="click_presubmission_checklist" data-track-context="article page top of reading companion" data-track-category="pre-submission-checklist" data-track-action="clicked article page checklist banner test 2 old version" data-track-label="link" href="https://beta.springernature.com/pre-submission?journalId=10489"
            data-test="article-checklist-banner-link">
            <span class="app-card-service__link-text">Use our pre-submission checklist</span>
            <svg class="app-card-service__link-icon" aria-hidden="true" focusable="false"><use xlink:href="#icon-eds-i-arrow-right-small"></use></svg>
            </a>
            <p class="app-card-service__description">Avoid common mistakes on your manuscript.</p>
        </div>
        <div class="app-card-service__icon-container">
            <svg class="app-card-service__icon" aria-hidden="true" focusable="false">
                <use xlink:href="#icon-eds-i-clipboard-check-medium"></use>
            </svg>
        </div>
    </div>

                    

                    
                        
                                <div class="main-content">
                                    <section data-title="Introduction"><div class="c-article-section" id="Sec1-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec1"><span class="c-article-section__title-number">1 </span>Introduction</h2><div class="c-article-section__content" id="Sec1-content"><p>In recent years, thanks to deep neural networks, Artificial Intelligence has experienced a remarkable expansion, achieving significant advances in several areas, including Natural Language Processing (NLP) and Computer Vision (CV) [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" title="Min B, Ross H, Sulem E, Veyseh APB, Nguyen TH, Sainz O, Agirre E, Heintz I, Roth D (2023) Recent advances in natural language processing via large pre-trained language models: a survey. ACM Comput Surv 56(2):140" href="#ref-CR1" id="ref-link-section-d305401337e488">1</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" title="Chai J, Zeng H, Li A, Ngai E (2021) Deep learning in computer vision: a critical review of emerging techniques and application scenarios. Mach Learn Appl 6:100134" href="#ref-CR2" id="ref-link-section-d305401337e488_1">2</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 3" title="Vaswani A, Shazeer N, Parmar N, Uszkoreit J, Jones L, Gomez AN, Kaiser , Polosukhin I (2017) Attention is All you Need. In: Proceedings of the international conference on advances in neural information processing systems (NIPS17), Long Beach, CA, USA. Curran Associates, p 30" href="/article/10.1007/s10489-025-06516-z#ref-CR3" id="ref-link-section-d305401337e491">3</a>]. Transformers are one of the key players in this development [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 3" title="Vaswani A, Shazeer N, Parmar N, Uszkoreit J, Jones L, Gomez AN, Kaiser , Polosukhin I (2017) Attention is All you Need. In: Proceedings of the international conference on advances in neural information processing systems (NIPS17), Long Beach, CA, USA. Curran Associates, p 30" href="/article/10.1007/s10489-025-06516-z#ref-CR3" id="ref-link-section-d305401337e494">3</a>]. Their architecture is based on attention blocks that search for semantic relationships between input elements. The way Transformers work gives them a big advantage over earlier architectures based on Recurrent Neural Networks (RNNs) and Convolutional Neural Networks (CNNs), whose performance degrades as input length increases. The first definition of Transformers was limited to NLP, but in 2021, the Vision Transformer (ViT) [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" title="Dosovitskiy A, Beyer L, Kolesnikov A, Weissenborn D, Zhai X, Unterthiner T, Dehghani M, Minderer M, Heigold G, Gelly S, Uszkoreit J, Houlsby N (2021) An image is worth 16x16 words: transformers for image recognition at scale. In: Proceedings of the international conference on learning representations (ICLR21), Virtual event. OpenReview.net" href="#ref-CR4" id="ref-link-section-d305401337e497">4</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" title="Waqas M, Tahir MA, Danish M, Al-Maadeed S, Bouridane A, Wu J (2024) Simultaneous instance pooling and bag representation selection approach for multiple-instance learning (MIL) using vision transformer. Neural Comput Appl 122. Springer" href="#ref-CR5" id="ref-link-section-d305401337e497_1">5</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 6" title="Poornam S, Angelina J (2024) VITALT: a robust and efficient brain tumor detection system using vision transformer with attention and linear transformation. Neural Comput Appl 117. Springer" href="/article/10.1007/s10489-025-06516-z#ref-CR6" id="ref-link-section-d305401337e500">6</a>] was introduced, which works in the field of Computer Vision. This architecture is inspired by that of Transformer in NLP, but instead of dividing a sentence into words, it divides an image into non-overlapping rectangular patches and looks for semantic correlations between them. ViTs are highly competitive and in some cases have been shown to be superior to CNNs [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 4" title="Dosovitskiy A, Beyer L, Kolesnikov A, Weissenborn D, Zhai X, Unterthiner T, Dehghani M, Minderer M, Heigold G, Gelly S, Uszkoreit J, Houlsby N (2021) An image is worth 16x16 words: transformers for image recognition at scale. In: Proceedings of the international conference on learning representations (ICLR21), Virtual event. OpenReview.net" href="/article/10.1007/s10489-025-06516-z#ref-CR4" id="ref-link-section-d305401337e504">4</a>]. However, despite their promising results, ViTs face a critical limitation: their computational complexity. The self-attention mechanism in ViTs requires the computation of attention scores between all pairs of patches in each layer, resulting in high computational cost, especially as the input size increases. To overcome this problem, researchers have proposed many variants of ViTs aimed at reducing the cost of attention layers [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" title="Child R, Gray S, Radford A, Sutskever I (2019) Generating long sequences with sparse transformers. &#xA;                  arXiv:1904.10509&#xA;                  &#xA;                " href="#ref-CR7" id="ref-link-section-d305401337e507">7</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" title="Fayyaz M, Koohpayegani SA, Jafari FR, Sengupta S, Joze HRV, Sommerlade E, Pirsiavash H, Gall J (2022) Adaptive token sampling for efficient vision transformers. In: Proceedings of the european conference on computer vision (ECCV22), Tel Aviv, Israel. Springer, pp 396414" href="#ref-CR8" id="ref-link-section-d305401337e507_1">8</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" title="Yin H, Vahdat A, Alvarez JM, Mallya A, Kautz J, Molchanov P (2022) A-vit: adaptive tokens for efficient vision transformer. In: Proceedings of the international conference on computer vision and pattern recognition (CVPR22), New Orleans, LA, USA, pp 1080910818" href="#ref-CR9" id="ref-link-section-d305401337e507_2">9</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" title="Renggli C, Pinto AS, Houlsby N, Mustafa B, Puigcerver J, Riquelme C (2022) Learning to merge tokens in vision transformers. &#xA;                  arXiv:2202.12015&#xA;                  &#xA;                " href="#ref-CR10" id="ref-link-section-d305401337e507_3">10</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 11" title="Liang Y, Ge C, Tong Z, Song Y, Wang J, Xie P (2022) EViT: expediting vision transformers via token reorganizations. In: Proceedings of the international conference on learning representations (ICLR22), Virtual event. OpenReview.net" href="/article/10.1007/s10489-025-06516-z#ref-CR11" id="ref-link-section-d305401337e510">11</a>].</p><p>At the same time, Reinforcement Learning (RL) has emerged as a powerful tool in AI, demonstrating remarkable potential in a variety of domains, from robotics to intelligent transportation systems [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" title="Polydoros AS, Nalpantidis L (2017) Survey of model-based reinforcement learning: applications on robotics. J Intell Robot Syst 86(2):153173" href="#ref-CR12" id="ref-link-section-d305401337e516">12</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" title="Coronato A, Naeem M, Pietro GD, Paragliola G (2020) Reinforcement learning for intelligent healthcare applications: a survey. Artif Intell Med 109:101964" href="#ref-CR13" id="ref-link-section-d305401337e516_1">13</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" title="Nian R, Liu J, Huang B (2020) A review on reinforcement learning: introduction and applications in industrial process control. Comput Chem Eng 139:106886" href="#ref-CR14" id="ref-link-section-d305401337e516_2">14</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" title="Luong NC, Hoang DT, Gong S, Niyato D, Wang P, Liang YC, Kim DI (2019) Applications of deep reinforcement learning in communications and networking: a survey. IEEE Commun Surv Tutor 21(4):31333174" href="#ref-CR15" id="ref-link-section-d305401337e516_3">15</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" title="Haydari A, Ylmaz Y (2020) Deep reinforcement learning for intelligent transportation systems: a survey. IEEE Trans Intell Trans Syst 23(1):1132" href="#ref-CR16" id="ref-link-section-d305401337e516_4">16</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 17" title="Fang W, Pang L, Yi W (2020) Survey on the application of deep reinforcement learning in image processing. J Artif Intell 2(1):3958" href="/article/10.1007/s10489-025-06516-z#ref-CR17" id="ref-link-section-d305401337e519">17</a>]. RL excels in environments where sequential decision-making is critical, making it a natural fit for optimization problems where tradeoffs between multiple objectives need to be explored dynamically. This motivates us to explore the use of RL as a means to mitigate the computational challenges of ViTs.</p><p>Two main strands of approaches are commonly used in the literature to reduce the computational complexity of ViTs, namely token merging and token pruning. Token merging combines tokens across ViT layers, preserving high-resolution information in the early layers, and reducing the number of tokens in the deeper layers [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 10" title="Renggli C, Pinto AS, Houlsby N, Mustafa B, Puigcerver J, Riquelme C (2022) Learning to merge tokens in vision transformers. &#xA;                  arXiv:2202.12015&#xA;                  &#xA;                " href="/article/10.1007/s10489-025-06516-z#ref-CR10" id="ref-link-section-d305401337e525">10</a>, <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 18" title="Yuan X, Fei H, Baek J (2024) Efficient transformer adaptation with soft token merging. In: Proceedings of the IEEE/CVF conference on computer vision and pattern recognition (CVPR24), Seattle, WA, USA, pp 36583668" href="/article/10.1007/s10489-025-06516-z#ref-CR18" id="ref-link-section-d305401337e528">18</a>, <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 19" title="Bolya D, Hoffman J (2023) Token merging for fast stable diffusion. In: Proceedings of the IEEE/CVF conference on computer vision and pattern recognition (CVPR23), Vancouver, British Columbia, Canada, pp 45984602" href="/article/10.1007/s10489-025-06516-z#ref-CR19" id="ref-link-section-d305401337e531">19</a>]. In contrast, token pruning focuses on selecting a subset of tokens to reduce the overall computational load [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" title="Tang Y, Han K, Wang Y, Xu C, Guo J, Xu C, Tao D (2022) Patch slimming for efficient vision transformers. In: Proceedings of the international conference on computer vision and pattern recognition (CVPR22), New Orleans, LA, USA, pp 1216512174" href="#ref-CR20" id="ref-link-section-d305401337e534">20</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" title="Rao Y, Zhao W, Liu B, Lu J, Zhou J, Hsieh CJ (2021) Dynamicvit: efficient vision transformers with dynamic token sparsification. Adv Neural Inf Process Syst 34:1393713949" href="#ref-CR21" id="ref-link-section-d305401337e534_1">21</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 22" title="Rao Y, Liu Z, Zhao W, Zhou J, Lu J (2023) Dynamic spatial sparsification for efficient vision transformers and convolutional neural networks. IEEE Trans Pattern Anal Mach Intell. IEEE" href="/article/10.1007/s10489-025-06516-z#ref-CR22" id="ref-link-section-d305401337e537">22</a>]. Despite the promising results of these approaches, they often require tradeoffs and still suffer from inherent limitations that hinder their adaptability to different datasets and tasks. Some approaches gradually reduce input resolution to reduce the number of operations, but this can lead to the loss of important fine-grained details, especially in images where important features are widely distributed.</p><p>Token pruning approaches select a subset of tokens based on predefined thresholds; however, they lack the flexibility to dynamically adjust as image complexity varies. For example, methods that rely on predefined thresholds or a fixed number of selected tokens, such as DyViT [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 21" title="Rao Y, Zhao W, Liu B, Lu J, Zhou J, Hsieh CJ (2021) Dynamicvit: efficient vision transformers with dynamic token sparsification. Adv Neural Inf Process Syst 34:1393713949" href="/article/10.1007/s10489-025-06516-z#ref-CR21" id="ref-link-section-d305401337e543">21</a>], may lack the flexibility to adapt dynamically to different input images. In fact, correctly classifying a complex image might require more tokens than the predefined threshold. As an example, consider an image with important details spread over a large area; it may require more tokens for accurate classification than an image in which the relevant object is confined to a smaller region. Alternative strategies, such as model compression or knowledge distillation, improve efficiency but often at the cost of reduced representational capacity and generalization.</p><p>These limitations highlight the need for a more adaptive framework that can dynamically balance efficiency and accuracy based on the characteristics of each input. In this scenario, RL provides a dynamic and adaptive framework for optimizing ViT training. By using an RL agent to learn and select the most relevant patches, we exploit the agents ability to balance computational efficiency and model performance. Unlike static methods, RL allows for an adaptive approach where the agent learns the optimal number of tokens based on the specific characteristics of the input data. This dynamic adaptation can lead to more efficient training without compromising accuracy, as the agent continuously improves its strategy based on feedback observed during ViT training.</p><p>Following this reasoning, we propose an innovative framework, called AgentViT, for ViT optimization. As suggested by the previous observations, our approach uses RL to reduce the computational complexity of the attention layers and, consequently, the training time of ViTs. To achieve these goals, we designed and implemented an RL agent that selects a subset of the image patches so that the ViT has to process them for its training, and thus less data than the original patches. The goal is to reduce training time while maintaining competitive performance. In addition, the use of an external agent enables transfer learning by using pre-trained weights to speed up training. The agent is trained to select relevant patches, a process that can potentially be generalized across models and tasks. Previously learned patch selection strategies can be leveraged through careful fine-tuning to adapt the agent to the new scenario. In AgentViT, we used the Double Deep Q-Learning Network (DDQN) algorithm [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 23" title="Hasselt HV, Guez A, Silver D (2016) Deep reinforcement learning with double q-learning. In: Proceedings of the AAAI conference on artificial intelligence (AAAI16), vol. 30. Phoenix, AZ, USA" href="/article/10.1007/s10489-025-06516-z#ref-CR23" id="ref-link-section-d305401337e553">23</a>]. For each training batch, the agent observes the attention values generated by the first ViT layer and selects a subset of the original patches to be used during the ViT training process. The ViT is then trained using only the selected patches. After a certain number of training iterations, the agent receives a reward based on a combination of training loss and the number of patches selected. To provide some flexibility, AgentViT allows the user to decide how much weight to assign to each of these two parameters in order to favor a set of patches that guarantees low training time or low training loss. We tested AgentViT on the CIFAR10 [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 24" title="Krizhevsky A, Nair V, Hinton G (2010) CIFAR-10 (Canadian Institute for Advanced Research). &#xA;                  https://www.cs.toronto.edu/~kriz/cifar.html&#xA;                  &#xA;                " href="/article/10.1007/s10489-025-06516-z#ref-CR24" id="ref-link-section-d305401337e556">24</a>], FashionMNIST [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 25" title="Xiao H, Rasul K, Vollgraf R (2017) Fashion-MNIST: a novel image dataset for benchmarking machine learning algorithms. &#xA;                  arXiv:1708.07747&#xA;                  &#xA;                " href="/article/10.1007/s10489-025-06516-z#ref-CR25" id="ref-link-section-d305401337e559">25</a>], and Imagenette<span class="mathjax-tex">\(^+\)</span> datasets; the latter is a subset of the ImageNet [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 26" title="Deng J, Dong W, Socher R, Li LJ, Kai L, Li FF (2009) ImageNet: a large-scale hierarchical image database. In: Proceedings of the International IEEE Conference on Computer Vision and Pattern Recognition (CVPR09), Miami, FL, USA. IEEE, pp 248255" href="/article/10.1007/s10489-025-06516-z#ref-CR26" id="ref-link-section-d305401337e586">26</a>] dataset. During these tests, we compared the results obtained by AgentViT with those obtained by other related approaches available in the literature taking both efficiency and classification metrics into account.</p><p>This paper is organized as follows: in Section <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10489-025-06516-z#Sec2">2</a>, we present related work. In Section <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10489-025-06516-z#Sec5">3</a>, we describe the proposed framework. In Section <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10489-025-06516-z#Sec11">4</a>, we illustrate our experimental campaign aimed at testing AgentViT on different datasets with different parameter values and comparing the results obtained with those of related approaches. In Section <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10489-025-06516-z#Sec19">5</a>, we propose a discussion of the results obtained and the implications and limitations of AgentViT. Finally, in Section <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10489-025-06516-z#Sec20">6</a>, we draw our conclusions and look at possible future developments of our research efforts in this area.</p></div></div></section><section data-title="Related work"><div class="c-article-section" id="Sec2-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec2"><span class="c-article-section__title-number">2 </span>Related work</h2><div class="c-article-section__content" id="Sec2-content"><p>In this section, we present approaches already proposed in the literature that are related to our own. Specifically, in Subsection <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10489-025-06516-z#Sec3">2.1</a> we present RL approaches in complex environments, while in Subsection <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10489-025-06516-z#Sec4">2.2</a> we illustrate approaches for speeding up ViTs.</p><h3 class="c-article__sub-heading" id="Sec3"><span class="c-article-section__title-number">2.1 </span>Reinforcement learning in complex environments</h3><p>Reinforcement Learning (RL) has been widely applied in complex environments where decision-making involves sequential interactions with dynamic systems, such as robotics, autonomous systems, and generative models [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" title="Perera ATD, Kamalaruban P (2021) Applications of reinforcement learning in energy systems. Renew Sustain Energy Rev 137:110618" href="#ref-CR27" id="ref-link-section-d305401337e628">27</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" title="Zuccotto M, Castellini A, Torre DL, Mola L, Farinelli A (2024) Reinforcement learning applications in environmental sustainability: a review. Artif Intell Rev 57(4):88" href="#ref-CR28" id="ref-link-section-d305401337e628_1">28</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 29" title="Mai V, Maisonneuve P, Zhang T, Nekoei H, Paull L, Lesage-Landry A (2024) Multi-agent reinforcement learning for fast-timescale demand response of residential loads. Mach Learn 113(5):33553355" href="/article/10.1007/s10489-025-06516-z#ref-CR29" id="ref-link-section-d305401337e631">29</a>]. Unlike traditional supervised learning approaches that rely on large labeled datasets, RL optimizes long-term objectives by learning from rewards, making it particularly useful for fine-tuning pre-trained models, while adapting them to new constraints or feedback mechanisms.</p><p>One such example can be found in [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 30" title="Zhao X, Ding S, An Y, Jia W (2019) Applications of asynchronous deep reinforcement learning based on dynamic updating weights. Appl Intell 49:581591" href="/article/10.1007/s10489-025-06516-z#ref-CR30" id="ref-link-section-d305401337e637">30</a>], where the authors integrate DWA3C, an Asynchronous Advantage Actor-Critical approach, which incorporates dynamic update weights to improve convergence speed and performance in deep RL. DWA3C improves learning efficiency by dynamically weighting updates to prevent ineffective updates from slowing learning. This enhancement is particularly valuable in high-dimensional and real-time environments, such as robotics and autonomous driving, where faster convergence results in better control, optimized decision-making, and reduced training time.</p><p>Similarly, the authors of [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 31" title="Ding S, Zhao X, Xu X, Sun T, Jia W (2019) An effective asynchronous framework for small scale reinforcement learning problems. Appl Intell 49:43034318" href="/article/10.1007/s10489-025-06516-z#ref-CR31" id="ref-link-section-d305401337e643">31</a>] introduce four asynchronous RL algorithms to address slow convergence and instability in small-scale discrete problems. By leveraging parallel actor-learners and asynchronous updates, these algorithms improve learning efficiency, making RL more viable for real-time decision-making. The integration of eligibility traces and model-based learning further optimizes performance in resource-constrained environments.</p><p>Beyond robotics, RL has also demonstrated effectiveness in energy management. For example, the authors of [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 29" title="Mai V, Maisonneuve P, Zhang T, Nekoei H, Paull L, Lesage-Landry A (2024) Multi-agent reinforcement learning for fast-timescale demand response of residential loads. Mach Learn 113(5):33553355" href="/article/10.1007/s10489-025-06516-z#ref-CR29" id="ref-link-section-d305401337e649">29</a>] present a multi-agent RL approach that uses multi-agent proximal policy optimization to regulate residential power demand by coordinating air conditioners. The model optimizes consumption while maintaining indoor temperature stability, and explores hand-engineered and learned communication strategies. The study shows that localized decision-making can enhance demand response without significant infrastructure investment, contributing to autonomous, resilient power systems that improve grid stability and renewable energy integration.</p><p>In the context of generative models, RL has been used to refine text-to-image diffusion models. For example, the authors of [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 32" title="Fan Y, Watkins O, Du Y, Liu H, Ryu M, Boutilier C, Abbeel P, Ghavamzadeh M, Lee K, Lee K (2023) Reinforcement learning for fine-tuning text-to-image diffusion models. In: Proceedings of the annual conference on neural information processing systems (NeurIPS23), New Orleans, LA, USA" href="/article/10.1007/s10489-025-06516-z#ref-CR32" id="ref-link-section-d305401337e656">32</a>] present DPOK, an RL-based fine-tuning approach that optimizes a model using policy gradient methods and human feedback-driven rewards. To maintain alignment with the pre-trained model, DPOK applies Kullback-Leibler divergence regularization. This online update mechanism continuously evaluates results, reducing biases, enhancing interpretability, and correcting distortions in ambiguous prompts.</p><p>RL has also been widely applied in computer vision, where it allows models to dynamically optimize complex tasks that require adaptive decision-making [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" title="Alrebdi N, Alrumiah S, Almansour A, Rassam M (2022) Reinforcement learning in image classification: a review. In: Proceedings of the international conference on computing and information technology (ICCIT22), Tabuk, Saudi Arabia. IEEE, pp 7986" href="#ref-CR33" id="ref-link-section-d305401337e662">33</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" title="Jiu M, Song X, Sahbi H, Li S, Chen Y, Guo W, Guo L, Xu M (2024) Image classification with deep reinforcement active learning. &#xA;                  arXiv:2412.19877&#xA;                  &#xA;                " href="#ref-CR34" id="ref-link-section-d305401337e662_1">34</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 35" title="Zhou SK, Le HN, Luu K, Nguyen HV, Ayache N (2021) Deep reinforcement learning in medical imaging: a literature review. Med Image Anal 73:102193" href="/article/10.1007/s10489-025-06516-z#ref-CR35" id="ref-link-section-d305401337e665">35</a>]. For instance, the authors of [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 36" title="Gupta SK (2020) Reinforcement based learning on classification task could yield better generalization and adversarial accuracy. In: Proceedings of the annual conference on neural information processing systems workshop on shared visual representations in human and machine intelligence (SVRHM@NeurIPS20), Virtual event" href="/article/10.1007/s10489-025-06516-z#ref-CR36" id="ref-link-section-d305401337e668">36</a>] present an RL-based approach to image classification that replaces cross-entropy loss with reward-based optimization inspired by the vanilla policy gradient method. This way of proceeding improves the robustness of the model against adversarial attacks, demonstrating superior accuracy against Fast Gradient Sign Method and AutoAttack compared to traditional training methods.</p><p>RL-based approaches have also been applied to object detection in high-resolution images, especially in remote sensing. For example, the authors of [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 37" title="Uzkent B, Yeh C, Ermon S (2020) Efficient object detection in large images using deep reinforcement learning. In: Proceedings of the IEEE/CVF winter conference on applications of computer vision (WACV20), Snowmass Village, CO, USA, pp 18241833" href="/article/10.1007/s10489-025-06516-z#ref-CR37" id="ref-link-section-d305401337e674">37</a>] propose an adaptive approach that selects spatial resolution in a two-step process. This approach first analyzes a low-resolution image to capture the global context and then selectively requests high-resolution patches for detailed object detection. This strategy significantly reduces computational cost while maintaining accuracy, and provides an efficient alternative to traditional sliding window techniques and full-resolution processing.</p><p>Another area where RL has shown remarkable effectiveness is feature selection. By dynamically optimizing feature selection, RL reduces redundancy while retaining informative features; thus, it is able to efficiently handle high-dimensional datasets and balance performance with the number of features [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" title="Moslemi A (2023) A tutorial-based survey on feature selection: recent advancements on feature selection. Eng Appl Artif Intell 126:107136" href="#ref-CR38" id="ref-link-section-d305401337e680">38</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" title="Paniri M, Dowlatshahi MB, Nezamabadi-pour H (2021) Ant-TD: ant colony optimization plus temporal difference reinforcement learning for multi-label feature selection. Swarm Evol Comput 64:100892" href="#ref-CR39" id="ref-link-section-d305401337e680_1">39</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 40" title="Zhang L, Jin L, Gan M, Zhao L, Yin H (2023) Reinforced feature selection using Q-learning based on collaborative agents. Int J Mach Learn Cybern 14(11):38673882" href="/article/10.1007/s10489-025-06516-z#ref-CR40" id="ref-link-section-d305401337e683">40</a>]. For example, the authors of [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 41" title="Liu K, Fu Y, Wu L, Li X, Aggarwal C, Xiong H (2021) Automated feature selection: a reinforcement learning perspective. IEEE Trans Knowl Data Eng 35(3):22722284" href="/article/10.1007/s10489-025-06516-z#ref-CR41" id="ref-link-section-d305401337e686">41</a>] apply a multi-agent approach based on RL for feature selection. This approach treats each feature as an agent that decides its inclusion based on statistical descriptions, autoencoders, and Graph Convolutional Networks. A tailored reward mechanism improves coordination, while generative sampling strategies and interactive exploration improve efficiency and convergence. Notably, the role of RL in feature selection aligns with its application in patch selection for image analysis. As RL identifies the most relevant features in tabular data to reduce dimensionality while preserving predictive power, it can also determine the most meaningful image regions by discarding less relevant patches; in this way, it can improve computational efficiency without significantly degrading performance. Taken together, these studies illustrate the versatility of RL across a wide range of applications demonstrating its potential to enhance decision-making in complex, high-dimensional environments.</p><h3 class="c-article__sub-heading" id="Sec4"><span class="c-article-section__title-number">2.2 </span>Speeding up vision transformers</h3><p>Several approaches have been proposed in the literature to reduce the computational load of the attention layers present in Transformers. These approaches are based on different techniques, such as quantization [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" title="Wang K, Liu Z, Lin Y, Lin J, Han S (2019) Haq: hardware-aware automated quantization with mixed precision. In: Proceedings of the international conference on computer vision and pattern recognition (CVPR19), Long Beach, CA, USA, pp 86128620" href="#ref-CR42" id="ref-link-section-d305401337e697">42</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" title="Gong Y, Liu Y, Yang M, Bourdev L (2014) Compressing deep convolutional networks using vector quantization. &#xA;                  arXiv:1412.6115&#xA;                  &#xA;                " href="#ref-CR43" id="ref-link-section-d305401337e697_1">43</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" title="Yuan Z, Xue C, Chen Y, Wu Q, Sun G (2022) Ptq4vit: Post-training quantization for vision transformers with twin uniform quantization. In: Proc. of the European Conference on Computer Vision (ECCV22), Tel Aviv, Israel. Springer, pp 191207" href="#ref-CR44" id="ref-link-section-d305401337e697_2">44</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" title="Lin Y, Zhang T, Sun P, Li Z, Zhou S (2022) FQ-ViT: post-training quantization for fully quantized vision transformer. In: Proceedings of the international joint conference on artificial intelligence (IJCAI22), Vienna, Austria, pp 11731179" href="#ref-CR45" id="ref-link-section-d305401337e697_3">45</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" title="Ding Y, Qin H, Yan Q, Chai Z, Liu J, Wei X, Liu X (2022) Towards accurate post-training quantization for vision transformer. In: Proceedings of the international conference on multimedia (MM22), Lisbon, Portugal, pp 53805388" href="#ref-CR46" id="ref-link-section-d305401337e697_4">46</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" title="Li Z, Yang T, Wang P, Cheng J (2022) Q-vit: fully differentiable quantization for vision transformer. &#xA;                  arXiv:2201.07703&#xA;                  &#xA;                " href="#ref-CR47" id="ref-link-section-d305401337e697_5">47</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 48" title="Liu Z, Wang Y, Han K, Zhang W, Ma S, Gao W (2021) Post-training quantization for vision transformer. Adv Neural Inf Process Syst 34:2809228103" href="/article/10.1007/s10489-025-06516-z#ref-CR48" id="ref-link-section-d305401337e700">48</a>], pruning [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" title="He Y, Zhang X, Sun J (2017) Channel pruning for accelerating very deep neural networks. In: Proceedings of the international conference on computer vision (ICCV17), Venice, Italy, pp 13891397" href="#ref-CR49" id="ref-link-section-d305401337e703">49</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" title="Rao Y, Lu J, Lin J, Zhou J (2018) Runtime network routing for efficient image classification. IEEE Trans Pattern Anal Mach Intell 41(10):22912304" href="#ref-CR50" id="ref-link-section-d305401337e703_1">50</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" title="Zhu M, Tang Y, Han K (2021) Vision transformer pruning. &#xA;                  arXiv:2104.08500&#xA;                  &#xA;                " href="#ref-CR51" id="ref-link-section-d305401337e703_2">51</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 52" title="Yu F, Huang K, Wang M, Cheng Y, Chu W, Cui L (2022) Width &amp; depth pruning for vision transformers. In: Proceedings of the international conference on artificial intelligence (AAAI22), vol. 36. Virtual event, pp 31433151" href="/article/10.1007/s10489-025-06516-z#ref-CR52" id="ref-link-section-d305401337e706">52</a>], low-rank factorization [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 53" title="Yu X, Liu T, Wang X, Tao D (2017) On compressing deep models by low rank and sparse decomposition. In: Proceedings of the international conference on computer vision and pattern recognition (CVPR17), Honolulu, HI, USA, pp 73707379" href="/article/10.1007/s10489-025-06516-z#ref-CR53" id="ref-link-section-d305401337e709">53</a>, <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 54" title="Jaderberg M, Vedaldi A, Zisserman A (2014) Speeding up convolutional neural networks with low rank expansions. &#xA;                  arXiv:1405.3866&#xA;                  &#xA;                " href="/article/10.1007/s10489-025-06516-z#ref-CR54" id="ref-link-section-d305401337e713">54</a>] and knowledge distillation [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" title="Hinton G, Vinyals O, Dean J (2015) Distilling the knowledge in a neural network. &#xA;                  arXiv:1503.02531&#xA;                  &#xA;                " href="#ref-CR55" id="ref-link-section-d305401337e716">55</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" title="Liu B, Rao Y, Lu J, Zhou J, Hsieh CJ (2020) Metadistiller: network self-boosting via meta-learned top-down distillation. In: Proceedings of the european conference on computer vision (ECCV20), Glasgow, Scotland, UK. Springer, pp 694709" href="#ref-CR56" id="ref-link-section-d305401337e716_1">56</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" title="Wang W, Wei F, Dong L, Bao H, Yang N, Zhou M (2020) Minilm: deep self-attention distillation for task-agnostic compression of pre-trained transformers. Adv Neural Inf Process Syst 33:57765788" href="#ref-CR57" id="ref-link-section-d305401337e716_2">57</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" title="Liu Y, Bech P, Tamura K, Dlez LT, Crochet S, Petersen CCH (2024) Cell class-specific long-range axonal projections of neurons in mouse whisker-related somatosensory cortices. eLife 13:97602. &#xA;                  https://doi.org/10.7554/eLife.97602.3&#xA;                  &#xA;                " href="#ref-CR58" id="ref-link-section-d305401337e716_3">58</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" title="Chen X, Cao Q, Zhong Y, Zhang J, Gao S, Tao D (2022) Dearkd: data-efficient early knowledge distillation for vision transformers. In: Proceedings of the international conference on computer vision and pattern recognition (CVPR22), New Orleans, LA, USA, pp 1205212062" href="#ref-CR59" id="ref-link-section-d305401337e716_4">59</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 60" title="Touvron H, Cord M, Douze M, Massa F, Sablayrolles A, Jgo H (2021) Training data-efficient image transformers &amp; distillation through attention. In: Proceedings of the International Conference on Machine Learning (ICML21), Virtual event. PMLR, pp 1034710357" href="/article/10.1007/s10489-025-06516-z#ref-CR60" id="ref-link-section-d305401337e719">60</a>]. For example, TinyBERT [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 61" title="Jiao X, Yin Y, Shang L, Jiang X, Chen X, Li L, Wang F, Liu Q (2020) TinyBERT: distilling BERT for natural language understanding. In: Findings of the association for computational linguistics: EMNLP20. Virtual event. Association for Computational Linguistics, pp 41634174" href="/article/10.1007/s10489-025-06516-z#ref-CR61" id="ref-link-section-d305401337e722">61</a>] uses a student-teacher knowledge distillation technique where a smaller model is trained by the predictions of a larger model. Other approaches try to reduce the complexity of attention layers; for example, in [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 62" title="Guo Q, Qiu X, Liu P, Shao Y, Xue X, Zhang Z (2019) Star-transformer. &#xA;                  arXiv:1902.09113&#xA;                  &#xA;                " href="/article/10.1007/s10489-025-06516-z#ref-CR62" id="ref-link-section-d305401337e725">62</a>] the authors replace the fully connected structure typical of the classical attention mechanism with a star topology, in which each pair of non-adjacent nodes is connected by a shared node. In [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 63" title="Roy A, Saffar M, Vaswani A, Grangier D (2021) Efficient content-based sparse attention with routing transformers. Trans Assoc Comput Ling 9:5368" href="/article/10.1007/s10489-025-06516-z#ref-CR63" id="ref-link-section-d305401337e728">63</a>], the authors use a sparse routing module based on k-means to reduce the overall computational complexity of attention. The authors of [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 64" title="Gregor K, Danihelka I, Graves A, Rezende D, Wierstra D (2015) Draw: a recurrent neural network for image generation. In: Proceedings of the international conference on machine learning (ICML15), Lille, France. PMLR, pp 14621471" href="/article/10.1007/s10489-025-06516-z#ref-CR64" id="ref-link-section-d305401337e732">64</a>] introduce a hierarchical attention strategy; it first considers which part of the inputs should be attended, and then computes full attention in a contiguous neighborhood of the selected area. In [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 7" title="Child R, Gray S, Radford A, Sutskever I (2019) Generating long sequences with sparse transformers. &#xA;                  arXiv:1904.10509&#xA;                  &#xA;                " href="/article/10.1007/s10489-025-06516-z#ref-CR7" id="ref-link-section-d305401337e735">7</a>], the authors propose an approach that reduces computational complexity by performing the sparse factorization of the attention matrix.</p><p>As for reducing the computational load of ViTs, [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 65" title="Li Y, Liu A, Fu X, Mckeown MJ, Wang ZJ, Chen X (2022) Atlas-guided parcellation: individualized functionally-homogenous parcellation in cerebral cortex. Comput Biol Med 150:106078" href="/article/10.1007/s10489-025-06516-z#ref-CR65" id="ref-link-section-d305401337e741">65</a>] is one of the first attempts to apply self-supervised learning to ViTs. The authors start from the observation that a multi-stage architecture with sparse self-attention can reduce modeling complexity, but at the cost of losing the ability to capture fine-grained correspondences between image regions. In [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 66" title="Grainger R, Paniagua T, Song X, Cuntoor N, Lee MW, Wu T (2023) PaCa-ViT: learning patch-to-cluster attention in vision transformers. In: Proceedings of the IEEE/CVF conference on computer vision and pattern recognition (CVPR23), Vancouver, British Columbia, Canada, pp 1856818578" href="/article/10.1007/s10489-025-06516-z#ref-CR66" id="ref-link-section-d305401337e744">66</a>], the authors introduce Patch-to-Cluster attention to improve the efficiency and interpretability of ViTs. This is done by learning cluster-based attention with linear complexity and improving interpretability through semantically meaningful clusters. However, the approaches most similar to ours are those that reduce the number of operations by minimizing the number of patches processed. These approaches can be divided into two main strands, focusing on token merging and token pruning, respectively; both modify the ViT input to reduce the computational cost during training and/or inference.</p><p>The first strand includes token merging approaches, where tokens are combined across multiple layers of the ViT. In general, high-resolution information is preserved in early layers to capture fine-grained details, while tokens are merged in deeper layers to reduce their number and computational requirements [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 67" title="Kim M, Gao S, Hsu YC, Shen Y, Jin H (2024) Token fusion: bridging the gap between token pruning and token merging. In: Proceedings of the IEEE/CVF winter conference on applications of computer vision (WCAV24), Waikoloa, HI, USA, pp 13831392" href="/article/10.1007/s10489-025-06516-z#ref-CR67" id="ref-link-section-d305401337e750">67</a>]. This process allows the ViT to achieve high performance by exploiting information in early layers and reducing computation in deeper layers. PatchMerger [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 10" title="Renggli C, Pinto AS, Houlsby N, Mustafa B, Puigcerver J, Riquelme C (2022) Learning to merge tokens in vision transformers. &#xA;                  arXiv:2202.12015&#xA;                  &#xA;                " href="/article/10.1007/s10489-025-06516-z#ref-CR10" id="ref-link-section-d305401337e753">10</a>] falls into this category; it consists of a module that reduces the number of patches the network has to process by merging patches between two successive intermediate layers. This module can be used at different depths to gradually reduce the number of tokens processed by the ViT. The authors of [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 18" title="Yuan X, Fei H, Baek J (2024) Efficient transformer adaptation with soft token merging. In: Proceedings of the IEEE/CVF conference on computer vision and pattern recognition (CVPR24), Seattle, WA, USA, pp 36583668" href="/article/10.1007/s10489-025-06516-z#ref-CR18" id="ref-link-section-d305401337e756">18</a>] present a different approach to merging. In each layer, a soft fusion algorithm reduces the number of processed tokens; the output of this module is processed by the transformer layer and then oversampled to the original number of tokens. Another approach is ToMe [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 19" title="Bolya D, Hoffman J (2023) Token merging for fast stable diffusion. In: Proceedings of the IEEE/CVF conference on computer vision and pattern recognition (CVPR23), Vancouver, British Columbia, Canada, pp 45984602" href="/article/10.1007/s10489-025-06516-z#ref-CR19" id="ref-link-section-d305401337e759">19</a>], which is based on similarity and bipartite soft matching. This module can merge similar tokens without introducing too much overhead in computing token similarity.</p><p>The second strand concerns token pruning methods, which includes our approach. Approaches in this strand aim at reducing the number of tokens processed by the transformer layers, thus reducing the computational and memory overhead. For example, in [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 21" title="Rao Y, Zhao W, Liu B, Lu J, Zhou J, Hsieh CJ (2021) Dynamicvit: efficient vision transformers with dynamic token sparsification. Adv Neural Inf Process Syst 34:1393713949" href="/article/10.1007/s10489-025-06516-z#ref-CR21" id="ref-link-section-d305401337e765">21</a>], the authors propose DynamicViT (DyViT), a special token pruning approach that uses a set of layers trained during ViT training epochs. These layers are placed after the attention layers and select a subset of tokens that are then used in the deeper layers. The number of tokens must be set by the user, which can be disadvantageous because there is a risk of using too few tokens in complex images or too many tokens in simple images. In [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 20" title="Tang Y, Han K, Wang Y, Xu C, Guo J, Xu C, Tao D (2022) Patch slimming for efficient vision transformers. In: Proceedings of the international conference on computer vision and pattern recognition (CVPR22), New Orleans, LA, USA, pp 1216512174" href="/article/10.1007/s10489-025-06516-z#ref-CR20" id="ref-link-section-d305401337e768">20</a>], the authors present an approach that discards unnecessary patches in a top-down paradigm. It identifies effective patches in the last layer and uses them to guide the selection of patches from previous layers. Specifically, for each layer it approximates the impact of a patch on the features of the final output and removes patches with less impact. The authors of [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 22" title="Rao Y, Liu Z, Zhao W, Zhou J, Lu J (2023) Dynamic spatial sparsification for efficient vision transformers and convolutional neural networks. IEEE Trans Pattern Anal Mach Intell. IEEE" href="/article/10.1007/s10489-025-06516-z#ref-CR22" id="ref-link-section-d305401337e771">22</a>] propose a token sparsification framework to progressively and dynamically prune redundant tokens based on the input. In particular, they propose a lightweight module to estimate the importance of each token based on its current features. This module is added to multiple layers to hierarchically prune the redundant tokens. In [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 68" title="Meng L, Li H, Chen BC, Lan S, Wu Z, Jiang YG, Lim SN (2022) Adavit: adaptive vision transformers for efficient image recognition. In: Proceedings of the international conference on computer vision and pattern recognition (CVPR22), New Orleans, LA, USA, pp 1230912318" href="/article/10.1007/s10489-025-06516-z#ref-CR68" id="ref-link-section-d305401337e774">68</a>], the authors propose AdaViT, an adaptive computational framework that learns which patches, self-attention heads, and transformer blocks to use throughout the model depending on the input. In [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 8" title="Fayyaz M, Koohpayegani SA, Jafari FR, Sengupta S, Joze HRV, Sommerlade E, Pirsiavash H, Gall J (2022) Adaptive token sampling for efficient vision transformers. In: Proceedings of the european conference on computer vision (ECCV22), Tel Aviv, Israel. Springer, pp 396414" href="/article/10.1007/s10489-025-06516-z#ref-CR8" id="ref-link-section-d305401337e777">8</a>], the authors introduce Adaptive Token Sampling Vision Transformer (ATSViT), which exploits the importance of tokens to reduce computational complexity. Specifically, after computing a metric based on the attention value, ATSViT uses replacement sampling, where each token has a probability of being selected equal to the value of the metric. The main differences between ATSViT and previous approaches are the absence of additional parameters in the ViT and the dynamic computation of the number of tokens based on the input image batch, rather than being statically defined by the user. Another relevant approach is Expediting Vision Transformer (EViT) [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 11" title="Liang Y, Ge C, Tong Z, Song Y, Wang J, Xie P (2022) EViT: expediting vision transformers via token reorganizations. In: Proceedings of the international conference on learning representations (ICLR22), Virtual event. OpenReview.net" href="/article/10.1007/s10489-025-06516-z#ref-CR11" id="ref-link-section-d305401337e781">11</a>], which uses attention scores to evaluate the importance of each token. To reduce complexity, EViT divides tokens into three groups: attentive, semi-attentive and non-attentive tokens. The non-attentive tokens are pruned, the semi-attentive are merged, and the resulting token is concatenated with the attentive tokens.</p><p>Now that we have seen several related approaches, let us see how AgentViT compares to them. AgentViT shares with ATSViT [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 8" title="Fayyaz M, Koohpayegani SA, Jafari FR, Sengupta S, Joze HRV, Sommerlade E, Pirsiavash H, Gall J (2022) Adaptive token sampling for efficient vision transformers. In: Proceedings of the european conference on computer vision (ECCV22), Tel Aviv, Israel. Springer, pp 396414" href="/article/10.1007/s10489-025-06516-z#ref-CR8" id="ref-link-section-d305401337e788">8</a>] the policy of using a dynamic number of tokens based on the input images. This allows it to better adapt to the input images. However, the mechanisms used by AgentViT and ATSViT are completely different. In fact, ATSViT allows the user to specify the maximum number of tokens to be selected. Therefore, if after resampling the number of tokens is greater than the user-defined value, some of them will still have to be discarded. Instead, AgentViT allows the user to specify the desired number of tokens and the agent is trained to select a number of tokens that is as close as possible to the number specified by the user. In particular, if it achieves a particularly low training loss, the total reward will make it understand that it is advantageous to select a lower number of tokens for that particular batch of images. On the other hand, if it achieves a high training loss for a particular batch, the reward mechanism will cause it to select a higher number of tokens than the one specified by the user. Similar to [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 21" title="Rao Y, Zhao W, Liu B, Lu J, Zhou J, Hsieh CJ (2021) Dynamicvit: efficient vision transformers with dynamic token sparsification. Adv Neural Inf Process Syst 34:1393713949" href="/article/10.1007/s10489-025-06516-z#ref-CR21" id="ref-link-section-d305401337e791">21</a>, <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 22" title="Rao Y, Liu Z, Zhao W, Zhou J, Lu J (2023) Dynamic spatial sparsification for efficient vision transformers and convolutional neural networks. IEEE Trans Pattern Anal Mach Intell. IEEE" href="/article/10.1007/s10489-025-06516-z#ref-CR22" id="ref-link-section-d305401337e794">22</a>, <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 68" title="Meng L, Li H, Chen BC, Lan S, Wu Z, Jiang YG, Lim SN (2022) Adavit: adaptive vision transformers for efficient image recognition. In: Proceedings of the international conference on computer vision and pattern recognition (CVPR22), New Orleans, LA, USA, pp 1230912318" href="/article/10.1007/s10489-025-06516-z#ref-CR68" id="ref-link-section-d305401337e797">68</a>], AgentViT uses an external module to learn which patches should flow into the next layers. This feature may reduce the generality of our approach, as it requires a fine-tuning cycle to be applied to a specific ViT.</p><p>Unlike previously mentioned methods such as ATSViT and ToMe, the use of an external agent allows AgentViT to leverage transfer learning by using the weights of a pre-trained agent, thus speeding up the training process. Specifically, our approach involves training the agent to select a subset of relevant patches, a task that remains consistent across different models and datasets. As a result, once the agent has been trained to select patches in a particular task or model, it can be reused and adapted for different models or tasks after a careful fine-tuning. This feature allows our agent to be more adaptable to the dataset. In contrast, the aforementioned approaches use attention values and similarity between embeddings, which in some cases could lead to suboptimal selections due to their heuristic nature. In contrast, AgentViT dynamically learns the best subset of patches based on the specific characteristics of the dataset, rather than relying on predefined rules or fixed attention patterns.</p></div></div></section><section data-title="Description of AgentVIT"><div class="c-article-section" id="Sec5-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec5"><span class="c-article-section__title-number">3 </span>Description of AgentVIT</h2><div class="c-article-section__content" id="Sec5-content"><p>In this section, we present the details of AgentViT. Specifically, in Subsection <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10489-025-06516-z#Sec6">3.1</a> we propose an overview of the RL scenario and show a schematic workflow of our framework. In Subsection <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10489-025-06516-z#Sec7">3.2</a>, we define the states of our RL environment. In Subsection <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10489-025-06516-z#Sec8">3.3</a>, we describe the actions that our agent can perform. In Subsection <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10489-025-06516-z#Sec9">3.4</a> we illustrate the reward function used to train our agent. Finally, in Subsection <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10489-025-06516-z#Sec10">3.5</a> we discuss the convergence of the proposed method.</p><h3 class="c-article__sub-heading" id="Sec6"><span class="c-article-section__title-number">3.1 </span>Schematic workflow of AgentViT</h3><p>Like many RL approaches [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 69" title="Le Lan C, Tu S, Oberman A, Agarwal R, Bellemare MG (2022) On the generalization of representations in reinforcement learning. In: Proceedings of the international conference on artificial intelligence and statistics (AISTATS22). Virtual event. PMLR, vol 151, pp 41324157" href="/article/10.1007/s10489-025-06516-z#ref-CR69" id="ref-link-section-d305401337e834">69</a>], AgentViT employs a Markov Decision Process (MDP) <span class="mathjax-tex">\(\mathcal {M} = \langle \mathcal {S}, \mathcal {A}, \mathcal {R}, \mathcal {P}, \gamma \rangle \)</span>, with a state space <span class="mathjax-tex">\(\mathcal {S}\)</span>, a discrete set of actions <span class="mathjax-tex">\(\mathcal {A}\)</span>, a reward function <span class="mathjax-tex">\(\mathcal {R}: \mathcal {S} \times \mathcal {A} \rightarrow \mathbb {R}\)</span>, a transition kernel <span class="mathjax-tex">\(\mathcal {P}: \mathcal {S} \times \mathcal {A} \rightarrow \mathcal {S}\)</span>, and a discount factor <span class="mathjax-tex">\(\gamma \in [0, 1)\)</span>. In an MDP, a stationary policy <span class="mathjax-tex">\(\pi : \mathcal {S} \rightarrow \mathcal {A}\)</span> is a mapping from states to actions; it indicates the action taken by an agent when it is in a given state. It is used to describe how an agent interacts with the environment. Specifically, we describe the state space <span class="mathjax-tex">\(\mathcal {S}\)</span> in Subsection <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10489-025-06516-z#Sec7">3.2</a>, present the action space <span class="mathjax-tex">\(\mathcal {A}\)</span> in Subsection <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10489-025-06516-z#Sec8">3.3</a>, and finally illustrate the reward formulation <span class="mathjax-tex">\(\mathcal {R}\)</span> in Subsection <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10489-025-06516-z#Sec9">3.4</a>.</p><p>To estimate the expected cumulative reward that an agent can obtain from a particular state-action pair (<i>s</i>,<i>a</i>) with <span class="mathjax-tex">\(s \in \mathcal {S}\)</span> and <span class="mathjax-tex">\(a \in \mathcal {A}\)</span>, we employ an Action Value Function <i>Q</i>(<i>s</i>,<i>a</i>) introduced in Q-Learning [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 70" title="Arulkumaran K, Deisenroth MP, Brundage M, Bharath AA (2017) Deep reinforcement learning: a brief survey. IEEE Signal Process Mag 34(6):2638" href="/article/10.1007/s10489-025-06516-z#ref-CR70" id="ref-link-section-d305401337e1202">70</a>]. It represents the expected cumulative reward that the agent can achieve starting from state <i>s</i>, taking action <i>a</i>, and then following a certain policy <span class="mathjax-tex">\(\pi \)</span> for the next decisions. The Q-value <i>Q</i>(<i>s</i>,<i>a</i>) can be defined recursively using the Bellman equation [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 71" title="Watkins CJCH, Dayan P (1992) Q-learning. Mach Learn 8:279292" href="/article/10.1007/s10489-025-06516-z#ref-CR71" id="ref-link-section-d305401337e1238">71</a>] as follows:</p><div id="Equ1" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex">$$\begin{aligned} Q(s, a) = \mathbb {E} \left[ \mathcal {R}_{s,a} + \gamma \cdot \max _{a' \in \mathcal {A}} Q(s^*, a') \right] \end{aligned}$$</span></div><div class="c-article-equation__number">
                    (3.1)
                </div></div><p>where:</p><ul class="u-list-style-bullet">
                  <li>
                    <p><span class="mathjax-tex">\(\mathbb {E}\)</span> denotes the expected value.</p>
                  </li>
                  <li>
                    <p><span class="mathjax-tex">\(\mathcal {R}_{s,a}\)</span> is the reward obtained for the state <i>s</i>, after executing the action <i>a</i>.</p>
                  </li>
                  <li>
                    <p><span class="mathjax-tex">\(\gamma \)</span> is the discount factor, which represents the importance of reward immediacy. Specifically, if <span class="mathjax-tex">\(\gamma = 0\)</span> , actions that produce an immediate reward are more important than the others. If <span class="mathjax-tex">\(\gamma = 1\)</span>, all rewards have equal weight, regardless of when they are obtained; this favors a long-term view.</p>
                  </li>
                  <li>
                    <p><span class="mathjax-tex">\(s^*\)</span> is the next state, i.e., the state in which the agent arrives by starting from <i>s</i> and executing the action <i>a</i>.</p>
                  </li>
                </ul><p>This equation specifies that the Q-value associated with the pair (<i>s</i>,<i>a</i>) and relative to the policy <span class="mathjax-tex">\(\pi \)</span> is a function of the reward obtained after taking the action <i>a</i> and the cumulative reward associated with the next state <span class="mathjax-tex">\(s^*\)</span> when the best action <span class="mathjax-tex">\(a' \in \mathcal{A}\)</span> is taken in it. The weight of the reward associated with the action <i>a</i> and the ones associated with the next state are determined by the discount factor <span class="mathjax-tex">\(\gamma \)</span>.</p><p>The Q-Learning algorithm uses a table that consists of a number of rows equal to the number of observable states and a number of columns equal to the number of possible actions. During the execution of the algorithm, the values in the table are updated by means of the formula given in (<a data-track="click" data-track-label="link" data-track-action="equation anchor" href="/article/10.1007/s10489-025-06516-z#Equ2">3.2</a>). This formula allows the cumulative rewards associated with each pair <i>Q</i>(<i>s</i>,<i>a</i>) to be obtained recursively:</p><div id="Equ2" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex">$$\begin{aligned} Q(s, a) \leftarrow Q(s, a) + \eta \cdot \left[ \mathcal {R}_{s,a} + \gamma \cdot \max _{a' \in \mathcal{A}}(Q(s^*, a')) - Q(s, a) \right] \end{aligned}$$</span></div><div class="c-article-equation__number">
                    (3.2)
                </div></div><p>Here:</p><ul class="u-list-style-bullet">
                  <li>
                    <p><span class="mathjax-tex">\(\eta \)</span> is the learning rate; it is a number in the real interval [0,1], which specifies the rate at which the agent learns;</p>
                  </li>
                  <li>
                    <p><span class="mathjax-tex">\(\gamma \)</span> and <span class="mathjax-tex">\(s^*\)</span> have been introduced above.</p>
                  </li>
                </ul><p>This algorithm struggles when the number of states is high or when the states involved are continuous. In these cases the table is replaced by a neural network, which is referred to as Deep Q-Network. It receives the vector representing the state <i>s</i> and computes the Q-values associated with each pair <span class="mathjax-tex">\((s, a^*)\)</span>, where <span class="mathjax-tex">\(a^*\)</span> represents any action that the agent can perform in the state <i>s</i>. The agent selects the action with the highest Q-value. The DQN algorithm, which uses the same neural network to both select and evaluate actions, is prone to overestimation because of the maximization step [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 72" title="Fan J, Wang Z, Xie Y, Yang Z (2020) A theoretical analysis of deep Q-learning. In: Proceedings of the international conference on learning for dynamics and control (L4DC20), Berkeley, CA, USA. PMLR, pp 486489" href="/article/10.1007/s10489-025-06516-z#ref-CR72" id="ref-link-section-d305401337e1963">72</a>]. In this step, the action with the highest predicted Q-value is selected, and the network is simultaneously used to estimate the value of that selected action. This dual use of a single network tends to amplify noise and inaccuracies in the value estimation, leading to systematic overestimation of Q-values. To mitigate this bias, we used the Double Deep Q-Network (DDQN) [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 23" title="Hasselt HV, Guez A, Silver D (2016) Deep reinforcement learning with double q-learning. In: Proceedings of the AAAI conference on artificial intelligence (AAAI16), vol. 30. Phoenix, AZ, USA" href="/article/10.1007/s10489-025-06516-z#ref-CR23" id="ref-link-section-d305401337e1967">23</a>], which decouples the selection of actions from their evaluation. Instead of relying on a single network for both processes, DDQN uses two separate neural networks. The Q-Network selects the action by identifying the highest Q-value, while a secondary target network is used to evaluate the value of that action. This separation reduces the bias introduced during the maximization step, resulting in a more accurate Q-value estimation and, consequently, a more stable and reliable learning policy. Other possible RL algorithms designed to optimize decision-making in dynamic environments are Proximal Policy Optimization (PPO) [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 73" title="Schulman J, Wolski F, Dhariwal P, Radford A, Klimov O (2017) Proximal policy optimization algorithms. &#xA;                  arXiv:1707.06347&#xA;                  &#xA;                " href="/article/10.1007/s10489-025-06516-z#ref-CR73" id="ref-link-section-d305401337e1970">73</a>] and Advantage Actor-Critic (A2C) [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 74" title="Mnih V, Badia AP, Mirza M, Graves A, Lillicrap T, Harley T, Silver D, Kavukcuoglu K (2016) Asynchronous methods for deep reinforcement learning. In: Proceedings of the international conference on machine learning and computer application (ICML2016), New York, NY, USA. PmLR, pp 19281937" href="/article/10.1007/s10489-025-06516-z#ref-CR74" id="ref-link-section-d305401337e1973">74</a>], which refine policy gradient methods to improve stability and efficiency in learning complex strategies. These algorithms favor exploration over exploitation, making them well-suited for environments that require extensive strategic planning and adaptability. However, in structured tasks with clear reward pathways, their efficiency may be suboptimal due to longer training times and the necessity to fine-tune hyperparameters to balance performance and stability. In particular, A2C is highly sensitive to the choice of learning rate, as small variations in it can lead to either slow convergence or instability, making it more challenging to tune effectively [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 75" title="De La Fuente N, Guerra DAV (2024) A comparative study of deep reinforcement learning models: DQN vs PPO vs A2C . In: Proceedings of the ACM knowledge discovery and data mining (ACM KDD24), Barcelona, Spain" href="/article/10.1007/s10489-025-06516-z#ref-CR75" id="ref-link-section-d305401337e1976">75</a>].</p><p>In our case, we chose DDQN over PPO and A2C because of its superior learning efficiency and adaptability. DDQN effectively mitigates overestimation bias by using a separate target network, resulting in more stable learning dynamics and improved convergence rates. In addition, its experience replay mechanism allows the agent to better generalize by learning from a diverse set of past experiences, reducing overfitting to specific trajectories. Unlike PPO and A2C, which often require extensive fine-tuning to achieve optimal results, DDQN proves resilient to changes in hyperparameters, making it a more practical and efficient choice. In structured environments like our scenario, where immediate rewards and well-defined strategies play a key role, DDQNs ability to quickly assimilate optimal policies while maintaining computational efficiency makes it the preferred approach. In Section <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10489-025-06516-z#Sec16">4.4.1</a>, we compare the performance of DDQN and PPO when applied to AgentViT.</p><p>Figure <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10489-025-06516-z#Fig1">1</a> details the workflow that describes the behavior of AgentViT.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-1" data-title="Fig. 1"><figure><figcaption><b id="Fig1" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 1</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10489-025-06516-z/figures/1" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10489-025-06516-z/MediaObjects/10489_2025_6516_Fig1_HTML.png?as=webp"><img aria-describedby="Fig1" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10489-025-06516-z/MediaObjects/10489_2025_6516_Fig1_HTML.png" alt="figure 1" loading="lazy" width="685" height="593"></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-1-desc"><p>AgentViT workflow. AgentViT consists of a ViT and a DDQN-based agent. <i>(i)</i> The image is processed by the first ViT layer, where attention values are extracted and an average attention value per patch is computed to form the state representation. <i>(ii)</i> The agent computes Q-values for each patch and only patches with Q-values greater than the mean are selected. <i>(iii)</i> The selected patches are propagated through the remaining ViT layers. <i>(iv)</i> The agent receives a reward based on the number of patches selected and the training classification loss, thus refining its policy during training iterations</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10489-025-06516-z/figures/1" data-track-dest="link:Figure1 Full size image" aria-label="Full size image figure 1" rel="nofollow"><span>Full size image</span><svg width="16" height="16" focusable="false" role="img" aria-hidden="true" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#icon-eds-i-chevron-right-small"></use></svg></a></div></figure></div><p>As shown in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10489-025-06516-z#Fig1">1</a>, there are two basic components in AgentViT, namely a ViT and the agent, which consists of a pair of networks implementing the DDQN algorithm. Specifically:</p><ul class="u-list-style-bullet">
                  <li>
                    <p>In <i>(i)</i>, the image flows through the first ViT layer, where we extract the output of the attention layer and compute a vector in which each element represents the average attention value of a patch. This vector corresponds to a state in the agent environment.</p>
                  </li>
                  <li>
                    <p>In <i>(ii)</i>, the agent receives the state representation <i>s</i>, consisting of the attention values of the <i>N</i> patches, and computes the corresponding Q-values <span class="mathjax-tex">\(Q(s,a_i)\)</span> for each of the <i>N</i> possible actions <span class="mathjax-tex">\(a_i\)</span>, where each action corresponds to the selection of a specific patch. Rather than selecting a single action per step and iteratively updating the state, we adopt a batch action selection approach to ensure greater efficiency. This approach is described in detail in the next sections.</p>
                  </li>
                  <li>
                    <p>In <i>(iii)</i>, the selected patches flow from the second to the last layer of the ViT.</p>
                  </li>
                  <li>
                    <p>In <i>(iv)</i>, the reward of the agent after the classification step is calculated as a combination of the number of selected patches and the training loss. The Deep Q-Network uses this reward to tune its behavior for the next step.</p>
                  </li>
                </ul><p>This process is repeated for each image in the training set and for each epoch defined by the user. It is also important to note that the agent does not iteratively update the state-action space after each selection of patches but instead determines the set of retained patches in a single step. Specifically, the Q-network evaluates the importance of each patch and selects those with Q-values exceeding the mean Q-value across all patches. This approach significantly reduces the computational overhead while maintaining flexibility in patch selection.</p><p>The main strength of RL is its ability to balance short-term and long-term objectives by optimizing a cumulative reward function. In the short term, the agent seeks to reduce the number of patches processed per training iteration, which leads to an immediate reduction in computational complexity. However, pruning can degrade the performance of the model by discarding informative patches. The RL agent mitigates this risk by continuously updating its value function, which estimates the expected benefit of retaining or discarding specific patches based on observed outcomes. Through iterative exploration and exploitation, the agent refines its patch selection strategy to converge on an optimal tradeoff between efficiency and accuracy.</p><p>The RL-based patch selection method can theoretically be viewed as an optimal sequential decision-making process where the agent dynamically adjusts its policy to maximize the cumulative reward over time. Unlike static or heuristic approaches that rely on predefined rules, the RL agent models patch selection as a stochastic optimization problem, in which the state of the system evolves based on the decisions made at each step. The theoretical advantage of this approach lies in its ability to learn an adaptive policy that optimally selects patches based on their contribution to model accuracy while minimizing computational overhead. In addition, the RL agents reward function acts as a self-regulating mechanism that ensures generalization across different datasets and ViT architectures. By incorporating both accuracy-driven and efficiency-driven reward terms, the agent autonomously discovers the best patch selection policy, avoiding both over-pruning (which would reduce accuracy) and under-pruning (which would lead to computational waste). The use of replay memory and structured exploration strategies ensures that the agent does not converge to suboptimal local minima but instead refines its policy over successive training iterations. In the next subsections, we will explain in detail the components of the algorithm underlying AgentViT.</p><h3 class="c-article__sub-heading" id="Sec7"><span class="c-article-section__title-number">3.2 </span>State</h3><p>In the context of the Markov Decision Process, on which the RL mechanism underlying AgentViT is based, a state <span class="mathjax-tex">\(s \in \mathcal {S}\)</span> observed by the agent is represented by a vector of <i>N</i> real values, which models the current conditions of the environment. In AgentViT, the state of the environment is represented by attention values obtained from an image processed by the first attention layer of the ViT. Specifically, each element represents the average attention score assigned to the corresponding image patch by the first attention layer of the ViT. Formally speaking, given an input image decomposed into <i>N</i> patches, the output of the first attention layer generates an attention matrix of size <span class="mathjax-tex">\(N \times d\)</span>, where <i>d</i> is the embedding size. We then aggregate these attention values over the <i>d</i> dimensions to obtain a state representation. Formally, this can be represented as:</p><div id="Equ3" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex">$$\begin{aligned} s = (s_1, s_2, \dots , s_N), \ \ \ \text {where} \ s_i = \frac{1}{d}\sum _{j=1}^d{A_{ij}} \end{aligned}$$</span></div><div class="c-article-equation__number">
                    (3.3)
                </div></div><p>In this formula <span class="mathjax-tex">\(A_{ij}\)</span> represents the attention value assigned to patch <i>i</i> in the <i>j</i>-th dimension. Essentially, this vector represents the average attention value (and therefore importance) given to the patches by the attention layer, as well as the state given as input to the Deep Q-Network of AgentViT.</p><h3 class="c-article__sub-heading" id="Sec8"><span class="c-article-section__title-number">3.3 </span>Action</h3><p>The action space <span class="mathjax-tex">\(\mathcal{A}\)</span> of AgentViT consists of <i>N</i> discrete actions, each corresponding to the selection of a specific patch. Formally, an action <span class="mathjax-tex">\(a_i \in \mathcal{A}\)</span> represents the decision to retain the <i>i</i>-patch for further processing. Unlike standard sequential decision-making frameworks that iteratively update the state with each action[<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 76" title="Sutton RS, Barto AG (2018) Reinforcement learning: an introduction. MIT Press" href="/article/10.1007/s10489-025-06516-z#ref-CR76" id="ref-link-section-d305401337e2439">76</a>], we decide to optimize efficiency by selecting a batch of actions in a single step. In the following, we first illustrate the general idea behind this process and then explain each step in detail.</p><p>The first step of the patch selection mechanism regards the Q-network. It starts by computing the Q-values for all possible actions <span class="mathjax-tex">\(a_i\)</span>, where <span class="mathjax-tex">\(i=1,\dots ,N\)</span>. Then, the patches corresponding to actions with a Q-value greater than the mean Q-value across all actions are selected. Formally, this can be expressed as:</p><div id="Equ4" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex">$$\begin{aligned} \mathcal{A}' = \{a_i \in \mathcal{A} \ | \ Q(s,a_i) &gt; \frac{1}{N}\sum _{j=1}^{N} Q(s,a_j)\} \end{aligned}$$</span></div><div class="c-article-equation__number">
                    (3.4)
                </div></div><p>Indeed, selecting only one patch per image is not sufficient to effectively train a ViT, as it happens in the classic Q-Learning algorithm[<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 76" title="Sutton RS, Barto AG (2018) Reinforcement learning: an introduction. MIT Press" href="/article/10.1007/s10489-025-06516-z#ref-CR76" id="ref-link-section-d305401337e2644">76</a>]. Therefore, the ViT is trained using only the selected patches, thus effectively reducing computational complexity while retaining the most relevant information. Updates to the Q-network are then made using single state-action pairs (<i>s</i>,<i>a</i>), ensuring compliance with the basic principles of Q-Learning, thus preserving its theoretical properties[<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 72" title="Fan J, Wang Z, Xie Y, Yang Z (2020) A theoretical analysis of deep Q-learning. In: Proceedings of the international conference on learning for dynamics and control (L4DC20), Berkeley, CA, USA. PMLR, pp 486489" href="/article/10.1007/s10489-025-06516-z#ref-CR72" id="ref-link-section-d305401337e2654">72</a>]. In essence, this process implies that the action for a given state consists of the selection of the most promising patches.</p><p>The transition kernel function <span class="mathjax-tex">\(\mathcal {P}\)</span> (see Section <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10489-025-06516-z#Sec6">3.1</a>), which maps a given state-action pair to a new state, follows the classical framework of Deep Q-Learning. Specifically, after the agent has chosen an action <span class="mathjax-tex">\(a \in \mathcal {A}\)</span> and received a reward <span class="mathjax-tex">\(\mathcal {R}_{s,a}\)</span>, the Q-value associated with the state <i>s</i> and the action <i>a</i> is updated by means of the following formula [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 70" title="Arulkumaran K, Deisenroth MP, Brundage M, Bharath AA (2017) Deep reinforcement learning: a brief survey. IEEE Signal Process Mag 34(6):2638" href="/article/10.1007/s10489-025-06516-z#ref-CR70" id="ref-link-section-d305401337e2743">70</a>]:</p><div id="Equ5" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex">$$\begin{aligned} Q(s, a) \leftarrow Q(s, a) + \eta \cdot \left[ \mathcal{L}_H( \mathcal {R}_{s,a} + \gamma \cdot \max _{a' \in \mathcal{A}}(Q(s^*, a')), Q(s, a)) \right] \end{aligned}$$</span></div><div class="c-article-equation__number">
                    (3.5)
                </div></div><p>Similar to (<a data-track="click" data-track-label="link" data-track-action="equation anchor" href="/article/10.1007/s10489-025-06516-z#Equ2">3.2</a>), this formula updates the Q-value for the pair (<i>s</i>,<i>a</i>) by taking into account the previous Q-value and the difference between the maximum cumulative reward for the next state <span class="mathjax-tex">\(s^*\)</span> and the current Q-value. Instead of using a simple algebraic difference, we use the Huber loss function <span class="mathjax-tex">\(\mathcal{L}_H\)</span> [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 77" title="Huber PJ (1992) Robust estimation of a location parameter. In: Breakthroughs in statistics: methodology and distribution. Springer, pp 492518" href="/article/10.1007/s10489-025-06516-z#ref-CR77" id="ref-link-section-d305401337e2995">77</a>], chosen for its robustness to outliers and its ability to prevent gradient explosion in some cases. The cumulative reward for the next state is predicted by the Target Network, following the DDQN algorithm [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 23" title="Hasselt HV, Guez A, Silver D (2016) Deep reinforcement learning with double q-learning. In: Proceedings of the AAAI conference on artificial intelligence (AAAI16), vol. 30. Phoenix, AZ, USA" href="/article/10.1007/s10489-025-06516-z#ref-CR23" id="ref-link-section-d305401337e2998">23</a>]. The Target Network is an exact replica of the Q-Network, but its weights are not updated by backpropagation; instead, they are updated periodically using a soft-copy mechanism. As discussed in Section <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10489-025-06516-z#Sec6">3.1</a>, this approach stabilizes the learning process.</p><p>In addition, AgentViT is provided with a mechanism to avoid falling into a local optimum. In fact, with a probability equal to <span class="mathjax-tex">\(\epsilon \)</span>, the agent, instead of choosing the action that maximizes the value of <i>Q</i>, chooses a random action. To avoid excessive instability, the value of <span class="mathjax-tex">\(\epsilon \)</span> decays exponentially as training proceeds. This mechanism ensures a good exploratory capability in the early stages of training, while also maintaining stable AgentViT performance as the ViT progresses through its training stages.</p><p>Finally, to improve the generalizability of AgentViT, we leverage a replay memory [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 78" title="Liu R, Zou J (2018) The effects of memory replay in reinforcement learning. In: Proceedings of the annual allerton conference on communication, control, and computing (Allerton18), Monticello, IL, USA. IEEE, pp 478485" href="/article/10.1007/s10489-025-06516-z#ref-CR78" id="ref-link-section-d305401337e3048">78</a>, <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 79" title="Lin LJ (1992) Self-improving reactive agents based on reinforcement learning, planning and teaching. Mach Learn 8:293321" href="/article/10.1007/s10489-025-06516-z#ref-CR79" id="ref-link-section-d305401337e3051">79</a>]. It stores observed data, allowing the agent to reuse past experience during training. In this way, we help break unwanted temporal correlations in the data, ensuring that the model learns from a more diverse and representative set of experiences, which ultimately leads to more robust and stable learning outcomes.</p><h3 class="c-article__sub-heading" id="Sec9"><span class="c-article-section__title-number">3.4 </span>Reward</h3><p>Since the ultimate goal of AgentViT is to improve the training of the ViT, the reward <span class="mathjax-tex">\(\mathcal{R}_t\)</span> obtained at iteration <i>t</i> starting from a state <span class="mathjax-tex">\(s_t\)</span> and performing an action <span class="mathjax-tex">\(a_t\)</span> must consider the goodness of training. We mentioned above that this goodness must take into account both the time necessary for training and the corresponding accuracy. Therefore <span class="mathjax-tex">\(\mathcal{R}_t\)</span> must consider both training loss and training time. However, training time is affected by several factors, including the number of operations performed, server load, and other uncontrollable variables related to the machine running the ViT. Therefore, we decided to use the number of patches selected by the agent in the reward calculation, as this serves as a simple proxy for estimating the number of operations performed by the ViT. Thus, the reward <span class="mathjax-tex">\(\mathcal{R}_t\)</span> is a weighted mean of the training loss and the number of patches selected by the agent. It is worth noting that we deliberately chose not to include validation loss or test loss in the reward function, as this would have led to overfitting and compromised generalization. In fact, the RL agent only controls patch selection on training images, as its goal is to support ViT training. It has no direct influence on the validation or test images. The inclusion of validation loss or test loss could negatively affect the learning of the RL agent by introducing a non-stationary reward function, since their fluctuations over time are independent of the actions of the RL agent. This would result in an inconsistent expected reward distribution for each action, which would ultimately destabilize the learning process.</p><p>Therefore, our reward function <span class="mathjax-tex">\(\mathcal{R}_t\)</span> is formally defined as:</p><div id="Equ6" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex">$$\begin{aligned} \mathcal{R}_t = \alpha \cdot \mathcal{R}_t^{loss} + (1 - \alpha ) \cdot \mathcal{R}_t^{patch} \end{aligned}$$</span></div><div class="c-article-equation__number">
                    (3.6)
                </div></div><p>where:</p><ul class="u-list-style-bullet">
                  <li>
                    <p><span class="mathjax-tex">\(\mathcal{R}_t^{loss}\)</span> is the reward related to the training loss;</p>
                  </li>
                  <li>
                    <p><span class="mathjax-tex">\(\mathcal{R}_t^{patch}\)</span> is the reward related to the number of patches;</p>
                  </li>
                  <li>
                    <p><span class="mathjax-tex">\(\alpha \)</span> is a value belonging to the real interval [0,1], used to determine the weight to assign to <span class="mathjax-tex">\(\mathcal{R}_t^{loss}\)</span> with respect to <span class="mathjax-tex">\(\mathcal{R}_t^{patch}\)</span>.</p>
                  </li>
                </ul><p><span class="mathjax-tex">\(\mathcal{R}_t^{loss}\)</span> is obtained by computing the ratio between the value <span class="mathjax-tex">\(\mathcal{L}(0)\)</span> of the loss function at the starting iteration and the value <span class="mathjax-tex">\(\mathcal{L}(t)\)</span> of the same function at iteration <i>t</i>. Formally speaking:</p><div id="Equ7" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex">$$\begin{aligned} \mathcal{R}_t^{loss} = \frac{\mathcal{L}(0)}{\mathcal{L}(t)} \end{aligned}$$</span></div><div class="c-article-equation__number">
                    (3.7)
                </div></div><p><span class="mathjax-tex">\(\mathcal{R}_t^{patch}\)</span> can be defined by means of a fraction; the numerator represents the difference between the number of patches selected by the agent and the number of selected patches desired by the user; the denominator indicates the number of selected patches desired by the user. Formally speaking:</p><div id="Equ8" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex">$$\begin{aligned} \mathcal{R}_t^{patch} = \frac{|m - \mu |}{\mu } \end{aligned}$$</span></div><div class="c-article-equation__number">
                    (3.8)
                </div></div><p>Here:</p><ul class="u-list-style-bullet">
                  <li>
                    <p><i>m</i> is a positive integer indicating the number of patches selected by the agent at iteration <i>t</i>;</p>
                  </li>
                  <li>
                    <p><span class="mathjax-tex">\(\mu \)</span> is a positive integer denoting the number of selected patches desired by the user; if she does not specify any value, <span class="mathjax-tex">\(\mu \)</span> is set to 1 by default.</p>
                  </li>
                </ul><p>The agent is incentivized to select a number of patches close to the value specified by the user, or a very small number if she has not specified any value. It is also incentivized to choose a subset of the patches such that the training loss is minimized. <span class="mathjax-tex">\(\mathcal{R}_t^{loss}\)</span> and <span class="mathjax-tex">\(\mathcal{R}_t^{patch}\)</span> are weighted so that the user can specify how much importance to give to each of them. Finally, the weighting parameter <span class="mathjax-tex">\(\alpha \)</span> is essential in AgentViT because it controls the tradeoff between accuracy and computational efficiency. A well-chosen value of <span class="mathjax-tex">\(\alpha \)</span> ensures effective patch selection, while an improper value may lead to suboptimal learning, either by giving too much priority to efficiency or by retaining too many patches, and thus negatively affecting efficiency. When <span class="mathjax-tex">\(\alpha \)</span> is close to 1, the reward function favors minimizing training loss, which leads the agent to retain more patches. This ensures a richer feature extraction, but increases the computational cost. If <span class="mathjax-tex">\(\alpha \)</span> is too high, patch reduction may be ineffective, which would make AgentViT as costly as a standard ViT. Conversely, when <span class="mathjax-tex">\(\alpha \)</span> is close to 0, the agent aggressively prunes patches to improve efficiency. This reduces the computational overhead but risks discarding important information, potentially degrading classification accuracy and generalization across datasets.</p><p>The choice of <span class="mathjax-tex">\(\alpha \)</span> can be seen as a multi-objective optimization problem, where two conflicting objectives must be balanced, namely computational efficiency and classification accuracy. These two objectives are inherently conflicting because reducing computational cost typically requires selecting fewer patches, which can lead to a loss of accuracy, while preserving accuracy often requires processing more patches, which leads to increased computational complexity. This tradeoff can be visualized as a Pareto front. In this context, one axis of the Pareto front corresponds to computational efficiency, while the other axis represents classification accuracy. The choice of <span class="mathjax-tex">\(\alpha \)</span> determines where AgentViT operates along this Pareto front. The optimal <span class="mathjax-tex">\(\alpha \)</span> lies at a point on the Pareto front where both objectives are balanced, maximizing the overall performance of the model.</p><p>A further consideration regarding reward concerns the frequency <i>f</i> with which it is assigned to the agent. In fact, <i>f</i> indicates the number of iterations that must elapse before the reward is assigned to the agent. Clearly, the value of <i>f</i> is greater than or equal to 1. The frequency <i>f</i> can affect the performance of AgentViT. If <span class="mathjax-tex">\(f = 1\)</span> the agent receives the reward after every action, and thus it is continuously updated on the consequences of its choices. This helps to improve accuracy but increases the computational load of AgentViT, and consequently the training time. Conversely, a high value of <i>f</i> reduces the training time but may affect accuracy negatively. Clearly, a tradeoff between these two requirements is necessary, as we will see in Section <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10489-025-06516-z#Sec14">4.3</a>.</p><p>Finally, we emphasize that the overall framework is designed to support the integration of additional terms into the reward function. Thus, if another task beyond classification needs to be addressed, requiring a more sophisticated solution, a user can directly modify the reward function by adding the necessary terms while leaving the rest of the framework unchanged. This flexibility allows users to adapt AgentViT to tasks beyond the scope of this paper while preserving its core structure.</p><h3 class="c-article__sub-heading" id="Sec10"><span class="c-article-section__title-number">3.5 </span>Considerations on convergence and optimality</h3><p>At this point, after illustrating how our approach works, some considerations about convergence and optimality are in order. First, consider that our approach is based on DDQN, which is a well-established RL method that mitigates Q-value overestimation by using a separate target network[<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 23" title="Hasselt HV, Guez A, Silver D (2016) Deep reinforcement learning with double q-learning. In: Proceedings of the AAAI conference on artificial intelligence (AAAI16), vol. 30. Phoenix, AZ, USA" href="/article/10.1007/s10489-025-06516-z#ref-CR23" id="ref-link-section-d305401337e4025">23</a>]. The theoretical convergence properties of DDQN have been extensively studied in previous literature, and DDQN has been found to guarantee convergence under standard conditions, such as a sufficiently large replay buffer to ensure different training samples, and a learning rate that ensures stability during Q-network updates. While a formal proof of convergence is beyond the scope of this paper, we rely on the fact that our approach is strictly based on DDQN, and therefore the same properties of DDQN are guaranteed for it [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 23" title="Hasselt HV, Guez A, Silver D (2016) Deep reinforcement learning with double q-learning. In: Proceedings of the AAAI conference on artificial intelligence (AAAI16), vol. 30. Phoenix, AZ, USA" href="/article/10.1007/s10489-025-06516-z#ref-CR23" id="ref-link-section-d305401337e4028">23</a>]. Furthermore, as will become clear in the next section, our empirical results show stable training dynamics, as evidenced by the controlled decrease in loss and the convergence of the patch selection behavior over epochs.</p><p>As for the optimality of patch selection, we note that one of the objectives of AgentViT is to balance computation efficiency and classification accuracy, which inherently creates a multi-objective optimization problem. In fact, rather than searching for a globally optimal solution, our approach tries to learn a policy that dynamically adjusts the number of patches based on training loss and computational cost constraints. Defining theoretical guarantees of global optimality in RL is not a straightforward process, especially when no strong assumptions are made about the environment. However, we can identify some properties related to the selection mechanisms we have implemented in AgentViT. First, we believe that it ensures adaptivity by dynamically selecting patches with Q-values greater than a threshold equal to the mean Q-value. In addition, it follows standard Q-learning guarantees by updating the Q-network on individual state-action pairs, thus ensuring a stable policy improvement process[<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 23" title="Hasselt HV, Guez A, Silver D (2016) Deep reinforcement learning with double q-learning. In: Proceedings of the AAAI conference on artificial intelligence (AAAI16), vol. 30. Phoenix, AZ, USA" href="/article/10.1007/s10489-025-06516-z#ref-CR23" id="ref-link-section-d305401337e4034">23</a>, <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 71" title="Watkins CJCH, Dayan P (1992) Q-learning. Mach Learn 8:279292" href="/article/10.1007/s10489-025-06516-z#ref-CR71" id="ref-link-section-d305401337e4037">71</a>, <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 80" title="Even-Dar E, Mansour Y, Bartlett P (2003) Learning rates for Q-learning. J Mach Learn Res 5(1). MIT Press" href="/article/10.1007/s10489-025-06516-z#ref-CR80" id="ref-link-section-d305401337e4040">80</a>]. Moreover, there are similar scenarios, such as the online influence maximization problem [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 81" title="Guo J (2023) Online influence maximization: concept and algorithm. &#xA;                  arXiv:2312.00099&#xA;                  &#xA;                " href="/article/10.1007/s10489-025-06516-z#ref-CR81" id="ref-link-section-d305401337e4043">81</a>], where similar approaches are used, e.g., combinatorial multi-armed bandit[<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 82" title="Chen W, Wang Y, Yuan Y (2013) Combinatorial multi-armed bandit: general framework and applications. In: Proceedings of the international conference on machine learning (ICML 2013). PMLR, pp 151159" href="/article/10.1007/s10489-025-06516-z#ref-CR82" id="ref-link-section-d305401337e4046">82</a>, <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 83" title="Chen W, Hu W, Li F, Li J, Liu Y, Lu P (2016) Combinatorial multi-armed bandit with general reward functions. Adv Neural Inf Process Syst 29. NeurIPS" href="/article/10.1007/s10489-025-06516-z#ref-CR83" id="ref-link-section-d305401337e4050">83</a>]. Nevertheless, theoretical guarantees of optimality, especially in deep RL contexts, remain an open challenge in general. However, our empirical results strongly suggest that our approach learns an efficient patch selection strategy.</p></div></div></section><section data-title="Experiments"><div class="c-article-section" id="Sec11-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec11"><span class="c-article-section__title-number">4 </span>Experiments</h2><div class="c-article-section__content" id="Sec11-content"><p>In this section, we present the experiments conducted to evaluate the performance of AgentViT and compare it to existing approaches in the literature. The section is organized as follows: Subsection <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10489-025-06516-z#Sec12">4.1</a> describes the experimental setup, including models, hyperparameters, datasets, and comparison methods. Subsection <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10489-025-06516-z#Sec13">4.2</a> presents a hyperparameter analysis. Subsection <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10489-025-06516-z#Sec14">4.3</a> compares our approach with existing methods, considering both computational efficiency and accuracy. Subsection <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10489-025-06516-z#Sec15">4.4</a> presents an ablation study, where we assess different RL frameworks and analyze the impact of Replay Memory and Target Network on AgentViTs performance. Finally, Subsection <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10489-025-06516-z#Sec18">4.5</a> presents qualitative results obtained by AgentViT.</p><h3 class="c-article__sub-heading" id="Sec12"><span class="c-article-section__title-number">4.1 </span>Experimental setup</h3><p>AgentViT can be applied to any ViT model because it relies on attention scores. Therefore, in our experiments, we could have chosen any ViT available in the literature. To thoroughly evaluate our approach, we used both the classical ViT [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 4" title="Dosovitskiy A, Beyer L, Kolesnikov A, Weissenborn D, Zhai X, Unterthiner T, Dehghani M, Minderer M, Heigold G, Gelly S, Uszkoreit J, Houlsby N (2021) An image is worth 16x16 words: transformers for image recognition at scale. In: Proceedings of the international conference on learning representations (ICLR21), Virtual event. OpenReview.net" href="/article/10.1007/s10489-025-06516-z#ref-CR4" id="ref-link-section-d305401337e4084">4</a>] and SimpleViT [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 84" title="Beyer L, Zhai X, Kolesnikov A (2022) Better plain ViT baselines for ImageNet-1k. &#xA;                  arXiv:2205.01580&#xA;                  &#xA;                " href="/article/10.1007/s10489-025-06516-z#ref-CR84" id="ref-link-section-d305401337e4087">84</a>]. The latter does not include a CLS token, which allows us to demonstrate the versatility of our method. In the following experiments, we will refer to AgentViT as our RL-enhanced solution based on classical ViT, while will refer to AgentSimpleViT as our approach applied to SimpleViT. We chose the hyperparameters of the ViT models and the approaches into evaluation to ensure a lightweight model while maintaining good performance. This is in line with the objective of our work, which is to propose an approach with a low training time while achieving performance comparable to the state-of-the-art.</p><p>Specifically, the hyperparameters of ViT and SimpleViT are the following: <i>(i)</i> <span class="u-monospace">patch_size</span>: the pixel size of each non-overlapping patch; <i>(ii)</i> <span class="u-monospace">dim</span>: the embedding size of each patch; <i>(iii)</i> <span class="u-monospace">depth</span>: the number of ViT layers; <i>(iv)</i> <span class="u-monospace">heads</span>: the number of multi-attention heads; <i>(v)</i> <span class="u-monospace">mlp_dim</span>: the dimensionality of every feedforward block; <i>(vi)</i> <span class="u-monospace">dropout</span>: the percentage of activations to randomly set to zero; <i>(vii)</i> <span class="u-monospace">emb_dropout</span>: the percentage of embeddings to randomly set to zero. Their values are shown in Table <a data-track="click" data-track-label="link" data-track-action="table anchor" href="/article/10.1007/s10489-025-06516-z#Tab1">1</a>.</p><div class="c-article-table" data-test="inline-table" data-container-section="table" id="table-1"><figure><figcaption class="c-article-table__figcaption"><b id="Tab1" data-test="table-caption">Table 1 Values of the hyperparameters of ViT and SimpleViT</b></figcaption><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="table-link" data-track="click" data-track-action="view table" data-track-label="button" rel="nofollow" href="/article/10.1007/s10489-025-06516-z/tables/1" aria-label="Full size table 1"><span>Full size table</span><svg width="16" height="16" focusable="false" role="img" aria-hidden="true" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#icon-eds-i-chevron-right-small"></use></svg></a></div></figure></div><div class="c-article-table" data-test="inline-table" data-container-section="table" id="table-2"><figure><figcaption class="c-article-table__figcaption"><b id="Tab2" data-test="table-caption">Table 2 Values of the hyperparameters of AgentViT and AgentSimpleViT</b></figcaption><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="table-link" data-track="click" data-track-action="view table" data-track-label="button" rel="nofollow" href="/article/10.1007/s10489-025-06516-z/tables/2" aria-label="Full size table 2"><span>Full size table</span><svg width="16" height="16" focusable="false" role="img" aria-hidden="true" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#icon-eds-i-chevron-right-small"></use></svg></a></div></figure></div><p>Instead, the hyperparameters of AgentViT and AgentSimpleViT are the followings: <i>(i)</i> <span class="u-monospace">buffer_size</span>: the total capacity of the replay memory; <i>(ii)</i> <span class="u-monospace">buffer_batch_size</span>: the number of elements randomly sampled from the replay memory; <i>(iii)</i> <span class="u-monospace">gamma</span>: the discount factor that controls the weight of future rewards; <i>(iv)</i> <span class="u-monospace">eps_start</span>, <span class="u-monospace">eps_end</span>, and <span class="u-monospace">eps_decay</span>: they manage the tradeoff between exploration and exploitation; in particular, they define the initial exploration rate, the final exploration rate, and the decay rate, respectively; <i>(v)</i> <span class="u-monospace">eta</span>: the learning rate of the Q-Network; <i>(vi)</i> <span class="u-monospace">tau</span>: the soft update rate of the Target Network; <i>(vii)</i> <span class="u-monospace">update_every</span>: how often the Target Network updates its weights; <i>(viii)</i> <span class="u-monospace">frequency</span> (<i>f</i>): how often (i.e. every how many iterations) the model receives the reward; <i>(ix)</i> <span class="u-monospace">n_patches_desired</span>: the desired number of patches the agent should select; <i>(x)</i> <span class="u-monospace">alpha</span> (<span class="mathjax-tex">\(\alpha \)</span>): the contribution of the training loss to the reward calculation with respect to the number of selected patches. In Table <a data-track="click" data-track-label="link" data-track-action="table anchor" href="/article/10.1007/s10489-025-06516-z#Tab2">2</a>, we report the values of the previous hyperparameters, except for <span class="u-monospace">buffer_size</span>, <i>f</i> and <span class="mathjax-tex">\(\alpha \)</span>. In fact, in order to keep the level of complexity of the experiments and the space devoted to them reasonable, we could not tune all the hyperparameters; therefore, we performed this task on the three that we considered to have the most impact on the behavior of AgentViT and AgentSimpleViT.</p><p>As datasets used to evaluate our approach across different domains and levels of complexity, we used CIFAR10, FashionMNIST, and an extended version of Imagenette. CIFAR10 [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 24" title="Krizhevsky A, Nair V, Hinton G (2010) CIFAR-10 (Canadian Institute for Advanced Research). &#xA;                  https://www.cs.toronto.edu/~kriz/cifar.html&#xA;                  &#xA;                " href="/article/10.1007/s10489-025-06516-z#ref-CR24" id="ref-link-section-d305401337e4923">24</a>] consists of 60,000 RGB images of size <span class="mathjax-tex">\(32 \times 32\)</span> pixels, categorized into 10 classes. It is split into a training set of 50,000 images and a test set of 10,000 images, with each class containing the same number of training samples (specifically, 5,000 per class). FashionMNIST [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 25" title="Xiao H, Rasul K, Vollgraf R (2017) Fashion-MNIST: a novel image dataset for benchmarking machine learning algorithms. &#xA;                  arXiv:1708.07747&#xA;                  &#xA;                " href="/article/10.1007/s10489-025-06516-z#ref-CR25" id="ref-link-section-d305401337e4950">25</a>] (FMNIST) contains 70,000 grayscale images of size <span class="mathjax-tex">\(28 \times 28\)</span> pixels, representing 10 classes of clothing items, such as t-shirts, trousers, dresses, and sneakers. It represents a more challenging benchmark for image classification than the original MNIST dataset. Imagenette<sup><a href="#Fn1"><span class="u-visually-hidden">Footnote </span>1</a></sup> is a subset of ImageNet [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 26" title="Deng J, Dong W, Socher R, Li LJ, Kai L, Li FF (2009) ImageNet: a large-scale hierarchical image database. In: Proceedings of the International IEEE Conference on Computer Vision and Pattern Recognition (CVPR09), Miami, FL, USA. IEEE, pp 248255" href="/article/10.1007/s10489-025-06516-z#ref-CR26" id="ref-link-section-d305401337e4989">26</a>] designed to be a smaller and more manageable benchmark for deep learning models. It consists of 13,394 images of <span class="mathjax-tex">\(320 \times 320\)</span> pixels categorized into 10 classes. To improve this benchmark, we extended Imagenette by adding 86,606 additional images from the original ImageNet dataset. These images belong to the same classes as Imagenette and are distributed in a balanced way across them. We will call this extended version Imagenette<span class="mathjax-tex">\(^+\)</span>. It thus contains 100,000 images categorized in the 10 classes of the original Imagenette.</p><p>The approaches we chose to compare with AgentViT are ATS [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 8" title="Fayyaz M, Koohpayegani SA, Jafari FR, Sengupta S, Joze HRV, Sommerlade E, Pirsiavash H, Gall J (2022) Adaptive token sampling for efficient vision transformers. In: Proceedings of the european conference on computer vision (ECCV22), Tel Aviv, Israel. Springer, pp 396414" href="/article/10.1007/s10489-025-06516-z#ref-CR8" id="ref-link-section-d305401337e5044">8</a>], PatchMerger [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 10" title="Renggli C, Pinto AS, Houlsby N, Mustafa B, Puigcerver J, Riquelme C (2022) Learning to merge tokens in vision transformers. &#xA;                  arXiv:2202.12015&#xA;                  &#xA;                " href="/article/10.1007/s10489-025-06516-z#ref-CR10" id="ref-link-section-d305401337e5047">10</a>], EViT [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 11" title="Liang Y, Ge C, Tong Z, Song Y, Wang J, Xie P (2022) EViT: expediting vision transformers via token reorganizations. In: Proceedings of the international conference on learning representations (ICLR22), Virtual event. OpenReview.net" href="/article/10.1007/s10489-025-06516-z#ref-CR11" id="ref-link-section-d305401337e5050">11</a>], and the Vanilla case (i.e., the original ViT and SimpleViT as baselines). We chose these approaches because they represent some of the best-performing approaches in their respective categories [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 85" title="Haurum JB, Escalera S, Taylor GW, Moeslund TB (2023) Which tokens to use? investigating token reduction in vision transformers. In: Proceedings of the IEEE/CVF international conference on computer vision (ICCV23), Paris, France, pp 773783" href="/article/10.1007/s10489-025-06516-z#ref-CR85" id="ref-link-section-d305401337e5053">85</a>], i.e., dynamic pruning (ATS), token merging (PatchMerger), and a combination of token merging and pruning (EViT). However, since EViT and ATS rely on the classification token, they cannot be applied to SimpleViT, which lacks this token.</p><p>The hyperparameters of ATS, PatchMerger and EViT are the ones of ViT or SimpleViT, depending on the Vision Transformer model underlying them. In addition to these hyperparameters there is a further one, called <span class="u-monospace">keep_rate</span>, which represents the number of tokens to keep in each layer of the Vision Transformer. In our experiments, we set <span class="u-monospace">keep_rate</span> to the following lists of values: [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 9" title="Yin H, Vahdat A, Alvarez JM, Mallya A, Kautz J, Molchanov P (2022) A-vit: adaptive tokens for efficient vision transformer. In: Proceedings of the international conference on computer vision and pattern recognition (CVPR22), New Orleans, LA, USA, pp 1080910818" href="/article/10.1007/s10489-025-06516-z#ref-CR9" id="ref-link-section-d305401337e5065">9</a>, <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 9" title="Yin H, Vahdat A, Alvarez JM, Mallya A, Kautz J, Molchanov P (2022) A-vit: adaptive tokens for efficient vision transformer. In: Proceedings of the international conference on computer vision and pattern recognition (CVPR22), New Orleans, LA, USA, pp 1080910818" href="/article/10.1007/s10489-025-06516-z#ref-CR9" id="ref-link-section-d305401337e5068">9</a>, <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 9" title="Yin H, Vahdat A, Alvarez JM, Mallya A, Kautz J, Molchanov P (2022) A-vit: adaptive tokens for efficient vision transformer. In: Proceedings of the international conference on computer vision and pattern recognition (CVPR22), New Orleans, LA, USA, pp 1080910818" href="/article/10.1007/s10489-025-06516-z#ref-CR9" id="ref-link-section-d305401337e5071">9</a>, <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 17" title="Fang W, Pang L, Yi W (2020) Survey on the application of deep reinforcement learning in image processing. J Artif Intell 2(1):3958" href="/article/10.1007/s10489-025-06516-z#ref-CR17" id="ref-link-section-d305401337e5075">17</a>, <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 33" title="Alrebdi N, Alrumiah S, Almansour A, Rassam M (2022) Reinforcement learning in image classification: a review. In: Proceedings of the international conference on computing and information technology (ICCIT22), Tabuk, Saudi Arabia. IEEE, pp 7986" href="/article/10.1007/s10489-025-06516-z#ref-CR33" id="ref-link-section-d305401337e5078">33</a>, <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 66" title="Grainger R, Paniagua T, Song X, Cuntoor N, Lee MW, Wu T (2023) PaCa-ViT: learning patch-to-cluster attention in vision transformers. In: Proceedings of the IEEE/CVF conference on computer vision and pattern recognition (CVPR23), Vancouver, British Columbia, Canada, pp 1856818578" href="/article/10.1007/s10489-025-06516-z#ref-CR66" id="ref-link-section-d305401337e5081">66</a>] for CIFAR10 and FMNIST, and [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 26" title="Deng J, Dong W, Socher R, Li LJ, Kai L, Li FF (2009) ImageNet: a large-scale hierarchical image database. In: Proceedings of the International IEEE Conference on Computer Vision and Pattern Recognition (CVPR09), Miami, FL, USA. IEEE, pp 248255" href="/article/10.1007/s10489-025-06516-z#ref-CR26" id="ref-link-section-d305401337e5084">26</a>, <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 26" title="Deng J, Dong W, Socher R, Li LJ, Kai L, Li FF (2009) ImageNet: a large-scale hierarchical image database. In: Proceedings of the International IEEE Conference on Computer Vision and Pattern Recognition (CVPR09), Miami, FL, USA. IEEE, pp 248255" href="/article/10.1007/s10489-025-06516-z#ref-CR26" id="ref-link-section-d305401337e5087">26</a>, <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 26" title="Deng J, Dong W, Socher R, Li LJ, Kai L, Li FF (2009) ImageNet: a large-scale hierarchical image database. In: Proceedings of the International IEEE Conference on Computer Vision and Pattern Recognition (CVPR09), Miami, FL, USA. IEEE, pp 248255" href="/article/10.1007/s10489-025-06516-z#ref-CR26" id="ref-link-section-d305401337e5090">26</a>, <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 51" title="Zhu M, Tang Y, Han K (2021) Vision transformer pruning. &#xA;                  arXiv:2104.08500&#xA;                  &#xA;                " href="/article/10.1007/s10489-025-06516-z#ref-CR51" id="ref-link-section-d305401337e5094">51</a>] for Imagenette<span class="mathjax-tex">\(^+\)</span>.</p><div class="c-article-table" data-test="inline-table" data-container-section="table" id="table-3"><figure><figcaption class="c-article-table__figcaption"><b id="Tab3" data-test="table-caption">Table 3 Accuracy and Cumulative Training Time of AgentSimpleViT using CIFAR10 for seven different values of <span class="mathjax-tex">\(\alpha \)</span></b></figcaption><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="table-link" data-track="click" data-track-action="view table" data-track-label="button" rel="nofollow" href="/article/10.1007/s10489-025-06516-z/tables/3" aria-label="Full size table 3"><span>Full size table</span><svg width="16" height="16" focusable="false" role="img" aria-hidden="true" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#icon-eds-i-chevron-right-small"></use></svg></a></div></figure></div><p>The evaluation of AgentViT and the other approaches was done using metrics that consider both the classification performance of the trained model and the corresponding computational efficiency. For classification performance, we used the standard metrics of Accuracy, Precision, Recall, and F1-Score. For computational efficiency, we considered the following metrics:</p><ul class="u-list-style-bullet">
                  <li>
                    <p><i>Cumulative Training Time (CTT)</i>: It is the total time required to train the model. In the case of AgentViT, it includes the time needed to train both the ViT and the RL agent, since the latter is trained together with the ViT.</p>
                  </li>
                  <li>
                    <p><i>Memory</i>: It indicates the megabytes required to load the deep learning model, including any additional parameters required for token pruning. For AgentViT, it includes the model size, the memory used by the Deep-Q and Target neural networks, and the buffer size. For PatchMerger, it includes the model size and the additional parameter storage.</p>
                  </li>
                  <li>
                    <p><i>Frames per Second (FPS)</i>: It is the number of frames the approach can process per second. The higher the value, the faster the approach.</p>
                  </li>
                  <li>
                    <p><i>Giga Floating Point Operations (GFLOPs)</i>: It is the number of billions of floating point operations required by the approach to process an image. The higher the value, the more computationally intensive the approach.</p>
                  </li>
                </ul><p>We conducted our experiments on a server having an Intel(R) Xeon(R) W5-3435X 3.10 GHz CPU, 128 GB of RAM, and an NVIDIA RTX A2000 GPU with 12 GB of memory. The code used for the implementation of AgentViT and the one created to perform the experiments described in the next sections can be found on GitHub at the following link: <a href="https://github.com/DavideTraini/RL-for-ViT">https://github.com/DavideTraini/RL-for-ViT</a>.</p><h3 class="c-article__sub-heading" id="Sec13"><span class="c-article-section__title-number">4.2 </span>Hyperparameter analysis</h3><p>In this section, we describe the preliminary experiments conducted to evaluate the impact on AgentViTs performance of the three hyperparameters <span class="mathjax-tex">\(\alpha \)</span>, <i>f</i>, and <span class="u-monospace">buffer_size</span> that we had not set to fixed values in Table <a data-track="click" data-track-label="link" data-track-action="table anchor" href="/article/10.1007/s10489-025-06516-z#Tab2">2</a>. We show the results obtained using CIFAR10 and SimpleViT. Similar results were obtained for the other two datasets and for ViT. We trained AgentSimpleViT for 100 epochs; during each epoch, we simultaneously trained the underlying SimpleViT and the agent inside AgentSimpleViT.</p><p>The choice of not directly fixing the values of <span class="mathjax-tex">\(\alpha \)</span> and <i>f</i> is motivated by the fact that they are intrinsically linked with our approach and directly influence its performance (see Section <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10489-025-06516-z#Sec9">3.4</a>). Indeed, <span class="mathjax-tex">\(\alpha \)</span> and <i>f</i> play a crucial role in shaping the learning dynamics of our model, which makes their impact particularly relevant to our experimental campaign. The choice of not directly fixing the value of <span class="u-monospace">buffer_size</span> is related to the fact that, as evidenced by [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 86" title="Zhang S, Sutton RS (2017) A deeper look at experience replay. &#xA;                  arXiv:1712.01275&#xA;                  &#xA;                " href="/article/10.1007/s10489-025-06516-z#ref-CR86" id="ref-link-section-d305401337e5600">86</a>], the performance of DDQN is sensitive to this parameter. This makes the latter very influential on learning stability and efficiency; for this reason, we considered it essential to evaluate its effects within our framework.</p><p>The first parameter we considered was <span class="mathjax-tex">\(\alpha \)</span>; it is present in the reward computation (<a data-track="click" data-track-label="link" data-track-action="equation anchor" href="/article/10.1007/s10489-025-06516-z#Equ6">3.6</a>) and determines the weight of the reward associated with training loss against that associated with the number of patches selected. We performed a grid search on <span class="mathjax-tex">\(\alpha \)</span>; for each value of this parameter taken into consideration, we calculated the Accuracy and Cumulative Training Time of AgentSimpleViT. Table <a data-track="click" data-track-label="link" data-track-action="table anchor" href="/article/10.1007/s10489-025-06516-z#Tab3">3</a> shows the results obtained.</p><p>From the analysis of this table we can see that the values of Accuracy are very similar as <span class="mathjax-tex">\(\alpha \)</span> varies, while there are differences in the Cumulative Training Time. In particular, it varies from 3,947 seconds for <span class="mathjax-tex">\(\alpha =0.99\)</span> to 3,536 seconds for <span class="mathjax-tex">\(\alpha =0.001\)</span>, with a decrease of 10.41%. This result is a consequence of the reward mechanism underlying AgentSimpleViT, described in Section <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10489-025-06516-z#Sec9">3.4</a>. In fact, a lower value of <span class="mathjax-tex">\(\alpha \)</span> encourages the agent to select fewer patches. This leads to a reduction of the Cumulative Training Time, but also to an increase of the training loss and thus to a decrease of the Accuracy.</p><p>Another interesting aspect of rewards is the frequency with which the agent receives them. Indeed, this frequency could affect its learning process and, ultimately, the performance of the whole AgentSimpleViT (see Section <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10489-025-06516-z#Sec9">3.4</a>). To evaluate this aspect, we calculated the Accuracy and the Cumulative Training Time against epochs considering three different frequency values, namely 2, 10 and 100. The results are shown in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10489-025-06516-z#Fig2">2</a>. From the analysis of this figure, we can see that there are no significant differences between the three cases, neither in Accuracy nor in the Cumulative Training Time. The only notable observation is a slight reduction in Cumulative Training Time when the frequency is set to 10. This frequency value is the intermediate one among those we considered. This leads us to conclude that a frequency value of 10 is a good tradeoff between training time and speed of convergence to the optimal accuracy value.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-2" data-title="Fig. 2"><figure><figcaption><b id="Fig2" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 2</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10489-025-06516-z/figures/2" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10489-025-06516-z/MediaObjects/10489_2025_6516_Fig2_HTML.png?as=webp"><img aria-describedby="Fig2" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10489-025-06516-z/MediaObjects/10489_2025_6516_Fig2_HTML.png" alt="figure 2" loading="lazy" width="685" height="943"></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-2-desc"><p>Accuracy and Cumulative Training Time of AgentSimpleViT against epochs for three values of the reward frequency</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10489-025-06516-z/figures/2" data-track-dest="link:Figure2 Full size image" aria-label="Full size image figure 2" rel="nofollow"><span>Full size image</span><svg width="16" height="16" focusable="false" role="img" aria-hidden="true" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#icon-eds-i-chevron-right-small"></use></svg></a></div></figure></div><p>Finally, we tested the impact of <span class="u-monospace">buffer_size</span> related to replay memory on AgentSimpleViT. In fact, a larger buffer size requires more memory to train the agent, but might help its learning. Specifically, as shown in [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 78" title="Liu R, Zou J (2018) The effects of memory replay in reinforcement learning. In: Proceedings of the annual allerton conference on communication, control, and computing (Allerton18), Monticello, IL, USA. IEEE, pp 478485" href="/article/10.1007/s10489-025-06516-z#ref-CR78" id="ref-link-section-d305401337e5767">78</a>, <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 79" title="Lin LJ (1992) Self-improving reactive agents based on reinforcement learning, planning and teaching. Mach Learn 8:293321" href="/article/10.1007/s10489-025-06516-z#ref-CR79" id="ref-link-section-d305401337e5770">79</a>], the replay memory buffer allows the agent to learn from previous memories by speeding up training and breaking unwanted temporal correlations. However, too small or too large values of the buffer could lead to significant degradation of the agents learning.</p><p>As in the other cases, in this experiment we calculated the trend of Accuracy and Cumulative Training Time against epochs. In particular, we considered four values of the buffer size, namely 128, 256, 1,024 and 2,048 elements. The results obtained are shown in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10489-025-06516-z#Fig3">3</a>. From the analysis of this figure, we can see that the differences between the four cases are not significant. This is because the chosen values of buffer size are neither too high nor too low and therefore do not cause any performance degradation. The only notable difference in Fig.<a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10489-025-06516-z#Fig3">3</a> is the slightly lower Cumulative Training Time achieved with a buffer size of 1,024. This can be explained by the fact that this buffer size, intermediate between 128 and 256 on the one hand, and 2048 on the other hand, promotes the agents ability to find optimal patches faster, resulting in a lower Cumulative Training Time.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-3" data-title="Fig. 3"><figure><figcaption><b id="Fig3" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 3</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10489-025-06516-z/figures/3" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10489-025-06516-z/MediaObjects/10489_2025_6516_Fig3_HTML.png?as=webp"><img aria-describedby="Fig3" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10489-025-06516-z/MediaObjects/10489_2025_6516_Fig3_HTML.png" alt="figure 3" loading="lazy" width="685" height="943"></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-3-desc"><p>Accuracy and Cumulative Training Time of AgentSimpleViT against epochs for four replay memory buffer sizes</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10489-025-06516-z/figures/3" data-track-dest="link:Figure3 Full size image" aria-label="Full size image figure 3" rel="nofollow"><span>Full size image</span><svg width="16" height="16" focusable="false" role="img" aria-hidden="true" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#icon-eds-i-chevron-right-small"></use></svg></a></div></figure></div><p>Therefore, at the end of this hyperparameter analysis, we have that the optimal hyperparameter configuration for AgentViT is <span class="mathjax-tex">\(\alpha =0.1\)</span>, <span class="mathjax-tex">\(f=10\)</span>, and <span class="u-monospace">buffer_size</span> = 1,024. We will use this configuration in the next experiments for both AgentViT and AgentSimpleViT.</p><h3 class="c-article__sub-heading" id="Sec14"><span class="c-article-section__title-number">4.3 </span>Evaluation and comparative analysis</h3><p>As a first experiment, we tested AgentViT and the other related approaches selected in Section <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10489-025-06516-z#Sec12">4.1</a> on the CIFAR10 dataset. The corresponding results are reported in Table<a data-track="click" data-track-label="link" data-track-action="table anchor" href="/article/10.1007/s10489-025-06516-z#Tab4">4</a>.</p><div class="c-article-table" data-test="inline-table" data-container-section="table" id="table-4"><figure><figcaption class="c-article-table__figcaption"><b id="Tab4" data-test="table-caption">Table 4 Performance comparison of AgentViT with other related approaches using ViT and SimpleViT as underlying Vision Transformers, trained on CIFAR10</b></figcaption><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="table-link" data-track="click" data-track-action="view table" data-track-label="button" rel="nofollow" href="/article/10.1007/s10489-025-06516-z/tables/4" aria-label="Full size table 4"><span>Full size table</span><svg width="16" height="16" focusable="false" role="img" aria-hidden="true" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#icon-eds-i-chevron-right-small"></use></svg></a></div></figure></div><div class="c-article-table" data-test="inline-table" data-container-section="table" id="table-5"><figure><figcaption class="c-article-table__figcaption"><b id="Tab5" data-test="table-caption">Table 5 Performance comparison of AgentViT with other related approaches using ViT and SimpleViT as underlying Vision Transformers, trained on FMNIST</b></figcaption><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="table-link" data-track="click" data-track-action="view table" data-track-label="button" rel="nofollow" href="/article/10.1007/s10489-025-06516-z/tables/5" aria-label="Full size table 5"><span>Full size table</span><svg width="16" height="16" focusable="false" role="img" aria-hidden="true" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#icon-eds-i-chevron-right-small"></use></svg></a></div></figure></div><p>The results show the effectiveness of AgentViT and AgentSimpleViT compared to other pruning approaches. AgentViT achieves the highest Accuracy, Precision, Recall, and F1-Score outperforming ATS, PatchMerger, and EViT. Specifically, AgentViT improves Accuracy by 8.50% over ATS, 1.19% over PatchMerger and 0.73% over EViT, while reducing Computational Training Time by 30.91% over the Vanilla ViT, 0.34% over PatchMerger, and 2.91% over EViT. This indicates that AgentViT can effectively balance token selection while maintaining high classification performance. Similarly, AgentSimpleViT achieves the highest Accuracy, Precision, Recall, and F1-Score values among SimpleViT-based approaches, outperforming both the Vanilla model and PatchMerger. Specifically, it improves Accuracy by 2.08% over Vanilla SimpleViT and by 1.96% over PatchMerger, while reducing Computational Training Time by 21.42% compared to Vanilla SimpleViT and 3.23% over PatchMerger. This demonstrates that AgentSimpleViT not only enhances predictive performance but also reduces computational cost.</p><p>We then performed the same experiment on FMNIST, and the results are shown in Table <a data-track="click" data-track-label="link" data-track-action="table anchor" href="/article/10.1007/s10489-025-06516-z#Tab5">5</a>.</p><p>Similar to the CIFAR10 case, AgentViT outperforms all other ViT-based approaches, achieving the highest values of Accuracy, Precision, Recall, and F1-Score. Compared to ATS (resp., PatchMerger, EViT) AgentViT improves Accuracy by 2.25% (resp., 0.66%, 0.41%). It also significantly reduces Computational Training Time to 2,188 <i>s</i>, which is 13.8% less than Vanilla ViT, 11.49% less than ATS, 9.47% less than EViT, and 9.76% less than PatchMerger, demonstrating its efficiency in reducing computational overhead. Similarly, AgentSimpleViT achieves the highest Accuracy, Precision, Recall, and F1-Score among SimpleViT-based methods. Specifically, its Accuracy is 0.25% (resp., 0.66%) higher than Vanilla SimpleViT (resp., PatchMerger). It also reduces the Computational Training Time to 2,264 <i>s</i>, which is a reduction of 8.67% compared to Vanilla SimpleViT and 2.58% compared to PatchMerger.</p><p>To complete the performance evaluation, we tested our approach and the others seen above on Imagenette<span class="mathjax-tex">\(^+\)</span>, which is larger and more complex than the previous two datasets (see Section <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10489-025-06516-z#Sec12">4.1</a>). As a matter of fact, Imagenette<span class="mathjax-tex">\(^+\)</span> is more challenging for the approaches into evaluation, since both the number of its images and their resolution are much higher than the ones of CIFAR10 and FMNIST. The corresponding results are presented in Table<a data-track="click" data-track-label="link" data-track-action="table anchor" href="/article/10.1007/s10489-025-06516-z#Tab6">6</a>.</p><div class="c-article-table" data-test="inline-table" data-container-section="table" id="table-6"><figure><figcaption class="c-article-table__figcaption"><b id="Tab6" data-test="table-caption">Table 6 Performance comparison of AgentViT with other related approaches using ViT and SimpleViT as underlying Vision Transformers, trained on Imagenette<span class="mathjax-tex">\(^+\)</span></b></figcaption><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="table-link" data-track="click" data-track-action="view table" data-track-label="button" rel="nofollow" href="/article/10.1007/s10489-025-06516-z/tables/6" aria-label="Full size table 6"><span>Full size table</span><svg width="16" height="16" focusable="false" role="img" aria-hidden="true" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#icon-eds-i-chevron-right-small"></use></svg></a></div></figure></div><p>Once again, the results demonstrate the effectiveness of AgentViT and AgentSimpleViT. In particular, AgentViT achieves the highest Accuracy, Precision, Recall, and F1-Score among all ViT-based approaches. Compared to ATS (resp., PatchMerger, EViT) AgentViT improves Accuracy by 2.83% (resp., 0.63%, 0.27%). It also significantly reduces the Computational Training Time to 24,747 <i>s</i>, which is 15.79% less than Vanilla ViT, 10.58% less than ATS, 4.72% less than PatchMerger, and 6.13% less than EViT. Similarly, AgentSimpleViT outperforms all other SimpleViT-based approaches, achieving the highest Accuracy, Precision, Recall, and F1-Score. In particular, it improves Accuracy over Vanilla SimpleViT by 0.22% and over PatchMerger by 0.36%. Moreover, it achieves a Computational Training Time of 24,168 <i>s</i>, which represents a reduction of 16.09% compared to Vanilla SimpleViT and 4.49% compared to PatchMerger.</p><p>Finally, we assessed the computational load of AgentViT and the other approaches considered. This evaluation is crucial because using an RL agent for token pruning could introduce additional overhead to ViT operations. To quantify this impact, we measured the allocated Memory and GFLOPs during training and inference tasks, as well as the values of FPS. The results are presented in Table <a data-track="click" data-track-label="link" data-track-action="table anchor" href="/article/10.1007/s10489-025-06516-z#Tab7">7</a>.</p><div class="c-article-table" data-test="inline-table" data-container-section="table" id="table-7"><figure><figcaption class="c-article-table__figcaption"><b id="Tab7" data-test="table-caption">Table 7 Comparison of the computational efficiency of AgentViT and other approaches using ViT and SimpleViT as Vision Transformers</b></figcaption><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="table-link" data-track="click" data-track-action="view table" data-track-label="button" rel="nofollow" href="/article/10.1007/s10489-025-06516-z/tables/7" aria-label="Full size table 7"><span>Full size table</span><svg width="16" height="16" focusable="false" role="img" aria-hidden="true" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#icon-eds-i-chevron-right-small"></use></svg></a></div></figure></div><p>They show the computational efficiency of AgentViT and AgentSimpleViT compared to the other approaches. In fact, if we compare AgentViT with the Vanilla model, we can observe that with a small increase in the memory required for training and a very small increase in the memory required for inference, it requires 49.93% less GFLOPs during training and 50.81% less GFLOPs during inference. Its pruning activity also makes it capable of processing a much larger number of images per second. In fact, it achieves a 92.78% increase over the Vanilla model. Compared to ATS, PatchMerger and EViT, AgentViT requires more memory for training (due to the presence of the agent) and slightly more memory for inference. Instead, the GFLOPs it requires for both training and inference are comparable. However, it achieves a significant increase in FPS due to the pruning activities; in particular, this increase is 61.21% over ATS, 23.84% over PatchMerger, and 25.78% over EViT. A similar argument can be made for SimpleViT. In fact, it requires more training memory and slightly more inference memory than the Vanilla model and PatchMerger. The GFLOPs required by it for training and inference are much lower than the Vanilla model and comparable to PatchMerger. However, it achieves a 90.13% increase in FPS over the Vanilla model and a 26.75% increase over PatchMerger. These results allow us to conclude that AgentViT and AgentSimpleViT provide an excellent tradeoff between memory and CPU consumption on the one hand, and image processing efficiency on the other hand. In fact, since it selects the patches to be pruned at the first layer of the ViT, the computational load is reduced very soon; instead, the other approaches are constructed in such a way as to prune the tokens layer by layer, gradually reducing the computational load.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-4" data-title="Fig. 4"><figure><figcaption><b id="Fig4" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 4</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10489-025-06516-z/figures/4" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10489-025-06516-z/MediaObjects/10489_2025_6516_Fig4_HTML.png?as=webp"><img aria-describedby="Fig4" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10489-025-06516-z/MediaObjects/10489_2025_6516_Fig4_HTML.png" alt="figure 4" loading="lazy" width="685" height="943"></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-4-desc"><p>Comparison between DDQN and PPO using AgentViT on CIFAR10</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10489-025-06516-z/figures/4" data-track-dest="link:Figure4 Full size image" aria-label="Full size image figure 4" rel="nofollow"><span>Full size image</span><svg width="16" height="16" focusable="false" role="img" aria-hidden="true" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#icon-eds-i-chevron-right-small"></use></svg></a></div></figure></div><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-5" data-title="Fig. 5"><figure><figcaption><b id="Fig5" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 5</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10489-025-06516-z/figures/5" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10489-025-06516-z/MediaObjects/10489_2025_6516_Fig5_HTML.png?as=webp"><img aria-describedby="Fig5" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10489-025-06516-z/MediaObjects/10489_2025_6516_Fig5_HTML.png" alt="figure 5" loading="lazy" width="685" height="999"></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-5-desc"><p>Analysis of the DDQN Components to the AgentViT performances</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10489-025-06516-z/figures/5" data-track-dest="link:Figure5 Full size image" aria-label="Full size image figure 5" rel="nofollow"><span>Full size image</span><svg width="16" height="16" focusable="false" role="img" aria-hidden="true" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#icon-eds-i-chevron-right-small"></use></svg></a></div></figure></div><p>More generally, if we consider these results together with previous ones concerning Accuracy, Precision, Recall, F1-Score, and Cumulative Training Time, we can conclude that AgentViT and AgentSimpleViT provide the best tradeoff between computational efficiency and performance, making them particularly suitable for applications that require fast image processing, and thus for a large number of real-world cases.</p><h3 class="c-article__sub-heading" id="Sec15"><span class="c-article-section__title-number">4.4 </span>Ablation study</h3><p>In this section, we present the results of the ablation study we performed on our approach. Due to space limitations, we detail the results obtained with SimpleViT and CIFAR10, although we obtained similar results with the other models and datasets. In particular, in Subsection <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10489-025-06516-z#Sec16">4.4.1</a> we test the performance of AgentViT using a different RL algorithm from the previous one, while in Subsection <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10489-025-06516-z#Sec17">4.4.2</a> we remove the DDQN components step by step to check their impact on our approach.</p><h4 class="c-article__sub-heading c-article__sub-heading--small" id="Sec16"><span class="c-article-section__title-number">4.4.1 </span>Training with a different reinforcement learning algorithm</h4><p>Due to its modularity, our approach can be implemented using different RL frameworks. For this reason, in this experiment we decided to replace the previously used DDQN approach with the Proximal Policy Optimization (PPO) [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 73" title="Schulman J, Wolski F, Dhariwal P, Radford A, Klimov O (2017) Proximal policy optimization algorithms. &#xA;                  arXiv:1707.06347&#xA;                  &#xA;                " href="/article/10.1007/s10489-025-06516-z#ref-CR73" id="ref-link-section-d305401337e10133">73</a>] algorithm to perform the patch selection task. Unlike DDQN, PPO is a Monte Carlo-based approach that works only with episodic environments. In our case, we define an episode as a training epoch of the ViT model. Similar to the DDQN algorithm, the PPO algorithm uses a pair of neural networks, namely, the Policy Network and the Value Network. The former takes the state <i>s</i> as input and returns a probability distribution of possible actions <i>a</i>. This network is trained to maximize the expected cumulative reward. The latter estimates the advantage function and is updated by minimizing the mean squared error between the predicted state values and the observed returns. This similarity allows us to use PPO directly in our approach. In fact, the Policy Network works like the Q-Networks described in Section <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10489-025-06516-z#Sec6">3.1</a>, but instead of returning a Q-value for each patch, it returns the distribution of the Value function over the patches. The patches selected are those with an attention value higher than the mean of all attention values. Figure <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10489-025-06516-z#Fig4">4</a> shows the performance of the PPO algorithm compared to the AgentSimpleViT (based on DDQN). In Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10489-025-06516-z#Fig4">4</a>, we compare the Accuracy and Cumulative Training Time of DDQN and PPO.</p><p>As shown in this figure, there is no significant difference in Cumulative Training Time between DDQN and PPO; however, PPO shows a worse performance in terms of Accuracy. This is probably due to the episode partitioning, as long episodes can slow down learning. This happens because the feedback needed to update the model is delayed since the agent does not receive the reward until the end of the episode. Also, a higher number of episodes could improve the PPO results, but to achieve this advantage it is necessary to increase the number of epochs and therefore the training time.</p><h4 class="c-article__sub-heading c-article__sub-heading--small" id="Sec17"><span class="c-article-section__title-number">4.4.2 </span>Analysis of DDQN components</h4><p>In this section, we report the results of our approach by eliminating some of the components of the DDQN described in Sections <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10489-025-06516-z#Sec6">3.1</a> and <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10489-025-06516-z#Sec8">3.3</a>. We decided to test AgentSimpleViT without the Replay Memory, which contains the past experiences that the agent uses at each iteration to train itself, and without the Target Network, which ensures that the maximization step is carried out by the Q-Network itself. The corresponding Accuracy and Cumulative Training Time results are reported in Fig.<a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10489-025-06516-z#Fig5">5</a>.</p><p>From the analysis of this figure, we can see that the removal of Replay Memory results in a slight decrease in both Accuracy and Cumulative Training Time. While Replay Memory introduces a small overhead due to inserting observations into the buffer and sampling them during training, it improves the stability of the training process. In contrast, removing the Target Network has a more significant impact on agent performance, resulting in a higher reduction in Accuracy. The combined removal of Replay Memory and Target Network results in much lower Accuracy.</p><div class="c-article-table" data-test="inline-table" data-container-section="table" id="table-8"><figure><figcaption class="c-article-table__figcaption"><b id="Tab8" data-test="table-caption">Table 8 Batches employed in our qualitative analysis</b></figcaption><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="table-link" data-track="click" data-track-action="view table" data-track-label="button" rel="nofollow" href="/article/10.1007/s10489-025-06516-z/tables/8" aria-label="Full size table 8"><span>Full size table</span><svg width="16" height="16" focusable="false" role="img" aria-hidden="true" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#icon-eds-i-chevron-right-small"></use></svg></a></div></figure></div><h3 class="c-article__sub-heading" id="Sec18"><span class="c-article-section__title-number">4.5 </span>Qualitative analysis</h3><p>In the previous sections, we have performed a quantitative analysis of AgentViT and we have seen that it achieves satisfactory results. In this section, we want to add a qualitative analysis, which aims to verify the quality of the patch filtering performed by AgentSimpleViT as it is visually perceived by an end user. To conduct this analysis, we constructed 7 batches each consisting of 4 images randomly selected from CIFAR10. In Table <a data-track="click" data-track-label="link" data-track-action="table anchor" href="/article/10.1007/s10489-025-06516-z#Tab8">8</a>, we report the classes of the four images belonging to each batch.</p><p>As can be seen, the batches are very heterogeneous with each other, where images belong to very different classes. Therefore, finding patches that can be filtered across all the images in a batch without losing important details is extremely difficult. Quantitative analysis has shown that AgentSimpleViT succeeds in doing this task satisfactorily. Here, we want to verify whether this is also true from a visual analysis point of view. The results of applying AgentSimpleViT to the previously constructed batches are shown in Figs.<a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10489-025-06516-z#Fig6">6</a>, <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10489-025-06516-z#Fig7">7</a> and <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10489-025-06516-z#Fig8">8</a>. Specifically, for each batch, the first row shows the four original images, while the second row shows the same images without the patches filtered out by AgentSimpleViT.</p><p>From the analysis of these figures, we can observe that, in each batch, AgentSimpleViT is indeed capable of filtering patches not closely related to the objects of the classes to which the corresponding figures belong. In fact, in most cases the patches filtered out are those related to the background (see, for example, Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10489-025-06516-z#Fig6">6</a>c). In some cases, AgentViT removes patches close to the object of interest, but mostly preserves the center of the image, which often contains the object determining the class.</p><p>An additional interesting aspect we want to highlight is that AgentSimpleViT was able to automatically determine the best number of patches to filter out based on the input images. In fact, during this experiment, we had specified that the desired number of patches to be filtered out was 32 for each batch. Actually, AgentSimpleViT did not slavishly follow our indications, which is correct based on what we said in Section <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10489-025-06516-z#Sec9">3.4</a>. Specifically, the number of patches that AgentSimpleViT removed for each batch is reported in Table <a data-track="click" data-track-label="link" data-track-action="table anchor" href="/article/10.1007/s10489-025-06516-z#Tab9">9</a>. Comparing the results of this table and the images of Figs. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10489-025-06516-z#Fig6">6</a><a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10489-025-06516-z#Fig8">8</a>, we can observe that AgentSimpleViT removes more patches in the case where the image has a lot of background, while it removes fewer patches in the other case. In fact, it understands that removing more patches in this latter case would lead to a deterioration of the overall accuracy. This is exactly the behavior we hoped to achieve when we defined the requirements for AgentSimpleViT.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-6" data-title="Fig. 6"><figure><figcaption><b id="Fig6" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 6</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10489-025-06516-z/figures/6" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10489-025-06516-z/MediaObjects/10489_2025_6516_Fig6_HTML.png?as=webp"><img aria-describedby="Fig6" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10489-025-06516-z/MediaObjects/10489_2025_6516_Fig6_HTML.png" alt="figure 6" loading="lazy" width="685" height="1217"></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-6-desc"><p>Qualitative analysis of AgentViT on seven batches of images (Batches 1-3)</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10489-025-06516-z/figures/6" data-track-dest="link:Figure6 Full size image" aria-label="Full size image figure 6" rel="nofollow"><span>Full size image</span><svg width="16" height="16" focusable="false" role="img" aria-hidden="true" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#icon-eds-i-chevron-right-small"></use></svg></a></div></figure></div><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-7" data-title="Fig. 7"><figure><figcaption><b id="Fig7" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 7</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10489-025-06516-z/figures/7" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10489-025-06516-z/MediaObjects/10489_2025_6516_Fig7_HTML.png?as=webp"><img aria-describedby="Fig7" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10489-025-06516-z/MediaObjects/10489_2025_6516_Fig7_HTML.png" alt="figure 7" loading="lazy" width="685" height="1209"></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-7-desc"><p>Qualitative analysis of AgentViT on seven batches of images (Batches 4-6)</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10489-025-06516-z/figures/7" data-track-dest="link:Figure7 Full size image" aria-label="Full size image figure 7" rel="nofollow"><span>Full size image</span><svg width="16" height="16" focusable="false" role="img" aria-hidden="true" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#icon-eds-i-chevron-right-small"></use></svg></a></div></figure></div><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-8" data-title="Fig. 8"><figure><figcaption><b id="Fig8" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 8</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10489-025-06516-z/figures/8" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10489-025-06516-z/MediaObjects/10489_2025_6516_Fig8_HTML.png?as=webp"><img aria-describedby="Fig8" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10489-025-06516-z/MediaObjects/10489_2025_6516_Fig8_HTML.png" alt="figure 8" loading="lazy" width="685" height="388"></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-8-desc"><p>Qualitative analysis of AgentViT on seven batches of images (Batch 7)</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10489-025-06516-z/figures/8" data-track-dest="link:Figure8 Full size image" aria-label="Full size image figure 8" rel="nofollow"><span>Full size image</span><svg width="16" height="16" focusable="false" role="img" aria-hidden="true" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#icon-eds-i-chevron-right-small"></use></svg></a></div></figure></div></div></div></section><section data-title="Discussion"><div class="c-article-section" id="Sec19-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec19"><span class="c-article-section__title-number">5 </span>Discussion</h2><div class="c-article-section__content" id="Sec19-content"><p>s seen in the previous sections, AgentViT (resp., AgentSimpleViT) introduces the use of RL to select patches for optimal filtering. In our opinion, the qualitative and quantitative results obtained show that its performance is solid. In fact, they show that AgentViT (resp., AgentSimpleViT) is able to train a Vision Transformer in less time than that required by ViT (resp., SimpleViT) when trained without removing any patches. This reduction in training time is achieved with comparable accuracy. In addition, the Deep Q-Learning hyperparameters allowed us to tune AgentViT to achieve the desired tradeoff between accuracy and training time. Finally, the comparison with other approaches showed that AgentViT is the one that provides the best tradeoff between Cumulative Training Time, Memory Usage, FPS and GFLOPs on the one hand, and Accuracy, Precision, Recall and F1-Score on the other hand.</p><p>In addition, AgentViT has other interesting implications. One of these is the ability to use larger ViTs. In fact, AgentViTs ability to reduce training time ensures that architectures that would not normally be adopted due to excessive training time can now be adopted within AgentViT. A second implication concerns the use of AgentViT to construct reduced synthetic datasets from the original datasets (such as CIFAR10), which can be used to train deep neural networks. A third implication concerns the possible extension of AgentViT to contexts other than ViTs. In fact, the idea behind our framework is general and can be applied not only to ViTs but also to any Transformer architecture (e.g., an architecture operating in the NLP context). The only requirement is that the input of the agent present in AgentViT is provided in the form of an attention matrix.</p><p>To complete the discussion of AgentViT, it is worth mentioning some limitations of our framework. One of its main limitations is its reliance on Deep Q-Learning, which requires tuning of several hyperparameters, including buffer size and reward frequency. These hyperparameters significantly influence the agents ability to learn an optimal patch selection strategy, affecting both efficiency and accuracy. The need to tune some hyperparameters could make it difficult for less experienced users to apply AgentViT effectively, as the relationship between these parameters and model performance is not always intuitive. For example, the choice of buffer size determines how much past experience the agent can leverage, striking a delicate balance between preserving relevant information and avoiding overfitting with outdated decisions. Too small a buffer might lead to unstable learning, while too large a buffer may slow down updates, making the agent less responsive to new observations. Similarly, reward frequency affects learning dynamics; indeed, too frequent rewards can lead to overfitting on short-term gains, while infrequent rewards can lead to inefficient exploration.</p><div class="c-article-table" data-test="inline-table" data-container-section="table" id="table-9"><figure><figcaption class="c-article-table__figcaption"><b id="Tab9" data-test="table-caption">Table 9 Number of patches filtered out by AgentViT for each batch</b></figcaption><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="table-link" data-track="click" data-track-action="view table" data-track-label="button" rel="nofollow" href="/article/10.1007/s10489-025-06516-z/tables/9" aria-label="Full size table 9"><span>Full size table</span><svg width="16" height="16" focusable="false" role="img" aria-hidden="true" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#icon-eds-i-chevron-right-small"></use></svg></a></div></figure></div><p>Another limitation concerns the number of patches required by AgentViT to work effectively. Since the agent selects patches based on attention values from the first ViT layer, it needs a sufficiently large number of patches to detect meaningful patterns in the input image. If the ViT model works with a small number of patches, the agent may have difficulty distinguishing between informative and redundant regions, ultimately leading to suboptimal patch selection. When the number of patches is very small, the granularity of patch selection decreases, reducing the agents ability to make accurate decisions. In such cases, removing even a small number of patches might result in the loss of critical information, negatively impacting classification performance. Furthermore, with few patches, the variability of attention values across different regions of the image decreases, making it more challenging for the agent to dynamically identify the most relevant patches.</p><p>Finally, another limitation of AgentViT is that it relies on attention scores to guide its RL agent. This reliance on attention mechanisms means that AgentViT cannot be directly applied to architectures that do not incorporate self-attention, such as traditional CNNs. This makes it difficult to integrate AgentViT with CNN-based architectures, requiring alternative methods to identify important regions of the image. Consequently, its applicability is limited to transformer-based models, which reduces its generalizability, especially in domains where CNNs remain the preferred choice.</p></div></div></section><section data-title="Conclusion"><div class="c-article-section" id="Sec20-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec20"><span class="c-article-section__title-number">6 </span>Conclusion</h2><div class="c-article-section__content" id="Sec20-content"><p>In this paper, we have proposed the application of Reinforcement Learning to reduce the training time of a Vision Transformer without significantly reducing the quality of its results. For this purpose, we adopted the classical Markov Decision Process mechanism to represent an environment for the image classification task, which required us to redefine the state, action and reward necessary to train our agent. We tested AgentViT using ViT and SimpleViT as Vision Transformers, a Double Deep Q-Network as the internal agent, and applying it to CIFAR10, FashionMNIST, and Imagenette<span class="mathjax-tex">\(^+\)</span>. Our analysis showed that AgentViT leads to a significant reduction in training time while achieving competitive accuracy. Specifically, when compared to ViT running alone (i.e., not supported by our system), AgentViT achieves a 2.08% (resp., 0.25%, 0.22%) increase in Accuracy and a 21.42% (resp., 8.67%, 16.09%) reduction in Cumulative Training Time when CIFAR10 (resp., FMNIST, Imagenette<span class="mathjax-tex">\(^+\)</span>) is used as dataset. We also checked the impact of some hyperparameters on the performance of AgentSimpleViT, which allowed us to identify the corresponding optimal values. We then performed some experiments on CIFAR10, FMNIST. and Imagenette<span class="mathjax-tex">\(^+\)</span> datasets and compared AgentViT with several related approaches already proposed in the literature and were able to conclude that our framework provides the best tradeoff between Cumulative Training Time, Memory Usage, FPS and GFLOPs on the one hand, and Accuracy, Precision, Recall and F1-Score, on the other. We performed an ablation study to evaluate the ability of our framework to follow the training process with another RL algorithm and the importance of the various components of DDQN. In addition, we presented a qualitative experiment to verify whether the patches filtered by AgentViT are indeed those in the background of an image or, in any case, those not useful for its classification. We received very positive feedback for this experiment as well, and also appreciated AgentViTs ability to dynamically select the number of patches to be filtered.</p><p>In the future, we plan to extend our approach in several directions. For example, we would like to extend it to include deeper attention layers in the patch selection process. However, this extension is not trivial, as dynamically removing patches across layers introduces a non-stationary state space and action space for the RL agent. Furthermore, we can think of defining a new metric similar to the Akaike information criterion [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 87" title="Akaike H (1974) A new look at the statistical model identification. IEEE Trans Autom Control 19(6):716723" href="/article/10.1007/s10489-025-06516-z#ref-CR87" id="ref-link-section-d305401337e10779">87</a>] that takes into account the performance of the model and the number of tokens. In addition, we would like to test other Reinforcement Learning algorithms, such as Multi-Agent Reinforcement Learning and Contextual Multi-Armed Bandit, instead of the Deep Q-Network, to see if they can further improve the performance of a ViT. These algorithms could help select the right actions and patches to speed up ViT training. One possible research direction is to extend our approach, which was specifically developed for classification, to other computer vision tasks, such as object recognition and segmentation. This would involve restructuring the agents objective function so that the reward given to the RL agent takes into account the task for which the system is working. Finally, we plan to evaluate the impact of our approach on other ViT architectures.</p></div></div></section>
                                </div>
                        
                    

                    
                        
                    

                    
                        
                    

                    
                    <section data-title="Data Availability"><div class="c-article-section" id="data-availability-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="data-availability">Data Availability</h2><div class="c-article-section__content" id="data-availability-content">
              
              <p>The dataset used for our study is publicly available at the link <a href="https://github.com/DavideTraini/RL-for-ViT">https://github.com/DavideTraini/RL-for-ViT</a>.</p>
            </div></div></section><section data-title="Notes"><div class="c-article-section" id="notes-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="notes">Notes</h2><div class="c-article-section__content" id="notes-content"><ol class="c-article-footnote c-article-footnote--listed"><li class="c-article-footnote--listed__item" id="Fn1" data-counter="1."><div class="c-article-footnote--listed__content"><p><a href="https://github.com/fastai/imagenette">https://github.com/fastai/imagenette</a></p></div></li></ol></div></div></section><div id="MagazineFulltextArticleBodySuffix"><section aria-labelledby="Bib1" data-title="References"><div class="c-article-section" id="Bib1-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Bib1">References</h2><div class="c-article-section__content" id="Bib1-content"><div data-container-section="references"><ol class="c-article-references" data-track-component="outbound reference" data-track-context="references section"><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="1."><p class="c-article-references__text" id="ref-CR1">Min B, Ross H, Sulem E, Veyseh APB, Nguyen TH, Sainz O, Agirre E, Heintz I, Roth D (2023) Recent advances in natural language processing via large pre-trained language models: a survey. ACM Comput Surv 56(2):140</p><p class="c-article-references__links u-hide-print"><a data-track="click_references" rel="nofollow noopener" data-track-label="10.1145/3605943" data-track-item_id="10.1145/3605943" data-track-value="article reference" data-track-action="article reference" href="https://doi.org/10.1145%2F3605943" aria-label="Article reference 1" data-doi="10.1145/3605943">Article</a>
    <a data-track="click_references" data-track-action="google scholar reference" data-track-value="google scholar reference" data-track-label="link" data-track-item_id="link" rel="nofollow noopener" aria-label="Google Scholar reference 1" href="http://scholar.google.com/scholar_lookup?&amp;title=Recent%20advances%20in%20natural%20language%20processing%20via%20large%20pre-trained%20language%20models%3A%20a%20survey&amp;journal=ACM%20Comput%20Surv&amp;doi=10.1145%2F3605943&amp;volume=56&amp;issue=2&amp;pages=1-40&amp;publication_year=2023&amp;author=Min%2CB&amp;author=Ross%2CH&amp;author=Sulem%2CE&amp;author=Veyseh%2CAPB&amp;author=Nguyen%2CTH&amp;author=Sainz%2CO&amp;author=Agirre%2CE&amp;author=Heintz%2CI&amp;author=Roth%2CD">
                    Google Scholar</a>
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="2."><p class="c-article-references__text" id="ref-CR2">Chai J, Zeng H, Li A, Ngai E (2021) Deep learning in computer vision: a critical review of emerging techniques and application scenarios. Mach Learn Appl 6:100134</p><p class="c-article-references__links u-hide-print"><a data-track="click_references" data-track-action="google scholar reference" data-track-value="google scholar reference" data-track-label="link" data-track-item_id="link" rel="nofollow noopener" aria-label="Google Scholar reference 2" href="http://scholar.google.com/scholar_lookup?&amp;title=Deep%20learning%20in%20computer%20vision%3A%20a%20critical%20review%20of%20emerging%20techniques%20and%20application%20scenarios&amp;journal=Mach%20Learn%20Appl&amp;volume=6&amp;publication_year=2021&amp;author=Chai%2CJ&amp;author=Zeng%2CH&amp;author=Li%2CA&amp;author=Ngai%2CE">
                    Google Scholar</a>
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="3."><p class="c-article-references__text" id="ref-CR3">Vaswani A, Shazeer N, Parmar N, Uszkoreit J, Jones L, Gomez AN, Kaiser , Polosukhin I (2017) Attention is All you Need. In: Proceedings of the international conference on advances in neural information processing systems (NIPS17), Long Beach, CA, USA. Curran Associates, p 30</p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="4."><p class="c-article-references__text" id="ref-CR4">Dosovitskiy A, Beyer L, Kolesnikov A, Weissenborn D, Zhai X, Unterthiner T, Dehghani M, Minderer M, Heigold G, Gelly S, Uszkoreit J, Houlsby N (2021) An image is worth 16x16 words: transformers for image recognition at scale. In: Proceedings of the international conference on learning representations (ICLR21), Virtual event. OpenReview.net</p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="5."><p class="c-article-references__text" id="ref-CR5">Waqas M, Tahir MA, Danish M, Al-Maadeed S, Bouridane A, Wu J (2024) Simultaneous instance pooling and bag representation selection approach for multiple-instance learning (MIL) using vision transformer. Neural Comput Appl 122. Springer</p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="6."><p class="c-article-references__text" id="ref-CR6">Poornam S, Angelina J (2024) VITALT: a robust and efficient brain tumor detection system using vision transformer with attention and linear transformation. Neural Comput Appl 117. Springer</p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="7."><p class="c-article-references__text" id="ref-CR7">Child R, Gray S, Radford A, Sutskever I (2019) Generating long sequences with sparse transformers. <a href="http://arxiv.org/abs/1904.10509" data-track="click_references" data-track-action="external reference" data-track-value="external reference" data-track-label="http://arxiv.org/abs/1904.10509">arXiv:1904.10509</a></p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="8."><p class="c-article-references__text" id="ref-CR8">Fayyaz M, Koohpayegani SA, Jafari FR, Sengupta S, Joze HRV, Sommerlade E, Pirsiavash H, Gall J (2022) Adaptive token sampling for efficient vision transformers. In: Proceedings of the european conference on computer vision (ECCV22), Tel Aviv, Israel. Springer, pp 396414</p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="9."><p class="c-article-references__text" id="ref-CR9">Yin H, Vahdat A, Alvarez JM, Mallya A, Kautz J, Molchanov P (2022) A-vit: adaptive tokens for efficient vision transformer. In: Proceedings of the international conference on computer vision and pattern recognition (CVPR22), New Orleans, LA, USA, pp 1080910818</p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="10."><p class="c-article-references__text" id="ref-CR10">Renggli C, Pinto AS, Houlsby N, Mustafa B, Puigcerver J, Riquelme C (2022) Learning to merge tokens in vision transformers. <a href="http://arxiv.org/abs/2202.12015" data-track="click_references" data-track-action="external reference" data-track-value="external reference" data-track-label="http://arxiv.org/abs/2202.12015">arXiv:2202.12015</a></p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="11."><p class="c-article-references__text" id="ref-CR11">Liang Y, Ge C, Tong Z, Song Y, Wang J, Xie P (2022) EViT: expediting vision transformers via token reorganizations. In: Proceedings of the international conference on learning representations (ICLR22), Virtual event. OpenReview.net</p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="12."><p class="c-article-references__text" id="ref-CR12">Polydoros AS, Nalpantidis L (2017) Survey of model-based reinforcement learning: applications on robotics. J Intell Robot Syst 86(2):153173</p><p class="c-article-references__links u-hide-print"><a data-track="click_references" rel="noopener" data-track-label="10.1007/s10846-017-0468-y" data-track-item_id="10.1007/s10846-017-0468-y" data-track-value="article reference" data-track-action="article reference" href="https://link.springer.com/doi/10.1007/s10846-017-0468-y" aria-label="Article reference 12" data-doi="10.1007/s10846-017-0468-y">Article</a>
    <a data-track="click_references" data-track-action="google scholar reference" data-track-value="google scholar reference" data-track-label="link" data-track-item_id="link" rel="nofollow noopener" aria-label="Google Scholar reference 12" href="http://scholar.google.com/scholar_lookup?&amp;title=Survey%20of%20model-based%20reinforcement%20learning%3A%20applications%20on%20robotics&amp;journal=J%20Intell%20Robot%20Syst&amp;doi=10.1007%2Fs10846-017-0468-y&amp;volume=86&amp;issue=2&amp;pages=153-173&amp;publication_year=2017&amp;author=Polydoros%2CAS&amp;author=Nalpantidis%2CL">
                    Google Scholar</a>
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="13."><p class="c-article-references__text" id="ref-CR13">Coronato A, Naeem M, Pietro GD, Paragliola G (2020) Reinforcement learning for intelligent healthcare applications: a survey. Artif Intell Med 109:101964</p><p class="c-article-references__links u-hide-print"><a data-track="click_references" rel="nofollow noopener" data-track-label="10.1016/j.artmed.2020.101964" data-track-item_id="10.1016/j.artmed.2020.101964" data-track-value="article reference" data-track-action="article reference" href="https://doi.org/10.1016%2Fj.artmed.2020.101964" aria-label="Article reference 13" data-doi="10.1016/j.artmed.2020.101964">Article</a>
    <a data-track="click_references" data-track-action="google scholar reference" data-track-value="google scholar reference" data-track-label="link" data-track-item_id="link" rel="nofollow noopener" aria-label="Google Scholar reference 13" href="http://scholar.google.com/scholar_lookup?&amp;title=Reinforcement%20learning%20for%20intelligent%20healthcare%20applications%3A%20a%20survey&amp;journal=Artif%20Intell%20Med&amp;doi=10.1016%2Fj.artmed.2020.101964&amp;volume=109&amp;publication_year=2020&amp;author=Coronato%2CA&amp;author=Naeem%2CM&amp;author=Pietro%2CGD&amp;author=Paragliola%2CG">
                    Google Scholar</a>
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="14."><p class="c-article-references__text" id="ref-CR14">Nian R, Liu J, Huang B (2020) A review on reinforcement learning: introduction and applications in industrial process control. Comput Chem Eng 139:106886</p><p class="c-article-references__links u-hide-print"><a data-track="click_references" rel="nofollow noopener" data-track-label="10.1016/j.compchemeng.2020.106886" data-track-item_id="10.1016/j.compchemeng.2020.106886" data-track-value="article reference" data-track-action="article reference" href="https://doi.org/10.1016%2Fj.compchemeng.2020.106886" aria-label="Article reference 14" data-doi="10.1016/j.compchemeng.2020.106886">Article</a>
    <a data-track="click_references" data-track-action="google scholar reference" data-track-value="google scholar reference" data-track-label="link" data-track-item_id="link" rel="nofollow noopener" aria-label="Google Scholar reference 14" href="http://scholar.google.com/scholar_lookup?&amp;title=A%20review%20on%20reinforcement%20learning%3A%20introduction%20and%20applications%20in%20industrial%20process%20control&amp;journal=Comput%20Chem%20Eng&amp;doi=10.1016%2Fj.compchemeng.2020.106886&amp;volume=139&amp;publication_year=2020&amp;author=Nian%2CR&amp;author=Liu%2CJ&amp;author=Huang%2CB">
                    Google Scholar</a>
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="15."><p class="c-article-references__text" id="ref-CR15">Luong NC, Hoang DT, Gong S, Niyato D, Wang P, Liang YC, Kim DI (2019) Applications of deep reinforcement learning in communications and networking: a survey. IEEE Commun Surv Tutor 21(4):31333174</p><p class="c-article-references__links u-hide-print"><a data-track="click_references" rel="nofollow noopener" data-track-label="10.1109/COMST.2019.2916583" data-track-item_id="10.1109/COMST.2019.2916583" data-track-value="article reference" data-track-action="article reference" href="https://doi.org/10.1109%2FCOMST.2019.2916583" aria-label="Article reference 15" data-doi="10.1109/COMST.2019.2916583">Article</a>
    <a data-track="click_references" data-track-action="google scholar reference" data-track-value="google scholar reference" data-track-label="link" data-track-item_id="link" rel="nofollow noopener" aria-label="Google Scholar reference 15" href="http://scholar.google.com/scholar_lookup?&amp;title=Applications%20of%20deep%20reinforcement%20learning%20in%20communications%20and%20networking%3A%20a%20survey&amp;journal=IEEE%20Commun%20Surv%20Tutor&amp;doi=10.1109%2FCOMST.2019.2916583&amp;volume=21&amp;issue=4&amp;pages=3133-3174&amp;publication_year=2019&amp;author=Luong%2CNC&amp;author=Hoang%2CDT&amp;author=Gong%2CS&amp;author=Niyato%2CD&amp;author=Wang%2CP&amp;author=Liang%2CYC&amp;author=Kim%2CDI">
                    Google Scholar</a>
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="16."><p class="c-article-references__text" id="ref-CR16">Haydari A, Ylmaz Y (2020) Deep reinforcement learning for intelligent transportation systems: a survey. IEEE Trans Intell Trans Syst 23(1):1132</p><p class="c-article-references__links u-hide-print"><a data-track="click_references" rel="nofollow noopener" data-track-label="10.1109/TITS.2020.3008612" data-track-item_id="10.1109/TITS.2020.3008612" data-track-value="article reference" data-track-action="article reference" href="https://doi.org/10.1109%2FTITS.2020.3008612" aria-label="Article reference 16" data-doi="10.1109/TITS.2020.3008612">Article</a>
    <a data-track="click_references" data-track-action="google scholar reference" data-track-value="google scholar reference" data-track-label="link" data-track-item_id="link" rel="nofollow noopener" aria-label="Google Scholar reference 16" href="http://scholar.google.com/scholar_lookup?&amp;title=Deep%20reinforcement%20learning%20for%20intelligent%20transportation%20systems%3A%20a%20survey&amp;journal=IEEE%20Trans%20Intell%20Trans%20Syst&amp;doi=10.1109%2FTITS.2020.3008612&amp;volume=23&amp;issue=1&amp;pages=11-32&amp;publication_year=2020&amp;author=Haydari%2CA&amp;author=Y%C4%B1lmaz%2CY">
                    Google Scholar</a>
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="17."><p class="c-article-references__text" id="ref-CR17">Fang W, Pang L, Yi W (2020) Survey on the application of deep reinforcement learning in image processing. J Artif Intell 2(1):3958</p><p class="c-article-references__links u-hide-print"><a data-track="click_references" rel="nofollow noopener" data-track-label="10.32604/jai.2020.09789" data-track-item_id="10.32604/jai.2020.09789" data-track-value="article reference" data-track-action="article reference" href="https://doi.org/10.32604%2Fjai.2020.09789" aria-label="Article reference 17" data-doi="10.32604/jai.2020.09789">Article</a>
    <a data-track="click_references" data-track-action="google scholar reference" data-track-value="google scholar reference" data-track-label="link" data-track-item_id="link" rel="nofollow noopener" aria-label="Google Scholar reference 17" href="http://scholar.google.com/scholar_lookup?&amp;title=Survey%20on%20the%20application%20of%20deep%20reinforcement%20learning%20in%20image%20processing&amp;journal=J%20Artif%20Intell&amp;doi=10.32604%2Fjai.2020.09789&amp;volume=2&amp;issue=1&amp;pages=39-58&amp;publication_year=2020&amp;author=Fang%2CW&amp;author=Pang%2CL&amp;author=Yi%2CW">
                    Google Scholar</a>
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="18."><p class="c-article-references__text" id="ref-CR18">Yuan X, Fei H, Baek J (2024) Efficient transformer adaptation with soft token merging. In: Proceedings of the IEEE/CVF conference on computer vision and pattern recognition (CVPR24), Seattle, WA, USA, pp 36583668</p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="19."><p class="c-article-references__text" id="ref-CR19">Bolya D, Hoffman J (2023) Token merging for fast stable diffusion. In: Proceedings of the IEEE/CVF conference on computer vision and pattern recognition (CVPR23), Vancouver, British Columbia, Canada, pp 45984602</p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="20."><p class="c-article-references__text" id="ref-CR20">Tang Y, Han K, Wang Y, Xu C, Guo J, Xu C, Tao D (2022) Patch slimming for efficient vision transformers. In: Proceedings of the international conference on computer vision and pattern recognition (CVPR22), New Orleans, LA, USA, pp 1216512174</p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="21."><p class="c-article-references__text" id="ref-CR21">Rao Y, Zhao W, Liu B, Lu J, Zhou J, Hsieh CJ (2021) Dynamicvit: efficient vision transformers with dynamic token sparsification. Adv Neural Inf Process Syst 34:1393713949</p><p class="c-article-references__links u-hide-print"><a data-track="click_references" data-track-action="google scholar reference" data-track-value="google scholar reference" data-track-label="link" data-track-item_id="link" rel="nofollow noopener" aria-label="Google Scholar reference 21" href="http://scholar.google.com/scholar_lookup?&amp;title=Dynamicvit%3A%20efficient%20vision%20transformers%20with%20dynamic%20token%20sparsification&amp;journal=Adv%20Neural%20Inf%20Process%20Syst&amp;volume=34&amp;pages=13937-13949&amp;publication_year=2021&amp;author=Rao%2CY&amp;author=Zhao%2CW&amp;author=Liu%2CB&amp;author=Lu%2CJ&amp;author=Zhou%2CJ&amp;author=Hsieh%2CCJ">
                    Google Scholar</a>
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="22."><p class="c-article-references__text" id="ref-CR22">Rao Y, Liu Z, Zhao W, Zhou J, Lu J (2023) Dynamic spatial sparsification for efficient vision transformers and convolutional neural networks. IEEE Trans Pattern Anal Mach Intell. IEEE</p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="23."><p class="c-article-references__text" id="ref-CR23">Hasselt HV, Guez A, Silver D (2016) Deep reinforcement learning with double q-learning. In: Proceedings of the AAAI conference on artificial intelligence (AAAI16), vol. 30. Phoenix, AZ, USA</p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="24."><p class="c-article-references__text" id="ref-CR24">Krizhevsky A, Nair V, Hinton G (2010) CIFAR-10 (Canadian Institute for Advanced Research). <a href="https://www.cs.toronto.edu/%7ekriz/cifar.html" data-track="click_references" data-track-action="external reference" data-track-value="external reference" data-track-label="https://www.cs.toronto.edu/%7ekriz/cifar.html">https://www.cs.toronto.edu/~kriz/cifar.html</a></p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="25."><p class="c-article-references__text" id="ref-CR25">Xiao H, Rasul K, Vollgraf R (2017) Fashion-MNIST: a novel image dataset for benchmarking machine learning algorithms. <a href="http://arxiv.org/abs/1708.07747" data-track="click_references" data-track-action="external reference" data-track-value="external reference" data-track-label="http://arxiv.org/abs/1708.07747">arXiv:1708.07747</a></p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="26."><p class="c-article-references__text" id="ref-CR26">Deng J, Dong W, Socher R, Li LJ, Kai L, Li FF (2009) ImageNet: a large-scale hierarchical image database. In: Proceedings of the International IEEE Conference on Computer Vision and Pattern Recognition (CVPR09), Miami, FL, USA. IEEE, pp 248255</p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="27."><p class="c-article-references__text" id="ref-CR27">Perera ATD, Kamalaruban P (2021) Applications of reinforcement learning in energy systems. Renew Sustain Energy Rev 137:110618</p><p class="c-article-references__links u-hide-print"><a data-track="click_references" rel="nofollow noopener" data-track-label="10.1016/j.rser.2020.110618" data-track-item_id="10.1016/j.rser.2020.110618" data-track-value="article reference" data-track-action="article reference" href="https://doi.org/10.1016%2Fj.rser.2020.110618" aria-label="Article reference 27" data-doi="10.1016/j.rser.2020.110618">Article</a>
    <a data-track="click_references" data-track-action="google scholar reference" data-track-value="google scholar reference" data-track-label="link" data-track-item_id="link" rel="nofollow noopener" aria-label="Google Scholar reference 27" href="http://scholar.google.com/scholar_lookup?&amp;title=Applications%20of%20reinforcement%20learning%20in%20energy%20systems&amp;journal=Renew%20Sustain%20Energy%20Rev&amp;doi=10.1016%2Fj.rser.2020.110618&amp;volume=137&amp;publication_year=2021&amp;author=Perera%2CATD&amp;author=Kamalaruban%2CP">
                    Google Scholar</a>
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="28."><p class="c-article-references__text" id="ref-CR28">Zuccotto M, Castellini A, Torre DL, Mola L, Farinelli A (2024) Reinforcement learning applications in environmental sustainability: a review. Artif Intell Rev 57(4):88</p><p class="c-article-references__links u-hide-print"><a data-track="click_references" rel="noopener" data-track-label="10.1007/s10462-024-10706-5" data-track-item_id="10.1007/s10462-024-10706-5" data-track-value="article reference" data-track-action="article reference" href="https://link.springer.com/doi/10.1007/s10462-024-10706-5" aria-label="Article reference 28" data-doi="10.1007/s10462-024-10706-5">Article</a>
    <a data-track="click_references" data-track-action="google scholar reference" data-track-value="google scholar reference" data-track-label="link" data-track-item_id="link" rel="nofollow noopener" aria-label="Google Scholar reference 28" href="http://scholar.google.com/scholar_lookup?&amp;title=Reinforcement%20learning%20applications%20in%20environmental%20sustainability%3A%20a%20review&amp;journal=Artif%20Intell%20Rev&amp;doi=10.1007%2Fs10462-024-10706-5&amp;volume=57&amp;issue=4&amp;publication_year=2024&amp;author=Zuccotto%2CM&amp;author=Castellini%2CA&amp;author=Torre%2CDL&amp;author=Mola%2CL&amp;author=Farinelli%2CA">
                    Google Scholar</a>
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="29."><p class="c-article-references__text" id="ref-CR29">Mai V, Maisonneuve P, Zhang T, Nekoei H, Paull L, Lesage-Landry A (2024) Multi-agent reinforcement learning for fast-timescale demand response of residential loads. Mach Learn 113(5):33553355</p><p class="c-article-references__links u-hide-print"><a data-track="click_references" rel="noopener" data-track-label="10.1007/s10994-024-06514-1" data-track-item_id="10.1007/s10994-024-06514-1" data-track-value="article reference" data-track-action="article reference" href="https://link.springer.com/doi/10.1007/s10994-024-06514-1" aria-label="Article reference 29" data-doi="10.1007/s10994-024-06514-1">Article</a>
    <a data-track="click_references" rel="nofollow noopener" data-track-label="link" data-track-item_id="link" data-track-value="mathscinet reference" data-track-action="mathscinet reference" href="http://www.ams.org/mathscinet-getitem?mr=4741175" aria-label="MathSciNet reference 29">MathSciNet</a>
    <a data-track="click_references" data-track-action="google scholar reference" data-track-value="google scholar reference" data-track-label="link" data-track-item_id="link" rel="nofollow noopener" aria-label="Google Scholar reference 29" href="http://scholar.google.com/scholar_lookup?&amp;title=Multi-agent%20reinforcement%20learning%20for%20fast-timescale%20demand%20response%20of%20residential%20loads&amp;journal=Mach%20Learn&amp;doi=10.1007%2Fs10994-024-06514-1&amp;volume=113&amp;issue=5&amp;pages=3355-3355&amp;publication_year=2024&amp;author=Mai%2CV&amp;author=Maisonneuve%2CP&amp;author=Zhang%2CT&amp;author=Nekoei%2CH&amp;author=Paull%2CL&amp;author=Lesage-Landry%2CA">
                    Google Scholar</a>
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="30."><p class="c-article-references__text" id="ref-CR30">Zhao X, Ding S, An Y, Jia W (2019) Applications of asynchronous deep reinforcement learning based on dynamic updating weights. Appl Intell 49:581591</p><p class="c-article-references__links u-hide-print"><a data-track="click_references" rel="noopener" data-track-label="10.1007/s10489-018-1296-x" data-track-item_id="10.1007/s10489-018-1296-x" data-track-value="article reference" data-track-action="article reference" href="https://link.springer.com/doi/10.1007/s10489-018-1296-x" aria-label="Article reference 30" data-doi="10.1007/s10489-018-1296-x">Article</a>
    <a data-track="click_references" data-track-action="google scholar reference" data-track-value="google scholar reference" data-track-label="link" data-track-item_id="link" rel="nofollow noopener" aria-label="Google Scholar reference 30" href="http://scholar.google.com/scholar_lookup?&amp;title=Applications%20of%20asynchronous%20deep%20reinforcement%20learning%20based%20on%20dynamic%20updating%20weights&amp;journal=Appl%20Intell&amp;doi=10.1007%2Fs10489-018-1296-x&amp;volume=49&amp;pages=581-591&amp;publication_year=2019&amp;author=Zhao%2CX&amp;author=Ding%2CS&amp;author=An%2CY&amp;author=Jia%2CW">
                    Google Scholar</a>
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="31."><p class="c-article-references__text" id="ref-CR31">Ding S, Zhao X, Xu X, Sun T, Jia W (2019) An effective asynchronous framework for small scale reinforcement learning problems. Appl Intell 49:43034318</p><p class="c-article-references__links u-hide-print"><a data-track="click_references" rel="noopener" data-track-label="10.1007/s10489-019-01501-9" data-track-item_id="10.1007/s10489-019-01501-9" data-track-value="article reference" data-track-action="article reference" href="https://link.springer.com/doi/10.1007/s10489-019-01501-9" aria-label="Article reference 31" data-doi="10.1007/s10489-019-01501-9">Article</a>
    <a data-track="click_references" data-track-action="google scholar reference" data-track-value="google scholar reference" data-track-label="link" data-track-item_id="link" rel="nofollow noopener" aria-label="Google Scholar reference 31" href="http://scholar.google.com/scholar_lookup?&amp;title=An%20effective%20asynchronous%20framework%20for%20small%20scale%20reinforcement%20learning%20problems&amp;journal=Appl%20Intell&amp;doi=10.1007%2Fs10489-019-01501-9&amp;volume=49&amp;pages=4303-4318&amp;publication_year=2019&amp;author=Ding%2CS&amp;author=Zhao%2CX&amp;author=Xu%2CX&amp;author=Sun%2CT&amp;author=Jia%2CW">
                    Google Scholar</a>
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="32."><p class="c-article-references__text" id="ref-CR32">Fan Y, Watkins O, Du Y, Liu H, Ryu M, Boutilier C, Abbeel P, Ghavamzadeh M, Lee K, Lee K (2023) Reinforcement learning for fine-tuning text-to-image diffusion models. In: Proceedings of the annual conference on neural information processing systems (NeurIPS23), New Orleans, LA, USA</p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="33."><p class="c-article-references__text" id="ref-CR33">Alrebdi N, Alrumiah S, Almansour A, Rassam M (2022) Reinforcement learning in image classification: a review. In: Proceedings of the international conference on computing and information technology (ICCIT22), Tabuk, Saudi Arabia. IEEE, pp 7986</p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="34."><p class="c-article-references__text" id="ref-CR34">Jiu M, Song X, Sahbi H, Li S, Chen Y, Guo W, Guo L, Xu M (2024) Image classification with deep reinforcement active learning. <a href="http://arxiv.org/abs/2412.19877" data-track="click_references" data-track-action="external reference" data-track-value="external reference" data-track-label="http://arxiv.org/abs/2412.19877">arXiv:2412.19877</a></p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="35."><p class="c-article-references__text" id="ref-CR35">Zhou SK, Le HN, Luu K, Nguyen HV, Ayache N (2021) Deep reinforcement learning in medical imaging: a literature review. Med Image Anal 73:102193</p><p class="c-article-references__links u-hide-print"><a data-track="click_references" rel="nofollow noopener" data-track-label="10.1016/j.media.2021.102193" data-track-item_id="10.1016/j.media.2021.102193" data-track-value="article reference" data-track-action="article reference" href="https://doi.org/10.1016%2Fj.media.2021.102193" aria-label="Article reference 35" data-doi="10.1016/j.media.2021.102193">Article</a>
    <a data-track="click_references" data-track-action="google scholar reference" data-track-value="google scholar reference" data-track-label="link" data-track-item_id="link" rel="nofollow noopener" aria-label="Google Scholar reference 35" href="http://scholar.google.com/scholar_lookup?&amp;title=Deep%20reinforcement%20learning%20in%20medical%20imaging%3A%20a%20literature%20review&amp;journal=Med%20Image%20Anal&amp;doi=10.1016%2Fj.media.2021.102193&amp;volume=73&amp;publication_year=2021&amp;author=Zhou%2CSK&amp;author=Le%2CHN&amp;author=Luu%2CK&amp;author=Nguyen%2CHV&amp;author=Ayache%2CN">
                    Google Scholar</a>
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="36."><p class="c-article-references__text" id="ref-CR36">Gupta SK (2020) Reinforcement based learning on classification task could yield better generalization and adversarial accuracy. In: Proceedings of the annual conference on neural information processing systems workshop on shared visual representations in human and machine intelligence (SVRHM@NeurIPS20), Virtual event</p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="37."><p class="c-article-references__text" id="ref-CR37">Uzkent B, Yeh C, Ermon S (2020) Efficient object detection in large images using deep reinforcement learning. In: Proceedings of the IEEE/CVF winter conference on applications of computer vision (WACV20), Snowmass Village, CO, USA, pp 18241833</p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="38."><p class="c-article-references__text" id="ref-CR38">Moslemi A (2023) A tutorial-based survey on feature selection: recent advancements on feature selection. Eng Appl Artif Intell 126:107136</p><p class="c-article-references__links u-hide-print"><a data-track="click_references" rel="nofollow noopener" data-track-label="10.1016/j.engappai.2023.107136" data-track-item_id="10.1016/j.engappai.2023.107136" data-track-value="article reference" data-track-action="article reference" href="https://doi.org/10.1016%2Fj.engappai.2023.107136" aria-label="Article reference 38" data-doi="10.1016/j.engappai.2023.107136">Article</a>
    <a data-track="click_references" data-track-action="google scholar reference" data-track-value="google scholar reference" data-track-label="link" data-track-item_id="link" rel="nofollow noopener" aria-label="Google Scholar reference 38" href="http://scholar.google.com/scholar_lookup?&amp;title=A%20tutorial-based%20survey%20on%20feature%20selection%3A%20recent%20advancements%20on%20feature%20selection&amp;journal=Eng%20Appl%20Artif%20Intell&amp;doi=10.1016%2Fj.engappai.2023.107136&amp;volume=126&amp;publication_year=2023&amp;author=Moslemi%2CA">
                    Google Scholar</a>
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="39."><p class="c-article-references__text" id="ref-CR39">Paniri M, Dowlatshahi MB, Nezamabadi-pour H (2021) Ant-TD: ant colony optimization plus temporal difference reinforcement learning for multi-label feature selection. Swarm Evol Comput 64:100892</p><p class="c-article-references__links u-hide-print"><a data-track="click_references" rel="nofollow noopener" data-track-label="10.1016/j.swevo.2021.100892" data-track-item_id="10.1016/j.swevo.2021.100892" data-track-value="article reference" data-track-action="article reference" href="https://doi.org/10.1016%2Fj.swevo.2021.100892" aria-label="Article reference 39" data-doi="10.1016/j.swevo.2021.100892">Article</a>
    <a data-track="click_references" data-track-action="google scholar reference" data-track-value="google scholar reference" data-track-label="link" data-track-item_id="link" rel="nofollow noopener" aria-label="Google Scholar reference 39" href="http://scholar.google.com/scholar_lookup?&amp;title=Ant-TD%3A%20ant%20colony%20optimization%20plus%20temporal%20difference%20reinforcement%20learning%20for%20multi-label%20feature%20selection&amp;journal=Swarm%20Evol%20Comput&amp;doi=10.1016%2Fj.swevo.2021.100892&amp;volume=64&amp;publication_year=2021&amp;author=Paniri%2CM&amp;author=Dowlatshahi%2CMB&amp;author=Nezamabadi-pour%2CH">
                    Google Scholar</a>
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="40."><p class="c-article-references__text" id="ref-CR40">Zhang L, Jin L, Gan M, Zhao L, Yin H (2023) Reinforced feature selection using Q-learning based on collaborative agents. Int J Mach Learn Cybern 14(11):38673882</p><p class="c-article-references__links u-hide-print"><a data-track="click_references" rel="noopener" data-track-label="10.1007/s13042-023-01869-8" data-track-item_id="10.1007/s13042-023-01869-8" data-track-value="article reference" data-track-action="article reference" href="https://link.springer.com/doi/10.1007/s13042-023-01869-8" aria-label="Article reference 40" data-doi="10.1007/s13042-023-01869-8">Article</a>
    <a data-track="click_references" data-track-action="google scholar reference" data-track-value="google scholar reference" data-track-label="link" data-track-item_id="link" rel="nofollow noopener" aria-label="Google Scholar reference 40" href="http://scholar.google.com/scholar_lookup?&amp;title=Reinforced%20feature%20selection%20using%20Q-learning%20based%20on%20collaborative%20agents&amp;journal=Int%20J%20Mach%20Learn%20Cybern&amp;doi=10.1007%2Fs13042-023-01869-8&amp;volume=14&amp;issue=11&amp;pages=3867-3882&amp;publication_year=2023&amp;author=Zhang%2CL&amp;author=Jin%2CL&amp;author=Gan%2CM&amp;author=Zhao%2CL&amp;author=Yin%2CH">
                    Google Scholar</a>
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="41."><p class="c-article-references__text" id="ref-CR41">Liu K, Fu Y, Wu L, Li X, Aggarwal C, Xiong H (2021) Automated feature selection: a reinforcement learning perspective. IEEE Trans Knowl Data Eng 35(3):22722284</p><p class="c-article-references__links u-hide-print"><a data-track="click_references" data-track-action="google scholar reference" data-track-value="google scholar reference" data-track-label="link" data-track-item_id="link" rel="nofollow noopener" aria-label="Google Scholar reference 41" href="http://scholar.google.com/scholar_lookup?&amp;title=Automated%20feature%20selection%3A%20a%20reinforcement%20learning%20perspective&amp;journal=IEEE%20Trans%20Knowl%20Data%20Eng&amp;volume=35&amp;issue=3&amp;pages=2272-2284&amp;publication_year=2021&amp;author=Liu%2CK&amp;author=Fu%2CY&amp;author=Wu%2CL&amp;author=Li%2CX&amp;author=Aggarwal%2CC&amp;author=Xiong%2CH">
                    Google Scholar</a>
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="42."><p class="c-article-references__text" id="ref-CR42">Wang K, Liu Z, Lin Y, Lin J, Han S (2019) Haq: hardware-aware automated quantization with mixed precision. In: Proceedings of the international conference on computer vision and pattern recognition (CVPR19), Long Beach, CA, USA, pp 86128620</p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="43."><p class="c-article-references__text" id="ref-CR43">Gong Y, Liu Y, Yang M, Bourdev L (2014) Compressing deep convolutional networks using vector quantization. <a href="http://arxiv.org/abs/1412.6115" data-track="click_references" data-track-action="external reference" data-track-value="external reference" data-track-label="http://arxiv.org/abs/1412.6115">arXiv:1412.6115</a></p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="44."><p class="c-article-references__text" id="ref-CR44">Yuan Z, Xue C, Chen Y, Wu Q, Sun G (2022) Ptq4vit: Post-training quantization for vision transformers with twin uniform quantization. In: Proc. of the European Conference on Computer Vision (ECCV22), Tel Aviv, Israel. Springer, pp 191207</p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="45."><p class="c-article-references__text" id="ref-CR45">Lin Y, Zhang T, Sun P, Li Z, Zhou S (2022) FQ-ViT: post-training quantization for fully quantized vision transformer. In: Proceedings of the international joint conference on artificial intelligence (IJCAI22), Vienna, Austria, pp 11731179</p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="46."><p class="c-article-references__text" id="ref-CR46">Ding Y, Qin H, Yan Q, Chai Z, Liu J, Wei X, Liu X (2022) Towards accurate post-training quantization for vision transformer. In: Proceedings of the international conference on multimedia (MM22), Lisbon, Portugal, pp 53805388</p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="47."><p class="c-article-references__text" id="ref-CR47">Li Z, Yang T, Wang P, Cheng J (2022) Q-vit: fully differentiable quantization for vision transformer. <a href="http://arxiv.org/abs/2201.07703" data-track="click_references" data-track-action="external reference" data-track-value="external reference" data-track-label="http://arxiv.org/abs/2201.07703">arXiv:2201.07703</a></p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="48."><p class="c-article-references__text" id="ref-CR48">Liu Z, Wang Y, Han K, Zhang W, Ma S, Gao W (2021) Post-training quantization for vision transformer. Adv Neural Inf Process Syst 34:2809228103</p><p class="c-article-references__links u-hide-print"><a data-track="click_references" data-track-action="google scholar reference" data-track-value="google scholar reference" data-track-label="link" data-track-item_id="link" rel="nofollow noopener" aria-label="Google Scholar reference 48" href="http://scholar.google.com/scholar_lookup?&amp;title=Post-training%20quantization%20for%20vision%20transformer&amp;journal=Adv%20Neural%20Inf%20Process%20Syst&amp;volume=34&amp;pages=28092-28103&amp;publication_year=2021&amp;author=Liu%2CZ&amp;author=Wang%2CY&amp;author=Han%2CK&amp;author=Zhang%2CW&amp;author=Ma%2CS&amp;author=Gao%2CW">
                    Google Scholar</a>
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="49."><p class="c-article-references__text" id="ref-CR49">He Y, Zhang X, Sun J (2017) Channel pruning for accelerating very deep neural networks. In: Proceedings of the international conference on computer vision (ICCV17), Venice, Italy, pp 13891397</p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="50."><p class="c-article-references__text" id="ref-CR50">Rao Y, Lu J, Lin J, Zhou J (2018) Runtime network routing for efficient image classification. IEEE Trans Pattern Anal Mach Intell 41(10):22912304</p><p class="c-article-references__links u-hide-print"><a data-track="click_references" rel="nofollow noopener" data-track-label="10.1109/TPAMI.2018.2878258" data-track-item_id="10.1109/TPAMI.2018.2878258" data-track-value="article reference" data-track-action="article reference" href="https://doi.org/10.1109%2FTPAMI.2018.2878258" aria-label="Article reference 50" data-doi="10.1109/TPAMI.2018.2878258">Article</a>
    <a data-track="click_references" data-track-action="google scholar reference" data-track-value="google scholar reference" data-track-label="link" data-track-item_id="link" rel="nofollow noopener" aria-label="Google Scholar reference 50" href="http://scholar.google.com/scholar_lookup?&amp;title=Runtime%20network%20routing%20for%20efficient%20image%20classification&amp;journal=IEEE%20Trans%20Pattern%20Anal%20Mach%20Intell&amp;doi=10.1109%2FTPAMI.2018.2878258&amp;volume=41&amp;issue=10&amp;pages=2291-2304&amp;publication_year=2018&amp;author=Rao%2CY&amp;author=Lu%2CJ&amp;author=Lin%2CJ&amp;author=Zhou%2CJ">
                    Google Scholar</a>
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="51."><p class="c-article-references__text" id="ref-CR51">Zhu M, Tang Y, Han K (2021) Vision transformer pruning. <a href="http://arxiv.org/abs/2104.08500" data-track="click_references" data-track-action="external reference" data-track-value="external reference" data-track-label="http://arxiv.org/abs/2104.08500">arXiv:2104.08500</a></p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="52."><p class="c-article-references__text" id="ref-CR52">Yu F, Huang K, Wang M, Cheng Y, Chu W, Cui L (2022) Width &amp; depth pruning for vision transformers. In: Proceedings of the international conference on artificial intelligence (AAAI22), vol. 36. Virtual event, pp 31433151</p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="53."><p class="c-article-references__text" id="ref-CR53">Yu X, Liu T, Wang X, Tao D (2017) On compressing deep models by low rank and sparse decomposition. In: Proceedings of the international conference on computer vision and pattern recognition (CVPR17), Honolulu, HI, USA, pp 73707379</p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="54."><p class="c-article-references__text" id="ref-CR54">Jaderberg M, Vedaldi A, Zisserman A (2014) Speeding up convolutional neural networks with low rank expansions. <a href="http://arxiv.org/abs/1405.3866" data-track="click_references" data-track-action="external reference" data-track-value="external reference" data-track-label="http://arxiv.org/abs/1405.3866">arXiv:1405.3866</a></p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="55."><p class="c-article-references__text" id="ref-CR55">Hinton G, Vinyals O, Dean J (2015) Distilling the knowledge in a neural network. <a href="http://arxiv.org/abs/1503.02531" data-track="click_references" data-track-action="external reference" data-track-value="external reference" data-track-label="http://arxiv.org/abs/1503.02531">arXiv:1503.02531</a></p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="56."><p class="c-article-references__text" id="ref-CR56">Liu B, Rao Y, Lu J, Zhou J, Hsieh CJ (2020) Metadistiller: network self-boosting via meta-learned top-down distillation. In: Proceedings of the european conference on computer vision (ECCV20), Glasgow, Scotland, UK. Springer, pp 694709</p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="57."><p class="c-article-references__text" id="ref-CR57">Wang W, Wei F, Dong L, Bao H, Yang N, Zhou M (2020) Minilm: deep self-attention distillation for task-agnostic compression of pre-trained transformers. Adv Neural Inf Process Syst 33:57765788</p><p class="c-article-references__links u-hide-print"><a data-track="click_references" data-track-action="google scholar reference" data-track-value="google scholar reference" data-track-label="link" data-track-item_id="link" rel="nofollow noopener" aria-label="Google Scholar reference 57" href="http://scholar.google.com/scholar_lookup?&amp;title=Minilm%3A%20deep%20self-attention%20distillation%20for%20task-agnostic%20compression%20of%20pre-trained%20transformers&amp;journal=Adv%20Neural%20Inf%20Process%20Syst&amp;volume=33&amp;pages=5776-5788&amp;publication_year=2020&amp;author=Wang%2CW&amp;author=Wei%2CF&amp;author=Dong%2CL&amp;author=Bao%2CH&amp;author=Yang%2CN&amp;author=Zhou%2CM">
                    Google Scholar</a>
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="58."><p class="c-article-references__text" id="ref-CR58">Liu Y, Bech P, Tamura K, Dlez LT, Crochet S, Petersen CCH (2024) Cell class-specific long-range axonal projections of neurons in mouse whisker-related somatosensory cortices. eLife 13:97602. <a href="https://doi.org/10.7554/eLife.97602.3" data-track="click_references" data-track-action="external reference" data-track-value="external reference" data-track-label="10.7554/eLife.97602.3">https://doi.org/10.7554/eLife.97602.3</a></p><p class="c-article-references__links u-hide-print"><a data-track="click_references" rel="nofollow noopener" data-track-label="10.7554/eLife.97602.3" data-track-item_id="10.7554/eLife.97602.3" data-track-value="article reference" data-track-action="article reference" href="https://doi.org/10.7554%2FeLife.97602.3" aria-label="Article reference 58" data-doi="10.7554/eLife.97602.3">Article</a>
    <a data-track="click_references" data-track-action="google scholar reference" data-track-value="google scholar reference" data-track-label="link" data-track-item_id="link" rel="nofollow noopener" aria-label="Google Scholar reference 58" href="http://scholar.google.com/scholar_lookup?&amp;title=Cell%20class-specific%20long-range%20axonal%20projections%20of%20neurons%20in%20mouse%20whisker-related%20somatosensory%20cortices&amp;journal=eLife&amp;doi=10.7554%2FeLife.97602.3&amp;volume=13&amp;publication_year=2024&amp;author=Liu%2CY&amp;author=Bech%2CP&amp;author=Tamura%2CK&amp;author=D%C3%A9lez%2CLT&amp;author=Crochet%2CS&amp;author=Petersen%2CCCH">
                    Google Scholar</a>
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="59."><p class="c-article-references__text" id="ref-CR59">Chen X, Cao Q, Zhong Y, Zhang J, Gao S, Tao D (2022) Dearkd: data-efficient early knowledge distillation for vision transformers. In: Proceedings of the international conference on computer vision and pattern recognition (CVPR22), New Orleans, LA, USA, pp 1205212062</p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="60."><p class="c-article-references__text" id="ref-CR60">Touvron H, Cord M, Douze M, Massa F, Sablayrolles A, Jgo H (2021) Training data-efficient image transformers &amp; distillation through attention. In: Proceedings of the International Conference on Machine Learning (ICML21), Virtual event. PMLR, pp 1034710357</p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="61."><p class="c-article-references__text" id="ref-CR61">Jiao X, Yin Y, Shang L, Jiang X, Chen X, Li L, Wang F, Liu Q (2020) TinyBERT: distilling BERT for natural language understanding. In: Findings of the association for computational linguistics: EMNLP20. Virtual event. Association for Computational Linguistics, pp 41634174</p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="62."><p class="c-article-references__text" id="ref-CR62">Guo Q, Qiu X, Liu P, Shao Y, Xue X, Zhang Z (2019) Star-transformer. <a href="http://arxiv.org/abs/1902.09113" data-track="click_references" data-track-action="external reference" data-track-value="external reference" data-track-label="http://arxiv.org/abs/1902.09113">arXiv:1902.09113</a></p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="63."><p class="c-article-references__text" id="ref-CR63">Roy A, Saffar M, Vaswani A, Grangier D (2021) Efficient content-based sparse attention with routing transformers. Trans Assoc Comput Ling 9:5368</p><p class="c-article-references__links u-hide-print"><a data-track="click_references" data-track-action="google scholar reference" data-track-value="google scholar reference" data-track-label="link" data-track-item_id="link" rel="nofollow noopener" aria-label="Google Scholar reference 63" href="http://scholar.google.com/scholar_lookup?&amp;title=Efficient%20content-based%20sparse%20attention%20with%20routing%20transformers&amp;journal=Trans%20Assoc%20Comput%20Ling&amp;volume=9&amp;pages=53-68&amp;publication_year=2021&amp;author=Roy%2CA&amp;author=Saffar%2CM&amp;author=Vaswani%2CA&amp;author=Grangier%2CD">
                    Google Scholar</a>
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="64."><p class="c-article-references__text" id="ref-CR64">Gregor K, Danihelka I, Graves A, Rezende D, Wierstra D (2015) Draw: a recurrent neural network for image generation. In: Proceedings of the international conference on machine learning (ICML15), Lille, France. PMLR, pp 14621471</p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="65."><p class="c-article-references__text" id="ref-CR65">Li Y, Liu A, Fu X, Mckeown MJ, Wang ZJ, Chen X (2022) Atlas-guided parcellation: individualized functionally-homogenous parcellation in cerebral cortex. Comput Biol Med 150:106078</p><p class="c-article-references__links u-hide-print"><a data-track="click_references" rel="nofollow noopener" data-track-label="10.1016/j.compbiomed.2022.106078" data-track-item_id="10.1016/j.compbiomed.2022.106078" data-track-value="article reference" data-track-action="article reference" href="https://doi.org/10.1016%2Fj.compbiomed.2022.106078" aria-label="Article reference 65" data-doi="10.1016/j.compbiomed.2022.106078">Article</a>
    <a data-track="click_references" data-track-action="google scholar reference" data-track-value="google scholar reference" data-track-label="link" data-track-item_id="link" rel="nofollow noopener" aria-label="Google Scholar reference 65" href="http://scholar.google.com/scholar_lookup?&amp;title=Atlas-guided%20parcellation%3A%20individualized%20functionally-homogenous%20parcellation%20in%20cerebral%20cortex&amp;journal=Comput%20Biol%20Med&amp;doi=10.1016%2Fj.compbiomed.2022.106078&amp;volume=150&amp;publication_year=2022&amp;author=Li%2CY&amp;author=Liu%2CA&amp;author=Fu%2CX&amp;author=Mckeown%2CMJ&amp;author=Wang%2CZJ&amp;author=Chen%2CX">
                    Google Scholar</a>
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="66."><p class="c-article-references__text" id="ref-CR66">Grainger R, Paniagua T, Song X, Cuntoor N, Lee MW, Wu T (2023) PaCa-ViT: learning patch-to-cluster attention in vision transformers. In: Proceedings of the IEEE/CVF conference on computer vision and pattern recognition (CVPR23), Vancouver, British Columbia, Canada, pp 1856818578</p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="67."><p class="c-article-references__text" id="ref-CR67">Kim M, Gao S, Hsu YC, Shen Y, Jin H (2024) Token fusion: bridging the gap between token pruning and token merging. In: Proceedings of the IEEE/CVF winter conference on applications of computer vision (WCAV24), Waikoloa, HI, USA, pp 13831392</p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="68."><p class="c-article-references__text" id="ref-CR68">Meng L, Li H, Chen BC, Lan S, Wu Z, Jiang YG, Lim SN (2022) Adavit: adaptive vision transformers for efficient image recognition. In: Proceedings of the international conference on computer vision and pattern recognition (CVPR22), New Orleans, LA, USA, pp 1230912318</p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="69."><p class="c-article-references__text" id="ref-CR69">Le Lan C, Tu S, Oberman A, Agarwal R, Bellemare MG (2022) On the generalization of representations in reinforcement learning. In: Proceedings of the international conference on artificial intelligence and statistics (AISTATS22). Virtual event. PMLR, vol 151, pp 41324157</p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="70."><p class="c-article-references__text" id="ref-CR70">Arulkumaran K, Deisenroth MP, Brundage M, Bharath AA (2017) Deep reinforcement learning: a brief survey. IEEE Signal Process Mag 34(6):2638</p><p class="c-article-references__links u-hide-print"><a data-track="click_references" rel="nofollow noopener" data-track-label="10.1109/MSP.2017.2743240" data-track-item_id="10.1109/MSP.2017.2743240" data-track-value="article reference" data-track-action="article reference" href="https://doi.org/10.1109%2FMSP.2017.2743240" aria-label="Article reference 70" data-doi="10.1109/MSP.2017.2743240">Article</a>
    <a data-track="click_references" data-track-action="google scholar reference" data-track-value="google scholar reference" data-track-label="link" data-track-item_id="link" rel="nofollow noopener" aria-label="Google Scholar reference 70" href="http://scholar.google.com/scholar_lookup?&amp;title=Deep%20reinforcement%20learning%3A%20a%20brief%20survey&amp;journal=IEEE%20Signal%20Process%20Mag&amp;doi=10.1109%2FMSP.2017.2743240&amp;volume=34&amp;issue=6&amp;pages=26-38&amp;publication_year=2017&amp;author=Arulkumaran%2CK&amp;author=Deisenroth%2CMP&amp;author=Brundage%2CM&amp;author=Bharath%2CAA">
                    Google Scholar</a>
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="71."><p class="c-article-references__text" id="ref-CR71">Watkins CJCH, Dayan P (1992) Q-learning. Mach Learn 8:279292</p><p class="c-article-references__links u-hide-print"><a data-track="click_references" data-track-action="google scholar reference" data-track-value="google scholar reference" data-track-label="link" data-track-item_id="link" rel="nofollow noopener" aria-label="Google Scholar reference 71" href="http://scholar.google.com/scholar_lookup?&amp;title=Q-learning&amp;journal=Mach%20Learn&amp;volume=8&amp;pages=279-292&amp;publication_year=1992&amp;author=Watkins%2CCJCH&amp;author=Dayan%2CP">
                    Google Scholar</a>
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="72."><p class="c-article-references__text" id="ref-CR72">Fan J, Wang Z, Xie Y, Yang Z (2020) A theoretical analysis of deep Q-learning. In: Proceedings of the international conference on learning for dynamics and control (L4DC20), Berkeley, CA, USA. PMLR, pp 486489</p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="73."><p class="c-article-references__text" id="ref-CR73">Schulman J, Wolski F, Dhariwal P, Radford A, Klimov O (2017) Proximal policy optimization algorithms. <a href="http://arxiv.org/abs/1707.06347" data-track="click_references" data-track-action="external reference" data-track-value="external reference" data-track-label="http://arxiv.org/abs/1707.06347">arXiv:1707.06347</a></p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="74."><p class="c-article-references__text" id="ref-CR74">Mnih V, Badia AP, Mirza M, Graves A, Lillicrap T, Harley T, Silver D, Kavukcuoglu K (2016) Asynchronous methods for deep reinforcement learning. In: Proceedings of the international conference on machine learning and computer application (ICML2016), New York, NY, USA. PmLR, pp 19281937</p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="75."><p class="c-article-references__text" id="ref-CR75">De La Fuente N, Guerra DAV (2024) A comparative study of deep reinforcement learning models: DQN vs PPO vs A2C . In: Proceedings of the ACM knowledge discovery and data mining (ACM KDD24), Barcelona, Spain</p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="76."><p class="c-article-references__text" id="ref-CR76">Sutton RS, Barto AG (2018) Reinforcement learning: an introduction. MIT Press</p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="77."><p class="c-article-references__text" id="ref-CR77">Huber PJ (1992) Robust estimation of a location parameter. In: Breakthroughs in statistics: methodology and distribution. Springer, pp 492518</p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="78."><p class="c-article-references__text" id="ref-CR78">Liu R, Zou J (2018) The effects of memory replay in reinforcement learning. In: Proceedings of the annual allerton conference on communication, control, and computing (Allerton18), Monticello, IL, USA. IEEE, pp 478485</p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="79."><p class="c-article-references__text" id="ref-CR79">Lin LJ (1992) Self-improving reactive agents based on reinforcement learning, planning and teaching. Mach Learn 8:293321</p><p class="c-article-references__links u-hide-print"><a data-track="click_references" rel="nofollow noopener" data-track-label="10.1023/A:1022628806385" data-track-item_id="10.1023/A:1022628806385" data-track-value="article reference" data-track-action="article reference" href="https://doi.org/10.1023%2FA%3A1022628806385" aria-label="Article reference 79" data-doi="10.1023/A:1022628806385">Article</a>
    <a data-track="click_references" data-track-action="google scholar reference" data-track-value="google scholar reference" data-track-label="link" data-track-item_id="link" rel="nofollow noopener" aria-label="Google Scholar reference 79" href="http://scholar.google.com/scholar_lookup?&amp;title=Self-improving%20reactive%20agents%20based%20on%20reinforcement%20learning%2C%20planning%20and%20teaching&amp;journal=Mach%20Learn&amp;doi=10.1023%2FA%3A1022628806385&amp;volume=8&amp;pages=293-321&amp;publication_year=1992&amp;author=Lin%2CLJ">
                    Google Scholar</a>
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="80."><p class="c-article-references__text" id="ref-CR80">Even-Dar E, Mansour Y, Bartlett P (2003) Learning rates for Q-learning. J Mach Learn Res 5(1). MIT Press</p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="81."><p class="c-article-references__text" id="ref-CR81">Guo J (2023) Online influence maximization: concept and algorithm. <a href="http://arxiv.org/abs/2312.00099" data-track="click_references" data-track-action="external reference" data-track-value="external reference" data-track-label="http://arxiv.org/abs/2312.00099">arXiv:2312.00099</a></p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="82."><p class="c-article-references__text" id="ref-CR82">Chen W, Wang Y, Yuan Y (2013) Combinatorial multi-armed bandit: general framework and applications. In: Proceedings of the international conference on machine learning (ICML 2013). PMLR, pp 151159</p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="83."><p class="c-article-references__text" id="ref-CR83">Chen W, Hu W, Li F, Li J, Liu Y, Lu P (2016) Combinatorial multi-armed bandit with general reward functions. Adv Neural Inf Process Syst 29. NeurIPS</p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="84."><p class="c-article-references__text" id="ref-CR84">Beyer L, Zhai X, Kolesnikov A (2022) Better plain ViT baselines for ImageNet-1k. <a href="http://arxiv.org/abs/2205.01580" data-track="click_references" data-track-action="external reference" data-track-value="external reference" data-track-label="http://arxiv.org/abs/2205.01580">arXiv:2205.01580</a></p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="85."><p class="c-article-references__text" id="ref-CR85">Haurum JB, Escalera S, Taylor GW, Moeslund TB (2023) Which tokens to use? investigating token reduction in vision transformers. In: Proceedings of the IEEE/CVF international conference on computer vision (ICCV23), Paris, France, pp 773783</p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="86."><p class="c-article-references__text" id="ref-CR86">Zhang S, Sutton RS (2017) A deeper look at experience replay. <a href="http://arxiv.org/abs/1712.01275" data-track="click_references" data-track-action="external reference" data-track-value="external reference" data-track-label="http://arxiv.org/abs/1712.01275">arXiv:1712.01275</a></p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="87."><p class="c-article-references__text" id="ref-CR87">Akaike H (1974) A new look at the statistical model identification. IEEE Trans Autom Control 19(6):716723</p><p class="c-article-references__links u-hide-print"><a data-track="click_references" rel="nofollow noopener" data-track-label="10.1109/TAC.1974.1100705" data-track-item_id="10.1109/TAC.1974.1100705" data-track-value="article reference" data-track-action="article reference" href="https://doi.org/10.1109%2FTAC.1974.1100705" aria-label="Article reference 87" data-doi="10.1109/TAC.1974.1100705">Article</a>
    <a data-track="click_references" rel="nofollow noopener" data-track-label="link" data-track-item_id="link" data-track-value="mathscinet reference" data-track-action="mathscinet reference" href="http://www.ams.org/mathscinet-getitem?mr=423716" aria-label="MathSciNet reference 87">MathSciNet</a>
    <a data-track="click_references" data-track-action="google scholar reference" data-track-value="google scholar reference" data-track-label="link" data-track-item_id="link" rel="nofollow noopener" aria-label="Google Scholar reference 87" href="http://scholar.google.com/scholar_lookup?&amp;title=A%20new%20look%20at%20the%20statistical%20model%20identification&amp;journal=IEEE%20Trans%20Autom%20Control&amp;doi=10.1109%2FTAC.1974.1100705&amp;volume=19&amp;issue=6&amp;pages=716-723&amp;publication_year=1974&amp;author=Akaike%2CH">
                    Google Scholar</a>
                </p></li></ol><p class="c-article-references__download u-hide-print"><a data-track="click" data-track-action="download citation references" data-track-label="link" rel="nofollow" href="https://citation-needed.springer.com/v2/references/10.1007/s10489-025-06516-z?format=refman&amp;flavour=references">Download references<svg width="16" height="16" focusable="false" role="img" aria-hidden="true" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#icon-eds-i-download-medium"></use></svg></a></p></div></div></div></section></div><section data-title="Acknowledgements"><div class="c-article-section" id="Ack1-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Ack1">Acknowledgements</h2><div class="c-article-section__content" id="Ack1-content"><p>We acknowledge the support of the PNRR project FAIR - Future AI Research (PE00000013), Spoke 9 - AI, under the NRRP MUR program funded by the NextGenerationEU.</p></div></div></section><section data-title="Funding"><div class="c-article-section" id="Fun-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Fun">Funding</h2><div class="c-article-section__content" id="Fun-content"><p>Open access funding provided by Universit Politecnica delle Marche within the CRUI-CARE Agreement.</p></div></div></section><section aria-labelledby="author-information" data-title="Author information"><div class="c-article-section" id="author-information-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="author-information">Author information</h2><div class="c-article-section__content" id="author-information-content"><h3 class="c-article__sub-heading" id="affiliations">Authors and Affiliations</h3><ol class="c-article-author-affiliation__list"><li id="Aff1"><p class="c-article-author-affiliation__address">DIEM, University of Salerno, Fisciano, Italy</p><p class="c-article-author-affiliation__authors-list">Francesco Cauteruccio</p></li><li id="Aff2"><p class="c-article-author-affiliation__address">DII, Polytechnic University of Marche, Ancona, Italy</p><p class="c-article-author-affiliation__authors-list">Michele Marchetti,Domenico Ursino&amp;Luca Virgili</p></li><li id="Aff3"><p class="c-article-author-affiliation__address">CHIMOMO, University of Modena and Reggio Emilia, Modena, Italy</p><p class="c-article-author-affiliation__authors-list">Davide Traini</p></li></ol><div class="u-js-hide u-hide-print" data-test="author-info"><span class="c-article__sub-heading">Authors</span><ol class="c-article-authors-search u-list-reset"><li id="auth-Francesco-Cauteruccio-Aff1"><span class="c-article-authors-search__title u-h3 js-search-name">Francesco Cauteruccio</span><div class="c-article-authors-search__list"><div class="c-article-authors-search__item c-article-authors-search__list-item--left"><a href="/search?sortBy=newestFirst&amp;dc.creator=Francesco%20Cauteruccio" class="c-article-button" data-track="click" data-track-action="author link - publication" data-track-label="link" rel="nofollow">View author publications</a></div><div class="c-article-authors-search__item c-article-authors-search__list-item--right"><p class="search-in-title-js c-article-authors-search__text"><span class="c-article-authors-search__links-text">Search author on:</span><span class="c-article-identifiers"><a class="c-article-identifiers__item" href="https://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=Francesco%20Cauteruccio" data-track="click" data-track-action="author link - pubmed" data-track-label="link" rel="nofollow">PubMed</a><span class="u-hide"></span><a class="c-article-identifiers__item" href="https://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Francesco%20Cauteruccio%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" data-track="click" data-track-action="author link - scholar" data-track-label="link" rel="nofollow">Google Scholar</a></span></p></div></div></li><li id="auth-Michele-Marchetti-Aff2"><span class="c-article-authors-search__title u-h3 js-search-name">Michele Marchetti</span><div class="c-article-authors-search__list"><div class="c-article-authors-search__item c-article-authors-search__list-item--left"><a href="/search?sortBy=newestFirst&amp;dc.creator=Michele%20Marchetti" class="c-article-button" data-track="click" data-track-action="author link - publication" data-track-label="link" rel="nofollow">View author publications</a></div><div class="c-article-authors-search__item c-article-authors-search__list-item--right"><p class="search-in-title-js c-article-authors-search__text"><span class="c-article-authors-search__links-text">Search author on:</span><span class="c-article-identifiers"><a class="c-article-identifiers__item" href="https://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=Michele%20Marchetti" data-track="click" data-track-action="author link - pubmed" data-track-label="link" rel="nofollow">PubMed</a><span class="u-hide"></span><a class="c-article-identifiers__item" href="https://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Michele%20Marchetti%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" data-track="click" data-track-action="author link - scholar" data-track-label="link" rel="nofollow">Google Scholar</a></span></p></div></div></li><li id="auth-Davide-Traini-Aff3"><span class="c-article-authors-search__title u-h3 js-search-name">Davide Traini</span><div class="c-article-authors-search__list"><div class="c-article-authors-search__item c-article-authors-search__list-item--left"><a href="/search?sortBy=newestFirst&amp;dc.creator=Davide%20Traini" class="c-article-button" data-track="click" data-track-action="author link - publication" data-track-label="link" rel="nofollow">View author publications</a></div><div class="c-article-authors-search__item c-article-authors-search__list-item--right"><p class="search-in-title-js c-article-authors-search__text"><span class="c-article-authors-search__links-text">Search author on:</span><span class="c-article-identifiers"><a class="c-article-identifiers__item" href="https://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=Davide%20Traini" data-track="click" data-track-action="author link - pubmed" data-track-label="link" rel="nofollow">PubMed</a><span class="u-hide"></span><a class="c-article-identifiers__item" href="https://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Davide%20Traini%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" data-track="click" data-track-action="author link - scholar" data-track-label="link" rel="nofollow">Google Scholar</a></span></p></div></div></li><li id="auth-Domenico-Ursino-Aff2"><span class="c-article-authors-search__title u-h3 js-search-name">Domenico Ursino</span><div class="c-article-authors-search__list"><div class="c-article-authors-search__item c-article-authors-search__list-item--left"><a href="/search?sortBy=newestFirst&amp;dc.creator=Domenico%20Ursino" class="c-article-button" data-track="click" data-track-action="author link - publication" data-track-label="link" rel="nofollow">View author publications</a></div><div class="c-article-authors-search__item c-article-authors-search__list-item--right"><p class="search-in-title-js c-article-authors-search__text"><span class="c-article-authors-search__links-text">Search author on:</span><span class="c-article-identifiers"><a class="c-article-identifiers__item" href="https://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=Domenico%20Ursino" data-track="click" data-track-action="author link - pubmed" data-track-label="link" rel="nofollow">PubMed</a><span class="u-hide"></span><a class="c-article-identifiers__item" href="https://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Domenico%20Ursino%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" data-track="click" data-track-action="author link - scholar" data-track-label="link" rel="nofollow">Google Scholar</a></span></p></div></div></li><li id="auth-Luca-Virgili-Aff2"><span class="c-article-authors-search__title u-h3 js-search-name">Luca Virgili</span><div class="c-article-authors-search__list"><div class="c-article-authors-search__item c-article-authors-search__list-item--left"><a href="/search?sortBy=newestFirst&amp;dc.creator=Luca%20Virgili" class="c-article-button" data-track="click" data-track-action="author link - publication" data-track-label="link" rel="nofollow">View author publications</a></div><div class="c-article-authors-search__item c-article-authors-search__list-item--right"><p class="search-in-title-js c-article-authors-search__text"><span class="c-article-authors-search__links-text">Search author on:</span><span class="c-article-identifiers"><a class="c-article-identifiers__item" href="https://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=Luca%20Virgili" data-track="click" data-track-action="author link - pubmed" data-track-label="link" rel="nofollow">PubMed</a><span class="u-hide"></span><a class="c-article-identifiers__item" href="https://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Luca%20Virgili%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" data-track="click" data-track-action="author link - scholar" data-track-label="link" rel="nofollow">Google Scholar</a></span></p></div></div></li></ol></div><h3 class="c-article__sub-heading" id="corresponding-author">Corresponding author</h3><p id="corresponding-author-list">Correspondence to
                <a id="corresp-c1" aria-label="email Luca Virgili" href="mailto:luca.virgili@univpm.it">Luca Virgili</a>.</p></div></div></section><section data-title="Ethics declarations"><div class="c-article-section" id="ethics-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="ethics">Ethics declarations</h2><div class="c-article-section__content" id="ethics-content">
              
              
                <h3 class="c-article__sub-heading" id="FPar1">Competing Interest</h3>
                <p>The authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper.</p>
              
            </div></div></section><section data-title="Additional information"><div class="c-article-section" id="additional-information-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="additional-information">Additional information</h2><div class="c-article-section__content" id="additional-information-content"><h3 class="c-article__sub-heading">Publisher's Note</h3><p>Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.</p></div></div></section><section data-title="Rights and permissions"><div class="c-article-section" id="rightslink-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="rightslink">Rights and permissions</h2><div class="c-article-section__content" id="rightslink-content">
                <p><b>Open Access</b>  This article is licensed under a Creative Commons Attribution 4.0 International License, which permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons licence, and indicate if changes were made. The images or other third party material in this article are included in the article's Creative Commons licence, unless indicated otherwise in a credit line to the material. If material is not included in the article's Creative Commons licence and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this licence, visit <a href="http://creativecommons.org/licenses/by/4.0/" rel="license">http://creativecommons.org/licenses/by/4.0/</a>.</p>
              <p class="c-article-rights"><a data-track="click" data-track-action="view rights and permissions" data-track-label="link" href="https://s100.copyright.com/AppDispatchServlet?title=Adaptive%20patch%20selection%20to%20improve%20Vision%20Transformers%20through%20Reinforcement%20Learning&amp;author=Francesco%20Cauteruccio%20et%20al&amp;contentID=10.1007%2Fs10489-025-06516-z&amp;copyright=The%20Author%28s%29&amp;publication=0924-669X&amp;publicationDate=2025-04-01&amp;publisherName=SpringerNature&amp;orderBeanReset=true&amp;oa=CC%20BY">Reprints and permissions</a></p></div></div></section><section aria-labelledby="article-info" data-title="About this article"><div class="c-article-section" id="article-info-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="article-info">About this article</h2><div class="c-article-section__content" id="article-info-content"><div class="c-bibliographic-information"><div class="u-hide-print c-bibliographic-information__column c-bibliographic-information__column--border"><a data-crossmark="10.1007/s10489-025-06516-z" target="_blank" rel="noopener" href="https://crossmark.crossref.org/dialog/?doi=10.1007/s10489-025-06516-z" data-track="click" data-track-action="Click Crossmark" data-track-label="link" data-test="crossmark"><img loading="lazy" width="57" height="81" alt="Check for updates. Verify currency and authenticity via CrossMark" src="data:image/svg+xml;base64,<svg height="81" width="57" xmlns="http://www.w3.org/2000/svg"><g fill="none" fill-rule="evenodd"><path d="m17.35 35.45 21.3-14.2v-17.03h-21.3" fill="#989898"/><path d="m38.65 35.45-21.3-14.2v-17.03h21.3" fill="#747474"/><path d="m28 .5c-12.98 0-23.5 10.52-23.5 23.5s10.52 23.5 23.5 23.5 23.5-10.52 23.5-23.5c0-6.23-2.48-12.21-6.88-16.62-4.41-4.4-10.39-6.88-16.62-6.88zm0 41.25c-9.8 0-17.75-7.95-17.75-17.75s7.95-17.75 17.75-17.75 17.75 7.95 17.75 17.75c0 4.71-1.87 9.22-5.2 12.55s-7.84 5.2-12.55 5.2z" fill="#535353"/><path d="m41 36c-5.81 6.23-15.23 7.45-22.43 2.9-7.21-4.55-10.16-13.57-7.03-21.5l-4.92-3.11c-4.95 10.7-1.19 23.42 8.78 29.71 9.97 6.3 23.07 4.22 30.6-4.86z" fill="#9c9c9c"/><path d="m.2 58.45c0-.75.11-1.42.33-2.01s.52-1.09.91-1.5c.38-.41.83-.73 1.34-.94.51-.22 1.06-.32 1.65-.32.56 0 1.06.11 1.51.35.44.23.81.5 1.1.81l-.91 1.01c-.24-.24-.49-.42-.75-.56-.27-.13-.58-.2-.93-.2-.39 0-.73.08-1.05.23-.31.16-.58.37-.81.66-.23.28-.41.63-.53 1.04-.13.41-.19.88-.19 1.39 0 1.04.23 1.86.68 2.46.45.59 1.06.88 1.84.88.41 0 .77-.07 1.07-.23s.59-.39.85-.68l.91 1c-.38.43-.8.76-1.28.99-.47.22-1 .34-1.58.34-.59 0-1.13-.1-1.64-.31-.5-.2-.94-.51-1.31-.91-.38-.4-.67-.9-.88-1.48-.22-.59-.33-1.26-.33-2.02zm8.4-5.33h1.61v2.54l-.05 1.33c.29-.27.61-.51.96-.72s.76-.31 1.24-.31c.73 0 1.27.23 1.61.71.33.47.5 1.14.5 2.02v4.31h-1.61v-4.1c0-.57-.08-.97-.25-1.21-.17-.23-.45-.35-.83-.35-.3 0-.56.08-.79.22-.23.15-.49.36-.78.64v4.8h-1.61zm7.37 6.45c0-.56.09-1.06.26-1.51.18-.45.42-.83.71-1.14.29-.3.63-.54 1.01-.71.39-.17.78-.25 1.18-.25.47 0 .88.08 1.23.24.36.16.65.38.89.67s.42.63.54 1.03c.12.41.18.84.18 1.32 0 .32-.02.57-.07.76h-4.36c.07.62.29 1.1.65 1.44.36.33.82.5 1.38.5.29 0 .57-.04.83-.13s.51-.21.76-.37l.55 1.01c-.33.21-.69.39-1.09.53-.41.14-.83.21-1.26.21-.48 0-.92-.08-1.34-.25-.41-.16-.76-.4-1.07-.7-.31-.31-.55-.69-.72-1.13-.18-.44-.26-.95-.26-1.52zm4.6-.62c0-.55-.11-.98-.34-1.28-.23-.31-.58-.47-1.06-.47-.41 0-.77.15-1.07.45-.31.29-.5.73-.58 1.3zm2.5.62c0-.57.09-1.08.28-1.53.18-.44.43-.82.75-1.13s.69-.54 1.1-.71c.42-.16.85-.24 1.31-.24.45 0 .84.08 1.17.23s.61.34.85.57l-.77 1.02c-.19-.16-.38-.28-.56-.37-.19-.09-.39-.14-.61-.14-.56 0-1.01.21-1.35.63-.35.41-.52.97-.52 1.67 0 .69.17 1.24.51 1.66.34.41.78.62 1.32.62.28 0 .54-.06.78-.17.24-.12.45-.26.64-.42l.67 1.03c-.33.29-.69.51-1.08.65-.39.15-.78.23-1.18.23-.46 0-.9-.08-1.31-.24-.4-.16-.75-.39-1.05-.7s-.53-.69-.7-1.13c-.17-.45-.25-.96-.25-1.53zm6.91-6.45h1.58v6.17h.05l2.54-3.16h1.77l-2.35 2.8 2.59 4.07h-1.75l-1.77-2.98-1.08 1.23v1.75h-1.58zm13.69 1.27c-.25-.11-.5-.17-.75-.17-.58 0-.87.39-.87 1.16v.75h1.34v1.27h-1.34v5.6h-1.61v-5.6h-.92v-1.2l.92-.07v-.72c0-.35.04-.68.13-.98.08-.31.21-.57.4-.79s.42-.39.71-.51c.28-.12.63-.18 1.04-.18.24 0 .48.02.69.07.22.05.41.1.57.17zm.48 5.18c0-.57.09-1.08.27-1.53.17-.44.41-.82.72-1.13.3-.31.65-.54 1.04-.71.39-.16.8-.24 1.23-.24s.84.08 1.24.24c.4.17.74.4 1.04.71s.54.69.72 1.13c.19.45.28.96.28 1.53s-.09 1.08-.28 1.53c-.18.44-.42.82-.72 1.13s-.64.54-1.04.7-.81.24-1.24.24-.84-.08-1.23-.24-.74-.39-1.04-.7c-.31-.31-.55-.69-.72-1.13-.18-.45-.27-.96-.27-1.53zm1.65 0c0 .69.14 1.24.43 1.66.28.41.68.62 1.18.62.51 0 .9-.21 1.19-.62.29-.42.44-.97.44-1.66 0-.7-.15-1.26-.44-1.67-.29-.42-.68-.63-1.19-.63-.5 0-.9.21-1.18.63-.29.41-.43.97-.43 1.67zm6.48-3.44h1.33l.12 1.21h.05c.24-.44.54-.79.88-1.02.35-.24.7-.36 1.07-.36.32 0 .59.05.78.14l-.28 1.4-.33-.09c-.11-.01-.23-.02-.38-.02-.27 0-.56.1-.86.31s-.55.58-.77 1.1v4.2h-1.61zm-47.87 15h1.61v4.1c0 .57.08.97.25 1.2.17.24.44.35.81.35.3 0 .57-.07.8-.22.22-.15.47-.39.73-.73v-4.7h1.61v6.87h-1.32l-.12-1.01h-.04c-.3.36-.63.64-.98.86-.35.21-.76.32-1.24.32-.73 0-1.27-.24-1.61-.71-.33-.47-.5-1.14-.5-2.02zm9.46 7.43v2.16h-1.61v-9.59h1.33l.12.72h.05c.29-.24.61-.45.97-.63.35-.17.72-.26 1.1-.26.43 0 .81.08 1.15.24.33.17.61.4.84.71.24.31.41.68.53 1.11.13.42.19.91.19 1.44 0 .59-.09 1.11-.25 1.57-.16.47-.38.85-.65 1.16-.27.32-.58.56-.94.73-.35.16-.72.25-1.1.25-.3 0-.6-.07-.9-.2s-.59-.31-.87-.56zm0-2.3c.26.22.5.37.73.45.24.09.46.13.66.13.46 0 .84-.2 1.15-.6.31-.39.46-.98.46-1.77 0-.69-.12-1.22-.35-1.61-.23-.38-.61-.57-1.13-.57-.49 0-.99.26-1.52.77zm5.87-1.69c0-.56.08-1.06.25-1.51.16-.45.37-.83.65-1.14.27-.3.58-.54.93-.71s.71-.25 1.08-.25c.39 0 .73.07 1 .2.27.14.54.32.81.55l-.06-1.1v-2.49h1.61v9.88h-1.33l-.11-.74h-.06c-.25.25-.54.46-.88.64-.33.18-.69.27-1.06.27-.87 0-1.56-.32-2.07-.95s-.76-1.51-.76-2.65zm1.67-.01c0 .74.13 1.31.4 1.7.26.38.65.58 1.15.58.51 0 .99-.26 1.44-.77v-3.21c-.24-.21-.48-.36-.7-.45-.23-.08-.46-.12-.7-.12-.45 0-.82.19-1.13.59-.31.39-.46.95-.46 1.68zm6.35 1.59c0-.73.32-1.3.97-1.71.64-.4 1.67-.68 3.08-.84 0-.17-.02-.34-.07-.51-.05-.16-.12-.3-.22-.43s-.22-.22-.38-.3c-.15-.06-.34-.1-.58-.1-.34 0-.68.07-1 .2s-.63.29-.93.47l-.59-1.08c.39-.24.81-.45 1.28-.63.47-.17.99-.26 1.54-.26.86 0 1.51.25 1.93.76s.63 1.25.63 2.21v4.07h-1.32l-.12-.76h-.05c-.3.27-.63.48-.98.66s-.73.27-1.14.27c-.61 0-1.1-.19-1.48-.56-.38-.36-.57-.85-.57-1.46zm1.57-.12c0 .3.09.53.27.67.19.14.42.21.71.21.28 0 .54-.07.77-.2s.48-.31.73-.56v-1.54c-.47.06-.86.13-1.18.23-.31.09-.57.19-.76.31s-.33.25-.41.4c-.09.15-.13.31-.13.48zm6.29-3.63h-.98v-1.2l1.06-.07.2-1.88h1.34v1.88h1.75v1.27h-1.75v3.28c0 .8.32 1.2.97 1.2.12 0 .24-.01.37-.04.12-.03.24-.07.34-.11l.28 1.19c-.19.06-.4.12-.64.17-.23.05-.49.08-.76.08-.4 0-.74-.06-1.02-.18-.27-.13-.49-.3-.67-.52-.17-.21-.3-.48-.37-.78-.08-.3-.12-.64-.12-1.01zm4.36 2.17c0-.56.09-1.06.27-1.51s.41-.83.71-1.14c.29-.3.63-.54 1.01-.71.39-.17.78-.25 1.18-.25.47 0 .88.08 1.23.24.36.16.65.38.89.67s.42.63.54 1.03c.12.41.18.84.18 1.32 0 .32-.02.57-.07.76h-4.37c.08.62.29 1.1.65 1.44.36.33.82.5 1.38.5.3 0 .58-.04.84-.13.25-.09.51-.21.76-.37l.54 1.01c-.32.21-.69.39-1.09.53s-.82.21-1.26.21c-.47 0-.92-.08-1.33-.25-.41-.16-.77-.4-1.08-.7-.3-.31-.54-.69-.72-1.13-.17-.44-.26-.95-.26-1.52zm4.61-.62c0-.55-.11-.98-.34-1.28-.23-.31-.58-.47-1.06-.47-.41 0-.77.15-1.08.45-.31.29-.5.73-.57 1.3zm3.01 2.23c.31.24.61.43.92.57.3.13.63.2.98.2.38 0 .65-.08.83-.23s.27-.35.27-.6c0-.14-.05-.26-.13-.37-.08-.1-.2-.2-.34-.28-.14-.09-.29-.16-.47-.23l-.53-.22c-.23-.09-.46-.18-.69-.3-.23-.11-.44-.24-.62-.4s-.33-.35-.45-.55c-.12-.21-.18-.46-.18-.75 0-.61.23-1.1.68-1.49.44-.38 1.06-.57 1.83-.57.48 0 .91.08 1.29.25s.71.36.99.57l-.74.98c-.24-.17-.49-.32-.73-.42-.25-.11-.51-.16-.78-.16-.35 0-.6.07-.76.21-.17.15-.25.33-.25.54 0 .14.04.26.12.36s.18.18.31.26c.14.07.29.14.46.21l.54.19c.23.09.47.18.7.29s.44.24.64.4c.19.16.34.35.46.58.11.23.17.5.17.82 0 .3-.06.58-.17.83-.12.26-.29.48-.51.68-.23.19-.51.34-.84.45-.34.11-.72.17-1.15.17-.48 0-.95-.09-1.41-.27-.46-.19-.86-.41-1.2-.68z" fill="#535353"/></g></svg>"></a></div><div class="c-bibliographic-information__column"><h3 class="c-article__sub-heading" id="citeas">Cite this article</h3><p class="c-bibliographic-information__citation">Cauteruccio, F., Marchetti, M., Traini, D. <i>et al.</i> Adaptive patch selection to improve Vision Transformers through Reinforcement Learning.
                    <i>Appl Intell</i> <b>55</b>, 607 (2025). https://doi.org/10.1007/s10489-025-06516-z</p><p class="c-bibliographic-information__download-citation u-hide-print"><a data-test="citation-link" data-track="click" data-track-action="download article citation" data-track-label="link" data-track-external="" rel="nofollow" href="https://citation-needed.springer.com/v2/references/10.1007/s10489-025-06516-z?format=refman&amp;flavour=citation">Download citation<svg width="16" height="16" focusable="false" role="img" aria-hidden="true" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#icon-eds-i-download-medium"></use></svg></a></p><ul class="c-bibliographic-information__list" data-test="publication-history"><li class="c-bibliographic-information__list-item"><p>Accepted<span class="u-hide">: </span><span class="c-bibliographic-information__value"><time datetime="2025-03-25">25 March 2025</time></span></p></li><li class="c-bibliographic-information__list-item"><p>Published<span class="u-hide">: </span><span class="c-bibliographic-information__value"><time datetime="2025-04-01">01 April 2025</time></span></p></li><li class="c-bibliographic-information__list-item"><p>Version of record<span class="u-hide">: </span><span class="c-bibliographic-information__value"><time datetime="2025-04-01">01 April 2025</time></span></p></li><li class="c-bibliographic-information__list-item c-bibliographic-information__list-item--full-width"><p><abbr title="Digital Object Identifier">DOI</abbr><span class="u-hide">: </span><span class="c-bibliographic-information__value">https://doi.org/10.1007/s10489-025-06516-z</span></p></li></ul><div data-component="share-box"><div class="c-article-share-box u-display-none" hidden=""><h3 class="c-article__sub-heading">Share this article</h3><p class="c-article-share-box__description">Anyone you share the following link with will be able to read this content:</p><button class="js-get-share-url c-article-share-box__button" type="button" id="get-share-url" data-track="click" data-track-label="button" data-track-external="" data-track-action="get shareable link">Get shareable link</button><div class="js-no-share-url-container u-display-none" hidden=""><p class="js-c-article-share-box__no-sharelink-info c-article-share-box__no-sharelink-info">Sorry, a shareable link is not currently available for this article.</p></div><div class="js-share-url-container u-display-none" hidden=""><p class="js-share-url c-article-share-box__only-read-input" id="share-url" data-track="click" data-track-label="button" data-track-action="select share url"></p><button class="js-copy-share-url c-article-share-box__button--link-like" type="button" id="copy-share-url" data-track="click" data-track-label="button" data-track-action="copy share url" data-track-external="">Copy shareable link to clipboard</button></div><p class="js-c-article-share-box__additional-info c-article-share-box__additional-info">
                            Provided by the Springer Nature SharedIt content-sharing initiative
                        </p></div></div><h3 class="c-article__sub-heading">Keywords</h3><ul class="c-article-subject-list"><li class="c-article-subject-list__subject"><span><a href="/search?query=Vision%20transformers&amp;facet-discipline=&#34;Computer%20Science&#34;" data-track="click" data-track-action="view keyword" data-track-label="link">Vision transformers</a></span></li><li class="c-article-subject-list__subject"><span><a href="/search?query=Training%20time%20reduction&amp;facet-discipline=&#34;Computer%20Science&#34;" data-track="click" data-track-action="view keyword" data-track-label="link">Training time reduction</a></span></li><li class="c-article-subject-list__subject"><span><a href="/search?query=Reinforcement%20learning&amp;facet-discipline=&#34;Computer%20Science&#34;" data-track="click" data-track-action="view keyword" data-track-label="link">Reinforcement learning</a></span></li><li class="c-article-subject-list__subject"><span><a href="/search?query=Computer%20vision&amp;facet-discipline=&#34;Computer%20Science&#34;" data-track="click" data-track-action="view keyword" data-track-label="link">Computer vision</a></span></li></ul><div data-component="article-info-list"></div></div></div></div></div></section>

                    
    <div id="researcher-profile-container">
        <h3>Profiles</h3>
        <ol>
            
                <li data-test="researcher-profile-data"  data-profile-index="0">
                    <span data-test="researcher-profile-name">Francesco Cauteruccio</span>
                    <a class="js-cta-popup-link c-article-authors-search__cta-link" href="/researchers/51021894SN" data-track="click_view_profile" data-test="researcher-profile-link">
                        <span class="eds-c-button eds-c-button--primary">
                            <svg class="c-article-authors-search__cta-icon" aria-hidden="true" focusable="false" width="24" height="24">
                                <use xlink:href="#icon-eds-i-user-single-medium"></use>
                            </svg><span>View author profile</span>
                        </span>
                    </a>
                </li>
            
                <li data-test="researcher-profile-data"  data-profile-index="1">
                    <span data-test="researcher-profile-name">Michele Marchetti</span>
                    <a class="js-cta-popup-link c-article-authors-search__cta-link" href="/researchers/42858211SN" data-track="click_view_profile" data-test="researcher-profile-link">
                        <span class="eds-c-button eds-c-button--primary">
                            <svg class="c-article-authors-search__cta-icon" aria-hidden="true" focusable="false" width="24" height="24">
                                <use xlink:href="#icon-eds-i-user-single-medium"></use>
                            </svg><span>View author profile</span>
                        </span>
                    </a>
                </li>
            
                <li data-test="researcher-profile-data"  data-profile-index="3">
                    <span data-test="researcher-profile-name">Domenico Ursino</span>
                    <a class="js-cta-popup-link c-article-authors-search__cta-link" href="/researchers/16827008SN" data-track="click_view_profile" data-test="researcher-profile-link">
                        <span class="eds-c-button eds-c-button--primary">
                            <svg class="c-article-authors-search__cta-icon" aria-hidden="true" focusable="false" width="24" height="24">
                                <use xlink:href="#icon-eds-i-user-single-medium"></use>
                            </svg><span>View author profile</span>
                        </span>
                    </a>
                </li>
            
                <li data-test="researcher-profile-data"  data-profile-index="4">
                    <span data-test="researcher-profile-name">Luca Virgili</span>
                    <a class="js-cta-popup-link c-article-authors-search__cta-link" href="/researchers/55607304SN" data-track="click_view_profile" data-test="researcher-profile-link">
                        <span class="eds-c-button eds-c-button--primary">
                            <svg class="c-article-authors-search__cta-icon" aria-hidden="true" focusable="false" width="24" height="24">
                                <use xlink:href="#icon-eds-i-user-single-medium"></use>
                            </svg><span>View author profile</span>
                        </span>
                    </a>
                </li>
            
        </ol>
    </div>


                    
                </div>
            </main>

            <div class="c-article-sidebar u-text-sm u-hide-print l-with-sidebar__sidebar" id="sidebar"
                 data-container-type="reading-companion" data-track-component="reading companion">
                <aside aria-label="reading companion">
                    

                    
                        
    <div class="app-card-service" data-test="article-checklist-banner">
        <div>
            <a class="app-card-service__link" data-track="click_presubmission_checklist" data-track-context="article page top of reading companion" data-track-category="pre-submission-checklist" data-track-action="clicked article page checklist banner test 2 old version" data-track-label="link" href="https://beta.springernature.com/pre-submission?journalId=10489"
            data-test="article-checklist-banner-link">
            <span class="app-card-service__link-text">Use our pre-submission checklist</span>
            <svg class="app-card-service__link-icon" aria-hidden="true" focusable="false"><use xlink:href="#icon-eds-i-arrow-right-small"></use></svg>
            </a>
            <p class="app-card-service__description">Avoid common mistakes on your manuscript.</p>
        </div>
        <div class="app-card-service__icon-container">
            <svg class="app-card-service__icon" aria-hidden="true" focusable="false">
                <use xlink:href="#icon-eds-i-clipboard-check-medium"></use>
            </svg>
        </div>
    </div>

                    

                    
                        <div data-test="collections">
                            
    

                        </div>
                    

                    <div data-test="editorial-summary">
                        
                    </div>

                    <div class="c-reading-companion">
                        <div class="c-reading-companion__sticky" data-component="reading-companion-sticky"
                             data-test="reading-companion-sticky">
                            
                            
                                
                                    
                                
                            
                            <div
                                class="c-reading-companion__panel c-reading-companion__sections c-reading-companion__panel--active"
                                id="tabpanel-sections">
                                <div class="u-lazy-ad-wrapper u-mt-16 u-hide"
                                     data-component-mpu><div class="c-ad c-ad--300x250">
    <div class="c-ad__inner">
        <p class="c-ad__label">Advertisement</p>
        <div id="div-gpt-ad-MPU1"
             class="div-gpt-ad grade-c-hide"
             data-pa11y-ignore
             data-gpt
             data-gpt-unitpath="/270604982/springerlink/10489/article"
             data-gpt-sizes="300x250" data-test="MPU1-ad"
             data-gpt-targeting="pos=MPU1;articleid=s10489-025-06516-z;">
        </div>
    </div>
</div>

<script>
    window.SN = window.SN || {};
    window.SN.libs = window.SN.libs || {};
    window.SN.libs.ads = window.SN.libs.ads || {};
    window.SN.libs.ads.slotConfig = window.SN.libs.ads.slotConfig || {};
    window.SN.libs.ads.slotConfig['MPU1'] = {
        'pos': 'MPU1',
        'type': 'MPU1',
    };
    window.SN.libs.ads.slotConfig['unitPath'] = '/270604982/springerlink/10489/article';
</script>

</div>
                            </div>
                            <div
                                class="c-reading-companion__panel c-reading-companion__figures c-reading-companion__panel--full-width"
                                id="tabpanel-figures"></div>
                            <div
                                class="c-reading-companion__panel c-reading-companion__references c-reading-companion__panel--full-width"
                                id="tabpanel-references"></div>
                        </div>
                    </div>
                </aside>
            </div>
        </div>
    </article>
    
    <div class="app-elements" data-test="footer">
    <nav aria-label="expander navigation">



    
        <div class="eds-c-header__expander eds-c-header__expander--search" id="eds-c-header-popup-search">
            <h2 class="eds-c-header__heading">Search</h2>
            <div class="u-container">
                <search class="eds-c-header__search" role="search" aria-label="Search from the header">
                    <form method="GET" action="//link.springer.com/search"
                        
                            data-test="header-search"
                        
                            data-track="search"
                        
                            data-track-context="search from header"
                        
                            data-track-action="submit search form"
                        
                            data-track-category="unified header"
                        
                            data-track-label="form"
                        
					>
                        <label for="eds-c-header-search" class="eds-c-header__search-label">Search by keyword or author</label>
                        <div class="eds-c-header__search-container">
                            <input id="eds-c-header-search" class="eds-c-header__search-input" autocomplete="off" name="query" type="search" value="" required>
                            <button class="eds-c-header__search-button" type="submit">
                                <svg class="eds-c-header__icon" aria-hidden="true" focusable="false">
                                    <use xlink:href="#icon-eds-i-search-medium"></use>
                                </svg>
                                <span class="u-visually-hidden">Search</span>
                            </button>
                        </div>
                    </form>
                </search>
            </div>
        </div>
    


<div class="eds-c-header__expander eds-c-header__expander--menu" id="eds-c-header-nav">
    
        <h2 class="eds-c-header__heading">Navigation</h2>
        <ul class="eds-c-header__list">
            
                <li class="eds-c-header__list-item">
                   <a class="eds-c-header__link" href="https://link.springer.com/journals/"
                        
                            data-track="nav_find_a_journal"
                        
                            data-track-context="unified header"
                        
                            data-track-action="click find a journal"
                        
                            data-track-category="unified header"
                        
                            data-track-label="link"
                        
					>
                        Find a journal
                    </a>
                </li>
            
                <li class="eds-c-header__list-item">
                   <a class="eds-c-header__link" href="https://www.springernature.com/gp/authors"
                        
                            data-track="nav_how_to_publish"
                        
                            data-track-context="unified header"
                        
                            data-track-action="click publish with us link"
                        
                            data-track-category="unified header"
                        
                            data-track-label="link"
                        
					>
                        Publish with us
                    </a>
                </li>
            
                <li class="eds-c-header__list-item">
                   <a class="eds-c-header__link" href="https://link.springernature.com/home/"
                        
                            data-track="nav_track_your_research"
                        
                            data-track-context="unified header"
                        
                            data-track-action="click track your research"
                        
                            data-track-category="unified header"
                        
                            data-track-label="link"
                        
					>
                        Track your research
                    </a>
                </li>
            
        </ul>
    
</div>
</nav>
    <footer >
	<div class="eds-c-footer"
		
	>
		
			
				<div class="eds-c-footer__container">
		<div class="eds-c-footer__grid eds-c-footer__group--separator">
			
			<div class="eds-c-footer__group">
				<h3 class="eds-c-footer__heading">Discover content</h3>
				<ul class="eds-c-footer__list">
					
						<li class="eds-c-footer__item"><a class="eds-c-footer__link" href="https://link.springer.com/journals/a/1" data-track="nav_journals_a_z" data-track-action="journals a-z" data-track-context="unified footer" data-track-label="link">Journals A-Z</a></li>
					
						<li class="eds-c-footer__item"><a class="eds-c-footer__link" href="https://link.springer.com/books/a/1" data-track="nav_books_a_z" data-track-action="books a-z" data-track-context="unified footer" data-track-label="link">Books A-Z</a></li>
					
				</ul>
			</div>
			
			<div class="eds-c-footer__group">
				<h3 class="eds-c-footer__heading">Publish with us</h3>
				<ul class="eds-c-footer__list">
					
						<li class="eds-c-footer__item"><a class="eds-c-footer__link" href="https://link.springer.com/journals" data-track="nav_journal_finder" data-track-action="journal finder" data-track-context="unified footer" data-track-label="link">Journal finder</a></li>
					
						<li class="eds-c-footer__item"><a class="eds-c-footer__link" href="https://www.springernature.com/gp/authors" data-track="nav_publish_your_research" data-track-action="publish your research" data-track-context="unified footer" data-track-label="link">Publish your research</a></li>
					
						<li class="eds-c-footer__item"><a class="eds-c-footer__link" href="https://authorservices.springernature.com/go/sn/?utm_source&#x3D;SNLinkfooter&amp;utm_medium&#x3D;Web&amp;utm_campaign&#x3D;SNReferral" data-track="nav_language_editing" data-track-action="language editing" data-track-context="unified footer" data-track-label="link">Language editing</a></li>
					
						<li class="eds-c-footer__item"><a class="eds-c-footer__link" href="https://www.springernature.com/gp/open-science/about/the-fundamentals-of-open-access-and-open-research" data-track="nav_open_access_publishing" data-track-action="open access publishing" data-track-context="unified footer" data-track-label="link">Open access publishing</a></li>
					
				</ul>
			</div>
			
			<div class="eds-c-footer__group">
				<h3 class="eds-c-footer__heading">Products and services</h3>
				<ul class="eds-c-footer__list">
					
						<li class="eds-c-footer__item"><a class="eds-c-footer__link" href="https://www.springernature.com/gp/products" data-track="nav_our_products" data-track-action="our products" data-track-context="unified footer" data-track-label="link">Our products</a></li>
					
						<li class="eds-c-footer__item"><a class="eds-c-footer__link" href="https://www.springernature.com/gp/librarians" data-track="nav_librarians" data-track-action="librarians" data-track-context="unified footer" data-track-label="link">Librarians</a></li>
					
						<li class="eds-c-footer__item"><a class="eds-c-footer__link" href="https://www.springernature.com/gp/societies" data-track="nav_societies" data-track-action="societies" data-track-context="unified footer" data-track-label="link">Societies</a></li>
					
						<li class="eds-c-footer__item"><a class="eds-c-footer__link" href="https://www.springernature.com/gp/partners" data-track="nav_partners_and_advertisers" data-track-action="partners and advertisers" data-track-context="unified footer" data-track-label="link">Partners and advertisers</a></li>
					
				</ul>
			</div>
			
			<div class="eds-c-footer__group">
				<h3 class="eds-c-footer__heading">Our brands</h3>
				<ul class="eds-c-footer__list">
					
						<li class="eds-c-footer__item"><a class="eds-c-footer__link" href="https://link.springer.com/brands/springer" data-track="nav_imprint_Springer" data-track-action="Springer" data-track-context="unified footer" data-track-label="link">Springer</a></li>
					
						<li class="eds-c-footer__item"><a class="eds-c-footer__link" href="https://www.nature.com/" data-track="nav_imprint_Nature_Portfolio" data-track-action="Nature Portfolio" data-track-context="unified footer" data-track-label="link">Nature Portfolio</a></li>
					
						<li class="eds-c-footer__item"><a class="eds-c-footer__link" href="https://link.springer.com/brands/bmc" data-track="nav_imprint_BMC" data-track-action="BMC" data-track-context="unified footer" data-track-label="link">BMC</a></li>
					
						<li class="eds-c-footer__item"><a class="eds-c-footer__link" href="/brands/palgrave" data-track="nav_imprint_Palgrave_Macmillan" data-track-action="Palgrave Macmillan" data-track-context="unified footer" data-track-label="link">Palgrave Macmillan</a></li>
					
						<li class="eds-c-footer__item"><a class="eds-c-footer__link" href="https://www.apress.com/" data-track="nav_imprint_Apress" data-track-action="Apress" data-track-context="unified footer" data-track-label="link">Apress</a></li>
					
						<li class="eds-c-footer__item"><a class="eds-c-footer__link" href="https://link.springer.com/brands/discover" data-track="nav_imprint_Discover" data-track-action="Discover" data-track-context="unified footer" data-track-label="link">Discover</a></li>
					
				</ul>
			</div>
			
		</div>
	</div>

		
		
		<div class="eds-c-footer__container">
	
		<nav aria-label="footer navigation">
			<ul class="eds-c-footer__links">
				
					<li class="eds-c-footer__item">
						
						
							<button class="eds-c-footer__link" data-cc-action="preferences"
								 data-track="dialog_manage_cookies" data-track-action="Manage cookies" data-track-context="unified footer" data-track-label="link"><span class="eds-c-footer__button-text">Your privacy choices/Manage cookies</span></button>
						
					</li>
				
					<li class="eds-c-footer__item">
						
							<a class="eds-c-footer__link" href="https://www.springernature.com/gp/legal/ccpa"
								 data-track="nav_california_privacy_statement" data-track-action="california privacy statement" data-track-context="unified footer" data-track-label="link">Your US state privacy rights</a>
						
						
					</li>
				
					<li class="eds-c-footer__item">
						
							<a class="eds-c-footer__link" href="https://link.springer.com/accessibility"
								 data-track="nav_accessibility_statement" data-track-action="accessibility statement" data-track-context="unified footer" data-track-label="link">Accessibility statement</a>
						
						
					</li>
				
					<li class="eds-c-footer__item">
						
							<a class="eds-c-footer__link" href="https://link.springer.com/termsandconditions"
								 data-track="nav_terms_and_conditions" data-track-action="terms and conditions" data-track-context="unified footer" data-track-label="link">Terms and conditions</a>
						
						
					</li>
				
					<li class="eds-c-footer__item">
						
							<a class="eds-c-footer__link" href="https://link.springer.com/privacystatement"
								 data-track="nav_privacy_policy" data-track-action="privacy policy" data-track-context="unified footer" data-track-label="link">Privacy policy</a>
						
						
					</li>
				
					<li class="eds-c-footer__item">
						
							<a class="eds-c-footer__link" href="https://support.springernature.com/en/support/home"
								 data-track="nav_help_and_support" data-track-action="help and support" data-track-context="unified footer" data-track-label="link">Help and support</a>
						
						
					</li>
				
					<li class="eds-c-footer__item">
						
							<a class="eds-c-footer__link" href="https://link.springer.com/legal-notice"
								 data-track="nav_legal_notice" data-track-action="legal notice" data-track-context="unified footer" data-track-label="link">Legal notice</a>
						
						
					</li>
				
					<li class="eds-c-footer__item">
						
							<a class="eds-c-footer__link" href="https://support.springernature.com/en/support/solutions/articles/6000255911-subscription-cancellations"
								 data-track-action="cancel contracts here">Cancel contracts here</a>
						
						
					</li>
				
			</ul>
		</nav>
	
	
		
			<div class="eds-c-footer__user">
				<p class="eds-c-footer__user-info">
					
					<span data-test="footer-user-ip">71.193.247.28</span>
				</p>
				<p class="eds-c-footer__user-info" data-test="footer-business-partners">Not affiliated</p>
			</div>
		
	
	
		<a href="https://www.springernature.com/" class="eds-c-footer__link">
			<img src="/oscar-static/images/logo-springernature-white-19dd4ba190.svg" alt="Springer Nature" loading="lazy" width="200" height="20"/>
		</a>
	
	<p class="eds-c-footer__legal" data-test="copyright">&copy; 2025 Springer Nature</p>
</div>

	</div>
</footer>
</div>


    </body>
</html>


