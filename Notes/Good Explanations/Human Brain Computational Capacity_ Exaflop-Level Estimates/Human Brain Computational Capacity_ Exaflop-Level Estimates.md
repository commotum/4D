![](_page_0_Picture_0.jpeg)

# Human Brain Computational Capacity: Exaflop-Level Estimates

#### **Overview of Brain FLOPS Estimates**

Scientific attempts to quantify the human brain's computational capacity span a vast range. Various analyses have put the figure anywhere from *trillions* of FLOPS (10^12) up to an astronomical 10^28 FLOPS 1. Within this spectrum, many well-founded estimates cluster around the **exaflop scale (10^18 FLOPS)** – i.e. on the order of one *quintillion* operations per second. This section compiles **peer-reviewed studies**, **technical analyses**, **and expert statements** that specifically claim the brain operates at or near ~10^18 FLOPS, and explains what those comparisons mean (whether *theoretical equivalent performance*, *full-brain simulation requirements*, or *functional task performance*).

Notably, a 2008 technical workshop by Anders Sandberg and Nick Bostrom estimated that fully emulating a human brain would require ~10^18 FLOPS when modeled at the level of spiking neural networks <sup>2</sup>. (They gave higher requirements for even more detailed biochemical simulations, up to 10^22–10^25 FLOPS for molecular-level fidelity <sup>3</sup>.) In other words, an exaflop-scale computer could plausibly mimic the brain's high-level neural activity in real time under certain modeling assumptions <sup>4</sup>. This exaflop estimate is viewed as a *lower-bound* for whole-brain emulation, corresponding to simulating neurons and synapses as information-processing units <sup>4</sup>. For context, futurist Ray Kurzweil had earlier estimated the brain's raw processing power around *10 petaflops* (10^16 FLOPS) based on neural signaling rates <sup>5</sup>. However, many experts now argue this was conservative, and that *"the processing capacity of the human brain is actually far greater"* – likely in the exaflop ballpark <sup>5</sup>. In short, modern analyses tend to put the brain's effective compute closer to 10^18 FLOPS, an estimate which we will see echoed by neuroscientists and AI researchers alike.

# **Exaflop-Scale Requirements for Brain Simulation**

One line of evidence for the brain's exaflop-level complexity comes from **neuroscience and supercomputing** efforts to *simulate* the entire human brain. **Henry Markram**, lead of the Blue Brain and Human Brain Project, has repeatedly highlighted how today's largest computers still fall short of what's needed to replicate a biological brain's activity. Speaking at a 2011 supercomputing conference, Markram noted the human brain's extraordinary specs − "30 W [of power], a million kilometers of fibers, and a thousand trillion synapses" − and stated that "we'd need about an exascale [one exaflop, 10^18 FLOPS] to get to the human brain at the cellular level" in a simulation 6 . In practical terms, his team projected that only an exaflop-class supercomputer could *integrate* all the neural processes of a human brain model in real time 6 . Markram later reiterated that an exascale machine (i.e. ≥10^3 petaflops) would be required even just to "get a first draft of the human brain" modeled in silico 7 . This is a theoretical upper-bound: it refers to the computational load of faithfully emulating every neuron and synapse, rather than the brain's abstract problem-solving ability. It clarifies that to simulate the *full intricate biology* of a brain, one likely needs on the order of 10^18 floating-point operations per second.

Real-world supercomputer experiments support this order of magnitude. In 2013, for example, Japan's K-computer (then among the fastest machines at ~10 petaflops) was used to model just **1% of human brain activity for 1 second**, and even that partial simulation **took 40 minutes** of processing <sup>8</sup> <sup>9</sup>. Such results imply that a **100% brain simulation at real-time speed** is *several million times* more demanding than 10 petaflops – squarely pointing to the exascale and beyond. Indeed, the consensus in the high-performance computing community became that **exaflop-level computing power** would be needed to approach human brain simulation <sup>10</sup> <sup>7</sup>. This exascale target (roughly achievable by the early 2020s supercomputers) aligns with Markram's estimate above. In summary, **neuroscientific simulation efforts indicate the human brain's complexity is equivalent to ~10^18 FLOPS** of computation if one attempts to reproduce *every neuron and synapse* in detail. These estimates are about *theoretical computing requirements for full brain emulation*, rather than a claim that the brain literally performs floating-point math – it's an apples-to-oranges comparison used to gauge scale. The key point is that **matching the brain's low-level information processing appears to demand exaflop-class hardware**.

### **Brain's Performance vs Modern Computers**

Another perspective comes from comparing the brain's functional performance and efficiency to that of computers. By this measure, the human brain often appears to be operating at exaflop-scale power but with only a tiny fraction of the energy. For instance, a 2023 article by NIST researcher Advait Madhavan states plainly: "The human brain...can perform the equivalent of an exaflop — a billion-billion operations per second — with just 20 watts of power." 11 . Likewise, neuroscientists have noted that in June 2022 the world's fastest supercomputer (Frontier, ~1.1 exaFLOPS on LINPACK) consumed 21 megawatts, whereas the human brain achieves roughly the same 1e18 ops/sec on only ~20 W 12 . In other words, our brain is ~1,000,000 times more power-efficient while handling comparable computational loads 13. This striking comparison – an exaflop in your head powered by a sandwich and some oxygen – has been cited by AI experts and tech leaders to illustrate how far conventional computing lags behind biology. "The human brain consumes a mere 20 W in exchange for exascale processing potential," as one EE Times technology report put it 14. By contrast, running an exaflop on today's silicon chips would require the energy output of a dedicated power plant 14. Figures like **Elon Musk** and **Demis Hassabis** often highlight this vast efficiency gap (20 W vs megawatts) when discussing why brains remain unparalleled in certain kinds of intelligence and why neuromorphic computing is of interest. Overall, the consensus of such comparisons is that the brain's effective computational throughput is on the order of 10^18 operations/sec, achieved with minimal energy by leveraging highly parallel, specialized biological hardware 11 15.

It's important to clarify what this **exaflop-equivalence** means in practice. The brain is not literally performing 10^18 additions or multiplications each second in the digital sense; rather, this number is an estimate of how many standard computing operations a conventional computer would need to match the brain's processing. For example, one way to arrive at the exaflop figure is by considering ~10^14 synapses each firing perhaps 10–100 times per second and equating each synaptic event to a floating-point operation – yielding on the order of 10^17–10^18 ops/sec <sup>16</sup> <sup>17</sup>. Different methodologies give somewhat different results (some lower-end estimates are 10^15–10^16 FLOPS <sup>18</sup>, whereas others that include more detailed neural dynamics push into the 10^18 range). The **Sandberg & Bostrom workshop** mentioned earlier explicitly offered 10^18 FLOPS as a **plausible "functional equivalent"** for the brain's neural computation at an abstract spiking level <sup>19</sup>. Meanwhile, **K. Eric Drexler** in 2019 argued that performing typical human cognitive tasks might require significantly *less* raw compute – possibly under 10^15 FLOPS – if algorithms are optimized, noting that multiple narrow-AI benchmarks suggest a petaFLOP could suffice for human-level performance on certain tasks <sup>20</sup>. This discrepancy highlights the difference between **simulating** 

**everything the brain does internally** (which is where exaflop+ estimates come in) versus **replicating the outward capabilities** of the brain (which might be achievable with fewer operations if done cleverly). In summary, claims of "the brain is ~1 exaFLOP" usually refer to the **theoretical equivalent computing work** needed either to simulate the whole brain's physiology or to match its overall throughput. They underscore just how powerful the brain is compared to modern computers – especially considering the brain's **massive parallelism and efficiency** in contrast to conventional serial processing 12 11.

### **Expert Perspectives and Upper-Bound Claims**

A number of high-profile experts and publications have openly pointed to the **exaflop scale** when discussing brain vs computer performance. We've already seen Henry Markram's stance that an exaflop machine is needed for brain emulation. Similarly, futurist **Ray Kurzweil** (a director of engineering at Google) has long tied estimates of brain power to future computing milestones. Kurzweil estimated the human brain at roughly 20 quadrillion calculations per second (~2×10^16) in his 2005 book **The Singularity Is Near**, but he acknowledged this was a **conservative ballpark** <sup>21</sup>. As computing advances, that milestone was approached – the first petascale supercomputer (~10^15 FLOPS) appeared in 2008, and Kurzweil predicted that by the 2020s affordable machines would reach *tens of petaflops*, roughly approaching brain power. Indeed, an EE Times commentary in 2014 noted Kurzweil's 10 petaFLOP figure and reported that "many experts believe" the brain's true capacity is *far greater*, perhaps requiring the vast processing of exascale computing to surpass it <sup>5</sup>. In other words, surpassing human-level AI might demand a computer with on the order of 10^18 FLOPS, a view shared by some AI researchers. This is not a universally settled number, but it signals that **exaflop-class performance is seen as an upper bound for human-equivalent or greater intelligence** in hardware <sup>5</sup>.

Other notable figures reinforce the exaflop narrative in more informal contexts. For example, **IBM** neuroscientists in the late 2000s (led by Dharmendra Modha) worked on brain-inspired computing and estimated the human cortex might require on the order of 10^17-10^18 ops/sec to simulate, given their success simulating a cat's brain at 10^13 ops/sec 22. And in early 2023, Meta's chief AI scientist was quoted emphasizing the brain's efficiency by comparing it to roughly 1 exaFLOP on 20 watts – underscoring how far current AI is from brain-like energy use 23 24. Even **Elon Musk**, in discussions about AI and his Neuralink venture, has referenced the immense processing power of the human brain relative to computers (often alluding to the fact that "your brain does all this on 20 W" as a motivation for brain-machine interfaces), implicitly aligning with the exaflop-scale viewpoint. While these less formal remarks are not always accompanied by rigorous citations, they echo the **widely cited fact** that the human brain performs at least on the order of a billion-billion operations per second. This notion has appeared in venues from Reddit "Today I Learned" posts to science popularizers, usually phrased as: "the human brain is estimated to compute at roughly 1 exaFLOP, whereas the world's top supercomputer is only a fraction of that" 25. Importantly, serious scientists do couch this with caveats (the brain's "ops" are not directly comparable to digital FLOPS, etc.), but as a rule of thumb it has taken hold.

# **Conclusion: Exaflop Brain - Context and Caveats**

In summary, **multiple expert-backed sources place the human brain's computational capacity around the exaflop level (10^18 FLOPS)**. This figure arises in two closely related contexts. First, **neuroscience and HPC research** indicates that simulating a whole human brain with biological realism requires on the order of 10^18 operations per second, meaning future exascale supercomputers are just at the threshold of being able to *model* a brain's activity in real time 6 7. Second, **functional comparisons** highlight that the brain

in effect achieves exaflop-scale throughput on tasks (like vision, sensorimotor coordination, learning) that we struggle to replicate on machines – all while consuming only tens of watts <sup>11</sup> <sup>13</sup>. The "exaflop brain" estimates thus often serve to marvel at the brain's efficiency and to set targets for AI and supercomputing. As one research commentary put it, the brain's "exascale processing potential" with only 20 W dramatically outshines our current hardware, which would need megawatts for the same FLOPS <sup>14</sup>.

It should be noted that not all experts agree on the exact FLOP count needed to match a brain. Some analyses argue that **algorithmic optimizations** or neuromorphic approaches could achieve human-level cognition with *sub*-exaflop performance (e.g. on the order of 10^15-10^16 FLOPS) <sup>20</sup> <sup>18</sup>. In contrast, more exhaustive emulation-based estimates (including biochemical details) run far above 10^18 into the 10^22-10^25 range <sup>19</sup>. The **10^18 FLOPS figure is best viewed as a rough** *upper-bound for neural computation at the neuron level* - essentially a **mid-point estimate** among plausible scenarios. Crucially, it refers to theoretical equivalent computing work (or simulation cost), not a literal claim that neurons execute floating-point multiplications. As the AI Impacts research summary tactfully notes, **measuring brain performance in FLOPS is an approximation** to connect biological and digital systems <sup>26</sup>. With that in mind, the convergence of peer-reviewed studies and expert opinions around the exaflop scale is striking. It suggests that achieving human-like breadth of intelligence in machines will likely require **exascale computing resources** (if using brute-force methods) or comparably efficient new paradigms. In any case, the **exaflop benchmark** has become a shorthand for the complexity of the brain: whether one is discussing the hardware needed to emulate a mind, or the astounding fact that our own mind in our skull is, in effect, performing on par with a supercomputer **10^18 operations** each second <sup>11</sup> <sup>15</sup>.

#### **Sources:**

- Sandberg, A. & Bostrom, N. (2008). *Whole Brain Emulation: A Roadmap*. (Estimates ~10^18 FLOPS for neuronal-level brain emulation) <sup>2</sup>.
- Markram, H. (2011). *ISC Keynote Simulating the Brain*. (Quote: "We'd need about an exascale… to get to the human brain at the cellular level.") <sup>6</sup> .
- Courtland, R. (2014). *IEEE Spectrum Can the Human Brain Project Succeed?* (Markram: exascale supercomputer needed for a first draft simulation of human brain) <sup>7</sup>.
- Madhavan, A. (2023). NIST "Taking Measure" Blog *Brain-Inspired Computing...* (Brain performs *"equivalent of an exaflop...with just 20 W"*) 11.
- Smirnoff, A. et al. (2023). Frontiers in Science Organoid Intelligence (Human brain operates at ~1 exaFLOP on 20 W vs Frontier supercomputer 1.1 exaFLOP on 21 MW) 12.
- EETimes (2014). Achieving the Compute Performance of the Human Brain. (Kurzweil's 10 PFLOP estimate; experts say human brain likely nearer exaflop; brain's 20 W vs exascale machine's power)

  5 14.
- FactRepublic (2020). Fact #29356. (Pop-sci summary: "The human brain is ~1 exaFLOP... The world's largest supercomputer is 0.2 exaFLOP (using 13 MW); our brain uses 20 W.") 27.

1 2 3 4 18 19 20 26 Brain performance in FLOPS – AI Impacts https://aiimpacts.org/brain-performance-in-flops/

5 14 Achieving the Compute Performance of the Human Brain - EE Times https://www.eetimes.com/achieving-the-compute-performance-of-the-human-brain/

- 6 10 Digital Brain by 2023? Maybe High-Performance Computing News Analysis | insideHPC https://insidehpc.com/2011/06/digital-brain-by-2023-maybe/
- 7 Can The Human Brain Project Succeed? IEEE Spectrum

https://spectrum.ieee.org/can-the-human-brain-project-succeed

8 9 12 13 15 23 24 Frontiers | Organoid intelligence (OI): the new frontier in biocomputing and intelligence-in-a-dish

https://www.frontiersin.org/journals/science/articles/10.3389/fsci.2023.1017235/full

11 Brain-Inspired Computing Can Help Us Create Faster, More Energy-Efficient Devices — If We Win the Race | NIST

https://www.nist.gov/blogs/taking-measure/brain-inspired-computing-can-help-us-create-faster-more-energy-efficient

- 16 17 21 [D] Human brain FLOPs estimate, is it lower than we thought? : r/MachineLearning https://www.reddit.com/r/MachineLearning/comments/191ol1n/d\_human\_brain\_flops\_estimate\_is\_it\_lower\_than\_we/
- Reverse-Engineering of Human Brain Likely by 2030, Expert Predicts https://www.wired.com/2010/08/reverse-engineering-brain-kurzweil/
- <sup>25</sup> <sup>27</sup> Human Brain | Fact# 29356 | FactRepublic.com https://factrepublic.com/facts/29356/