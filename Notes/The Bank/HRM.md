HRM is built as a **sequence-to-sequence** model that maps an input token sequence (x=(x_1,\dots,x_l)) to an output token sequence (y=(y_1,\dots,y'_{l})) using an input embedding network (f_I) and an output head (f_O(z;\theta_O)=\mathrm{softmax}(\theta_O z)).  Computation does **not** proceed by consuming tokens left-to-right; instead, HRM maintains two coupled recurrent state tensors over the whole sequence—(z_L^i) (low-level) and (z_H^i) (high-level)—and advances them over recurrent timesteps (i=1,\dots,N!\times!T). At each timestep, the L-module updates based on ((z_L^{i-1}, z_H^{i-1}, \tilde{x})), while the H-module updates only once per cycle (every (T) steps) using the L-module’s final state, so the “time” axis is the recurrent iteration index (i), not the token index.  Because both (f_L) and (f_H) are implemented with **encoder-only Transformer blocks**, each recurrent update can mix information bidirectionally across *all* positions in the sequence—so the sequence positions function as **spatial memory slots** that are revised together within a step. 

HRM’s decoding is therefore **parallel across positions at each recurrent step**: you obtain a distribution for *every* token position by applying the same output projection (\theta_O) and softmax to that position’s hidden vector, rather than generating tokens autoregressively.  In the reference implementation this is literally a single batched head ( \texttt{output = lm_head}(z_H)) producing logits for all positions simultaneously,  and the model is configured with **`causal=False` (Non-autoregressive)** so token indices do not impose a left-to-right dependency.  Concretely for the **9th position** (slot (j=9)), the hidden vector (z_{H,9}^{(k)}) at recurrent step (k) is what gets multiplied by (\theta_O) and softmaxed to yield the model’s current predicted state/distribution for that same slot; then the model advances from (k\to k+1) by updating **all** slots in parallel via the recurrent (f_L/f_H) iterations, and the updated vector (z_{H,9}^{(k+1)}) is again passed through the same (\theta_O+)softmax to determine the **9th slot’s** state at the next step. 
