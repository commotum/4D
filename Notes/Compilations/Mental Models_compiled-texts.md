---
title: "A Framework for Putting Mental Models to Practice"
source: "https://commoncog.com/a-framework-for-putting-mental-models-to-practice/"
author:
  - "[[Cedric Chin]]"
published: 2018-12-24
created: 2025-05-30
description: "A Framework for Putting Mental Models to Practice is a series that examines the idea of putting Charlie Munger's Elementary Worldly Wisdom to practice in one's life in the pursuit of better decision making."
tags:
  - "clippings"
---
![Feature image for A Framework for Putting Mental Models to Practice](https://commoncog.com/content/images/size/w600/2018/12/4588663010_0e997c3b59_o--1--2.jpg)

**A Framework for Putting Mental Models to Practice** is a series of posts that examines the notion of putting ' [mental models](https://fs.blog/mental-models/?ref=commoncog.com) ' to practice in one's life, in the pursuit of learning better decision making.

The ideas presented in this series are a guided tour of rationality research, decision science, applied psychology and classical epistemology. My primary goal when developing this framework was to create a conceptual structure for the *personal* pursuit of improving decision making and judgment in pursuit of my goals.

It was originally written and published in [Farnam Street's Learning Community](https://fs.blog/membership/?ref=commoncog.com), and exists as a constructive alternative to the criticisms I levelled in [The Mental Model Fallacy](https://commoncog.com/the-mental-model-fallacy/).

If you do not have the time nor the inclination to read the entire series, a short summary post is available at [The Mental Model FAQ](https://commoncog.com/the-mental-model-faq/).

1. [Munger's Speech](https://commoncog.com/a-framework-for-putting-mental-models-to-practice-part-1/)
2. [An Introduction to Rationality](https://commoncog.com/putting-mental-models-to-practice-part-2-introduction-to-rationality/)
3. [Better Trial and Error](https://commoncog.com/putting-mental-models-to-practice-part-3-better-trial-and-error/)
4. [Expert Decision Making](https://commoncog.com/putting-mental-models-to-practice/)
5. [Skill Extraction](https://commoncog.com/putting-mental-models-to-practice-part-5-skill-extraction/)
6. [A Personal Epistemology of Practice](https://commoncog.com/putting-mental-models-to-practice-part-6-a-personal-epistemology-of-practice/)

Originally published , last updated .

---
title: "Mental Models: Munger's Speech"
source: "https://commoncog.com/a-framework-for-putting-mental-models-to-practice-part-1/"
author:
  - "[[Cedric Chin]]"
published: 2018-12-23
created: 2025-05-30
description: "There has been an uptick in self-help books and blogs about mental models. But, there's a problem when putting it in practice."
tags:
  - "clippings"
---
## A Framework for Putting Mental Models to Practice, Part 1: Munger's Speech

By

![Feature image for A Framework for Putting Mental Models to Practice, Part 1: Munger's Speech](https://commoncog.com/content/images/size/w600/2018/12/4588663010_0e997c3b59_o--1--1.jpg)

## Table of Contents

1. [The Problem](https://commoncog.com/a-framework-for-putting-mental-models-to-practice-part-1/#the-problem)
2. [Quick Digression: Epistemological Setup](https://commoncog.com/a-framework-for-putting-mental-models-to-practice-part-1/#quick-digression-epistemological-setup)
3. [The Framework](https://commoncog.com/a-framework-for-putting-mental-models-to-practice-part-1/#the-framework)

## Fan of great business?

Join 8,000+ sharp investors and operators like yourself, and we'll send you a collection of Commoncog's best articles right away:

*Note: this is the first part of a [series of posts](https://commoncog.com/a-framework-for-putting-mental-models-to-practice/) that summarises what I’ve learnt about improving my decision making and thinking. It builds on many of the ideas that we’ve covered on Commonplace, and is motivated by the observation that ‘mental models’ are only valuable to the extent that they are practicable.*

*This series provides a constructive alternative to the criticism I made in [The Mental Model Fallacy](https://commoncog.com/the-mental-model-fallacy/), which took [Farnam Street](https://fs.blog/?ref=commoncog.com) as its primary target.  Shane Parrish of Farnam Street reached out to me over my criticism and invited me to join his private discussion group; this series of posts was thus originally written and published in the [Farnam Street Learning Community](https://fs.blog/membership/?ref=commoncog.com).*

## The Problem

There’s a famous speech delivered by [Charlie Munger](https://en.wikipedia.org/wiki/Charlie_Munger?ref=commoncog.com) at the USC's Business School in 1994, titled *[Elementary Worldly Wisdom](https://old.ycombinator.com/munger.html?ref=commoncog.com)*. In it, Munger argues that you should learn lots of mental models from a large selection of disciplines, and then use that to provide context and colour to your decision making processes. This practice, Munger asserts, leads to wisdom; it is how he became so successful at investing and at life.

In the years since Munger’s original speech, there has been an uptick in self-help books and blogs focused around exactly this effort. The idea goes that if you summarise a large selection of mental models from a wide variety of fields, you would be able to use this list to ‘improve’ your thinking. The most popular amongst these blogs is Shane Parrish’s [Farnam Street](https://fs.blog/?ref=commoncog.com), who was the first person to notice this method, and the first to begin writing widely about it.

I have one problem with this approach, though. I think Munger’s prescription isn’t rigorously actionable. Or, to put this another way: *I've put it into practice, and it doesn’t seem to make me better*.

I suspect that I’m not alone in this. In Farnam Street’s Learning Community alone, I’ve seen threads by multiple people asking how they should put mental models to practice. So far, the answers I’ve read haven’t been very satisfactory.

I don’t mean that in a disrespectful way. When I first read Munger’s speech five years ago, I thought it was a remarkable approach written by one of the best investors in the world, and so I thought I should find a way to learn this new approach he’d talked about.

But then adopting Munger’s prescription to read widely and pick elementary findings from Physics, Chemistry, Biology and Psychology *didn’t* cause me to get substantially better in pursuit of my chosen goals. (My Computer Science degree had a requirement for us to take a handful of basic science classes — freshman level Chemistry, Physics, or Biology, all of which we hated — so it wasn’t like I had to go out of my way to acquire models from those domains. I merely needed to do more reading in Psychology).

It is usually at this point that someone pipes in to say that “mental models are ways of looking at the world. I don’t think you can really practice them.” I reject this claim as strongly as the ocean rejects oil from a capsized tanker. If you’re reading this, it’s highly likely that you think mental models are a method to self improvement. You spend time reading about mental models in order to be more effective at your decision-making and at your life.

With that goal in mind, imagine that you are now a professional basketball coach, and you find yourself with an NBA player who says “Oh, I think technique is just a way of seeing the world. I don’t think you can really practice it.” What would you do?

You’d fire him, that’s what.

I believe that you need practice to truly *know* something, the same way that reading a math tool in a textbook and applying it to solve a math problem are two different levels of knowing. Similarly, claiming that “mental models are ways of seeing the world to find how the world *truly* works” is a bizarre claim if you don’t have the feedback loop of practice. How *can* you know if your mental models are right if you don’t test them rigorously?

So I think there are two interpretations to what’s going on here. I’m not yet sure which is the right one.

The first is that perhaps Munger *wasn’t speaking to me.* He wasn’t speaking to beginner or intermediate practitioners. He was assuming that the people he was giving his worldly wisdom speech to (USC Business School, if you recall) were going to become master practitioners through the usual skill-stacking means, and once they became good at what they did, he believed that ‘having a diverse latticework of mental models’ would give them the edge that makes him so rare in the business world. To put this another way, I don’t believe Munger was recommending ‘Elementary Worldly Wisdom’ as a *path to mastery*. I think he was recommending it *in addition* to the normal ways humans achieve mastery.

The second interpretation is that perhaps Munger’s recommendations lends itself more strongly to investing and finance. It makes sense to me that investors have to use many models as they go about doing their work; Munger himself goes into detail of ‘the art of stock picking as a subset of elementary worldly wisdom’ in his original speech. In order to pick good investments, you’re likely to need a *large* selection of models to make sense of diverse businesses in different markets, the economic fundamentals governing their external environment, management principles to evaluate their leadership, consumer psychology, and so on. See this Scott Page [article on this argument, for instance](https://hbr.org/2018/11/why-many-model-thinkers-make-better-decisions?ref=commoncog.com).

~~Investors have an additional advantage over the rest of us: their field offers good feedback loops because performance is well-defined. Given time, you know if you’ve picked the right stocks because it’s relatively easy to measure performance (compared to other messier fields like management, or teaching, or computer programming).~~ This is not correct, as I discover in [Part 4](https://commoncog.com/putting-mental-models-to-practice/).

The fact that Farnam Street has a [wide readership in finance](https://www.nytimes.com/2018/11/11/business/intelligence-expert-wall-street.html?ref=commoncog.com) seems to bear this out.

So where does this leave us? Let’s consider the implications of the above interpretations.

If you are an investor or finance wonk, you’re likely to benefit from the Farnam Street approach — read widely for mental models, grow your bag of ‘lenses’ for evaluating investment opportunities. The nature of your work will allow you to check if your models are correct because finance affords you tight feedback loops.

But if you *aren’t* in finance, as I am, then you’ll need an alternative approach.

My background is in software development and management. My goal is to build a business of my own. I know that there are many of us are not investors — those who are interested in mental models and self improvement include teachers, writers, designers, data scientists, and more.

This framework is for this second group.

## Quick Digression: Epistemological Setup

I’ll write a separate, longer post on this topic later on in this series, but we need to spend a little time talking about epistemological evaluation in our framework.

‘Epistemological evaluation’ is simply a fancy way of saying ‘how do you know this is true?’ The reason we need to talk about this is because I’m going to be making a bunch of assertions in the coming paragraphs (in fact, I’ve already made a bunch of assertions above) — and we need to agree on a basis of evaluation. To put this another way, in order to be rigorous, we need to have a framework for evaluating truth in the context of practice.

In science, what is ‘true’ was worked out by a bunch of philosophers in a branch of knowledge we call [philosophy of science](https://en.wikipedia.org/wiki/Philosophy_of_science?ref=commoncog.com). There are well-known methods of evaluating scientific claims: you understand that truth is always conditional, that you may only disprove something, never truly ‘prove’ it. For contemporary science, you know to ask questions like “what is the statistical power of the study?”, “is there a meta-analysis on this topic” and failing that, “Is there a double blind study on this topic?” and you are careful to avoid trusting single small-sized studies because of the dangers of p-hacking. (For a longer, more rigorous treatment of the problems with null hypothesis statistical testing, I refer you to gwern’s [excellent summary](https://www.lesswrong.com/posts/ttvnPRTxFyru9Hh2H/against-nhst?ref=commoncog.com) over at LessWrong).

But there are alternative forms of knowledge. For instance, when you want to learn to defend yourself, you go to a martial arts sensei, not a martial arts scientist. Nicholas Nassem Taleb refers to this kind of knowledge, for instance, when he [says that](http://fooledbyrandomness.com/ConvexityScience.pdf?ref=commoncog.com) if you want to create a better hummus, you should turn to cooks, not food scientists. The Ancient Greeks call this knowledge ‘techne’, or art. I’ll call it ‘practice’.

The reason we must be clear of the differences is because smart people conflate scientific standards for knowledge with practical standards for knowledge all the time. See, for instance, [this example](https://commoncog.com/robin-hanson-on-ray-dalio/) of economist Robin Hanson holding Ray Dalio’s *Principles* up to the standards of science. He finds it badly-written and wanting. But a practitioner can mine Dalio’s *Principles* for actionable techniques and apply it to their own lives to great effect.

I can and will write a much longer treatment on this topic, but for now, I want to focus on two principles.

The first — and probably the single most important — principle is to ‘let reality be the teacher’. That is — if you have some expectations of a technique and try it out, and then it doesn’t work — either the technique is bad, or the technique is not suitable to your specific context, or your implementation of the technique is bad, or your expectations are wrong.

However, for the purposes of practice, reality is a higher resolution teacher than words on a page, or instructions from a practitioner. If it doesn’t work, it doesn’t work. Maybe you’re not ready for it yet. Maybe it doesn’t work for your unique personality, or your unique situation. Go try something else.

The implication here is that you *shouldn’t* rubbish techniques and suggestions documented by other practitioners — provided it has been shown to work for them. If, for example, a technique that a practitioner like Andy Grove uses at Intel cannot possibly work in your organisation, there is still value at examining *why* the technique works in his, and then attempting to adapt that principle to the unique context of your life.

The second principle flows from the first. When it comes to practice, one should pay attention to *actual practitioners*. This is because their approaches have been tested by reality.

This principle is formalised in Ray Dalio’s book as the [Believability](https://commoncog.com/believability/) metric. It makes sense because you *do* need a filter for advice, which for Dalio means paying more attention to people who are believable in their chosen domains. This means that you should judge them according to what they’ve *actually accomplished*, keeping in mind that what works for them might not work for you because of hidden differences in their person or in their situation.

A second order implication means that if you tell me something you have *actually done* — I will pay close attention! And I will do so even if you have not accomplished as much as Dalio’s believability metric demands (i.e. at least three successes in your field + a credible explanation for your approach). This is because you have put it to the test in your life, which means you have let reality be your teacher.

A more concrete example suffices. There are many self-help blog posts about deliberate practice, but a quick way to filter for usefulness is to scan the article for hints that the writer has actually tried putting deliberate practice into … well, practice. If they haven’t, then you should ignore them. If they have, then *read carefully.*

It is *really, really* difficult to create a deliberate practice program for an unstructured skill — even Anders Ericsson himself admits that he does not know how to deploy deliberate practice for skills like solving crossword puzzles and folk dancing. The self-help hacks who have never actually tried putting this into practice will not mention that it’s really difficult — nearly, in some cases, impossible — because they have not tried to do so. Therefore, you’re not going to get much value out of them.

To wrap up this section: is this approach less rigorous than the scientific method? Yes. Absolutely. Where science makes claims, practitioner knowledge should give way. But that doesn’t mean practitioner knowledge is useless. And it doesn’t mean that we can’t be rigorous about evaluating it.

## The Framework

Without explanation, my framework is as follows:

1. Use *intelligent* trial and error in service of solving problems. This means two sub-approaches: first, using the field of instrumental rationality to get more efficient at trial and error. Second, using a meta-skill I call ‘skill extraction’ to extract approaches from practitioners in your field.
2. Concurrently use the two techniques known for building expertise (deliberate practice and perceptual exposure) to build skills in order to get at more difficult problems.
3. Periodically attempt to generalise from what you have learnt during the above steps into *explicit mental models*.

Of course, none of this will make sense without a *framework* — that is, a structure to organise these concepts and to explain why it works. So, in the next few posts, I’ll cover:

- Why we should look to rationality research as the starting point for our framework for putting mental models to practice (t)
- What instrumental rationality teaches us about reducing the cycles of trial and error needed to succeed. (t)
- How to measure if you are getting better at decision making (t)
- The epistemic underpinnings of practice (wip)
- The relationship between instrumental rationality and building expertise.
- How to apply ‘skill extraction’, with worked examples. (t)
- How to *actually use* deliberate practice, along with caveats, notes, thoughts, and references. (wip)
- How to use perceptual exposure in service of learning. (wip)
- How to generalise from practice to mental models. (t)

I’ve marked the above topics as (t) for topics in which I have tested through practice, or which contain arguments that I am quite confident in. I have marked the topics (wip) for topics that are still a work-in-progress in my life, or where I’m still working out the full implications of my ideas. For example, I’ve gotten quite good at writing, but I did so over a decade, and not through a rigorous process of deliberate practice. Later, I got quite good at management through deliberate practice-like techniques (over three years and in the context of engineering offices in corrupt, underdeveloped South East Asian countries) but not at the levels of rigour that are possible (and that I’m going to write about). Similarly, I only recently learnt of [perceptual exposure](https://commoncog.com/chicken-sexing-and-perceptual-learning-as-a-path-to-expertise/), and am starting to [apply it to my life](https://commoncog.com/the-playlist-of-awesome-putting-perceptual-exposure-to-practice/). It will take a few years before I know the efficacy of these techniques.

The concise summary of my approach is that I believe experience from practice leads to usable mental models.

![](https://commoncog.com/content/images/2018/12/Paper.Commonplace.30--1--1.png)

(The alert reader will notice that this is the entire crux of Dalio’s *Principles*, heh).

In flipping the above illustration, the alert reader will also notice that I disagree with Shane on the reverse approach, that is, using a list of mental models and attempting to apply it to reality. I think it’s too inefficient for practitioners outside finance — though I’ll admit this isn’t a very qualified opinion.

![](https://commoncog.com/content/images/2018/12/Paper.Commonplace.29--1-.png)

However, I will freely admit that there is something to using FS’s list of mental models to provide structure to patterns that you might notice from practice. In this scenario, reading someone else’s mental model, or looking to alternate domains for new models, occasionally results in a written description that captures some noticed patterns or intuitions from your practice. This borrowed model then gives you the ability to search the literature for similar treatments, or it gives you words to describe the things you’ve noticed in your practice.

![](https://commoncog.com/content/images/2018/12/Paper.Commonplace.31--1-.png)

I know I’ve certainly experienced this in relation to Shane’s writing — although admittedly very rarely.

In Part 2, we’re going to talk about the field of rationality research, and what we can learn from studying the results in that body of knowledge.

*Go to [Part 2: An Introduction to Rationality](https://commoncog.com/putting-mental-models-to-practice-part-2-introduction-to-rationality/).*

*Image of Charlie Munger taken by [Nick Webb](https://flic.kr/p/7Zu6Du?ref=commoncog.com).*

Originally published , last updated .

Previous Post

#### ← Putting Perceptual Exposure to Practice

Next Post

[![Feature image for What Process Improvement in Education Looks Like](https://commoncog.com/content/images/size/w300/2024/09/rigorous_process_improvement_education.jpg)](https://commoncog.com/what-process-improvement-in-education/)[![Feature image for Business Ecosystem Change Takes Time](https://commoncog.com/content/images/size/w300/2024/06/business_ecosystem_change_takes_time.jpg)](https://commoncog.com/business-ecosystem-change/)[![Feature image for Follow Your Nose](https://commoncog.com/content/images/size/w300/2021/03/follow_your_nose.jpeg)](https://commoncog.com/follow-your-nose/)[![Feature image for Create Your Own Rituals](https://commoncog.com/content/images/size/w300/2021/02/create_your_own_rituals.jpeg)](https://commoncog.com/create-your-own-rituals/)

---
title: "Mental Models: An Introduction to Rationality"
source: "https://commoncog.com/putting-mental-models-to-practice-part-2-introduction-to-rationality/"
author:
  - "[[Cedric Chin]]"
published: 2018-12-24
created: 2025-05-30
description: "Any discussion of practicing mental models must begin with a discussion of rationality. We look at what the research tells us about it."
tags:
  - "clippings"
---
## Putting Mental Models to Practice, Part 2: An Introduction to Rationality

By

![Feature image for Putting Mental Models to Practice, Part 2: An Introduction to Rationality](https://commoncog.com/content/images/size/w600/2018/12/sunyu-kim-1053375-unsplash.jpg)

## Table of Contents

## Fan of great business?

Join 8,000+ sharp investors and operators like yourself, and we'll send you a collection of Commoncog's best articles right away:

*This post is Part 2 of [A Framework for Putting Mental Models to Practice](https://commoncog.com/a-framework-for-putting-mental-models-to-practice/). Read Part 1 [here](https://commoncog.com/a-framework-for-putting-mental-models-to-practice-part-1/).*

Any discussion about putting mental models to practice must begin with a discussion of rationality.

Why? Well, ‘rationality’ is the study of thinking in service of achieving goals. If we want a framework for putting mental models to practice, what better place to start than the academic topic most focused on applying tools of thought in the service of better judgments and decisions?

Rationality, however, is a loaded word. It brings to mind ‘rational’ characters like Spock from Star Trek, who are unable to express emotion adequately and who reason robotically. What *exactly* do we mean when we say ‘rationality’?

The best definition that I’ve found is Jonathan Baron’s, laid out in his textbook on the topic *Thinking and Deciding*: “rationality is **whatever kind of thinking best helps** people **achieve their goals**.” Note the implications of this definition, however: if your goal is to find love and live happily ever after, then rationality is the kind of thinking or decision making that is necessary for you to achieve that; if your goal is to destroy the planet, then rationality is whatever thinking is necessary for you to do so. Rationality has nothing to do with emotion, or lack thereof; it merely describes the *effectiveness* with which a person pursues and achieves his or her stated goals.

Rationality as defined like this is not unknown in the popular literature. Here’s Warren Buffett, in a 1998 interview in Fortune titled [The Bill and Warren Show](https://money.cnn.com/magazines/fortune/fortune_archive/1998/07/20/245683/index.htm?ref=commoncog.com):

> How I got here is pretty simple in my case. It's not IQ, I'm sure you'll be glad to hear. The big thing is rationality. I always look at IQ and talent as representing the horsepower of the motor, but that the output — the efficiency with which that motor works — depends on rationality. A lot of people start out with 400-horsepower motors but only get a hundred horsepower of output. It's way better to have a 200-horsepower motor and get it all into output.

Buffett’s conception of rationality as ‘effectiveness’ is not unique. There has been an organised attempt in recent years to quantify rationality, out of the belief that rationality explains how IQ alone isn't sufficient to explain success in life. This pursuit has led to the development of a ‘ [Rationality Quotient](https://mitpress.mit.edu/books/rationality-quotient?ref=commoncog.com) ’, led primarily by the efforts of University of Toronto Professor of Psychology and Human Development Keith Stanovich. I’ll quote him as described by this [Credit Suisse paper](https://research-doc.credit-suisse.com/docView?language=ENG&format=PDF&source_id=csplusresearchcp&document_id=1048541371&serialid=mofPYk1Y4WanTeErbeMtPx6ur0SCIcSlaZ7sKGPdQQU%3D&ref=commoncog.com) on RQ:

> Keith Stanovich, a professor of applied psychology at the University of Toronto, distinguishes between intelligence quotient (IQ) and rationality quotient (RQ). Psychologists measure IQ through specific tests, including the Wechsler Adult Intelligence Scale, and it correlates highly with standardized tests such as the SAT.  
>   
> IQ measures something real, and it is associated with certain outcomes. For example, thirteen-year-old children who scored in the top decile of the top percent (99.9th percentile) on the math section of the SAT were eighteen times more likely to earn a doctorate degree in math or science than children who scored in the bottom decile of the top percent (99.1st percentile).  
>   
> **RQ is the ability to think rationally and, as a consequence, to make good decisions.** Whereas we generally think of intelligence and rationality as going together, Stanovich's work shows that the correlation coefficient between IQ and RQ is relatively low at.20 to.35. **IQ tests are not designed to capture the thinking that leads to judicious decisions.***(emphasis mine)*  
>   
> Stanovich laments that almost all societies are focused on intelligence when the costs of irrational behavior are so high. **But you can pick out the signatures of rational thinking if you are alert to them. According to Stanovich, they include adaptive behavioral acts, efficient behavioral regulation, sensible goal prioritization, reflectivity, and the proper treatment of evidence.***(emphasis mine)*

I include RQ in our discussion only because I find RQ to be a remarkably useful concept to have. When we say that someone is ‘effective’ at attaining their goals, or that they are ‘street smart’, what we really mean is that they are epistemically and instrumentally rational. RQ, IQ, and EQ also helps explain certain observable differences in humans: it explains, for instance, why smart people can be jerks, empathic people can believe in crazy things, and effective people can have average intelligence.

**The Two Types of Rationality**  
Rationality is commonly divided into two categories. The first is epistemic rationality, which concerns thinking about beliefs. The second is instrumental rationality, which concerns thinking about decisions.

- Epistemic rationality is “how do you know what you believe is true?”
- Instrumental rationality is “how do you make better decisions to achieve your goals?”

When you say “I believe Facebook stock is currently undervalued”, you are engaged in the kinds of thinking epistemic rationality is concerned with.

When you ask “Which college degree should I study: computer science or economics?” you are engaged in the kinds of thinking instrumental rationality is concerned with.

We may now see that [Farnam Street’s list of mental models](https://fs.blog/mental-models/?ref=commoncog.com) is really a list of three types of models:

1. *Descriptive* mental models that come from domains like physics, chemistry, economics, or math, that describe some property of the world.
2. *Thinking* mental models that have to do with divining the truth (epistemic rationality) — e.g. [Bayesian updating](https://fs.blog/2018/09/bayes-theorem/?ref=commoncog.com), [base rate failures](https://www.farnamstreetblog.com/2012/11/mental-model-bias-from-insensitivity-to-base-rates/?ref=commoncog.com), the [availability heuristic](https://www.farnamstreetblog.com/2011/08/mental-model-availability-bias/?ref=commoncog.com).
3. *Thinking* mental models that have to do with decision making (instrumental rationality) — e.g. [inversion](https://fs.blog/2013/10/inversion/?ref=commoncog.com), ‘ [tendency to want to do something](https://www.farnamstreetblog.com/2015/06/do-something-syndrome/?ref=commoncog.com) ’, [sensitivity to fairness](https://www.farnamstreetblog.com/2011/09/mental-model-kantian-fairness-tendency/?ref=commoncog.com), [commitment & consistency bias](https://www.farnamstreetblog.com/2016/08/commitment-consistency-bias/?ref=commoncog.com).

The vast majority of Farnam Street’s mental models are of the descriptive variety, as per Munger’s prescription in [Elementary Worldly Wisdom](https://old.ycombinator.com/munger.html?ref=commoncog.com), which means that they don’t naturally lend themselves to practice. As many in this learning community have pointed out, you can’t really put ‘thermodynamics’ to practice, say, in the way that you can put ‘bayesian updating’ to practice.

I’ll talk about the usefulness of descriptive mental models much later in this series. For now, I want to focus on the mental models that are the *methods of thought* — that is, epistemic rationality and instrumental rationality. Taken together, the promise of rationality training is incredibly attractive: that we may improve our thinking in order to achieve our goals — whatever our goals may be.\*

*\*(Note: this is not a ridiculous claim to make: if we applied the methods of rationality to goal-selection itself, we would not pick unachievable goals.)*

## Putting Epistemic Rationality to Practice

One of the crowning achievements of modern economics and psychology is the [Cognitive Biases and Heuristics](https://en.wikipedia.org/wiki/Heuristics_in_judgment_and_decision-making?ref=commoncog.com) research program created by Daniel Kahneman and Amos Tversky. In it, Kahneman and Tversky document the various ways in which human beings act ‘irrationally’ — that is, that they think and perform in ways at odds with what is rational.

When I began my search for a framework to improve my thinking, the cognitive biases research program appeared like a blinding billboard on the road to enlightenment. It seemed for awhile that a multitude of paths led to some cognitive bias or other. And indeed: a rich academic field of applied psychology, judgment and decision making has since sprung up around the findings from Kahneman and Tversky’s work; in 2002, they were awarded the Nobel Memorial Prize in Economics.

It’s no surprise, then, that the topic we call ‘epistemic rationality’ is in practice the study of methods and mental models to overcome the cognitive biases that Kahneman and Tversky have discovered. And the best place to hunt for such methods is a community blog by the name of [LessWrong](https://www.lesswrong.com/?ref=commoncog.com).

LessWrong was created in 2009 by writer and artificial intelligence theorist Eliezer Yudkowsky. Over the next five years the community developed into an enthusiastic group of thinkers, writers and practitioners, all dedicated to the search for better methods of thinking by way of overcoming bias. Their primary method for doing so was to mine the fields of applied psychology, economics and cognitive science for the latest findings on *preventing* bias; they then wrote up the findings for presentation to a wider audience and encouraged sharing of notes from putting these methods into practice. LessWrong is responsible, for instance, for popularising Bayesian updating, effective altruism, and the idea of rationality research itself. From its community comes bloggers like [Scott Alexander](http://slatestarcodex.com/?ref=commoncog.com) and [gwern](https://www.gwern.net/?ref=commoncog.com), both of whom continue to write about related topics.

Here’s a small sample of such threads:

- [The Cognitive Science of Rationality](https://www.lesswrong.com/posts/xLm9mgJRPvmPGpo7Q/the-cognitive-science-of-rationality?ref=commoncog.com) documents some of the findings LessWrong has aggregated over the first two years of its life. To see some concrete recommendations, jump to the section titled ‘Rationality Skills’.
- Fixing the [planning fallacy](https://www.lesswrong.com/posts/CPm5LTwHrvBJCa9h5/planning-fallacy?ref=commoncog.com) simply demands that you compare your current project with the ‘outside view’, that is, to other projects that are superficially similar to yours.
- There are multiple treatments of Bayesian Updating, or ‘Bayesian Thinking’ on LessWrong. This [introduction](https://www.lesswrong.com/posts/AN2cBr6xKWCB8dRQG/what-is-bayesianism?ref=commoncog.com) to the general modes of thought is worth reading. Here is a [story](https://www.lesswrong.com/posts/NKaPFf98Y5otMbsPk/bayesian-judo?ref=commoncog.com) illustrating [Aumann’s Agreement Theorem](https://en.wikipedia.org/wiki/Aumann%27s_agreement_theorem?ref=commoncog.com), sadly also demonstrating the tendency for Yudkowsky and co to use long-winded stories while explaining their ideas. Here is some [reflection](https://www.lesswrong.com/posts/JBnaLpsrYXLXjFocu/what-bayesianism-taught-me?ref=commoncog.com) by a forum member on what practicing Bayesian has taught him.
- I particularly enjoyed this explanation of the [Shelling Fence](https://www.lesswrong.com/posts/Kbm6QnJv9dgWsPHQP/schelling-fences-on-slippery-slopes?ref=commoncog.com) — a concept designed to defend against slippery slope arguments, as well as hyperbolic discounting.

LessWrong’s approach makes up the first part of my framework for putting mental models to practice. Remember the second principle in my epistemological setup: when it comes to practice, one should pay attention to *actual practitioners*. In this case, LessWrong represents nearly nine years of putting cognitive bias prevention methods into practice. If you are interested in mental models that will help you refine your beliefs, then you should go to LessWrong and search for threads on topics of interest, and pay special attention to threads where people have put those methods to practice in their lives.

If you have the opportunity, you may also consider paying to attend a workshop conducted by the [Center For Applied Rationality](http://www.rationality.org/?ref=commoncog.com) (or CFAR) — a non-profit organisation set up by LessWrong community members with the aim of teaching the methods of rationality to a general audience.

LessWrong has grown significantly less active over time, however. If you can’t find a LessWrong thread on a specific method you are interested in (or if you cannot attend a CFAR workshop), you would do well to adopt their *original* methodology: that is, diving into the academic literature on judgment and decision making to find methods to use.

In order to do this, however, you’ll need to understand the underlying approach in the field of decision making. Consider: what does it mean to have made ‘better’ decisions? How do you measure the concept of ‘better’?

The approach decision scientists have used to study this is to analyse thinking in terms of three models (Baron, *Thinking and Deciding*, Chapter 2):

- The **descriptive** model — the study of how humans *actually* think, a field of study that draws from the sort of investigative work that Kahneman and Tversky pioneered.
- The **normative** model — how humans *ought* to think. This represents the ‘ideal’ type of thinking that should be achieved. For instance, if we were studying decision making, this would be the ‘best’ decision one could take.
- The **prescriptive** model — what is needed in order to help humans change their thinking from the (usually flawed) descriptive model to the normative model.

For instance, ‘neglect of prior information’ is a cognitive bias — that is, a descriptive model of how humans naturally think. The normative model in this case is Bayes’s theorem. And the prescriptive model is [Bayesian updating](https://fs.blog/2018/09/bayes-theorem/?ref=commoncog.com).

When searching for practicable information in the academic literature, then, you should focus your attention on prescriptive models. But you should understand that the prescriptive models are always developed in service of helping humans achieve the modes of thinking as defined in some normative model. The papers that describe prescriptive models will almost always give you the normative model at the same time. And here we face our first serious problem.

The alert reader might notice a gaping hole in my description of the decision science approach: “But Cedric!” I hear you cry, “How can one possibly know what the ‘best’ decision is in real life?!”

You, dear reader, have hit on the central problem of any framework for practice. In some situations, such as with errors of probabilisitic thinking, we have a clear normative model: that is, the theory of probability. (Alert readers will realise the descriptive model in this case is prospect theory — the theory that won Kahneman and Tversky the Nobel prize). Having a clear normative model allows us in many cases to check to see if our practice has made us better at thinking — that is, have gotten us closer to the normative model. But in many cases of decision making in real life, we have no way of knowing what the ‘best’ decision is.

*(That is not to say that decision science doesn’t have normative models for good decision making. But I’ll explore the problems with those later.)*

This is partly the reason why we are talking with epistemic rationality first — the normative models are clearer, the descriptive models are richer, and the prescriptive models to correct our thinking are easier to put into practice. A community of practitioners have sprung up around this topic. It is *instrumental rationality* that is the problem for our framework of practice.

It’s taken me four years to find a suitable answer to that question — three of which was spent building a small company in Singapore and Vietnam. I’ll talk about my answers in the next part to this series. But for now, let us look at LessWrong’s attempts to grapple with this problem.

## LessWrong’s Mysterious Flaw

LessWrong was created in the pursuit of both kinds of rationality. The central premise of the site was that if you improve your mental models for understanding the world (epistemic rationality), you would make better decisions, and you would be able to ‘win’ at life — the LessWrong term for achieving your goals.

In September 2010, roughly a year into LessWrong’s life, [Patri Friedman](https://en.wikipedia.org/wiki/Patri_Friedman?ref=commoncog.com) wrote a critique titled [Self-Improvement or Shiny Distraction: Why Less Wrong is anti-Instrumental Rationality](https://www.lesswrong.com/posts/uFYQaGCRwt3wKtyZP/self-improvement-or-shiny-distraction-why-less-wrong-is-anti?ref=commoncog.com). In it, Friedman argued that this goal was a lie: LessWrong was primarily focused on improving methods at getting at the truth. This was separate from instrumental rationality — that is, the decision making ability necessary to *achieve* goals in one’s life.

LessWrong, Friedman asserted, consisted of people who were happy to sit around and talk about ‘better ways of seeing the world’ but were not so interested in actually *doing* things, or attempting to achieve real goals. It was, in other terms, a ‘shiny distraction’.

About three years later, respected community member Luke Muehlhauser [observed](https://www.lesswrong.com/posts/NLJ6NyHFZPJ2oNSZ8/explicit-and-tacit-rationality?ref=commoncog.com) that people like Oprah Winfrey were incredibly successful at life without the sort of epistemic rigour that LessWrong so prized.

> Oprah isn't known for being a rational thinker. She is a known peddler of pseudoscience, and she attributes her success (in part) to allowing "the energy of the universe" to lead her.  
>   
> Yet she must be doing something right. Oprah is a true rags-to-riches story. Born in Mississippi to an unwed teenage housemaid, she was so poor she wore dresses made of potato sacks. She was molested by a cousin, an uncle, and a family friend. She became pregnant at age 14.  
>   
> But in high school she became an honors student, won oratory contests and a beauty pageant, and was hired by a local radio station to report the news. She became the youngest-ever news anchor at Nashville's WLAC-TV, then hosted several shows in Baltimore, then moved to Chicago and within months her own talk show shot from last place to first place in the ratings there. Shortly afterward her show went national. She also produced and starred in several TV shows, was nominated for an Oscar for her role in a Steven Spielberg movie, launched her own TV cable network and her own magazine (the "most successful startup ever in the magazine industry" according to Fortune), and became the world's first female black billionaire.  
>   
> I'd like to suggest that Oprah's climb probably didn't come merely through inborn talent, hard work, and luck. To get from potato sack dresses to the Forbes billionaire list, Oprah had to make thousands of pretty good decisions. She had to make pretty accurate guesses about the likely consequences of various actions she could take. When she was wrong, she had to correct course fairly quickly. In short, she had to be fairly rational, at least in some domains of her life.

What is true of Oprah is true of many entrepreneurs I know.

You’ve probably watched *[Crazy Rich Asians](https://en.wikipedia.org/wiki/Crazy_Rich_Asians_\(film\)?ref=commoncog.com)*, a movie about the super rich Chinese of Singapore, Malaysia, Indonesia, Hong Kong, and Taiwan. ‘Traditional’ Chinese businessmen are a cultural trope of the Overseas Chinese diaspora — a diaspora that I belong to. Earlier this year, I wrote a [series of posts](https://commoncog.com/the-chinese-businessman-paradox/) to reflect on my experiences working with and competing against such businesspeople. My [central observation](https://commoncog.com/chinese-businessmen-superstition-doesnt-count/): while *incredibly* superstitious, these businessmen were often very good decision makers. The primary model for most successful Chinese businesses is that of a conglomerate: a single business controlling interests in a wide array of industries such as palm oil, shipping, hospitality, rice, flour, steel, and telecommunications. The first generation of Chinese businessmen had no formal education, and had to survive various challenges like Malaysia’s [preferential race policies](https://en.wikipedia.org/wiki/Ketuanan_Melayu?ref=commoncog.com), and Indonesia’s [Chinese genocide](https://en.wikipedia.org/wiki/Discrimination_against_Chinese_Indonesians?ref=commoncog.com).

It’s difficult to believe that the best Chinese businessmen could achieve such success without being able to make thousands of good decisions across so many industries.

Why is this the case? One possible reason was the one [given](http://www.aaronsw.com/weblog/optimalbias?ref=commoncog.com) by [Aaron Swartz](https://en.wikipedia.org/wiki/Aaron_Swartz?ref=commoncog.com): that LessWrong was too consumed with cognitive biases as presented by the literature, and not consumed enough by cognitive biases *that mattered in real life.* In simple terms, perhaps the sort of cognitive biases explored by Kahneman and Tversky were simply the most obvious ones — or the ones most easily measured in a lab. On LessWrong, Swartz [wrote](https://www.lesswrong.com/lw/di4/reply_to_holden_on_the_singularity_institute/75an?ref=commoncog.com):

> Cognitive biases cause people to make choices that are most obviously irrational, but not most importantly irrational... Since cognitive biases are the primary focus of research into rationality, rationality tests mostly measure how good you are at avoiding them... LW readers tend to be fairly good at avoiding cognitive biases... But there a whole series of much more important irrationalities that LWers suffer from. (Let's call them "practical biases" as opposed to "cognitive biases," even though both are ultimately practical and cognitive.)  
>   
> …Rationality, properly understood, is in fact a predictor of success. Perhaps if LWers used success as their metric (as opposed to getting better at avoiding obvious mistakes), they might focus on their most important irrationalities (instead of their most obvious ones), which would lead them to be more rational and more successful.

## Where Epistemic Rationality Matters

Were they — Swartz, Muehlhauser and Friedman correct? Is instrumental rationality more important to success than epistemic rationality?

Well … yes and no.

In some fields, epistemic rationality matters a great deal. These fields are the ones where getting things right are really important. Or, to flip that around, epistemic rationality matters in fields where getting things wrong are *very costly:* for instance, in finance, where a wrong, leveraged trade can cost you your firm, and in statecraft or government intelligence — where the wrong assessment might mean hundreds of deaths.

However, in fields where the cost of getting things wrong isn’t that high, another approach dominates: trial and error.

The mainstream thinker that best tackles this dichotomy is Nicholas Nassem Taleb. Taleb points out that in situations where the cost of failure is low but the potential upside is high, trial and error dominates as the optimal strategy. He says that some situations have ‘positive convexity’, and argues that in situations where you have such convexity, you are set up to benefit from randomness.

A few examples suffices to illustrate this concept. In business, an entrepreneur who is able to quickly and cheaply experiment has a higher chance of success. His downside is capped to the opportunity cost of time and the upfront capital necessary to start the business; the Lean Startup methodology reflects this reality by suggesting that entrepreneurs run quick and cheap experiments in the beginning in order to find a viable idea. In startup investing, each investment is downside-capped to the amount that the investor puts into the company. However, the upside is often 10x higher; the optimal strategy here is to make lots of little bets, in the hopes that one or two will pay for the costs of everything else.

When Naval Ravikant says, in [his FS interview](https://fs.blog/naval-ravikant/?ref=commoncog.com):  “Basically, I try and set up good systems and then the individual decisions don’t matter that matter much. I think our ability to make individual decisions is actually not great” — this is what he means. When Taleb [says](https://ecorner.stanford.edu/in-brief/trial-with-small-error/?ref=commoncog.com) “you don’t have to be smart if you have (positive) convexity” this is what he’s referring to.

## Where Instrumental Rationality Matters

It’s worth it to take a step back and consider this from a broader perspective. Why is trial and error so effective at getting to success? And why do we need to be told this? A baby learning to walk or speak for the first time needs no instruction; he or she learns by trial and error. But we adults need thinkers like Taleb or frameworks like The Lean Startup to tell us to engage in rigorous trial and error.

To answer this question, I’m going to adapt a [section](https://commoncog.com/chinese-businessmen-let-reality-be-the-teacher/#the-two-approaches-to-problem-solving) I wrote in my Chinese businessmen series:

The answer, I think, lies in education.

Broadly speaking, there are [two basic approaches](https://en.wikipedia.org/wiki/Trial_and_error?ref=commoncog.com) to problem solving: trial and error, and theory and insight. It's difficult to overstate how much we are driven towards theory and insight by the formal education we receive.

Consider the problem of bridge building. The trial and error approach to bridge building would be to slap together a bunch of things until you find a design that works. Today, this approach is considered horrifying and is never used. We have since worked out the underlying physics of bridges, and have developed sophisticated tools to design them. Structural engineers can tell whether a design would work even before construction begins; an engineer who gets things wrong, in fact, would find himself shamed by society, and punished severely by the professional bodies in his industry.

And so we internalise: "trial and error is a stupid way to build bridges" and extend that. We think: "study hard in school; learn math, physics and engineering, and you will be able to *save yourself from error*."

Our education system prioritises learning from theory and insight. This seeps into other parts of our lives. Consider: when you have a decision to make, the bias of your education is to slow things down, to think things through, to look before you leap. It doesn't even cross your mind that failure might be the smart move. There is a straight line from the lesson about building bridges to your approach to decision making - you don't want to fail because failure is bad in 'theory and insight'. So you spend more time thinking than acting.

The good news is that thinking deeply and then acting is optimal for fields where a body of knowledge exists. It is also necessary for fields where the inherent cost of failure is high. But in fields where little is known, where failure costs can be made low, or where things change too quickly for theory to find handholds, trial and error dominates as the superior problem solving strategy. It dominates because it is *faster*: where the cost of failure is low, and iteration speed is high, each iteration allows you more information because it lets reality be the teacher.

In these fields, failure is an acceptable cost of learning. In these fields, thoughtfulness takes on a different form.

I believe that business is one such field. If you are an entrepreneur, the optimal strategy for learning in business is trial and error, because industries often change too quickly for there to be immutable and universal rules. The ‘traditional Chinese businessmen’ that dominate in South East Asia are thus the product of trial and error. They were not highly educated, nor did they go through academic-style epistemic rationality training. They engaged in trial and error and were *rational enough to learn from it*.

Taleb notes two important caveats to trial and error: first, you need to prevent yourself from blowing up. You can't trial and error if you go bankrupt, after all. Second, you need to be sufficiently rational when experimenting so that you do not make the same mistakes repeatedly. (And I will add a third injunction: Taleb’s model assumes infinite time; you also need to be sufficiently rational when picking trials instead of picking randomly, so that you don't die before you achieve success).

In a nutshell, Taleb is saying that the sort of rationality involved when engaged in trial and error is *instrumental rationality*. Here we have our first hints of an answer.

## Conclusion

We will follow the hint presented by Taleb’s ideas in the next part of this series, in our search for a good answer to the normative problem I mentioned earlier. For now, I want to wrap up what we’ve covered in Part 2, and mention a few caveats.

The first caveat is that while I present epistemic rationality and instrumental rationality as two different things — in accordance to the literature — I don’t mean to say that they are unrelated. Even the most epistemically rigorous trader has to have some amount of instrumental rationality if she is to be successful at trading. Conversely, a business person needs to to be able to evaluate evidence properly in her domain if she is to make good business decisions, even with the benefit of trial and error.

The main purpose of separating the two in our discussion is to recognise that the approach for putting models to practice in each type of rationality differs. If you are in a field where getting things *right* is important, then it might be worth it to work through the list of cognitive biases and their corresponding prescriptive models as LessWrong has done. But if you are like me — in a field where instrumental rationality matters more — then … well, stick around for the ideas in the next two parts.

The second caveat is that — while I have presented nothing genuinely novel in this part, that is going to change in the near future. In Part 3, I will venture outside the field of judgment and decision making and will begin to make some assertions of my own. In so doing, I will draw from my own practice (that is to say, my experience in testing this framework). This is vastly less rigorous. I urge you to be patient with me; in a latter part I hope to present a full account of the epistemological setup I mentioned in passing in Part 1. In other words, I will give you a new way to evaluate my claims, aka I will hand you the rope with which you can hang me on my credibility.

Alright, time to wrap up. Here’s a concept map of what we’ve covered so far:

![](https://commoncog.com/content/images/2018/12/Paper.Commonplace.32--1-.png)

First, I presented ‘rationality’ as the basis of structure for organising mental models. I explained how the study of rationality broadly splits the venture into studying ‘epistemic rationality’ and ‘instrumental rationality’. I argue that the list of mental models in Farnam Street belong to three different categories: descriptive models from other domains, thinking models that deal with epistemic rationality, and thinking models that deal with instrumental rationality.

I mentioned that the descriptive models might not be practicable, though we’ll discuss that later, once we touch on learning science (probably Part 4 or 5, I can’t tell yet).

I mentioned that the mental models in epistemic and instrumental rationality belong to a body of work that offers practicable methods, and gave you the underlying approach decision scientists use when presenting research on the topic so you may evaluate papers in the field yourself.

I also mentioned that epistemic rationality is far easier to start putting to practice, and that a community of such learners exist at [LessWrong](https://www.lesswrong.com/?ref=commoncog.com); they also have a non-profit, [CFAR](http://www.rationality.org/?ref=commoncog.com), dedicated to teaching such methods.

Next week, we’ll take a closer look at the normative problem brought up earlier in this essay: that is, “In decision making in real life, how do you know you’re getting better at making decisions? How do you know you are making the ‘best’ decisions?” In doing so, we will begin to move out of pure decision science, and into body of work around developing expertise. I’ll also explain what I’ve learnt from putting some of my ideas in practice, and what I think happens to be LessWrong’s big blindspot.

Go to *[Part 3: Better Trial and Error](https://commoncog.com/putting-mental-models-to-practice-part-3-better-trial-and-error/)*.

Originally published , last updated .

Previous Post

#### ← A Framework for Putting Mental Models to Practice, Part 1: Munger's Speech

Next Post

[![Feature image for What Process Improvement in Education Looks Like](https://commoncog.com/content/images/size/w300/2024/09/rigorous_process_improvement_education.jpg)](https://commoncog.com/what-process-improvement-in-education/)[![Feature image for Business Ecosystem Change Takes Time](https://commoncog.com/content/images/size/w300/2024/06/business_ecosystem_change_takes_time.jpg)](https://commoncog.com/business-ecosystem-change/)[![Feature image for Follow Your Nose](https://commoncog.com/content/images/size/w300/2021/03/follow_your_nose.jpeg)](https://commoncog.com/follow-your-nose/)[![Feature image for Create Your Own Rituals](https://commoncog.com/content/images/size/w300/2021/02/create_your_own_rituals.jpeg)](https://commoncog.com/create-your-own-rituals/)

---
title: "Putting Mental Models to Practice Part 3: Better Trial and Error"
source: "https://commoncog.com/putting-mental-models-to-practice-part-3-better-trial-and-error/"
author:
  - "[[Cedric Chin]]"
published: 2018-12-30
created: 2025-05-30
description: "Instrumental rationality is the sort of thinking that allows you to achieve your goals. We take a closer look at what decision science says is the 'best' way to pursue this purpose."
tags:
  - "clippings"
---
By

![Feature image for Putting Mental Models to Practice Part 3: Better Trial and Error](https://commoncog.com/content/images/size/w600/2018/12/burst-530182-unsplash--1-.jpg)

## Table of Contents

## Fan of great business?

Join 8,000+ sharp investors and operators like yourself, and we'll send you a collection of Commoncog's best articles right away:

*This post is Part 3 of [A Framework for Putting Mental Models to Practice](https://commoncog.com/a-framework-for-putting-mental-models-to-practice/). In [Part 1](https://commoncog.com/a-framework-for-putting-mental-models-to-practice-part-1/) I described my problem with Munger’s approach to mental models after I applied it to my life. In [Part 2](https://commoncog.com/putting-mental-models-to-practice-part-2-introduction-to-rationality/) I argued that the branch of academia (applied psychology, decision science, and behavioural economics) that studies rationality is a good place to start for a framework of practice. We closed our discussion in Part 2 with the observation that [Farnam Street’s list of mental models](https://fs.blog/mental-models/?ref=commoncog.com) were really made up of three types of models: descriptive models, thinking models concerned with judgment (epistemic rationality), and thinking models concerned with decision making (instrumental rationality). In Part 3, we will discuss putting into practice the mental models that are concerned with instrumental rationality.*

Let’s begin our discussion on instrumental rationality with a discussion of trial and error, as per Part 2’s discussion of Taleb’s ideas. To be precise, we want to know what an *ideal* form of trial and error looks like, so we may begin to discuss what it means to be ‘instrumentally rational’ in service of applying it to our lives.

To make this section more fun to read, I’ll invent a person named ‘Marlie Chunger’, who bears zero similarity to anyone real and living. Marlie is currently on a journey to mastery in some chosen domain with ‘positive convexity’ — that is, he can afford to conduct trial and error because the costs of getting things wrong in his domain are not so great.

So let’s begin by imagining that Marlie Chunger is faced with a problem.

![](https://commoncog.com/content/images/2018/12/Paper.Commonplace.33.png)

Baron's Search-Inference Framework of Thinking

In order to do trial and error, Marlie marshals his thinking, and then makes his first attempt at solving the problem.

![](https://commoncog.com/content/images/2018/12/Paper.Commonplace.34.png)

Baron's Search-Inference Framework of Thinking

The attempt fails. Marlie reflects on the failure, draws lessons from this first, failed attempt, and then — by incorporating these lessons — tries again.

![](https://commoncog.com/content/images/2018/12/Paper.Commonplace.35.png)

Baron's Search-Inference Framework of Thinking

After *n* tries of failure and reflection, Marlie finally finds an approach that succeeds.

![](https://commoncog.com/content/images/2018/12/Paper.Commonplace.36.png)

Baron's Search-Inference Framework of Thinking

Marlie then reflects on the successful approach in an attempt to generalise it. He doesn’t enshrine it in his toolbox quite yet, as his success may well have been a fluke. Instead, he waits for a second problem that is much like the first to appear in his life, and attempts to apply his previously successful approach. Sometimes this doesn’t work out the way it did the first time, but it gets him 80% of the way there. By tweaking this approach, he gets it to work. He then generalises it into a ‘principle’ or ‘approach’ that is a full member of his toolbox.

![](https://commoncog.com/content/images/2018/12/Paper.Commonplace.37.png)

Baron's Search-Inference Framework of Thinking

(Of course, in future iterations of this problem, Marlie might find he would need to make further tweaks to his successful technique because of variations he hadn't accounted for, or perhaps in the future he might come across a new technique that is superior to his discovered approach in every given way. A fully rational Marlie would then either modify his approach by testing it, or toss out his hard-won approach in favour of the superior one).

So let’s ask: how might this process fail? By applying even a little thought, we may see that trial and error may fail for a variety of reasons:

1. You may fail by blowing up — that is, you go bankrupt after one trial, or suffer a failure so bad it puts you out of the game forever.
2. You select trials randomly, without ever learning from your failures. This scattershot approach means you have to rely on luck. A more likely version of this failure is:
3. You select trials *suboptimally*, because you don’t search for relevant approaches, or you don't fully reflect on your failures to figure out what to vary for future trials. This isn’t a weakness per se, because with sufficient trials even a bad selector may eventually converge on a solution.
4. You irrationally repeat the same trial over and over again, expecting different results.
5. You erroneously consider the problem unsolvable, and opt to stop your cycle of trial and error.
6. You never pause to generalise your working approach, and so have to continue performing trial and error when coming across a new problem that is actually similar to the current one. This robs you of the efficiency that applying an approach that has already been shown to work.

“Aha!” I hear you think. “Ideal trial and error seems easy enough to apply to my life — I just have to avoid all these problems! Stand back and watch me *win!*”

Not so fast. Consider for a moment the difficulty of dealing with problematic people in your life. It’s likely that you — like me — have one or more ‘personality types’ that you have trouble working with. For instance, you might dislike ‘Susan’, and have difficulty with everyone you meet who is similar to ‘Susan’, or who reminds you of ‘Susan’. You complain to your friends about ‘people like Susan’, and say “I can’t stand her! She drives me nuts!”

A disinterested observer, standing outside of your life, might suggest that your emotional cascade from dealing with ‘people like Susan’ is blinding you to the fact that you are reacting with her ‘personality type’\* the same way over and over again, each time you meet someone like her in your life. Your reactions cause the same results, because *of course they would*. Each confirmation simply convinces you ever more that you *can’t* deal with people like ‘Susan’, and that you should just avoid people you think are like her, and that “it’s not worth it to try.”

*\* (I don’t mean ‘personality type’ here in a scientific sense. We know from research that we have all sorts of biases that cloud our judgment of people, and may in fact contribute to our problems. This is somewhat besides the point; I’m using 'personality type' in this case to describe the situations where you react to the perceived similarity of some person in bad ways.)*

I bring up ‘dealing with difficult people’ as an example because it represents for many of us a vivid illustration of the difficulties of trial and error in a real life situation. For whatever reason, when it comes to working with people, we tend to fall prey to our biases, blind spots, and emotional cascades, and greatly reduce the odds of successfully applying trial and error when developing approaches to dealing with them.

Here we have our first whiff of potential problems with trial and error.*Why* do people make such mistakes? *How* do they fall into the trap of one of the five failure modes of trial and error? Can we learn models from instrumental rationality that will help us with these failure modes?

The answer is yes. But because each of these failures involve thinking in the service of decision making, we will need a framework of thinking in order to answer this question with a satisfactory amount of specificity. And so now we turn to Jonathan Baron.

## A Framework for Thinking

In *Thinking and Deciding*, Jonathan Baron offers up a framework of thinking that allows us to analyse the prescriptive models that appear in the decision making literature (Chapter 1, pg 6). He calls it the ‘search-inference framework’, and it’s the framework that I’ve adopted for my approach to putting mental models to practice.

The search inference framework states that all of thinking can be modelled as a *search* for Possibilities, Evaluation Criteria (that Baron calls ‘Goals’), and Evidence. In addition to a process of search, a process of *inference* also happens as we strengthen or weaken the possibilities, by weighing the evidence we have found for each possibility in accordance to a set of evaluation criteria.

That description of the framework is a little dry, so I’ve adapted the following example from Baron’s book.

Imagine that you are a college student trying to decide which courses you will take next term. You are left with one elective to select, having already scheduled the required courses for your major. The question that starts your thinking is “which course should I take?”

You begin your search by browsing the course catalog of possible elective courses. You also ask friends for possible electives that they’ve enjoyed. One friend says that she enjoyed a course last term on Soviet-American relations. You think that the subject sounds interesting, and you’ve always wanted to learn more about modern history. You ask her about the workload, and she says that there was a lot of reading and a twenty-page paper. You realise that you can’t cope with that amount of work because of the assignments you’ll have to do for your required courses. You resolve to look elsewhere.

As you browse the list of courses on your college website, you come across a course on ‘American History After World War II’. You remember hearing about this course before, from your friend Sam. This course has the same advantage as the first course — it sounds interesting, and it is about modern history — and you think that the course work wouldn’t be too difficult. You go off to find Sam to ask her questions about your choice.

In this example we see all the elements of the search-inference framework in action. Your thinking process involves a search directed at removing your doubt. You search for possibilities — that is, possible course options — by searching internally (from your memory) and externally (from the course catalog website, and from your friends). As you perform this search, you determined the good features of two courses, some bad features of one course, and a set of evaluation criteria, such as the fact that you don’t want a heavy course load for this elective. You also made an inference: you rejected the course on Soviet-American relations because the work was too hard.

The search-inference framework, then, concerns three objects:

1. **Possibilities** are possible answers to the original question. In this case they are the course options you may take.
2. **Evaluation criteria** (or ‘ **goals** ’, as Baron originally calls them) are the criteria by which you evaluate the possibilities. You have three goals in the above example: you want an interesting course, you want to learn something about modern history, and you want to keep your work load manageable.
3. **Evidence** consists of any belief or potential belief that helps you determine the extent to which a possibility achieves some goal. In this example, the evidence consists of your friend’s report that the course was interesting and the work load was heavy. At the end of the example, you resolved to find your friend Sam for more evidence about the work load on the second course.
![](https://commoncog.com/content/images/2018/12/Screenshot-2018-12-30-at-12.35.06-AM-1.png)

Baron's Search-Inference Framework of Thinking

While the example above is primarily concerned with thinking about decisions (instrumental rationality), Baron’s search-inference framework also applies to thinking about beliefs (epistemic rationality). When you think about beliefs, you are doing the same search-inference process: that is, you search for possibilities (“I believe the death penalty is moral”, or “I don’t believe the death penalty is moral”), marshal evidence in favour of each possibility, and construct a set of evaluation criteria or ‘goals’ (e.g. “I want to have beliefs aligned with my religion”) that would strengthen or weaken (or ignore!) the evidence for each possibility.

Why just these phases: the search for possibilities, evidence and evaluation criteria/goals, and inference? Baron explains:

> Thinking is, in its most general sense, a method of finding and choosing among potential possibilities, that is, possible actions, beliefs, or personal goals. For any choice, there must be purposes or goals, and goals can be added to or removed from the list. I can search for (or be open to) new goals; therefore, search for goals is always possible. There must also be objects that can be brought to bear on the choice among possibilities. Hence, there must be evidence, and it can always be sought. Finally, the evidence must be used, or it might as well not have been gathered. These phases are “necessary” in this sense.

I’ve found that the search-inference framework for thinking to be incredibly useful as an **organising framework for mental models of thinking**. For instance: now that we have this framework, we may ask ourselves: how do we think badly? Baron argues that there are three primary ways the search-inference framework can fail:

First, the framework will fail if we don’t search properly for something that we should have discovered (that is, possibilities, evaluation criteria, or evidence), or we act with high confidence after little search. For example, in the course-selection example above, if you had not discovered ‘American History After World War II’ as a possible course option, you would not have selected it; alternatively, if you had not considered ‘light course load’ as an evaluation criteria, you would not have chosen well.

Second, we may think badly if we seek evidence and make inferences in ways that prevent us from choosing the best possibility. For instance, myside bias, ego, or a ‘desire to look good’ will prevent you from drawing the best inferences. Baron notes that this is the most dangerous form of error of the three listed here. People tend to seek evidence, seek evaluation criteria and make inferences in a way that favours possibilities that already appeal to them.

Third, the search-inference process can go badly if we think too much. Remember that good thinking and decision-making satisfies multiple goals — including the conservation of mental power. You want to spend an amount of time that is commensurate with the importance of the decision, because otherwise you would hit diminishing returns in your search and inference process. In practice, the error of thinking too much is often caused by a lack of expertise.

The search-inference framework makes all sorts of questions that you might have regarding mental models for decision making clearer. For instance: *why is [inversion](https://fs.blog/2013/10/inversion/?ref=commoncog.com) effective?* The answer, immediately offered to us by the search-inference framework, is that inversion helps us with *search*. That is, it allows us to search more effectively by whittling down undesirable evaluation criteria, or possibilities that get in the way of our goals. It also gives us *new possibilities* that we might not have considered otherwise. In this way we may say that inversion helps us with combating the first error: that is, the error of insufficient search.

The search-inference framework suggests a quick exercise that we may do: go through the [list of mental models on Farnam Street](https://fs.blog/mental-models/?ref=commoncog.com), and pick out the ones that are concerned with instrumental rationality. Ask yourself: which part of the search-inference framework does this help me with?

What you’ll find is that *all* the mental models that concern themselves with decision making help with one or more of the three types of errors above.

Consider this also: when Charlie Munger says that ‘having a latticework of mental models’ will help you become better at decision making, *what part of the search-inference framework is he talking about?* What are the limitations of this approach? What are the benefits? In which fields or applications would Munger’s prescription work better, and in which fields or applications would it not?

I’ll deal more directly with the questions about Munger’s prescription later in the series. For now, however, I’d like to turn to the great mystery of putting instrumental rationality to practice: how do you know that you’re getting better at decision making? And, in a roundabout way, why did LessWrong not get very far on this very question?

## What Does The Research Consider ‘Good Decision Making’?

I’ve mentioned in Part 2 of this series that LessWrong’s approach to epistemic rationality was to reach into the literature on judgment and decision making and draw out prescriptive and normative models for practice. This approach works rather well when used on the study of epistemic rationality, because so much of the practice of epistemic rationality is to find systematic biases and squash them. But I’ve asserted that it deals rather badly with the field of instrumental rationality — that is, the field of decision making.

Why is this the case?

To answer this, we need to take a look at the normative approach that the field of decision science has used in order to evaluate good decision making. The dominant approach in decision science is something called *[expected utility theory](https://en.wikipedia.org/wiki/Expected_utility_hypothesis?ref=commoncog.com)*, which was created by Daniel Bernoulli in 1738. It asserts that a person acts rationally when they choose that which maximises their utility — that is, whatever decision it is that *brings them the most benefits in pursuit of their goals.*

(There is a secondary claim in Bernoulli’s original theory, which is the concept of risk aversion. For example, if you were gambling and had become rich, you would become more and more risk adverse with each additional bet, because the additional utility of money you might win would not be as valuable to you as when you were poor. We call this ‘marginal utility’ today, and expected utility theory is the first instance someone came up with this idea. This is not related to our discussion; I’m just leaving this here for completeness.)

Utility in the sense of expected utility theory is not the same as pleasure or goodness. It does not mean optimising for money, or happiness. You might be reminded of Jeremy Bentham’s conception of utility in [Utilitarianism](https://en.wikipedia.org/wiki/Utilitarianism?ref=commoncog.com), but this isn’t exactly that either. Instead, utility represents a *measure of goal achievement* — and it respects whatever goal it is that you want to achieve.

For example, if your goal is to “make lots of money doing work you love”, and you are given the choice of doing a degree in Medicine, Computer Science, or Economics, expected utility theory asserts that the best choice in this case is the one that best allows you achieve your goal of “making lots of money doing work you love”. The way that expected utility theory is applied here is that it gives a probability to each state of the world that might exist, and for each state, multiplies that probability by the expected utility of the state. The overall expected utility for a given option is the sum of all the states and probabilities.

![](https://commoncog.com/content/images/2018/12/Paper.Commonplace.38.png)

Expected utility calculation for three courses and four possible states.

In this illustration, each cell contains a utility value. I’ve given arbitrary values for each of the three options given the four possible states of the world: a state in which you make lots of money and love your work, a state in which you make lots of money but hate your work, a state in which you make no money but love your work, and a state where you make no money and you hate your work. Notice that it doesn’t matter what the scale is for these utility values — you merely have to ensure that the scale is consistent across the three options.

The expected value calculation of selecting Medicine, then, is the sum of the product of multiplying the probability of each state with the utility value of each state.

Applying these probabilities to each state of the world and coming up with these utility values is an act of judgment — which is, of course, fallible. But if you choose the option with the highest utility, you would be said to be instrumentally rational.

Now, I can already hear the alarm bells going off in your head. “Can you even measure utility? Isn’t any attempt to measure utility doomed from the beginning, because of real-world uncertainty? What if you change your goals midway, like you suddenly realise that Medicine isn’t for you the first time you see the insides of a real human?”

These are all valid objections. Let’s deal with them quickly.

First, expected utility theory is about *inference*, not search. It assumes you have all the decisions laid out before you, and it concerns itself with what you *currently know* at this point in time. If you gain more information later, or decide to change your goals after inserting a urinary catheter for the first time — that isn’t a problem for the theory, since all it says is that you have chosen rationally at a point in time in the past, given the information that you had then.

Second, you are absolutely right in that judging utility is difficult — nearly impossible! — in real-world situations. But one way decision scientists have gotten around this is to show that, so long as you follow the [Von Neumann-Morgenstern axioms](https://en.wikipedia.org/wiki/Von_Neumann%E2%80%93Morgenstern_utility_theorem?ref=commoncog.com), you may be said to have acted in ways that maximised your utility. In other words, so long as you don’t violate certain ‘rules’ (which the fancy shmancy academics call ‘axioms’ because they really like math), you may be considered instrumentally rational without worrying about your ability to judge actual utility.

The Von Neumann-Morgenstern axioms are sometimes called the ‘axioms of choice’. Of these, the two most important are the ‘axiom of completeness’ and the ‘axiom of irrelevant alternatives’. The other axioms follow logically from these, so we won’t discuss them.

The axiom of completeness, sometimes called the ‘principle of weak-ordering’ is the idea that you *must* have an order of preferences when making decisions. It doesn’t matter if each decision has a multitude of pros and cons, or that you find it very difficult to state your preferences for them; you simply *must* have a preference — this is the requirement we impose. For instance, if you are given the choice of Medicine, Computer Science and Economics, you must order them in some way: for instance, that you prefer Medicine to Computer Science, and Computer Science to Economics. You are also allowed to be indifferent to some of these choices, e.g. “I prefer Medicine to Computer Science, but I don’t care if you give me Computer Science or Economics. They’re both equivalently bad in my mind.”

The first axiom also implies ‘transitivity’, which is just a fancy way of saying that if you prefer Medicine to Computer Science, and Computer Science to Economics, you must also prefer Medicine to Economics. You can’t say “oh, I prefer Economics to Medicine”, because this means you are crazy.

The second axiom is the idea that you should ignore irrelevant alternatives that have no bearing on your decisions. The simplest example I’ve found is from Michael Lewis’s *The Undoing Project*, which goes something like:

> You walk into a deli to get a sandwich and the man behind the counter says he has only roast beef and turkey. You choose turkey. As he makes your sandwich he looks up and says, “Oh, yeah, I forgot I have ham.” And you say, “Oh, then I’ll take the roast beef.” (…) you can’t be considered rational if you switch from turkey to roast beef just because they found some ham in the back.”

It may seem trivially easy to follow these axioms, but Kahneman and Tversky’s work on [Prospect Theory](https://en.wikipedia.org/wiki/Prospect_theory?ref=commoncog.com) shows us that humans violate these two principles in all sorts of interesting (and systematic) ways. This finding implies that humans are naturally irrational, even in straight-forward decision making problems of the sort tested by their research program. But it also implies that being able to make decisions in ways that don’t violate these axioms is quite the achievement indeed.

A naive approach is to assume that one may simply take this model of instrumental rationality and apply it to one’s life. This is, in fact, what LessWrong tried to do. And in this, I believe that LessWrong has failed miserably.

## What Do Expert Practitioners Really Do?

It’s safe to assume that expert practitioners make better decisions on a day-to-day basis than a novice in their domains. If you assigned a decision scientist to them, and have them follow their every move as they make decisions for whatever domains they are in, we should expect to find that they deviate less from expected utility theory than less successful practitioners given similar goals.

(There are some problems with this assertion, because — as per Taleb and Mouboussin, these practitioners could merely be lucky. But we shall ignore this for now).

So let’s consider: *how* do they achieve their results? The answer isn’t that they applied the methods of expected utility theory as a prescriptive model. Nobody really expects Warren Buffett to sit down and do utility calculations for *all* of the decisions he is about to make — this would mean he would have little time to spend on anything else.

While expected utility theory is sometimes used for [decision analysis](https://en.wikipedia.org/wiki/Decision_analysis?ref=commoncog.com) — especially in business and in medicine — it is too impractical to recommend as a general decision-making framework. As Baron puts it: “search has negative utility”. The more time you spend analysing a given decision, the more negative utility you incur because of diminishing returns.

The second problem with using expected utility theory as a personal prescriptive model is that, in the real world, *judgments and results actually matter*. I’ve described a method for evaluating instrumental rationality independent of goals or proper utility judgment — that is, we simply check to see if someone has violated the axioms of choice. But instrumental rationality *is not the goal in the real world.* Achieving your goals is the goal of better decision making in the real world. You want to ‘win’, not simply ‘get better at a measure of instrumental rationality’.

The blind spot that LessWrong had, I think, is that none of them were *practitioners* — of the sort that might go after worldly goals. They were, as Patri Friedman put it, “people who liked to sit around talking about better ways of seeing the world”. They were interested in the clean, easily provable results of judgment and decision making research, and less interested in looking at the messy reality of the world to see how expert practitioners *actually* put rationality to practice.

So how *do* expert practitioners put rationality to practice?

The answer is that in many fields, they do not.

## The Trick

What you might not have noticed is that I've just played a trick on you.

I have combined two world views of decision making into one essay. The first view, hinted at by my illustration of Marlie Chunger, is the world view of Nicholas Nassem Taleb, Herbert Simon, and Gary Klein: a view that I’m going to lump together under the banner of Klein’s field of *naturalistic decision making.* This world view stems from the premise that we cannot know the state of the world, that we do not have the mental power to make comprehensive searches or inferences, and that we should build our theories of decision making by empirical research — that is, find out what experts *actually do* when making decisions in the field, and use that as the starting point for decision making.

This view strives to build tacit mental models via ‘experience’ or ‘expertise’ — that is, through training, practice, or trial and error. It strives for ‘satisficing’ a set of goals, not ‘optimising’ for some utility function. It is antithetical to the study of mental models independent of practice. This view sees heuristics — the same sorts of heuristics that lead to cognitive biases — as strengths, not weaknesses.

The second view is the view of Munger, Baron, Tversky, Kahneman, and Stanovich: that of rational decision analysis. This is the world view that we have explored for most of this essay. It assumes that you want to make the best decisions you can, perhaps because they are not reversible. It asserts that you can strive to achieve the modes of thought dictated by expected utility theory, which is designed to optimise your decisions with regard to your goals. It gives us a framework of thinking, and tells us that we may apply mental models to our decision making processes in order to prevent the three problems of the search-inference framework. It prescribes building a latticework of mental models, because building this latticework assists us with search — and sometimes with inference.

You can probably tell that I sit more with the Taleb and Klein view of the world, as opposed to the Munger and Baron view. But both are valid, and needed. Shane’s defence of his methods is fair: there *is* a place for the elucidation of mental models independent of practice.

Why is this the case?

Consider the examples of decision making that we’ve considered above. Questions like ‘what course should I choose for my next term?’ are trivial, perhaps, but there are more important decisions in life like “should I move to Tokyo?”, “should I take that job?” and “what degree should I pick?” that will determine the shape of our lives for years to come. These decisions lend themselves well to the mental models of instrumental rationality. We need to conduct sufficient search, and we must guard against our biases when inferring conclusions from evidence.

Instrumental rationality also lends itself to the sort of thinking that makes for efficient trial and error. With proper search and inference, we may prevent ourselves from falling into one of the five failure modes that we’ve covered when we discussed Marlie Chunger.

But this is not all there is to it. Perhaps you might have noticed that I’ve *not* talked about the approaches that Marlie Chunger keeps in his toolbox. These approaches — built by trial and error — are mental models, are they not? They allow Marlie to solve problems in pursuit of his goals.

![](https://commoncog.com/content/images/2018/12/Paper.Commonplace.37-1.png)

However, they are *not* the kind of mental models that may be put into the search-inference framework that Jonathan Baron proposes. These approaches are intuitive. They belong to ‘System 1’, Kahneman’s name for the system that produces fast thinking. These models are *tacit*, not explicit, and they perform very differently from the search and inference methods of explicit thinking. We often call them ‘expert intuition’. They make up the basis of expertise.

Let me state the question that has bugged me in the years since I tried to put Munger’s talk to practice in my life, and failed. If rational thinking and good decision making are so important, how is it that an entire generation of Chinese businessmen have build a variety of successful businesses in South East Asia — in the face of racial discrimination, immigration, and the after-effects of colonisation, with *nearly no education and little demonstrable rationality?*

We could say that they were lucky. But attributing their success to luck is not useful to us. What intrigues me is that I’ve seen some of these businesspeople up close, and they commonly demonstrate wisdom in their decision making, built on top of intuition and (in Robert Kuok’s words) ‘rhythm’. I do not believe these are perfect decision makers. I think they would perform terribly at Kahneman and Tversky’s cognitive bias tests, and I think they would fail if they were pit against investors like Munger, in a field like stock picking. But in the fields of business that they are in, where trial and error are possible, ‘good enough’ expert intuition seem to also lead to successful outcomes. Optimal search and inference thinking can’t be all that’s at work here.

We should find out what it is.

## Conclusion

I must close this essay with an apology. I know that I promised a good answer to the normative question “how do I know that I’m making better decisions?” But I had too much ground to cover, and ran out of space.

What have we covered in this essay? We’ve covered the basics of trial and error, and the five ways it may fail. We have covered Baron’s search-inference framework of thinking, and used it as an organising framework for mental models of decision making. We have covered the foundations of decision science — or at least, the foundations of decision science as related to instrumental rationality. You now understand the basics of expected utility theory — the normative model that is used as the goal of mental models in decision making.

As per usual, here is a concept map of what we’ve covered:

![](https://commoncog.com/content/images/2018/12/Paper.Commonplace.39-2--1-.png)

How do these concepts help you with practice? I believe that Baron's Search-Inference Framework gives us a template to analyse our thinking. You may now organise your search for techniques that will help you in preventing one of the three errors — for example, you may search for mental models that help you perform more efficient search, or mental models that will help prevent your biases from affecting your judgment. If you've made a bad decision, you may now introspect and ask yourself: which of the three errors have I committed? What biases did I display when attempting to do deliberative decision making?How might I conduct better search in the future?

More importantly, you now understand that *deliberative* decision making is just one of two decision making approaches. There is another, less discussed form of decision making, and it will be the topic of our next essay.

Next week, we’ll talk about building expertise, and the kinds of decision making that emerges from it. (I should note that the tension between expert intuition and decision-making rigour has been covered by Shane on Farnam Street many, many times in the past.) Finally, I hope to tie things up by giving a good normative goal for practice — or at least, one that seems to work well for me.

*Go to [Part 4: Expert Decision Making](https://commoncog.com/putting-mental-models-to-practice/)*

Originally published , last updated .

Previous Post

#### ← Putting Mental Models to Practice, Part 2: An Introduction to Rationality

Next Post

[![Feature image for What Process Improvement in Education Looks Like](https://commoncog.com/content/images/size/w300/2024/09/rigorous_process_improvement_education.jpg)](https://commoncog.com/what-process-improvement-in-education/)[![Feature image for Business Ecosystem Change Takes Time](https://commoncog.com/content/images/size/w300/2024/06/business_ecosystem_change_takes_time.jpg)](https://commoncog.com/business-ecosystem-change/)[![Feature image for Follow Your Nose](https://commoncog.com/content/images/size/w300/2021/03/follow_your_nose.jpeg)](https://commoncog.com/follow-your-nose/)[![Feature image for Create Your Own Rituals](https://commoncog.com/content/images/size/w300/2021/02/create_your_own_rituals.jpeg)](https://commoncog.com/create-your-own-rituals/)

---
title: "Mental Models: Expert Decision Making"
source: "https://commoncog.com/putting-mental-models-to-practice/"
author:
  - "[[Cedric Chin]]"
published: 2019-01-05
created: 2025-05-30
description: "Experts make decisions in ways that are very, very different from conventional decision science models. How so? Let's find out."
tags:
  - "clippings"
---
## Putting Mental Models to Practice Part 4: Expert Decision Making

By

![Feature image for Putting Mental Models to Practice Part 4: Expert Decision Making](https://commoncog.com/content/images/size/w600/2019/01/connor-betts-736233-unsplash-2.jpg)

## Table of Contents

## Fan of great business?

Join 8,000+ sharp investors and operators like yourself, and we'll send you a collection of Commoncog's best articles right away:

*This is Part 4 of a [series of posts](https://commoncog.com/a-framework-for-putting-mental-models-to-practice/) on putting mental models to practice. In [Part 1](https://commoncog.com/a-framework-for-putting-mental-models-to-practice-part-1/) I described my problem with Munger’s approach to mental models after I applied it to my life. In [Part 2](https://commoncog.com/putting-mental-models-to-practice-part-2-introduction-to-rationality/) I argued that the study of rationality is a good place to start for a framework of practice. We learnt that [Farnam Street’s list of mental models](https://fs.blog/mental-models/?ref=commoncog.com) consists of three types of models: descriptive models, thinking models concerned with judgment (epistemic rationality), and thinking models concerned with decision making (instrumental rationality). In [Part 3](https://commoncog.com/putting-mental-models-to-practice-part-3-better-trial-and-error/), we were introduced to the search-inference framework for thinking, and discussed how mental models from instrumental rationality may help prevent the three errors of thinking as prescribed by the search-inference framework. We also took a quick detour to look at optimal trial and error through the eyes of Marlie Chunger. In Part 4, we will explore the idea that expertise leads to its own form of decision making. This is where my views begin to diverge from Farnam Street’s.*

Many experts, in many fields, don’t do the sort of thinking we have explored in our previous part. They don’t do rational choice analysis. They don’t do expected utility calculations. They don’t adjust for biases, they don’t do appropriate search-inference thinking and they don’t use a ‘latticework of mental models from other fields’ to draw inferences.

And yet, despite a lack of grounding in the ‘best practices’ of decision science, they appear to perform remarkably well in the real world.

So what do they do?

They do [recognition-primed decision making](https://en.wikipedia.org/wiki/Recognition_primed_decision?ref=commoncog.com).

Recognition-primed decision making (henceforth called RPD) is a *descriptive* model of decision-making: that is, it describes how humans make decisions in real world environments. RPD is one of the thinking models from the field of [Naturalistic Decision Making](https://en.wikipedia.org/wiki/Naturalistic_decision-making?ref=commoncog.com) (NDM), which is concerned with how practitioners *actually* make decisions on the job. NDM researchers eschew the lab and embed themselves in organisational settings, and take an almost anthropological approach to research. They follow firefighters on calls, sit in tanks during military exercises, and interview interface designers and programmers in the office, during work tasks. The way the researchers do this is to use an interviewing technique they call Cognitive Task Analysis, which is designed to draw out *tacit mental models of expertise*.

NDM is interesting to us because it represents an alternative school of thought to conventional decision analysis. The growth of the cognitive biases and heuristics research program has created a number of prescriptive models — rooted in expected utility theory — designed to help us make more ‘rational’ decisions. We’ve covered the basics of that approach in [Part 3](https://commoncog.com/putting-mental-models-to-practice-part-3-better-trial-and-error/). But the cognitive biases and heuristics program has also sucked up a lot of attention in recent years. We continually hear endless stories about the fallibility of human judgment and the errors and biases that systematically cloud our thinking. Whatever popular science books have to say about NDM is limited to the sort of ‘magical intuition’ as portrayed by Malcolm Gladwell in *Blink*.

This is a terrible portrayal. Because expert intuition is often portrayed as ‘magical’, we ignore it and turn to more rational, deliberative modes of decision making. We do not believe that intuition can be trained, or replicated. We think that rational choice analysis is the answer to everything, and that amassing a large collection of mental models in service of the search-inference framework is the ‘best’ way to make decisions.

It is telling, however, that the US military uses RPD models for training and analysis of battlefield decision-making situations. The field of NDM arose out of psychologist Gary Klein’s work, done in the 90s, for the US Army Research Institute for the Behavioural and Social Sciences. In the 70s and the 80s, the US Government had spent millions on decision science research — that is, on *conventional* models rooted in rational choice analysis — and used these findings to develop expensive decision aids for battle commanders.

The problem they discovered was that nobody actually used these aids in the real world. The army had spent ten years worth of time and money on research that didn’t work at all. It needed a new approach. *(Chapter 2, Gary Klein, Sources of Power)*.

## How Experts Make Decisions

RPD describes the decision making that emerges from expertise. This is a huge differentiating factor; the majority of results in classical decision science was built on behavioural experiments performed on ordinary people. The model goes as follows:

Let’s say that an expert encounters a problem in the real world. The first step in the RPD model is recognition: that is, the expert perceives the situation in a changing environment and pattern matches it against a collection of prototypes. The more experience she has, the more prototypes or analogues she has stored in her [implicit memory](https://en.wikipedia.org/wiki/Implicit_memory?ref=commoncog.com).

This instantly generates four things. The first is a set of ‘expectancies’ — the expert has some expectations for happens next. For instance, in a firefighting environment, an experienced firefighter can tell where a fire would travel, and how a bad situation might develop. An experienced computer programmer can glance at some code and tell if the chosen abstraction would lead to problems months down the road. A manager can hear two pieces of information during lunch and predict production delays two months in the future.

The second thing this recognition generates is a set of *plausible goals* — that is, priorities of what needs to be accomplished. In a life-or-death situation, a platoon commander has to prioritise between keeping his troop alive, getting to advantageous cover, and achieving mission objectives. His recognised prototype instantly tells him where his priorities lie in a given situation, freeing cognitive resources to conduct other forms of thinking.

The third thing that is generated is a set of relevant cues. A novice looking at a chaotic situation will not notice the same things that the expert does. Noticing cues to evaluate a changing environment is one of the benefits of experience — and it is necessary in order to prevent information overload. Think of driving a car: in the beginning, you find yourself overwhelmed with the number of dials and knobs and mirrors you have to keep track of. After a few months, you do these things intuitively and notice only select cues in your environment as you drive.

Last, but not least, a sequence of actions is generated by the expert, sometimes called an ‘action script’ — and it presents itself, fully formed, in the expert’s head.

![](https://commoncog.com/content/images/2019/01/Paper.Commonplace.42.png)

The fact that RPD depends so much on implicit memory means that this entire process happens in a blink of an eye. Consider how you might recognise a person walking into the room as your friend ‘Mary’ or ‘Joe’. Facial recognition is an implicit memory task: the information ‘magically’ makes itself known to you. Similarly, an expert confronted with a work-related problem will perform the recognition and generate the four by-products instantaneously. When initially interviewing firefighters, Klein’s researchers found that the firefighters would often assert that they were *not* decision making: instead, they arrived at the scene of a fire and knew immediately what to do.

This is, of course, not a complete picture. What happens when the situation is unclear, or when the initial diagnosis is flawed? Here’s a story from *Sources of Power* to demonstrate what happens next:

> “The initial report is of flames in the basement of a four-story apartment building: a one-alarm fire. The commander arrives quickly and does not see anything. There are no signs of smoke anywhere. He finds the door to the basement, around the side of the building, enters, and sees flames spreading up the laundry chute. That’s simple: a vertical fire that will spread straight up. Since there are no external signs of smoke, it must just be starting.  
>   
> The way to fight a vertical fire is to get above it and spray water down, so he sends one crew up to the first floor and another to the second floor. Both report that the fire has gotten past them. The commander goes outside and walks around to the front of the building. Now he can see smoke coming out from under the eaves of the roof. It is obvious what has happened: the fire has gone straight up to the fourth floor, has hit the ceiling there, and is pushing smoke down the hall. Since there was no smoke when he arrived just a minute earlier, this must have just happened.  
>   
> It is obvious to him how to proceed now that the chance to put out the fire quickly is gone. He needs to switch to search and rescue, to get everyone out of the building, and he calls in a second alarm. The side staircase near the laundry chute had been the focus of activity before. Now the attention shifts to the front stairway as the evacuation route.”

Where were the decisions here? The firefighter sees a situation — a vertical fire — and immediately knows what to do. But a few seconds later, he cancels that diagnosis because he recognises a different situation, and orders a different set of actions.

In rational choice theory, the firefighter is not making any decisions at all because he is not comparing between possibilities. He is simply reading the environment and taking action. Klein and co argue that decisions are still made, however: at multiple points in the story, the firefighter could have chosen from an array of options. The fact that he merely considers one option at each decision point doesn’t mean he *didn’t* make a decision — it is simply that he considers his options in linear fashion, finds the first that is satisfactory, and then acts on it.

There are two conclusions from looking at this process. The first is to conclude that when we make decisions, we naturally *look at our options one at a time.* It just so happens that the first option an expert practitioner generates is often good enough for her goals. In many real world situations, it doesn’t matter that you arrive at the best solution — Herbert Simon argues that the ‘administrative man’ strives to find the first solution that satisfies a set of goals and constraints, instead of searching for an optimal solution to a problem. He calls this process ‘satisficing’; that is certainly what is happening here.

But even so, with enough practice in a ‘regular’ field (like chess), it *is* possible for this process to arrive at optimal or near-optimal decisions. When Klein and his team studied grandmasters, they found that most of them examined options one at a time, and engaged in comparison with a second option only in a minority of instances. However, the bulk of their decisions were still considered to be close to optimal.

The second conclusion we may draw is to say that our tendency to consider only one option at a time leaves us open to the first type of failure in the search-inference framework of thinking: that is, that we fail to conduct proper search. This is a good point, and RPD presents an explanation for why thinking failures affect us so systematically. But Klein points out that if you are in a field where you can build expertise, the answer *isn’t* to learn better deliberative decision making skills that slow things down — instead, the answer is *to go out and build expertise of your own*.

At any rate, we need to update the RPD model to include diagnosis. At the first step of RPD, prototype recognition generates a set of expectancies — things that the expert expects to happen. When the expectancies are violated, the firefighter immediately goes back to prototype recognition.

![](https://commoncog.com/content/images/2019/01/Paper.Commonplace.40.png)

Is this all there is? No, it isn’t. There is an additional, final part to this model, needed in order to explain expert problem solving: that is, the occurrence of novel, sometimes wildly creative problem solving. What happens in such situations? The answer is this: when a novel or complicated situation presents itself to the expert, the expert will generate a sequence of actions and simulate it in her head. If a problem with those actions emerges during the simulation, she would go back to the drawing board and redo the solution, this time with a different approach. This continues until the expert settles on a workable solution. If the expert fails to come up with a workable solution, she will then drop back to diagnosis — to see if she has missed something in her reading of the situation.

Our final RPD model now looks like this:

![](https://commoncog.com/content/images/2019/01/Paper.Commonplace.41.png)

I should note that RPD is valuable to me for one other reason: that is, after a years-long search with the goal of self-improvement, I have finally found a usable model that *captures the essence of expertise.*

## Heuristics as strengths, not weaknesses

Alert readers will note that in the hands of an expert practitioner, what are considered sources of bias in rational choice analysis are considered strengths in the RPD model.

The [availability heuristic](https://en.wikipedia.org/wiki/Availability_heuristic?ref=commoncog.com), demonstrated by Kahneman and Tversky in 1973, says that we bring what is immediately available in our working memories to bear on our thinking. In the lab, unsuspecting subjects rely on what is easily available to them to make judgments — leading them to make wildly wrong ones. In the real world, experts in regular fields draw from a large reservoir of perceptual knowledge to make quick, correct decisions.

The [representativeness heuristic](https://en.wikipedia.org/wiki/Representativeness_heuristic?ref=commoncog.com), again demonstrated by Kahneman and Tversky in 1973, argues that humans pick wrong representations on which to build our judgments. In the hands of novices, this heuristic leads them to make wildly wrong probability estimations. In fields with expertise, however, experts deploy the representativeness heuristic to quickly pattern match against prototypes and to pick the right environmental cues in evaluating a complicated, evolving situation.

Finally, Kahneman and Tversky’s [simulation heuristic](https://en.wikipedia.org/wiki/Simulation_heuristic?ref=commoncog.com) is used by experts to construct powerful explanatory models and to accurately simulate the consequences of their actions. Both require powerful stores of experience; novices asked to simulate the effects of actions in a dynamic environment often can’t do so. In Kahneman and Tversky’s work, the simulation heuristic is used to explain that people are biased towards information that is easily imagined or mentally simulated.

Is RPD an artefact of System 1 thinking, or an artefact of System 2 thinking? The correct answer is that it is both. The initial prototype recognition is a System 1 result, but the subsequent linear evaluation and simulation process is deliberative: System 2. That it uses Kahneman and Tversky’s documented heuristics is but an implementation detail. This doesn’t make RPD flawed, however. In fact, Klein’s approach is refreshingly different: he thinks that RPD captures how humans naturally think, and instead of using unnatural, explicit methods from decision science, he believes we should look for ways to build on what we already have.

Klein also argues that the cognitive heuristics and biases program is an incomplete picture: that is, it discounts the importance of expertise when making judgments and decisions. In fields where expertise genuinely exists, we should not fight against natural impulses like the one to generate stories. Instead, we should understand the strengths and weaknesses of each tool and use them accordingly.

In Kahneman’s defence, I should note that both he and Tversky have never said that heuristics worked badly. Instead, they have always argued that heuristics exist because they *work well for a wide variety of circumstances*. It is merely in our interpretations of their results that we seem to have forgotten this. In 2009, Kahneman [wrote a paper](https://www.youtube.com/watch?v=sW5sMgGo7dw&ref=commoncog.com) with Klein on this topic, titled [Conditions for Intuitive Expertise: A Failure to Disagree](https://www.fs.fed.us/rmrs/sites/default/files/Kahneman2009_ConditionsforIntuitiveExpertise_AFailureToDisagree.pdf?ref=commoncog.com), in which the two describe the approaches of both of their fields, and elucidated the conditions under which which expert judgment may be trusted (or distrusted!).

In this, they draw from [Robin Hogarth’s](https://www.press.uchicago.edu/ucp/books/book/chicago/E/bo3624460.html?ref=commoncog.com), [Anders Ericsson’s](https://www.cambridge.org/core/books/cambridge-handbook-of-expertise-and-expert-performance/95B7A17EA9EE0E02A804B2930EB50C28?ref=commoncog.com) and James Shanteau’s research. Shanteau presents the following features in [his paper](http://www.sciencedirect.com/science/article/pii/074959789290064E?ref=commoncog.com) on the conditions for expertise. (I’ve copied this table representation from [Stuart Armstrong of LessWrong](https://www.lesswrong.com/posts/yrNW4ApXrpn2KMhxr/competence-in-experts-summary?ref=commoncog.com)):

![](https://commoncog.com/content/images/2019/01/Screenshot-2019-01-05-at-1.48.31-PM.png)

I should note, however, that Shanteau’s list of features represent properties that lie on a spectrum. Klein and co have examined various fields in which expertise exists alongside subjective analysis, dynamic stimuli, and even where ‘decisions about behaviour’ are expected. For this reason, Kahneman and Klein have reduced the requirements for expertise in their paper to only two points: first, that the field must provide adequately valid and regular cues to the nature of the situation, and second that there must be ample opportunities for learning those cues.

For example, firefighters operate in an environment of regularity — there are consistent early indications that a building is about to collapse. There are also consistent early indications that a premature infant is fighting an infection. Conversely, Kahneman, Klein and Shanteau [all argue](https://www.fs.fed.us/rmrs/sites/default/files/Kahneman2009_ConditionsforIntuitiveExpertise_AFailureToDisagree.pdf?ref=commoncog.com) that stock picking is a highly irregular activity:

> ... it is unlikely that there is publicly available information that could be used to predict how well a particular stock will do—if such valid information existed, the price of the stock would already reflect it. Thus, we have more reason to trust the intuition of an experienced fireground commander about the stability of a building, or the intuitions of a nurse about an infant, than to trust the intuitions of a trader about a stock.

Earlier in this series I said that perhaps Munger’s approach to Elementary Worldly Wisdom works because the feedback loop from stock picking is more apparent than that of other fields. I see now that I am mistaken: rather, Munger’s recommendation to rely on rational decision analysis is borne out of the *difficulties of building expertise in stock picking*. In other words, expert intuition has little place in stock investing, and Munger is rightly concerned with proper decision-making tools as prescribed by Baron et all.

What other fields are implicated? In his 1992 paper, Shanteau noted the following list of professions (combined with a few more examples from Kahneman and Klein):

![](https://commoncog.com/content/images/2019/01/Screenshot-2019-01-05-at-3.41.18-PM.png)

What should we conclude from these results? The immediately obvious view is that if you are in a field where the environment is irregular and where there are few opportunities for feedback, you should focus your skills on instrumentally rational mental models, as we have discussed in Part 3. You should protect yourself against the three errors of the search-inference framework, you should [score yourself](https://fs.blog/smart-decisions/?ref=commoncog.com) based on your decision-making, you should [keep a decision journal](https://fs.blog/2014/02/decision-journal/?ref=commoncog.com) and you should build your latticework of mental models.

But then consider the reverse of this idea: if you are *not* in a field like stock-picking, perhaps you should *not do as Munger says*.

## How bad is deliberative decision making, really?

It’s worth pausing now to ask: why is it so important to build expertise in fields where expertise exist? Why can’t we just apply rational decision analysis to everything?

The answer is that deliberative decision-making, of the sort recommended by Farnam Street and Munger — *is incredibly slow.* Remember what Baron said in Part 3: “search is negative utility!” The search process that you have to perform using rational choice analysis is a very meticulous, explicit one. It guarantees a certain amount of negative utility every time that it is used.

In certain fields like firefighting, flying, or emergency medical care, rational choice analysis is not viable because of the time-sensitive nature of the field. But there are implications for us in other fields as well. In competitive fields where expertise exists, the person who stops to consider every decision *will always be beaten by the expert who has developed intuitive mental models of the problem domain*. The expert can rely on intuition to guide her search; the novice cannot.

Speed and accuracy of decision-making is not just *one* competitive advantage — it leads to many interrelated advantages. Klein and co document eight ways that experts outperform novices due to the depth of their tacit knowledge: they are able to see patterns that novices do not, they are able to notice anomalies that indicate events far earlier than less experienced practitioners, they have good situational awareness throughout problem solving processes, they understand deeply the interactions of things in their problem domain, they see opportunities and improvisations that others do not, they are able to simulate events in the near future or infer events that have happened in the recent past, they perceive differences that are too small for novices to detect, and they recognise their own limitations. *(Sources of Power, Chapter 10)*

In fact, in many cases, Klein notes that practitioners who engage in rational choice analysis in fields where expertise exists are *novices, not experts.* They have no choice but to do choice comparison because they do not know how to read the situation quickly and intuitively. A large part of his work has been to develop training programs to move novices away from deliberate decision-making — for he notes that if novice practitioners are never given the opportunity to train their intuitions, their rate of learning will be forever stunted.

Herbert Simon, who won a Nobel prize for his work on organisational decision making, also offers us another reason that rational choice analysis alone cannot provide optimal outcomes. Simon [argues](http://psycnet.apa.org/record/1957-01985-001?ref=commoncog.com) that decision makers are subjected to ‘bounded rationality’: that is, it is impossible to make any important decision by gathering and analysing all the facts. There are too many facts and too many combinations of facts. The more complex the decision, the larger the combinatorial explosion becomes.

Klein also [points](http://science.sciencemag.org/content/275/5304/1293?ref=commoncog.com) [to](https://www.apa.org/pubs/books/4318761.aspx?ref=commoncog.com) [research](http://psycnet.apa.org/record/2003-06843-010?ref=commoncog.com) that is starting to pile up showing that decision making without expertise leads to bad outcomes; in *Power of Intuition* he says: “The evidence is growing that those who do not or cannot trust their intuitions are less effective decision makers, and that as long as they reject their intuitions, they are destined to remain so. Attempts to promote analysis over intuition will be futile and counterproductive.” Finally Klein notes with some satisfaction that the field of decision science is slowly coming round to a balance between expertise, bias avoidance and rational choice analysis.

The question now is *what does this mean for us.* Here the research presentation ends and the editorialising begins.

## A Personal Take

I believe that most of us work in domains that have what Kahneman and Klein call “fractionated expertise”. (In the 2009 paper they state that they believe *most* domains are fractionated). Fractionated expertise means that a practitioner may possess expertise for some portion of skills in the field, but not for others. Kahneman and Klein write:

> ... auditors who have expertise in “hard” data such as accounts receivable may do much less well with “soft” data such as indications of fraud (…) In most domains, professionals will occasionally have to deal with situations and tasks that they have not had an opportunity to master. Physicians, as is well known, encounter from time to time diagnostic problems that are entirely new to them—they have expertise in some diagnoses but not in others.

This explains why doctors, nurses and auditors appear on both sides of Shanteau’s list:

![](https://commoncog.com/content/images/2019/01/Screenshot-2019-01-05-at-3.41.18-PM-1.png)

Kahneman and Klein note that good physicians often have the ‘intuitive ability to realise the characteristics of the case do not fit any familiar category and call for a deliberate search for the true diagnosis’. That is, they know when to switch from RPD to rational choice analysis.

The most powerful lesson from their joint paper is that in fields with fractionated expertise, it is *incredibly important to recognise where one has expertise and where one does not.* The confidence produced by expert intuition will feel equally strong in cases where you have real expertise, and in cases where you do not. But that doesn’t negate the need for developing expertise in the first place.

What does this have to do with my practice? Well, I ran the software engineering for a small company in Singapore and Vietnam from 2014 to 2017. I believe that management and business is a domain with fractionated experience, because that squares up with my experience.

At my previous company, I was called to provide input on large client projects, or to discuss business strategy with my boss at certain points during the year. In these situations, we engaged in rational choice analysis, and took the time to adjust our inferences. But I noticed that in certain types of decision making, we seemed to get better with experience … in other types of decision making we did not.

In the day-to-day management of my engineering office, however, I became very competent at running my department in service of producing software for the business. I built mental models of management in the pursuit of this goal — by reading books, finding mentors, but most importantly by adapting recommendations into a series of tests that I applied to my slice of the company. Each time I failed, I gained some new understanding of the principles behind the technique, and why they might or might not apply to my unique situation. I also constructed good mental models of the inner workings on the company — again, mostly through trial and error. Eventually, I could predict potential problems between sales, engineering and customer support — often before people in those departments saw them. I cultivated information sources inside our growing company, modified small portions of the sales process when I saw it was starting to cause problems for engineering and customer support, and refined a hiring process I could codify and pass down. I became a valuable part of my boss’s operations; at the end of three years, we were making S$4.5 million a year in revenue, up from 300k two years before.

Note that in none of these situations did an understanding of Darwin’s theory of evolution or a grasp of thermodynamics helped me. What helped the most was building expertise; and expertise in management is *not* built by reading widely. It is built by practice: by breaking down techniques, applying them, and reflecting on the failures. It is built by finding mentors, asking them for advice tailored to your situation, watching how they think as they gave their recommendations, and then simulating their thinking while implementing that advice.

The mental models that were most important for my success was the mental model of my company, of the processes that produced results, of the information flow that might lead to misunderstandings, and — much later — of the overall lay of the land of our market in Singapore, and of the nature of our competitors. Once I possessed these mental models, I found I could navigate work-related problems effortlessly.

A small example suffices. Shortly after I hired our first HR exec, she asked what potential problems there might be within the engineering department. I paused for a moment, and then explained the team dynamics that were developing, the problems of miscommunication that would result from my hiring of a new team manager, the lack of engineering processes on one of the teams compared to the other and the consequences of that imbalance. I also explained what interventions I was performing to prevent some of these problems from worsening and my confidence (or lack of) in the likelihood of success of each of these interventions. The HR exec was impressed, but I was left with the conundrum of teaching her my internal mental models. I did not succeed.

This was when I began to suspect that something was wrong with Munger’s prescription.

Why was expertise and the building of tacit, personal mental models more important for me than building a latticework of general descriptive models? One possible answer, perhaps, is that at the bottom levels of most domains, deliberate decision-making matters less and expertise matters more. I was only rarely called to make strategic decisions; this changed as the company grew. I expect that as you rise, your ability to do good deliberative decision-making becomes more important.

That said, compare my story with that of Klein’s ‘friend’ (from the *Power of Intuition,* Chapter 1):

> “Consider a former executive I know, a man who headed a very large organization. He was known for doing meticulous work. He rose through the hierarchy and eventually was appointed to run a division. And that’s when everyone realised that his meticulous attention to detail had a down side—he wasn’t willing to use his intuition to make decisions. He had to gather more and more data. He had to weigh all the pros and cons of each option. He had to delay and agonise, and drive his organization crazy. He never missed any essential deadlines, but his daily indecision sapped morale and made for lost opportunities. After a decade of mismanagement, he retired, to the relief of his colleagues and staff members. And then came the news—he was diagnosed with prostate cancer. Fortunately, medical science has developed a range of treatments for prostate cancer. Unfortunately, patients have to make decisions about which treatment to accept. As I write this, more than ten months have elapsed since the diagnosis, and the former executive still hasn’t settled on a treatment. He is busy acquiring more data and assessing his options.”

Do you know someone like this? Conversely, do you know someone who is completely driven by intuition and gut feel — who makes decisions without analysis? How do you think this has turned out for them? What do you think is the right mix?

There is one final aspect to this. I mentioned in Part 2 that trial and error dominates as a problem solving strategy in ill-defined fields. I also opened Part 3 with a discussion of the failure modes of trial and error.

Here’s where we tie the two threads together. I think trial and error is how most of us will build expertise in our careers — a direct result of the lack of theory and insight for many practicable areas of interest. Even practitioners in areas with good theory — such as medicine, engineering, or computer programming — must spend a large amount of their time developing expertise through experience and practice.

Expertise happens to be the collection of approaches that Marlie Chunger keeps in his toolbox:

![](https://commoncog.com/content/images/2019/01/Paper.Commonplace.37.png)

In this pursuit of building expertise we can choose to be systematic, or we can choose to be thoughtless. I’ve noticed that some really smart friends of mine don’t seem to learn good lessons from events they’ve experienced. Others continually learn new lessons by reviewing old experiences and decisions. Anecdotally, the latter seem to have less problems than the former.

I’ve also observed that I am not very good at selecting trials or learning lessons where emotions are involved — such as when I deal with people I find ‘difficult’. I’ve noticed that I regularly need to ask people for a second opinion when evaluating an experience, to adjust for blind spots. And I’ve noticed that I need to consciously vary my approach the next time a similar problem emerges.

I think there’s a lot to be said for applying the methods of instrumental rationality to the process of trial and error itself. At each stage of the trial and error process, do proper search-inference thinking to evaluate if

- There is a risk that you will blow up
- There is a reasonably good amount of information you can learn from the trial you are about to do if it fails
- There is a more generalisable lesson to be learnt
- There are salient features to recognise a similar problem in the future

How do you know that you are getting better? For this, I think we should look to what actual practitioners do. In *Principles*, Ray Dalio suggests that we may use the *class* of problems we experience in our lives to gauge our progress. That is, while you might not be able to evaluate the results of a trial and error cycle immediately, you may, over time, observe to see if the problems that belong to that class seem to become easier to deal with. If you find that problems in that class no longer pose much of a challenge for you, then you may conclude that your collection of ‘principles’ or approaches are working and that you have improved.

I’ve used this to great success in the years since I started applying Dalio’s ideas (I read an earlier, terribly formatted PDF version of his book in 2011). Dalio deals with most of the problems that one might encounter during a trial and error cycle. In fact, I’d wager that his methods provide an effective prescriptive model for instrumental decision making. He calls his approach to trial and error the ‘ [5 Step Process](https://commoncog.com/dalios-5-step-process-to-getting-what-you-want/) ’ (Youtube video [here](https://www.youtube.com/watch?v=aYqoicJOUuk&ref=commoncog.com)).

As a final note, I think Klein’s RPD model and Taleb’s observation of the nature of trial and error gives us a plausible answer to the mystery of the ‘traditional Chinese businessmen’. My current hypothesis is this: if you are sufficiently rational in your trial and error cycles, and if you build expertise in a fractionated domain such as business, two things should eventually happen. First, the fractionated nature of business means that you would eventually build intuitions around the regularities in your domain — like hiring, retaining employees, evaluating deals, and negotiating with suppliers. Second, the nature of trial and error should eventually result in your success over a long enough period.

It doesn’t matter that each individual decision, examined in isolation, does not demonstrate the principles of good decision making. It is quite clear that traditional Chinese businessmen use intuition and ‘gut feel’ to guide their decisions — in some business domains this would work well, in others not so. But if they never break the cardinal rules of trial and error (that is, risk being blown up) — the convexity of their environment should eventually work out. They should eventually experience success.

## Putting this together

So let’s *finally* put everything together. How do you put mental models into practice?

First, let’s talk about beliefs.

Are you a policy maker? Or are you interested in holding better, more accurate beliefs — especially beliefs that can’t be checked through personal experience? For instance, do you want to have properly considered opinions on topics such as: “climate change is real”, or “immigration is beneficial to the economy” or “capital punishment costs the state more than it benefits society”?

If you do, what you want to do is to improve your epistemic rationality. Read the thinking mental models related to evaluating beliefs and overcoming cognitive biases, and look up LessWrong for prescriptive models on how to overcome them. Then read widely, think deeply, and (as Shane puts it) [do the work necessary to have an opinion](https://fs.blog/2013/04/the-work-required-to-have-an-opinion/?ref=commoncog.com) on the issue.

Second, if you are interested in the practice of mental models for better decision making, then you must first ask: are you in a field that is conducive to expertise formation? Or are you in one that is so irregular that it may be considered wicked (as per [Hogarth, 2001](https://www.press.uchicago.edu/ucp/books/book/chicago/E/bo3624460.html?ref=commoncog.com))? Or are you — like most of us — in a field with fractionated expertise?

If you belong to a field that is *not* conducive to expertise formation, then you should probably do as Farnam Street [prescribes](https://fs.blog/smart-decisions/?ref=commoncog.com): keep a [decision journal](https://fs.blog/2014/02/decision-journal/?ref=commoncog.com), use a [checklist](https://fs.blog/2012/09/how-to-improve-the-quality-of-our-decision-making/?ref=commoncog.com), build a [latticework of mental models](https://fs.blog/mental-models/?ref=commoncog.com), and — as we know from [Part 3](https://lc.fs.blog/t/a-framework-for-practicing-mental-models-part-3/8256/14?ref=commoncog.com) — avoid the three errors of good search and inference. You can’t evaluate a decision based on outcomes (because luck), so you should evaluate your decisions based on the thinking errors you detect in retrospect.

These methods also matter if you are going to make decisions that you have little expertise in. For instance, most of us won’t have expertise in choosing a life partner, picking a career, or choosing a good school to send our children to. In these kinds of decisions, proper deliberative decision making will probably help, as will a wider range of mental models.

That said, I think that *everyone* who is interested in decision making should pay attention to the nature of expert intuition. The adoption of intuitive decision-making as part of US military doctrine (in 2003) and the growth of NPD-based training programs for soldiers, nurses and firefighters is telling. The form of decision making that most of us do is recognition-primed decision making, not rational choice selection. We should pay close attention to what we *actually* use and figure out ways to improve it, instead of improving what we are told to use (but rarely do).

The core of my criticism of mental model education in [The Mental Model Fallacy](https://commoncog.com/the-mental-model-fallacy/) was that the most valuable mental models one can learn are tacit in nature. I asserted that these models are domain-specific, constructed from one’s reality, and built from practice, because practice is how you gain expertise, and because the types of knowledge that make up expertise are *always* tacit.

I do not believe that Munger and Buffett have zero expertise (and I doubt anyone would say so either!) But I also do not believe that Munger’s prescription of ‘reading widely and building a latticework of mental models’ is the sole reason (or even the primary reason!) for his success. I think that — even in the degenerate domain of stock picking, Munger and Buffett have fractionated pools of expertise — such as in rapidly analysing financial statements, in sizing up management teams, in judging partners, and in applying or developing decision-making tools for their investing.

I could be mistaken on this; I am in no way a Berkshire scholar. Let me know.

Next week, I want to turn my attention to building expertise. Specifically, I’d like to talk about the kinds of training methods that Klein and co have built to train RPD-based decision-making skills amongst soldiers, firefighters, and nurses. Experience is a great teacher when you’re building expertise, but it’s also a bad one because there are so little opportunities for it. So what do you do for training intuition in the absence of experience?

Let’s find out.

*Go to [Part 5: Skill Extraction](https://commoncog.com/putting-mental-models-to-practice-part-5-skill-extraction/)*

Originally published , last updated .

This article is part of the [Expertise Acceleration](https://commoncog.com/expertise) topic cluster. Read [more from this topic here→](https://commoncog.com/expertise)

Previous Post

#### ← Putting Mental Models to Practice Part 3: Better Trial and Error

Next Post

[![Feature image for What Process Improvement in Education Looks Like](https://commoncog.com/content/images/size/w300/2024/09/rigorous_process_improvement_education.jpg)](https://commoncog.com/what-process-improvement-in-education/)[![Feature image for Business Ecosystem Change Takes Time](https://commoncog.com/content/images/size/w300/2024/06/business_ecosystem_change_takes_time.jpg)](https://commoncog.com/business-ecosystem-change/)[![Feature image for Follow Your Nose](https://commoncog.com/content/images/size/w300/2021/03/follow_your_nose.jpeg)](https://commoncog.com/follow-your-nose/)[![Feature image for Create Your Own Rituals](https://commoncog.com/content/images/size/w300/2021/02/create_your_own_rituals.jpeg)](https://commoncog.com/create-your-own-rituals/)

---
title: "Putting Mental Models to Practice Part 5: Skill Extraction"
source: "https://commoncog.com/putting-mental-models-to-practice-part-5-skill-extraction/"
author:
  - "[[Cedric Chin]]"
published: 2019-01-21
created: 2025-05-30
description: "Last week we covered a model for expertise called recognition-primed decision making. This week, we talk about how to use that model to build expertise of your own."
tags:
  - "clippings"
---
By

![Feature image for Putting Mental Models to Practice Part 5: Skill Extraction](https://commoncog.com/content/images/size/w600/2019/01/jonathan-bean-284904-unsplash--1-.jpg)

## Table of Contents

## Fan of great business?

Join 8,000+ sharp investors and operators like yourself, and we'll send you a collection of Commoncog's best articles right away:

*This is Part 5 of a [series of posts](https://commoncog.com/a-framework-for-putting-mental-models-to-practice/) on putting mental models to practice. In [Part 1](https://commoncog.com/a-framework-for-putting-mental-models-to-practice-part-1/) I described my problem with Munger's latticework of mental models in service of decision-making after I applied it to my life. In [Part 2](https://commoncog.com/putting-mental-models-to-practice-part-2-introduction-to-rationality/) I argued that the study of rationality is a good place to start for a framework of practice. We learnt that Farnam Street's list of mental models consists of three types of models: descriptive models, thinking models concerned with judgment (epistemic rationality), and thinking models concerned with decision making (instrumental rationality).*

*In [Part 3](https://commoncog.com/putting-mental-models-to-practice-part-3-better-trial-and-error/), we were introduced to the search-inference framework for thinking, and discussed how mental models from instrumental rationality may help prevent the three types of errors as implied by the search-inference framework. We also took a quick detour to look at optimal trial and error through the eyes of Marlie Chunger. In [Part 4](https://commoncog.com/putting-mental-models-to-practice/), we explored the idea that expertise leads to its own form of decision making. The key takeaway from that is that rational decision making falters in domains where expertise is possible. Or to put it another way: don’t read general mental models if you haven’t gained sufficient expertise yet!*

*In Part 5, we discuss building expertise in service of making better decisions. What does this mean? What does the field of naturalistic decision making have to say about getting better at it?*

Let’s begin this essay with a simple question: what, exactly, *is* a mental model?

In teaching, a mental model is merely a ‘simplified representation of the most important parts of some problem domain that is good enough to enable problem solving’. (I’ve stolen this definition from Greg Wilson’s book *Teaching Tech Together* [here](http://teachtogether.tech/en/models/?ref=commoncog.com)). This definition is useful to us because it is an *instrumental* one — that is, the definition helps clarify the role of mental models in learning. In Wilson’s case, a teacher’s job is to help the student construct usable mental models in order to write computer programs. In this series, we are interested in acquiring mental models that help us make better decisions in service of achieving our goals.

If this definition sounds familiar, it is because it is: Wilson’s definition is very similar to the [one used by Farnam Street](https://fs.blog/mental-models/?ref=commoncog.com):

> Mental models are how we understand the world. Not only do they shape what we think and how we understand but they shape the connections and opportunities that we see. Mental models are how we simplify complexity, why we consider some things more relevant than others, and how we reason.  
>   
> A mental model is simply a representation of how something works. We cannot keep all of the details of the world in our brains, so we use models to simplify the complex into understandable and organisable chunks.

The reason I’m bringing this up is to point out that mental models *aren’t* some special thing that only exists in Farnam Street’s [list](https://fs.blog/mental-models/?ref=commoncog.com). Shane himself has done a good job of defining mental models, but I’ve noticed instances where people use the phrase ‘mental model’ to mean *only* the descriptive models referred to by Charlie Munger in [Elementary Worldly Wisdom](https://old.ycombinator.com/munger.html?ref=commoncog.com). I’ve also observed people using ‘mental model’ when ‘framework’ would’ve been a better word.

It’s probably worth reminding ourselves that mental models mean something far broader in the original psych literature. In that corner of academia, mental models are considered the basic building blocks of *all* learnt knowledge. Jean Piaget’s [Theory of Cognitive Development](https://www.simplypsychology.org/piaget.html?ref=commoncog.com), for instance, explains how a child constructs a mental model of their world. Piaget asserts that through a mix of biological maturation and environmental experience, children gradually build richer models for navigating reality — for instance, learning to grasp objects, learning to crawl, learning to walk without support, and then learning to manipulate fine objects and to interact with other human beings.

By this definition, mental models aren’t something that we can *choose* to use. They’re not a ‘thinking style’. They are at the very heart of how we function as human beings. This definition matters because we’ll be spending this essay talking about acquiring tacit mental models from the domain around us, in service of making better decisions.

## Domain Specific Mental Models

Let’s talk about the sorts of mental models we build in our careers.

Joe is a software engineer. In his first week of work, he observes a flickering fluorescent tube in the corner of his office. Annoyed by the flicker and impatient for janitorial staff to come fix it, he procures a replacement tube from the supply closet and grabs an office chair. Seeing a man in a formal suit walking by, he calls out to him: “Hey, would you mind holding this chair steady while I change this light?”

Joe changes the tube, thanks the man, and goes back to work. Much later in the day he discovers the man was actually the CEO of the company, and it was he that Joe requested to help hold the chair. Joe is mildly embarrassed, but remembers the man’s face. As a result, Joe has updated his mental model of the company’s org chart.

This organisation-focused, domain-specific knowledge is *also* a mental model. Over time, Joe will fill in more faces and attach them to positions in the company’s org chart. He will begin to learn about the political factions that exists, the people that occupy each faction, and on which issues they disagree over. He will begin to construct richer mental models about the way information flows within the company, the leverage points that he may use to get what he wants, and the larger concerns of his manager, his manager’s manager, and his department in relation to the rest of the corporation.

These sorts of mental models are ones that we intuitively build as we progress in our careers. They're also incredibly important to our effectiveness. For instance, your success as a manager or corporate decision maker will often require you to read shifting organisational winds — an ability you must develop. Unfortunately, the mental models that allow you to do so are not something that you can easily find in a book; nor do they tend to be general. You’ll have better luck watching more experienced hands in your company.

This doesn’t seem like a very controversial statement to make, though. *Of course* specific mental models will beat out general ones when it comes to performance in a specific domain. The question that interests me is the meta one implied here: is it possible to get better at acquiring these domain-specific, expertise-oriented mental models — even if the types of mental models that we’re talking about are the ones that make you more effective at company politics?

Klein and others suggests that it is. His key observation is that because expertise depends on experience, building up a store of meaningful experiences is key to shoring up your ability to use RPD. That said, while real-world experience is the best form of experience, it is not an ideal teacher. Many of us do not get the opportunity to accumulate enough real experiences in a particular field in service of developing expertise; to make things worse, many of us do not have the luxury of waiting until we’re doing something for real to learn from our mistakes. You don’t really want to make a bad move in your organisation, for instance, for all the damage that might do to your reputation. So how do you get savvier?

Klein argues that we should adhere to two common-sense principles: first, we must find substitutes for real-world experience for the specific subskills where we can’t practice in the real world. Second, we must get the most out of every experience that we *are* able to get.

His strategy for developing expertise-driven decision making, then, is four-fold:

1. First, identify discrete **decision points** in one’s field of work. Each of these decision points represent discrete areas of improvement you will now train deliberately for.
2. Second, whenever possible, look for ways to do trial and error in the course of doing. For instance, run smaller, cheaper experiments instead of launching the full-scale project you’re thinking of. Look for quick actions that you may use to tests aspects of your domain-specific mental models. This is, of course, not always possible. Which leads us to —
3. Run simulations where you cannot learn from doing. Klein and co have developed a technique for running simulations called ‘decision making exercises’, or DMXs. The DMX style of decision training was originally developed for Marine Corps rifle squad leaders and officers in 1996. It is still in use for squad leader training; the version I describe here has been adapted by Klein for corporate decision makers.
4. Fourth, because opportunities for experiences are relatively rare, you should maximise the amount of learning you can get out of each. Klein has specific recommendations for decision-making criticism, though it won’t surprise you to hear that these are very similar to existing recommendations for after-action reviews. We will mention this only in passing.

The purpose of this essay is to give you an idea of the overall approach the field of Naturalistic Decision Making has taken to decision training. The training methods here were originally designed for the US Military, but have been adapted for emergency room nurses, fireground commanders, managers and executives. I will leave out a lot of nuance, sacrificing detail for broad strokes. My recommendation here is to go out and purchase a copy of *[The Power of Intuition](https://www.amazon.com/dp/B001334J00/ref=dp-kindle-redirect?_encoding=UTF8&btkr=1&ref=commoncog.com)* — nearly every chapter is actionable, and Klein provides far greater detail for each of the techniques I’m going to cover.

To give you a taste of what a decision-making simulation looks like, and to perhaps convince you that there’s something to this, let’s consider the following decision game that Klein has developed for his corporate practice, outlined in *The Power of Intuition*. I’ve taken the liberty to adapt this exercise to Joe’s story.

## Joe’s Challenge

Three years after we first met Joe, we find him settled into the company as a senior project manager. The company has recently downsized due to an economic downturn. Because overall workload has gotten light, the CEO has decided to turn this into an advantage by proceeding with some internal research and development. Joe submits an idea for a project to develop a new type of business database that may be accessible through mobile phones; this proposal is given high ratings and Joe is told to start work. He is given eight months because the CEO believes the lull in work to be temporary. Joe is excited by this opportunity. He believes that it would give him better visibility within the company; motivated by this belief, he begins to gather his team.

Joe is given the full-time services of a staff of 12 people including himself. He has six people from his department, plus two human factors specialists on loan, plus another three communications specialists, also on loan. In addition, Joe arranges for a key piece of software to be developed by the company’s software team.

What follows is a series of events that unfolds over the course of the next couple of months. Your task is to read each event in sequential order (don't skip ahead!). As you do so, mark down on a piece of paper or in a text editor which of the events appear significant to the successful completion of Joe’s project. Put yourself in Joe’s shoes: what would you do in response to the events that appear significant to you? How can you ensure the best possible chances of success for the project? Jot those thoughts down.

The events:

1. A competitor has announced the near completion of a product that in some ways is similar to Joe’s.
2. An additional twenty layoffs are announced for Joe’s company, but none of these is from his team or his group.
3. The president of the company announces a hiring freeze that is absolute and (hopefully) temporary.
4. An experienced marketing executive tells Joe that she is dubious about the competitor’s announcement. They have made similar claims about vaporware in the past. Her inside sources hint that they’re just trying to discourage others from moving into this area.
5. Good news! The company wins a large contract with an insurance company to increase the usability of links from the Web to customer service.
6. Joe’s project is now starting on its third month and is on schedule.
7. Good news! The company announces that a new contract has been signed for a project to develop software for a major bank.
8. A different competitor has just hired some of the former employees of Joe’s company who had been laid off a few months earlier.
9. The company’s financials show a lot of red ink — it lost a fair amount of money in the preceding quarter.
10. Rumours are circulating in Joe’s office that the parent company is unhappy with the revenue picture. Some of Joe’s office mates are speculating that the CEO may be replaced soon.
11. Good news! Another large new contract is signed, with work set to begin in the next month. Many people — Joe included — begin hoping that this signals a turnaround for the company.
12. Joe hears from a friend in HR that the management information systems department requested a waiver of the hiring freeze, but was denied.
13. Two people from Joe’s department announce their resignations. They each describe different reasons, but others suspect that the revenue uncertainty is taking its toll. Neither of the people are from Joe’s project team.
14. Joe receives a company-wide email announcing that there will be a move in the next three months to a new office complex, as a way to cut costs.
15. Joe seeks and receives reassurances from upper management about the long-term importance of his project.
16. Joe’s project is starting its fourth month and is on schedule.
17. Joe is informed that the software group may be unable to deliver the programs that the project needs. The task is more difficult than expected.
18. The communications systems specialists complain to you that they are getting pressure to work on the new contracts that have just come through, because those will generate revenue.
19. Joe hears from a secretary that the problem with the software program was that the developers were being pulled into the bank software effort, and that’s why they are lagging in delivery.
20. Joe overhear’s a lunchroom discussion about how company leadership is actively considering rescinding the hiring freeze.
21. Joe has a productive meeting with the software group. He’s able to reduce the number of required features so that they may stay on schedule.
22. The project is now in its fifth month, and is falling behind schedule, though the reduced features that Joe has negotiated makes it a little difficult to estimate.
23. The human factors specialists on Joe’s team have missed the last two meetings. They say that they’re being on-boarded for the insurance company usability project.
24. A senior VP announces she’s taking early retirement.
25. The financial department begins releasing daily graphics showing revenue curves are heading up again.
26. Joe’s manager tells him that over half of the project team is being reassigned to projects for clients, to increase revenue. Joe is losing both human factors specialists, all three communication specialists, and two database specialists. Joe is asked to wrap up the project by using the remaining personnel to document progress, so that the effort can be shelved until the financial picture improves.

Now: look back over your notes. Did you see this coming? At which point did you realise that the project was doomed?

This situation is unfortunately all-too-common in the real world. Each move made by upper-management is frustrating and damaging to company morale, but ultimately understandable. It is event 26 that crushes Joe’s project, forcing him to wrap things up with little to show for his efforts.

When Klein started administering this decision game, he found that many people caught on to the potential problems by item 23 or item 19. One person wrote next to item 17 “this is the first major hitch”, and later “things are now unravelling” next to 18, and “major problems” next to 19.

The most experienced executives that played this game, however, had uneasy feelings from the very beginning (around items 5 and 7). These executives saw the contradiction between starting an internal project to use surplus labour while downsizing to reduce the labour supply. They picked up on the implications of the hiring freeze in item 3, and predicted that people were going to be pulled out from the project when new contracts were announced (items 5, 7, and 11). When two of Joe’s colleagues quit (item 14), they surmised that this would further intensify the labour shortage.

In this situation, identifying potential problems earlier gives you more room to manoeuvre. If Joe had the experience of the more capable executives, he might have recognised potential problems months in advance, and taken commensurate actions in response. For now, let’s pause to ask: could you see yourself in Joe’s shoes? What have you learnt from this experience?

This DMX works by simulating experience without actually having to have the experience. I enjoyed this format immensely because I could put myself in Joe's position, and I could experience the decision-making process as it occured. I could even imagine this scenario happening in my company. What lessons are contained in this DMX? Well, apart from an appreciation of the relationship between internal resource availability, revenue, and the importance of office gossip, Klein’s game teaches you the types of ‘seeing’ needed to do well in a similar situation. This particular scenario has now become part of your store of experience.

Here we arrive at the actionable question: how do we create, apply, and deploy this technique?

## Identifying and understanding decision points

The first observation is that we can rather easily create DMXs from our own experiences — that is, for training purposes. This has the added benefit of having DMXs that are adapted to your unique environment. In [Part 4](https://commoncog.com/putting-mental-models-to-practice/) I mentioned that I failed to teach my HR exec the mental models that I used to navigate my company’s growing demands; I see now that I could have done this by devising a series of DMXs — drawn from company history — to demonstrate the relationships between each department.

Klein points out that DMXs built from a real world scenario often converges on whatever action it was that was taken in real life. This is often considered the ‘right answer’. This isn’t ideal; in most situations, you want an open-ended game with no correct answer to allow for an energetic exchange of opinions. The actual decision is less important than the *simulated experience* of thinking through the situation.

One technique that I’ve found quite useful is NDM’s approach to identifying decision points. Klein says to identify a bunch of key, challenging decisions that you have to do as part of your work. For example, a manager at my company might identify the following decision points:

- Estimating timelines for a given project.
- Selecting one supplier over another.
- Hiring or promoting people.
- Assessing whether a project is derailing and if so, if it’s prudent to interfere.

For each decision, fill in the following ‘Decision Requirements’ table:

![](https://commoncog.com/content/images/2019/01/Screenshot-2019-01-18-at-11.32.44-PM.png)

The table requires you to answer three questions, though Klein encourages practitioners to adapt the questions to their situations. To illustrate, the aforementioned manager might write the following for ‘estimating project timelines’:

- **Decision Point:** “How do I estimate project timelines accurately? I seem to get blindsided by unexpected problems, even with buffer time in the project plan.”
- **What makes this decision difficult?** There are no simple rules for estimating time/effort requirements. There are also often competing goals, of which timeline is but one. Also, I cannot predict the nature of unpredictable problems.
- **What kinds of errors are often made?** I tend to be overly optimistic. My buffer times are never enough when unexpected problems occur. I am reluctant to reduce project scope, because stakeholders.
- **How would an expert make this decision differently from a novice? (Identify cues and strategies)** I notice Kevin seems to have a good track record as a project manager. He appears to know just how much buffer time to include in an estimation. He’s also delivered on time for most of his projects last year. He appears to have better luck when negotiating for reduced project scopes. I’m not sure how he does some of these things.

There are three things I’d like to highlight here. First, any difficult decision that you encounter in day-to-day work can be turned into a DMX. It may be useful to run these DMXs with colleagues or peers — keep the decision game simple; use paper and pencil; make sure it lasts no longer than 30 minutes … perhaps over lunch. Again, the primary benefit is to experience these simulated situations together.

Second, thinking about the kinds of errors that are often made in difficult decisions will usually yield a few actionable ideas. For example, when asked for potential approaches to the above decision problem, the manager suggested that she could check her optimism by keeping a ‘tickle list’ of common project issues that we’ve experienced together. She also said that she would “talk to Kevin”.

It’s this last option that I find *particularly* interesting. The third question in Klein’s Decision Requirements Table exists to prod you into observing the expertise of those around you. Klein’s supposition is that the mental models you require for good expertise-driven decision making is likely to already exist in the heads of the people around you. In order to acquire those mental models, you’ll need something that is marginally more sophisticated than “ask them how.”

## Skill extraction (or, ‘The Critical Decision Method’)

What happens when you ask an expert how he or she accomplishes a task? It’s likely that you already know the answer to this: the practitioner is probably going to say something along the lines of “it felt right”, or “I just knew what to do!” That isn’t useful to you.

Thankfully, Klein and his collaborators have developed a technique for extracting tacit mental models of expertise. Their overall approach is known as Cognitive Task Analysis, and the specific method that is of interest to us as practitioners is known as the ‘ [critical decision method](https://healthit.ahrq.gov/health-it-tools-and-resources/evaluation-resources/workflow-assessment-health-it-toolkit/all-workflow-tools/critical-decision-method?ref=commoncog.com) ’, or CDM. This method requires some skill to use, but the simple form as relayed by Klein in *Sources of Power* is practical enough for us to attempt to apply.

The setup for CDM is to use the human instinct for storytelling to elicit mental models from the expert practitioner. Don’t ask how they did it — ask what happened, and then use cognitive probes to tease out their models.

The Recognition-Primed Decision Making model that we covered in [Part 4](https://commoncog.com/putting-mental-models-to-practice/) gives us a map for what we’re looking for. An expert recognises a situation via pattern-matching and generates four by-products: expectancies, cues, goals, and an action-script, and then simulates the actions to see if it is applicable. For the sake of simplicity, we may reduce this to:

- Cues let us recognise patterns.
- Patterns activate action scripts.
- Action scripts are assessed through mental simulation.
- Mental simulation is driven by mental models.

In this simplified view, the elements that are most accessible to us are the cues and the action scripts. If you don’t have time to probe for all four by-products, then focus primarily on the cues and the actionable strategies that the expert uses. From my experience in applying this, these two bits are often enough to provide hints as to the shape of the tacit mental models that exists in our expert’s head. CDM goes as follows:

1. The first step is to find a good story to probe. The story might be non-routine, where a novice might have faltered. Or it could be about a routine event that involves making perceptual judgments and decisions that would prove challenging to a less experienced practitioner. Klein notes that a good story should be about an event with multiple decision-points — that is, parts in the narrative where a practitioner might have selected one of a few different types of actions. In this first step, just ask for a brief summary of the story to see if it’s worth digging into. Of course — if you already know what stories are available because you work in the same company, then you may skip this step entirely.
2. After such a story is located, ask for a front-to-back narrative of the event in question. As this occurs, keep track of the state of knowledge and how it changes as events occur. Note the situational awareness of each stage, and clarify the timeline of events if necessary. In Klein’s experience, this part is usually the easiest — practitioners like to tell stories about displays of skill, and other practitioners tend to want to listen in.
3. The third pass is to probe for their thought processes. At this point Klein’s team will usually ask what a person noticed (**cues**) when changing an assessment of the situation and what alternate **goals** might have existed at that point. If your practitioner chose a **course of action**, ask them what other actions were possible, whether they considered any of them, and if so, what the factors were that favoured that option. Make time to generate hypotheticals — for instance, if they had not had access to a key piece of information, what would they have done instead? If an event had not happened, what could explain such a thing? Hypotheticals are how Klein’s team pin down the shape of the mental models that power these decisions.
4. If time permits, Klein will then do a fourth and final pass. This time, at each decision point, he would ask for mistakes that a novice could make. For example: “If I were the one making the decision, if by some fluke of events I got pressed into service during this emergency, would I see this the same way you did? What mistakes could I make? Why would I make them?” This probe is pretty useful for eliciting information on cues that an expert would notice but that a novice would not.

Of course, getting a verbal representation of this knowledge isn’t enough. In order to use it, you have to put it to practice. The goal of such knowledge is to guide the construction of tacit mental models of your own.

How do you identify expert practitioners to talk to? Sometimes this is easy: in naturalistic decision making, researchers rely on a history of successful outcomes. The most common method is to ask for peer evaluations, or, as Kahneman and Klein [write](https://www.ncbi.nlm.nih.gov/pubmed/19739881?ref=commoncog.com):

> The conditions for defining expertise are the existence of a consensus and evidence that the consensus reflects aspects of successful performance that are objective even if they are not quantified explicitly. If the performance of different professionals can be compared, the best practitioners define the standard.

In practice, if you are looking for an expert in your own company, you have an advantage: you are a peer, and you likely already know which practitioners are good at what tasks. Ray Dalio, however, has a useful notion called ‘ [Believability](https://commoncog.com/believability/) ’ that he uses when reaching out to other practitioners for advice. I’ve found this quite useful, so I’m including this here:

Someone is defined to be believable if they have a record of at least three relevant successes, and have a good explanation of their approach when probed. Dalio’s application of this idea is the following advice-seeking protocol:

1. If you’re talking to a more believable person, suppress your instinct to debate and instead ask questions to understand their approach. This is far more effective in getting to the truth than wasting time debating.
2. You’re only allowed to debate someone who has roughly equal believability compared to you.
3. If you’re dealing with someone with lower believability, spend the minimum amount of time to see if they have objections that you’d not considered before. Otherwise, don’t spend that much time on them.

I’ve found Dalio’s heuristic to be rather useful over the past year. If you think it makes sense, feel free to apply it to your search for expertise.

## Decision Making Critique

The last part of Klein’s decision training is to engage in decision-making critique. On this, he recommends that you periodically review decisions that you’ve made with the following list of questions:

1. What was the timeline? Write down the key judgments and decisions that were made as the incident unfolded.
2. Circle the tough decisions in this project or episode. For each one, ask the following questions:
3. Why was this difficult?
4. How were you interpreting the situation? In hindsight, what are the cues and patterns you should have been picking up?
5. Why did you pick the course of action you adopted?
6. In hindsight, should you have considered or selected a different course of action?

Klein then suggests that you use this to identify decision points you need to practice further. This practice may happen with a DMX, or perhaps by actively arranging for specific types of work assignments.

## Conclusion

I think the broad approach I’ve taken to putting mental models to practice has to go after expertise *first*, because my field warrants it. In this pursuit I have found that the most useful mental models are often in the heads of expert practitioners.

This essay provides an outline of techniques that one may use to identify and ‘extract’ skills from the domain that you inhabit. The goal, again, is in pursuit of ‘better decision making’; the approach that I’ve described depends on it being possible to build expertise in your chosen domain.

I have said in [Part 1](https://commoncog.com/a-framework-for-putting-mental-models-to-practice-part-1/) that I believe that a latticework of mental models might matter if a) you are already an expert practitioner and you need an edge, or b) if you are in a domain where you need to perform rational analysis. I’ve covered the conditions for b) in [Part 4](https://commoncog.com/putting-mental-models-to-practice/), when discussing the nature of domains amenable to expertise. I’d like to deal with a) now.

I think it’s perfectly plausible for expert practitioners who are competing against each other to win based on the breadth of their knowledge. Certainly you need an edge from somewhere; acquiring mental models from a broad array of disciplines might augment your pattern-recognition or solution generation, as per the recognition-primed decision model. But as I am neither an expert practitioner, nor a practitioner in a field where expert intuition should be distrusted, I cannot say for certain. Check back in 10 years, when I am no longer merely competent.

I *should* probably highlight that I am not against books or reading. This very framework was constructed from synthesising books and papers, in pursuit of an organising framework that matched my experiences. I am *not* arguing against learning from ‘what other people have figured out’ nor am I arguing to ‘only learn from trial and error’. But I do believe that if we model at all, we should start from reality first and let it be the teacher, instead of modelling enthusiastically and trying to overfit reality to those models. I’ll talk about this in my next (and final!) part.

What I *do* disagree with is the central premise of Farnam Street’s approach to decision making: that is, the ‘ **best** ’ way to make good decisions is to acquire a broad base of ‘multi-disciplinary descriptive models’.

![](https://commoncog.com/content/images/2019/01/DraggedImage.d63155bfda844e0494126eb8b2bbe08a.png)

It isn’t the best way. It is certainly *one* good way, and it can be a worthwhile pursuit given one’s domain. But the approach to decision making that it inhabits is not the full picture that’s available to us. It isn’t very effective if you are a novice getting started in some fractionated field.

Acquiring mental models of expertise represent the other half of good decision making — and finding a balance between the two approaches appears to be the increasingly mainstream prescription of decision science (well, if Klein is to be believed, that is).

I hope this has been useful to you. I’d repeat my recommendation: buy *[The Power of Intuition](https://www.amazon.com/dp/B001334J00/ref=dp-kindle-redirect?_encoding=UTF8&btkr=1&ref=commoncog.com)* and practice away.

*Go to [Part 6: A Personal Epistemology of Practice](https://commoncog.com/putting-mental-models-to-practice-part-6-a-personal-epistemology-of-practice/).*

Originally published , last updated .

This article is part of the [Expertise Acceleration](https://commoncog.com/expertise) topic cluster. Read [more from this topic here→](https://commoncog.com/expertise)

Previous Post

#### ← Putting Mental Models to Practice Part 4: Expert Decision Making

Next Post

[![Feature image for What Process Improvement in Education Looks Like](https://commoncog.com/content/images/size/w300/2024/09/rigorous_process_improvement_education.jpg)](https://commoncog.com/what-process-improvement-in-education/)[![Feature image for Business Ecosystem Change Takes Time](https://commoncog.com/content/images/size/w300/2024/06/business_ecosystem_change_takes_time.jpg)](https://commoncog.com/business-ecosystem-change/)[![Feature image for Follow Your Nose](https://commoncog.com/content/images/size/w300/2021/03/follow_your_nose.jpeg)](https://commoncog.com/follow-your-nose/)[![Feature image for Create Your Own Rituals](https://commoncog.com/content/images/size/w300/2021/02/create_your_own_rituals.jpeg)](https://commoncog.com/create-your-own-rituals/)

---
title: "A Personal Epistemology of Practice"
source: "https://commoncog.com/putting-mental-models-to-practice-part-6-a-personal-epistemology-of-practice/"
author:
  - "[[Cedric Chin]]"
published: 2019-03-05
created: 2025-05-30
description: "Here we try to answer a deceptively simple question: when someone offers advice, how do you evaluate that advice before putting it into practice?"
tags:
  - "clippings"
---
## Putting Mental Models to Practice Part 6: A Personal Epistemology of Practice

By

![Feature image for Putting Mental Models to Practice Part 6: A Personal Epistemology of Practice](https://commoncog.com/content/images/size/w600/2019/03/olivier-miche-512215-unsplash.jpg)

## Table of Contents

## Fan of great business?

Join 8,000+ sharp investors and operators like yourself, and we'll send you a collection of Commoncog's best articles right away:

*This is the final part of a [series of posts](https://commoncog.com/a-framework-for-putting-mental-models-to-practice/) on putting mental models to practice. In [Part 1](https://commoncog.com/a-framework-for-putting-mental-models-to-practice-part-1/) I described my problems with Munger’s prescription to ‘build a latticework of mental models’ in service of better decision making, and then dove into the decision making literature from Parts 2 to 4.*

*We learnt that there is a difference between judgment and decision making, and that the field of decision making itself is split into classical decision theory and naturalistic methods. Both approaches fundamentally agree that decision-making is a form of ‘search’; however, classical approaches emphasise utility optimisation and rational analysis, whereas naturalistic approaches think satisficing is enough and basically just tell you to go get expertise. In [Part 5](https://commoncog.com/putting-mental-models-to-practice-part-5-skill-extraction/), we looked at some approaches for building expertise, in particular eliciting the tacit mental models from expert practitioners around you.*

*Part 6 brings this series to a close with some personal reflections on the epistemology of practice. I promised to write a treatment in the first part of this series, and so here we are. This begins with a simple question, and ends with a fairly complicated answer. And so here we go.*

When someone gives you advice, how do you evaluate that advice before putting the ideas to practice?

It must be obvious to us that *some* process exists in your head — you don’t accept all the advice you’re given, after all. If your friend Bob tells you to do something for your persistent cough, you’re more likely to listen to him if he were an accomplished doctor than if he were a car mechanic.

Choosing between a doctor and a mechanic is easy, however. In reality, we’re frequently called to evaluate claims to truth from various sources — some of them from well-meaning people, others from consultants we’re willing to pay, and still others from third-hand accounts, or from books written by practitioners wanting to leave some of their knowledge behind.

Could we come up with better rules for when we should listen to advice, and when we should not? The default for many people, after all, is to just go “I trust my gut that she’s correct.” A slightly better answer would be to say “just look at what the science says!”, or “look at the weight of evidence!” but this throws you into a thorny pit of *other* problems, problems like ‘are you willing to do the work to evaluate a whole body of academic literature?’ and ‘how likely is it that the scientists involved with this subfield were [p-hacking](https://en.wikipedia.org/wiki/Data_dredging?ref=commoncog.com) their way to tenure with fake results?’ and ‘is this area of science affected by the [replication crisis](https://en.wikipedia.org/wiki/Replication_crisis?ref=commoncog.com)?’ or ‘are contemporary research methods able to measure the effects I’m interested in applying to my life, without confounding variables?’ and ‘does science have anything to say at all with regard to this area of self-improvement?’

These are not easy questions to answer, as we’ll see in a bit. But it does seem like it is worth it to sit down and think through the question; the default of ‘I’ll go with my gut’ doesn’t appear to be very good.

## Why epistemology?

First things first, though. I think it’s reasonable to ask what an essay on epistemology is doing in a series about putting mental models to practice. We’ve spent parts 1 to 5 in the pursuit of practical prescriptive models, so why the sudden switch to *epistemology* — something so theoretical and abstract?

There are two reasons I'm including this piece in the series. First, coming up with a standard for truth keeps me intellectually honest. I wrote in Part 1 that I would provide an epistemic basis for this framework by the time we were done, so that you may nail me against the ideas that I have presented here. This is that essay, as promised.

Second, an epistemic basis is important because it separates this framework from mere argumentation. What do I mean by this? Well, just because something sounds convincing doesn’t make it true. As with most things, what makes something true is not the persuasiveness of the argument, or even the plausibility of it, but whether that thing maps to reality. In our context — the context of a framework of practice — what makes this framework true is if it *works for you*.

This seems a little trite to say, but I think it underpins a pretty important idea. The idea is that **rhetoric is powerful, so you can't simply trust the things that you read.**

I wonder if many of you have had the experience where you read some non-fiction book and came away absolutely convinced of the argument presented within, only to read some critique of the book years later and come to believe that *that* critique was absolutely true.

Or perhaps you've read some book and are absolutely convinced, but then realise, months later, that there was a *ridiculously big hole in the author’s argument, and good god why hadn't you seen that earlier?* I'm not sure if this happens to you, but it happens to me all the time, and it's frequent enough that I'm beginning to think that I'm not good enough to be appropriately critical while reading.

My point is that sufficiently smart authors with sufficiently good rhetorical skills can be pretty damned convincing, and it pays to have some explicitly thought-out epistemology as a mental defence.

You can perhaps see the sort of direction I’m taking already — I’ve spent a lot of time in this series saying variants of ‘let reality be the teacher’. The naive interpretation of this doesn’t work for all scenarios — for instance, it’s unclear that ‘let reality be the teacher’ would work when it comes to matters of economic policy, despite claims to the contrary (you can’t personally test the claims of each faction, and though all sides cite research papers and statistics to back up their positions, you should realise that divining the truth from a set of studies is a [lot more difficult than you think](https://slatestarcodex.com/2014/12/12/beware-the-man-of-one-study/?ref=commoncog.com).)

But ‘let reality be the teacher’ works pretty well when it comes to practical matters — like self-improvement, say, or when you’re trying to evaluate a framework for putting mental models to practice. It works because you have a much simpler test available to you: you try it to see if it works.

## The Epistemology of Scientific Knowledge

Before we dive into the details of putting practical advice to the test, let’s take a look at the ‘gold standard’ for knowledge in our world. For many people the gold standard is scientific knowledge: that is, the things we know about reality that’s been tested through the scientific method. Philosophy of science is the branch of academia most concerned with the nature of truth in the scientific method.

(“Oh no,” I hear you think, “He’s going to talk about *philosophy* — this isn’t going to be very useful, is it?”)

Well, I’ll give you the really short version. There are two big ideas in scientific epistemology that I think are useful to us.

The first idea is about falsification. You’ve probably heard of the [story of the black swan](https://en.wikipedia.org/wiki/Problem_of_induction?ref=commoncog.com): for the longest time, people thought that all swans were white. Then one day Willem de Vlamingh and his expedition went to Australia, found black swans on the shore of the Swan River, and suddenly people realised that this wasn’t true at all.

The philosopher David Hume thus observed: “No amount of observations of white swans can allow the inference that all swans are white, but the observation of a single black swan is sufficient to refute that conclusion.” Great guy, this Hume, who then went on to say that we could never really identify causal relationships in reality, and so what was the point of science anyway?

But what Hume was getting at with that quote was the asymmetry between confirmation and disconfirmation. No amount of confirmation may allow you to conclude that your hypothesis is correct, but a single disconfirmation can disprove your hypothesis with remarkable ease. Therefore, the scientific tradition after Hume, Kant, and Popper has made disconfirmation its core focus; the thinking now goes that you may only attempt to falsify hypotheses, never to prove them.

Of course, in practice we *do* act as if we have proven certain things. With every failed falsification, certain results become more ‘true’ over time. This is what we mean when we say that all truths in the scientific tradition are ‘conditionally true’ — that is, we regard things as true until we find some remarkable, disconfirming evidence that appears to disprove it. In the meantime, the idea is that scientists get to try their darnest to disprove things.

Let’s take a moment here to restate the preceding paragraph in terms of probabilities. Saying that something is ‘conditionally true’ is to say that we can never be 100% sure of something. Instead, we attempt to disprove our hypotheses, and with each failed disconfirmation, we take our belief in the hypothesis — perhaps we start with 0.6 — and increase it in increments. Eventually, after failing to find disconfirming evidence repeatedly, we say “all swans are either black or white” but our confidence in the statement never hits 1; it hovers, perhaps, around 0.95. If we find a group of orange swans, that belief drops to near 0.

(Hah! I’m kidding, that’s not what happens at all; what happens is that scientists who have invested their entire careers in black and white swans would take to the opinion pages of *Nature* and argue that the orange birds are *not* swans and can you believe the young whippersnappers who published this paper!? Science is neat only in theory, rarely in practice).

This incomplete sketch of scientific epistemology is sufficient to understand the second idea: that is, the notion that not all scientific studies are created equal. Even a naive understanding of falsifiability will tell us that no single study is indicative of truth; it is only the broad trend, taken over many studies, that can tell us if some given hypothesis is ‘true’.

This notion is most commonly called ‘the hierarchy of evidence’, and it is usually [presented in the following form](https://thelogicofscience.com/2016/01/12/the-hierarchy-of-evidence-is-the-studys-design-robust/?ref=commoncog.com):

![](https://commoncog.com/content/images/2019/03/Hierarchy-of-evidence-pyramid-The-pyramidal-shape-qualitatively-integrates-the-amount-of.0185dbc111de4ead97a067dc6c841731.png)

The [pyramid of evidence](https://www.researchgate.net/figure/Hierarchy-of-evidence-pyramid-The-pyramidal-shape-qualitatively-integrates-the-amount-of_fig1_311504831?ref=commoncog.com) that you see above is most commonly taught to healthcare professionals, and it shows us the various types of studies we may use when we need to make a judgment on some statement — for instance, “does smoking cause lung cancer?” or “is wearing sunscreen bad?” At the bottom of the pyramid are things like editorials and expert opinion, followed by mechanistic studies (efforts to uncover a mechanism), then case reports and case studies (basically anecdotes), then cross-sectional studies and surveys (“are there patterns in this population?”) then case-control studies (“let’s take two populations and examine patterns retrospectively”), then cohort studies (“let’s take two populations that differ in some potential cause and observe them going forward to see if bad things happen”), then randomised controlled trials (“let’s have a control group and an intervention group and see what happens after we intervene”) and then, finally, at the tippy-top, we have systematic reviews and meta-analyses, which studies the results of many studies.

So: ‘ [is sunscreen harmful?](https://www.outsideonline.com/2380751/sunscreen-sun-exposure-skin-cancer-science?ref=commoncog.com)’ If you’re lazy, the answer is to dive into the academic literature for a systematic review, or better yet: a meta-analysis. Meta-analyses in particular are the gold standard for truth in science; these studies are essentially a study of studies — they summarise the results from a broad selection of papers, and weight the evidence by their statistical power. It *is* possible to do a bad meta-analysis, of course, for the same reasons that null-hypothesis statistical tests [may be misused](https://www.lesswrong.com/posts/ttvnPRTxFyru9Hh2H/against-nhst?ref=commoncog.com) to prop up crap research. But by-and-large, the scientific system is the best method we have for finding the truth.

It’s unfortunate, then, that large bits of it aren’t terribly useful for personal practice.

## The Problems with Scientific Research as Applied to Practice

Let’s say that you’re trying to create a hiring program, and you decide to look into the academic literature for pointers. One of the most well replicated results in psychology is the notion that [conscientiousness](https://pdfs.semanticscholar.org/aa70/9cc7835a6d772727856e7096056a996523a6.pdf?ref=commoncog.com) and [IQ](https://www.ncbi.nlm.nih.gov/pubmed/22233090?ref=commoncog.com) are [good](https://www.sciencedirect.com/science/article/abs/pii/S0927537117303287?ref=commoncog.com) [predictors](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3498890/?ref=commoncog.com) of job performance. So, the answer to your hiring problems is to create an interview program that filters for high conscientiousness and high IQ, right?

Well … **no**.

This is a pretty terrible idea — and I should know: thanks to my lack of statistical sophistication, *I’ve tried*. The objections to this are two-fold. First, statistical predictors don’t predict well for a given individual*.* Second, science often cannot lead us to the best practices for our specific situation, because it is only concerned with effect sizes that are large enough to be detected in a sizeable population.

These two objections are specific to the scenario of hiring, but can also be made more generally when you are applying scientific research to your life. I’ve found that they share certain similarities with the challenge of evaluating expert advice … so let’s examine them in order.

The first problem with applying scientific research to practice is the nature of statistical predictors. Let's talk about IQ. We know that IQ correlates with job performance in a band between [0.45 and 0.58](https://www.researchgate.net/publication/228641079_Why_do_IQ_scores_predict_job_performance_An_alternative_sociological_explanation?ref=commoncog.com), with the effect being stronger in jobs with higher complexity. And this is *IQ* we’re talking about, one of the strongest results we can find in all of applied psychology; literally thousands of studies have been done on IQ over the past four decades, with dozens of meta-analyses to pick from.

Can we trust the correlations? Yes. Can we use them to predict individual job performance given an IQ score? No.

Why is this the case? In [Against Individual IQ Worries](https://slatestarcodex.com/2017/09/27/against-individual-iq-worries/?ref=commoncog.com), Scott Alexander explains with a comparison to income inequality:

> Consider something like income inequality: kids from rich families are at an advantage in life; kids from poor families are at a disadvantage.  
>   
> From a research point of view, it’s really important to understand this is true. A scientific establishment in denial that having wealthy parents gave you a leg up in life would be an intellectual disgrace. Knowing that wealth runs in families is vital for even a minimal understanding of society, and anybody forced to deny that for political reasons would end up so hopelessly confused that they might as well just give up on having a coherent world-view.  
>   
> From a personal point of view, coming from a poor family probably isn’t great but shouldn’t be infinitely discouraging. It doesn’t suggest that some kid should think to herself “I come from a family that only makes $30,000 per year, guess that means I’m doomed to be a failure forever, might as well not even try”. A poor kid is certainly at a disadvantage relative to a rich kid, but probably she knew that already long before any scientist came around to tell her. If she took the scientific study of intergenerational income transmission as something more official and final than her general sense that life was hard – if she obsessively recorded every raise and bonus her parents got on the grounds that it determined her own hope for the future – she would be giving the science more weight than it deserves.

And this is actually a really pragmatic thing to do.

I have a friend in AI research who reacts to IQ studies in exactly this manner; he recognises, intellectually, that IQ is a real thing with real consequences, but he rejects all such studies at a personal level. When it comes to his research work, he assumes that everyone is equally smart and that scientific insight is developed through hard work and skill. And there are all sorts of practical benefits to this: I believe this framing protects him from crippling self-doubt, and it has the added benefit of denying him trite explanations like “oh, of course Frank managed to write that paper, he’s smarter than me.”

The point here is that for many types of questions, science is often interested in what is true in the large — what is true at the population level, for instance — as opposed to what works for the individual. It is pragmatic and totally correct to just accept these studies as true of societal level realities, but toss them out during day-to-day personal development. My way of remembering this is to say “scientists are interested in truth, but practitioners are interested in what is useful.”

A more concrete interpretation is that you should expect to find all sorts of incredibly high performing individuals with lower than average IQs and low conscientiousness scores; the statistics tell us these people are rarer, but then so what? A correlation of 0.58 only explains 34% of the variance. If your hiring program is set up for IQ tests and Big Five personality traits, how many potentially good performers are you leaving on the table? And while we’re on this topic, is your hiring program supposed to look for high scorers in IQ and conscientiousness tests, or is it supposed to be looking for, I don’t know, *actual high performers*?

This leads us to our second objection. Because scientific research is interested in what is generally true, most studies don't have the sorts of prescriptions that are specifically useful to you in your field. Or, to apply this to our hiring problem, it’s very likely that thoughtful experimentation would lead you to far better tests than anything you can possibly find in the academic literature.

Here’s an example. One of the things I did when designing my company’s hiring program was to use debugging ability as a first-pass filter for candidates in our hiring pipeline. This arose out of the observation that a programmer with good debugging skills might not be a great programmer, but a programmer with bad debugging skills could never be a good one. This seems obvious when stated retrospectively, but it took us a good amount of time to figure this out, and longer still to realise that a debugging skill assessment would be incredibly useful as a filtering test for candidates at the top of our funnel.

Is our debugging test more indicative of job performance *in our specific company* than an IQ test or a conscientiousness test? Yes. Is it a proxy for IQ? Maybe! Would we have found it in the academic literature? No.

Why is this the case? You would think that science — of all the forms of knowledge we have available to us — would have the best answers. But it frequently doesn’t, for two reasons. First, as previously mentioned, scientific research often focuses on what is generally true, not on what is prescriptively useful. I say ‘often’ here because the incentives have to align for there to be significant scientific attention on things that you can use — drug research, for instance, or sports science, for another. In such cases, science gives us a wonderful window into usable truths about our world.

But if the stars *don’t* align — if there is a dearth of attention or if there are a lack of financial incentives for certain types of research questions, you should expect to see an equivalent void of usable scientific research on the issue. My field of software engineering is one such field: the majority of our software engineering ‘best practices’ are constructed from anecdotes and shared experiences from expert programmers, who collectively contribute their opinions on ‘the right way to do things’ … through books, blog posts, and conference talks. (Exercise for the alert reader: what level of the hierarchy of evidence do we software engineers live in?)

The hiring example also illustrates this problem perfectly. When it comes to hiring, all we have are statistical predictors: that is, correlations between a measurement of some trait on the one hand (IQ, conscientiousness, grit) and some proxy for job performance (salary, levelling, peer assessments) on the other. As we’ve seen previously, statistical predictors are good for research purposes but are not very useful at the individual level; what we really want is some kind of intervention study, where a process is developed and then implemented in both an intervention group and a control.

This isn’t the least of it, though. The second reason science is often not useful to the practitioner is that even when there are financial incentives to perform ‘instrumentally useful’ research, there may *still* be a void of usable recommendations, for the simple reason that science moves relatively slowly.

Here’s an example: I’ve long been interested in burnout prevention, due to a personal tendency to work long hours. Late last year I decided to [dive into the academic literature on burnout](https://commoncog.com/nuanced-take-on-preventing-burnout/). Imagine my surprise when I discovered that the literature was only two decades old — and that things were looking up for the field; [Buchanan & Considine](http://www.worldcat.org/title/stop-telling-us-to-cope-nsw-nurses-explain-why-they-are-leaving-the-profession-a-report-for-the-nsw-nurses-association/oclc/225637011?ref=commoncog.com) observed in 2002 that half of Australian nurses leave the profession prematurely, the majority of them due to burnout. In other words, the urgency of finding a solution to burnout is now *really, really high*, and medical institutions with deep pockets are beginning to push researchers forward. This is the sort of attention and financial incentives that lead to instrumentally useful scientific research — the sorts that you and I can apply directly to our lives.

So, what have they found? After two decades of study, they have developed a test for detecting burnout — the [Maslach Burnout Inventory](https://en.wikipedia.org/wiki/Maslach_Burnout_Inventory?ref=commoncog.com) — and they have two developmental models for how burnout emerges and progresses in individuals. (You can read all about it in [this ‘state of the field’ summary paper](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4911781/?ref=commoncog.com) that Maslach published in 2016). What they *haven’t* discovered is a rigorous system to prevent burnout.

(Some people believe that inverting the developmental models gives us generally useful prescriptions for our workplaces. This is because the developmental models tell us how burnout progresses, which should then give us clues as to arresting that progression. But! Remember the caution of ‘generally useful’ interventions from our earlier discussion, and keep in mind where this lies in the hierarchy of evidence).

What interests me the most, however, is the small branch of the research that focuses on burnout resistance training — that is, the idea that individuals who experience burnout and recover from it develop better resilience to burnout later. I have high hopes for this branch to develop into something useful, but the only way to know is to give it another decade or so. Such is the price for truth.

The point here is that going for thoughtful trial and error in fields where scientific knowledge exists isn't a ridiculous position to take. It's trite to say “oh, I don't see why you can't take what psychologists have figured out and apply it to your practice” — but the answer is now easier to understand: science is interested in what is generally true, and it often doesn't give good prescriptions compared to what you (or others) can derive from observation and experimentation; worse still, if you start out with a model of reality drawn from some science and are convinced that the model is more valid for your unique situation than it actually is, you're likely to hold on to that model for longer than it is useful.

Of course, this is just a really complicated way of saying that some forms of knowledge are best learnt from practitioners — if you want to learn a martial art, you go to a sensei; when you want to learn cooking, you learn from cooks, not food scientists. Don’t start out with [epistêmê when it’s technê](https://plato.stanford.edu/entries/episteme-techne/?ref=commoncog.com) that you’re interested in learning.

Of course, I don’t mean to say that scientific knowledge isn’t useful for personal practice; I think it’s pretty clear that instrumentally useful research exists, and wherever available we should defer to those results. But I am saying that understanding the nature of truth in scientific knowledge matters, and it doesn’t absolve us of *the requirement to test things in our realities*. If a doctor prescribes Adderall to you and you find that it puts you to sleep — this doesn’t mean that Adderall is useless or that amphetamines should be reclassified as sleep aids. In fact, you shouldn’t even be that surprised; statistical truth tells us that most people would feel stimulated on Adderall; lived experience reminds us that individual variation is a thing. Even scientifically proven interventions fail to work for some people some of the time.

## Evaluating Anecdata

I’d say that the major takeaway at this point is that you never know if something might work until you let reality be the teacher and try it out for yourself. But it’s worth asking, though: if individual variation is this much of an issue when it comes to applying scientific research, how much worse can it get when we’re dealing with expert opinion and anecdotal evidence?

The question is worth asking because most spheres in life require us to work from evidence that’s nowhere near the rigour of science. Consider my example of hiring, above: in the absence of solid science, one obvious move that I can take is to talk to those with experience in the tech industry, in order to pick their brains for possible techniques to use for myself. As it is for hiring, so it is for [learning to manage difficult subordinates](https://commoncog.com/radical-candor/), for [learning to start and run a company](https://blog.asmartbear.com/kung-fu.html?ref=commoncog.com), and for learning to use martial arts in real world situations. In situations where you are interested in the how-to of things, you often have only two options: first, you can go off and experiment by yourself, or second, you can ask a practitioner for their advice.

So how do you evaluate the advice you’re given? How do you know who to take seriously, and who to discount?

I’d like to propose an alternative hierarchy of evidence for practitioners. When asking for advice, judge the advice according to the following pyramid:

![](https://commoncog.com/content/images/2019/03/Paper.Commonplace.43.png)

At the very top of the pyramid is advice that you’ve tested in your life. As I’ve repeatedly argued during this series: “let reality be the teacher!” You only truly know that a piece of advice works if you have tested it; the same way that a doctor will only truly know if a drug works after a patient has started her course of treatment. Before actual action, all one can have is a confidence judgment in how likely the intervention would work.

The second level of the pyramid is advice from practitioners who are **believable** and who are **exposed to negative outcomes**. Believability is a technique that I’ve already covered in a previous part in this series: originally proposed by hedge fund manager Ray Dalio in his book *Principles*, believability is a method for evaluating expertise.

The idea goes as follows — when asking people for advice, apply a suitable weight to their recommendations:

1. The person must have had at least three successes. This reduces the probability that they are a fluke.
2. They must have a credible explanation for their approach when probed. This increases the probability that you'll get useful information out of them.

If an expert passes these two requirements, you may consider them ‘believable’. Dalio then suggests a communication protocol built on top of believability: if you’re talking to someone with higher believability, shut up and ask questions; if you’re talking to someone with equal believability, you are allowed to debate; if you’re talking to someone with lower believability, spend the least amount of time hearing them out, on the off chance they have an objection you haven’t considered before; otherwise, just discount their opinions.

The second requirement is that an expert is more believable if they are exposed to negative outcomes in their domain. This idea is one that I’ve stolen from Nicholas Nassem Taleb’s *Skin in the Game* — which essentially argues that experts who are exposed to downside risk are more prudent in their judgments and decisions compared to experts who aren’t.

So, for instance, Singaporean diplomats are nearly all ‘ [realists](https://en.wikipedia.org/wiki/Realism_\(international_relations\)?ref=commoncog.com) ’ because they can’t afford to get things wrong in their assessments of the world (the city-state is pretty much screwed if they ever piss off a more powerful neighbour … and *all* their neighbours are more powerful than they are); in America, officials in the state department can afford to hold more ideologically-motivated ideas around foreign policy. This isn’t a personal observation, mind — I have to admit that I am quite influenced by my friends in Singapore’s Ministry of Foreign Affairs; nevertheless, I have always found the Singaporean perspective of world affairs to be more incisive than that of many other countries.

I’ll leave the full argument for this idea to Taleb, but I will say that the rule seems to hold up in my experience. By total coincidence, in the opening of *Principles*, Dalio tells the story of being shopped around to see the various central banks in the aftermath of the 2008 financial crisis. Dalio’s fund had developed models that predicted the financial crisis; the economists who ran the central banks did not. An armchair critic projecting Taleb’s pugnacious personality might argue that Dalio was exposed to downside risk, whereas the average central banker wasn’t. Whether this principle holds true at a universal level is an exercise for the alert reader.

The third level, below advice from people who are both believable and exposed to downside risk, are people who are ‘merely’ believable. Expert opinion is still better than non-expert opinion, and we should seek it out whenever possible. I think this is as good a time as any to discuss one of the biggest objections people have with Dalio’s believability.

When I tell people about Dalio’s believability metric, they often bristle at the idea that one should discount the opinions of lower-believability people. “Isn’t that just [ad-hominem](https://en.wikipedia.org/wiki/Ad_hominem?ref=commoncog.com)?” they say. “An opinion or argument should be evaluated by its own merits, not on how credible the person making it is.”

This is, I think, the most counter-intuitive implication of Dalio’s believability rule. Traditionally, we are taught that good debate should not take into account the person making an argument; any argument that is of the form “Person is X, therefore his argument is wrong” is bad because it commits the ad-hominem fallacy. Instead, a ‘good’ counter-argument should attack either the logical structure of the argument or its premises.

But then consider the common-sense scenario where you’re asking friends for advice. Let’s say that you want some pointers on swimming, and you go to three friends. Which of the following friends would you pay more attention to: Tom, who is a competitive swimmer, Jasmine, who is a casual swimmer, or Ben, who does not know how to swim? It’s likely that you’ll pay special attention to Tom and Jasmine, but ignore (or heavily discount) whatever Ben says.

Credibility counts when it comes to practical matters. Just because Ben makes a convincing and rhetorically-compelling argument doesn’t change the fact that he hasn’t tested it against reality. Don’t get me wrong: I’m not saying that Ben is certainly mistaken — he could be right, for all we know. But it’s just as likely that he’s wrong, and if you’re like most practitioners, you don’t have a lot of time to test the assertions that he makes. The common-sense approach is to go with whoever seems more credible, along with the assumption that it still might not work for you; we could say here that you’re applying a probability rating to each piece of advice, where the rating is tied to the believability of the person giving said advice.

When I began writing this essay I didn’t expect to defend ad-hominem as a second-order implication of Dalio’s believability. But then I realised: this *isn’t* about argumentation — this is about figuring out what works. You don’t necessarily have to debate anyone; you can simply apply this rule in your head in place of whatever gut-level intuition you currently use. And it’s probably worth a reminder that you don’t have to make a black-or-white assessment when it comes to low-believability advice. The protocol that Dalio prescribes asks that you ‘do the bare minimum to evaluate what they have to say … on the off-chance that they have an objection you’ve not considered before’. Or, to phrase that in Bayesian terms: keep the objection in mind, but apply a low confidence rating to it.

Why does believability work? I think it works because argumentation alone cannot determine truth. This is a corollary to the observation at the beginning of this essay that ‘rhetoric is powerful’ — and I think some form of this is clear to those of us who have had to live with decisions at an organisational level. Consider: have you ever been in a situation where there were multiple, equally valid, compellingly plausible options — and it isn't clear which option is best? In many organisations, the most effective answer when one has reached this point isn't to debate endlessly; instead, it’s far better to agree to a test that can falsify one argument or the other, and then run it to see which survives reality.

The logical conclusion from “you can't evaluate advice by argument alone” is that you'll have to use a different metric to weight the validity of an aforementioned argument. The best test is a check against reality. The next best test is to look for *proxies* of checking against reality — such as a track record for acting in a given domain. This is, fundamentally, why believability works — it acts as a proxy for reality, in the form of “has this person actually checked?” And if not, it's probably okay to discount their opinion.

## Lower Levels of the Pyramid

Below believability we get to sketchier territory. The fourth level of my proposed hierarchy of evidence is advice from people who have **actually tried the advice in question***.* This isn't as good as “believable expert who has succeeded in domain”, but it's still better than “random shmuck who writes about self-help that he hasn't tried”.

Advice from this level of practitioner is still useful because you may now compare implementation notes with each other. A person who has attempted to put some knowledge into practice is likely to also have some insight into the challenges of implementing aforementioned knowledge. These notes are useful, in the same way that case studies are useful — they provide a record of what has worked, and in what context.

There’s also one added benefit to studying advice from this level of practitioner: you may check against the person's results if you have no time to implement such an intervention yourself. It doesn’t cost you much to circle back to a self-help blogger or person and ping them: “hey, your experiment with deliberate practice — how did it go?” I’ve occasionally found it worthwhile to schedule 15-minute Skype calls with willing practitioners to probe them for the results of their experience.

The last and final rung on my proposed hierarchy is **plausible argument**. This is the lowest-level form of evidence, because — as I’ve argued before — the persuasiveness of an argument should not affect your judgment of the argument’s actual truth.

Some friends have pointed out to me that the structure of an argument should at least be logically valid — that is, that the argument should be free of [argumentative fallacies](https://en.wikipedia.org/wiki/List_of_fallacies?ref=commoncog.com), and have a propositionally valid argumentation form. If an argument fails even this basic test, surely it cannot be correct?

I think there’s some merit to this view, but I also think that there’s relatively little one can gain from studying the internal consistency of an argument. To state this differently, I think that it is almost always better to run a test for some given advice — if such a cheap test exists! — compared to endlessly ruminating about its potential usefulness.

## Luck and Other Confounding Variables

At this point you're probably ready to leap out of your chair to point out: “What’s the use of relying on this hierarchy of evidence? The expert that you are seeking advice from might just have been lucky!”

Yes, and luck is a valid objection! Confounding variables like luck are one of the biggest problems we face when operating at the level of anecdotal evidence. We don’t have the rigour that comes with the scientific method, where we can isolate variables from each other.

(Dealing with luck is also one reason Dalio’s believability standard calls for three successes, to reduce the probability that the practitioner is a fluke.)

But why stop at luck? Luck isn't the only confounding variable when it comes to anecdata, after all. There's also:

- **Genetics**. The expert could have certain genetic differences that make it easier for them to do what they do.
- **Cultural environment**. The expert could be giving you advice that only works in their culture — organisational or otherwise.
- **Prerequisite sub-skills**. ‘ [The curse of expertise](https://en.wikipedia.org/wiki/The_curse_of_expertise?ref=commoncog.com) ’ is what happens when an expert practitioner forgets what it's like to be a novice, and gives advice that can't work without a necessary set of sub-skills — skills that the expert mastered too long ago to remember.
- **Context-specific differences**. The expert could be operating in a completely different context — for instance, advice from someone operating in stock picking might not apply cleanly to those running a business.
- **External advantages**. Reputation, network, and so on.

Let’s say that you are given some advice by a believable person with exposure to downside effects — which is the second highest level of credibility in my proposed hierarchy of practical evidence. You attempt to put the advice to practice, but you find that it doesn’t work. What do you conclude?

The naive view is to conclude that the advice is flawed, the expert is not believable, or that some confounding variable might have gotten in the way. For instance, you might say “oh, that worked for Bill Gates, but it’s never going to work for me — Gates got lucky.”

What is a practitioner to do in the face of so many confounding variables? Should you just throw your hands in the air and say that there’s absolutely no way to know if a given piece of advice is useful? Should you just give up on advice in general?

Well, no, of course not! There are far better moves available to you, and I want to spend the remainder of this essay arguing that this is the case.

One of the ideas that I’ve sort of snuck into this essay is the notion of applying a probability rating to some statement of belief. For instance, in the segment about falsifiability, earlier in the essay, I mentioned that we could take our belief in a hypothesis (with swan colours, I said that perhaps we start with 0.6) and then increase or decrease that belief as we gather more evidence. Some people call this activity ‘ [Bayesian updating](https://fs.blog/2018/09/bayes-theorem/?ref=commoncog.com) ’, and I’d like to suggest that we can adapt this to our practical experimentation.

Here’s a recent example by way of demonstration: a few months back I summarised Cal Newport’s *[Deep Work](https://commoncog.com/deep-work/)*, and started systematically applying the ideas from his book to my life. I’ve found that Newport’s method of ‘ [taking breaks from focus](https://commoncog.com/deep-work/#technique21takebreaksfromfocusnotdistraction) ’ to be particularly difficult to implement — I would try it for one or two days, and then regress to where I was before.

I started the experiment with the notion that Newport was believable — after all, he mentioned that he used the techniques in his book to achieve tenure at a relatively young age. My estimation of the technique working for me began at around 0.8.

After putting his technique to the test and failing at getting it to work, I sat back to consider the confounding variables:

- **Luck:** was Newport lucky? I didn’t think so. Luck has little to do with the applicability of this technique.
- **Genetics:** could Newport have genetic advantages that allowed him to focus for longer? This is plausible. Thanks to the power of twin studies, we know that there is a genetic basis for self-control — around 60% of the variance if we take this [meta-analysis](https://www.sciencedirect.com/science/article/pii/S0149763418307905?ref=commoncog.com) as evidence.
- **Prerequisite sub-skills:** could Newport have built pre-requisite sub-skills? This is also plausible. Newport has had a long history of developing his ability to focus in his previous life as a postdoc researcher at MIT. There may be certain intermediate practices or habits that I would have to cultivate in order to attempt his technique successfully.
- **Context-specific differences:** Newport could also benefit from his work environment. He has said that an ability to perform Deep Work is what sets good academics apart from their peers. This might provide him with a motivational tailwind that others might not possess.
- **External advantages:** I can’t think of any external advantages Newport might have deployed in service of this technique.

It’s important to note here that I am *not* committing to any one reason. There are too many confounding variables to consider, so I’m not attempting to do more than generate different plausible explanations. These plausible explanations will each have a confidence rating attached to them; as I continue to adapt the technique to my unique circumstances, my intention is to update my probability estimate for each explanation accordingly. These explanations exist as potential hypotheses — I am in essence asking the question: “why isn't this working for me, and what must I change before it does?”

Regardless of my success, I will never know for sure why Newport’s technique works for him and not for me. I will only ever have suspicions … measured by those probability judgments. This is a fairly important point to consider: as a practitioner, I am often only interested in what works *for me*. Rarely will I be interested in some larger truth like why some technique works for one person but not for another. This is, I think, a point in favour of a personal epistemology: the standards for truth are lower when you’re dealing with effects on a sample size of one.

If this is a form of Bayesian updating, when does the updating occur? The answer is that it occurs during the application. In my attempt to apply Newport’s technique, I’ve gained an important piece of information: I now know that without modification, Newport’s ‘breaks from focus’ advice is unlikely to work for me. I would have to modify his advice pretty substantially. The update is negative — my overall confidence in this particular technique is now down to 0.7.

The path forward is clear: I may attempt to continue experimenting with Newport’s technique — or I may shelve this piece of advice when my confidence dips below … 0.5, say. There are many variations to consider before I hit that level, though. I could attempt to build some easier, self-control sub-skills first. Or I could attempt to meditate to grow my ability to focus. I could clear my workspace of distractions, or attempt to pair Newport’s technique with a [pomodoro timer](https://www.marinaratimer.com/?ref=commoncog.com). And even if I fail and shelf Newport’s technique, I could still stumble upon a successful variation by some other practitioner years from now, and decide to take the technique down from my mental shelf to have another go at it.

The point of this example is to demonstrate that confounding variables are a *normal* thing we have to grapple with as practitioners. We don’t have the luxury of the scientific method, or the clarity of more rigorous forms of knowledge. We have only our hunches, which we update through trial and error. But even hunches, properly calibrated, can be useful.

## Fin

I have presented an epistemology that has guided my practice for the past seven years or so. I’ve found it personally useful, and I believe much of it is common sense made explicit. But I also know that this epistemology isn’t at all finished; it is merely the first time that I’ve attempted to articulate it in a single essay.

To summarise:

- Let reality be the teacher. This applies for both scientific knowledge and anecdotal evidence.
- When spelunking in the research literature, keep in mind that science is interested in what is true, not necessarily what is useful to you.
- When evaluating anecdata, weight advice according to a hierarchy of practical evidence.
- When testing advice against reality, use some form of Bayesian updating while iterating, in order to filter out the confounding variables that are inherent in any case study.

If I were to compress this framework for putting mental models to practice into a single motivating statement, I would say that the entire framework can be reconstructed through a personal pursuit of the truth — and that in the context of practice, this truth takes the form of the question: “What can I do to make me better? What is it that works *for me?”*

Now perhaps you’ve noticed the meta aspect of this epistemology.

What happens if we apply the standard of truth that I’ve developed in this essay to the very series within which I’ve chosen to publish it in?

The answer is this: my framework should not be very convincing to you. I am not believable in any of the domains that I currently practice in: I have built two successful organisations of under 50 people each; I’ve had only one business success. I am at least a decade away from becoming believable at the level of success that Dalio demands.

What I *can* promise you, however, is that everything in this series — with one exception\* — has been tested in personal practice. I currently apply this epistemology of practice to my own life. I spent a few ‘misguided’ years on epistemic rationality training, of the sorts recommended by LessWrong. I still sometimes feel the itch to do rational choice analysis — even though I know that such analysis works best in irregular domains. And I have spent the last three years in the pursuit of tacit mental models of expertise.

(\*The only exception is the critical decision method, covered in Part 5. As of writing, I've only had two months of experience with the method.)

I’ve spent a great deal of time on the rhetoric of this series. I’ve used narrative to propel the reader through certain segments where I’ve had to tackle abstract ideas, and I have attempted to summarise the least controversial, most established findings of the judgment and decision making literature from which rationality research sits upon. But you should not believe a single word that I have written. In fact, I would go further and suggest that your degree of belief should be informed mostly by that which you have tested against reality. In Bruce Lee’s words: absorb what is useful, discard what is useless and add what is specifically your own. In Bayesian terms: everything I say should be regarded as an assertion with a confidence rating of far less than 1.

Perhaps this is taking it too far. But then again, perhaps Hume had a point. There is ultimately no truth except that which you uncover for yourself.

I hope you’ve found this series useful.

Originally published , last updated .

This article is part of the [Expertise Acceleration](https://commoncog.com/expertise) topic cluster. Read [more from this topic here→](https://commoncog.com/expertise)

Previous Post

#### ← The Problems with Deliberate Practice

Next Post

[![Feature image for What Process Improvement in Education Looks Like](https://commoncog.com/content/images/size/w300/2024/09/rigorous_process_improvement_education.jpg)](https://commoncog.com/what-process-improvement-in-education/)[![Feature image for Business Ecosystem Change Takes Time](https://commoncog.com/content/images/size/w300/2024/06/business_ecosystem_change_takes_time.jpg)](https://commoncog.com/business-ecosystem-change/)[![Feature image for Follow Your Nose](https://commoncog.com/content/images/size/w300/2021/03/follow_your_nose.jpeg)](https://commoncog.com/follow-your-nose/)[![Feature image for Create Your Own Rituals](https://commoncog.com/content/images/size/w300/2021/02/create_your_own_rituals.jpeg)](https://commoncog.com/create-your-own-rituals/)

