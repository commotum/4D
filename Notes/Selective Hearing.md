### The Engineering Challenge in HRM for ARC-AGI: Balancing Meta-Learning with Architectural Constraints

The Hierarchical Reasoning Model (HRM) fundamentally falls short in capturing the essence of meta-learning, which demands inferring abstract rules from multiple correlated input-output grid pairs within a single task. HRM's sequence-to-sequence design processes grids in isolation, treating each as a standalone flattened sequence and relying on data augmentation like rotations and reflections to simulate variability. This sidesteps the core requirement of holistic task processing—analyzing inter-example connections to abstract and apply hidden transformations to a test grid. Without concatenating multiple grids into the input context, the model can't perform true few-shot adaptation, as it never learns to reason across demonstrations during training or inference. Counterintuitively, experiments showed that utilizing multi-grid inputs actually led to performance degradation and convergence failures.

The degradations likely stem from two intertwined issues in HRM's transformer-based setup. First, the positional encoding via RoPE on 1D-flattened grids distorts spatial relationships, treating row-column adjacencies anisotropically. These distortions compound when multiple grids are concatenated, as the 1D flattening fails to encode inter-grid semantics, spatial symmetries within grids, and the temporal-causal relationships across input-output pairs, which collectively define the shared transformation rule governing the task. Second, the variable grid sizes within tasks and grid pairs require additional padding tokens to achieve uniform 30x30 grids, the maximum ARC grid size. This padding unnecessarily balloons token counts, inflating quadratic attention costs dramatically. For example, a single grid requires 810,000 operations, but a full task with 5-6 grids could exceed 29 million, making training on multi-grid inputs infeasible on modest hardware while introducing noise from irrelevant padding tokens. These challenges—inefficient spatial representation and compute explosion—block scaling to multi-grid inputs for effective meta-learning.

To overcome these limitations and enable true meta-learning on multi-grid contexts in ARC-like tasks, we propose two innovations tailored to address these specific issues. First, we introduce a novel positional encoding mechanism designed to capture isotropic spatiotemporal relationships, replacing RoPE’s 1D approach with a structure that preserves grid semantics and inter-grid connections. RoPE encodes a token’s 1D index by applying **unit-circle rotations** to **pairs of embedding dimensions** (treating each consecutive pair as a complex number and multiplying by $e^{i\theta(m)}$ at frequency-scaled angles). Because these 2×2 rotations are orthogonal, they preserve the Euclidean norm and satisfy the absolute-to-relative identity $\langle R_m q,\; R_n k\rangle=\langle q,\; R_{n-m}k\rangle$; blockwise application across all pairs (with a multiscale frequency schedule) extends to higher-dimensional embeddings.

Our proposed encoding, MonSTERs (Minkowski Space Time Embedding Rotors), extends this concept to spacetime, directly addressing the spatial and temporal distortions of flattened representations. MonSTERs encode a token’s **4D spacetime position** $s=(t,x,y,z)$ by applying **bivector-generated Lorentz rotors (boosts + spatial rotations)** to **blockwise 4D subspaces** of the embedding (organized per-frequency triads for X/Y/Z). The rapidities/angles are linear in $s$ (scaled by per-frequency wavelengths), so the transform $L(s)$ is an isometry of the **Minkowski metric** $\mathrm{diag}(1,-1,-1,-1)$, preserving the Minkowski norm and yielding the RoPE-style fusion $\langle L(s_q)q,\; L(s_k)k\rangle_{\eta}=\langle q,\; L(s_k-s_q)k\rangle_{\eta}$. In short: RoPE uses **unit-circle rotations on pairs** to encode 1D positions; MonSTERs use **Lorentz rotors on 4D blocks** to encode 4D spacetime—both achieving exact absolute-relative equivalence at multiple scales. 
This part is perfect:

To address the computational challenges posed by padding and quadratic attention in multi-grid contexts, we introduce a novel dynamic selective attention mechanism inspired by reinforcement learning (RL) principles. Unlike traditional approaches that modify attention scores across all tokens in a sequence or employ patch-based attention (e.g., self-attention within patches followed by cross-attention between groups), our mechanism treats attention as an active, policy-driven selection process. It dynamically chooses a sparse subset of tokens for processing at each step, focusing computational resources on semantically relevant elements while minimizing noise from padding and irrelevant grid positions. This RL-flavored design not only reduces the effective sequence length—often to a fraction of the full context—but also imbues the model with a form of "agency," allowing it to learn anticipatory behaviors about which tokens drive transformations and how actions propagate changes across the task.

The input to the model is structured as a unified context comprising three key components:

  1. Console Space: A dedicated workspace for intermediate reasoning, akin to a "scratchpad" or agent's internal state. This includes tokens representing strategies, concepts, references to prior observations, and crucially, actions selected in previous steps. The console is initialized with special tokens (e.g., placeholders for hypotheses or rule abstractions) and evolves over multiple forward passes.
  
  2. Demonstration Grids: A variable number of input-output grid pairs from the task examples, using MonSTER positional encodings to preserve spatial and inter-grid relationships.

  3. Test Grids: The input test grid alongside a partially masked output test grid, where unknown positions are filled with special "unknown" tokens to indicate hidden values that the model must predict.

The full context, which may span thousands of tokens due to padding for uniform grid sizes (e.g., up to 30x30 per grid), is first compressed into a low-dimensional state representation. This state is derived via a lightweight encoder (e.g., a mean-pooled or attention-bottlenecked projection of the entire input embedding), capturing global task features such as grid statistics, detected patterns, or high-level symmetries without full quadratic computation.

This low-dimensional state serves as input to an RL policy network, parameterized as a small MLP or lightweight transformer head appended to the main HRM architecture. The policy outputs a boolean tensor of the same length as the full context, where each entry indicates whether the corresponding token should be selected for attention processing (1 for selected, 0 otherwise). To ensure sparsity and efficiency, the policy is trained to select far fewer tokens than the total context—typically 10-20%—prioritizing those likely to contribute to meta-learning. For instance, selections often encompass the entire console space (to maintain reasoning continuity) alongside targeted groupings from the grids, such as contiguous regions exhibiting potential transformations or boundary elements that align across input-output pairs.

Once selected, the sparse subset of tokens undergoes self-attention in a dedicated layer. Importantly, this is not global attention over the full sequence; instead, we extract the selected tokens into a compact subsequence, apply standard transformer self-attention (with MonSTERs encodings preserved), and then scatter the updated embeddings back into their original positions in the full context. The model then decodes outputs only for the selected positions, generating new token values where applicable. This output step includes not just grid updates (e.g., filling "unknown" tokens in the test output) but also action tokens in the console space—discrete choices from a predefined action vocabulary, such as "highlight edge," "apply rotation to region," or "abstract rule hypothesis." These actions effectively "edit" the context in subsequent steps, simulating iterative reasoning.
This setup introduces two key emergent capabilities essential for meta-learning:

  1. Anticipatory Selection: The policy must learn to foresee not only which tokens cause changes (e.g., pivotal elements in demonstration grids that reveal the transformation rule) but also which will be changed (e.g., console tokens for action placement or test grid positions for prediction). During training, this is reinforced via an RL objective: a reward signal based on task progress, such as partial accuracy on grid predictions or efficiency metrics (e.g., minimizing selected tokens while maximizing rule abstraction). Exploration is encouraged through techniques like entropy regularization or epsilon-greedy sampling of the boolean tensor.

  2. Action-Outcome Understanding: By placing selected actions back into the console and propagating their effects through repeated forward passes (e.g., in a loop until convergence or a fixed horizon), the model builds an implicit model of how its choices influence the space. For example, selecting an action like "mirror across axis" might trigger updates in linked grid tokens, allowing the policy to refine future selections based on observed causal chains. This fosters few-shot adaptation, as the console accumulates cross-example insights, enabling holistic reasoning over inter-grid connections without processing the entire context at once.

In practice, this mechanism dramatically curbs computational costs: for a multi-grid task with ~5,000 tokens, selecting ~1,000 reduces attention complexity from O(5,000²) to O(1,000²), making training feasible on modest hardware (e.g., single GPUs). During inference, the iterative nature allows adaptive refinement, where early steps focus on coarse rule detection across demonstrations, and later steps zoom in on test grid predictions. Combined with MonSTERs, this enables HRM to scale to true meta-learning, processing tasks as interconnected wholes rather than isolated sequences, and unlocking performance gains on ARC-like benchmarks where holistic abstraction is paramount.