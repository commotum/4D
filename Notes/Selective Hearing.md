### The Engineering Challenge in HRM for ARC-AGI: Balancing Meta-Learning with Architectural Constraints

The Hierarchical Reasoning Model (HRM) fundamentally falls short in capturing the essence of meta-learning, which demands inferring abstract rules from multiple correlated input-output grid pairs within a single task. HRM's sequence-to-sequence design processes grids in isolation, treating each as a standalone flattened sequence and relying on data augmentation like rotations and reflections to simulate variability. This sidesteps the core requirement of holistic task processing—analyzing inter-example connections to abstract and apply hidden transformations to a test grid. Without concatenating multiple grids into the input context, the model can't perform true few-shot adaptation, as it never learns to reason across demonstrations during training or inference. Counterintuitively, experiments showed that utilizing multi-grid inputs actually led to performance degradation and convergence failures.

The degradations likely stem from two intertwined issues in HRM's transformer-based setup. First, the positional encoding via RoPE on 1D-flattened grids distorts spatial relationships, treating row-column adjacencies anisotropically. These distortions compound when multiple grids are concatenated, as the 1D flattening fails to encode inter-grid semantics, spatial symmetries within grids, and the temporal-causal relationships across input-output pairs, which collectively define the shared transformation rule governing the task. Second, the variable grid sizes within tasks and grid pairs require additional padding tokens to achieve uniform 30x30 grids, the maximum ARC grid size. This padding unnecessarily balloons token counts, inflating quadratic attention costs dramatically. For example, a single grid requires 810,000 operations, but a full task with 5-6 grids could exceed 29 million, making training on multi-grid inputs infeasible on modest hardware while introducing noise from irrelevant padding tokens. These challenges—inefficient spatial representation and compute explosion—block scaling to multi-grid inputs for effective meta-learning.

To overcome these limitations and enable true meta-learning on multi-grid contexts in ARC-like tasks, we propose two innovations tailored to address these specific issues. First, we introduce a novel positional encoding mechanism designed to capture isotropic spatiotemporal relationships, replacing RoPE’s 1D approach with a structure that preserves grid semantics and inter-grid connections. RoPE encodes a token’s 1D index by applying **unit-circle rotations** to **pairs of embedding dimensions** (treating each consecutive pair as a complex number and multiplying by $e^{i\theta(m)}$ at frequency-scaled angles). Because these 2×2 rotations are orthogonal, they preserve the Euclidean norm and satisfy the absolute-to-relative identity $\langle R_m q,\; R_n k\rangle=\langle q,\; R_{n-m}k\rangle$; blockwise application across all pairs (with a multiscale frequency schedule) extends to higher-dimensional embeddings.

Our proposed encoding, MonSTERs (Minkowski Space Time Embedding Rotors), extends this concept to spacetime, directly addressing the spatial and temporal distortions of flattened representations. MonSTERs encode a token’s **4D spacetime position** $s=(t,x,y,z)$ by applying **bivector-generated Lorentz rotors (boosts + spatial rotations)** to **blockwise 4D subspaces** of the embedding (organized per-frequency triads for X/Y/Z). The rapidities/angles are linear in $s$ (scaled by per-frequency wavelengths), so the transform $L(s)$ is an isometry of the **Minkowski metric** $\mathrm{diag}(1,-1,-1,-1)$, preserving the Minkowski norm and yielding the RoPE-style fusion $\langle L(s_q)q,\; L(s_k)k\rangle_{\eta}=\langle q,\; L(s_k-s_q)k\rangle_{\eta}$. In short: RoPE uses **unit-circle rotations on pairs** to encode 1D positions; MonSTERs use **Lorentz rotors on 4D blocks** to encode 4D spacetime—both achieving exact absolute-relative equivalence at multiple scales. 

To address the computational challenges posed by padding and quadratic attention in multi-grid contexts, we introduce a novel dynamic selective attention mechanism inspired by reinforcement learning (RL) principles. Unlike traditional approaches that modify attention scores across all tokens in a sequence or employ patch-based attention (e.g., self-attention within patches followed by cross-attention between groups), our mechanism treats attention as an active, policy-driven selection process. It dynamically chooses a sparse subset of tokens for processing at each step, focusing computational resources on semantically relevant elements while minimizing noise from padding and irrelevant grid positions. This RL-flavored design not only reduces the effective sequence length—often to a fraction of the full context—but also imbues the model with a form of "agency," allowing it to learn anticipatory behaviors about which tokens drive transformations and how actions propagate changes across the task.

The input to the model is structured as a unified context comprising three key components:

  1. Console Space: A dedicated workspace for intermediate reasoning, akin to a "scratchpad" or agent's internal state. This includes tokens representing strategies, concepts, references to prior observations, and crucially, actions selected in previous steps. The console is initialized with special tokens (e.g., placeholders for hypotheses or rule abstractions) and evolves over multiple forward passes.
  
  2. Demonstration Grids: A variable number of input-output grid pairs from the task examples, using MonSTER positional encodings to preserve spatial and inter-grid relationships.

  3. Test Grids: The input test grid alongside a partially masked output test grid, where unknown positions are filled with special "unknown" tokens to indicate hidden values that the model must predict.

The full context, which may span thousands of tokens due to padding for uniform grid sizes (e.g., up to 30x30 per grid), is first compressed into a low-dimensional state representation. This state is derived via a lightweight encoder (e.g., a mean-pooled or attention-bottlenecked projection of the entire input embedding), capturing global task features such as grid statistics, detected patterns, or high-level symmetries without full quadratic computation.

This low-dimensional state serves as input to an RL policy network, parameterized as a small MLP or lightweight transformer head appended to the main HRM architecture. The policy outputs a boolean tensor of the same length as the full context, where each entry indicates whether the corresponding token should be selected for attention processing (1 for selected, 0 otherwise). To ensure sparsity and efficiency, the policy is trained to select far fewer tokens than the total context—typically 10-20%—prioritizing those likely to contribute to meta-learning. For instance, selections often encompass the entire console space (to maintain reasoning continuity) alongside targeted groupings from the grids, such as contiguous regions exhibiting potential transformations or boundary elements that align across input-output pairs.

Once selected, the sparse subset of tokens undergoes self-attention in a dedicated layer. Importantly, this is not global attention over the full sequence; instead, we extract the selected tokens into a compact subsequence, apply standard transformer self-attention (with MonSTERs encodings preserved), and then scatter the updated embeddings back into their original positions in the full context. The model then decodes outputs only for the selected positions, generating new token values where applicable. This output step includes not just grid updates (e.g., filling "unknown" tokens in the test output) but also action tokens in the console space—discrete choices from a predefined action vocabulary, such as "highlight edge," "apply rotation to region," or "abstract rule hypothesis." These actions effectively "edit" the context in subsequent steps, simulating iterative reasoning.

This setup introduces two key emergent capabilities essential for meta-learning:

  1. Anticipatory Selection: The policy must learn to foresee not only which tokens cause changes (e.g., pivotal elements in demonstration grids that reveal the transformation rule) but also which will be changed (e.g., console tokens for action placement or test grid positions for prediction). During training, this is reinforced via an RL objective: a reward signal based on task progress, such as partial accuracy on grid predictions or efficiency metrics (e.g., minimizing selected tokens while maximizing rule abstraction). Exploration is encouraged through techniques like entropy regularization or epsilon-greedy sampling of the boolean tensor.

  2. Action-Outcome Understanding: By placing selected actions back into the console and propagating their effects through repeated forward passes (e.g., in a loop until convergence or a fixed horizon), the model builds an implicit model of how its choices influence the space. For example, selecting an action like "mirror across axis" might trigger updates in linked grid tokens, allowing the policy to refine future selections based on observed causal chains. This fosters few-shot adaptation, as the console accumulates cross-example insights, enabling holistic reasoning over inter-grid connections without processing the entire context at once.

In practice, this mechanism dramatically curbs computational costs: for a multi-grid task with ~5,000 tokens, selecting ~1,000 reduces attention complexity from O(5,000²) to O(1,000²), making training feasible on modest hardware (e.g., single GPUs). During inference, the iterative nature allows adaptive refinement, where early steps focus on coarse rule detection across demonstrations, and later steps zoom in on test grid predictions. Combined with MonSTERs, this enables HRM to scale to true meta-learning, processing tasks as interconnected wholes rather than isolated sequences, and unlocking performance gains on ARC-like benchmarks where holistic abstraction is paramount.

### Training Regimen for the Dynamic Selective Attention Mechanism

To effectively train the policy network and integrate it with the overall HRM architecture, we employ a multi-stage training approach that progressively shifts from supervised learning to reinforcement learning, ensuring stable convergence and optimal performance.

- **Stage 1: Supervised Fine-Tuning with Teacher Forcing**: In the initial phase, we perform supervised fine-tuning on both the token position selection for the dynamic attention mechanism and the state evolution process. For token selection, we use expert demonstrations (e.g., heuristically predefined or oracle-selected sparse subsets based on ground-truth relevant tokens, such as those involved in the true transformation rule) to train the policy network via binary cross-entropy loss, treating selection as a classification task. Simultaneously, for state evolution (i.e., how the console space and grid tokens update over steps), we apply teacher forcing: during forward passes, the model is fed ground-truth actions and intermediate states from curated trajectories, minimizing prediction errors on grid outputs and console updates using standard sequence-to-sequence losses (e.g., cross-entropy for token predictions). This stage bootstraps the model with reliable patterns, stabilizing early training and preventing divergence in sparse attention setups.

- **Stage 2: Pure Reinforcement Learning for Token Selection**: Once the supervised foundation is established, we transition to pure RL focused exclusively on token selection. The policy network now samples selections autonomously, and training proceeds via policy gradient methods (e.g., REINFORCE with baseline) or actor-critic variants. Rewards are derived from downstream task success (e.g., accuracy in filling test grid unknowns) and efficiency (e.g., sparsity bonuses to encourage minimal selections). State evolution remains guided by the fine-tuned decoder, but without teacher forcing—allowing the model to learn from its own generated trajectories. This stage refines the anticipatory selection capability, teaching the policy to adaptively choose tokens that maximize meta-learning efficiency across diverse ARC tasks.

- **Stage 3: Group Relative Policy Optimization (GRPO)**: In the final stage, we adopt a GRPO approach, inspired by advancements like those seen in models such as DeepSeek, to further optimize the entire system. GRPO, an enhancement over Proximal Policy Optimization (PPO), leverages group-based relative comparisons to stabilize and accelerate policy learning. Unlike PPO, which relies on clipping to constrain policy updates, GRPO optimizes by comparing groups of sampled trajectories, prioritizing those that collectively maximize a reward function based on criteria like rule abstraction quality, prediction accuracy, and computational efficiency. This group-wise perspective reduces variance in gradient estimates, enabling more robust updates to both the policy network (for token selection) and the decoder (for state evolution). We train the model to select actions that align with high-reward trajectories, using rewards derived from task success and sparsity bonuses, thereby enhancing generalization to unseen ARC tasks. This stage refines emergent capabilities, such as action-outcome understanding, and fosters robust few-shot adaptation by ensuring the model learns to prioritize impactful token selections and state updates across diverse scenarios.

This staged progression ensures gradual complexity buildup, mitigating issues like reward sparsity in pure RL from scratch and leveraging supervision for faster convergence.

### Bootstrapping with CSP Puzzles for Spatiotemporal Reasoning

To bootstrap the initial HRM model and cultivate its spatiotemporal reasoning capabilities prior to full-scale ARC training, we will leverage a diverse set of constraint satisfaction problem (CSP) puzzles. These include classic grid-based challenges such as Sudoku, Slitherlink, Minesweeper, Tic-Tac-Toe, and more. The selection of these puzzles is deliberate: they inherently require sequential decision-making, pattern recognition across spatial dimensions, and logical inference over evolving states—skills directly transferable to the abstract rule induction demanded by ARC-AGI tasks. By starting with these more structured domains, we can pre-train the model on interpretable, scalable data, building a foundation for handling the noisier, less constrained ARC puzzles.

Our approach will require deploying specialized CSP solvers based on Denis Berthier’s CSP-Rules 2.0 methodology, tailored to each puzzle type and implemented in Python to generate comprehensive solution trajectories. As the solver progresses through each puzzle, we meticulously log every aspect of the process: the initial unsolved grid, each intermediate state after an action, the full solution path including backtracks or dead-ends (to teach recovery from errors), and the final completed puzzle. Crucially, we also capture the solver's internal reasoning traces, such as variable domains, constraint violations, and heuristic choices. This comprehensive tracing transforms each puzzle solution into a rich trajectory dataset, where each step represents a micro-task of observation, deduction, and update—mirroring the iterative reasoning loop in our dynamic selective attention mechanism.

To align this data with ARC's few-shot meta-learning paradigm, we augment the raw trajectories with transformations that preserve logical integrity while varying surface features. These include digit or color remappings (e.g., swapping Sudoku numbers or recoloring Minesweeper flags without altering adjacency rules), geometric operations like rotations, reflections, or scalings (ensuring constraints remain isomorphic), and even minor perturbations such as adding distractor elements that don't impact solvability. For a single original puzzle trajectory, this augmentation pipeline can generate 2-5 variant examples per step, each consisting of transformed input-output grid pairs. The unaltered original is reserved as the "test" case, encouraging the model to infer the underlying rules from demonstrations and apply them to the unseen variant—directly emulating ARC's structure.

At the core of this dataset is the decomposition of each solution step into three key components, which serve as supervised signals for training the policy network and console space dynamics. First, the **scope** delineates the spatial focus of the solver at that moment, such as a specific row, column, or subgrid in Sudoku, or a cluster of edges in Slitherlink. This teaches the model to select relevant token subsets dynamically, aligning with our anticipatory selection capability. Second, the **trigger** captures the precise conditions observed within the scope that prompted a decision, like an empty cell with only one possible value due to row exclusions, or a "2" clue in Slitherlink indicating exactly two adjacent edges must be filled. These triggers foster pattern recognition and causal understanding, essential for abstracting transformation rules. Third, the **action** details the response executed, such as filling a cell, marking a mine, or propagating a constraint, including any conditional logic or probabilistic choices in heuristic solvers. By training on these decomposed steps, the model learns to chain scope-trigger-action triplets into coherent trajectories, enhancing its action-outcome understanding.

This bootstrapping strategy not only addresses data scarcity in ARC-like domains but also injects inductive biases for spatiotemporal reasoning early in training. For example, Sudoku variants emphasize global consistency across local changes, while Minesweeper introduces uncertainty and probabilistic inference—broadening the model's robustness. As we transition to ARC fine-tuning, these pre-trained weights will accelerate convergence in the multi-stage regimen, particularly in Stage 1 where supervised signals from CSP trajectories can serve as "expert demonstrations" for token selection and state evolution. Ultimately, this method scales efficiently: with automated solvers and augmentation, we can generate millions of diverse examples from a modest puzzle corpus, paving the way for HRM to achieve true meta-learning prowess on challenging benchmarks.

---

Next Steps:

1. Generic CSP Solver based on Berthier's CSP Rules 2.0
2. Sudoku Solver
3. Slitherlink
4. 