I want the truth, but I am trying to also show a pattern here. RGB pixels are fundamentally "reality" based. They come from human knowledge of physics. They are the digital scalars of light. Their input format is semantically meaningful and has geometric structure. Text is fundamentally "convenience" based. It's a digital mapping to language, but it lacks geometric structure, and therefore it lacks semantic meaning. This is why Bengio and Mikolov are so important. They show that you can discover the geometric structure of language.

And my argument is that when our knowledge is accurate, using it is not a bad thing. However, it doesn't mean that his message wasn't relevant at the time. 

President Truman said, "It is amazing what you can accomplish if you do not care who gets the credit." I think what Sutton was arguing against was the trend of researchers to ignore or reject scalable general methods like search and learning in favor of knowledge systems. In large part I think the tendency towards knowledge systems was driven by two causes. 1. It is difficult to seem/look smart to others if what you implement is simple or obvious. In this case, it's researchers prioritizing their personal appearance as an intelligent researcher capable of making creative or insightful contributions. This was to the detriment of their systems, but was personally advantageous. They needed to take credit. 2. There is a very real sense that much of search and learning is wasteful. The entire field of Constraint-Satisfaction-Problem Solvers is because you can reduce your electricity and compute costs by orders of magnitude if you switch from exhaustive naive search to constructive structured search. In fact true intelligence can be seen as the ability to do more with less. To accrue energy instead of wasting it. This gave researchers a reasonable excuse that they could lie to themselves with. They could say their approach was intelligent or strong or expert based and not wasteful or naive or brute. They could say this was their motivating factor. But in reality they were setting a limit on who could contribute, and on who could take credit. They didn't want the computers to take credit, and so they prevented what could be a reliable partnership.

Now, I want you to think about these two reasons I gave. Think about how Robert Greene would describe it, and what he would attribute as motivations. Now a lot of people find his writing distasteful, because it's rather blunt, and shines a light on less than noble human tendencies, but that doesn't mean he's wrong. In fact, while he may oversimplify things, he is more directionally correct than most. Now that you've thought about human nature, think about what Sutton's shadow motives might be for rejecting human knowledge. 

Finally, as our last observational exercise, it seems clear, the current computational capacity of the human brain is greater in scale, more widely distributed, and more energy efficient, than any single computer system. A single human brain can do an exaflop of compute. Its access to sensory data far outstrips the access of any computer, and will remain that way for the foreseeable future. It is also obvious that through the internet, todays largest AI systems far outstrip humans in total human knowledge consumed, in bandwidth for high capacity communication and cooperation, and for duplication. Additionally, compute will only continue to scale.

It seems logical to observe that Sutton was reacting to individual researchers limiting contributions and rejecting compute.
It also seems logical to observe, like Sutton, that using human knowledge and compute rarely go together, as time spent on one is at the expense of the other.
It is inherently obvious that the reason expert systems failed and were brittle was not because of the information and knowledge they programmed in, but because of the ignorance they programmed in. Researchers simply didn't know what they didn't know. They focused on what they could make observable progress on. 

However, what if there was a system that made it easy for humans to share their knowledge. Currently AI systems excel at coding, in large part because their creators excel at coding. How much information or knowledge is hidden inside of the brains of our best doctors, scientists, mechanics, machinists, manufacturers, and engineers? It's clear that none of them will likely ever work on general AI systems, or on general learning or search methods. The cost of learning another discipline (computer programming) in fact, another language, is so monumental. But what if we made that easy. What if we drastically scaled up the amount of human knowledge being created, copied, learned, and observed by AI systems, by making it easy for anyone to contribute the best that they had, and with a system that could go out and test that against reality.

At the olympic level, the 4x100m relay record is about 15% faster than the individual 400m record.

Sutton made the argument that in a head to head race, computers would win. And in a straight sprint, he's right. But if we treat it as a relay, the decisive advantage isn’t speed. It’s the handoff. And the team that takes the best from both sides will always beat the team that dogmatically insists on one way or the other. The team that makes iteration cycles seamless. The team that builds and builds momentum. That's the team that will win.

So I need the history of Reinforcement Learning and of Language Modeling as a series of changes where we removed an architectural constant that baked in ignorance not knowledge. Because the problem isn't humans vs computers. It's statics vs dynamics. Those who view the world as fixed. And those who view the world as fluid. It's how do we accumulate compute, energy, and resources. How do we avoid waste and capitalize on the momentum of the world's current trajectory. 