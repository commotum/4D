In the context of extending Rotary Position Embeddings (RoPE) to incorporate Minkowski spacetime geometry in transformer architectures, a single precomputed matrix can efficiently transform vectors to enable the computation of the Minkowski inner product using standard Euclidean dot products. Specifically, for 4D blocks within the embedding dimension (with signature (+, -, -, -)), the Minkowski metric tensor η is a diagonal matrix with entries [1, -1, -1, -1]. By precomputing a mask matrix M identical to η (tiled across the full embedding dimension for blockwise application), one can adjust the key vectors (or queries) via elementwise multiplication: k' = M * k. This transformation ensures that the standard dot product q · k' equals the Minkowski inner product q^T η k, without altering the original vectors during training or inference. Since M consists solely of ±1 values, it incurs negligible computational overhead and can be created once at initialization.

This approach maintains the invariance properties crucial for normalization in Minkowski space. Lorentz transformations (implemented as Clifford algebra rotors) preserve the Minkowski norm, meaning ||v||_η^2 = v^T η v remains unchanged under rotation. By applying the mask only after rotations, the adjusted vectors k' allow the attention mechanism to compute similarities that respect relativistic geometry, such as emphasizing time-like over space-like separations. Importantly, because the mask is fixed and diagonal, it doesn't affect the gradients in a way that disrupts training stability, while enabling the model to learn position encodings that are Lorentz-invariant, potentially improving generalization in tasks involving causal or spatiotemporal sequences.

