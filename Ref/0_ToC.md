### 1. The Transformer Positional Encodings That Have Researchers Racking Their Brains
- **File Name**:   `Jianlin_Su_1.md`
- **Author**:      Su Jianlin
- **Publication**: Scientific Spaces
- **Headline**:    Su Jianlin's blog post surveys absolute, relative, and unconventional positional encoding methods for Transformers, Prior to his invention of RoPE.
- **Quotes**:
    > "Perhaps everyone defaults to addition because the addition of vectors has a clearer geometric meaning, but for deep learning models, this geometric meaning has little practical value".
    >
    > "...research is about enumerating all permutations to see which is optimal".

---

### 2. The Path to Upgrading Transformers: 2. Rotary Position Embedding...
- **File Name**:   `Jianlin_Su_2.md`
- **Author**:      Su Jianlin
- **Publication**: Scientific Spaces
- **Headline**:    Su Jianlin's blog post detailing the mathematical derivation of RoPE, a new type of positional encoding, fusing absolute and relative methods via complex plane rotations in a single, drop-in, widely-compatible method.
- **Quotes**:
    > "Quite interestingly, the dot product only depends on the relative position $m-n$! This cleverly fuses absolute and relative positions together."
    >
    > "It is worth noting that $R_m$ is an orthogonal matrix, which does not change the norm of the vector, so generally speaking, it will not change the stability of the original model."

---

### 3. Cormac McCarthy Explains the Unconscious
- **File Name**:   `McCarthy_On_The_Unconscious.md`
- **Author**:      Nick Romeo
- **Publication**: The New Yorker
- **Headline**:    Nick Romeo reviews Cormac McCarthy's essay on the unconscious, which McCarthy posits as an ancient, non-linguistic intelligence.
- **Quotes**:
    > "When I asked him why he never reads new novels, he looked as if I wanted to know why someone would not drink from a pool of muddy water. 'They’re not readable,' he said".
    >
    > "McCarthy’s fiction is often accused of offering a bleak, blood-drenched, nihilistic vision of human life. While his thoughts on the unconscious are framed as scientific reflections, they also creep toward theology...".

---

### 4. Quaternion Neural Network and Its Application
- **File Name**:   `Quaternion_Neural_Network_and_Its_Application.md`
- **Author**:      Isokawa et al.
- **Publication**: Academic Paper
- **Headline**:    Demonstrates that Quaternion Neural Networks maintain color fidelity in image compression by correctly learning 3D spatial transformations, a task where real-valued networks fail.
- **Quotes**:
    > "The output image obtained by real-valued BP is reddish and its tone is different from the original image. Since large portions of the training image (Fig.3(a)) are red, the network is trained so as to project most colors onto red colors".
    >
    > "Quaternion BP utilizes the affine transformation of the vectors in its processing. This affine transformation is done in the whole color space, whereas the transformation using real-valued BP applies only to a part of the color space".

---

### 5. Quaternion Recurrent Neural Networks
- **File Name**:   `Quaternion_Recurrent_Neural_Networks.md`
- **Author**:      Parcollet et al.
- **Publication**: Academic Paper
- **Headline**:    Details how Quaternion Recurrent Neural Networks use hypercomplex algebra to learn inter-dependencies in multidimensional sequences while drastically reducing model size by quartering parameter counts.
- **Quotes**:
    > "...a $4$-number quaternion weight linking two 4-number quaternion units only has $4$ degrees of freedom, whereas a standard neural net parametrization has $4 \times 4=16$, i.e., a 4-fold saving in memory".
    >
    > "The results reported in bold on tables are obtained with the best configurations of the neural networks observed with the validation set".

---

### 6. Rotary Embeddings: A Relative Revolution
- **File Name**:   `RoPE.md`
- **Author**:      Biderman, Black, et al.
- **Publication**: EleutherAI
- **Headline**:    EleutherAI blog post detailing the derivation and superior performance of RoPE for Transformers across various architectures.
- **Quotes**:
    > "Despite a tremendous number of papers that have come out claiming to improve the transformer architecture, very few approaches generalize well across codebases and tasks".
    >
    > "A response many of us at EleutherAI had when first coming across this was 'how does this differ from sinusoidal embeddings,' so we feel it is worth discussing this comparison".

---

### 7. The Kekulé Problem
- **File Name**:   `The_Kekulé_Problem.md`
- **Author**:      Cormac McCarthy
- **Publication**: Nautilus Magazine
- **Headline**:    Cormac McCarthy argues the unconscious is an ancient biological machine that predates language, and is 'loathe to speak', preferring to communicate through images and dreams.
- **Quotes**:
    > "The first dolphin anesthetized on an operating table simply died".
    >
    > "The simple understanding that one thing can be another is at the root of all things of our doing".