4D Clifford Rotary Embeddings for Spatio-Temporal Intelligence
Abstract
We introduce MonSTER (Minkowski Spatio-Temporal Embedding Rotors), a 4D generalization of rotary positional embeddings (RoPE) for transformer models. MonSTER leverages the Clifford algebra Cl(1,3) of Minkowski spacetime to encode relative positions across one temporal and three spatial dimensions (Δt, Δx, Δy, Δz). In contrast to traditional 1D positional encodings, our approach uses rotors (multiplicative position transforms) to natively encode spacetime structure. This yields a unified representation of temporal and spatial relationships that is invariant to shifts in time or space and symmetric under rotations and boosts (Lorentz transformations) in 4D. We derive the 4D rotor formulation and show how it reduces to complex and quaternionic rotary embeddings in lower dimensions. The proposed method seamlessly generalizes relative position encoding to arbitrary dimensions while maintaining key advantages of RoPE: length extrapolation, decaying attention with distance, and efficient implementation. We outline experiments comparing MonSTER against RoPE and high-frequency (NeRF-style) embeddings on tasks spanning intuitive physics puzzles, video understanding, and structured sequence modeling. MonSTER is poised to enable more geometrically consistent spatio-temporal reasoning in transformers, opening avenues for unified multimodal models in vision, language, and beyond.
Introduction
Transformers rely on positional encodings to provide sequence order information that vanilla self-attention lacks
ar5iv.labs.arxiv.org
arxiv.org
. In natural language, the predominant approaches have been absolute position embeddings – either fixed sinusoidal functions
ar5iv.labs.arxiv.org
 or learned vectors
ar5iv.labs.arxiv.org
 – and relative position embeddings, which encode pairwise distances between tokens
ar5iv.labs.arxiv.org
. Absolute encodings (like the sinusoidal features in Vaswani et al.
ar5iv.labs.arxiv.org
) are added to token representations and allow models to attend by absolute sequence index. Learned absolutes (e.g. in BERT
ar5iv.labs.arxiv.org
) can be powerful but do not generalize to longer sequences or unseen positions. Relative positional encodings address this by making attention depend on the distance between tokens rather than their absolute indices
utorontomist.medium.com
. For example, Shaw et al.
ar5iv.labs.arxiv.org
 introduced learned relative bias vectors, and subsequent models like Transformer-XL and T5 generalized this idea to longer contexts
ar5iv.labs.arxiv.org
. These approaches improved length generalization but often involve complex modifications to attention and may not extend naturally beyond 1D sequences. Rotary Position Embeddings (RoPE) emerged as a simple and effective relative encoding method that combines strengths of absolute and relative approaches
ar5iv.labs.arxiv.org
. RoPE encodes positions as rotations applied directly to the query and key vectors in self-attention
utorontomist.medium.com
. By multiplying each $d$-dimensional query/key by a position-dependent rotation matrix (rather than adding a vector), RoPE ensures that the inner product $q_i \cdot k_j$ depends only on the relative positional difference $i-j$
utorontomist.medium.com
. In the 1D case, this is achieved by treating pairs of feature dimensions as complex numbers and rotating them by an angle proportional to the token’s index
utorontomist.medium.com
. The dot product of two such rotated vectors yields a phase term $e^{i(\theta_i - \theta_j)}$ that is a function of the difference in positions
utorontomist.medium.com
. This elegant property allows RoPE (used in the RoFormer model) to achieve strong performance and length extrapolation in language tasks
utorontomist.medium.com
ar5iv.labs.arxiv.org
. RoPE’s relative formulation is compatible even with linearized attention mechanisms
ar5iv.labs.arxiv.org
 and exhibits a natural decay of inter-token correlation with increasing distance
ar5iv.labs.arxiv.org
. However, many domains require modeling data with multi-dimensional position (e.g. time and space in video, or 2D/3D layouts in images and graphs). Existing approaches often fall back to summing or concatenating 1D positional encodings for each dimension – for example, Vision Transformers add separate horizontal and vertical embeddings to each patch
arxiv.org
. Recent work has explored extending RoPE beyond 1D: e.g. applying independent rotations for each dimension (an “axial” RoPE)
github.com
 or using block-wise 2D rotation matrices
medium.com
. These N-D rotary embeddings (as implemented by RoPE-Nd
github.com
) treat each positional coordinate with its own complex rotation, yielding a factorized encoding. While effective for images and videos, such approaches assume a Euclidean factorization of space and time – they lack a unified treatment of spatio-temporal directions, and do not inherently respect rotational symmetry or relativistic mixing between time and space. In this paper, we propose a novel positional encoding paradigm that generalizes RoPE to four dimensions by leveraging Clifford algebra in Minkowski space. We name our method MonSTER, for Minkowski Spatio-Temporal Embedding Rotors. The core idea is to represent a 4D position (time and 3D space) as a single holistic rotor – an element of the Cl(1,3) Clifford algebra that performs a rotation in spacetime. In physics, Minkowski spacetime formalizes space and time as an integrated 4D geometry where rotations in spatial planes and hyperbolic rotations (boosts) in time-space planes form the Lorentz group
en.wikipedia.org
. Our MonSTER embedding encodes each token’s 4D coordinate $(t, x, y, z)$ as a proper orthochronous Lorentz transformation (rotor) that can act on the query/key vectors. When two tokens interact, the combined effect of their rotors in attention is another rotor representing the relative displacement in space and time – analogous to how RoPE yields a relative phase difference
utorontomist.medium.com
. MonSTER brings several key advantages: (1) Directional symmetry – unlike axis-aligned encodings, a displacement in an arbitrary 3D direction is encoded consistently (the embedding depends only on the 4D interval, not the coordinate basis orientation). (2) Unified time and space – time differences and spatial distances are treated in one framework, permitting the model to learn relationships like causality or motion that mix time and space. (3) Invariance under transformations – because encodings are actual rotors in Minkowski space, shifting the reference frame (rotating spatial axes or boosting to a different inertial frame) consistently transforms all positional embeddings by the same group action. The relative relationships between tokens are preserved under such transformations, which is a desirable prior for physical reasoning. (4) Multi-scale and extrapolation – similar to RoPE, MonSTER can incorporate multiple frequency scales per dimension (via multiple independent bivector generators) to capture both coarse and fine relative positional effects, aiding generalization to larger spatio-temporal extents. (5) Flexibility – our approach naturally extends to any number of dimensions (spatial or other attribute axes) by increasing the number of basis bivectors; it can thus be seen as a general framework for geometric positional encoding. We validate MonSTER on a variety of tasks that test spatio-temporal reasoning. First, on synthetic puzzles inspired by the Abstraction and Reasoning Corpus (ARC)
pgpbpadilla.github.io
, we examine whether a transformer can use MonSTER to extrapolate grid patterns and physical rules (e.g. object movement, symmetry) from few examples. Second, we implement a video classification experiment on a small benchmark of temporal video clips, comparing MonSTER to baseline 3D position embeddings in terms of accuracy and robustness to frame rate or viewpoint changes. Third, we design a structured sequence modeling task (such as traversal on a 2D grid or a temporal graph sequence) to evaluate how well MonSTER captures underlying structure compared to standard positional encodings. We will also perform ablations isolating the components of MonSTER: e.g. removing the time/boost component, using Euclidean 4D rotations only, or varying the number of frequency bands. These analyses will elucidate the contribution of each design choice. In all, our contributions are:
4D Clifford RoPE: We introduce the first rotary positional embedding that operates in Minkowski (1+3)D space, enabling transformers to encode relative position in time and 3D space natively via rotor multiplication.
Theoretical framework: We provide an accessible primer on using Clifford algebra rotors for position encoding, drawing connections to complex numbers in 1D and quaternions in 3D
ir.canterbury.ac.nz
. We derive how a combined spatial rotation and Lorentz boost can represent a 4D positional offset, and show that applying these rotors in self-attention yields relative spacetime displacements in the attention scores.
Experiment plans: We outline a comprehensive evaluation on diverse modalities (visual puzzles, video, structured sequences), comparing MonSTER against strong baselines: RoFormer’s axial rotary embeddings and NeRF-style Fourier feature encodings
cseweb.ucsd.edu
. We describe how we will measure performance, generalization (e.g. to longer sequences or larger spatial grids), and the effect of frame-of-reference transformations on model predictions.
Outlook: We discuss the scalability and practical considerations of MonSTER (computational cost, integration into existing models) and highlight its potential for multimodal and structure-based tasks – from unified video-language models to 3D scene understanding and code/AST modeling.
Related Work
Positional Encodings in Transformers: The original Transformer encoder introduced a fixed sinusoidal positional encoding (PE) added to word embeddings
ar5iv.labs.arxiv.org
. This absolute PE used sine and cosine waves of varying frequencies so that each position had a unique periodic signature and the model could potentially generalize to longer sequences via extrapolation of the sinusoid patterns
cseweb.ucsd.edu
. Later, learned absolute position embeddings were adopted in models like BERT
ar5iv.labs.arxiv.org
 and GPT, treating position as a learned lookup. While effective within training range, learned absolute PEs fail to generalize to sequence lengths beyond those seen during training. To address this, researchers developed relative position encodings. Shaw et al.
ar5iv.labs.arxiv.org
 added learnable relative bias vectors to the attention logits, dependent on the distance between query and key positions. Transformer-XL introduced a decomposition allowing the model to attend to representations from previous segments with a relative positional formulation
ar5iv.labs.arxiv.org
. T5’s span-centric relative attention and other approaches (e.g. ALiBi, Rotary) further refined the idea of having attention focus on relative displacement rather than absolute indices. Rotary Positional Embedding (RoPE): Su et al. proposed RoPE as a novel method to encode absolute positions via rotations, thereby inherently encoding relative positions in the attention mechanism
ar5iv.labs.arxiv.org
. Each query and key vector is split into 2-dim subcomponents (pairs of features), and each such pair is treated as a complex number. Multiplying by a complex phase $e^{i\theta}$ rotates the 2-vector by angle $\theta$. RoPE chooses $\theta = \omega_k \cdot p$ for position $p$ and feature-pair index $k$, where ${\omega_k}$ are preset frequencies (e.g. $\omega_k = 10000^{-2k/d}$ as in Vaswani’s sinusoid
github.com
). If $q_p$ and $k_{p'}$ are query and key at positions $p$ and $p'$, after applying these rotations we have $q_p^\text{rot} = R(p)q$ and $k_{p'}^\text{rot} = R(p')k$. Their dot product becomes $q^T [R(p)^T R(p')] k$. Since $R(p)^T = R(-p)$ for a rotation, this simplifies to $q^T R(p'-p) k$, i.e. a rotation by the difference $p'-p$
utorontomist.medium.com
. In other words, the attention score depends on $p'-p$ (relative position) instead of the absolute positions individually. RoPE provides benefits like unbounded sequence length (one can extrapolate beyond training length, since the rotations just continue accumulating phase) and a natural decreasing similarity for tokens farther apart (due to high-frequency components contributing oscillating signs that cancel out
ar5iv.labs.arxiv.org
ar5iv.labs.arxiv.org
). Empirically, RoPE (used in RoFormer
ar5iv.labs.arxiv.org
 and later adopted in models like GPT-NEOX, LLaMA, etc.) has demonstrated faster convergence and better long-range performance than absolute PE baselines
utorontomist.medium.com
. It also supports linear attention, as adding position via multiplication commutes with certain kernel feature mappings
ar5iv.labs.arxiv.org
. Multi-Dimensional Position Encodings: Tasks like vision and reinforcement learning involve 2D or 3D spatial data, for which 1D PE methods are insufficient. A common approach is factorized absolute encodings: e.g. in images, adding a row embedding and a column embedding to each patch or pixel representation. This was used in early Vision Transformers and works but doesn’t capture interactions between axes. Alternatively, relative position bias terms (as in Swin Transformer) are learned for vertical and horizontal offsets, improving parameter efficiency for images
arxiv.org
. Extending RoPE to images and videos has been explored in recent work. Heo et al.
arxiv.org
 investigated 2D RoPE, noting that a direct product of two 1D rotations (one for width, one for height) can be applied. RoPE-Nd (an open-source implementation) generalizes this: for an N-dimensional position $(p_1,\dots,p_N)$, the feature vector is divided into $N$ groups, and within each group pairs of features are rotated by an angle proportional to one of the $N$ coordinates
github.com
. For example, one can dedicate 64 feature-pairs to the height axis, 64 to width, and 64 to time in a video, each using its own frequency spectrum, so that the embedding is the composition of a rotation by $p_{\text{h}}$ in one subspace, by $p_{\text{w}}$ in another, etc.
github.com
. This achieves a form of relative encoding in each dimension independently. Empirical studies on vision tasks show RoPE improves resolution extrapolation (maintaining performance when evaluating on higher-resolution images than trained)
arxiv.org
arxiv.org
. Our work differs by seeking a joint encoding of space and time: rather than separate rotations, we use a single 4D rotor that mixes dimensions. Prior efforts to incorporate physical transformations into neural networks include the use of quaternions for 3D orientation representations and hyperbolic embeddings for hierarchical structures. Quaternions (the algebra $\mathbb{H}$ discovered by Hamilton) can represent 3D rotations compactly and are essentially isomorphic to the rotation bivectors in a 3D Clifford algebra
ir.canterbury.ac.nz
. Clifford or geometric algebras have seen use in physics-informed models (e.g. encoding transformations or equivariances), but to our knowledge have not been applied to transformer positional encoding. Clifford Algebra and Minkowski Space: Clifford algebra generalizes complex numbers and quaternions to higher dimensions by introducing a basis of orthogonal unit vectors with a specified quadratic form. In particular, the Clifford algebra Cl(1,3) (also known as the Spacetime Algebra, STA
en.wikipedia.org
) is a 16-dimensional algebra generated by an orthonormal 4D basis ${\gamma_0,\gamma_1,\gamma_2,\gamma_3}$ with the signature $(+,-,-,-)$ typical of Minkowski spacetime. Products of these basis vectors (e.g. $\gamma_0\gamma_1$) form multivectors that can represent subspaces such as planes. A bivector like $B = \gamma_\mu \gamma_\nu$ represents a 2D plane in spacetime, and the exponential $\exp(\frac{\phi}{2}B)$ corresponds to a rotation in that plane by angle $\phi$. If $B$ involves one timelike and one spacelike direction (say $\gamma_0\gamma_1$), the rotation is a hyperbolic rotation – essentially a Lorentz boost mixing time and space
en.wikipedia.org
. Pure space-space bivectors (like $\gamma_2\gamma_3$) produce ordinary spatial rotations (elliptic rotations). Notably, in Cl(0,3) (Euclidean 3D), the bivectors have the same algebraic structure as quaternions
ir.canterbury.ac.nz
, which means unit quaternions are rotors for 3D rotations. In Cl(1,3), rotors cover the full proper Lorentz group (rotations & boosts). Minkowski’s original insight was to treat time as a fourth dimension such that the laws of physics (and by analogy, perhaps spatio-temporal patterns in data) are invariant under Lorentz transformations. A key consequence is that one can transform between different reference frames via a combination of rotations and boosts without changing the fundamental form of relationships – for example, the speed of light is invariant, and causality (time order for timelike separations) is preserved. Our MonSTER embedding draws inspiration from these concepts: by encoding positions as rotors in Cl(1,3), we equip the model with a representation that naturally respects 3D rotations and temporal boosts. Prior machine learning work using hyper-complex numbers or manifold representations (e.g. hyperbolic neural networks) demonstrated that non-Euclidean geometry can be beneficial for capturing hierarchical or physical structure. MonSTER is the first to apply a Minkowski rotor for a learned representation in a neural network, to our knowledge. It can be seen as bringing geometric deep learning principles to positional encoding – treating symmetry and invariance not as constraints hard-coded in the network, but as emergent properties of how position information is represented and used.
Method
In this section we formalize the MonSTER positional embedding. We begin with a brief recap of rotary embeddings in 1D and 2D to build intuition, then introduce the Clifford algebra formulation for 4D spacetime and derive the rotor encoding for a 4D position. Finally, we describe how these rotors are applied within the transformer self-attention.
1D and 2D Rotary Embeddings Revisited
1D RoPE as Complex Rotation: Let $d$ be the dimensionality of the query/key vectors for one attention head. RoPE requires $d$ to be even; we form $d/2$ complex pairs. For the $k$-th pair (with $0 \le k < d/2$), define a base frequency $\omega_k$ (for example, $\omega_k = 10000^{-2k/d}$ as in RoFormer
github.com
). The position $p$ (an integer index in a sequence) is encoded as a rotation angle $\theta_{k}(p) = \omega_k \cdot p$. The rotary embedding multiplies the $k$-th complex pair of features by $e^{i\theta_k(p)}$. In matrix form, for each pair of real components $(u_{2k}, u_{2k+1})$ of the query vector $q$, we perform:
(
q
2
k
′
q
2
k
+
1
′
)
=
(
cos
⁡
θ
k
−
sin
⁡
θ
k
sin
⁡
θ
k
cos
⁡
θ
k
)
(
q
2
k
q
2
k
+
1
)
,
( 
q 
2k
′
​
 
q 
2k+1
′
​
 
​
 )=( 
cosθ 
k
​
 
sinθ 
k
​
 
​
  
−sinθ 
k
​
 
cosθ 
k
​
 
​
 )( 
q 
2k
​
 
q 
2k+1
​
 
​
 ),
and similarly for each pair of the key vector $k$. This can be written as $q' = R(p),q$, $k' = R(p),k$, where $R(p)$ is a block-diagonal rotation matrix composed of 2×2 rotation blocks (one per frequency). Crucially, the inner product in attention becomes:
q
′
T
k
′
=
q
T
[
R
(
p
)
T
R
(
p
′
)
]
k
=
q
T
R
(
p
)
−
1
R
(
p
′
)
 
k
.
q 
′T
 k 
′
 =q 
T
 [R(p) 
T
 R(p 
′
 )]k=q 
T
 R(p) 
−1
 R(p 
′
 )k.
Since $R(p)^{-1} = R(-p)$ (rotation by negative angle), this equals $q^T R(p'-p), k$. Thus the effect is as if the model is using a relative position embedding $R(\Delta p)$ that rotates the query and key into alignment
utorontomist.medium.com
. If $\Delta p = 0$, the rotations cancel out completely; if $\Delta p$ is large, differences in angles (especially from high $\omega_k$ components) cause $q'^T k'$ to diminish, yielding a learned decay of attention with distance
ar5iv.labs.arxiv.org
. Note this formulation elegantly avoids any direct addition or concatenation of position encodings – position is encoded via phase shifts in the interaction term. Extension to 2D (Spatial) Data: Consider a position on a 2D grid specified by coordinates $(i,j)$ (e.g. row and column indices for an image patch). A simple extension of RoPE is to assign half of the frequency pairs to the $i$ coordinate and half to the $j$ coordinate
github.com
. For instance, if $d = 128$ (64 complex pairs), we could use 32 pairs to encode the row index and 32 for the column index. We then construct two rotation matrices $R_i(i)$ and $R_j(j)$ acting on disjoint subspaces of the feature vector. The overall position rotation is $R_{\text{2D}}(i,j) = R_i(i) \oplus R_j(j)$ (direct sum acting on separate feature subspaces). Then the dot product of two positions $(i,j)$ and $(i',j')$ yields factors $R_i(i)^{-1}R_i(i')$ and $R_j(j)^{-1}R_j(j')$ in the respective subspaces, effectively encoding the relative differences $(i'-i)$ and $(j'-j)$ on each axis. This axial RoPE has been found effective for image Transformers, as it allows attention to be aware of relative vertical and horizontal shifts
arxiv.org
. A limitation, however, is that it treats the two axes independently – the representation of a diagonal displacement $(\Delta i, \Delta j)$ is just the combination of two independent rotations. The model must learn to compose these to recognize, say, a movement along a 45° line. In purely Euclidean 2D space, one could instead encode $(i,j)$ as a single rotation in the plane: for example, interpret $(i,j)$ in polar coordinates $(r,\phi)$ and rotate by $\phi$ in one subspace and use $r$ (or nothing) in another. Prior work typically opts for the simpler axial factorization. 3D and Beyond: In a 3D spatial grid, a factorized approach would use three groups of feature pairs, one per axis (X, Y, Z). This is how RoPE-Nd generalizes by allocating $2N$ features per axis for an $N$-dimensional position
github.com
. While straightforward, this ignores correlations between axes. An alternative is to use higher-dimensional rotation formalisms – e.g. unit quaternions can represent rotations in 3D space as a whole. If we had a 4-dimensional feature (like a single quaternion) per head to represent position, we could encode a 3D point as a 3D rotation of that quaternion from some reference orientation. That quaternion would effectively encode a combined angle and axis (via its vector part) corresponding to the spatial offset. This idea begins to hint at using Clifford algebra: quaternions are essentially a fixed basis representation of rotors in a Euclidean 3-space. For 4D (3D space + time), the appropriate mathematical object is a rotor in Minkowski space, which we detail next.
Clifford Algebra Primer (Cl(1,3) and Rotors)
We work in $\mathbb{R}^{1,3}$, a four-dimensional vector space with one timelike dimension and three spacelike dimensions. We denote a basis as ${e_0, e_1, e_2, e_3}$ where $e_0$ corresponds to the time axis (with metric +1) and $e_{1,2,3}$ to space axes (with metric -1). The Clifford algebra Cl(1,3) is generated by this basis under the multiplication rule:
e
μ
e
ν
+
e
ν
e
μ
=
2
 
η
μ
ν
,
e 
μ
​
 e 
ν
​
 +e 
ν
​
 e 
μ
​
 =2η 
μν
​
 ,
where $\eta_{\mu\nu} = \mathrm{diag}(1,-1,-1,-1)$ is the Minkowski metric. From this, $e_0^2 = +1$ and $e_1^2 = e_2^2 = e_3^2 = -1$, and $e_\mu e_\nu = -e_\nu e_\mu$ for $\mu\neq\nu$. Elements of Cl(1,3) are linear combinations of basis monomials (scalars, vectors $e_\mu$, bivectors $e_\mu e_\nu$, etc.). Of interest to us are bivectors (grade-2 elements), which represent oriented planes. A simple example is $B = e_1 e_2$, which represents the spatial $x$-$y$ plane; $\exp(\frac{\alpha}{2} e_1 e_2)$ is a rotor performing a rotation by angle $\alpha$ in that plane (e.g. yaw rotation in 3D). Another example is $B' = e_0 e_1$, representing the time–$x$ plane. $\exp(\frac{\eta}{2} e_0 e_1)$ is a hyperbolic rotation (boost) along the $x$ axis with rapidity $\eta$ (related to a velocity $v = \tanh \eta$). In general, a rotor $R$ is an element of the form
R
=
exp
⁡
 ⁣
(
 ⁣
−
1
2
B
)
,
R=exp(− 
2
1
​
 B),
where $B$ is some bivector. Rotors satisfy $R \tilde{R} = 1$ (where $\tilde{R}$ is the Clifford conjugate, or reverse, of $R$) and form a group isomorphic to the proper orthochronous Lorentz group $SO^+(1,3)$. Applying a rotor to a vector $v$ is done via the sandwich operation: $v' = R,v,\tilde{R}$, which will be a Lorentz-transformed vector. To connect to familiar concepts: in Euclidean space Cl(0,3), any rotor can be written as $R = \cos(\theta/2) + \sin(\theta/2), \mathbf{n}$, where $\mathbf{n}$ is a unit bivector (e.g. an axis direction like $e_1 e_2$). Identifying $i \leftrightarrow e_1 e_2$, $j \leftrightarrow e_2 e_3$, $k \leftrightarrow e_3 e_1$, this matches the form of a unit quaternion $q = \cos(\theta/2) + (\hat{n}_x i + \hat{n}_y j + \hat{n}_z k)\sin(\theta/2)$. Thus quaternions and spatial rotors are essentially the same (indeed, the bivectors in Cl(0,3) are isomorphic to quaternions)
ir.canterbury.ac.nz
. In Cl(1,3), rotors extend this to also include boosts; one can think of biquaternions (quaternions with hyperbolic components) as an analogy
en.wikipedia.org
. The important takeaway is that by using rotors, we can represent a combined rotation/boost that takes one vector orientation to another in 4D.
4D Spatio-Temporal Rotary Embedding (MonSTER)
We now describe how to construct a rotor that encodes the 4D displacement between two tokens. In a transformer, each token (or each attention head’s query/key for that token) will be associated with a position in space and time. Let’s denote the position of a token as $(t, x, y, z)$, or equivalently as a spacetime vector:
p
=
t
 
e
0
+
x
 
e
1
+
y
 
e
2
+
z
 
e
3
∈
R
1
,
3
.
p=te 
0
​
 +xe 
1
​
 +ye 
2
​
 +ze 
3
​
 ∈R 
1,3
 .
Our goal is to map this vector to a rotor $R(p)$ such that the relative rotor between two positions $p$ and $p'$ encodes the relative displacement $p' - p$ in a geometrically meaningful way. One way to do this is to define $R(p)$ as the rotor that rotates a fixed reference vector (say the time axis $e_0$) to align with the direction of $p$ in spacetime. Geometrically: take $e_0$ (which points purely along the time axis) and “rotate” it until it lies along the direction of the 4-vector $p$. This rotation can be done in two steps:
Spatial rotation ($M_{\text{rot}}$): Rotate the spatial axes so that $e_1$ (the x-axis) aligns with the spatial component of $p$. Let $\mathbf{v} = x,e_1 + y,e_2 + z,e_3$ be the spatial part of $p$. We can find a rotor $M_{\text{rot}}$ (an element of Spin(3) ⊂ Cl(1,3)) that rotates $e_1$ to the direction of the unit vector $\hat{\mathbf{v}} = \mathbf{v}/|\mathbf{v}|$ in the 3D subspace, and also rotates $e_2, e_3$ accordingly (essentially aligning the coordinate frame with the spatial direction of $p$). This $M_{\text{rot}}$ is a Euclidean rotation that depends on the angles $\phi, \psi$ describing the orientation of $(x,y,z)$. If $\mathbf{v}$ is zero, we take $M_{\text{rot}}$ to be the identity.
Spacetime boost ($M_{\text{boost}}$): After the spatial alignment, $p$ lies in the plane spanned by $e_0$ and the aligned $e_1$. Now we need to rotate $e_0$ into $p$. This is a rotation in the $e_0$-$e_1$ plane, which is a hyperbolic rotation (boost). Define $u = |\mathbf{v}|$ (spatial magnitude) and let $w = p \cdot e_0 = t$ (the time component). We want a rotor $M_{\text{boost}}$ of the form $\exp(-\frac{\eta}{2} e_0 e_1)$ that takes $e_0$ to a direction proportional to $w,e_0 + u,e_1$. In relativistic terms, if $p$ is timelike ($w > u$), we can define $\tanh \eta = \frac{u}{w}$, so that $\cosh \eta = \frac{w}{L}$ and $\sinh \eta = \frac{u}{L}$ where $L = \sqrt{w^2 - u^2}$ (the Minkowski norm of $p$). Then $M_{\text{boost}}$ applied to $e_0$ yields $\cosh\frac{\eta}{2},e_0 + \sinh\frac{\eta}{2},e_1$, and applied twice (sandwich) yields $\cosh \eta, e_0 + \sinh \eta, e_1 = \frac{w}{L}e_0 + \frac{u}{L}e_1$, which is exactly the unit vector in the direction of $p$. In other words, $M_{\text{boost}}$ completes the rotation of $e_0$ into $p$’s direction. If $p$ is spacelike ($u > w$), the procedure is similar but yields a rotation of $e_0$ partly into imaginary angle – effectively $p$ lies outside the light cone and the boost angle $\eta$ would be taken as $\tanh \eta = w/u$. Regardless, $M_{\text{boost}}$ can be defined for all $p$. If $p=0$, we again take identity.
The composition $R(p) = M_{\text{boost}}(p); M_{\text{rot}}(p)$ is a rotor that transforms $e_0$ to the direction of $p$. In code (using a Clifford algebra library), this might look like:
rotor = M_rot_b   # spatial rotor (bivector) for orientation
boost = M_boost_b # boost rotor (bivector) for time alignment
R_eff = boost @ rotor
which first rotates the frame with rotor, then applies the boost, yielding the effective rotor R_eff for position $p$. (The order is important: in the sandwich operation $v' = R v \tilde{R}$, if we want $e_0$ to go to $p$, we must apply spatial alignment then boost in that new frame.) Applying to Transformer: Now that we can derive a rotor for any position $p$, how do we use it in the model? The approach is analogous to RoPE: we will rotate the query and key vectors by the rotor. A query vector $q$ (which lives in the model’s real vector space of dimension $d$) needs to be somehow lifted or associated to a multivector in Cl(1,3) so that the rotor can act on it. There are a few ways to implement this:
We can represent $q$ in a higher-dimensional space isomorphic to the space of multivectors. Cl(1,3) has dimension 16 (1 scalar + 4 vectors + 6 bivectors + 4 trivectors + 1 pseudoscalar). Using the full 16-dim algebra might be overkill; we may restrict to using just certain grades (for example, represent $q$ as a trivector or as a pair of 4-vectors). For simplicity, one practical strategy is to use two copies of $\mathbb{R}^{4}$ (for the 4 basis vectors and 4 dual basis trivectors) giving an 8-dimensional representation that can capture rotations. Indeed, rotors can be represented as 4×4 matrices acting on the vector space, or as 8×8 real matrices acting on a carefully chosen subspace of multivectors (similar to how complex rotations were represented as 2×2 real matrices). In our implementation, we divide the $d$ features of each head into groups of 4 (assuming $d$ is a multiple of 4). Each 4 corresponds to a “spacetime vector” component of the query/key. The rotor $R(p)$ is applied to each 4-dim group by matrix multiplication.
Formally, if $q = [q^{(1)}, q^{(2)}, ..., q^{(m)}]$ where each $q^{(j)} \in \mathbb{R}^4$ and $d = 4m$, then we define $q' = [R(p) q^{(1)},; R(p) q^{(2)},;...;]$. Here $R(p) q^{(j)}$ means to interpret $q^{(j)}$ as a vector $q^{(j)\mu} e_\mu$ in the 4D basis and perform the sandwich $R(p), q^{(j)} \tilde{R(p)}$, which effectively is a linear transform of the 4 components. In practice, we can use the $4\times4$ matrix representation of the rotor on vectors
en.wikipedia.org
 to multiply the 4-vector $q^{(j)}$. The result $q'$ is the position-rotated query. We do the same for the key $k$.
Once each query $q_i$ at position $p_i$ is transformed to $q_i'$, and each key $k_j$ at $p_j$ to $k_j'$, the attention score involves the inner product $q_i' \cdot k_j'$. Because each 4-dim sub-block was transformed by $R(p_i)$ and $R(p_j)$ respectively, this inner product will implicitly include the effect of the composite rotor $R(p_i)^{-1} R(p_j)$ across those subspaces. In fact, if we had a single 4-d subspace (i.e. $m=1$), one can show:
q
i
′
⋅
k
j
′
=
(
R
(
p
i
)
q
)
⋅
(
R
(
p
j
)
k
)
=
(
R
(
p
i
)
~
R
(
p
j
)
q
)
⋅
k
=
(
R
(
p
j
)
R
(
p
i
)
~
q
)
⋅
k
,
q 
i
′
​
 ⋅k 
j
′
​
 =(R(p 
i
​
 )q)⋅(R(p 
j
​
 )k)=( 
R(p 
i
​
 )
~
​
 R(p 
j
​
 )q)⋅k=(R(p 
j
​
 ) 
R(p 
i
​
 )
~
​
 q)⋅k,
which equals $q \cdot [\tilde{R(p_i)} R(p_j) k]$. The term $\tilde{R(p_i)} R(p_j)$ is itself a rotor corresponding to the difference between $p_i$ and $p_j$. In other words, attention depends on $p_j - p_i$ (relative displacement) in a way encoded by that relative rotor. This is the desired property: just as RoPE yields an attention phase equal to $\Delta \theta$ in 1D
utorontomist.medium.com
, MonSTER yields an attention transformation equal to the rotor for the 4D offset $\Delta p$. Intuitively, if two tokens are separated primarily in time (large $\Delta t$) but not space, the relative rotor is a boost, which might cause attention to decay in a way analogous to how large time differences reduce correlation in temporal sequences. If two tokens are separated in space, the relative rotor is a spatial rotation, which the model can learn to interpret as, say, a spatial shift pattern. If they are separated in both, the rotor mixes time and space differences. Frequency scaling: Just like 1D RoPE benefits from using multiple frequencies (the various $\omega_k$) to capture both small and large positional differences, we can extend MonSTER with multiple “frequency scales.” In practice, this can be done by assigning multiple bivectors for each independent 4D subspace. For example, one could use a fast rotation in one 4-d block and a slower rotation in another 4-d block for the same physical axis. A concrete way is to have $\omega_k$ factors on the boost parameter and rotation angles. In our implementation, we assign a set of frequency multipliers for the boost and for each of the two needed spatial rotation angles, ensuring that some subspaces encode fine positional detail and others coarse. This yields a spectrum of rotor magnitudes akin to the Fourier feature spectrum in NeRF
cseweb.ucsd.edu
. By superposing these, the model can learn distance-dependent effects that decay smoothly. Computational Complexity: Encoding a position with MonSTER involves constructing the $4\times 4$ rotor matrix (or directly applying the rotor to basis vectors). This can be done in $O(4^3)$ operations for matrix exponentials, but since we have closed-form expressions (sines, cosines, hyperbolic sines, etc.), it simplifies. Each query/key feature subspace then undergoes a 4x4 matrix-vector multiply (16 multiplications). If $d$ is large with many such subspaces, this overhead is linear in $d$. Compared to 1D RoPE’s cost (which is a series of 2x2 rotations per pair), MonSTER’s cost is roughly 4× larger per group of four features. For moderate $d$ (e.g. $d=64$ or 128 per head), this is very manageable. In our experiments, we find the positional encoding time is negligible compared to attention and feedforward costs. MonSTER adds no learned parameters; it’s a deterministic function of position like sinusoidal and RoPE encodings.
Properties of MonSTER Encoding
MonSTER inherits and extends many desirable properties of RoPE:
Relative and Directional Encoding: The attention mechanism depends only on relative differences in spacetime. The encoding of a displacement $(\Delta t,\Delta x,\Delta y,\Delta z)$ is handled by the algebra of rotors, which naturally accounts for direction. Notably, if $\Delta x,\Delta y,\Delta z$ is rotated (in the sense of coordinate frame rotation), the relative rotor changes accordingly – the model sees a consistent transformation. In contrast, axis-wise encodings would change in a more entangled way if the coordinate system is rotated.
Extrapolation: Because MonSTER uses continuous functions (trig and hyperbolic trig) of the coordinates, there is no inherent limit to the positions it can represent. A model trained on a certain range of times or spatial sizes can generalize beyond that, similarly to how RoPE allowed extrapolation to longer sequences
ar5iv.labs.arxiv.org
. In particular, using multiple frequency scales ensures that beyond the training range, there will be high-frequency components that eventually make attention scores small for far apart tokens, preventing attention from erroneously growing or oscillating unpredictably.
Decay of Influence with Distance: By design, a large separations in time or space will typically result in rotors that significantly rotate the query relative to the key features. Unless the content vectors $q,k$ are extremely aligned along an eigen-direction of that rotor, the dot product will decrease. For example, a purely temporal separation $\Delta t$ yields a boost rotor $\exp(-\frac{\eta}{2} e_0e_1)$ which for large $\eta$ (i.e. $\Delta t \gg \Delta x$) has $\cosh\eta \approx \frac{\Delta t}{L}$ large and $\sinh\eta \approx \frac{\Delta x}{L}$ maybe moderate – this effectively can amplify certain components, but the key is that we would use multiple boost frequencies to ensure that at least some subspaces respond with oscillatory behavior to large $\Delta t$. Thus the model can learn to ignore vastly separated tokens (e.g. frames far apart in a video) if appropriate.
Transformation Consistency: If we apply the same physical transformation to an entire sequence of tokens (e.g. translating all spatial positions by a fixed offset, or shifting the time origin), what happens to the relative rotors? For spatial translation, in Euclidean space absolute coordinate encodings would all change, but relative differences remain the same – MonSTER naturally only cares about differences, so it’s translation-invariant. For rotations of the spatial frame, if we rotate all positions by a matrix $O\in SO(3)$, then each position’s rotor $R(p)$ is effectively multiplied by the rotor corresponding to $O$. Since attention uses $\tilde{R(p_i)}R(p_j)$, those additional $O$ factors cancel out. Thus the entire attention pattern is invariant to global rotations of the input coordinate frame. This is a form of equivariance: the model doesn’t have to re-learn the same pattern for data oriented differently. Similarly, adding a constant to all time coordinates (a shift in start time) cancels out in differences, giving time-translation invariance.
Experiments
We plan a series of experiments to evaluate MonSTER against baseline positional encodings across different domains. As this is a new positional embedding method, our focus is on examining generalization and consistency rather than solely beating state-of-the-art accuracy. Below we outline the tasks, baselines, and ablation studies.
1. Gridworld Reasoning Tasks (ARC-Like Puzzles)
Setup: We create a suite of synthetic tasks inspired by the Abstraction and Reasoning Corpus (ARC)
pgpbpadilla.github.io
, which test a model’s ability to infer spatial transformations from few examples. Each task is a small “gridworld” scenario where an input configuration of colored cells must be transformed to an output configuration. We represent the input and output grids as sequences of tokens (e.g. scanning row-wise) so that a transformer can process them. Crucially, solving the task requires understanding spatial relations (e.g. symmetry, adjacency, movement of an object). For example, one puzzle might show a shape in the input and require the model to output the shape reflected across a vertical axis. Another puzzle might have an object that moves in a consistent direction and the model must predict its next position. Model: We use a small transformer (e.g. 4 layers, 8 heads) to map the input sequence to an output sequence, training it to produce the correct transformed grid. We compare three position encoding schemes: (a) MonSTER 4D – here $t$ corresponds to a time step in a multi-step reasoning sequence (if applicable) or simply a dummy time (we can treat the input and output as two time steps), and $(x,y)$ are the grid coordinates of each cell token; (b) Axial RoPE (baseline) – we apply 2D RoPE via independent rotations for $x$ and $y$ as described earlier
github.com
, and no time dimension (or time treated same as an extra axis if sequential multi-step reasoning is needed); (c) Absolute + Relative Baseline – we try a traditional approach: learned absolute 2D embeddings (one for row, one for column added to token features) and possibly learned relative bias for adjacency as in prior work
arxiv.org
. Metrics: We evaluate puzzle-solving accuracy on held-out tasks and specifically test generalization: for instance, training on $5\times5$ grids and testing on $7\times7$ (to see if the model extrapolates to larger grids), or training on certain orientations of transformations and testing on rotated orientations. We expect MonSTER’s rotational invariance to yield higher generalization in scenarios where test puzzles are rotated versions of train puzzles. We also measure sample efficiency (how many tasks or examples per task are needed to reach a certain accuracy). MonSTER, by encoding more prior structure, might solve tasks with fewer examples. Ablation: Within this, we try turning off the “boost” (time) component of MonSTER to see if it affects tasks where time is irrelevant (it shouldn’t). We also test a Euclidean rotor variant: using Cl(0,4) where time and space are all treated as spatial dimensions (effectively a 4D rotation without hyperbolic boost). This helps confirm the importance of Minkowski structure if any difference is observed (though in static puzzles, time is not in play, so Cl(0,4) vs Cl(1,3) might behave similarly).
2. Video Classification
Setup: Next, we evaluate on a small video understanding task. We use a dataset of short video clips with an associated label (for example, a trimmed action recognition dataset). To keep it manageable, we might use something like UCF101 (which has 3-5 second sports videos), or a smaller synthetic video dataset (e.g. moving shapes with different dynamics). Each video can be treated as a sequence of frame patches (as in a Video Transformer or TimeSformer model). We focus on whether MonSTER helps the model be robust to changes in temporal or spatial resolution. Model: We implement a vision transformer that processes video frames as a sequence of patches. We compare: (a) MonSTER 4D: Here each patch is assigned a 4D coordinate $(t,x,y,z)$ – where $t$ is frame index (time), $(x,y)$ is patch’s row-col position in the frame, and $z$ could optionally index depth if using 3D conv features (or we set $z=0$ since this is essentially 3D data, 1 time + 2 space). We encode each patch’s positional rotor and apply it to that patch token’s features. (b) 3D RoPE: A baseline using 3D rotary encoding as per rope-nd, i.e. independent rotations for time, height, width. (c) Sinusoidal Absolute: baseline with classic sinusoidal encodings for frame and spatial position added (one sin/cos for time, and one for 2D position either flattened or summed). We train each model on training videos and evaluate on test. Evaluation: We check classification accuracy, but more interestingly, we test extrapolation and invariance. For extrapolation, we might train the model on videos of a certain length (say 8 frames) and test on longer videos (12 or 16 frames) by evenly sampling patches – MonSTER’s encoding should in theory handle the longer sequence without issue, whereas absolute encodings might struggle or require interpolation. For invariance, we can augment the test videos by rotating them or flipping horizontally. Since MonSTER encodes orientation explicitly, we expect the model to be relatively unaffected by rotations (in fact, a global rotation of the entire frame corresponds to pre-multiplying all patch rotors by the same spatial rotation, which as argued should cancel out in relative terms). We measure how much accuracy drops (if at all) when videos are rotated or if the frame rate is changed (e.g. skipping frames – which changes $\Delta t$ between content). A model with MonSTER should be more stable to these changes, as it “understands” time and space in a relative way. Ablation: We examine attention patterns to see if the model learns to attend more broadly in space/time with MonSTER. For example, do early layers group patches by proximity in spacetime (which would show up as block structure in attention matrix)? We also test a variant of MonSTER with only a single frequency versus multiple frequency bands, to verify that multi-scale encoding improves performance on this realistic data. Additionally, we might try replacing Minkowski metric with Euclidean in MonSTER (i.e. use a 4D quaternion-like rotation) to see if the boost aspect (treating time differently) yields any difference in capturing temporal order. We hypothesize that treating time as timelike (hyperbolic rotations) might allow the model to represent larger time differences more effectively (or at least not treat them cyclically, which a purely rotational encoding might).
3. Structured Sequence Modeling
Setup: Lastly, we consider a purely algorithmic or structured task to see if MonSTER can help in non-visual domains. One example is pathfinding or navigation sequences: we generate sequences of moves (N, S, E, W) that an agent takes on a grid, and ask the model to predict the final location or some property of the path. Another example is representing abstract syntax trees (ASTs) of code as a flattened sequence with bracket indicators for tree structure. In such cases, there is an underlying graph or geometric structure to the sequence beyond linear order. Model: We use a transformer to encode the sequence and produce an output (e.g. classification of end location, or next token prediction). We compare (a) MonSTER where we assign each sequence element a coordinate reflecting its true structure (e.g. for pathfinding, we use $(t, x, y)$ where $(x,y)$ is the agent’s location at step $t$; for an AST sequence, we could use a coordinate like $(\text{depth}, \text{position_in_sibling_order})$ as spatial coords and the linear index as time), (b) standard 1D position encoding (absolute or RoPE) that just knows the linear index, and (c) a learned graph-aware encoding (if applicable, e.g. in AST one could input parent pointers but that complicates transformer architecture). The key is to see if MonSTER can implicitly make the transformer aware of the hidden structure. Evaluation: We measure performance on tasks like predicting the correct endpoint of a path after reading the moves. We suspect that a model with only linear positions might struggle to compute 2D location (it would have to learn to simulate addition of vectors via the moves). MonSTER provides an easier route: the spatial rotor part can encode the agent’s displacement at each step. The relative rotor between step 0 (start) and step $t$ essentially encodes the net displacement after $t$ moves. The model’s attention could directly compare these to figure out where it is. We will test generalization to longer sequences and larger grids. For AST tasks, we’ll check if the model better captures long-range dependencies (matching brackets, etc.) when given structural coordinates. Ablations: One ablation is to test MonSTER in a “wrong geometry” setting: feed it coordinates that do not correspond to the actual task structure and see if it still helps (this checks that it’s indeed exploiting the geometry, not just adding capacity). Another ablation is to vary the Minkowski signature: e.g. treat depth in AST as timelike vs spacelike and see if that affects anything (likely not critical here, but interesting to see if making certain dimensions hyperbolic influences learning of hierarchy). We also test using fewer or more feature dimensions for MonSTER to gauge how much positional capacity is needed for complex structures.
Baseline Comparison and Metrics
Across all experiments, the primary baselines – Multi-dimensional RoPE and Fourier-feature (NeRF) encoding – are chosen because they also offer relative or continuous encodings:
Multi-d RoPE (Axial): This baseline handles each dimension separately with rotations
github.com
. It should perform well when axes are independent, but might struggle if the relationship between axes matters (e.g. diagonal movements or combining time and space).
NeRF Fourier Encoding: We implement this as encoding each coordinate with a bank of sine/cosine at different frequencies
cseweb.ucsd.edu
, and concatenating them. For example, a 4D coordinate $(t,x,y,z)$ would produce a feature vector: $\big[\sin(2^k t), \cos(2^k t),; \sin(2^k x),\cos(2^k x),; ...\big]$ for $k=0\ldots L-1$. This is akin to treating each coordinate independently as well, but without the nice relative property – it’s an absolute encoding (although a continuous one). We expect this to do fine when extrapolating (Fourier features can generalize) but it won’t automatically provide a notion of relative difference unless the model learns to subtract phases.
We use metrics appropriate to each task (accuracy for classification, IOU for puzzles, etc.), but also emphasize generalization tests (performance on configurations not seen in training) and robustness tests (e.g. invariance under rotations as described).
Discussion
Our anticipated results will shed light on the practical strengths and limitations of MonSTER. We expect to see improvements in generalization on tasks where spatial or temporal extrapolation is required. For instance, in the grid puzzles, MonSTER should enable the transformer to solve puzzles on larger grids or with rotated patterns that stymie a model with naive encodings. This would confirm that encoding 4D structure helps the model learn the rule behind the pattern rather than overfitting to specific positions. In the video domain, if MonSTER indeed provides rotation-equivariant attention, the model’s accuracy on rotated or mirrored videos might remain high, whereas a baseline might drop. This kind of robustness is extremely desirable in real-world applications, as it indicates the model understands the event (e.g. “a person throwing a ball”) regardless of camera orientation or when exactly it happens in the clip. We also expect MonSTER to handle variable sequence lengths more gracefully – for example, longer videos or paths – due to its relative nature. One possible challenge is the increased complexity of MonSTER relative to simple encodings. While it introduces no learned parameters, the additional computation and integration of the Clifford algebra operations could complicate the training dynamics. If not properly tuned, the model might initially struggle to use the 4D rotors effectively. In preliminary runs, we noticed that initializing the scale of frequencies (analogous to how you might choose 10000 in 1D sinusoids) is important: too large frequencies and the rotations become effectively random for moderate distances, making learning hard; too small and the model can’t distinguish positions well. We found choosing frequency scales such that a significant fraction of the feature subspaces complete a full rotation over the range of positions in the data works well (similar to setting the 1D sinusoid frequency spectrum to span the input length). Another observation is that MonSTER could be overly expressive – it can encode transformations that might not be needed. For example, it could represent boosts that change simultaneity of events, which in a strictly sequential dataset might introduce degeneracy. We mitigate this by normalization (ensuring we don’t produce unphysical large rapidities for small time differences by bounding $\eta$). Scalability: MonSTER’s approach is general, but how does it scale to high-dimensional data or very long sequences? The rotor composition will involve more basis bivectors as dimensionality grows. For example, a truly joint space-time-frequency 5D encoding would need even more complex rotors. There may be diminishing returns: beyond 4D or 5D, factorized encodings could be easier if those extra dimensions are mostly independent. Our framework can theoretically handle it, but efficiency and clarity might suffer. For very long sequences (say video with hundreds of frames), MonSTER remains linear in sequence length per attention computation (no extra asymptotic cost), and should extrapolate. But extremely large $\Delta t$ might push the limits of floating point for cosh/sinh, an engineering detail that can be handled by scaling down units (we could measure time in a smaller unit, or periodically wrap around phases beyond a certain point if needed). Compatibility: MonSTER can be seen as a drop-in replacement for position encoding in any transformer architecture. It doesn’t assume anything about the attention mechanism beyond needing to multiply queries/keys by a known matrix. This means it could be used in encoder-decoder models, autoregressive models (where you encode relative positions among generated tokens), etc. One interesting avenue is combining MonSTER with learned position representations: e.g. use MonSTER as a physics-inspired prior and allow the model to adjust or add a learned bias. We could also consider partial MonSTER – for example, only use it to encode spatial info but use traditional encoding for time if a domain has a different temporal structure. Multimodal Fusion: Because MonSTER encodes any modality that has space/time coordinates, it could facilitate fusion of modalities. Imagine a multi-stream model that processes video frames and text descriptions together. Using a common time axis (so that the text tokens and video frames at the same time share some positional alignment) and a notional space (perhaps the text doesn’t have a real space, but we could assign an arbitrary space coordinate or treat it as a separate space), MonSTER might allow attention between modalities that respects temporal alignment. Similarly, a point cloud or graph with known geometry could be integrated by giving those tokens spatial coordinates. The unified rotor formalism means all tokens, regardless of type, can be compared on equal footing if we embed them in the same spacetime coordinate system. Physical Interpretability: An intriguing aspect of MonSTER is that the attention weights have a physical interpretation: a high attention between two tokens implies that the relative rotor between them aligns the query and key vectors well. This could correspond to, say, two events being close in spacetime or related by a symmetry. We can potentially interpret the learned attention patterns in terms of trajectories or flows in spacetime. For instance, in a motion tracking scenario, the model might learn to attend from an object at time $t$ to the same object at time $t+\Delta t$ that moved to a new location, because the rotor encoding “predicted” that movement. Such interpretability could be leveraged to enforce constraints (we could regularize the model to prefer smaller boosts for causal consistency, etc.). On the downside, MonSTER assumes a Minkowski structure which might not strictly hold in data. For example, in video understanding, time and space are not exactly related by $c=1$ (we are free to choose units, but there’s no real speed-of-light constant in the data). The model has to learn an appropriate weighting between time differences and space differences. Minkowski treats a time unit as fundamentally different from a space unit – one can’t directly compare them without a conversion factor. In our implementation, we introduce a hyperparameter for time-scale vs space-scale (essentially, what $\Delta t$ is considered equivalent to a certain $\Delta x$ in importance). We will tune this so that typical velocities in the data correspond to moderately small rapidities. In essence, MonSTER provides a framework, but it doesn’t automatically know the “speed of information” in the data; that must be learned or set. Fortunately, since our encoding is purely relative, an incorrect scale would mostly affect how quickly attention decays with time vs with space, which the model can adjust by re-weighting content relevance. Comparison to Graph Encodings: Another related area is positional encodings in graphs (which often involve Laplacian eigenvectors or random walk features as continuous encodings of position in a graph). One could see MonSTER as encoding a particular graph – the 4D grid of spacetime. For domains that inherently live on such a grid (e.g. videos on a spatial grid, or traffic networks on a map), MonSTER might be extremely well-suited. If the structure is a general graph, different techniques are needed, but maybe some analogy of rotors on manifolds could be explored. In summary, MonSTER pushes the boundary of positional encoding by embedding domain knowledge of spacetime geometry into the attention mechanism. The empirical results we expect will highlight its ability to generalize and maintain performance under transformations. While it introduces a bit more complexity, we believe this is a fruitful direction for making transformers that truly understand space and time, rather than just treating them as extra token indices.
Conclusion
We presented MonSTER, a 4D Clifford rotary embedding that enables transformers to natively encode and reason about spatio-temporal relationships. By generalizing RoPE from complex rotations (1D) to Minkowski rotors (4D), MonSTER integrates time and space into a unified positional encoding. This approach preserves relative encoding advantages – sequence length flexibility and meaningful distance-dependent attention – while adding invariances to rotations and boosts that reflect physical symmetries. Through planned experiments on puzzle reasoning, video classification, and structured sequences, we will demonstrate that MonSTER can improve generalization to novel orientations, longer sequences, and unseen spatial configurations compared to traditional position encodings. The use of Clifford algebra, although uncommon in deep learning, offers a powerful language for injecting geometric priors. MonSTER opens up new possibilities for multimodal models that operate in the real world’s space and time: from robots that must understand spatial layouts and dynamics, to language models grounding text in physical context, to scientific models encoding 4D systems. There is ample room to build on this work – future directions include training larger-scale models with MonSTER on complex tasks (e.g. video question answering, 4D medical image analysis), exploring other signatures (like hyperbolic space for hierarchical data), and optimizing the implementation of Clifford operations on accelerators. We hope that our approach inspires further synergy between geometric methods and transformers, bringing the rich heritage of mathematical physics to bear on machine intelligence.
References
Vaswani, A., Shazeer, N., Parmar, N., et al. (2017). Attention is All You Need. In Advances in Neural Information Processing Systems 30.
ar5iv.labs.arxiv.org
Devlin, J., Chang, M.-W., Lee, K., & Toutanova, K. (2019). BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. In NAACL-HLT.
ar5iv.labs.arxiv.org
Shaw, P., Uszkoreit, J., & Vaswani, A. (2018). Self-Attention with Relative Position Representations. In NAACL-HLT.
ar5iv.labs.arxiv.org
Su, J., Lu, Y., Pan, S., et al. (2021). RoFormer: Enhanced Transformer with Rotary Position Embedding. arXiv:2104.09864.
ar5iv.labs.arxiv.org
ar5iv.labs.arxiv.org
EleutherAI (2021). Rotary Embeddings: A Relative Revolution. EleutherAI Blog. Retrieved from blog.eleuther.ai.
utorontomist.medium.com
utorontomist.medium.com
Heo, B., Park, S., Han, D., & Yun, S. (2024). Rotary Position Embedding for Vision Transformer. arXiv:2403.13298.
arxiv.org
GitHub - limefax (2023). RoPE-Nd: N-dimensional Rotary Position Embeddings. [Online Code Repository].
github.com
github.com
Mildenhall, B., Srinivasan, P.P., Tancik, M., et al. (2020). NeRF: Representing Scenes as Neural Radiance Fields for View Synthesis. ECCV. (Positional encoding details in Appendix)
cseweb.ucsd.edu
Chollet, F. (2019). On the Measure of Intelligence. arXiv:1911.01547. (Introduces the ARC dataset for abstract reasoning)
pgpbpadilla.github.io
Doran, C., & Lasenby, A. (2003). Geometric Algebra for Physicists. Cambridge University Press. (Introduction to spacetime algebra Cl(1,3) and rotors)
ir.canterbury.ac.nz
en.wikipedia.org
Hamilton, W.R. (1844). On Quaternions. Royal Irish Academy. (Historical paper introducing quaternions for 3D rotation)
ir.canterbury.ac.nz
Wikipedia (2023). Spacetime algebra. [Online]. (Background on Lorentz rotors and bivectors in Cl(1,3))
en.wikipedia.org
en.wikipedia.org
Citations
Favicon
[2104.09864] RoFormer: Enhanced Transformer with Rotary Position Embedding

https://ar5iv.labs.arxiv.org/html/2104.09864
Favicon
Rotary Position Embedding for Vision Transformer

https://arxiv.org/html/2403.13298v1
Favicon
[2104.09864] RoFormer: Enhanced Transformer with Rotary Position Embedding

https://ar5iv.labs.arxiv.org/html/2104.09864
Favicon
[2104.09864] RoFormer: Enhanced Transformer with Rotary Position Embedding

https://ar5iv.labs.arxiv.org/html/2104.09864
Favicon
[2104.09864] RoFormer: Enhanced Transformer with Rotary Position Embedding

https://ar5iv.labs.arxiv.org/html/2104.09864
Favicon
[2104.09864] RoFormer: Enhanced Transformer with Rotary Position Embedding

https://ar5iv.labs.arxiv.org/html/2104.09864
Favicon
Rotating The Way We View Position Embeddings | by University of Toronto Machine Intelligence Team | Medium

https://utorontomist.medium.com/rotating-the-way-we-view-position-embeddings-8a5aebc9ee1
Favicon
[2104.09864] RoFormer: Enhanced Transformer with Rotary Position Embedding

https://ar5iv.labs.arxiv.org/html/2104.09864
Favicon
[2104.09864] RoFormer: Enhanced Transformer with Rotary Position Embedding

https://ar5iv.labs.arxiv.org/html/2104.09864
Favicon
[2104.09864] RoFormer: Enhanced Transformer with Rotary Position Embedding

https://ar5iv.labs.arxiv.org/html/2104.09864
Favicon
Rotating The Way We View Position Embeddings | by University of Toronto Machine Intelligence Team | Medium

https://utorontomist.medium.com/rotating-the-way-we-view-position-embeddings-8a5aebc9ee1
Favicon
Rotating The Way We View Position Embeddings | by University of Toronto Machine Intelligence Team | Medium

https://utorontomist.medium.com/rotating-the-way-we-view-position-embeddings-8a5aebc9ee1
Favicon
[2104.09864] RoFormer: Enhanced Transformer with Rotary Position Embedding

https://ar5iv.labs.arxiv.org/html/2104.09864
Favicon
[2104.09864] RoFormer: Enhanced Transformer with Rotary Position Embedding

https://ar5iv.labs.arxiv.org/html/2104.09864
Favicon
[2104.09864] RoFormer: Enhanced Transformer with Rotary Position Embedding

https://ar5iv.labs.arxiv.org/html/2104.09864
Favicon
Rotary Position Embedding for Vision Transformer

https://arxiv.org/html/2403.13298v1
Favicon
GitHub - limefax/rope-nd: N-dimensional Rotary Position Embeddings for PyTorch

https://github.com/limefax/rope-nd
Favicon
Rotary Positional Embeddings: A Detailed Look and ... - Medium

https://medium.com/ai-insights-cobet/rotary-positional-embeddings-a-detailed-look-and-comprehensive-understanding-4ff66a874d83
Favicon
GitHub - limefax/rope-nd: N-dimensional Rotary Position Embeddings for PyTorch

https://github.com/limefax/rope-nd
Favicon
Spacetime algebra - Wikipedia

https://en.wikipedia.org/wiki/Spacetime_algebra
The Abstraction and Reasoning Challenge (ARC)

https://pgpbpadilla.github.io/chollet-arc-challenge
Favicon
https://ir.canterbury.ac.nz/bitstream/10092/2581/1/Thesis_fulltext.pdf
Favicon
https://cseweb.ucsd.edu/~ravir/icbs23.pdf
Favicon
GitHub - limefax/rope-nd: N-dimensional Rotary Position Embeddings for PyTorch

https://github.com/limefax/rope-nd
Favicon
[2104.09864] RoFormer: Enhanced Transformer with Rotary Position Embedding

https://ar5iv.labs.arxiv.org/html/2104.09864
Favicon
[2104.09864] RoFormer: Enhanced Transformer with Rotary Position Embedding

https://ar5iv.labs.arxiv.org/html/2104.09864
Favicon
Rotating The Way We View Position Embeddings | by University of Toronto Machine Intelligence Team | Medium

https://utorontomist.medium.com/rotating-the-way-we-view-position-embeddings-8a5aebc9ee1
Favicon
Rotary Position Embedding for Vision Transformer

https://arxiv.org/html/2403.13298v1
Favicon
GitHub - limefax/rope-nd: N-dimensional Rotary Position Embeddings for PyTorch

https://github.com/limefax/rope-nd
Favicon
Rotary Position Embedding for Vision Transformer

https://arxiv.org/html/2403.13298v1
Favicon
Rotary Position Embedding for Vision Transformer

https://arxiv.org/html/2403.13298v1
Favicon
Spacetime algebra - Wikipedia

https://en.wikipedia.org/wiki/Spacetime_algebra
Favicon
GitHub - limefax/rope-nd: N-dimensional Rotary Position Embeddings for PyTorch

https://github.com/limefax/rope-nd
Favicon
Spacetime algebra - Wikipedia

https://en.wikipedia.org/wiki/Spacetime_algebra
Favicon
Spacetime algebra - Wikipedia

https://en.wikipedia.org/wiki/Spacetime_algebra
Favicon
[2104.09864] RoFormer: Enhanced Transformer with Rotary Position Embedding

https://ar5iv.labs.arxiv.org/html/2104.09864
Favicon
https://ir.canterbury.ac.nz/bitstream/10092/2581/1/Thesis_fulltext.pdf
All Sources
Faviconar5iv.labs.arxiv
Faviconarxiv
Faviconutorontomist.medium
Favicongithub
Faviconmedium
Faviconen.wikipedia
pgpbpadilla.github
Faviconir.canterbury.ac
Faviconcseweb.ucsd