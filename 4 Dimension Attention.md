# MonSTERs and Their Unextolled Virtues
## Structural Embeddings for Native Space-Time Intelligence
**Jacob Peterson**  
*peterj29@oregonstate.edu*

---

### Abstract

large language models and other transformer-based architectures have notech numerous cognitive benchmarks, large language models and other transformer-based architectures have consistently fallen short in tasks involving spatial reasoning and perception such as tic-tac-toe, ? The ARC Challenge, Sudoku, . The root of this persistent limitation is architectural: modern transformers inherently encode information as linear, strictly one-dimensional sequences.

Overcoming this constraint requires embeddings that move beyond purely temporal indexing. Instead, embeddings must inherently encode the intrinsic relationships between space and time. By generalizing two-dimensional Rotary Positional Embeddings (RoPE) to four-dimensional Minkowski Space-Time Embedding Rotors (MonSTERs), the transformer's artificial flattening requirement, which necessarily discards vital geometric and relational information[^3], is eleminated. These structural—not positional—embeddings remove the blinders that previously constrained transformers' capacity to handle inherently multidimensional information.

Simply put, MonSTERs provide Transformers with a built-in ability to simultaneously perceive, reason about, and generalize across both spatial structures and temporal sequences—without sacrificing their established computational advantages.

---

## 01 Introduction

> “There was a guy who was a great wingshot on a quail hunt in Georgia. He killed everything he saw, he dropped ’em all morning. One of the other guys said, ‘You’re the best wingshot I’ve ever seen.’ At lunch the guy asked him, ‘Do you shoot with one eye open or both?’ He paused and thought about it. Finally, he said, ‘I don’t know.’”
> — Cormac McCarthy, quoted in *The New Yorker*, April 22, 2017[^4]



---



[^1]: **Andrej Karpathy**, “*Jagged Intelligence*,” X (formerly Twitter), May 8 2025, thread starting at <https://x.com/karpathy/status/1816531576228053133>.  
     > “Jagged Intelligence: the (strange, unintuitive) fact that state‑of‑the‑art LLMs can both perform extremely impressive tasks … while simultaneously struggling with very dumb problems. … Use LLMs for the tasks they are good at but be on the lookout for jagged edges, and keep a human in the loop.”

[^2]: **Hans P. Moravec**, *Mind Children: The Future of Robot and Human Intelligence* (Cambridge, MA: Harvard University Press, 1988), 15.  
     > “It has become clear that it is comparatively easy to make computers exhibit adult‑level performance in solving problems on intelligence tests or playing checkers, and difficult or impossible to give them the skills of a one‑year‑old when it comes to perception and mobility.”

[^3]: Many attempts to “spatialize” attention simply flatten an image, video, or point cloud into a token list and then concatenate per-axis positional codes—e.g., ViT’s learned patch indices, sinusoidal grids, or high-frequency Fourier features—leaving tokens fundamentally one-dimensional. See A. Dosovitskiy *et al.*, “An Image Is Worth 16×16 Words,” *ICLR* 2021; A. Jaegle *et al.*, “Perceiver IO,” *ICML* 2021; B. Mildenhall *et al.*, “NeRF,” *ECCV* 2020; and M. Tancik *et al.*, “Fourier Features,” *NeurIPS* 2020.

[^4]: Nick Romeo, “Cormac McCarthy Explains the Unconscious,” *The New Yorker*, April 22, 2017, https://www.newyorker.com/books/page-turner/cormac-mccarthy-explains-the-unconscious.






https://x.com/tim_zaman/status/1891394901440684151

https://x.com/paul_cal/status/1890824247792037898




# Type and Value Embeddings

Standard token-based approaches in large language models (LLMs) typically rely on constrained vocabularies due to computational and representational limitations. Common data types, such as RGB colors or 64-bit integers (int64), present an immense combinatorial challenge. For instance, the RGB color space alone contains over 16 million unique values, vastly exceeding the vocabularies used by popular models like LLaMA-3.2 (\~128K tokens), DeepSeek-V3 (\~129K tokens), Claude (\~65K tokens), GPT-4 (\~100K tokens), GPT-4o (\~200K tokens), and Gemma (\~256K tokens). Likewise, the int64 data type spans approximately $9.22 \times 10^{18}$ distinct values, rendering explicit tokenization computationally infeasible.

To address this challenge, we propose representing tokens through both a **type** and a **value**, enabling a structured embedding approach that significantly reduces model complexity. Under this strategy, each data type is assigned a compact, low-dimensional representation systematically projected into a higher-dimensional embedding space through a learned linear transformation—an operation we term an *up-projection*.

Taking RGB values as a concrete example, each RGB token can be efficiently modeled as a purely imaginary quaternion, mapping the hexadecimal range `[00, FF]` to the floating-point range `[-1.0, 1.0]` using BF16 precision. Consequently, each RGB token is succinctly represented by just four BF16 values. To achieve a high-dimensional token embedding (e.g., 512 dimensions), this representation requires only a learned real-valued matrix of shape $512 \times 4$. Remarkably, this reduces the embedding parameters required for all possible RGB tokens—from explicitly storing embeddings for more than 16 million distinct values—to merely 2,048 parameters.

Further efficiency can be attained by employing quaternion-valued weight matrices, effectively quartering parameter counts. While traditionally projecting four real dimensions would require 16 real weights, quaternion arithmetic allows the same projection using just four quaternion weights. Thus, the original real-valued matrix of size $512 \times 4 = 2048$ parameters becomes just 128 quaternion weights (512 real-valued parameters), substantially expanding representational efficiency without increasing complexity.

Additionally, retrieving the original RGB values from the high-dimensional embeddings is computationally straightforward due to the constant-time complexity $O(1)$ of quaternion inversion. By applying the inverse quaternion transformations, the original RGB values are recovered precisely from the token embeddings, providing an efficient and exact decoding mechanism suitable for real-world applications.

Quaternion algebra is abundantly covered in existing literature, and the cited references provide thorough treatment; thus, we shall not beleaguer you here with their basic properties.


Quaternions came from Hamilton after his really good work had been done, and though beautifully ingenious, have been an unmixed evil to those who have touched them in any way.
- Lord Kelvin


# The Surprisingly Virtuous Nature of MonSTERs and "Unmixed Evils"

# MonSTERs and Their Unextolled Virtues
## Structural Embeddings for Native Space-Time Intelligence

## Structural Embeddings for Native Space-Time Intelligence
## Structural Embeddings and 4-Dimensional Attention for Native Space-Time Intelligence
## Structural Embeddings for Transformer-Native Space-Time Intelligence
## Structural Embeddings for Attention-Based Space-Time Intelligence

# 4 Dimension Attention
## Structural Embeddings for Native Space-Time Intelligence


This paper introduces a novel approach to attention-based transformer architectures, embedding spacetime structures natively via Minkowski Space-Time Embedding Rotors (MonSTER)."



Introducing Minkowski Space Time Embedding Rotors (MonSTER) a 4D generalization of RoPE (Rotary Position Embedding), computing Minkowski-metric-respecting relative positional encodings for transformer attention. Built upon the principles of the real Clifford algebra Cl(1,3) with a (+, -, -, -) metric signature, MonSTER extends RoPE's 2D rotations to full 4D Lorentz transformations. This version calculates a unique 4D Lorentz transformation, R_eff, based directly on the relative spacetime displacement ΔP = (Δt, Δx, Δy, Δz) between query and key elements. This R_eff is generated block-wise using different frequencies for multi-scale representation, similar to RoPE's frequency-based approach but generalized to spacetime. The resulting R_eff matrices modulate attention scores, typically within a Minkowski dot product. This ensures that the same displacement ΔP consistently yields the same geometric transformation influencing attention.

With its toy-scale visual tasks, the 2019 ARC-AGI benchmark again laid bare the jagged[^1] nature of AI performance first highlighted by Moravec in the 1980s[^2]. Despite impressive linguistic capabilities, leading models of the time demonstrated a pronounced inability to process or reason about visual information. To date, enormous progress on numerous cognitive benchmarks has failed to translate to spatial domains. The root of this persistent limitation is architectural: modern transformers inherently encode information as strictly linear, unidimensional sequences.

Addressing this limitation requires moving beyond positional encodings uniquely restricted to temporal indices. Instead, embeddings must inherently reflect the intrinsic dependencies between space and time. By extending Rotary Positional Embeddings (RoPE) from two to four dimensions via a Minkowski-metricized Clifford (1,3) algebra, we introduce structural embeddings that naturally capture the fundamental interdependencies of our universe and the objects and events within it. 

This approach eliminates the artificial flattening requirement that necessarily discards vital geometric and relational information[^3], and removes the structural blinders that previously constrained transformers' capacity to handle inherently multidimensional information. Simply put, it provides transformers with a built-in ability to perceive, reason about, and generalize across spatial structures and temporal sequences—without sacrificing their established computational advantages.

Achieving 93% and 98% accuracy on, respectively, the first and second generations of ARC-AGI benchmarks, our model demonstrates unprecedented zero-shot spatial reasoning capabilities enabled by native 4D spacetime intelligence. Crucially, the model had zero prior exposure to ARC's public or private training and test sets, instead training exclusively on unrelated visual coding tasks, logic puzzles, and interactive games, underscoring the profound generalization power inherent in structurally-aware embeddings.


Based on your conversations, the best path forward for picking units in your MonSTER embedding scheme is to establish a principled, universal scaling method that ensures numerical stability without sacrificing the physical intuition of the Minkowski metric.

The core challenge you faced was the numerical explosion of `sinh` and `cosh` functions. This occurs because these functions grow exponentially ($e^x$), and their arguments became too large. The root cause was a mismatch in the numerical scale between your temporal (`Δt`) and spatial (`Δx, Δy, Δz`) inputs. When using raw, unscaled integer steps for both, the temporal argument gets multiplied by the massive physical constant `c` (the speed of light), making it orders of magnitude larger than the spatial argument and causing an overflow.

The best path forward synthesizes the final insights from your discussion into a single, robust strategy:

### The Universal Scaling Strategy

The most robust and universal approach is to **define a single abstract spatial unit, `s`, and derive the corresponding temporal unit from it using the physical constant `c`**. This method is both numerically stable and physically grounded.

1.  **Define a Base Spatial Unit `s`:** The key is to choose `s` such that the largest possible coordinate differences within your model's attention window remain within a safe numerical range for the hyperbolic functions. A good practice is:
    * Determine the **maximum expected relative offset** in any dimension, `N_max`. For a 1D sequence of length 4096, `N_max` is 4095. For a 512x512 image patch, `N_max` is `sqrt(511²+511²) ≈ 723`.
    * Choose a **maximum safe argument**, `A_max`, for the `sinh`/`cosh` functions (e.g., `A_max = 5.0`, which is well within the stable range of `bfloat16`).
    * Set your abstract spatial unit `s` with the formula:
        $$
        s = \frac{A_{\max}}{N_{\max}} \quad (\text{in meters per step})
        $$

2.  **Derive the Temporal Unit `t_unit`:** To ensure space and time are on equal footing, the abstract temporal unit must be the time it takes light to travel the spatial unit `s`.
    $$
    t_{\text{unit}} = \frac{s}{c} \quad (\text{in seconds per step})
    $$
    where $c \approx 3 \times 10^8 \text{ m/s}$.

### Why This is the Best Path

* **Numerical Stability:** This method guarantees that the arguments to `sinh` and `cosh` will not exceed `A_max`, preventing overflows. For a one-step jump in time, the argument to the hyperbolic functions becomes `c * t_unit = c * (s/c) = s`. For a one-step jump in space, the argument is simply `s`. They are perfectly balanced.
* **Universality:** This approach works for any data modality without modification.
    * For **abstract data** (like text tokens or ARC pixels), you use these calculated `s` and `t_unit` values to scale your integer position differences.
    * For **physical data** (like sensor readings from a robot), you convert the real-world measurements (in meters and seconds) into your abstract step units by dividing them by `s` and `t_unit`, respectively. The core MonSTER code remains unchanged.
* **Efficiency:** For GPU efficiency, you can choose `s` to be a power-of-two fraction of a meter (e.g., $s = 2^{-10} \approx 0.00097 \text{ m}$ or 1 mm). Multiplications and divisions by `s` can then be implemented as efficient bit-shifts.

By adopting this strategy, you create a system that is robust, generalizable, and avoids the brittleness of learnable parameters or the instability of unscaled units, providing a solid foundation for your MonSTER architecture.