Directory structure:
└── pufferai-pufferlib/
    ├── README.md
    ├── LICENSE
    ├── MANIFEST.in
    ├── pyproject.toml
    ├── setup.cfg
    ├── setup.py
    ├── config -> config
    ├── examples/
    │   ├── gym_env.py
    │   ├── gymnasium_env.py
    │   ├── pettingzoo_env.py
    │   ├── puffer_env.py
    │   ├── pufferl.py
    │   ├── render.py
    │   ├── structured_env.py
    │   └── vectorization.py
    ├── pufferlib/
    │   ├── __init__.py
    │   ├── cleanrl_ppo_atari.py
    │   ├── emulation.py
    │   ├── models.py
    │   ├── pufferl.py
    │   ├── pufferlib.py
    │   ├── pytorch.py
    │   ├── spaces.py
    │   ├── sweep.py
    │   ├── vector.py
    │   ├── config/
    │   │   ├── atari.ini
    │   │   ├── box2d.ini
    │   │   ├── bsuite.ini
    │   │   ├── butterfly.ini
    │   │   ├── classic_control.ini
    │   │   ├── classic_control_continuous.ini
    │   │   ├── craftax.ini
    │   │   ├── crafter.ini
    │   │   ├── default.ini
    │   │   ├── dm_control.ini
    │   │   ├── dm_lab.ini
    │   │   ├── doom.ini
    │   │   ├── gpudrive.ini
    │   │   ├── griddly.ini
    │   │   ├── gvgai.ini
    │   │   ├── kinetix.ini
    │   │   ├── magent.ini
    │   │   ├── mani_skill.ini
    │   │   ├── metta.ini
    │   │   ├── microrts.ini
    │   │   ├── minerl.ini
    │   │   ├── minigrid.ini
    │   │   ├── minihack.ini
    │   │   ├── mujoco.ini
    │   │   ├── nethack.ini
    │   │   ├── nmmo.ini
    │   │   ├── open_spiel.ini
    │   │   ├── pokemon_red.ini
    │   │   ├── procgen.ini
    │   │   ├── slimevolley.ini
    │   │   ├── stable_retro.ini
    │   │   ├── starcraft.ini
    │   │   ├── trade_sim.ini
    │   │   ├── tribal_village.ini
    │   │   └── ocean/
    │   │       ├── asteroids.ini
    │   │       ├── battle.ini
    │   │       ├── blastar.ini
    │   │       ├── boids.ini
    │   │       ├── breakout.ini
    │   │       ├── cartpole.ini
    │   │       ├── chain_mdp.ini
    │   │       ├── checkers.ini
    │   │       ├── connect4.ini
    │   │       ├── continuous.ini
    │   │       ├── convert.ini
    │   │       ├── convert_circle.ini
    │   │       ├── drive.ini
    │   │       ├── drone_race.ini
    │   │       ├── drone_swarm.ini
    │   │       ├── enduro.ini
    │   │       ├── freeway.ini
    │   │       ├── g2048.ini
    │   │       ├── go.ini
    │   │       ├── grid.ini
    │   │       ├── impulse_wars.ini
    │   │       ├── matsci.ini
    │   │       ├── memory.ini
    │   │       ├── moba.ini
    │   │       ├── nmmo3.ini
    │   │       ├── oldgrid.ini
    │   │       ├── onestateworld.ini
    │   │       ├── pacman.ini
    │   │       ├── pong.ini
    │   │       ├── pysquared.ini
    │   │       ├── rware.ini
    │   │       ├── sanity.ini
    │   │       ├── shared_pool.ini
    │   │       ├── slimevolley.ini
    │   │       ├── snake.ini
    │   │       ├── squared.ini
    │   │       ├── tactical.ini
    │   │       ├── target.ini
    │   │       ├── template.ini
    │   │       ├── terraform.ini
    │   │       ├── tetris.ini
    │   │       ├── tmaze.ini
    │   │       ├── tower_climb.ini
    │   │       ├── trash_pickup.ini
    │   │       ├── tripletriad.ini
    │   │       └── whisker_racer.ini
    │   ├── environments/
    │   │   ├── __init__.py
    │   │   ├── atari/
    │   │   │   ├── __init__.py
    │   │   │   ├── environment.py
    │   │   │   └── torch.py
    │   │   ├── box2d/
    │   │   │   ├── __init__.py
    │   │   │   ├── environment.py
    │   │   │   └── torch.py
    │   │   ├── bsuite/
    │   │   │   ├── __init__.py
    │   │   │   ├── environment.py
    │   │   │   └── torch.py
    │   │   ├── butterfly/
    │   │   │   ├── __init__.py
    │   │   │   ├── environment.py
    │   │   │   └── torch.py
    │   │   ├── classic_control/
    │   │   │   ├── __init__.py
    │   │   │   ├── environment.py
    │   │   │   └── torch.py
    │   │   ├── classic_control_continuous/
    │   │   │   ├── __init__.py
    │   │   │   ├── environment.py
    │   │   │   └── torch.py
    │   │   ├── craftax/
    │   │   │   ├── __init__.py
    │   │   │   ├── environment.py
    │   │   │   └── torch.py
    │   │   ├── crafter/
    │   │   │   ├── __init__.py
    │   │   │   ├── environment.py
    │   │   │   └── torch.py
    │   │   ├── dm_control/
    │   │   │   ├── __init__.py
    │   │   │   ├── environment.py
    │   │   │   └── torch.py
    │   │   ├── dm_lab/
    │   │   │   ├── __init__.py
    │   │   │   ├── environment.py
    │   │   │   └── torch.py
    │   │   ├── gpudrive/
    │   │   │   ├── __init__.py
    │   │   │   ├── environment.py
    │   │   │   └── torch.py
    │   │   ├── griddly/
    │   │   │   ├── __init__.py
    │   │   │   ├── environment.py
    │   │   │   └── torch.py
    │   │   ├── gvgai/
    │   │   │   └── environment.py
    │   │   ├── kinetix/
    │   │   │   ├── __init__.py
    │   │   │   ├── environment.py
    │   │   │   └── torch.py
    │   │   ├── links_awaken/
    │   │   │   ├── __init__.py
    │   │   │   ├── environment.py
    │   │   │   └── torch.py
    │   │   ├── magent/
    │   │   │   ├── __init__.py
    │   │   │   ├── environment.py
    │   │   │   └── torch.py
    │   │   ├── mani_skill/
    │   │   │   ├── __init__.py
    │   │   │   ├── environment.py
    │   │   │   └── torch.py
    │   │   ├── metta/
    │   │   │   ├── __init__.py
    │   │   │   ├── environment.py
    │   │   │   ├── metta.yaml
    │   │   │   └── torch.py
    │   │   ├── microrts/
    │   │   │   ├── __init__.py
    │   │   │   ├── environment.py
    │   │   │   └── torch.py
    │   │   ├── minerl/
    │   │   │   ├── __init__.py
    │   │   │   ├── environment.py
    │   │   │   └── torch.py
    │   │   ├── minigrid/
    │   │   │   ├── __init__.py
    │   │   │   ├── environment.py
    │   │   │   └── torch.py
    │   │   ├── minihack/
    │   │   │   ├── __init__.py
    │   │   │   ├── environment.py
    │   │   │   └── torch.py
    │   │   ├── mujoco/
    │   │   │   ├── __init__.py
    │   │   │   ├── environment.py
    │   │   │   └── torch.py
    │   │   ├── nethack/
    │   │   │   ├── __init__.py
    │   │   │   ├── environment.py
    │   │   │   ├── torch.py
    │   │   │   └── wrapper.py
    │   │   ├── nmmo/
    │   │   │   ├── __init__.py
    │   │   │   ├── environment.py
    │   │   │   └── torch.py
    │   │   ├── open_spiel/
    │   │   │   ├── __init__.py
    │   │   │   ├── environment.py
    │   │   │   ├── gymnasium_environment.py
    │   │   │   ├── pettingzoo_environment.py
    │   │   │   ├── torch.py
    │   │   │   └── utils.py
    │   │   ├── pokemon_red/
    │   │   │   ├── __init__.py
    │   │   │   ├── environment.py
    │   │   │   └── torch.py
    │   │   ├── procgen/
    │   │   │   ├── __init__.py
    │   │   │   ├── environment.py
    │   │   │   └── torch.py
    │   │   ├── slimevolley/
    │   │   │   ├── __init__.py
    │   │   │   ├── environment.py
    │   │   │   └── torch.py
    │   │   ├── smac/
    │   │   │   ├── __init__.py
    │   │   │   ├── environment.py
    │   │   │   └── torch.py
    │   │   ├── stable_retro/
    │   │   │   ├── __init__.py
    │   │   │   ├── environment.py
    │   │   │   └── torch.py
    │   │   ├── test/
    │   │   │   ├── __init__.py
    │   │   │   ├── environment.py
    │   │   │   ├── mock_environments.py
    │   │   │   └── torch.py
    │   │   ├── trade_sim/
    │   │   │   ├── __init__.py
    │   │   │   ├── environment.py
    │   │   │   └── torch.py
    │   │   ├── tribal_village/
    │   │   │   ├── __init__.py
    │   │   │   ├── environment.py
    │   │   │   └── torch.py
    │   │   └── vizdoom/
    │   │       ├── __init__.py
    │   │       ├── environment.py
    │   │       └── torch.py
    │   ├── extensions/
    │   │   ├── extensions.pyx
    │   │   ├── pufferlib.cpp
    │   │   ├── puffernet.h
    │   │   ├── puffernet.pyx
    │   │   └── cuda/
    │   │       └── pufferlib.cu
    │   ├── ocean/
    │   │   ├── __init__.py
    │   │   ├── env_binding.h
    │   │   ├── environment.py
    │   │   ├── render.py
    │   │   ├── sanity.py
    │   │   ├── torch.py
    │   │   ├── asteroids/
    │   │   │   ├── asteroids.c
    │   │   │   ├── asteroids.h
    │   │   │   ├── asteroids.py
    │   │   │   └── binding.c
    │   │   ├── battle/
    │   │   │   ├── battle.c
    │   │   │   ├── battle.h
    │   │   │   ├── battle.py
    │   │   │   ├── binding.c
    │   │   │   ├── rlights.h
    │   │   │   └── simplex.h
    │   │   ├── blastar/
    │   │   │   ├── binding.c
    │   │   │   ├── blastar.c
    │   │   │   ├── blastar.h
    │   │   │   └── blastar.py
    │   │   ├── boids/
    │   │   │   ├── binding.c
    │   │   │   ├── boids.c
    │   │   │   ├── boids.h
    │   │   │   └── boids.py
    │   │   ├── breakout/
    │   │   │   ├── binding.c
    │   │   │   ├── breakout.c
    │   │   │   ├── breakout.h
    │   │   │   └── breakout.py
    │   │   ├── cartpole/
    │   │   │   ├── binding.c
    │   │   │   ├── cartpole.c
    │   │   │   ├── cartpole.h
    │   │   │   └── cartpole.py
    │   │   ├── chain_mdp/
    │   │   │   ├── README.md
    │   │   │   ├── binding.c
    │   │   │   ├── chain_mdp.c
    │   │   │   ├── chain_mdp.h
    │   │   │   └── chain_mdp.py
    │   │   ├── checkers/
    │   │   │   ├── binding.c
    │   │   │   ├── checkers.c
    │   │   │   ├── checkers.h
    │   │   │   └── checkers.py
    │   │   ├── connect4/
    │   │   │   ├── binding.c
    │   │   │   ├── connect4.c
    │   │   │   ├── connect4.h
    │   │   │   └── connect4.py
    │   │   ├── convert/
    │   │   │   ├── binding.c
    │   │   │   ├── convert.c
    │   │   │   ├── convert.h
    │   │   │   └── convert.py
    │   │   ├── convert_circle/
    │   │   │   ├── binding.c
    │   │   │   ├── convert_circle.c
    │   │   │   ├── convert_circle.h
    │   │   │   └── convert_circle.py
    │   │   ├── drive/
    │   │   │   ├── binding.c
    │   │   │   ├── drive.c
    │   │   │   └── drive.py
    │   │   ├── drone_race/
    │   │   │   ├── binding.c
    │   │   │   ├── drone_race.c
    │   │   │   ├── drone_race.h
    │   │   │   ├── drone_race.py
    │   │   │   └── dronelib.h
    │   │   ├── drone_swarm/
    │   │   │   ├── binding.c
    │   │   │   ├── drone_swarm.c
    │   │   │   ├── drone_swarm.h
    │   │   │   ├── drone_swarm.py
    │   │   │   └── dronelib.h
    │   │   ├── enduro/
    │   │   │   ├── binding.c
    │   │   │   ├── enduro.c
    │   │   │   └── enduro.py
    │   │   ├── freeway/
    │   │   │   ├── binding.c
    │   │   │   ├── freeway.c
    │   │   │   ├── freeway.h
    │   │   │   ├── freeway.py
    │   │   │   └── freeway_levels.h
    │   │   ├── g2048/
    │   │   │   ├── 2048.h
    │   │   │   ├── binding.c
    │   │   │   ├── g2048.c
    │   │   │   └── g2048.py
    │   │   ├── go/
    │   │   │   ├── binding.c
    │   │   │   ├── go.c
    │   │   │   ├── go.h
    │   │   │   └── go.py
    │   │   ├── grid/
    │   │   │   ├── __init__.py
    │   │   │   ├── binding.c
    │   │   │   ├── c_grid.pyx
    │   │   │   ├── cy_grid.pyx
    │   │   │   ├── grid.c
    │   │   │   ├── grid.h
    │   │   │   └── grid.py
    │   │   ├── impulse_wars/
    │   │   │   ├── README.md
    │   │   │   ├── benchmark.c
    │   │   │   ├── binding.c
    │   │   │   ├── CMakeLists.txt
    │   │   │   ├── env.h
    │   │   │   ├── helpers.h
    │   │   │   ├── impulse_wars.c
    │   │   │   ├── impulse_wars.py
    │   │   │   ├── Makefile
    │   │   │   ├── map.h
    │   │   │   ├── pyproject.toml
    │   │   │   ├── scripted_agent.h
    │   │   │   ├── settings.h
    │   │   │   ├── types.h
    │   │   │   ├── .clang-format
    │   │   │   └── include/
    │   │   │       ├── cc_array.h
    │   │   │       ├── cc_common.h
    │   │   │       └── rlights.h
    │   │   ├── matsci/
    │   │   │   ├── binding.c
    │   │   │   ├── matsci.c
    │   │   │   ├── matsci.h
    │   │   │   └── matsci.py
    │   │   ├── memory/
    │   │   │   ├── binding.c
    │   │   │   ├── memory.c
    │   │   │   ├── memory.h
    │   │   │   └── memory.py
    │   │   ├── moba/
    │   │   │   ├── __init__.py
    │   │   │   ├── binding.c
    │   │   │   ├── cy_moba.pyx
    │   │   │   ├── moba.c
    │   │   │   └── moba.py
    │   │   ├── nmmo3/
    │   │   │   ├── binding.c
    │   │   │   ├── cy_nmmo3.pyx
    │   │   │   ├── make_sprite_sheets.py
    │   │   │   ├── nmmo3.c
    │   │   │   ├── nmmo3.py
    │   │   │   ├── simplex.h
    │   │   │   └── tile_atlas.h
    │   │   ├── onestateworld/
    │   │   │   ├── README.md
    │   │   │   ├── binding.c
    │   │   │   ├── onestateworld.c
    │   │   │   ├── onestateworld.h
    │   │   │   └── onestateworld.py
    │   │   ├── pacman/
    │   │   │   ├── binding.c
    │   │   │   ├── helpers.h
    │   │   │   ├── pacman.c
    │   │   │   ├── pacman.h
    │   │   │   └── pacman.py
    │   │   ├── pong/
    │   │   │   ├── binding.c
    │   │   │   ├── cy_pong.pyx
    │   │   │   ├── pong.c
    │   │   │   ├── pong.h
    │   │   │   └── pong.py
    │   │   ├── pysquared/
    │   │   │   └── pysquared.py
    │   │   ├── robocode/
    │   │   │   ├── build_local.sh
    │   │   │   ├── robocode.c
    │   │   │   └── robocode.h
    │   │   ├── rocket_lander/
    │   │   │   ├── cy_rocket_lander.pyx
    │   │   │   ├── rocket_lander.c
    │   │   │   ├── rocket_lander.h
    │   │   │   └── rocket_lander.py
    │   │   ├── rware/
    │   │   │   ├── binding.c
    │   │   │   ├── rware.c
    │   │   │   ├── rware.h
    │   │   │   └── rware.py
    │   │   ├── shared_pool/
    │   │   │   ├── binding.c
    │   │   │   ├── grid.h
    │   │   │   ├── shared_pool.c
    │   │   │   ├── shared_pool.h
    │   │   │   └── shared_pool.py
    │   │   ├── slimevolley/
    │   │   │   ├── binding.c
    │   │   │   ├── eval.py
    │   │   │   ├── slimevolley.c
    │   │   │   ├── slimevolley.h
    │   │   │   └── slimevolley.py
    │   │   ├── snake/
    │   │   │   ├── README.md
    │   │   │   ├── __init__.py
    │   │   │   ├── binding.c
    │   │   │   ├── snake.c
    │   │   │   ├── snake.h
    │   │   │   └── snake.py
    │   │   ├── squared/
    │   │   │   ├── binding.c
    │   │   │   ├── squared.c
    │   │   │   ├── squared.h
    │   │   │   └── squared.py
    │   │   ├── tactical/
    │   │   │   ├── __init__.py
    │   │   │   ├── binding.c
    │   │   │   ├── maps.h
    │   │   │   ├── tactical.c
    │   │   │   └── tactical.py
    │   │   ├── tcg/
    │   │   │   ├── build_local.sh
    │   │   │   ├── build_web.sh
    │   │   │   ├── tcg.c
    │   │   │   └── tcg.h
    │   │   ├── template/
    │   │   │   ├── binding.c
    │   │   │   ├── template.c
    │   │   │   ├── template.h
    │   │   │   └── template.py
    │   │   ├── terraform/
    │   │   │   ├── binding.c
    │   │   │   ├── simplex.h
    │   │   │   ├── terraform.c
    │   │   │   ├── terraform.h
    │   │   │   └── terraform.py
    │   │   ├── tetris/
    │   │   │   ├── binding.c
    │   │   │   ├── tetris.c
    │   │   │   ├── tetris.h
    │   │   │   ├── tetris.py
    │   │   │   └── tetrominoes.h
    │   │   ├── tmaze/
    │   │   │   ├── README.md
    │   │   │   ├── binding.c
    │   │   │   ├── tmaze.c
    │   │   │   ├── tmaze.h
    │   │   │   └── tmaze.py
    │   │   ├── tower_climb/
    │   │   │   ├── binding.c
    │   │   │   ├── tower_climb.c
    │   │   │   └── tower_climb.py
    │   │   ├── trash_pickup/
    │   │   │   ├── README.md
    │   │   │   ├── binding.c
    │   │   │   ├── cy_trash_pickup.pyx
    │   │   │   ├── trash_pickup.c
    │   │   │   ├── trash_pickup.h
    │   │   │   └── trash_pickup.py
    │   │   ├── tripletriad/
    │   │   │   ├── binding.c
    │   │   │   ├── tripletriad.c
    │   │   │   ├── tripletriad.h
    │   │   │   └── tripletriad.py
    │   │   └── whisker_racer/
    │   │       ├── binding.c
    │   │       ├── whisker_racer.c
    │   │       ├── whisker_racer.h
    │   │       └── whisker_racer.py
    │   └── resources/
    │       ├── battle/
    │       │   ├── bomber.glb
    │       │   ├── drone.glb
    │       │   ├── fighter.glb
    │       │   ├── mothership.glb
    │       │   ├── shader_330.fs
    │       │   └── shader_330.vs
    │       ├── impulse_wars/
    │       │   ├── create_texture.py
    │       │   └── shaders/
    │       │       ├── gls100/
    │       │       │   ├── bloom.fs
    │       │       │   ├── blur.fs
    │       │       │   ├── grid.fs
    │       │       │   └── shader.vs
    │       │       └── gls330/
    │       │           ├── bloom.fs
    │       │           ├── blur.fs
    │       │           ├── grid.fs
    │       │           └── shader.vs
    │       ├── moba/
    │       │   ├── bloom_shader_100.fs
    │       │   ├── bloom_shader_330.fs
    │       │   ├── game_map.npy
    │       │   ├── map_shader_100.fs
    │       │   └── map_shader_330.fs
    │       ├── nmmo3/
    │       │   ├── ASSETS_LICENSE.md
    │       │   ├── map_shader_100.fs
    │       │   └── map_shader_330.fs
    │       ├── terraform/
    │       │   ├── shader_100.fs
    │       │   ├── shader_100.vs
    │       │   ├── shader_330.fs
    │       │   ├── shader_330.vs
    │       │   ├── target_shader_100.fs
    │       │   ├── target_shader_330.fs
    │       │   └── target_shader_330.vs
    │       └── tower_climb/
    │           └── shaders/
    │               ├── gls100/
    │               │   ├── lighting.fs
    │               │   └── lighting.vs
    │               └── gls330/
    │                   ├── lighting.fs
    │                   └── lighting.vs
    ├── resources -> resources
    ├── scripts/
    │   ├── build_ocean.sh
    │   ├── build_simple.sh
    │   ├── minshell.html
    │   ├── sweep_atari.sh
    │   ├── train_atari.sh
    │   ├── train_ocean.sh
    │   ├── train_procgen.sh
    │   └── train_sanity.sh
    ├── tests/
    │   ├── __init__.py
    │   ├── mem_test.py
    │   ├── test.py
    │   ├── test_api.py
    │   ├── test_atari_reset.py
    │   ├── test_c_advantage.cu
    │   ├── test_carbs.py
    │   ├── test_cleanrl_utils.py
    │   ├── test_env_binding.py
    │   ├── test_extensions.py
    │   ├── test_flatten.py
    │   ├── test_import_performance.py
    │   ├── test_namespace.py
    │   ├── test_nested.py
    │   ├── test_newbind.py
    │   ├── test_nmmo3_compile.py
    │   ├── test_performance.py
    │   ├── test_pokemon_red.py
    │   ├── test_policy_pool.py
    │   ├── test_puffernet.py
    │   ├── test_pytorch.py
    │   ├── test_record_array.py
    │   ├── test_record_emulation.py
    │   ├── test_registry.sh
    │   ├── test_render.py
    │   ├── test_rich.py
    │   ├── test_sweep.py
    │   ├── test_utils.py
    │   ├── time_alloc.py
    │   └── pool/
    │       ├── envpool_results.npy
    │       ├── plot_packing.py
    │       ├── test_basic_multprocessing.py
    │       ├── test_envpool.py
    │       └── test_multiprocessing.py
    └── .github/
        └── workflows/
            └── install.yml

================================================
FILE: README.md
================================================
![figure](https://pufferai.github.io/source/resource/header.png)

[![PyPI version](https://badge.fury.io/py/pufferlib.svg)](https://badge.fury.io/py/pufferlib)
![PyPI - Python Version](https://img.shields.io/pypi/pyversions/pufferlib)
![Github Actions](https://github.com/PufferAI/PufferLib/actions/workflows/install.yml/badge.svg)
[![](https://dcbadge.vercel.app/api/server/spT4huaGYV?style=plastic)](https://discord.gg/spT4huaGYV)
[![Twitter](https://img.shields.io/twitter/url/https/twitter.com/cloudposse.svg?style=social&label=Follow%20%40jsuarez5341)](https://twitter.com/jsuarez5341)

PufferLib is the reinforcement learning library I wish existed during my PhD. It started as a compatibility layer to make working with complex environments a breeze. Now, it's a high-performance toolkit for research and industry with optimized parallel simulation, environments that run and train at 1M+ steps/second, and tons of quality of life improvements for practitioners. All our tools are free and open source. We also offer priority service for companies, startups, and labs!

![Trailer](https://github.com/PufferAI/puffer.ai/blob/main/docs/assets/puffer_2.gif?raw=true)

All of our documentation is hosted at [puffer.ai](https://puffer.ai "PufferLib Documentation"). @jsuarez5341 on [Discord](https://discord.gg/puffer) for support -- post here before opening issues. We're always looking for new contributors, too!

## Star to puff up the project!

<a href="https://star-history.com/#pufferai/pufferlib&Date">
 <picture>
   <source media="(prefers-color-scheme: dark)" srcset="https://api.star-history.com/svg?repos=pufferai/pufferlib&type=Date&theme=dark" />
   <source media="(prefers-color-scheme: light)" srcset="https://api.star-history.com/svg?repos=pufferai/pufferlib&type=Date" />
   <img alt="Star History Chart" src="https://api.star-history.com/svg?repos=pufferai/pufferlib&type=Date" />
 </picture>
</a>



================================================
FILE: LICENSE
================================================
MIT License

Copyright (c) 2022 PufferAI

Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files (the "Software"), to deal
in the Software without restriction, including without limitation the rights
to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the Software, and to permit persons to whom the Software is
furnished to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in all
copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
SOFTWARE.



================================================
FILE: MANIFEST.in
================================================
global-include *.pyx
global-include *.pxd
global-include *.h
global-include *.cpp
global-include *.cu
global-include *.py
recursive-include pufferlib/config *.ini
recursive-include pufferlib/resources *
recursive-exclude experiments *
recursive-exclude wandb *
recursive-exclude tests *
include raylib-5.5_linux_amd64/lib/libraylib.a
include raylib-5.5_macos/lib/libraylib.a
include box2d-linux-amd64/libbox2d.a
recursive-exclude box2d-web *
recursive-exclude raylib-5.5_webassembly * 
recursive-exclude pufferlib/ocean/impulse_wars/debug* *
recursive-exclude pufferlib/ocean/impulse_wars/release* *
recursive-exclude box2d* *



================================================
FILE: pyproject.toml
================================================
[project]
name = "pufferlib"
version = "3.0.0"
description = "Fast and sane reinforcement learning"
requires-python = ">=3.9"
readme = {file = "README.md", content-type = "text/markdown"}
license = "MIT"
license-files = ["LICENSE"]
authors = [{ name = "Joseph Suarez", email = "jsuarez@puffer.ai" }]
classifiers=[
    "Intended Audience :: Science/Research",
    "Intended Audience :: Developers",
    "Programming Language :: Python :: 3.9",
    "Programming Language :: Python :: 3.10",
    "Programming Language :: Python :: 3.11",
    "Programming Language :: Python :: 3.12",
    "Programming Language :: Python :: 3.13",
    "Programming Language :: Python :: 3.14",
]
dynamic = ["dependencies"]

# Default Gym/Gymnasium/PettingZoo versions
# Gym:
# - 0.26 still has deprecation warnings and is the last version of the package
# - 0.25 adds a breaking API change to reset, step, and render_modes
# - 0.24 is broken
# - 0.22-0.23 triggers deprecation warnings by calling its own functions
# - 0.21 is the most stable version
# - <= 0.20 is missing dict methods for gym.spaces.Dict
# - 0.18-0.21 require setuptools<=65.5.0
#'gym==0.23',
#'gymnasium==0.29.1',
#'pettingzoo==1.24.1',
[project.optional-dependencies]
avalon = [
    'gymnasium==0.29.1',
    'avalon-rl==1.0.0',
]

atari = [
    'gymnasium[accept-rom-license]==0.29.1',
    'opencv-python==3.4.17.63',
    'ale_py==0.9.0',
]

box2d = [
    'gymnasium[box2d]==0.29.1',
    'swig==4.1.1',
]

bsuite = [
    'gym==0.23',
    'gymnasium==0.29.1',
    'bsuite==0.3.5',
]

butterfly = [
    'gym==0.23',
    'gymnasium==0.29.1',
    'pettingzoo[butterfly]==1.24.1',
]

classic_control = [
    'gym==0.23',
    'gymnasium==0.29.1',
]

crafter = [
    'gym==0.23',
    'gymnasium==0.29.1',
    'crafter==1.8.3',
]

craftax = [
    'gymnasium==0.29.1',
    'craftax',
]

dm_control = [
    'gym==0.23',
    'gymnasium==0.29.1',
    'dm_control==1.0.11',
]

dm_lab = [
    'gym==0.23',
    'gymnasium==0.29.1',
    'gym_deepmindlab==0.1.2',
    'dm_env==1.6',
]

griddly = [
    'gym==0.23',
    'gymnasium==0.29.1',
    'griddly==1.6.7',
    'imageio',
]

kinetix = [
    'gym==0.23',
    'gymnasium==0.29.1',
    'kinetix-env',
]

magent = [
    'gym==0.23',
    'gymnasium==0.29.1',
    'pettingzoo==1.19.0',
    'magent==0.2.4',
    # The Magent2 package is broken for now
    #'magent2==0.3.2',
]

metta = [
    'gym==0.23',
    'gymnasium==0.29.1',
    'omegaconf',
    'hydra-core',
    'duckdb',
    'raylib>=5.5.0',
    'metta-common @ git+https://github.com/metta-ai/metta.git@main#subdirectory=common',
    'metta-mettagrid @ git+https://github.com/metta-ai/metta.git@main#subdirectory=mettagrid',
]

microrts = [
    'gym==0.23',
    'gymnasium==0.29.1',
    'ffmpeg==1.4',
    'gym_microrts==0.3.2',
]

minerl = [
    'gym==0.17.0',
    'gymnasium==0.29.1',
    #'git+https://github.com/minerllabs/minerl'
    # Compatiblity warning with urllib3 and chardet
    #'requests==2.31.0',
]

minigrid = [
    'gym==0.23',
    'gymnasium==0.29.1',
    'minigrid==2.3.1',
]

minihack =  [
    'gym==0.23',
    'gymnasium==0.29.1',
    'minihack==0.1.5',
]

mujoco = [
    'gymnasium[mujoco]==1.0.0',
    'moviepy',
]

nethack = [
    'gym==0.23',
    'gymnasium==0.29.1',
    'nle==0.9.1',
]

nmmo = [
    'gym==0.23',
    'gymnasium==0.29.1',
    'pettingzoo==1.24.1',
    'nmmo>=2.1',
]

open_spiel = [
    'gym==0.23',
    'gymnasium==0.29.1',
    'pettingzoo==1.19.0',
    'open_spiel==1.3',
]

pokemon_red = [
    'gym==0.23',
    'gymnasium==0.29.1',
    'pokegym>=0.2.0',
    'einops==0.6.1',
    'matplotlib',
    'scikit-image',
    'pyboy<2.0.0',
    'hnswlib==0.7.0',
    'mediapy',
    'pandas==2.0.2',
    'pettingzoo',
    'websockets',
]

procgen = [
    'gym==0.23',
    'gymnasium==0.29.1',
    'stable_baselines3==2.1.0',
    'procgen-mirror==0.10.7', # Procgen mirror for 3.11 and 3.12 support
    # Note: You need glfw==2.7 after installing for some torch versions
]

#'smac': [
#    'git+https://github.com/oxwhirl/smac.git',
#],
#'stable-retro': [
#    'git+https://github.com/Farama-Foundation/stable-retro.git',
#]

slimevolley = [
    'gym==0.23',
    'gymnasium==0.29.1',
    'slimevolley==0.1.0',
]

vizdoom = [
    'gym==0.23',
    'gymnasium==0.29.1',
    'vizdoom==1.2.3',
    'stable_baselines3==2.1.0',
]

ray = [
    'ray==2.23.0',
]

tribal-village = [
    'gym==0.23',
    'gymnasium==0.29.1',
    'numpy<2.0',
    'tribal-village @ git+https://github.com/Metta-AI/tribal-village.git@main',
]

cleanrl = [
    'stable_baselines3==2.1.0',
    'tensorboard==2.11.2',
    'tyro==0.8.6',
]

[project.scripts]
puffer = "pufferlib.pufferl:main"

[project.urls]
Homepage = "https://puffer.ai"

[build-system]
requires = ["setuptools", "wheel", "Cython", "numpy<2.0", "torch"]
build-backend = "setuptools.build_meta"

[tool.uv]
no-build-isolation-package = ["torch"]



================================================
FILE: setup.cfg
================================================
[build_ext]
inplace=true
force=true



================================================
FILE: setup.py
================================================
# Debug command:
#    DEBUG=1 python setup.py build_ext --inplace --force
#    CUDA_VISIBLE_DEVICES=None LD_PRELOAD=$(gcc -print-file-name=libasan.so) python3.12 -m pufferlib.clean_pufferl eval --train.device cpu

from setuptools import find_packages, find_namespace_packages, setup, Extension
import numpy
import os
import glob
import urllib.request
import zipfile
import tarfile
import platform
import shutil

from setuptools.command.build_ext import build_ext
from torch.utils import cpp_extension
from torch.utils.cpp_extension import (
    CppExtension,
    CUDAExtension,
    BuildExtension,
    CUDA_HOME,
)

# Build with DEBUG=1 to enable debug symbols
DEBUG = os.getenv("DEBUG", "0") == "1"
NO_OCEAN = os.getenv("NO_OCEAN", "0") == "1"
NO_TRAIN = os.getenv("NO_TRAIN", "0") == "1"

# Build raylib for your platform
RAYLIB_URL = 'https://github.com/raysan5/raylib/releases/download/5.5/'
RAYLIB_NAME = 'raylib-5.5_macos' if platform.system() == "Darwin" else 'raylib-5.5_linux_amd64'
RLIGHTS_URL = 'https://raw.githubusercontent.com/raysan5/raylib/refs/heads/master/examples/shaders/rlights.h'

def download_raylib(platform, ext):
    if not os.path.exists(platform):
        print(f'Downloading Raylib {platform}')
        urllib.request.urlretrieve(RAYLIB_URL + platform + ext, platform + ext)
        if ext == '.zip':
            with zipfile.ZipFile(platform + ext, 'r') as zip_ref:
                zip_ref.extractall()
        else:
            with tarfile.open(platform + ext, 'r') as tar_ref:
                tar_ref.extractall()

        os.remove(platform + ext)
        urllib.request.urlretrieve(RLIGHTS_URL, platform + '/include/rlights.h')

if not NO_OCEAN:
    download_raylib('raylib-5.5_webassembly', '.zip')
    download_raylib(RAYLIB_NAME, '.tar.gz')

BOX2D_URL = 'https://github.com/capnspacehook/box2d/releases/latest/download/'
BOX2D_NAME = 'box2d-macos-arm64' if platform.system() == "Darwin" else 'box2d-linux-amd64'

def download_box2d(platform):
    if not os.path.exists(platform):
        ext = ".tar.gz"

        print(f'Downloading Box2D {platform}')
        urllib.request.urlretrieve(BOX2D_URL + platform + ext, platform + ext)
        with tarfile.open(platform + ext, 'r') as tar_ref:
            tar_ref.extractall()

        os.remove(platform + ext)

if not NO_OCEAN:
    download_box2d('box2d-web')
    download_box2d(BOX2D_NAME)

# Shared compile args for all platforms
extra_compile_args = [
    '-DNPY_NO_DEPRECATED_API=NPY_1_7_API_VERSION',
    '-DPLATFORM_DESKTOP',
]
extra_link_args = [
    '-fwrapv'
]
cxx_args = [
    '-fdiagnostics-color=always',
]
nvcc_args = []

if DEBUG:
    extra_compile_args += [
        '-O0',
        '-g',
        '-fsanitize=address,undefined,bounds,pointer-overflow,leak',
        '-fno-omit-frame-pointer',
    ]
    extra_link_args += [
        '-g',
        '-fsanitize=address,undefined,bounds,pointer-overflow,leak',
    ]
    cxx_args += [
        '-O0',
        '-g',
    ]
    nvcc_args += [
        '-O0',
        '-g',
    ]
else:
    extra_compile_args += [
        '-O2',
        '-flto',
    ]
    extra_link_args += [
        '-O2',
    ]
    cxx_args += [
        '-O3',
    ]
    nvcc_args += [
        '-O3',
    ]

system = platform.system()
if system == 'Linux':
    extra_compile_args += [
        '-Wno-alloc-size-larger-than',
        '-Wno-implicit-function-declaration',
        '-fmax-errors=3',
    ]
    extra_link_args += [
        '-Bsymbolic-functions',
    ]
elif system == 'Darwin':
    extra_compile_args += [
        '-Wno-error=int-conversion',
        '-Wno-error=incompatible-function-pointer-types',
        '-Wno-error=implicit-function-declaration',
    ]
    extra_link_args += [
        '-framework', 'Cocoa',
        '-framework', 'OpenGL',
        '-framework', 'IOKit',
    ]
else:
    raise ValueError(f'Unsupported system: {system}')

# Default Gym/Gymnasium/PettingZoo versions
# Gym:
# - 0.26 still has deprecation warnings and is the last version of the package
# - 0.25 adds a breaking API change to reset, step, and render_modes
# - 0.24 is broken
# - 0.22-0.23 triggers deprecation warnings by calling its own functions
# - 0.21 is the most stable version
# - <= 0.20 is missing dict methods for gym.spaces.Dict
# - 0.18-0.21 require setuptools<=65.5.0

# Extensions 
class BuildExt(build_ext):
    def run(self):
        # Propagate any build_ext options (e.g., --inplace, --force) to subcommands
        build_ext_opts = self.distribution.command_options.get('build_ext', {})
        if build_ext_opts:
            # Copy flags so build_torch and build_c respect inplace/force
            self.distribution.command_options['build_torch'] = build_ext_opts.copy()
            self.distribution.command_options['build_c'] = build_ext_opts.copy()

        # Run the torch and C builds (which will handle copying when inplace is set)
        self.run_command('build_torch')
        self.run_command('build_c')

class CBuildExt(build_ext):
    def run(self, *args, **kwargs):
        self.extensions = [e for e in self.extensions if e.name != "pufferlib._C"]
        super().run(*args, **kwargs)

class TorchBuildExt(cpp_extension.BuildExtension):
    def run(self):
        self.extensions = [e for e in self.extensions if e.name == "pufferlib._C"]
        super().run()

INCLUDE = [f'{BOX2D_NAME}/include', f'{BOX2D_NAME}/src']
RAYLIB_A = f'{RAYLIB_NAME}/lib/libraylib.a'
extension_kwargs = dict(
    include_dirs=INCLUDE,
    extra_compile_args=extra_compile_args,
    extra_link_args=extra_link_args,
    extra_objects=[RAYLIB_A],
)

# Find C extensions
c_extensions = []
if not NO_OCEAN:
    c_extension_paths = glob.glob('pufferlib/ocean/**/binding.c', recursive=True)
    c_extensions = [
        Extension(
            path.rstrip('.c').replace('/', '.'),
            sources=[path],
            **extension_kwargs,
        )
        for path in c_extension_paths if 'matsci' not in path
    ]
    c_extension_paths = [os.path.join(*path.split('/')[:-1]) for path in c_extension_paths]

    for c_ext in c_extensions:
        if "impulse_wars" in c_ext.name:
            print(f"Adding {c_ext.name} to extra objects")
            c_ext.extra_objects.append(f'{BOX2D_NAME}/libbox2d.a')

        if 'matsci' in c_ext.name:
            c_ext.include_dirs.append('/usr/local/include')
            c_ext.extra_link_args.extend(['-L/usr/local/lib', '-llammps'])

# Check if CUDA compiler is available. You need cuda dev, not just runtime.
torch_extensions = []
if not NO_TRAIN:
    torch_sources = [
        "pufferlib/extensions/pufferlib.cpp",
    ]
    if shutil.which("nvcc"):
        extension = CUDAExtension
        torch_sources.append("pufferlib/extensions/cuda/pufferlib.cu")
    else:
        extension = CppExtension

    torch_extensions = [
       extension(
            "pufferlib._C",
            torch_sources,
            extra_compile_args = {
                "cxx": cxx_args,
                "nvcc": nvcc_args,
            }
        ),
    ]

# Prevent Conda from injecting garbage compile flags
from distutils.sysconfig import get_config_vars
cfg_vars = get_config_vars()
for key in ('CC', 'CXX', 'LDSHARED'):
    if cfg_vars[key]:
        cfg_vars[key] = cfg_vars[key].replace('-B /root/anaconda3/compiler_compat', '')
        cfg_vars[key] = cfg_vars[key].replace('-pthread', '')
        cfg_vars[key] = cfg_vars[key].replace('-fno-strict-overflow', '')

for key, value in cfg_vars.items():
    if value and '-fno-strict-overflow' in str(value):
        cfg_vars[key] = value.replace('-fno-strict-overflow', '')

install_requires = [
    'setuptools',
    'numpy<2.0',
    'shimmy[gym-v21]',
    'gym==0.23',
    'gymnasium==0.29.1',
    'pettingzoo==1.24.1',
]

if not NO_TRAIN:
    install_requires += [
        'torch',
        'psutil',
        'nvidia-ml-py',
        'rich',
        'rich_argparse',
        'imageio',
        'pyro-ppl',
        'heavyball<2.0.0',
        'neptune',
        'wandb',
    ]

setup(
    version="3.0.0",
    packages=find_namespace_packages() + find_packages() + c_extension_paths + ['pufferlib/extensions'],
    package_data={
        "pufferlib": [RAYLIB_NAME + '/lib/libraylib.a']
    },
    include_package_data=True,
    install_requires=install_requires,
    ext_modules = c_extensions + torch_extensions,
    cmdclass={
        "build_ext": BuildExt,
        "build_torch": TorchBuildExt,
        "build_c": CBuildExt,
    },
    include_dirs=[numpy.get_include(), RAYLIB_NAME + '/include'],
)



================================================
SYMLINK: config -> config
================================================



================================================
FILE: examples/gym_env.py
================================================
import gym
import pufferlib.emulation

class SampleGymEnv(gym.Env):
    def __init__(self):
        self.observation_space = gym.spaces.Box(low=-1, high=1, shape=(1,))
        self.action_space = gym.spaces.Discrete(2)

    def reset(self):
        return self.observation_space.sample()

    def step(self, action):
        return self.observation_space.sample(), 0.0, False, {}

if __name__ == '__main__':
    gym_env = SampleGymEnv()
    gymnasium_env = pufferlib.GymToGymnasium(gym_env)
    puffer_env = pufferlib.emulation.GymnasiumPufferEnv(gymnasium_env)
    observations, info = puffer_env.reset()
    action = puffer_env.action_space.sample()
    observation, reward, terminal, truncation, info = puffer_env.step(action)



================================================
FILE: examples/gymnasium_env.py
================================================
import gymnasium
import pufferlib.emulation

class SampleGymnasiumEnv(gymnasium.Env):
    def __init__(self):
        self.observation_space = gymnasium.spaces.Box(low=-1, high=1, shape=(1,))
        self.action_space = gymnasium.spaces.Discrete(2)

    def reset(self):
        return self.observation_space.sample(), {}

    def step(self, action):
        return self.observation_space.sample(), 0.0, False, False, {}

if __name__ == '__main__':
    gymnasium_env = SampleGymnasiumEnv()
    puffer_env = pufferlib.emulation.GymnasiumPufferEnv(gymnasium_env)
    observation, info = puffer_env.reset()
    action = puffer_env.action_space.sample()
    observation, reward, terminal, truncation, info = puffer_env.step(action)



================================================
FILE: examples/pettingzoo_env.py
================================================
import gymnasium
import pettingzoo
import pufferlib.emulation

class SamplePettingzooEnv(pettingzoo.ParallelEnv):
    def __init__(self):
        self.possible_agents = ['agent_0', 'agent_1']
        self.agents = ['agent_0', 'agent_1']

    def observation_space(self, agent):
        return gymnasium.spaces.Box(low=-1, high=1, shape=(1,))

    def action_space(self, agent):
        return gymnasium.spaces.Discrete(2)

    def reset(self, seed=None, options=None):
        observations = {agent: self.observation_space(agent).sample() for agent in self.agents}
        return observations, {}

    def step(self, action):
        observations = {agent: self.observation_space(agent).sample() for agent in self.agents}
        rewards = {agent: 0.0 for agent in self.agents}
        terminals = {agent: False for agent in self.agents}
        truncations = {agent: False for agent in self.agents}
        infos = {agent: {} for agent in self.agents}
        return observations, rewards, terminals, truncations, infos

if __name__ == '__main__':
    env = SamplePettingzooEnv()
    puffer_env = pufferlib.emulation.PettingZooPufferEnv(env)
    observations, infos = puffer_env.reset()
    actions = {agent: puffer_env.action_space(agent).sample() for agent in puffer_env.agents}
    observations, rewards, terminals, truncations, infos = puffer_env.step(actions)



================================================
FILE: examples/puffer_env.py
================================================
import gymnasium
import pufferlib.emulation

class SamplePufferEnv(pufferlib.PufferEnv):
    def __init__(self, buf=None, seed=0):
        self.single_observation_space = gymnasium.spaces.Box(low=-1, high=1, shape=(1,))
        self.single_action_space = gymnasium.spaces.Discrete(2)
        self.num_agents = 2
        super().__init__(buf)

    def reset(self, seed=0):
        self.observations[:] = self.observation_space.sample()
        return self.observations, []

    def step(self, action):
        self.observations[:] = self.observation_space.sample()
        infos = [{'infos': 'is a list of dictionaries'}]
        return self.observations, self.rewards, self.terminals, self.truncations, infos

if __name__ == '__main__':
    puffer_env = SamplePufferEnv()
    observations, infos = puffer_env.reset()
    actions = puffer_env.action_space.sample()
    observations, rewards, terminals, truncations, infos = puffer_env.step(actions)
    print('Puffer envs use a vector interface and in-place array updates')
    print('Observation:', observations)
    print('Reward:', rewards)
    print('Terminal:', terminals)
    print('Truncation:', truncations)



================================================
FILE: examples/pufferl.py
================================================
import torch
import pufferlib.vector
import pufferlib.ocean
from pufferlib import pufferl


# Equivalent to running puffer train puffer_breakout
def cli():
    pufferl.train('puffer_breakout')

class Policy(torch.nn.Module):
    def __init__(self, env):
        super().__init__()
        self.net = torch.nn.Sequential(
            pufferlib.pytorch.layer_init(torch.nn.Linear(env.single_observation_space.shape[0], 128)),
            torch.nn.ReLU(),
            pufferlib.pytorch.layer_init(torch.nn.Linear(128, 128)),
        )
        self.action_head = torch.nn.Linear(128, env.single_action_space.n)
        self.value_head = torch.nn.Linear(128, 1)

    def forward_eval(self, observations, state=None):
        hidden = self.net(observations)
        logits = self.action_head(hidden)
        values = self.value_head(hidden)
        return logits, values

    # We use this to work around a major torch perf issue
    def forward(self, observations, state=None):
        return self.forward_eval(observations, state)

# Managing your own trainer
if __name__ == '__main__':
    env_name = 'puffer_breakout'
    env_creator = pufferlib.ocean.env_creator(env_name)
    vecenv = pufferlib.vector.make(env_creator, num_envs=2, num_workers=2, batch_size=1,
        backend=pufferlib.vector.Multiprocessing, env_kwargs={'num_envs': 4096})
    policy = Policy(vecenv.driver_env).cuda()
    args = pufferl.load_config('default')
    args['train']['env'] = env_name

    trainer = pufferl.PuffeRL(args['train'], vecenv, policy)

    for epoch in range(10):
        trainer.evaluate()
        logs = trainer.train()

    trainer.print_dashboard()
    trainer.close()



================================================
FILE: examples/render.py
================================================

from pufferlib.ocean.breakout import breakout
env = breakout.Breakout()
env.reset()
while True:
    env.step(env.action_space.sample())
    frame = env.render()




================================================
FILE: examples/structured_env.py
================================================
import gymnasium
import pufferlib.emulation

class SampleGymnasiumEnv(gymnasium.Env):
    def __init__(self):
        self.observation_space = gymnasium.spaces.Dict({
            'foo': gymnasium.spaces.Box(low=-1, high=1, shape=(2,)),
            'bar': gymnasium.spaces.Box(low=2, high=3, shape=(3,)),
        })
        self.action_space = gymnasium.spaces.MultiDiscrete([2, 5])

    def reset(self):
        return self.observation_space.sample(), {}

    def step(self, action):
        return self.observation_space.sample(), 0.0, False, False, {}

if __name__ == '__main__':
    gymnasium_env = SampleGymnasiumEnv()
    puffer_env = pufferlib.emulation.GymnasiumPufferEnv(gymnasium_env)
    flat_observation, info = puffer_env.reset()
    flat_action = puffer_env.action_space.sample()
    flat_observation, reward, terminal, truncation, info = puffer_env.step(flat_action)
    print(f'PufferLib flattens observations and actions:\n{flat_observation}\n{flat_action}')

    observation = flat_observation.view(puffer_env.obs_dtype)
    print(f'You can unflatten observations with numpy:\n{observation}')

    import torch
    import pufferlib.pytorch
    flat_torch_observation = torch.from_numpy(flat_observation)
    torch_dtype = pufferlib.pytorch.nativize_dtype(puffer_env.emulated)
    torch_observation = pufferlib.pytorch.nativize_tensor(flat_torch_observation, torch_dtype)
    print(f'But we suggest unflattening observations with torch in your model forward pass:\n{torch_observation}')



================================================
FILE: examples/vectorization.py
================================================
import gymnasium
import pufferlib.emulation
import pufferlib.vector

class SamplePufferEnv(pufferlib.PufferEnv):
    def __init__(self, foo=0, bar=1, buf=None, seed=0):
        self.single_observation_space = gymnasium.spaces.Box(low=-1, high=1, shape=(1,))
        self.single_action_space = gymnasium.spaces.Discrete(2)
        self.num_agents = 2
        super().__init__(buf)

        # Sample args and kwargs
        self.foo = foo
        self.bar = bar

    def reset(self, seed=0):
        self.observations[:] = self.observation_space.sample()
        return self.observations, []

    def step(self, action):
        self.observations[:] = self.observation_space.sample()
        infos = [{'infos': 'is a list of dictionaries'}]
        return self.observations, self.rewards, self.terminals, self.truncations, infos

    def close(self):
        pass

if __name__ == '__main__':
    serial_vecenv = pufferlib.vector.make(
        SamplePufferEnv, num_envs=2, backend=pufferlib.vector.Serial)
    observations, infos = serial_vecenv.reset()
    actions = serial_vecenv.action_space.sample()
    o, r, d, t, i = serial_vecenv.step(actions)
    print('Serial VecEnv:')
    print('Observations:', o)
    print('Rewards:', r)
    print('Terminals:', t)
    print('Truncations:', d)

    # Pass arguments to all environments like this
    serial_vecenv = pufferlib.vector.make(
        SamplePufferEnv, num_envs=2, backend=pufferlib.vector.Serial,
        env_args=[3], env_kwargs={'bar': 4}
    )
    print('Foo: ', [env.foo for env in serial_vecenv.envs])
    print('Bar: ', [env.bar for env in serial_vecenv.envs])

    # Or to each environment like this
    serial_vecenv = pufferlib.vector.make(
        [SamplePufferEnv, SamplePufferEnv], num_envs=2, backend=pufferlib.vector.Serial,
        env_args=[[3], [4]], env_kwargs=[{'bar': 4}, {'bar': 5}]
    )
    print('Foo: ', [env.foo for env in serial_vecenv.envs])
    print('Bar: ', [env.bar for env in serial_vecenv.envs])

    vecenv = pufferlib.vector.make(SamplePufferEnv,
        num_envs=2, num_workers=2, batch_size=1, backend=pufferlib.vector.Multiprocessing)
    vecenv.async_reset() # You can also use the synchronous API with Multiprocessing
    o, r, d, t, i, env_ids, masks = vecenv.recv()
    actions = vecenv.action_space.sample()
    print('Policy computes actions for all agents in batch_size=1 of the total num_envs=2 environments')
    print('Actions:', actions)
    vecenv.send(actions)

    # New observations are ready while the other envs are running in the background
    o, r, d, t, i, env_ids, masks = vecenv.recv()
    print('Observations:', o)

    # Make sure to close the vecenv when you're done
    vecenv.close()

    try:
        vecenv = pufferlib.vector.make(SamplePufferEnv,
            num_envs=1, num_workers=2, batch_size=3, backend=pufferlib.vector.Multiprocessing)
    except pufferlib.APIUsageError:
        #Make sure num_envs divides num_workers, and both num_envs and num_workers should divide batch_size
        pass
    



================================================
FILE: pufferlib/__init__.py
================================================
__version__ = 3.0

import os
path = __path__[0]
link_to = os.path.join(path, 'resources')
try:
    os.symlink(link_to, 'resources')
except FileExistsError:
    pass

# Silence noisy dependencies
import warnings
warnings.filterwarnings("ignore", category=DeprecationWarning)

# Silence noisy packages
import sys
original_stdout = sys.stdout
original_stderr = sys.stderr
sys.stdout = open(os.devnull, 'w')
sys.stderr = open(os.devnull, 'w')
try:
    import gymnasium
    import pygame
except ImportError:
    pass
sys.stdout.close()
sys.stderr.close()
sys.stdout = original_stdout
sys.stderr = original_stderr

from pufferlib.pufferlib import *
from pufferlib import environments



================================================
FILE: pufferlib/cleanrl_ppo_atari.py
================================================
# docs and experiment results can be found at https://docs.cleanrl.dev/rl-algorithms/ppo/#ppo_ataripy
import os
import random
import time
from dataclasses import dataclass

import gymnasium as gym
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
import tyro
from torch.distributions.categorical import Categorical
from torch.utils.tensorboard import SummaryWriter

from stable_baselines3.common.atari_wrappers import (  # isort:skip
    ClipRewardEnv,
    EpisodicLifeEnv,
    FireResetEnv,
    MaxAndSkipEnv,
    NoopResetEnv,
)

@dataclass
class Args:
    exp_name: str = 'cleanrl_ppo_atari'
    """the name of this experiment"""
    seed: int = 1
    """seed of the experiment"""
    torch_deterministic: bool = True
    """if toggled, `torch.backends.cudnn.deterministic=False`"""
    cuda: bool = True
    """if toggled, cuda will be enabled by default"""
    track: bool = False
    """if toggled, this experiment will be tracked with Weights and Biases"""
    wandb_project_name: str = "cleanRL"
    """the wandb's project name"""
    wandb_entity: str = None
    """the entity (team) of wandb's project"""
    capture_video: bool = False
    """whether to capture videos of the agent performances (check out `videos` folder)"""

    # Algorithm specific arguments
    env_id: str = "breakout"
    """the id of the environment"""
    total_timesteps: int = 10000000
    """total timesteps of the experiments"""
    learning_rate: float = 2.5e-4
    """the learning rate of the optimizer"""
    num_envs: int = 8
    """the number of parallel game environments"""
    num_steps: int = 128
    """the number of steps to run in each environment per policy rollout"""
    anneal_lr: bool = True
    """Toggle learning rate annealing for policy and value networks"""
    gamma: float = 0.99
    """the discount factor gamma"""
    gae_lambda: float = 0.95
    """the lambda for the general advantage estimation"""
    num_minibatches: int = 4
    """the number of mini-batches"""
    update_epochs: int = 4
    """the K epochs to update the policy"""
    norm_adv: bool = True
    """Toggles advantages normalization"""
    clip_coef: float = 0.1
    """the surrogate clipping coefficient"""
    clip_vloss: bool = True
    """Toggles whether or not to use a clipped loss for the value function, as per the paper."""
    ent_coef: float = 0.01
    """coefficient of the entropy"""
    vf_coef: float = 0.5
    """coefficient of the value function"""
    max_grad_norm: float = 0.5
    """the maximum norm for the gradient clipping"""
    target_kl: float = None
    """the target KL divergence threshold"""

    # to be filled in runtime
    batch_size: int = 0
    """the batch size (computed in runtime)"""
    minibatch_size: int = 0
    """the mini-batch size (computed in runtime)"""
    num_iterations: int = 0
    """the number of iterations (computed in runtime)"""


def make_env(env_id, idx, capture_video, run_name):
    def thunk():
        if capture_video and idx == 0:
            env = gym.make(env_id, render_mode="rgb_array")
            env = gym.wrappers.RecordVideo(env, f"videos/{run_name}")
        else:
            env = gym.make(env_id)
        env = gym.wrappers.RecordEpisodeStatistics(env)
        if capture_video:
            if idx == 0:
                env = gym.wrappers.RecordVideo(env, f"videos/{run_name}")
        env = NoopResetEnv(env, noop_max=30)
        env = MaxAndSkipEnv(env, skip=4)
        env = EpisodicLifeEnv(env)
        if "FIRE" in env.unwrapped.get_action_meanings():
            env = FireResetEnv(env)
        env = ClipRewardEnv(env)
        env = gym.wrappers.ResizeObservation(env, (84, 84))
        env = gym.wrappers.GrayScaleObservation(env)
        env = gym.wrappers.FrameStack(env, 4)
        return env

    return thunk


def layer_init(layer, std=np.sqrt(2), bias_const=0.0):
    torch.nn.init.orthogonal_(layer.weight, std)
    torch.nn.init.constant_(layer.bias, bias_const)
    return layer


class Agent(nn.Module):
    def __init__(self, envs):
        super().__init__()
        self.network = nn.Sequential(
            layer_init(nn.Conv2d(4, 32, 8, stride=4)),
            nn.ReLU(),
            layer_init(nn.Conv2d(32, 64, 4, stride=2)),
            nn.ReLU(),
            layer_init(nn.Conv2d(64, 64, 3, stride=1)),
            nn.ReLU(),
            nn.Flatten(),
            layer_init(nn.Linear(64 * 6 * 9, 512)),
            nn.ReLU(),
        )
        self.actor = layer_init(nn.Linear(512, envs.single_action_space.n), std=0.01)
        self.critic = layer_init(nn.Linear(512, 1), std=1)

    def get_value(self, x):
        x = x.permute(0, 2, 1, 3)
        return self.critic(self.network(x / 255.0))

    def get_action_and_value(self, x, action=None):
        x = x.permute(0, 2, 1, 3)
        hidden = self.network(x / 255.0)
        logits = self.actor(hidden)
        probs = Categorical(logits=logits)
        if action is None:
            action = probs.sample()
        return action, probs.log_prob(action), probs.entropy(), self.critic(hidden)


if __name__ == "__main__":
    args, _ = tyro.cli(Args, return_unknown_args=True)
    args.batch_size = int(args.num_envs * args.num_steps)
    args.minibatch_size = int(args.batch_size // args.num_minibatches)
    args.num_iterations = args.total_timesteps // args.batch_size
    run_name = f"{args.env_id}__{args.exp_name}__{args.seed}__{int(time.time())}"
    if args.track:
        import wandb

        wandb.init(
            project=args.wandb_project_name,
            entity=args.wandb_entity,
            sync_tensorboard=True,
            config=vars(args),
            name=run_name,
            monitor_gym=True,
            save_code=True,
        )
    writer = SummaryWriter(f"runs/{run_name}")
    writer.add_text(
        "hyperparameters",
        "|param|value|\n|-|-|\n%s" % ("\n".join([f"|{key}|{value}|" for key, value in vars(args).items()])),
    )

    # TRY NOT TO MODIFY: seeding
    random.seed(args.seed)
    np.random.seed(args.seed)
    torch.manual_seed(args.seed)
    torch.backends.cudnn.deterministic = args.torch_deterministic

    device = torch.device("cuda" if torch.cuda.is_available() and args.cuda else "cpu")

    # PufferLib vectorization makes CleanRL ~65% faster!
    import pufferlib.vector
    import pufferlib.environments.atari
    envs = pufferlib.vector.make(
        pufferlib.environments.atari.env_creator(args.env_id),
        env_kwargs=dict(framestack=4),
        backend=pufferlib.vector.Multiprocessing,
        num_envs=args.num_envs,
    )

    agent = Agent(envs).to(device)
    optimizer = optim.Adam(agent.parameters(), lr=args.learning_rate, eps=1e-5)

    # ALGO Logic: Storage setup
    obs = torch.zeros((args.num_steps, args.num_envs) + envs.single_observation_space.shape).to(device)
    actions = torch.zeros((args.num_steps, args.num_envs) + envs.single_action_space.shape).to(device)
    logprobs = torch.zeros((args.num_steps, args.num_envs)).to(device)
    rewards = torch.zeros((args.num_steps, args.num_envs)).to(device)
    dones = torch.zeros((args.num_steps, args.num_envs)).to(device)
    values = torch.zeros((args.num_steps, args.num_envs)).to(device)

    # TRY NOT TO MODIFY: start the game
    global_step = 0
    start_time = time.time()
    next_obs, _ = envs.reset(seed=args.seed)
    next_obs = torch.Tensor(next_obs).to(device)
    next_done = torch.zeros(args.num_envs).to(device)

    for iteration in range(1, args.num_iterations + 1):
        # Annealing the rate if instructed to do so.
        if args.anneal_lr:
            frac = 1.0 - (iteration - 1.0) / args.num_iterations
            lrnow = frac * args.learning_rate
            optimizer.param_groups[0]["lr"] = lrnow

        for step in range(0, args.num_steps):
            global_step += args.num_envs
            obs[step] = next_obs
            dones[step] = next_done

            # ALGO LOGIC: action logic
            with torch.no_grad():
                action, logprob, _, value = agent.get_action_and_value(next_obs)
                values[step] = value.flatten()
            actions[step] = action
            logprobs[step] = logprob

            # TRY NOT TO MODIFY: execute the game and log data.
            next_obs, reward, terminations, truncations, infos = envs.step(action.cpu().numpy())
            next_done = np.logical_or(terminations, truncations)
            rewards[step] = torch.tensor(reward).to(device).view(-1)
            next_obs, next_done = torch.from_numpy(next_obs).to(device), torch.from_numpy(next_done).to(device).float()

            for item in infos:
                if "episode" in item.keys():
                    print(f"global_step={global_step}, episodic_return={item['episode']['r']}")
                    writer.add_scalar("charts/episodic_return", item["episode"]["r"], global_step)
                    writer.add_scalar("charts/episodic_length", item["episode"]["l"], global_step)
                    break

        # bootstrap value if not done
        with torch.no_grad():
            next_value = agent.get_value(next_obs).reshape(1, -1)
            advantages = torch.zeros_like(rewards).to(device)
            lastgaelam = 0
            for t in reversed(range(args.num_steps)):
                if t == args.num_steps - 1:
                    nextnonterminal = 1.0 - next_done
                    nextvalues = next_value
                else:
                    nextnonterminal = 1.0 - dones[t + 1]
                    nextvalues = values[t + 1]
                delta = rewards[t] + args.gamma * nextvalues * nextnonterminal - values[t]
                advantages[t] = lastgaelam = delta + args.gamma * args.gae_lambda * nextnonterminal * lastgaelam
            returns = advantages + values

        # flatten the batch
        b_obs = obs.reshape((-1,) + envs.single_observation_space.shape)
        b_logprobs = logprobs.reshape(-1)
        b_actions = actions.reshape((-1,) + envs.single_action_space.shape)
        b_advantages = advantages.reshape(-1)
        b_returns = returns.reshape(-1)
        b_values = values.reshape(-1)

        # Optimizing the policy and value network
        b_inds = np.arange(args.batch_size)
        clipfracs = []
        for epoch in range(args.update_epochs):
            np.random.shuffle(b_inds)
            for start in range(0, args.batch_size, args.minibatch_size):
                end = start + args.minibatch_size
                mb_inds = b_inds[start:end]

                _, newlogprob, entropy, newvalue = agent.get_action_and_value(b_obs[mb_inds], b_actions.long()[mb_inds])
                logratio = newlogprob - b_logprobs[mb_inds]
                ratio = logratio.exp()

                with torch.no_grad():
                    # calculate approx_kl http://joschu.net/blog/kl-approx.html
                    old_approx_kl = (-logratio).mean()
                    approx_kl = ((ratio - 1) - logratio).mean()
                    clipfracs += [((ratio - 1.0).abs() > args.clip_coef).float().mean().item()]

                mb_advantages = b_advantages[mb_inds]
                if args.norm_adv:
                    mb_advantages = (mb_advantages - mb_advantages.mean()) / (mb_advantages.std() + 1e-8)

                # Policy loss
                pg_loss1 = -mb_advantages * ratio
                pg_loss2 = -mb_advantages * torch.clamp(ratio, 1 - args.clip_coef, 1 + args.clip_coef)
                pg_loss = torch.max(pg_loss1, pg_loss2).mean()

                # Value loss
                newvalue = newvalue.view(-1)
                if args.clip_vloss:
                    v_loss_unclipped = (newvalue - b_returns[mb_inds]) ** 2
                    v_clipped = b_values[mb_inds] + torch.clamp(
                        newvalue - b_values[mb_inds],
                        -args.clip_coef,
                        args.clip_coef,
                    )
                    v_loss_clipped = (v_clipped - b_returns[mb_inds]) ** 2
                    v_loss_max = torch.max(v_loss_unclipped, v_loss_clipped)
                    v_loss = 0.5 * v_loss_max.mean()
                else:
                    v_loss = 0.5 * ((newvalue - b_returns[mb_inds]) ** 2).mean()

                entropy_loss = entropy.mean()
                loss = pg_loss - args.ent_coef * entropy_loss + v_loss * args.vf_coef

                optimizer.zero_grad()
                loss.backward()
                nn.utils.clip_grad_norm_(agent.parameters(), args.max_grad_norm)
                optimizer.step()

            if args.target_kl is not None and approx_kl > args.target_kl:
                break

        y_pred, y_true = b_values.cpu().numpy(), b_returns.cpu().numpy()
        var_y = np.var(y_true)
        explained_var = np.nan if var_y == 0 else 1 - np.var(y_true - y_pred) / var_y

        # TRY NOT TO MODIFY: record rewards for plotting purposes
        writer.add_scalar("charts/learning_rate", optimizer.param_groups[0]["lr"], global_step)
        writer.add_scalar("losses/value_loss", v_loss.item(), global_step)
        writer.add_scalar("losses/policy_loss", pg_loss.item(), global_step)
        writer.add_scalar("losses/entropy", entropy_loss.item(), global_step)
        writer.add_scalar("losses/old_approx_kl", old_approx_kl.item(), global_step)
        writer.add_scalar("losses/approx_kl", approx_kl.item(), global_step)
        writer.add_scalar("losses/clipfrac", np.mean(clipfracs), global_step)
        writer.add_scalar("losses/explained_variance", explained_var, global_step)
        print("SPS:", int(global_step / (time.time() - start_time)))
        writer.add_scalar("charts/SPS", int(global_step / (time.time() - start_time)), global_step)

    envs.close()
    writer.close()



================================================
FILE: pufferlib/emulation.py
================================================
from pdb import set_trace as T

import numpy as np
import warnings

import gymnasium
import inspect

import pufferlib
import pufferlib.spaces
from pufferlib.spaces import Discrete, Tuple, Dict

def emulate(struct, sample):
    if isinstance(sample, dict):
        for k, v in sample.items():
            emulate(struct[k], v)
    elif isinstance(sample, tuple):
        for i, v in enumerate(sample):
            emulate(struct[f'f{i}'], v)
    else:
        struct[()] = sample

def make_buffer(arr_dtype, struct_dtype, struct, n=None):
    '''None instead of 1 makes it work for 1 agent PZ envs'''
    '''
    if n is None:
        struct = np.zeros(1, dtype=struct_dtype)
    else:
        struct = np.zeros(n, dtype=struct_dtype)
    '''

    arr = struct.view(arr_dtype)

    if n is None:
        arr = arr.ravel()
    else:
        arr = arr.reshape(n, -1)

    return arr

def _nativize(struct, space):
    if isinstance(space, Discrete):
        return struct.item()
    elif isinstance(space, Tuple):
        return tuple(_nativize(struct[f'f{i}'], elem)
            for i, elem in enumerate(space))
    elif isinstance(space, Dict):
        return {k: _nativize(struct[k], value)
            for k, value in space.items()}
    else:
        return struct

def nativize(arr, space, struct_dtype):
    struct = np.asarray(arr).view(struct_dtype)[0]
    return _nativize(struct, space)

# TODO: Uncomment?
'''
try:
    from pufferlib.extensions import emulate, nativize
except ImportError:
    warnings.warn('PufferLib Cython extensions not installed. Using slow Python versions')
'''

def get_dtype_bounds(dtype):
    if dtype == bool:
        return 0, 1
    elif np.issubdtype(dtype, np.integer):
        return np.iinfo(dtype).min, np.iinfo(dtype).max
    elif np.issubdtype(dtype, np.unsignedinteger):
        return np.iinfo(dtype).min, np.iinfo(dtype).max
    elif np.issubdtype(dtype, np.floating):
        # Gym fails on float64
        return np.finfo(np.float32).min, np.finfo(np.float32).max
    else:
        raise ValueError(f"Unsupported dtype: {dtype}")


def dtype_from_space(space):
    if isinstance(space, pufferlib.spaces.Tuple):
        dtype = []
        for i, elem in enumerate(space):
            dtype.append((f'f{i}', dtype_from_space(elem)))
    elif isinstance(space, pufferlib.spaces.Dict):
        dtype = []
        for k, value in space.items():
            dtype.append((k, dtype_from_space(value)))
    elif isinstance(space, (pufferlib.spaces.Discrete)):
        dtype = (np.int32, ())
    elif isinstance(space, (pufferlib.spaces.MultiDiscrete)):
        dtype = (np.int32, (len(space.nvec),))
    else:
        dtype = (space.dtype, space.shape)

    return np.dtype(dtype, align=True)

def flatten_space(space):
    if isinstance(space, pufferlib.spaces.Tuple):
        subspaces = []
        for e in space:
            subspaces.extend(flatten_space(e))
        return subspaces
    elif isinstance(space, pufferlib.spaces.Dict):
        subspaces = []
        for e in space.values():
            subspaces.extend(flatten_space(e))
        return subspaces
    else:
        return [space]

def emulate_observation_space(space):
    emulated_dtype = dtype_from_space(space)

    if isinstance(space, pufferlib.spaces.Box):
        return space, emulated_dtype

    leaves = flatten_space(space)
    dtypes = [e.dtype for e in leaves]
    if dtypes.count(dtypes[0]) == len(dtypes):
        dtype = dtypes[0]
    else:
        dtype = np.dtype(np.uint8)

    mmin, mmax = get_dtype_bounds(dtype)
    numel = emulated_dtype.itemsize // dtype.itemsize
    emulated_space = gymnasium.spaces.Box(low=mmin, high=mmax, shape=(numel,), dtype=dtype)
    return emulated_space, emulated_dtype

def emulate_action_space(space):
    if isinstance(space, pufferlib.spaces.Box):
        return space, space.dtype
    elif isinstance(space, (pufferlib.spaces.Discrete, pufferlib.spaces.MultiDiscrete)):
        return space, np.int32

    emulated_dtype = dtype_from_space(space)
    leaves = flatten_space(space)
    emulated_space = gymnasium.spaces.MultiDiscrete([e.n for e in leaves])
    return emulated_space, emulated_dtype


class GymnasiumPufferEnv(gymnasium.Env):
    def __init__(self, env=None, env_creator=None, env_args=[], env_kwargs={}, buf=None, seed=0):
        self.env = make_object(env, env_creator, env_args, env_kwargs)

        self.initialized = False
        self.done = True

        self.is_observation_checked = False
        self.is_action_checked = False

        self.observation_space, self.obs_dtype = emulate_observation_space(
            self.env.observation_space)
        self.action_space, self.atn_dtype = emulate_action_space(
            self.env.action_space)
        self.single_observation_space = self.observation_space
        self.single_action_space = self.action_space
        self.num_agents = 1

        self.is_obs_emulated = self.single_observation_space is not self.env.observation_space
        self.is_atn_emulated = self.single_action_space is not self.env.action_space
        self.emulated = dict(
            observation_dtype=self.observation_space.dtype,
            emulated_observation_dtype=self.obs_dtype,
        )

        self.render_modes = 'human rgb_array'.split()

        pufferlib.set_buffers(self, buf)
        if isinstance(self.env.observation_space, pufferlib.spaces.Box):
            self.obs_struct = self.observations
        else:
            self.obs_struct = self.observations.view(self.obs_dtype)
 
    @property
    def render_mode(self):
        return self.env.render_mode

    def seed(self, seed):
        self.env.seed(seed)

    def reset(self, seed=None):
        self.initialized = True
        self.done = False

        ob, info = _seed_and_reset(self.env, seed)
        if not self.is_observation_checked:
            self.is_observation_checked = check_space(
                ob, self.env.observation_space)

        if self.is_obs_emulated:
            emulate(self.obs_struct, ob)
        else:
            self.observations[:] = ob

        self.rewards[0] = 0
        self.terminals[0] = False
        self.truncations[0] = False
        self.masks[0] = True
 
        return self.observations, info
 
    def step(self, action):
        '''Execute an action and return (observation, reward, done, info)'''
        if not self.initialized:
            raise pufferlib.APIUsageError('step() called before reset()')
        if self.done:
            raise pufferlib.APIUsageError('step() called after environment is done')

        # Unpack actions from multidiscrete into the original action space
        if self.is_atn_emulated:
            action = nativize(action, self.env.action_space, self.atn_dtype)
        elif isinstance(action, np.ndarray):
            action = action.ravel()
            # TODO: profile or speed up
            if isinstance(self.action_space, pufferlib.spaces.Discrete):
                action = action[0]

        if not self.is_action_checked:
            self.is_action_checked = check_space(
                action, self.env.action_space)

        ob, reward, done, truncated, info = self.env.step(action)


        if self.is_obs_emulated:
            emulate(self.obs_struct, ob)
        else:
            self.observations[:] = ob

        self.rewards[0] = reward
        self.terminals[0] = done
        self.truncations[0] = truncated
        self.masks[0] = True
                  
        self.done = done or truncated
        return self.observations, reward, done, truncated, info

    def render(self):
        return self.env.render()

    def close(self):
        return self.env.close()

class PettingZooPufferEnv:
    def __init__(self, env=None, env_creator=None, env_args=[], env_kwargs={}, buf=None, seed=0):
        self.env = make_object(env, env_creator, env_args, env_kwargs)
        self.initialized = False
        self.all_done = True

        self.is_observation_checked = False
        self.is_action_checked = False

        # Compute the observation and action spaces
        single_agent = self.possible_agents[0]
        self.env_single_observation_space = self.env.observation_space(single_agent)
        self.env_single_action_space = self.env.action_space(single_agent)
        self.single_observation_space, self.obs_dtype = (
            emulate_observation_space(self.env_single_observation_space))
        self.single_action_space, self.atn_dtype = (
            emulate_action_space(self.env_single_action_space))
        self.is_obs_emulated = self.single_observation_space is not self.env_single_observation_space
        self.is_atn_emulated = self.single_action_space is not self.env_single_action_space
        self.emulated = dict(
            observation_dtype = self.single_observation_space.dtype,
            emulated_observation_dtype = self.obs_dtype,
        )

        self.num_agents = len(self.possible_agents)

        pufferlib.set_buffers(self, buf)
        if isinstance(self.env_single_observation_space, pufferlib.spaces.Box):
            self.obs_struct = self.observations
        else:
            self.obs_struct = self.observations.view(self.obs_dtype)

    @property
    def render_mode(self):
        return self.env.render_mode

    @property
    def agents(self):
        return self.env.agents

    @property
    def possible_agents(self):
        return self.env.possible_agents

    @property
    def done(self):
        return len(self.agents) == 0 or self.all_done

    def observation_space(self, agent):
        '''Returns the observation space for a single agent'''
        if agent not in self.possible_agents:
            raise pufferlib.InvalidAgentError(agent, self.possible_agents)

        return self.single_observation_space

    def action_space(self, agent):
        '''Returns the action space for a single agent'''
        if agent not in self.possible_agents:
            raise pufferlib.InvalidAgentError(agent, self.possible_agents)

        return self.single_action_space

    def reset(self, seed=None):
        if not self.initialized:
            self.dict_obs = {agent: self.observations[i] for i, agent in enumerate(self.possible_agents)}

        self.initialized = True
        self.all_done = False
        self.mask = {k: False for k in self.possible_agents}

        obs, info = self.env.reset(seed=seed)

        if not self.is_observation_checked:
            for k, ob in obs.items():
                self.is_observation_checked = check_space(
                    ob, self.env.observation_space(k))

        # Call user featurizer and flatten the observations
        self.observations[:] = 0
        for i, agent in enumerate(self.possible_agents):
            if agent not in obs:
                continue

            ob = obs[agent]
            self.mask[agent] = True
            if self.is_obs_emulated:
                emulate(self.obs_struct[i], ob)
            else:
                self.observations[i] = ob

        self.rewards[:] = 0
        self.terminals[:] = False
        self.truncations[:] = False
        self.masks[:] = True
        return self.dict_obs, info

    def step(self, actions):
        '''Step the environment and return (observations, rewards, dones, infos)'''
        if not self.initialized:
            raise pufferlib.APIUsageError('step() called before reset()')
        if self.done:
            raise pufferlib.APIUsageError('step() called after environment is done')

        if isinstance(actions, np.ndarray):
            if not self.is_action_checked and len(actions) != self.num_agents:
                raise pufferlib.APIUsageError(
                    f'Actions specified as len {len(actions)} but environment has {self.num_agents} agents')

            actions = {agent: actions[i] for i, agent in enumerate(self.possible_agents)}

        # Postprocess actions and validate action spaces
        if not self.is_action_checked:
            for agent in actions:
                if agent not in self.possible_agents:
                    raise pufferlib.InvalidAgentError(agent, self.possible_agents)

            self.is_action_checked = check_space(
                next(iter(actions.values())),
                self.single_action_space
            )

        # Unpack actions from multidiscrete into the original action space
        unpacked_actions = {}
        for agent, atn in actions.items():
            if agent not in self.possible_agents:
                raise pufferlib.InvalidAgentError(agent, self.agents)

            if agent not in self.agents:
                continue

            if self.is_atn_emulated:
                atn = nativize(atn, self.env_single_action_space, self.atn_dtype)

            unpacked_actions[agent] = atn

        obs, rewards, dones, truncateds, infos = self.env.step(unpacked_actions)
        # TODO: Can add this assert once NMMO Horizon is ported to puffer
        # assert all(dones.values()) == (len(self.env.agents) == 0)
        self.mask = {k: False for k in self.possible_agents}
        self.rewards[:] = 0
        self.terminals[:] = True
        self.truncations[:] = False
        for i, agent in enumerate(self.possible_agents):
            # TODO: negative padding buf
            if agent not in obs:
                self.observations[i] = 0
                self.rewards[i] = 0
                self.terminals[i] = True
                self.truncations[i] = False
                self.masks[i] = False
                continue

            ob = obs[agent] 
            self.mask[agent] = True
            if self.is_obs_emulated:
                emulate(self.obs_struct[i], ob)
            else:
                self.observations[i] = ob

            self.rewards[i] = rewards[agent]
            self.terminals[i] = dones[agent]
            self.truncations[i] = truncateds[agent]
            self.masks[i] = True
     
        self.all_done = all(dones.values()) or all(truncateds.values())
        rewards = pad_agent_data(rewards, self.possible_agents, 0)
        dones = pad_agent_data(dones, self.possible_agents, True) # You changed this from false to match api test... is this correct?
        truncateds = pad_agent_data(truncateds, self.possible_agents, False)
        return self.dict_obs, rewards, dones, truncateds, infos

    def render(self):
        return self.env.render()

    def close(self):
        return self.env.close()

def pad_agent_data(data, agents, pad_value):
    return {agent: data[agent] if agent in data else pad_value
        for agent in agents}
 
def make_object(object_instance=None, object_creator=None, creator_args=[], creator_kwargs={}):
    if (object_instance is None) == (object_creator is None):
        raise ValueError('Exactly one of object_instance or object_creator must be provided')

    if object_instance is not None:
        if callable(object_instance) or inspect.isclass(object_instance):
            raise TypeError('object_instance must be an instance, not a function or class')
        return object_instance

    if object_creator is not None:
        if not callable(object_creator):
            raise TypeError('object_creator must be a callable')
        
        if creator_args is None:
            creator_args = []

        if creator_kwargs is None:
            creator_kwargs = {}

        return object_creator(*creator_args, **creator_kwargs)

def check_space(data, space):
    try:
        contains = space.contains(data)
    except:
        raise pufferlib.APIUsageError(
            f'Error checking space {space} with sample :\n{data}')

    if not contains:
        raise pufferlib.APIUsageError(
            f'Data:\n{data}\n not in space:\n{space}')
    
    return True

def _seed_and_reset(env, seed):
    if seed is None:
        # Gym bug: does not reset env correctly
        # when seed is passed as explicit None
        return env.reset()

    try:
        obs, info = env.reset(seed=seed)
    except:
        try:
            env.seed(seed)
            obs, info = env.reset()
        except:
            obs, info = env.reset()
            warnings.warn('WARNING: Environment does not support seeding.', DeprecationWarning)

    return obs, info

class GymnaxPufferEnv(pufferlib.PufferEnv):
    def __init__(self, env, env_params, num_envs=1, buf=None):
        from gymnax.spaces import gymnax_space_to_gym_space

        gymnax_obs_space = env.observation_space(env_params)
        self.single_observation_space = gymnax_space_to_gym_space(gymnax_obs_space)

        gymnax_act_space = env.action_space(env_params)
        self.single_action_space = gymnax_space_to_gym_space(gymnax_act_space)

        self.num_agents = num_envs

        super().__init__(buf)
        self.env_params = env_params
        self.env = env

        import jax
        self.reset_fn = jax.jit(jax.vmap(env.reset, in_axes=(0, None)))
        self.step_fn = jax.jit(jax.vmap(env.step, in_axes=(0, 0, 0, None)))
        self.rng = jax.random.PRNGKey(0)

    def reset(self, rng, params=None):
        import jax
        self.rng, _rng = jax.random.split(self.rng)
        self.rngs = jax.random.split(_rng, self.num_agents)
        obs, self.state = self.reset_fn(self.rngs, params)
        from torch.utils import dlpack as torch_dlpack
        self.observations = torch_dlpack.from_dlpack(jax.dlpack.to_dlpack(obs))
        return self.observations, []

    def step(self, action):
        import jax
        #self.rng, _rng = jax.random.split(self.rng)
        #rngs = jax.random.split(_rng, self.num_agents)
        obs, self.state, reward, done, info = self.step_fn(self.rngs, self.state, action, self.env_params)

        # Convert JAX array to DLPack, then to PyTorch tensor
        from torch.utils import dlpack as torch_dlpack
        self.observations = torch_dlpack.from_dlpack(jax.dlpack.to_dlpack(obs))
        self.rewards = np.asarray(reward)
        self.terminals = np.asarray(done)
        infos = [{k: v.mean().item() for k, v in info.items()}]
        return self.observations, self.rewards, self.terminals, self.terminals, infos



================================================
FILE: pufferlib/models.py
================================================
from pdb import set_trace as T
import numpy as np

import torch
import torch.nn as nn

import pufferlib.emulation
import pufferlib.pytorch
import pufferlib.spaces


class Default(nn.Module):
    '''Default PyTorch policy. Flattens obs and applies a linear layer.

    PufferLib is not a framework. It does not enforce a base class.
    You can use any PyTorch policy that returns actions and values.
    We structure our forward methods as encode_observations and decode_actions
    to make it easier to wrap policies with LSTMs. You can do that and use
    our LSTM wrapper or implement your own. To port an existing policy
    for use with our LSTM wrapper, simply put everything from forward() before
    the recurrent cell into encode_observations and put everything after
    into decode_actions.
    '''
    def __init__(self, env, hidden_size=128):
        super().__init__()
        self.hidden_size = hidden_size
        self.is_multidiscrete = isinstance(env.single_action_space,
                pufferlib.spaces.MultiDiscrete)
        self.is_continuous = isinstance(env.single_action_space,
                pufferlib.spaces.Box)
        try:
            self.is_dict_obs = isinstance(env.env.observation_space, pufferlib.spaces.Dict) 
        except:
            self.is_dict_obs = isinstance(env.observation_space, pufferlib.spaces.Dict) 

        if self.is_dict_obs:
            self.dtype = pufferlib.pytorch.nativize_dtype(env.emulated)
            input_size = int(sum(np.prod(v.shape) for v in env.env.observation_space.values()))
            self.encoder = nn.Linear(input_size, self.hidden_size)
        else:
            num_obs = np.prod(env.single_observation_space.shape)
            self.encoder = torch.nn.Sequential(
                pufferlib.pytorch.layer_init(nn.Linear(num_obs, hidden_size)),
                nn.GELU(),
            )
            
        if self.is_multidiscrete:
            self.action_nvec = tuple(env.single_action_space.nvec)
            num_atns = sum(self.action_nvec)
            self.decoder = pufferlib.pytorch.layer_init(
                    nn.Linear(hidden_size, num_atns), std=0.01)
        elif not self.is_continuous:
            num_atns = env.single_action_space.n
            self.decoder = pufferlib.pytorch.layer_init(
                nn.Linear(hidden_size, num_atns), std=0.01)
        else:
            self.decoder_mean = pufferlib.pytorch.layer_init(
                nn.Linear(hidden_size, env.single_action_space.shape[0]), std=0.01)
            self.decoder_logstd = nn.Parameter(torch.zeros(
                1, env.single_action_space.shape[0]))

        self.value = pufferlib.pytorch.layer_init(
            nn.Linear(hidden_size, 1), std=1)

    def forward_eval(self, observations, state=None):
        hidden = self.encode_observations(observations, state=state)
        logits, values = self.decode_actions(hidden)
        return logits, values

    def forward(self, observations, state=None):
        return self.forward_eval(observations, state)

    def encode_observations(self, observations, state=None):
        '''Encodes a batch of observations into hidden states. Assumes
        no time dimension (handled by LSTM wrappers).'''
        batch_size = observations.shape[0]
        if self.is_dict_obs:
            observations = pufferlib.pytorch.nativize_tensor(observations, self.dtype)
            observations = torch.cat([v.view(batch_size, -1) for v in observations.values()], dim=1)
        else: 
            observations = observations.view(batch_size, -1)
        return self.encoder(observations.float())

    def decode_actions(self, hidden):
        '''Decodes a batch of hidden states into (multi)discrete actions.
        Assumes no time dimension (handled by LSTM wrappers).'''
        if self.is_multidiscrete:
            logits = self.decoder(hidden).split(self.action_nvec, dim=1)
        elif self.is_continuous:
            mean = self.decoder_mean(hidden)
            logstd = self.decoder_logstd.expand_as(mean)
            std = torch.exp(logstd)
            logits = torch.distributions.Normal(mean, std)
        else:
            logits = self.decoder(hidden)

        values = self.value(hidden)
        return logits, values

class LSTMWrapper(nn.Module):
    def __init__(self, env, policy, input_size=128, hidden_size=128):
        '''Wraps your policy with an LSTM without letting you shoot yourself in the
        foot with bad transpose and shape operations. This saves much pain.
        Requires that your policy define encode_observations and decode_actions.
        See the Default policy for an example.'''
        super().__init__()
        self.obs_shape = env.single_observation_space.shape

        self.policy = policy
        self.input_size = input_size
        self.hidden_size = hidden_size
        self.is_continuous = self.policy.is_continuous

        for name, param in self.named_parameters():
            if 'layer_norm' in name:
                continue
            if "bias" in name:
                nn.init.constant_(param, 0)
            elif "weight" in name and param.ndim >= 2:
                nn.init.orthogonal_(param, 1.0)

        self.lstm = nn.LSTM(input_size, hidden_size)

        self.cell = torch.nn.LSTMCell(input_size, hidden_size)
        self.cell.weight_ih = self.lstm.weight_ih_l0
        self.cell.weight_hh = self.lstm.weight_hh_l0
        self.cell.bias_ih = self.lstm.bias_ih_l0
        self.cell.bias_hh = self.lstm.bias_hh_l0

        #self.pre_layernorm = nn.LayerNorm(hidden_size)
        #self.post_layernorm = nn.LayerNorm(hidden_size)

    def forward_eval(self, observations, state):
        '''Forward function for inference. 3x faster than using LSTM directly'''
        hidden = self.policy.encode_observations(observations, state=state)
        h = state['lstm_h']
        c = state['lstm_c']

        # TODO: Don't break compile
        if h is not None:
            assert h.shape[0] == c.shape[0] == observations.shape[0], 'LSTM state must be (h, c)'
            lstm_state = (h, c)
        else:
            lstm_state = None

        #hidden = self.pre_layernorm(hidden)
        hidden, c = self.cell(hidden, lstm_state)
        #hidden = self.post_layernorm(hidden)
        state['hidden'] = hidden
        state['lstm_h'] = hidden
        state['lstm_c'] = c
        logits, values = self.policy.decode_actions(hidden)
        return logits, values

    def forward(self, observations, state):
        '''Forward function for training. Uses LSTM for fast time-batching'''
        x = observations
        lstm_h = state['lstm_h']
        lstm_c = state['lstm_c']

        x_shape, space_shape = x.shape, self.obs_shape
        x_n, space_n = len(x_shape), len(space_shape)
        if x_shape[-space_n:] != space_shape:
            raise ValueError('Invalid input tensor shape', x.shape)

        if x_n == space_n + 1:
            B, TT = x_shape[0], 1
        elif x_n == space_n + 2:
            B, TT = x_shape[:2]
        else:
            raise ValueError('Invalid input tensor shape', x.shape)

        if lstm_h is not None:
            assert lstm_h.shape[1] == lstm_c.shape[1] == B, 'LSTM state must be (h, c)'
            lstm_state = (lstm_h, lstm_c)
        else:
            lstm_state = None

        x = x.reshape(B*TT, *space_shape)
        hidden = self.policy.encode_observations(x, state)
        assert hidden.shape == (B*TT, self.input_size)

        hidden = hidden.reshape(B, TT, self.input_size)

        hidden = hidden.transpose(0, 1)
        #hidden = self.pre_layernorm(hidden)
        hidden, (lstm_h, lstm_c) = self.lstm.forward(hidden, lstm_state)
        hidden = hidden.float()
 
        #hidden = self.post_layernorm(hidden)
        hidden = hidden.transpose(0, 1)

        flat_hidden = hidden.reshape(B*TT, self.hidden_size)
        logits, values = self.policy.decode_actions(flat_hidden)
        values = values.reshape(B, TT)
        #state.batch_logits = logits.reshape(B, TT, -1)
        state['hidden'] = hidden
        state['lstm_h'] = lstm_h.detach()
        state['lstm_c'] = lstm_c.detach()
        return logits, values

class Convolutional(nn.Module):
    def __init__(self, env, *args, framestack, flat_size,
            input_size=512, hidden_size=512, output_size=512,
            channels_last=False, downsample=1, **kwargs):
        '''The CleanRL default NatureCNN policy used for Atari.
        It's just a stack of three convolutions followed by a linear layer
        
        Takes framestack as a mandatory keyword argument. Suggested default is 1 frame
        with LSTM or 4 frames without.'''
        super().__init__()
        self.channels_last = channels_last
        self.downsample = downsample

        #TODO: Remove these from required params
        self.hidden_size = hidden_size
        self.is_continuous = False

        self.network= nn.Sequential(
            pufferlib.pytorch.layer_init(nn.Conv2d(framestack, 32, 8, stride=4)),
            nn.ReLU(),
            pufferlib.pytorch.layer_init(nn.Conv2d(32, 64, 4, stride=2)),
            nn.ReLU(),
            pufferlib.pytorch.layer_init(nn.Conv2d(64, 64, 3, stride=1)),
            nn.ReLU(),
            nn.Flatten(),
            pufferlib.pytorch.layer_init(nn.Linear(flat_size, hidden_size)),
            nn.ReLU(),
        )
        self.actor = pufferlib.pytorch.layer_init(
            nn.Linear(hidden_size, env.single_action_space.n), std=0.01)
        self.value_fn = pufferlib.pytorch.layer_init(
            nn.Linear(output_size, 1), std=1)

    def forward(self, observations, state=None):
        hidden = self.encode_observations(observations)
        actions, value = self.decode_actions(hidden)
        return actions, value

    def forward_train(self, observations, state=None):
        return self.forward(observations, state)

    def encode_observations(self, observations, state=None):
        if self.channels_last:
            observations = observations.permute(0, 3, 1, 2)
        if self.downsample > 1:
            observations = observations[:, :, ::self.downsample, ::self.downsample]
        return self.network(observations.float() / 255.0)

    def decode_actions(self, flat_hidden):
        action = self.actor(flat_hidden)
        value = self.value_fn(flat_hidden)
        return action, value

class ProcgenResnet(nn.Module):
    '''Procgen baseline from the AICrowd NeurIPS 2020 competition
    Based on the ResNet architecture that was used in the Impala paper.'''
    def __init__(self, env, cnn_width=16, mlp_width=256):
        super().__init__()
        h, w, c = env.single_observation_space.shape
        shape = (c, h, w)
        conv_seqs = []
        for out_channels in [cnn_width, 2*cnn_width, 2*cnn_width]:
            conv_seq = ConvSequence(shape, out_channels)
            shape = conv_seq.get_output_shape()
            conv_seqs.append(conv_seq)
        conv_seqs += [
            nn.Flatten(),
            nn.ReLU(),
            nn.Linear(in_features=shape[0] * shape[1] * shape[2], out_features=mlp_width),
            nn.ReLU(),
        ]
        self.network = nn.Sequential(*conv_seqs)
        self.actor = pufferlib.pytorch.layer_init(
                nn.Linear(mlp_width, env.single_action_space.n), std=0.01)
        self.value = pufferlib.pytorch.layer_init(
                nn.Linear(mlp_width, 1), std=1)

    def forward(self, observations, state=None):
        hidden = self.encode_observations(observations)
        actions, value = self.decode_actions(hidden)
        return actions, value

    def forward_train(self, observations, state=None):
        return self.forward(observations, state)

    def encode_observations(self, x):
        hidden = self.network(x.permute((0, 3, 1, 2)) / 255.0)
        return hidden
 
    def decode_actions(self, hidden):
        '''linear decoder function'''
        action = self.actor(hidden)
        value = self.value(hidden)
        return action, value

class ResidualBlock(nn.Module):
    def __init__(self, channels):
        super().__init__()
        self.conv0 = nn.Conv2d(in_channels=channels, out_channels=channels, kernel_size=3, padding=1)
        self.conv1 = nn.Conv2d(in_channels=channels, out_channels=channels, kernel_size=3, padding=1)

    def forward(self, x):
        inputs = x
        x = nn.functional.relu(x)
        x = self.conv0(x)
        x = nn.functional.relu(x)
        x = self.conv1(x)
        return x + inputs

class ConvSequence(nn.Module):
    def __init__(self, input_shape, out_channels):
        super().__init__()
        self._input_shape = input_shape
        self._out_channels = out_channels
        self.conv = nn.Conv2d(in_channels=self._input_shape[0], out_channels=self._out_channels, kernel_size=3, padding=1)
        self.res_block0 = ResidualBlock(self._out_channels)
        self.res_block1 = ResidualBlock(self._out_channels)

    def forward(self, x):
        x = self.conv(x)
        x = nn.functional.max_pool2d(x, kernel_size=3, stride=2, padding=1)
        x = self.res_block0(x)
        x = self.res_block1(x)
        assert x.shape[1:] == self.get_output_shape()
        return x

    def get_output_shape(self):
        _c, h, w = self._input_shape
        return (self._out_channels, (h + 1) // 2, (w + 1) // 2)



================================================
FILE: pufferlib/pufferl.py
================================================
## puffer [train | eval | sweep] [env_name] [optional args] -- See https://puffer.ai for full detail0
# This is the same as python -m pufferlib.pufferl [train | eval | sweep] [env_name] [optional args]
# Distributed example: torchrun --standalone --nnodes=1 --nproc-per-node=6 -m pufferlib.pufferl train puffer_nmmo3

import contextlib
import warnings
warnings.filterwarnings('error', category=RuntimeWarning)

import os
import sys
import glob
import ast
import time
import random
import shutil
import argparse
import importlib
import configparser
from threading import Thread
from collections import defaultdict, deque

import numpy as np
import psutil

import torch
import torch.distributed
from torch.distributed.elastic.multiprocessing.errors import record
import torch.utils.cpp_extension

import pufferlib
import pufferlib.sweep
import pufferlib.vector
import pufferlib.pytorch
try:
    from pufferlib import _C
except ImportError:
    raise ImportError('Failed to import C/CUDA advantage kernel. If you have non-default PyTorch, try installing with --no-build-isolation')

import rich
import rich.traceback
from rich.table import Table
from rich.console import Console
from rich_argparse import RichHelpFormatter
rich.traceback.install(show_locals=False)

import signal # Aggressively exit on ctrl+c
signal.signal(signal.SIGINT, lambda sig, frame: os._exit(0))

# Assume advantage kernel has been built if CUDA compiler is available
ADVANTAGE_CUDA = shutil.which("nvcc") is not None

class PuffeRL:
    def __init__(self, config, vecenv, policy, logger=None):
        # Backend perf optimization
        torch.set_float32_matmul_precision('high')
        torch.backends.cudnn.deterministic = config['torch_deterministic']
        torch.backends.cudnn.benchmark = True

        # Reproducibility
        seed = config['seed']
        #random.seed(seed)
        #np.random.seed(seed)
        #torch.manual_seed(seed)

        # Vecenv info
        vecenv.async_reset(seed)
        obs_space = vecenv.single_observation_space
        atn_space = vecenv.single_action_space
        total_agents = vecenv.num_agents
        self.total_agents = total_agents

        # Experience
        if config['batch_size'] == 'auto' and config['bptt_horizon'] == 'auto':
            raise pufferlib.APIUsageError('Must specify batch_size or bptt_horizon')
        elif config['batch_size'] == 'auto':
            config['batch_size'] = total_agents * config['bptt_horizon']
        elif config['bptt_horizon'] == 'auto':
            config['bptt_horizon'] = config['batch_size'] // total_agents

        batch_size = config['batch_size']
        horizon = config['bptt_horizon']
        segments = batch_size // horizon
        self.segments = segments
        if total_agents > segments:
            raise pufferlib.APIUsageError(
                f'Total agents {total_agents} <= segments {segments}'
            )

        device = config['device']
        self.observations = torch.zeros(segments, horizon, *obs_space.shape,
            dtype=pufferlib.pytorch.numpy_to_torch_dtype_dict[obs_space.dtype],
            pin_memory=device == 'cuda' and config['cpu_offload'],
            device='cpu' if config['cpu_offload'] else device)
        self.actions = torch.zeros(segments, horizon, *atn_space.shape, device=device,
            dtype=pufferlib.pytorch.numpy_to_torch_dtype_dict[atn_space.dtype])
        self.values = torch.zeros(segments, horizon, device=device)
        self.logprobs = torch.zeros(segments, horizon, device=device)
        self.rewards = torch.zeros(segments, horizon, device=device)
        self.terminals = torch.zeros(segments, horizon, device=device)
        self.truncations = torch.zeros(segments, horizon, device=device)
        self.ratio = torch.ones(segments, horizon, device=device)
        self.importance = torch.ones(segments, horizon, device=device)
        self.ep_lengths = torch.zeros(total_agents, device=device, dtype=torch.int32)
        self.ep_indices = torch.arange(total_agents, device=device, dtype=torch.int32)
        self.free_idx = total_agents

        # LSTM
        if config['use_rnn']:
            n = vecenv.agents_per_batch
            h = policy.hidden_size
            self.lstm_h = {i*n: torch.zeros(n, h, device=device) for i in range(total_agents//n)}
            self.lstm_c = {i*n: torch.zeros(n, h, device=device) for i in range(total_agents//n)}

        # Minibatching & gradient accumulation
        minibatch_size = config['minibatch_size']
        max_minibatch_size = config['max_minibatch_size']
        self.minibatch_size = min(minibatch_size, max_minibatch_size)
        if minibatch_size > max_minibatch_size and minibatch_size % max_minibatch_size != 0:
            raise pufferlib.APIUsageError(
                f'minibatch_size {minibatch_size} > max_minibatch_size {max_minibatch_size} must divide evenly')

        if batch_size < minibatch_size:
            raise pufferlib.APIUsageError(
                f'batch_size {batch_size} must be >= minibatch_size {minibatch_size}'
            )

        self.accumulate_minibatches = max(1, minibatch_size // max_minibatch_size)
        self.total_minibatches = int(config['update_epochs'] * batch_size / self.minibatch_size)
        self.minibatch_segments = self.minibatch_size // horizon 
        if self.minibatch_segments * horizon != self.minibatch_size:
            raise pufferlib.APIUsageError(
                f'minibatch_size {self.minibatch_size} must be divisible by bptt_horizon {horizon}'
            )

        # Torch compile
        self.uncompiled_policy = policy
        self.policy = policy
        if config['compile']:
            self.policy = torch.compile(policy, mode=config['compile_mode'])
            self.policy.forward_eval = torch.compile(policy, mode=config['compile_mode'])
            pufferlib.pytorch.sample_logits = torch.compile(pufferlib.pytorch.sample_logits, mode=config['compile_mode'])

        # Optimizer
        if config['optimizer'] == 'adam':
            optimizer = torch.optim.Adam(
                self.policy.parameters(),
                lr=config['learning_rate'],
                betas=(config['adam_beta1'], config['adam_beta2']),
                eps=config['adam_eps'],
            )
        elif config['optimizer'] == 'muon':
            from heavyball import ForeachMuon
            warnings.filterwarnings(action='ignore', category=UserWarning, module=r'heavyball.*')
            import heavyball.utils
            heavyball.utils.compile_mode = config['compile_mode'] if config['compile'] else None
            optimizer = ForeachMuon(
                self.policy.parameters(),
                lr=config['learning_rate'],
                betas=(config['adam_beta1'], config['adam_beta2']),
                eps=config['adam_eps'],
            )
        else:
            raise ValueError(f'Unknown optimizer: {config["optimizer"]}')

        self.optimizer = optimizer

        # Logging
        self.logger = logger
        if logger is None:
            self.logger = NoLogger(config)

        # Learning rate scheduler
        epochs = config['total_timesteps'] // config['batch_size']
        self.scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs)
        self.total_epochs = epochs

        # Automatic mixed precision
        precision = config['precision']
        self.amp_context = contextlib.nullcontext()
        if config.get('amp', True) and config['device'] == 'cuda':
            self.amp_context = torch.amp.autocast(device_type='cuda', dtype=getattr(torch, precision))
        if precision not in ('float32', 'bfloat16'):
            raise pufferlib.APIUsageError(f'Invalid precision: {precision}: use float32 or bfloat16')

        # Initializations
        self.config = config
        self.vecenv = vecenv
        self.epoch = 0
        self.global_step = 0
        self.last_log_step = 0
        self.last_log_time = time.time()
        self.start_time = time.time()
        self.utilization = Utilization()
        self.profile = Profile()
        self.stats = defaultdict(list)
        self.last_stats = defaultdict(list)
        self.losses = {}

        # Dashboard
        self.model_size = sum(p.numel() for p in policy.parameters() if p.requires_grad)
        self.print_dashboard(clear=True)

    @property
    def uptime(self):
        return time.time() - self.start_time

    @property
    def sps(self):
        if self.global_step == self.last_log_step:
            return 0

        return (self.global_step - self.last_log_step) / (time.time() - self.last_log_time)

    def evaluate(self):
        profile = self.profile
        epoch = self.epoch
        profile('eval', epoch)
        profile('eval_misc', epoch, nest=True)

        config = self.config
        device = config['device']

        if config['use_rnn']:
            for k in self.lstm_h:
                self.lstm_h[k] = torch.zeros(self.lstm_h[k].shape, device=device)
                self.lstm_c[k] = torch.zeros(self.lstm_c[k].shape, device=device)

        self.full_rows = 0
        while self.full_rows < self.segments:
            profile('env', epoch)
            o, r, d, t, info, env_id, mask = self.vecenv.recv()

            profile('eval_misc', epoch)
            env_id = slice(env_id[0], env_id[-1] + 1)

            done_mask = d + t # TODO: Handle truncations separately
            self.global_step += int(mask.sum())

            profile('eval_copy', epoch)
            o = torch.as_tensor(o)
            o_device = o.to(device)#, non_blocking=True)
            r = torch.as_tensor(r).to(device)#, non_blocking=True)
            d = torch.as_tensor(d).to(device)#, non_blocking=True)

            profile('eval_forward', epoch)
            with torch.no_grad(), self.amp_context:
                state = dict(
                    reward=r,
                    done=d,
                    env_id=env_id,
                    mask=mask,
                )

                if config['use_rnn']:
                    state['lstm_h'] = self.lstm_h[env_id.start]
                    state['lstm_c'] = self.lstm_c[env_id.start]

                logits, value = self.policy.forward_eval(o_device, state)
                action, logprob, _ = pufferlib.pytorch.sample_logits(logits)
                r = torch.clamp(r, -1, 1)

            profile('eval_copy', epoch)
            with torch.no_grad():
                if config['use_rnn']:
                    self.lstm_h[env_id.start] = state['lstm_h']
                    self.lstm_c[env_id.start] = state['lstm_c']

                # Fast path for fully vectorized envs
                l = self.ep_lengths[env_id.start].item()
                batch_rows = slice(self.ep_indices[env_id.start].item(), 1+self.ep_indices[env_id.stop - 1].item())

                if config['cpu_offload']:
                    self.observations[batch_rows, l] = o
                else:
                    self.observations[batch_rows, l] = o_device

                self.actions[batch_rows, l] = action
                self.logprobs[batch_rows, l] = logprob
                self.rewards[batch_rows, l] = r
                self.terminals[batch_rows, l] = d.float()
                self.values[batch_rows, l] = value.flatten()

                # Note: We are not yet handling masks in this version
                self.ep_lengths[env_id] += 1
                if l+1 >= config['bptt_horizon']:
                    num_full = env_id.stop - env_id.start
                    self.ep_indices[env_id] = self.free_idx + torch.arange(num_full, device=config['device']).int()
                    self.ep_lengths[env_id] = 0
                    self.free_idx += num_full
                    self.full_rows += num_full

                action = action.cpu().numpy()
                if isinstance(logits, torch.distributions.Normal):
                    action = np.clip(action, self.vecenv.action_space.low, self.vecenv.action_space.high)

            profile('eval_misc', epoch)
            for i in info:
                for k, v in pufferlib.unroll_nested_dict(i):
                    if isinstance(v, np.ndarray):
                        v = v.tolist()
                    elif isinstance(v, (list, tuple)):
                        self.stats[k].extend(v)
                    else:
                        self.stats[k].append(v)

            profile('env', epoch)
            self.vecenv.send(action)

        profile('eval_misc', epoch)
        self.free_idx = self.total_agents
        self.ep_indices = torch.arange(self.total_agents, device=device, dtype=torch.int32)
        self.ep_lengths.zero_()
        profile.end()
        return self.stats

    @record
    def train(self):
        profile = self.profile
        epoch = self.epoch
        profile('train', epoch)
        losses = defaultdict(float)
        config = self.config
        device = config['device']

        b0 = config['prio_beta0']
        a = config['prio_alpha']
        clip_coef = config['clip_coef']
        vf_clip = config['vf_clip_coef']
        anneal_beta = b0 + (1 - b0)*a*self.epoch/self.total_epochs
        self.ratio[:] = 1

        for mb in range(self.total_minibatches):
            profile('train_misc', epoch, nest=True)
            self.amp_context.__enter__()

            shape = self.values.shape
            advantages = torch.zeros(shape, device=device)
            advantages = compute_puff_advantage(self.values, self.rewards,
                self.terminals, self.ratio, advantages, config['gamma'],
                config['gae_lambda'], config['vtrace_rho_clip'], config['vtrace_c_clip'])

            profile('train_copy', epoch)
            adv = advantages.abs().sum(axis=1)
            prio_weights = torch.nan_to_num(adv**a, 0, 0, 0)
            prio_probs = (prio_weights + 1e-6)/(prio_weights.sum() + 1e-6)
            idx = torch.multinomial(prio_probs, self.minibatch_segments)
            mb_prio = (self.segments*prio_probs[idx, None])**-anneal_beta
            mb_obs = self.observations[idx]
            mb_actions = self.actions[idx]
            mb_logprobs = self.logprobs[idx]
            mb_rewards = self.rewards[idx]
            mb_terminals = self.terminals[idx]
            mb_truncations = self.truncations[idx]
            mb_ratio = self.ratio[idx]
            mb_values = self.values[idx]
            mb_returns = advantages[idx] + mb_values
            mb_advantages = advantages[idx]

            profile('train_forward', epoch)
            if not config['use_rnn']:
                mb_obs = mb_obs.reshape(-1, *self.vecenv.single_observation_space.shape)

            state = dict(
                action=mb_actions,
                lstm_h=None,
                lstm_c=None,
            )

            logits, newvalue = self.policy(mb_obs, state)
            actions, newlogprob, entropy = pufferlib.pytorch.sample_logits(logits, action=mb_actions)

            profile('train_misc', epoch)
            newlogprob = newlogprob.reshape(mb_logprobs.shape)
            logratio = newlogprob - mb_logprobs
            ratio = logratio.exp()
            self.ratio[idx] = ratio.detach()

            with torch.no_grad():
                old_approx_kl = (-logratio).mean()
                approx_kl = ((ratio - 1) - logratio).mean()
                clipfrac = ((ratio - 1.0).abs() > config['clip_coef']).float().mean()

            adv = advantages[idx]
            adv = compute_puff_advantage(mb_values, mb_rewards, mb_terminals,
                ratio, adv, config['gamma'], config['gae_lambda'],
                config['vtrace_rho_clip'], config['vtrace_c_clip'])
            adv = mb_advantages
            adv = mb_prio * (adv - adv.mean()) / (adv.std() + 1e-8)

            # Losses
            pg_loss1 = -adv * ratio
            pg_loss2 = -adv * torch.clamp(ratio, 1 - clip_coef, 1 + clip_coef)
            pg_loss = torch.max(pg_loss1, pg_loss2).mean()

            newvalue = newvalue.view(mb_returns.shape)
            v_clipped = mb_values + torch.clamp(newvalue - mb_values, -vf_clip, vf_clip)
            v_loss_unclipped = (newvalue - mb_returns) ** 2
            v_loss_clipped = (v_clipped - mb_returns) ** 2
            v_loss = 0.5*torch.max(v_loss_unclipped, v_loss_clipped).mean()

            entropy_loss = entropy.mean()

            loss = pg_loss + config['vf_coef']*v_loss - config['ent_coef']*entropy_loss
            self.amp_context.__enter__() # TODO: AMP needs some debugging

            # This breaks vloss clipping?
            self.values[idx] = newvalue.detach().float()

            # Logging
            profile('train_misc', epoch)
            losses['policy_loss'] += pg_loss.item() / self.total_minibatches
            losses['value_loss'] += v_loss.item() / self.total_minibatches
            losses['entropy'] += entropy_loss.item() / self.total_minibatches
            losses['old_approx_kl'] += old_approx_kl.item() / self.total_minibatches
            losses['approx_kl'] += approx_kl.item() / self.total_minibatches
            losses['clipfrac'] += clipfrac.item() / self.total_minibatches
            losses['importance'] += ratio.mean().item() / self.total_minibatches

            # Learn on accumulated minibatches
            profile('learn', epoch)
            loss.backward()
            if (mb + 1) % self.accumulate_minibatches == 0:
                torch.nn.utils.clip_grad_norm_(self.policy.parameters(), config['max_grad_norm'])
                self.optimizer.step()
                self.optimizer.zero_grad()

        # Reprioritize experience
        profile('train_misc', epoch)
        if config['anneal_lr']:
            self.scheduler.step()

        y_pred = self.values.flatten()
        y_true = advantages.flatten() + self.values.flatten()
        var_y = y_true.var()
        explained_var = torch.nan if var_y == 0 else 1 - (y_true - y_pred).var() / var_y
        losses['explained_variance'] = explained_var.item()

        profile.end()
        logs = None
        self.epoch += 1
        done_training = self.global_step >= config['total_timesteps']
        if done_training or self.global_step == 0 or time.time() > self.last_log_time + 0.25:
            logs = self.mean_and_log()
            self.losses = losses
            self.print_dashboard()
            self.stats = defaultdict(list)
            self.last_log_time = time.time()
            self.last_log_step = self.global_step
            profile.clear()

        if self.epoch % config['checkpoint_interval'] == 0 or done_training:
            self.save_checkpoint()
            self.msg = f'Checkpoint saved at update {self.epoch}'

        return logs

    def mean_and_log(self):
        config = self.config
        for k in list(self.stats.keys()):
            v = self.stats[k]
            try:
                v = np.mean(v)
            except:
                del self.stats[k]

            self.stats[k] = v

        device = config['device']
        agent_steps = int(dist_sum(self.global_step, device))
        logs = {
            'SPS': dist_sum(self.sps, device),
            'agent_steps': agent_steps,
            'uptime': time.time() - self.start_time,
            'epoch': int(dist_sum(self.epoch, device)),
            'learning_rate': self.optimizer.param_groups[0]["lr"],
            **{f'environment/{k}': v for k, v in self.stats.items()},
            **{f'losses/{k}': v for k, v in self.losses.items()},
            **{f'performance/{k}': v['elapsed'] for k, v in self.profile},
            #**{f'environment/{k}': dist_mean(v, device) for k, v in self.stats.items()},
            #**{f'losses/{k}': dist_mean(v, device) for k, v in self.losses.items()},
            #**{f'performance/{k}': dist_sum(v['elapsed'], device) for k, v in self.profile},
        }

        if torch.distributed.is_initialized():
           if torch.distributed.get_rank() != 0:
               self.logger.log(logs, agent_steps)
               return logs
           else:
               return None

        self.logger.log(logs, agent_steps)
        return logs

    def close(self):
        self.vecenv.close()
        self.utilization.stop()
        model_path = self.save_checkpoint()
        run_id = self.logger.run_id
        path = os.path.join(self.config['data_dir'], f'{self.config["env"]}_{run_id}.pt')
        shutil.copy(model_path, path)
        return path

    def save_checkpoint(self):
        if torch.distributed.is_initialized():
           if torch.distributed.get_rank() != 0:
               return
 
        run_id = self.logger.run_id
        path = os.path.join(self.config['data_dir'], f'{self.config["env"]}_{run_id}')
        if not os.path.exists(path):
            os.makedirs(path)

        model_name = f'model_{self.config["env"]}_{self.epoch:06d}.pt'
        model_path = os.path.join(path, model_name)
        if os.path.exists(model_path):
            return model_path

        torch.save(self.uncompiled_policy.state_dict(), model_path)

        state = {
            'optimizer_state_dict': self.optimizer.state_dict(),
            'global_step': self.global_step,
            'agent_step': self.global_step,
            'update': self.epoch,
            'model_name': model_name,
            'run_id': run_id,
        }
        state_path = os.path.join(path, 'trainer_state.pt')
        torch.save(state, state_path + '.tmp')
        os.replace(state_path + '.tmp', state_path)
        return model_path

    def print_dashboard(self, clear=False, idx=[0],
            c1='[cyan]', c2='[white]', b1='[bright_cyan]', b2='[bright_white]'):
        config = self.config
        sps = dist_sum(self.sps, config['device'])
        agent_steps = dist_sum(self.global_step, config['device'])
        if torch.distributed.is_initialized():
           if torch.distributed.get_rank() != 0:
               return
 
        profile = self.profile
        console = Console()
        dashboard = Table(box=rich.box.ROUNDED, expand=True,
            show_header=False, border_style='bright_cyan')
        table = Table(box=None, expand=True, show_header=False)
        dashboard.add_row(table)

        table.add_column(justify="left", width=30)
        table.add_column(justify="center", width=12)
        table.add_column(justify="center", width=12)
        table.add_column(justify="center", width=13)
        table.add_column(justify="right", width=13)

        table.add_row(
            f'{b1}PufferLib {b2}3.0 {idx[0]*" "}:blowfish:',
            f'{c1}CPU: {b2}{np.mean(self.utilization.cpu_util):.1f}{c2}%',
            f'{c1}GPU: {b2}{np.mean(self.utilization.gpu_util):.1f}{c2}%',
            f'{c1}DRAM: {b2}{np.mean(self.utilization.cpu_mem):.1f}{c2}%',
            f'{c1}VRAM: {b2}{np.mean(self.utilization.gpu_mem):.1f}{c2}%',
        )
        idx[0] = (idx[0] - 1) % 10
            
        s = Table(box=None, expand=True)
        remaining = 'A hair past a freckle'
        if sps != 0:
            remaining = duration((config['total_timesteps'] - agent_steps)/sps, b2, c2)

        s.add_column(f"{c1}Summary", justify='left', vertical='top', width=10)
        s.add_column(f"{c1}Value", justify='right', vertical='top', width=14)
        s.add_row(f'{c2}Env', f'{b2}{config["env"]}')
        s.add_row(f'{c2}Params', abbreviate(self.model_size, b2, c2))
        s.add_row(f'{c2}Steps', abbreviate(agent_steps, b2, c2))
        s.add_row(f'{c2}SPS', abbreviate(sps, b2, c2))
        s.add_row(f'{c2}Epoch', f'{b2}{self.epoch}')
        s.add_row(f'{c2}Uptime', duration(self.uptime, b2, c2))
        s.add_row(f'{c2}Remaining', remaining)

        delta = profile.eval['buffer'] + profile.train['buffer']
        p = Table(box=None, expand=True, show_header=False)
        p.add_column(f"{c1}Performance", justify="left", width=10)
        p.add_column(f"{c1}Time", justify="right", width=8)
        p.add_column(f"{c1}%", justify="right", width=4)
        p.add_row(*fmt_perf('Evaluate', b1, delta, profile.eval, b2, c2))
        p.add_row(*fmt_perf('  Forward', c2, delta, profile.eval_forward, b2, c2))
        p.add_row(*fmt_perf('  Env', c2, delta, profile.env, b2, c2))
        p.add_row(*fmt_perf('  Copy', c2, delta, profile.eval_copy, b2, c2))
        p.add_row(*fmt_perf('  Misc', c2, delta, profile.eval_misc, b2, c2))
        p.add_row(*fmt_perf('Train', b1, delta, profile.train, b2, c2))
        p.add_row(*fmt_perf('  Forward', c2, delta, profile.train_forward, b2, c2))
        p.add_row(*fmt_perf('  Learn', c2, delta, profile.learn, b2, c2))
        p.add_row(*fmt_perf('  Copy', c2, delta, profile.train_copy, b2, c2))
        p.add_row(*fmt_perf('  Misc', c2, delta, profile.train_misc, b2, c2))

        l = Table(box=None, expand=True, )
        l.add_column(f'{c1}Losses', justify="left", width=16)
        l.add_column(f'{c1}Value', justify="right", width=8)
        for metric, value in self.losses.items():
            l.add_row(f'{c2}{metric}', f'{b2}{value:.3f}')

        monitor = Table(box=None, expand=True, pad_edge=False)
        monitor.add_row(s, p, l)
        dashboard.add_row(monitor)

        table = Table(box=None, expand=True, pad_edge=False)
        dashboard.add_row(table)
        left = Table(box=None, expand=True)
        right = Table(box=None, expand=True)
        table.add_row(left, right)
        left.add_column(f"{c1}User Stats", justify="left", width=20)
        left.add_column(f"{c1}Value", justify="right", width=10)
        right.add_column(f"{c1}User Stats", justify="left", width=20)
        right.add_column(f"{c1}Value", justify="right", width=10)
        i = 0

        if self.stats:
            self.last_stats = self.stats

        for metric, value in (self.stats or self.last_stats).items():
            try: # Discard non-numeric values
                int(value)
            except:
                continue

            u = left if i % 2 == 0 else right
            u.add_row(f'{c2}{metric}', f'{b2}{value:.3f}')
            i += 1
            if i == 30:
                break

        if clear:
            console.clear()

        with console.capture() as capture:
            console.print(dashboard)

        print('\033[0;0H' + capture.get())

def compute_puff_advantage(values, rewards, terminals,
        ratio, advantages, gamma, gae_lambda, vtrace_rho_clip, vtrace_c_clip):
    '''CUDA kernel for puffer advantage with automatic CPU fallback. You need
    nvcc (in cuda-dev-tools or in a cuda-dev docker base) for PufferLib to
    compile the fast version.'''

    device = values.device
    if not ADVANTAGE_CUDA:
        values = values.cpu()
        rewards = rewards.cpu()
        terminals = terminals.cpu()
        ratio = ratio.cpu()
        advantages = advantages.cpu()

    torch.ops.pufferlib.compute_puff_advantage(values, rewards, terminals,
        ratio, advantages, gamma, gae_lambda, vtrace_rho_clip, vtrace_c_clip)

    if not ADVANTAGE_CUDA:
        return advantages.to(device)

    return advantages


def abbreviate(num, b2, c2):
    if num < 1e3:
        return str(num)
    elif num < 1e6:
        return f'{num/1e3:.1f}K'
    elif num < 1e9:
        return f'{num/1e6:.1f}M'
    elif num < 1e12:
        return f'{num/1e9:.1f}B'
    else:
        return f'{num/1e12:.2f}T'

def duration(seconds, b2, c2):
    if seconds < 0:
        return f"{b2}0{c2}s"
    seconds = int(seconds)
    h = seconds // 3600
    m = (seconds % 3600) // 60
    s = seconds % 60
    return f"{b2}{h}{c2}h {b2}{m}{c2}m {b2}{s}{c2}s" if h else f"{b2}{m}{c2}m {b2}{s}{c2}s" if m else f"{b2}{s}{c2}s"

def fmt_perf(name, color, delta_ref, prof, b2, c2):
    percent = 0 if delta_ref == 0 else int(100*prof['buffer']/delta_ref - 1e-5)
    return f'{color}{name}', duration(prof['elapsed'], b2, c2), f'{b2}{percent:2d}{c2}%'

def dist_sum(value, device):
    if not torch.distributed.is_initialized():
        return value

    tensor = torch.tensor(value, device=device)
    torch.distributed.all_reduce(tensor, op=torch.distributed.ReduceOp.SUM)
    return tensor.item()

def dist_mean(value, device):
    if not torch.distributed.is_initialized():
        return value

    return dist_sum(value, device) / torch.distributed.get_world_size()

class Profile:
    def __init__(self, frequency=5):
        self.profiles = defaultdict(lambda: defaultdict(float))
        self.frequency = frequency
        self.stack = []

    def __iter__(self):
        return iter(self.profiles.items())

    def __getattr__(self, name):
        return self.profiles[name]

    def __call__(self, name, epoch, nest=False):
        if epoch % self.frequency != 0:
            return

        #if torch.cuda.is_available():
        #    torch.cuda.synchronize()

        tick = time.time()
        if len(self.stack) != 0 and not nest:
            self.pop(tick)

        self.stack.append(name)
        self.profiles[name]['start'] = tick

    def pop(self, end):
        profile = self.profiles[self.stack.pop()]
        delta = end - profile['start']
        profile['elapsed'] += delta
        profile['delta'] += delta

    def end(self):
        #if torch.cuda.is_available():
        #    torch.cuda.synchronize()

        end = time.time()
        for i in range(len(self.stack)):
            self.pop(end)

    def clear(self):
        for prof in self.profiles.values():
            if prof['delta'] > 0:
                prof['buffer'] = prof['delta']
                prof['delta'] = 0

class Utilization(Thread):
    def __init__(self, delay=1, maxlen=20):
        super().__init__()
        self.cpu_mem = deque([0], maxlen=maxlen)
        self.cpu_util = deque([0], maxlen=maxlen)
        self.gpu_util = deque([0], maxlen=maxlen)
        self.gpu_mem = deque([0], maxlen=maxlen)
        self.stopped = False
        self.delay = delay
        self.start()

    def run(self):
        while not self.stopped:
            self.cpu_util.append(100*psutil.cpu_percent()/psutil.cpu_count())
            mem = psutil.virtual_memory()
            self.cpu_mem.append(100*mem.active/mem.total)
            if torch.cuda.is_available():
                # Monitoring in distributed crashes nvml
                if torch.distributed.is_initialized():
                   time.sleep(self.delay)
                   continue

                self.gpu_util.append(torch.cuda.utilization())
                free, total = torch.cuda.mem_get_info()
                self.gpu_mem.append(100*(total-free)/total)
            else:
                self.gpu_util.append(0)
                self.gpu_mem.append(0)

            time.sleep(self.delay)

    def stop(self):
        self.stopped = True

def downsample(arr, m):
    if len(arr) < m:
        return arr

    if m == 0:
        return [arr[-1]]

    orig_arr = arr
    last = arr[-1]
    arr = arr[:-1]
    arr = np.array(arr)
    n = len(arr)
    n = (n//m)*m
    arr = arr[-n:]
    downsampled = arr.reshape(m, -1).mean(axis=1)
    return np.concatenate([downsampled, [last]])

class NoLogger:
    def __init__(self, args):
        self.run_id = str(int(100*time.time()))

    def log(self, logs, step):
        pass

    def close(self, model_path):
        pass

class NeptuneLogger:
    def __init__(self, args, load_id=None, mode='async'):
        import neptune as nept
        neptune_name = args['neptune_name']
        neptune_project = args['neptune_project']
        neptune = nept.init_run(
            project=f"{neptune_name}/{neptune_project}",
            capture_hardware_metrics=False,
            capture_stdout=False,
            capture_stderr=False,
            capture_traceback=False,
            with_id=load_id,
            mode=mode,
            tags = [args['tag']] if args['tag'] is not None else [],
        )
        self.run_id = neptune._sys_id
        self.neptune = neptune
        for k, v in pufferlib.unroll_nested_dict(args):
            neptune[k].append(v)

    def log(self, logs, step):
        for k, v in logs.items():
            self.neptune[k].append(v, step=step)

    def close(self, model_path):
        self.neptune['model'].track_files(model_path)
        self.neptune.stop()

    def download(self):
        self.neptune["model"].download(destination='artifacts')
        return f'artifacts/{self.run_id}.pt'
 
class WandbLogger:
    def __init__(self, args, load_id=None, resume='allow'):
        import wandb
        wandb.init(
            id=load_id or wandb.util.generate_id(),
            project=args['wandb_project'],
            group=args['wandb_group'],
            allow_val_change=True,
            save_code=False,
            resume=resume,
            config=args,
            tags = [args['tag']] if args['tag'] is not None else [],
        )
        self.wandb = wandb
        self.run_id = wandb.run.id

    def log(self, logs, step):
        self.wandb.log(logs, step=step)

    def close(self, model_path):
        artifact = self.wandb.Artifact(self.run_id, type='model')
        artifact.add_file(model_path)
        self.wandb.run.log_artifact(artifact)
        self.wandb.finish()

    def download(self):
        artifact = self.wandb.use_artifact(f'{self.run_id}:latest')
        data_dir = artifact.download()
        model_file = max(os.listdir(data_dir))
        return f'{data_dir}/{model_file}'
 
def train(env_name, args=None, vecenv=None, policy=None, logger=None):
    args = args or load_config(env_name)

    # Assume TorchRun DDP is used if LOCAL_RANK is set
    if 'LOCAL_RANK' in os.environ:
        world_size = int(os.environ.get('WORLD_SIZE', 1))
        print("World size", world_size)
        master_addr = os.environ.get('MASTER_ADDR', 'localhost')
        master_port = os.environ.get('MASTER_PORT', '29500')
        local_rank = int(os.environ["LOCAL_RANK"])
        print(f"rank: {local_rank}, MASTER_ADDR={master_addr}, MASTER_PORT={master_port}")
        torch.cuda.set_device(local_rank)
        os.environ["CUDA_VISIBLE_DEVICES"] = str(local_rank)

    vecenv = vecenv or load_env(env_name, args)
    policy = policy or load_policy(args, vecenv, env_name)

    if 'LOCAL_RANK' in os.environ:
        args['train']['device'] = torch.cuda.current_device()
        torch.distributed.init_process_group(backend='nccl', world_size=world_size)
        policy = policy.to(local_rank)
        model = torch.nn.parallel.DistributedDataParallel(
            policy, device_ids=[local_rank], output_device=local_rank
        )
        if hasattr(policy, 'lstm'):
            #model.lstm = policy.lstm
            model.hidden_size = policy.hidden_size

        model.forward_eval = policy.forward_eval
        policy = model.to(local_rank)

    if args['neptune']:
        logger = NeptuneLogger(args)
    elif args['wandb']:
        logger = WandbLogger(args)

    train_config = dict(**args['train'], env=env_name)
    pufferl = PuffeRL(train_config, vecenv, policy, logger)

    all_logs = []
    while pufferl.global_step < train_config['total_timesteps']:
        if train_config['device'] == 'cuda':
            torch.compiler.cudagraph_mark_step_begin()
        pufferl.evaluate()
        if train_config['device'] == 'cuda':
            torch.compiler.cudagraph_mark_step_begin()
        logs = pufferl.train()

        if logs is not None:
            if pufferl.global_step > 0.20*train_config['total_timesteps']:
                all_logs.append(logs)

    # Final eval. You can reset the env here, but depending on
    # your env, this can skew data (i.e. you only collect the shortest
    # rollouts within a fixed number of epochs)
    i = 0
    stats = {}
    while i < 32 or not stats:
        stats = pufferl.evaluate()
        i += 1

    logs = pufferl.mean_and_log()
    if logs is not None:
        all_logs.append(logs)

    pufferl.print_dashboard()
    model_path = pufferl.close()
    pufferl.logger.close(model_path)
    return all_logs

def eval(env_name, args=None, vecenv=None, policy=None):
    args = args or load_config(env_name)
    backend = args['vec']['backend']
    if backend != 'PufferEnv':
        backend = 'Serial'

    args['vec'] = dict(backend=backend, num_envs=1)
    vecenv = vecenv or load_env(env_name, args)

    policy = policy or load_policy(args, vecenv, env_name)
    ob, info = vecenv.reset()
    driver = vecenv.driver_env
    num_agents = vecenv.observation_space.shape[0]
    device = args['train']['device']

    state = {}
    if args['train']['use_rnn']:
        state = dict(
            lstm_h=torch.zeros(num_agents, policy.hidden_size, device=device),
            lstm_c=torch.zeros(num_agents, policy.hidden_size, device=device),
        )

    frames = []
    while True:
        render = driver.render()
        if len(frames) < args['save_frames']:
            frames.append(render)

        # Screenshot Ocean envs with F12, gifs with control + F12
        if driver.render_mode == 'ansi':
            print('\033[0;0H' + render + '\n')
            time.sleep(1/args['fps'])
        elif driver.render_mode == 'rgb_array':
            pass
            #import cv2
            #render = cv2.cvtColor(render, cv2.COLOR_RGB2BGR)
            #cv2.imshow('frame', render)
            #cv2.waitKey(1)
            #time.sleep(1/args['fps'])

        with torch.no_grad():
            ob = torch.as_tensor(ob).to(device)
            logits, value = policy.forward_eval(ob, state)
            action, logprob, _ = pufferlib.pytorch.sample_logits(logits)
            action = action.cpu().numpy().reshape(vecenv.action_space.shape)

        if isinstance(logits, torch.distributions.Normal):
            action = np.clip(action, vecenv.action_space.low, vecenv.action_space.high)

        ob = vecenv.step(action)[0]

        if len(frames) > 0 and len(frames) == args['save_frames']:
            import imageio
            imageio.mimsave(args['gif_path'], frames, fps=args['fps'], loop=0)
            frames.append('Done')

def sweep(args=None, env_name=None):
    args = args or load_config(env_name)
    if not args['wandb'] and not args['neptune']:
        raise pufferlib.APIUsageError('Sweeps require either wandb or neptune')

    method = args['sweep'].pop('method')
    try:
        sweep_cls = getattr(pufferlib.sweep, method)
    except:
        raise pufferlib.APIUsageError(f'Invalid sweep method {method}. See pufferlib.sweep')

    sweep = sweep_cls(args['sweep'])
    points_per_run = args['sweep']['downsample']
    target_key = f'environment/{args["sweep"]["metric"]}'
    for i in range(args['max_runs']):
        seed = time.time_ns() & 0xFFFFFFFF
        random.seed(seed)
        np.random.seed(seed)
        torch.manual_seed(seed)
        sweep.suggest(args)
        total_timesteps = args['train']['total_timesteps']
        all_logs = train(env_name, args=args)
        all_logs = [e for e in all_logs if target_key in e]
        scores = downsample([log[target_key] for log in all_logs], points_per_run)
        costs = downsample([log['uptime'] for log in all_logs], points_per_run)
        timesteps = downsample([log['agent_steps'] for log in all_logs], points_per_run)
        for score, cost, timestep in zip(scores, costs, timesteps):
            args['train']['total_timesteps'] = timestep
            sweep.observe(args, score, cost)

        # Prevent logging final eval steps as training steps
        args['train']['total_timesteps'] = total_timesteps

def profile(args=None, env_name=None, vecenv=None, policy=None):
    args = load_config()
    vecenv = vecenv or load_env(env_name, args)
    policy = policy or load_policy(args, vecenv)

    train_config = dict(**args['train'], env=args['env_name'], tag=args['tag'])
    pufferl = PuffeRL(train_config, vecenv, policy, neptune=args['neptune'], wandb=args['wandb'])

    import torchvision.models as models
    from torch.profiler import profile, record_function, ProfilerActivity
    with profile(activities=[ProfilerActivity.CPU, ProfilerActivity.CUDA], record_shapes=True) as prof:
        with record_function("model_inference"):
            for _ in range(10):
                stats = pufferl.evaluate()
                pufferl.train()

    print(prof.key_averages().table(sort_by='cuda_time_total', row_limit=10))
    prof.export_chrome_trace("trace.json")

def export(args=None, env_name=None, vecenv=None, policy=None):
    args = args or load_config(env_name)
    vecenv = vecenv or load_env(env_name, args)
    policy = policy or load_policy(args, vecenv)

    weights = []
    for name, param in policy.named_parameters():
        weights.append(param.data.cpu().numpy().flatten())
        print(name, param.shape, param.data.cpu().numpy().ravel()[0])
    
    path = f'{args["env_name"]}_weights.bin'
    weights = np.concatenate(weights)
    weights.tofile(path)
    print(f'Saved {len(weights)} weights to {path}')

def autotune(args=None, env_name=None, vecenv=None, policy=None):
    package = args['package']
    module_name = 'pufferlib.ocean' if package == 'ocean' else f'pufferlib.environments.{package}'
    env_module = importlib.import_module(module_name)
    env_name = args['env_name']
    make_env = env_module.env_creator(env_name)
    pufferlib.vector.autotune(make_env, batch_size=args['train']['env_batch_size'])
 
def load_env(env_name, args):
    package = args['package']
    module_name = 'pufferlib.ocean' if package == 'ocean' else f'pufferlib.environments.{package}'
    env_module = importlib.import_module(module_name)
    make_env = env_module.env_creator(env_name)
    return pufferlib.vector.make(make_env, env_kwargs=args['env'], **args['vec'])

def load_policy(args, vecenv, env_name=''):
    package = args['package']
    module_name = 'pufferlib.ocean' if package == 'ocean' else f'pufferlib.environments.{package}'
    env_module = importlib.import_module(module_name)

    device = args['train']['device']
    policy_cls = getattr(env_module.torch, args['policy_name'])
    policy = policy_cls(vecenv.driver_env, **args['policy'])

    rnn_name = args['rnn_name']
    if rnn_name is not None:
        rnn_cls = getattr(env_module.torch, args['rnn_name'])
        policy = rnn_cls(vecenv.driver_env, policy, **args['rnn'])

    policy = policy.to(device)

    load_id = args['load_id']
    if load_id is not None:
        if args['neptune']:
            path = NeptuneLogger(args, load_id, mode='read-only').download()
        elif args['wandb']:
            path = WandbLogger(args, load_id).download()
        else:
            raise pufferlib.APIUsageError('No run id provided for eval')

        state_dict = torch.load(path, map_location=device)
        state_dict = {k.replace('module.', ''): v for k, v in state_dict.items()}
        policy.load_state_dict(state_dict)

    load_path = args['load_model_path']
    if load_path == 'latest':
        load_path = max(glob.glob(f"experiments/{env_name}*.pt"), key=os.path.getctime)

    if load_path is not None:
        state_dict = torch.load(load_path, map_location=device)
        state_dict = {k.replace('module.', ''): v for k, v in state_dict.items()}
        policy.load_state_dict(state_dict)
        #state_path = os.path.join(*load_path.split('/')[:-1], 'state.pt')
        #optim_state = torch.load(state_path)['optimizer_state_dict']
        #pufferl.optimizer.load_state_dict(optim_state)

    return policy

def load_config(env_name):
    parser = argparse.ArgumentParser(
        description=f':blowfish: PufferLib [bright_cyan]{pufferlib.__version__}[/]'
        ' demo options. Shows valid args for your env and policy',
        formatter_class=RichHelpFormatter, add_help=False)
    parser.add_argument('--load-model-path', type=str, default=None,
        help='Path to a pretrained checkpoint')
    parser.add_argument('--load-id', type=str,
        default=None, help='Kickstart/eval from from a finished Wandb/Neptune run')
    parser.add_argument('--render-mode', type=str, default='auto',
        choices=['auto', 'human', 'ansi', 'rgb_array', 'raylib', 'None'])
    parser.add_argument('--save-frames', type=int, default=0)
    parser.add_argument('--gif-path', type=str, default='eval.gif')
    parser.add_argument('--fps', type=float, default=15)
    parser.add_argument('--max-runs', type=int, default=200, help='Max number of sweep runs')
    parser.add_argument('--wandb', action='store_true', help='Use wandb for logging')
    parser.add_argument('--wandb-project', type=str, default='pufferlib')
    parser.add_argument('--wandb-group', type=str, default='debug')
    parser.add_argument('--neptune', action='store_true', help='Use neptune for logging')
    parser.add_argument('--neptune-name', type=str, default='pufferai')
    parser.add_argument('--neptune-project', type=str, default='ablations')
    parser.add_argument('--local-rank', type=int, default=0, help='Used by torchrun for DDP')
    parser.add_argument('--tag', type=str, default=None, help='Tag for experiment')
    args = parser.parse_known_args()[0]

    # Load defaults and config
    puffer_dir = os.path.dirname(os.path.realpath(__file__))
    puffer_config_dir = os.path.join(puffer_dir, 'config/**/*.ini')
    puffer_default_config = os.path.join(puffer_dir, 'config/default.ini')
    if env_name == 'default':
        p = configparser.ConfigParser()
        p.read(puffer_default_config)
    else:
        for path in glob.glob(puffer_config_dir, recursive=True):
            p = configparser.ConfigParser()
            p.read([puffer_default_config, path])
            if env_name in p['base']['env_name'].split(): break
        else:
            raise pufferlib.APIUsageError('No config for env_name {}'.format(env_name))

    # Dynamic help menu from config
    def puffer_type(value):
        try:
            return ast.literal_eval(value)
        except:
            return value

    for section in p.sections():
        for key in p[section]:
            fmt = f'--{key}' if section == 'base' else f'--{section}.{key}'
            parser.add_argument(
                fmt.replace('_', '-'),
                default=puffer_type(p[section][key]),
                type=puffer_type
            )

    parser.add_argument('-h', '--help', default=argparse.SUPPRESS,
        action='help', help='Show this help message and exit')

    # Unpack to nested dict
    parsed = vars(parser.parse_args())
    args = defaultdict(dict)
    for key, value in parsed.items():
        next = args
        for subkey in key.split('.'):
            prev = next
            next = next.setdefault(subkey, {})

        prev[subkey] = value

    args['train']['use_rnn'] = args['rnn_name'] is not None
    return args

def main():
    err = 'Usage: puffer [train, eval, sweep, autotune, profile, export] [env_name] [optional args]. --help for more info'
    if len(sys.argv) < 3:
        raise pufferlib.APIUsageError(err)

    mode = sys.argv.pop(1)
    env_name = sys.argv.pop(1)
    if mode == 'train':
        train(env_name=env_name)
    elif mode == 'eval':
        eval(env_name=env_name)
    elif mode == 'sweep':
        sweep(env_name=env_name)
    elif mode == 'autotune':
        autotune(env_name=env_name)
    elif mode == 'profile':
        profile(env_name=env_name)
    elif mode == 'export':
        export(env_name=env_name)
    else:
        raise pufferlib.APIUsageError(err)

if __name__ == '__main__':
    main()



================================================
FILE: pufferlib/pufferlib.py
================================================
import os
import sys
import warnings

from contextlib import redirect_stdout, redirect_stderr, contextmanager
from types import SimpleNamespace
from collections.abc import Mapping
from io import StringIO
from functools import wraps

import numpy as np
import gymnasium

import pufferlib.spaces

ENV_ERROR = '''
Environment missing required attribute {}. The most common cause is
calling super() before you have assigned the attribute.
'''


def set_buffers(env, buf=None):
    if buf is None:
        obs_space = env.single_observation_space
        env.observations = np.zeros((env.num_agents, *obs_space.shape), dtype=obs_space.dtype)
        env.rewards = np.zeros(env.num_agents, dtype=np.float32)
        env.terminals = np.zeros(env.num_agents, dtype=bool)
        env.truncations = np.zeros(env.num_agents, dtype=bool)
        env.masks = np.ones(env.num_agents, dtype=bool)

        # TODO: Major kerfuffle on inferring action space dtype. This needs some asserts?
        atn_space = pufferlib.spaces.joint_space(env.single_action_space, env.num_agents)
        if isinstance(env.single_action_space, pufferlib.spaces.Box):
            env.actions = np.zeros(atn_space.shape, dtype=atn_space.dtype)
        else:
            env.actions = np.zeros(atn_space.shape, dtype=np.int32)
    else:
        env.observations = buf['observations']
        env.rewards = buf['rewards']
        env.terminals = buf['terminals']
        env.truncations = buf['truncations']
        env.masks = buf['masks']
        env.actions = buf['actions']

class PufferEnv:
    def __init__(self, buf=None):
        if not hasattr(self, 'single_observation_space'):
            raise APIUsageError(ENV_ERROR.format('single_observation_space'))
        if not hasattr(self, 'single_action_space'):
            raise APIUsageError(ENV_ERROR.format('single_action_space'))
        if not hasattr(self, 'num_agents'):
            raise APIUsageError(ENV_ERROR.format('num_agents'))
        if self.num_agents < 1:
            raise APIUsageError('num_agents must be >= 1')

        if hasattr(self, 'observation_space'):
            raise APIUsageError('PufferEnvs must define single_observation_space, not observation_space')
        if hasattr(self, 'action_space'):
            raise APIUsageError('PufferEnvs must define single_action_space, not action_space')
        if not isinstance(self.single_observation_space, pufferlib.spaces.Box):
            raise APIUsageError('Native observation_space must be a Box')
        if (not isinstance(self.single_action_space, pufferlib.spaces.Discrete)
                and not isinstance(self.single_action_space, pufferlib.spaces.MultiDiscrete)
                and not isinstance(self.single_action_space, pufferlib.spaces.Box)):
            raise APIUsageError('Native action_space must be a Discrete, MultiDiscrete, or Box')

        set_buffers(self, buf)

        self.action_space = pufferlib.spaces.joint_space(self.single_action_space, self.num_agents)
        self.observation_space = pufferlib.spaces.joint_space(self.single_observation_space, self.num_agents)
        self.agent_ids = np.arange(self.num_agents)

    @property
    def agent_per_batch(self):
        return self.num_agents

    @property
    def emulated(self):
        '''Native envs do not use emulation'''
        return False

    @property
    def done(self):
        '''Native envs handle resets internally'''
        return False

    @property
    def driver_env(self):
        '''For compatibility with Multiprocessing'''
        return self

    def reset(self, seed=None):
        raise NotImplementedError

    def step(self, actions):
        raise NotImplementedError

    def close(self):
        raise NotImplementedError

    def async_reset(self, seed=None):
        _, self.infos = self.reset(seed)
        assert isinstance(self.infos, list), 'PufferEnvs must return info as a list of dicts'

    def send(self, actions):
        _, _, _, _, self.infos = self.step(actions)
        assert isinstance(self.infos, list), 'PufferEnvs must return info as a list of dicts'

    def recv(self):
        return (self.observations, self.rewards, self.terminals,
            self.truncations, self.infos, self.agent_ids, self.masks)

### Postprocessing
class ResizeObservation(gymnasium.Wrapper):
    '''Fixed downscaling wrapper. Do NOT use gym.wrappers.ResizeObservation
    It uses a laughably slow OpenCV resize. -50% on Atari just from that.'''
    def __init__(self, env, downscale=2):
        super().__init__(env)
        self.downscale = downscale
        y_size, x_size = env.observation_space.shape
        assert y_size % downscale == 0 and x_size % downscale == 0
        y_size = env.observation_space.shape[0] // downscale
        x_size = env.observation_space.shape[1] // downscale
        self.observation_space = gymnasium.spaces.Box(
            low=0, high=255, shape=(y_size, x_size), dtype=np.uint8)

    def reset(self, seed=None, options=None):
        obs, info = self.env.reset(seed=seed, options=options)
        return obs[::self.downscale, ::self.downscale], info

    def step(self, action):
        obs, reward, terminal, truncated, info = self.env.step(action)
        return obs[::self.downscale, ::self.downscale], reward, terminal, truncated, info

class ClipAction(gymnasium.Wrapper):
    '''Wrapper for Gymnasium environments that clips actions'''
    def __init__(self, env):
        self.env = env
        assert isinstance(env.action_space, gymnasium.spaces.Box)
        self._observation_space = env.observation_space
        self._action_space = env.action_space
        dtype_info = np.finfo(env.action_space.dtype)
        self.action_space = gymnasium.spaces.Box(
            low=dtype_info.min,
            high=dtype_info.max,
            shape=env.action_space.shape,
            dtype=env.action_space.dtype,
        )

    def step(self, action):
        action = np.clip(action, self.env.action_space.low, self.env.action_space.high)
        return self.env.step(action)


class EpisodeStats(gymnasium.Wrapper):
    '''Wrapper for Gymnasium environments that stores
    episodic returns and lengths in infos'''
    def __init__(self, env):
        self.env = env
        self.observation_space = env.observation_space
        self.action_space = env.action_space
        self.reset()

    def reset(self, seed=None, options=None):
        self.info = dict(episode_return=[], episode_length=0)
        # TODO: options
        return self.env.reset(seed=seed)#, options=options)

    def step(self, action):
        observation, reward, terminated, truncated, info = super().step(action)

        for k, v in unroll_nested_dict(info):
            if k not in self.info:
                self.info[k] = []

            self.info[k].append(v)

        self.info['episode_return'].append(reward)
        self.info['episode_length'] += 1

        info = {}
        if terminated or truncated:
            for k, v in self.info.items():
                try:
                    info[k] = sum(v)
                    continue
                except TypeError:
                    pass

                if isinstance(v, str):
                    info[k] = v
                    continue

                try:
                    x = int(v) # probably a value
                    info[k] = v
                    continue
                except TypeError:
                    pass

        return observation, reward, terminated, truncated, info

class PettingZooWrapper:
    '''PettingZoo does not provide a ParallelEnv wrapper. This code is adapted from
    their AEC wrapper, to prevent unneeded conversions to/from AEC'''
    def __init__(self, env):
        self.env = env

    def __getattr__(self, name):
        '''Returns an attribute with ``name``, unless ``name`` starts with an underscore.'''
        if name.startswith('_') and name != '_cumulative_rewards':
            raise AttributeError(f'accessing private attribute "{name}" is prohibited')
        return getattr(self.env, name)

    @property
    def unwrapped(self):
        return self.env.unwrapped

    def close(self):
        self.env.close()

    def render(self):
        return self.env.render()

    def reset(self, seed=None, options=None):
        try:
            return self.env.reset(seed=seed, options=options)
        except TypeError:
            return self.env.reset(seed=seed)

    def observe(self, agent):
        return self.env.observe(agent)

    def state(self):
        return self.env.state()

    def step(self, action):
        return self.env.step(action)

    def observation_space(self, agent):
        return self.env.observation_space(agent)

    def action_space(self, agent):
        return self.env.action_space(agent)

    def __str__(self) -> str:
        '''Returns a name which looks like: "max_observation<space_invaders_v1>".'''
        return f'{type(self).__name__}<{str(self.env)}>'

class MeanOverAgents(PettingZooWrapper):
    '''Averages over agent infos'''
    def _mean(self, infos):
        list_infos = {}
        for agent, info in infos.items():
            for k, v in info.items():
                if k not in list_infos:
                    list_infos[k] = []

                list_infos[k].append(v)

        mean_infos = {}
        for k, v in list_infos.items():
            try:
                mean_infos[k] = np.mean(v)
            except:
                pass

        return mean_infos

    def reset(self, seed=None, options=None):
        observations, infos = super().reset(seed, options)
        infos = self._mean(infos)
        return observations, infos

    def step(self, actions):
        observations, rewards, terminations, truncations, infos = super().step(actions)
        infos = self._mean(infos)
        return observations, rewards, terminations, truncations, infos

class MultiagentEpisodeStats(PettingZooWrapper):
    '''Wrapper for PettingZoo environments that stores
    episodic returns and lengths in infos'''
    def reset(self, seed=None, options=None):
        observations, infos = super().reset(seed=seed, options=options)
        self.infos = {
            agent: dict(episode_return=[], episode_length=0)
            for agent in self.possible_agents
        }
        return observations, infos

    def step(self, actions):
        observations, rewards, terminations, truncations, infos = super().step(actions)

        all_infos = {}
        for agent in infos:
            agent_info = self.infos[agent]
            for k, v in unroll_nested_dict(infos[agent]):
                if k not in agent_info:
                    agent_info[k] = []

                agent_info[k].append(v)

            # Saved to self. TODO: Clean up
            agent_info['episode_return'].append(rewards[agent])
            agent_info['episode_length'] += 1

            agent_info = {}
            all_infos[agent] = agent_info
            if terminations[agent] or truncations[agent]:
                for k, v in self.infos[agent].items():
                    try:
                        agent_info[k] = sum(v)
                        continue
                    except TypeError:
                        pass

                    if isinstance(v, str):
                        agent_info[k] = v
                        continue

                    try:
                        x = int(v) # probably a value
                        agent_info[k] = v
                        continue
                    except TypeError:
                        pass

        return observations, rewards, terminations, truncations, all_infos
### Exceptions
class EnvironmentSetupError(RuntimeError):
    def __init__(self, e, package):
        super().__init__(self.message)

class APIUsageError(RuntimeError):
    """Exception raised when the API is used incorrectly."""

    def __init__(self, message="API usage error."):
        self.message = message
        super().__init__(self.message)

class InvalidAgentError(ValueError):
    """Exception raised when an invalid agent key is used."""

    def __init__(self, agent_id, agents):
        message = (
            f'Invalid agent/team ({agent_id}) specified. '
            f'Valid values:\n{agents}'
        )
        super().__init__(message)

class GymToGymnasium:
    def __init__(self, env):
        self.env = env
        self.observation_space = env.observation_space
        self.action_space = env.action_space
        self.render = env.render
        self.metadata = env.metadata

    def reset(self, seed=None, options=None):
        if seed is not None:
            ob = self.env.reset(seed=seed)
        else:
            ob = self.env.reset()
        return ob, {}

    def step(self, action):
        observation, reward, done, info = self.env.step(action)
        return observation, reward, done, False, info

    def close(self):
        self.env.close()

### Wrappers
class PettingZooTruncatedWrapper:
    def __init__(self, env):
        self.env = env
        self.observation_space = env.observation_space
        self.action_space = env.action_space
        self.render = env.render

    @property
    def render_mode(self):
        return self.env.render_mode

    @property
    def possible_agents(self):
        return self.env.possible_agents

    @property
    def agents(self):
        return self.env.agents

    def reset(self, seed=None):
        if seed is not None:
            ob, info = self.env.reset(seed=seed)
        else:
            ob, info = self.env.reset()
        info = {k: {} for k in ob}
        return ob, info

    def step(self, actions):
        observations, rewards, terminals, truncations, infos = self.env.step(actions)
        return observations, rewards, terminals, truncations, infos

    def close(self):
        self.env.close()

### Misc
def unroll_nested_dict(d):
    if not isinstance(d, dict):
        return d

    for k, v in d.items():
        if isinstance(v, dict):
            for k2, v2 in unroll_nested_dict(v):
                yield f"{k}/{k2}", v2
        else:
            yield k, v

def silence_warnings(original_func, category=DeprecationWarning):
    @wraps(original_func)
    def wrapper(*args, **kwargs):
        with warnings.catch_warnings():
            warnings.simplefilter("ignore", category=category)
            return original_func(*args, **kwargs)
    return wrapper

class Suppress():
    def __init__(self):
        self.f = StringIO()
        self.null_1 = os.open(os.devnull, os.O_WRONLY | os.O_TRUNC | os.O_CREAT)
        self.null_2 = os.open(os.devnull, os.O_WRONLY | os.O_TRUNC | os.O_CREAT)

    def __enter__(self):
        # Suppress C library outputs
        self.orig_stdout = os.dup(1)
        self.orig_stderr = os.dup(2)
        os.dup2(self.null_1, 1)
        os.dup2(self.null_2, 2)

        # Suppress Python outputs
        self._stdout_redirector = redirect_stdout(self.f)
        self._stderr_redirector = redirect_stderr(self.f)
        self._stdout_redirector.__enter__()
        self._stderr_redirector.__enter__()

    def __exit__(self, exc_type, exc_val, exc_tb):
        # Enable C library outputs
        os.dup2(self.orig_stdout, 1)
        os.dup2(self.orig_stderr, 2)
        os.close(self.orig_stdout)
        os.close(self.orig_stderr)
        os.close(self.null_1)
        os.close(self.null_2)

        # Enable Python outputs
        self._stdout_redirector.__exit__(exc_type, exc_val, exc_tb)
        self._stderr_redirector.__exit__(exc_type, exc_val, exc_tb)



================================================
FILE: pufferlib/pytorch.py
================================================
import sys
from pdb import set_trace as T
from typing import Dict, List, Tuple, Union

import numpy as np
import torch
from torch import nn
from torch.distributions import Categorical
from torch.distributions.utils import logits_to_probs

import pufferlib
import pufferlib.models


numpy_to_torch_dtype_dict = {
    np.dtype("float64"): torch.float64,
    np.dtype("float32"): torch.float32,
    np.dtype("float16"): torch.float16,
    np.dtype("uint64"): torch.uint64,
    np.dtype("uint32"): torch.uint32,
    np.dtype("uint16"): torch.uint16,
    np.dtype("uint8"): torch.uint8,
    np.dtype("int64"): torch.int64,
    np.dtype("int32"): torch.int32,
    np.dtype("int16"): torch.int16,
    np.dtype("int8"): torch.int8,
}


LITTLE_BYTE_ORDER = sys.byteorder == "little"

# USER NOTE: You should not get any errors in nativize.
# This is a complicated piece of code that attempts to convert
# flat bytes to structured tensors without breaking torch.compile.
# If you hit any errors, please post on discord.gg/puffer
# One exception: make sure you didn't change the dtype of your data
# ie by doing torch.Tensor(data) instead of torch.from_numpy(data)

# dtype of the tensor
# shape of the tensor
# starting element of the observation
# number of elements of the observation to take
# could be a namedtuple or dataclass
NativeDTypeValue = Tuple[torch.dtype, List[int], int, int]
NativeDType = Union[NativeDTypeValue, Dict[str, Union[NativeDTypeValue, "NativeDType"]]]

# TODO: handle discrete obs
# Spend some time trying to break this fn with differnt obs
def nativize_dtype(emulated) -> NativeDType:
    # sample dtype - the dtype of what we obtain from the environment (usually bytes)
    sample_dtype: np.dtype = emulated['observation_dtype']
    # structured dtype - the gym.Space converted numpy dtype

    # the observation represents (could be dict, tuple, box, etc.)
    structured_dtype: np.dtype = emulated['emulated_observation_dtype']
    subviews, dtype, shape, offset, delta = _nativize_dtype(sample_dtype, structured_dtype)
    if subviews is None:
        return (dtype, shape, offset, delta)
    else:
        return subviews

def round_to(x, base):
    return int(base * np.ceil(x/base))

def _nativize_dtype(sample_dtype: np.dtype,
        structured_dtype: np.dtype,
        offset: int = 0) -> NativeDType:
    if structured_dtype.fields is None:
        if structured_dtype.subdtype is not None:
            dtype, shape = structured_dtype.subdtype
        else:
            dtype = structured_dtype
            shape = (1,)

        delta = int(np.prod(shape))
        if sample_dtype.base.itemsize == 1:
            offset = round_to(offset, dtype.alignment)
            delta *= dtype.itemsize
        else:
            assert dtype.itemsize == sample_dtype.base.itemsize

        return None, numpy_to_torch_dtype_dict[dtype], shape, offset, delta
    else:
        subviews = {}
        start_offset = offset
        all_delta = 0
        for name, (dtype, _) in structured_dtype.fields.items():
            views, dtype, shape, offset, delta = _nativize_dtype(
                sample_dtype, dtype, offset)

            if views is not None:
                subviews[name] = views
            else:
                subviews[name] = (dtype, shape, offset, delta)

            offset += delta
            all_delta += delta

        return subviews, dtype, shape, start_offset, all_delta


def nativize_tensor(observation: torch.Tensor, native_dtype: NativeDType):
    return _nativize_tensor(observation, native_dtype)


# torch.view(dtype) does not compile
# This is a workaround hack
# @thatguy - can you figure out a more robust way to handle cast?
# I think it may screw up for non-uint data... so I put a hard .view
# fallback that breaks compile
def compilable_cast(u8, dtype):
    if dtype in (torch.uint8, torch.uint16, torch.uint32, torch.uint64):
        n = dtype.itemsize
        bytes = [u8[..., i::n].to(dtype) for i in range(n)]
        if not LITTLE_BYTE_ORDER:
            bytes = bytes[::-1]

        bytes = sum(bytes[i] << (i * 8) for i in range(n))
        return bytes.view(dtype)
    return u8.view(dtype)  # breaking cast


def _nativize_tensor(observation: torch.Tensor, native_dtype: NativeDType):
    if isinstance(native_dtype, tuple):
        dtype, shape, offset, delta = native_dtype
        torch._check_is_size(offset)
        torch._check_is_size(delta)
        # Important, we are assuming that obervations of shape
        # [N, D] where N is number of examples and D is number of
        # bytes per example is being passed in
        slice = observation.narrow(1, offset, delta)
        # slice = slice.contiguous()
        # slice = compilable_cast(slice, dtype)
        slice = slice.view(dtype)
        slice = slice.view(observation.shape[0], *shape)
        return slice
    else:
        subviews = {}
        for name, dtype in native_dtype.items():
            subviews[name] = _nativize_tensor(observation, dtype)
        return subviews


def nativize_observation(observation, emulated):
    # TODO: Any way to check that user has not accidentally cast data to float?
    # float is natively supported, but only if that is the actual correct type
    return nativize_tensor(
        observation,
        emulated['observation_dtype'],
        emulated['emulated_observation_dtype'],
    )

def flattened_tensor_size(native_dtype):
    return _flattened_tensor_size(native_dtype)

def _flattened_tensor_size(native_dtype):
    if isinstance(native_dtype, tuple):
        return np.prod(native_dtype[1])  # shape
    else:
        res = 0
        for _, dtype in native_dtype.items():
            res += _flattened_tensor_size(dtype)
        return res

def layer_init(layer, std=np.sqrt(2), bias_const=0.0):
    """CleanRL's default layer initialization"""
    torch.nn.init.orthogonal_(layer.weight, std)
    torch.nn.init.constant_(layer.bias, bias_const)
    return layer

# taken from torch.distributions.Categorical
def log_prob(logits, value):
    value = value.long().unsqueeze(-1)
    value, log_pmf = torch.broadcast_tensors(value, logits)
    value = value[..., :1]
    return log_pmf.gather(-1, value).squeeze(-1)

# taken from torch.distributions.Categorical
def entropy(logits):
    min_real = torch.finfo(logits.dtype).min
    logits = torch.clamp(logits, min=min_real)
    p_log_p = logits * logits_to_probs(logits)
    return -p_log_p.sum(-1)

def entropy_probs(logits, probs):
    p_log_p = logits * probs
    return -p_log_p.sum(-1)

def sample_logits(logits, action=None):
    is_discrete = isinstance(logits, torch.Tensor)
    if isinstance(logits, torch.distributions.Normal):
        batch = logits.loc.shape[0]
        if action is None:
            action = logits.sample().view(batch, -1)

        log_probs = logits.log_prob(action.view(batch, -1)).sum(1)
        logits_entropy = logits.entropy().view(batch, -1).sum(1)
        return action, log_probs, logits_entropy
    elif is_discrete:
        logits = logits.unsqueeze(0)
    # TODO: Double check this
    else: #multi-discrete
        logits = torch.nn.utils.rnn.pad_sequence(
            [l.transpose(0,1) for l in logits], 
            batch_first=False, 
            padding_value=-torch.inf
        ).permute(1,2,0)

    # This can fail on nans etc
    normalized_logits = logits - logits.logsumexp(dim=-1, keepdim=True)
    probs = logits_to_probs(logits)

    if action is None:
        probs = torch.nan_to_num(probs, 1e-8, 1e-8, 1e-8)
        action = torch.multinomial(probs.reshape(-1, probs.shape[-1]), 1, replacement=True).int()
        action = action.reshape(probs.shape[:-1])
    else:
        batch = logits[0].shape[0]
        action = action.view(batch, -1).T

    assert len(logits) == len(action)
    logprob = log_prob(normalized_logits, action)
    logits_entropy = entropy(normalized_logits).sum(0)

    if is_discrete:
        return action.squeeze(0), logprob.squeeze(0), logits_entropy.squeeze(0)

    return action.T, logprob.sum(0), logits_entropy



================================================
FILE: pufferlib/spaces.py
================================================
import numpy as np
import gym
import gymnasium

Box = (gym.spaces.Box, gymnasium.spaces.Box)
Dict = (gym.spaces.Dict, gymnasium.spaces.Dict)
Discrete = (gym.spaces.Discrete, gymnasium.spaces.Discrete)
MultiBinary = (gym.spaces.MultiBinary, gymnasium.spaces.MultiBinary)
MultiDiscrete = (gym.spaces.MultiDiscrete, gymnasium.spaces.MultiDiscrete)
Tuple = (gym.spaces.Tuple, gymnasium.spaces.Tuple)

def joint_space(space, n):
    if isinstance(space, Discrete):
        return gymnasium.spaces.MultiDiscrete([space.n] * n)
    elif isinstance(space, MultiDiscrete):
        return gymnasium.spaces.Box(low=0,
            high=np.repeat(space.nvec[None] - 1, n, axis=0),
            shape=(n, len(space)), dtype=space.dtype)
    elif isinstance(space, Box):
        low = np.repeat(space.low[None], n, axis=0)
        high = np.repeat(space.high[None], n, axis=0)
        return gymnasium.spaces.Box(low=low, high=high,
            shape=(n, *space.shape), dtype=space.dtype)
    else:
        raise ValueError(f'Unsupported space: {space}')



================================================
FILE: pufferlib/sweep.py
================================================
import numpy as np
import random
import math
import warnings

from copy import deepcopy

import pufferlib

import torch
import pyro
from pyro.contrib import gp as gp
from pyro.contrib.gp.kernels import Kernel
from pyro.contrib.gp.models import GPRegression


class Space:
    def __init__(self, min, max, scale, mean, is_integer=False):
        self.min = min
        self.max = max
        self.scale = scale
        self.mean = mean # TODO: awkward to have just this normalized
        self.norm_min = self.normalize(min)
        self.norm_max = self.normalize(max)
        self.norm_mean = self.normalize(mean)
        self.is_integer = is_integer

class Linear(Space):
    def __init__(self, min, max, scale, mean, is_integer=False):
        if scale == 'auto':
            scale = 0.5

        super().__init__(min, max, scale, mean, is_integer)

    def normalize(self, value):
        #assert isinstance(value, (int, float))
        zero_one = (value - self.min)/(self.max - self.min)
        return 2*zero_one - 1

    def unnormalize(self, value):
        zero_one = (value + 1)/2
        value = zero_one * (self.max - self.min) + self.min
        if self.is_integer:
            value = round(value)
        return value

class Pow2(Space):
    def __init__(self, min, max, scale, mean, is_integer=False):
        if scale == 'auto':
            scale = 0.5
            #scale = 2 / (np.log2(max) - np.log2(min))

        super().__init__(min, max, scale, mean, is_integer)

    def normalize(self, value):
        #assert isinstance(value, (int, float))
        #assert value != 0.0
        zero_one = (math.log(value, 2) - math.log(self.min, 2))/(math.log(self.max, 2) - math.log(self.min, 2))
        return 2*zero_one - 1

    def unnormalize(self, value):
        zero_one = (value + 1)/2
        log_spaced = zero_one*(math.log(self.max, 2) - math.log(self.min, 2)) + math.log(self.min, 2)
        rounded = round(log_spaced)
        return 2 ** rounded

class Log(Space):
    base: int = 10

    def __init__(self, min, max, scale, mean, is_integer=False):
        if scale == 'time':
            # TODO: Set scaling param intuitively based on number of jumps from min to max
            scale = 1 / (np.log2(max) - np.log2(min))
        elif scale == 'auto':
            scale = 0.5

        super().__init__(min, max, scale, mean, is_integer)

    def normalize(self, value):
        #assert isinstance(value, (int, float))
        #assert value != 0.0
        zero_one = (math.log(value, self.base) - math.log(self.min, self.base))/(math.log(self.max, self.base) - math.log(self.min, self.base))
        return 2*zero_one - 1

    def unnormalize(self, value):
        zero_one = (value + 1)/2
        log_spaced = zero_one*(math.log(self.max, self.base) - math.log(self.min, self.base)) + math.log(self.min, self.base)
        value = self.base ** log_spaced
        if self.is_integer:
            value = round(value)
        return value

class Logit(Space):
    base: int = 10

    def __init__(self, min, max, scale, mean, is_integer=False):
        if scale == 'auto':
            scale = 0.5

        super().__init__(min, max, scale, mean, is_integer)

    def normalize(self, value):
        #assert isinstance(value, (int, float))
        #assert value != 0.0
        #assert value != 1.0
        zero_one = (math.log(1-value, self.base) - math.log(1-self.min, self.base))/(math.log(1-self.max, self.base) - math.log(1-self.min, self.base))
        return 2*zero_one - 1

    def unnormalize(self, value):
        zero_one = (value + 1)/2
        log_spaced = zero_one*(math.log(1-self.max, self.base) - math.log(1-self.min, self.base)) + math.log(1-self.min, self.base)
        return 1 - self.base**log_spaced

def _params_from_puffer_sweep(sweep_config):
    param_spaces = {}
    for name, param in sweep_config.items():
        if name in ('method', 'metric', 'goal', 'downsample'):
            continue

        assert isinstance(param, dict)
        if any(isinstance(param[k], dict) for k in param):
            param_spaces[name] = _params_from_puffer_sweep(param)
            continue
 
        assert 'distribution' in param
        distribution = param['distribution']
        search_center = param['mean']
        kwargs = dict(
            min=param['min'],
            max=param['max'],
            scale=param['scale'],
            mean=search_center,
        )
        if distribution == 'uniform':
            space = Linear(**kwargs)
        elif distribution == 'int_uniform':
            space = Linear(**kwargs, is_integer=True)
        elif distribution == 'uniform_pow2':
            space = Pow2(**kwargs, is_integer=True)
        elif distribution == 'log_normal':
            space = Log(**kwargs)
        elif distribution == 'logit_normal':
            space = Logit(**kwargs)
        else:
            raise ValueError(f'Invalid distribution: {distribution}')

        param_spaces[name] = space

    return param_spaces

class Hyperparameters:
    def __init__(self, config, verbose=True):
        self.spaces = _params_from_puffer_sweep(config)
        self.flat_spaces = dict(pufferlib.unroll_nested_dict(self.spaces))
        self.num = len(self.flat_spaces)

        self.metric = config['metric']
        goal = config['goal']
        assert goal in ('maximize', 'minimize')
        self.optimize_direction = 1 if goal == 'maximize' else -1

        self.search_centers = np.array([
            e.norm_mean for e in self.flat_spaces.values()])
        self.min_bounds = np.array([
            e.norm_min for e in self.flat_spaces.values()])
        self.max_bounds = np.array([
            e.norm_max for e in self.flat_spaces.values()])
        self.search_scales = np.array([
            e.scale for e in self.flat_spaces.values()])

        if verbose:
            print('Min random sample:')
            for name, space in self.flat_spaces.items():
                print(f'\t{name}: {space.unnormalize(max(space.norm_mean - space.scale, space.norm_min))}')

            print('Max random sample:')
            for name, space in self.flat_spaces.items():
                print(f'\t{name}: {space.unnormalize(min(space.norm_mean + space.scale, space.norm_max))}')

    def sample(self, n, mu=None, scale=1):
        if mu is None:
            mu = self.search_centers

        if len(mu.shape) == 1:
            mu = mu[None, :]

        n_input, n_dim = mu.shape
        scale = scale * self.search_scales
        mu_idxs = np.random.randint(0, n_input, n)
        samples = scale*(2*np.random.rand(n, n_dim) - 1) + mu[mu_idxs]
        return np.clip(samples, self.min_bounds, self.max_bounds)

    def from_dict(self, params):
        flat_params = dict(pufferlib.unroll_nested_dict(params))
        values = []
        for key, space in self.flat_spaces.items():
            assert key in flat_params, f'Missing hyperparameter {key}'
            val = flat_params[key]
            normed = space.normalize(val)
            values.append(normed)

        return np.array(values)

    def to_dict(self, sample, fill=None):
        params = deepcopy(self.spaces) if fill is None else fill
        self._fill(params, self.spaces, sample)
        return params

    def _fill(self, params, spaces, flat_sample, idx=0):
        for name, space in spaces.items():
            if isinstance(space, dict):
                idx = self._fill(params[name], spaces[name], flat_sample, idx=idx)
            else:
                params[name] = spaces[name].unnormalize(flat_sample[idx])
                idx += 1

        return idx


def pareto_points(observations, eps=1e-6):
    scores = np.array([e['output'] for e in observations])
    costs = np.array([e['cost'] for e in observations])
    pareto = []
    idxs = []
    for idx, obs in enumerate(observations):
        higher_score = scores + eps > scores[idx]
        lower_cost = costs - eps < costs[idx]
        better = higher_score & lower_cost
        better[idx] = False
        if not better.any():
            pareto.append(obs)
            idxs.append(idx)

    return pareto, idxs

class Random:
    def __init__(self,
            sweep_config,
            global_search_scale = 1,
            random_suggestions = 1024,
        ):

        self.hyperparameters = Hyperparameters(sweep_config)
        self.global_search_scale = global_search_scale
        self.random_suggestions = random_suggestions
        self.success_observations = []

    def suggest(self, fill=None):
        suggestions = self.hyperparameters.sample(self.random_suggestions)
        self.suggestion = random.choice(suggestions)
        return self.hyperparameters.to_dict(self.suggestion, fill), {}

    def observe(self, hypers, score, cost, is_failure=False):
        params = self.hyperparameters.from_dict(hypers)
        self.success_observations.append(dict(
            input=hypers,
            output=score,
            cost=cost,
            is_failure=is_failure,
        ))

class ParetoGenetic:
    def __init__(self,
            sweep_config,
            global_search_scale = 1,
            suggestions_per_pareto = 1,
            bias_cost = True,
            log_bias = False,
        ):

        self.hyperparameters = Hyperparameters(sweep_config)
        self.global_search_scale = global_search_scale
        self.suggestions_per_pareto = suggestions_per_pareto
        self.bias_cost = bias_cost
        self.log_bias = log_bias
        self.success_observations = []

    def suggest(self, fill=None):
        if len(self.success_observations) == 0:
            suggestion = self.hyperparameters.search_centers
            return self.hyperparameters.to_dict(suggestion, fill), {}

        candidates, _ = pareto_points(self.success_observations)
        pareto_costs = np.array([e['cost'] for e in candidates])

        if self.bias_cost:
            if self.log_bias:
                cost_dists = np.abs(np.log(pareto_costs[:, None]) - np.log(pareto_costs[None, :]))
            else:
                cost_dists = np.abs(pareto_costs[:, None] - pareto_costs[None, :])

            cost_dists += (np.max(pareto_costs) + 1)*np.eye(len(pareto_costs)) # mask self-distance
            idx = np.argmax(np.min(cost_dists, axis=1))
            search_centers = candidates[idx]['input']
        else:
            search_centers = np.stack([e['input'] for e in candidates])

        suggestions = self.hyperparameters.sample(
            len(candidates)*self.suggestions_per_pareto, mu=search_centers)
        suggestion = suggestions[np.random.randint(0, len(suggestions))]
        return self.hyperparameters.to_dict(suggestion, fill), {}

    def observe(self, hypers, score, cost, is_failure=False):
        params = self.hyperparameters.from_dict(hypers)
        self.success_observations.append(dict(
            input=params,
            output=score,
            cost=cost,
            is_failure=is_failure,
        ))


def create_gp(x_dim, scale_length=1.0):
    # Dummy data
    X = scale_length * torch.ones((1, x_dim))
    y = torch.zeros((1,))

    matern_kernel = gp.kernels.Matern32(input_dim=x_dim, lengthscale=X)
    linear_kernel = gp.kernels.Polynomial(x_dim, degree=1)
    kernel = gp.kernels.Sum(linear_kernel, matern_kernel)

    # Params taken from HEBO: https://arxiv.org/abs/2012.03826
    model = gp.models.GPRegression(X, y, kernel=kernel, jitter=1.0e-4)
    model.noise = pyro.nn.PyroSample(pyro.distributions.LogNormal(math.log(1e-2), 0.5))
    optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)
    return model, optimizer

# TODO: Eval defaults
class Protein:
    def __init__(self,
            sweep_config,
            max_suggestion_cost = 3600,
            resample_frequency = 0,
            num_random_samples = 50,
            global_search_scale = 1,
            random_suggestions = 1024,
            suggestions_per_pareto = 256,
            seed_with_search_center = True,
            expansion_rate = 0.25,
        ):
        self.hyperparameters = Hyperparameters(sweep_config)
        self.num_random_samples = num_random_samples
        self.global_search_scale = global_search_scale
        self.random_suggestions = random_suggestions
        self.suggestions_per_pareto = suggestions_per_pareto
        self.seed_with_search_center = seed_with_search_center
        self.resample_frequency = resample_frequency
        self.max_suggestion_cost = max_suggestion_cost
        self.expansion_rate = expansion_rate

        self.success_observations = []
        self.failure_observations = []
        self.suggestion_idx = 0

        self.gp_score, self.score_opt = create_gp(self.hyperparameters.num)
        self.gp_cost, self.cost_opt = create_gp(self.hyperparameters.num)

    def suggest(self, fill):
        # TODO: Clip random samples to bounds so we don't get bad high cost samples
        info = {}
        self.suggestion_idx += 1
        if len(self.success_observations) == 0 and self.seed_with_search_center:
            best = self.hyperparameters.search_centers
            return self.hyperparameters.to_dict(best, fill), info
        elif not self.seed_with_search_center and len(self.success_observations) < self.num_random_samples:
            suggestions = self.hyperparameters.sample(self.random_suggestions)
            self.suggestion = random.choice(suggestions)
            return self.hyperparameters.to_dict(self.suggestion, fill), info
        elif self.resample_frequency and self.suggestion_idx % self.resample_frequency == 0:
            candidates, _ = pareto_points(self.success_observations)
            suggestions = np.stack([e['input'] for e in candidates])
            best_idx = np.random.randint(0, len(candidates))
            best = suggestions[best_idx]
            return self.hyperparameters.to_dict(best, fill), info

        params = np.array([e['input'] for e in self.success_observations])
        params = torch.from_numpy(params)

        # Scores variable y
        y = np.array([e['output'] for e in self.success_observations])

        # Transformed scores
        min_score = np.min(y)
        max_score = np.max(y)
        y_norm = (y - min_score) / (np.abs(max_score - min_score) + 1e-6)

        self.gp_score.set_data(params, torch.from_numpy(y_norm))
        self.gp_score.train()
        gp.util.train(self.gp_score, self.score_opt)
        self.gp_score.eval()

        # Log costs
        c = np.array([e['cost'] for e in self.success_observations])

        log_c = np.log(c)

        # Linear input norm creates clean 1 mean fn
        log_c_min = np.min(log_c)
        log_c_max = np.max(log_c)
        log_c_norm = (log_c - log_c_min) / (log_c_max - log_c_min + 1e-6)

        self.gp_cost.mean_function = lambda x: 1
        self.gp_cost.set_data(params, torch.from_numpy(log_c_norm))
        self.gp_cost.train()
        gp.util.train(self.gp_cost, self.cost_opt)
        self.gp_cost.eval()

        candidates, pareto_idxs = pareto_points(self.success_observations)
        pareto_costs = np.array([e['cost'] for e in candidates])

        ### Sample suggestions
        search_centers = np.stack([e['input'] for e in candidates])
        suggestions = self.hyperparameters.sample(
            len(candidates)*self.suggestions_per_pareto, mu=search_centers)

        ### Predict scores and costs
        suggestions = torch.from_numpy(suggestions)
        with torch.no_grad():
            gp_y_norm, gp_y_norm_var = self.gp_score(suggestions)
            gp_log_c_norm, gp_log_c_norm_var = self.gp_cost(suggestions)

        gp_y_norm = gp_y_norm.numpy()
        gp_log_c_norm = gp_log_c_norm.numpy()

        # Unlinearize
        gp_y = gp_y_norm*(max_score - min_score) + min_score

        gp_log_c = gp_log_c_norm*(log_c_max - log_c_min) + log_c_min
        gp_c = np.exp(gp_log_c)

        gp_c_min = np.min(gp_c)
        gp_c_max = np.max(gp_c)
        gp_c_norm = (gp_c - gp_c_min) / (gp_c_max - gp_c_min + 1e-6)

        pareto_y = y[pareto_idxs]
        pareto_c = c[pareto_idxs]
        pareto_log_c_norm = log_c_norm[pareto_idxs]

        max_c = np.max(c)
        min_c = np.min(c)

        max_c_mask = gp_c < self.max_suggestion_cost

        target = (1 + self.expansion_rate)*np.random.rand()
        weight = 1 - abs(target - gp_log_c_norm)

        suggestion_scores = self.hyperparameters.optimize_direction * max_c_mask * (
                gp_y_norm*weight)

        best_idx = np.argmax(suggestion_scores)
        info = dict(
            cost = gp_c[best_idx].item(),
            score = gp_y[best_idx].item(),
            rating = suggestion_scores[best_idx].item(),
        )
        print('Predicted -- ',
            f'Score: {info["score"]:.3f}',
            f'Cost: {info["cost"]:.3f}',
            f'Rating: {info["rating"]:.3f}',
        )
        '''
        if info['rating'] < 10:
            from bokeh.models import ColumnDataSource, LinearColorMapper
            from bokeh.plotting import figure, show
            from bokeh.palettes import Turbo256

            source = ColumnDataSource(data=dict(
                x=c,
                y=y,
                order=np.argsort(c),
            ))
            mapper = LinearColorMapper(
                palette=Turbo256,
                low=0,
                high=len(c)
            )

            idxs = np.argsort(pareto_c)
            pareto_source = ColumnDataSource(data=dict(
                x=pareto_c[idxs],
                y=pareto_y[idxs],
            ))

            c_sorted = sorted(c)
            cost_source = ColumnDataSource(data=dict(
                x = c_sorted,
                y = np.cumsum(c_sorted) / np.sum(c_sorted),
            ))

            #gp_pareto_source = ColumnDataSource(data=dict(
            #    x=gp_c,
            #    y=gp_y,
            #    order=np.argsort(gp_c),
            #))

            preds = [{
                'output': gp_y[i],
                'cost': gp_c[i],
            } for i in range(len(gp_c))]
            _, pareto_idxs = pareto_points(preds)

            gp_c_pareto = gp_c[pareto_idxs]
            gp_y_pareto = gp_y[pareto_idxs]
            idxs = np.argsort(gp_c_pareto)
            gp_source = ColumnDataSource(data=dict(
                x=gp_c_pareto[idxs],
                y=gp_y_pareto[idxs],
            ))

            p = figure(title='Hyperparam Test', 
                       x_axis_label='Cost', 
                       y_axis_label='Score')

            # Original data
            p.scatter(
                x='x', 
                y='y', 
                color={'field': 'order', 'transform': mapper}, 
                size=10, 
                source=source
            )

            p.line(x='x', y='y', color='red', source=pareto_source)
            p.line(x='x', y='y', color='blue', source=gp_source)
            p.line(x='x', y='y', color='green', source=cost_source)
            #p.line(x='x', y='y', color='green', source=gp_pareto_source)

            show(p)
        '''

        best = suggestions[best_idx].numpy()
        return self.hyperparameters.to_dict(best, fill), info

    def observe(self, hypers, score, cost, is_failure=False):
        params = self.hyperparameters.from_dict(hypers)
        new_observation = dict(
            input=params,
            output=score,
            cost=cost,
            is_failure=is_failure,
        )

        if len(self.success_observations) == 0:
            self.success_observations.append(new_observation)
            return

        success_params = np.stack([e['input'] for e in self.success_observations])
        dist = np.linalg.norm(params - success_params, axis=1)
        same = np.where(dist < 1e-6)[0]
        if len(same) > 0:
            self.success_observations[same[0]] = new_observation
        else:
            self.success_observations.append(new_observation)

def _carbs_params_from_puffer_sweep(sweep_config):
    from carbs import (
        Param,
        LinearSpace,
        LogSpace,
        LogitSpace,
    )

    param_spaces = {}
    for name, param in sweep_config.items():
        if name in ('method', 'name', 'metric', 'max_score'):
            continue

        assert isinstance(param, dict)
        if any(isinstance(param[k], dict) for k in param):
            param_spaces[name] = _carbs_params_from_puffer_sweep(param)
            continue
 
        assert 'distribution' in param
        distribution = param['distribution']
        search_center = param['mean']
        kwargs = dict(
            min=param['min'],
            max=param['max'],
        )
        if distribution == 'uniform':
            space = LinearSpace(**kwargs)
        elif distribution in ('int_uniform', 'uniform_pow2'):
            space = LinearSpace(**kwargs, is_integer=True)
        elif distribution == 'log_normal':
            space = LogSpace(**kwargs)
        elif distribution == 'logit_normal':
            space = LogitSpace(**kwargs)
        else:
            raise ValueError(f'Invalid distribution: {distribution}')

        param_spaces[name] = Param(
            name=name,
            space=space,
            search_center=search_center
        )

    return param_spaces

class Carbs:
    def __init__(self,
            sweep_config: dict,
            max_suggestion_cost: float = 3600,
            resample_frequency: int = 5,
            num_random_samples: int = 10,
        ):

        param_spaces = _carbs_params_from_puffer_sweep(sweep_config)
        flat_spaces = [e[1] for e in pufferlib.unroll_nested_dict(param_spaces)]
        for e in flat_spaces:
            print(e.name, e.space)

        from carbs import (
            CARBSParams,
            CARBS,
        )

        carbs_params = CARBSParams(
            better_direction_sign=1,
            is_wandb_logging_enabled=False,
            resample_frequency=resample_frequency,
            num_random_samples=num_random_samples,
            max_suggestion_cost=max_suggestion_cost,
            is_saved_on_every_observation=False,
        )
        self.carbs = CARBS(carbs_params, flat_spaces)

    def suggest(self, args):
        self.suggestion = self.carbs.suggest().suggestion
        for k in ('train', 'env'):
            for name, param in args['sweep'][k].items():
                if name in self.suggestion:
                    args[k][name] = self.suggestion[name]

    def observe(self, hypers, score, cost, is_failure=False):
        from carbs import ObservationInParam
        self.carbs.observe(
            ObservationInParam(
                input=self.suggestion,
                output=score,
                cost=cost,
                is_failure=is_failure,
            )
        )



================================================
FILE: pufferlib/vector.py
================================================
# TODO: Check actions passed to envs are right shape? On first call at least

from pdb import set_trace as T

import numpy as np
import time
import psutil

from pufferlib.emulation import GymnasiumPufferEnv, PettingZooPufferEnv
from pufferlib import PufferEnv, set_buffers
import pufferlib.spaces
import gymnasium

RESET = 0
STEP = 1
SEND = 2
RECV = 3
CLOSE = 4
MAIN = 5
INFO = 6

def recv_precheck(vecenv):
    if vecenv.flag != RECV:
        raise pufferlib.APIUsageError('Call reset before stepping')

    vecenv.flag = SEND

def send_precheck(vecenv, actions):
    if vecenv.flag != SEND:
        raise pufferlib.APIUsageError('Call (async) reset + recv before sending')

    actions = np.asarray(actions)
    if not vecenv.initialized:
        vecenv.initialized = True
        if not vecenv.action_space.contains(actions):
            raise pufferlib.APIUsageError('Actions do not match action space')

    vecenv.flag = RECV
    return actions

def reset(vecenv, seed=42):
    vecenv.async_reset(seed)
    obs, rewards, terminals, truncations, infos, env_ids, masks = vecenv.recv()
    return obs, infos

def step(vecenv, actions):
    actions = np.asarray(actions)
    vecenv.send(actions)
    obs, rewards, terminals, truncations, infos, env_ids, masks = vecenv.recv()
    return obs, rewards, terminals, truncations, infos # include env_ids or no?

class Serial:
    reset = reset
    step = step

    @property
    def num_envs(self):
        return self.agents_per_batch
 
    def __init__(self, env_creators, env_args, env_kwargs, num_envs, buf=None, seed=0, **kwargs):
        self.driver_env = env_creators[0](*env_args[0], **env_kwargs[0])
        self.agents_per_batch = self.driver_env.num_agents * num_envs
        self.num_agents = self.agents_per_batch

        self.single_observation_space = self.driver_env.single_observation_space
        self.single_action_space = self.driver_env.single_action_space
        self.action_space = pufferlib.spaces.joint_space(self.single_action_space, self.agents_per_batch)
        self.observation_space = pufferlib.spaces.joint_space(self.single_observation_space, self.agents_per_batch)


        set_buffers(self, buf)

        self.envs = []
        ptr = 0
        for i in range(num_envs):
            end = ptr + self.driver_env.num_agents
            buf_i = dict(
                observations=self.observations[ptr:end],
                rewards=self.rewards[ptr:end],
                terminals=self.terminals[ptr:end],
                truncations=self.truncations[ptr:end],
                masks=self.masks[ptr:end],
                actions=self.actions[ptr:end]
            )
            ptr = end
            seed_i = seed + i if seed is not None else None
            env = env_creators[i](*env_args[i], buf=buf_i, seed=seed_i, **env_kwargs[i])
            self.envs.append(env)

        self.driver_env = driver = self.envs[0]
        self.emulated = self.driver_env.emulated
        check_envs(self.envs, self.driver_env)
        self.agents_per_env = [env.num_agents for env in self.envs]
        assert sum(self.agents_per_env) == self.agents_per_batch
        self.agent_ids = np.arange(self.num_agents)
        self.initialized = False
        self.flag = RESET

    def _avg_infos(self):
        infos = {}
        for e in self.infos:
            for k, v in pufferlib.unroll_nested_dict(e):
                if k not in infos:
                    infos[k] = []

                if isinstance(v, list):
                    infos[k].append(np.mean(v))
                else:
                    infos[k].append(v)

        for k in list(infos.keys()):
            try:
                infos[k] = np.mean(infos[k])
            except:
                del infos[k]

    def async_reset(self, seed=None):
        self.flag = RECV
        infos = []
        for i, env in enumerate(self.envs):
            if seed is None:
                ob, i = env.reset()
            else:
                ob, i = env.reset(seed=seed+i)
               
            if isinstance(i, list):
                infos.extend(i)
            else:
                infos.append(i)

        self.infos = infos
        self._avg_infos()

    def send(self, actions):
        if not actions.flags.contiguous:
            actions = np.ascontiguousarray(actions)

        actions = send_precheck(self, actions)
        rewards, dones, truncateds, self.infos = [], [], [], []
        ptr = 0
        for idx, env in enumerate(self.envs):
            end = ptr + self.agents_per_env[idx]
            atns = actions[ptr:end]
            if env.done:
                o, i = env.reset()
            else:
                o, r, d, t, i = env.step(atns)

            if i:
                if isinstance(i, list):
                    self.infos.extend(i)
                else:
                    self.infos.append(i)

            ptr = end

        self._avg_infos()

    def notify(self):
        for env in self.envs:
            env.notify()

    def recv(self):
        recv_precheck(self)
        return (self.observations, self.rewards, self.terminals, self.truncations,
            self.infos, self.agent_ids, self.masks)

    def close(self):
        for env in self.envs:
            env.close()

def _worker_process(env_creators, env_args, env_kwargs, obs_shape, obs_dtype, atn_shape, atn_dtype,
        num_envs, num_agents, num_workers, worker_idx, send_pipe, recv_pipe, shm, is_native, seed):

    # Environments read and write directly to shared memory
    shape = (num_workers, num_envs*num_agents)
    atn_arr = np.ndarray((*shape, *atn_shape),
        dtype=atn_dtype, buffer=shm['actions'])[worker_idx]
    buf = dict(
        observations=np.ndarray((*shape, *obs_shape),
            dtype=obs_dtype, buffer=shm['observations'])[worker_idx],
        rewards=np.ndarray(shape, dtype=np.float32, buffer=shm['rewards'])[worker_idx],
        terminals=np.ndarray(shape, dtype=bool, buffer=shm['terminals'])[worker_idx],
        truncations=np.ndarray(shape, dtype=bool, buffer=shm['truncateds'])[worker_idx],
        masks=np.ndarray(shape, dtype=bool, buffer=shm['masks'])[worker_idx],
        actions=atn_arr,
    )
    buf['masks'][:] = True

    if is_native and num_envs == 1:
        envs = env_creators[0](*env_args[0], **env_kwargs[0], buf=buf, seed=seed)
    else:
        envs = Serial(env_creators, env_args, env_kwargs, num_envs, buf=buf, seed=seed*num_envs)

    semaphores=np.ndarray(num_workers, dtype=np.uint8, buffer=shm['semaphores'])
    notify=np.ndarray(num_workers, dtype=bool, buffer=shm['notify'])
    start = time.time()
    while True:
        if notify[worker_idx]:
            envs.notify()
            notify[worker_idx] = False

        sem = semaphores[worker_idx]
        if sem >= MAIN:
            if time.time() - start > 0.5:
                time.sleep(0.01)
            continue

        start = time.time()
        if sem == RESET:
            seed = recv_pipe.recv()
            _, infos = envs.reset(seed=seed)
        elif sem == STEP:
            _, _, _, _, infos = envs.step(atn_arr)
        elif sem == CLOSE:
            envs.close()
            send_pipe.send(None)
            break

        if infos:
            semaphores[worker_idx] = INFO
            send_pipe.send(infos)
        else:
            semaphores[worker_idx] = MAIN

class Multiprocessing:
    '''Runs environments in parallel using multiprocessing

    Use this vectorization module for most applications
    '''
    reset = reset
    step = step

    @property
    def num_envs(self):
        return self.agents_per_batch
 
    def __init__(self, env_creators, env_args, env_kwargs,
            num_envs, num_workers=None, batch_size=None,
            zero_copy=True, sync_traj=True, overwork=False, seed=0, **kwargs):
        if batch_size is None:
            batch_size = num_envs
        if num_workers is None:
            num_workers = num_envs

        import psutil
        cpu_cores = psutil.cpu_count(logical=False)
        if num_workers > cpu_cores and not overwork:
            raise pufferlib.APIUsageError(' '.join([
                f'num_workers ({num_workers}) > hardware cores ({cpu_cores}) is disallowed by default.',
                'PufferLib multiprocessing is heavily optimized for 1 process per hardware core.',
                'If you really want to do this, set overwork=True (--vec-overwork in our demo.py).',
            ]))

        num_batches = num_envs / batch_size
        if zero_copy and num_batches != int(num_batches):
            # This is so you can have n equal buffers
            raise pufferlib.APIUsageError(
                'zero_copy: num_envs must be divisible by batch_size')

        self.num_environments = num_envs
        envs_per_worker = num_envs // num_workers
        self.envs_per_worker = envs_per_worker
        self.workers_per_batch = batch_size // envs_per_worker
        self.num_workers = num_workers

        # I really didn't want to need a driver process... with mp.shared_memory
        # we can fetch this data from the worker processes and ever perform
        # additional space checks. Unfortunately, SharedMemory has a janky integration
        # with the resource tracker that spams warnings and does not work with
        # forked processes. So for now, RawArray is much more reliable.
        # You can't send a RawArray through a pipe.
        self.driver_env = driver_env = env_creators[0](*env_args[0], **env_kwargs[0])
        is_native = isinstance(driver_env, PufferEnv)
        self.emulated = False if is_native else driver_env.emulated
        self.num_agents = num_agents = driver_env.num_agents * num_envs
        self.agents_per_batch = driver_env.num_agents * batch_size
        agents_per_worker = driver_env.num_agents * envs_per_worker
        obs_space = driver_env.single_observation_space
        obs_shape = obs_space.shape
        obs_dtype = obs_space.dtype
        obs_ctype = np.ctypeslib.as_ctypes_type(obs_dtype)
        atn_space = driver_env.single_action_space
        atn_shape = atn_space.shape
        atn_dtype = atn_space.dtype
        if isinstance(atn_space, (pufferlib.spaces.Discrete, pufferlib.spaces.MultiDiscrete)):
            atn_dtype = np.int32

        atn_ctype = np.ctypeslib.as_ctypes_type(atn_dtype)

        self.single_observation_space = driver_env.single_observation_space
        self.single_action_space = driver_env.single_action_space
        self.action_space = pufferlib.spaces.joint_space(self.single_action_space, self.agents_per_batch)
        self.observation_space = pufferlib.spaces.joint_space(self.single_observation_space, self.agents_per_batch)
        self.agent_ids = np.arange(num_agents).reshape(num_workers, agents_per_worker)

        from multiprocessing import RawArray, set_start_method
        # Mac breaks without setting fork... but setting it breaks sweeps on 2nd run
        #set_start_method('fork')
        self.shm = dict(
            observations=RawArray(obs_ctype, num_agents * int(np.prod(obs_shape))),
            actions=RawArray(atn_ctype, num_agents * int(np.prod(atn_shape))),
            rewards=RawArray('f', num_agents),
            terminals=RawArray('b', num_agents),
            truncateds=RawArray('b', num_agents),
            masks=RawArray('b', num_agents),
            semaphores=RawArray('c', num_workers),
            notify=RawArray('b', num_workers),
        )
        shape = (num_workers, agents_per_worker)
        self.obs_batch_shape = (self.agents_per_batch, *obs_shape)
        self.atn_batch_shape = (self.workers_per_batch, agents_per_worker, *atn_shape)
        self.actions = np.ndarray((*shape, *atn_shape),
            dtype=atn_dtype, buffer=self.shm['actions'])
        self.buf = dict(
            observations=np.ndarray((*shape, *obs_shape),
                dtype=obs_dtype, buffer=self.shm['observations']),
            rewards=np.ndarray(shape, dtype=np.float32, buffer=self.shm['rewards']),
            terminals=np.ndarray(shape, dtype=bool, buffer=self.shm['terminals']),
            truncations=np.ndarray(shape, dtype=bool, buffer=self.shm['truncateds']),
            masks=np.ndarray(shape, dtype=bool, buffer=self.shm['masks']),
            semaphores=np.ndarray(num_workers, dtype=np.uint8, buffer=self.shm['semaphores']),
            notify=np.ndarray(num_workers, dtype=bool, buffer=self.shm['notify']),
        )
        self.buf['semaphores'][:] = MAIN 

        from multiprocessing import Pipe, Process
        self.send_pipes, w_recv_pipes = zip(*[Pipe() for _ in range(num_workers)])
        w_send_pipes, self.recv_pipes = zip(*[Pipe() for _ in range(num_workers)])
        self.recv_pipe_dict = {p: i for i, p in enumerate(self.recv_pipes)}

        self.processes = []
        for i in range(num_workers):
            start = i * envs_per_worker
            end = start + envs_per_worker
            seed_i = seed + i if seed is not None else None
            p = Process(
                target=_worker_process,
                args=(env_creators[start:end], env_args[start:end],
                    env_kwargs[start:end], obs_shape, obs_dtype,
                    atn_shape, atn_dtype, envs_per_worker, driver_env.num_agents,
                    num_workers, i, w_send_pipes[i], w_recv_pipes[i],
                    self.shm, is_native, seed_i)
            )
            p.start()
            self.processes.append(p)

        self.flag = RESET
        self.initialized = False
        self.zero_copy = zero_copy
        self.sync_traj = sync_traj

        self.ready_workers = []
        self.waiting_workers = []

    def recv(self):
        recv_precheck(self)
        while True:
            # Bandaid patch for new experience buffer desync
            if self.sync_traj:
                worker = self.waiting_workers[0]
                sem = self.buf['semaphores'][worker]
                if sem >= MAIN:
                    self.waiting_workers.pop(0)
                    self.ready_workers.append(worker)
            else:
                worker = self.waiting_workers.pop(0)
                sem = self.buf['semaphores'][worker]
                if sem >= MAIN:
                    self.ready_workers.append(worker)
                else:
                    self.waiting_workers.append(worker)

            if sem == INFO:
                self.infos[worker] = self.recv_pipes[worker].recv()

            if not self.ready_workers:
                continue

            if self.workers_per_batch == 1:
                # Fastest path. Zero-copy optimized for batch size 1
                w_slice = self.ready_workers[0]
                s_range = [w_slice]
                self.waiting_workers.append(w_slice)
                self.ready_workers.pop(0)
                break
            elif self.workers_per_batch == self.num_workers:
                # Slowest path. Zero-copy synchornized for all workers
                if len(self.ready_workers) < self.num_workers:
                    continue

                w_slice = slice(0, self.num_workers)
                s_range = range(0, self.num_workers)
                self.waiting_workers.extend(s_range)
                self.ready_workers = []
                break
            elif self.zero_copy:
                # Zero-copy for batch size > 1. Has to wait for
                # a contiguous block of workers and adds a few
                # microseconds of extra index processing time
                completed = np.zeros(self.num_workers, dtype=bool)
                completed[self.ready_workers] = True
                buffers = completed.reshape(
                    -1, self.workers_per_batch).all(axis=1)
                start = buffers.argmax()
                if not buffers[start]:
                    continue

                start *= self.workers_per_batch
                end = start + self.workers_per_batch
                w_slice = slice(start, end)
                s_range = range(start, end)
                self.waiting_workers.extend(s_range)
                self.ready_workers = [e for e in self.ready_workers
                    if e not in s_range]
                break
            elif len(self.ready_workers) >= self.workers_per_batch:
                # Full async path for batch size > 1. Alawys copies
                # data because of non-contiguous worker indices
                # Can be faster for envs with small observations
                w_slice = self.ready_workers[:self.workers_per_batch]
                s_range = w_slice
                self.waiting_workers.extend(s_range)
                self.ready_workers = self.ready_workers[self.workers_per_batch:]
                break

        self.w_slice = w_slice
        buf = self.buf

        o = buf['observations'][w_slice].reshape(self.obs_batch_shape)
        r = buf['rewards'][w_slice].ravel()
        d = buf['terminals'][w_slice].ravel()
        t = buf['truncations'][w_slice].ravel()

        infos = []
        for i in s_range:
            if self.infos[i]:
                infos.extend(self.infos[i])
                self.infos[i] = []

        agent_ids = self.agent_ids[w_slice].ravel()
        m = buf['masks'][w_slice].ravel()
        self.batch_mask = m

        return o, r, d, t, infos, agent_ids, m

    def send(self, actions):
        actions = send_precheck(self, actions).reshape(self.atn_batch_shape)
        # TODO: What shape?
        
        idxs = self.w_slice
        self.actions[idxs] = actions
        self.buf['semaphores'][idxs] = STEP

    def async_reset(self, seed=0):
        # Flush any waiting workers
        while self.waiting_workers:
            worker = self.waiting_workers.pop(0)
            sem = self.buf['semaphores'][worker]
            if sem >= MAIN:
                self.ready_workers.append(worker)
                if sem == INFO:
                    self.recv_pipes[worker].recv()
            else:
                self.waiting_workers.append(worker)

        self.flag = RECV
        self.prev_env_id = []
        self.flag = RECV

        self.ready_workers = []
        self.ready_next_workers = [] # Used to evenly sample workers
        self.waiting_workers = list(range(self.num_workers))
        self.infos = [[] for _ in range(self.num_workers)]

        self.buf['semaphores'][:] = RESET
        for i in range(self.num_workers):
            start = i*self.envs_per_worker
            end = (i+1)*self.envs_per_worker
            self.send_pipes[i].send(seed+i)

    def notify(self):
        self.buf['notify'][:] = True

    def close(self):
        self.driver_env.close()
        for p in self.processes:
            p.terminate()

class Ray():
    '''Runs environments in parallel on multiple processes using Ray

    Use this module for distributed simulation on a cluster.
    '''
    reset = reset
    step = step

    def __init__(self, env_creators, env_args, env_kwargs, num_envs,
            num_workers=None, batch_size=None, **kwargs):
        if batch_size is None:
            batch_size = num_envs
        if num_workers is None:
            num_workers = num_envs

        self.env_pool = num_envs != batch_size
        envs_per_worker = num_envs // num_workers
        self.workers_per_batch = batch_size // envs_per_worker
        self.num_workers = num_workers

        driver_env = env_creators[0](*env_args[0], **env_kwargs[0])
        self.driver_env = driver_env
        self.emulated = driver_env.emulated
        self.num_agents = num_agents = driver_env.num_agents * num_envs
        self.agents_per_batch = driver_env.num_agents * batch_size
        agents_per_worker = driver_env.num_agents * envs_per_worker
        obs_space = driver_env.single_observation_space
        obs_shape = obs_space.shape
        atn_space = driver_env.single_action_space
        atn_shape = atn_space.shape

        shape = (num_workers, agents_per_worker)
        self.obs_batch_shape = (self.agents_per_batch, *obs_shape)
        self.atn_batch_shape = (self.workers_per_batch, agents_per_worker, *atn_shape)

        self.single_observation_space = driver_env.single_observation_space
        self.single_action_space = driver_env.single_action_space
        self.action_space = pufferlib.spaces.joint_space(self.single_action_space, self.agents_per_batch)
        self.observation_space = pufferlib.spaces.joint_space(self.single_observation_space, self.agents_per_batch)
 
        self.agent_ids = np.arange(num_agents).reshape(num_workers, agents_per_worker)

        import ray
        if not ray.is_initialized():
            import logging
            ray.init(
                include_dashboard=False,  # WSL Compatibility
                logging_level=logging.ERROR,
            )

        self.envs = []
        for i in range(num_workers):
            start = i * envs_per_worker
            end = start + envs_per_worker
            self.envs.append(
                ray.remote(Serial).remote(
                    env_creators[start:end],
                    env_args[start:end],
                    env_kwargs[start:end],
                    envs_per_worker
                )
            )

        self.async_handles = None
        self.initialized = False
        self.flag = RESET
        self.ray = ray
        self.prev_env_id = []

    def recv(self):
        recv_precheck(self)
        recvs = []
        next_env_id = []
        workers_per_batch = self.workers_per_batch
        if self.env_pool:
            recvs = self.ray.get(self.async_handles[:workers_per_batch])
            env_id = [_ for _ in range(workers_per_batch)]
        else:
            ready, busy = self.ray.wait(
                self.async_handles, num_returns=workers_per_batch)
            env_id = [self.async_handles.index(e) for e in ready]
            recvs = self.ray.get(ready)

        o, r, d, t, infos, ids, m = zip(*recvs)
        self.prev_env_id = env_id

        infos = [i for ii in infos for i in ii]

        o = np.stack(o, axis=0).reshape(self.obs_batch_shape)
        r = np.stack(r, axis=0).ravel()
        d = np.stack(d, axis=0).ravel()
        t = np.stack(t, axis=0).ravel()
        m = np.stack(m, axis=0).ravel()
        agent_ids = self.agent_ids[env_id].ravel()
        return o, r, d, t, infos, agent_ids, m

    def send(self, actions):
        actions = send_precheck(self, actions).reshape(self.atn_batch_shape)
        # TODO: What shape?

        handles = []
        for i, e in enumerate(self.prev_env_id):
            atns = actions[i]
            env = self.envs[e]
            env.send.remote(atns)
            handles.append(env.recv.remote())

        self.async_handles = handles

    def async_reset(self, seed=42):
        self.flag = RECV
        if seed is None:
            kwargs = {}
        else:
            kwargs = {"seed": seed}

        handles = []
        for idx, e in enumerate(self.envs):
            e.async_reset.remote(**kwargs)
            handles.append(e.recv.remote())

        self.async_handles = handles

    def close(self):
        self.ray.get([e.close.remote() for e in self.envs])
        self.ray.shutdown()


def make(env_creator_or_creators, env_args=None, env_kwargs=None, backend=PufferEnv, num_envs=1, seed=0, **kwargs):
    if num_envs < 1:
        raise pufferlib.APIUsageError('num_envs must be at least 1')
    if num_envs != int(num_envs):
        raise pufferlib.APIUsageError('num_envs must be an integer')

    if isinstance(backend, str):
        try:
            backend = getattr(pufferlib.vector, backend)
        except:
            raise pufferlib.APIUsageError(f'Invalid backend: {backend}')

    if backend == PufferEnv:
        env_args = env_args or []
        env_kwargs = env_kwargs or {}
        vecenv = env_creator_or_creators(*env_args, **env_kwargs)
        if not isinstance(vecenv, PufferEnv):
            raise pufferlib.APIUsageError('Native vectorization requires a native PufferEnv. Use Serial or Multiprocessing instead.')
        if num_envs != 1:
            raise pufferlib.APIUsageError('Native vectorization is for PufferEnvs that handle all per-process vectorization internally. If you want to run multiple separate Python instances on a single process, use Serial or Multiprocessing instead')

        return vecenv

    if 'num_workers' in kwargs:
        if kwargs['num_workers'] == 'auto':
            kwargs['num_workers'] = num_envs

        # TODO: None?
        envs_per_worker = num_envs / kwargs['num_workers']
        if envs_per_worker != int(envs_per_worker):
            raise pufferlib.APIUsageError('num_envs must be divisible by num_workers')

        if 'batch_size' in kwargs:
            if kwargs['batch_size'] == 'auto':
                if num_envs == 1:
                    kwargs['batch_size'] = 1
                else:
                    kwargs['batch_size'] = num_envs // 2

            batch_size = kwargs['batch_size']
            if batch_size is None:
                batch_size = num_envs

            if batch_size % envs_per_worker != 0:
                raise pufferlib.APIUsageError(
                    'batch_size must be divisible by (num_envs / num_workers)')
        
 
    if env_args is None:
        env_args = []

    if env_kwargs is None:
        env_kwargs = {}

    if not isinstance(env_creator_or_creators, (list, tuple)):
        env_creators = [env_creator_or_creators] * num_envs
        env_args = [env_args] * num_envs
        env_kwargs = [env_kwargs] * num_envs
    else:
        env_creators = env_creator_or_creators

    if len(env_creators) != num_envs:
        raise pufferlib.APIUsageError('env_creators must be a list of length num_envs')
    if len(env_args) != num_envs:
        raise pufferlib.APIUsageError('env_args must be a list of length num_envs')
    if len(env_kwargs) != num_envs:
        raise pufferlib.APIUsageError('env_kwargs must be a list of length num_envs')

    for i in range(num_envs):
        if not callable(env_creators[i]):
            raise pufferlib.APIUsageError('env_creators must be a list of callables')
        if not isinstance(env_args[i], (list, tuple)):
            raise pufferlib.APIUsageError('env_args must be a list of lists or tuples')
        if not isinstance(env_kwargs[i], dict):
            raise pufferlib.APIUsageError('env_kwargs must be a list of dictionaries')

    # Keeps batch size consistent when debugging with Serial backend
    if backend is Serial and 'batch_size' in kwargs:
        num_envs = kwargs['batch_size']

    # TODO: Check num workers is not greater than num envs. This results in
    # different Serial vs Multiprocessing behavior

    # Sanity check args
    for k in kwargs:
        if k not in ['num_workers', 'batch_size', 'zero_copy', 'overwork', 'backend']:
            raise pufferlib.APIUsageError(f'Invalid argument: {k}')

    # TODO: First step action space check
    
    return backend(env_creators, env_args, env_kwargs, num_envs, **kwargs)

def make_seeds(seed, num_envs):
    if isinstance(seed, int):
        return [seed + i for i in range(num_envs)]

    err = f'seed {seed} must be an integer or a list of integers'
    if isinstance(seed, (list, tuple)):
        if len(seed) != num_envs:
            raise pufferlib.APIUsageError(err)

        return seed

    raise pufferlib.APIUsageError(err)

def check_envs(envs, driver):
    valid = (PufferEnv, GymnasiumPufferEnv, PettingZooPufferEnv)
    if not isinstance(driver, valid):
        raise pufferlib.APIUsageError(f'env_creator must be {valid}')

    driver_obs = driver.single_observation_space
    driver_atn = driver.single_action_space
    for env in envs:
        if not isinstance(env, valid):
            raise pufferlib.APIUsageError(f'env_creators must be {valid}')
        obs_space = env.single_observation_space
        if obs_space != driver_obs:
            raise pufferlib.APIUsageError(f'\n{obs_space}\n{driver_obs} obs space mismatch')
        atn_space = env.single_action_space
        if atn_space != driver_atn:
            raise pufferlib.APIUsageError(f'\n{atn_space}\n{driver_atn} atn space mismatch')

def autotune(env_creator, batch_size, max_envs=194, model_forward_s=0.0,
        max_env_ram_gb=32, max_batch_vram_gb=0.05, time_per_test=5): 
    '''Determine the optimal vectorization parameters for your system'''
    # TODO: fix multiagent

    if batch_size is None:
        raise ValueError('batch_size must not be None')

    if max_envs < batch_size:
        raise ValueError('max_envs < min_batch_size')

    num_cores = psutil.cpu_count(logical=False)
    idle_ram = psutil.Process().memory_info().rss
    load_ram = idle_ram

    # Initial profile to estimate single-core performance
    print('Profiling single-core performance for ~', time_per_test, 'seconds')
    env = env_creator()
    env.reset()
    obs_space = env.single_observation_space
    actions = [
        np.array([env.single_action_space.sample()
            for _ in range(env.num_agents)])
        for _ in range(1000)
    ]

    num_agents = env.num_agents
    steps = 0
    step_times = []
    reset_times = []
    start = time.time()
    while time.time() - start < time_per_test:
        idle_ram = max(idle_ram, psutil.Process().memory_info().rss)
        s = time.time()
        if env.done:
            env.reset()
            reset_times.append(time.time() - s)
        else:
            env.step(actions[steps%1000])
            step_times.append(time.time() - s)
        steps += 1

    env.close()
    sum_time = sum(step_times) + sum(reset_times)
    reset_percent = 100 * sum(reset_times) / sum_time
    sps = steps * num_agents / sum_time
    step_variance = 100 * np.std(step_times) / np.mean(step_times)
    reset_mean = np.mean(reset_times)
    ram_usage = max(1, (idle_ram - load_ram)) / 1e9

    obs_size_gb = (
        np.prod(obs_space.shape)
        * np.dtype(obs_space.dtype).itemsize
        * num_agents
        / 1e9
    )

    # Max bandwidth
    bandwidth = obs_size_gb * sps
    throughput = bandwidth * num_cores

    print('Profile complete')
    print(f'    SPS: {sps:.3f}')
    print(f'    STD: {step_variance:.3f}%')
    print(f'    Reset: {reset_percent:.3f}%')
    print(f'    RAM: {1000*ram_usage:.3f} MB/env')
    print(f'    Bandwidth: {bandwidth:.3f} GB/s')
    print(f'    Throughput: {throughput:.3f} GB/s ({num_cores} cores)')
    print()

    # Cap envs based on max allowed RAM
    max_allowed_by_ram = max_env_ram_gb // ram_usage
    if max_allowed_by_ram < max_envs:
        max_envs = int(max_allowed_by_ram)
        print('Reducing max envs to', max_envs, 'based on RAM')

    # Cap envs based on estimated max speedup
    #linear_speedup = (num_cores * steps / sum_time) // 500
    #if linear_speedup < max_envs and linear_speedup > num_cores:
    #    max_envs = int(linear_speedup)
    #    print('Reducing max envs to', max_envs, 'based on single-core speed')

    # Cap envs based on hardware
    hardware_envs = max_envs - (max_envs % num_cores)
    if hardware_envs > batch_size and hardware_envs != max_envs:
        max_envs = int(hardware_envs)
        print('Reducing max envs to', max_envs, 'based on core division')

    max_allowed_by_vram = max_batch_vram_gb // obs_size_gb
    if max_allowed_by_vram < batch_size:
        raise ValueError('max_allowed_by_vram < batch_size')

    print()
    configs = []

    # Strategy 1: one batch per core
    strategy_cores = min(num_cores, max_envs // batch_size)
    configs.append(dict(
        num_envs=batch_size*strategy_cores,
        num_workers=strategy_cores,
        batch_size=batch_size,
        backend=Multiprocessing,
    ))

    strategy_min_envs_per_worker = int(np.ceil((batch_size+1) / num_cores))
    strategy_num_envs = []
    for envs_per_worker in range(strategy_min_envs_per_worker, batch_size):
        num_envs = envs_per_worker * num_cores
        if num_envs > max_envs:
            break
        elif batch_size % envs_per_worker != 0:
            continue

        # Strategy 2: Full async. Only reasonable for low bandwidth
        #if throughput < 1.5:
        configs.append(dict(
            num_envs=num_envs,
            num_workers=num_cores,
            batch_size=batch_size,
            zero_copy=False,
            backend=Multiprocessing,
        ))

        # Strategy 3: Contiguous blocks. Only reasonable for high bandwidth
        num_batchs = num_envs / batch_size
        if num_batchs != int(num_batchs):
            continue
        if throughput > 0.5:
            configs.append(dict(
                num_envs=num_envs,
                num_workers=num_cores,
                batch_size=batch_size,
                backend=Multiprocessing,
            ))
        
    # Strategy 4: Full sync - perhaps nichely useful
    for strategy_cores in range(num_cores, 1, -1):
        if batch_size % strategy_cores != 0:
            continue

        configs.append(dict(
            num_envs=batch_size,
            num_workers=strategy_cores,
            batch_size=batch_size,
            backend=Multiprocessing,
        ))

    # Strategy 5: Serial
    configs.append(dict(
        num_envs=batch_size,
        backend=Serial,
    ))

    for config in configs:
        with pufferlib.Suppress():
            envs = make(env_creator, **config)
            envs.reset()
        actions = [envs.action_space.sample() for _ in range(1000)]
        step_time = 0
        steps = 0
        start = time.time()
        while time.time() - start < time_per_test:
            s = time.time()
            envs.send(actions[steps%1000])
            step_time += time.time() - s

            if model_forward_s > 0:
                time.sleep(model_forward_s)

            s = time.time()
            envs.recv()
            step_time += time.time() - s

            steps += 1

        end = time.time()
        envs.close()
        sps = steps * envs.agents_per_batch / step_time
        print(f'SPS: {sps:.3f}')
        for k, v in config.items():
            if k == 'backend':
                v = v.__name__

            print(f'    {k}: {v}')

        print()



================================================
FILE: pufferlib/config/atari.ini
================================================
[base]
package = atari

env_name = adventure air_raid alien amidar assault asterix asteroids atlantis2 atlantis backgammon bank_heist basic_math battle_zone beam_rider berzerk blackjack bowling boxing breakout carnival casino centipede chopper_command combat crazy_climber crossbow darkchambers defender demon_attack donkey_kong double_dunk earthworld elevator_action enduro entombed et fishing_derby flag_capture freeway frogger frostbite galaxian gopher gravitar hangman haunted_house hero human_cannonball ice_hockey jamesbond journey_escape joust kaboom kangaroo keystone_kapers king_kong klax koolaid krull kung_fu_master laser_gates lost_luggage mario_bros maze_craze miniature_golf montezuma_revenge mr_do ms_pacman name_this_game othello pacman phoenix pitfall2 pitfall pong pooyan private_eye qbert riverraid road_runner robotank seaquest sir_lancelot skiing solaris space_invaders space_war star_gunner superman surround tennis tetris tic_tac_toe_3d time_pilot trondead turmoil tutankham up_n_down venture video_checkers video_chess video_cube video_pinball warlords wizard_of_wor word_zapper yars_revenge zaxxon

policy_name = Policy
rnn_name = Recurrent

[vec]
num_envs = 128
num_workers = 16
batch_size = 64

[train]
batch_size = 8192
minibatch_size = 2048
update_epochs = 1
bptt_horizon = 64
total_timesteps = 10_000_000
anneal_lr = False

[env]
frameskip = 1
repeat_action_probability = 0.0

[sweep.parameters.env.parameters.frameskip]
distribution = uniform
min = 1
max = 10

#[sweep.parameters.env.parameters.repeat_action_probability]
#distribution = uniform
#min = 0
#max = 1
 
[sweep.parameters.train.parameters.total_timesteps]
distribution = uniform
min = 5_000_000
max = 200_000_000

[sweep.parameters.train.parameters.batch_size]
distribution = uniform
min = 16384
max = 65536

[sweep.parameters.train.parameters.minibatch_size]
distribution = uniform
min = 512
max = 8192



================================================
FILE: pufferlib/config/box2d.ini
================================================
[base]
package = box2d
env_name = car-racing

[train]
num_envs = 48
num_workers = 24
env_batch_size = 48
zero_copy = False
batch_size = 16384
minibatch_size = 2048



================================================
FILE: pufferlib/config/bsuite.ini
================================================
[base]
package = bsuite
env_name = bandit/0

[train]
total_timesteps = 1_000_000
num_envs = 1



================================================
FILE: pufferlib/config/butterfly.ini
================================================
[base]
package = butterfly
env_name = cooperative_pong_v5



================================================
FILE: pufferlib/config/classic_control.ini
================================================
[base]
package = classic_control
env_name = cartpole mountaincar

[train]
total_timesteps = 500_000
num_envs = 64
env_batch_size = 64



================================================
FILE: pufferlib/config/classic_control_continuous.ini
================================================
[base]
package = classic_control_continuous
env_name = mountaincar-continuous

[train]
total_timesteps = 500_000
num_envs = 64
env_batch_size = 64



================================================
FILE: pufferlib/config/craftax.ini
================================================
[base]
package = craftax
env_name = Craftax-Symbolic-v1 Craftax-Classic-Symbolic-v1
policy_name = Policy
rnn_name = Recurrent

[env]
num_envs = 1024

[vec]
num_envs = 1
num_workers = 1
batch_size = 1

[train]
total_timesteps = 100_000_000
checkpoint_interval = 200
update_epochs = 4
batch_size = 131072
minibatch_size = 8192
learning_rate = 0.0002
gamma = 0.99
gae_lambda = 0.8
ent_coef = 0.01



================================================
FILE: pufferlib/config/crafter.ini
================================================
[base]
package = crafter
env_name = crafter

[train]
num_envs = 96
num_workers = 24
env_batch_size = 48
zero_copy = False
batch_size = 6144



================================================
FILE: pufferlib/config/default.ini
================================================
[base]
package = None
env_name = None
policy_name = Policy
rnn_name = None
max_suggestion_cost = 3600

[vec]
backend = Multiprocessing
num_envs = 2
num_workers = auto
batch_size = auto
zero_copy = True
seed = 42

[env]
[policy]
[rnn]

[train]
name = pufferai 
project = ablations

seed = 42
torch_deterministic = True
cpu_offload = False
device = cuda
optimizer = muon
anneal_lr = True
precision = float32
total_timesteps = 10_000_000
learning_rate = 0.015
gamma = 0.995
gae_lambda = 0.90
update_epochs = 1
clip_coef = 0.2
vf_coef = 2.0
vf_clip_coef = 0.2
max_grad_norm = 1.5
ent_coef = 0.001
adam_beta1 = 0.95
adam_beta2 = 0.999
adam_eps = 1e-12

data_dir = experiments
checkpoint_interval = 200
batch_size = auto
minibatch_size = 8192

# Accumulate gradients above this size
max_minibatch_size = 32768
bptt_horizon = 64
compile = False
compile_mode = max-autotune-no-cudagraphs
compile_fullgraph = True

vtrace_rho_clip = 1.0
vtrace_c_clip = 1.0

prio_alpha = 0.8
prio_beta0 = 0.2

[sweep]
method = Protein 
metric = score
goal = maximize
downsample = 10

#[sweep.vec.num_envs]
#distribution = uniform_pow2
#min = 1
#max = 16
#mean = 8
#scale = auto

# TODO: Elim from base
[sweep.train.total_timesteps]
distribution = log_normal
min = 5e7
max = 1e10
mean = 1e8
scale = time

[sweep.train.bptt_horizon]
distribution = uniform_pow2
min = 16
max = 64
mean = 64
scale = auto

[sweep.train.minibatch_size]
distribution = uniform_pow2
min = 8192
max = 65536
mean = 32768
scale = auto

[sweep.train.learning_rate]
distribution = log_normal
min = 0.00001
mean = 0.01
max = 0.1
scale = 0.5

[sweep.train.ent_coef]
distribution = log_normal
min = 0.00001
mean = 0.01
max = 0.2
scale = auto

[sweep.train.gamma]
distribution = logit_normal
min = 0.8
mean = 0.98
max = 0.9999
scale = auto

[sweep.train.gae_lambda]
distribution = logit_normal
min = 0.6
mean = 0.95
max = 0.995
scale = auto

[sweep.train.vtrace_rho_clip]
distribution = uniform
min = 0.0
max = 5.0
mean = 1.0
scale = auto

[sweep.train.vtrace_c_clip]
distribution = uniform
min = 0.0
max = 5.0
mean = 1.0
scale = auto

#[sweep.train.update_epochs]
#distribution = int_uniform
#min = 1
#max = 8
#mean = 1
#scale = 2.0

[sweep.train.clip_coef]
distribution = uniform
min = 0.01
max = 1.0
mean = 0.2
scale = auto

# Optimal vf clip can be lower than 0.1,
# but this results in jank unstable runs
[sweep.train.vf_clip_coef]
distribution = uniform
min = 0.1
max = 5.0
mean = 0.2
scale = auto

[sweep.train.vf_coef]
distribution = uniform
min = 0.0
max = 5.0
mean = 2.0
scale = auto

[sweep.train.max_grad_norm]
distribution = uniform
min = 0.0
mean = 1.0
max = 5.0
scale = auto

[sweep.train.adam_beta1]
distribution = logit_normal
min = 0.5
mean = 0.9
max = 0.999
scale = auto

[sweep.train.adam_beta2]
distribution = logit_normal
min = 0.9
mean = 0.999
max = 0.99999
scale = auto

[sweep.train.adam_eps]
distribution = log_normal
min = 1e-14
mean = 1e-8
max = 1e-4
scale = auto

[sweep.train.prio_alpha]
distribution = logit_normal
min = 0.1
mean = 0.85
max = 0.99
scale = auto

[sweep.train.prio_beta0]
distribution = logit_normal
min = 0.1
mean = 0.85
max = 0.99
scale = auto



================================================
FILE: pufferlib/config/dm_control.ini
================================================
[base]
package = dm_control
env_name = dmc



================================================
FILE: pufferlib/config/dm_lab.ini
================================================
[base]
package = dm_lab
env_name = dml



================================================
FILE: pufferlib/config/doom.ini
================================================
[base]
package = vizdoom
env_name = doom

[train]
num_envs = 144
num_workers = 24
env_batch_size = 48
zero_copy = False
batch_size = 8192
minibatch_size = 2048
update_epochs = 1



================================================
FILE: pufferlib/config/gpudrive.ini
================================================
[base]
package = gpudrive
env_name = gpudrive
policy_name = Policy
rnn_name = Recurrent

[env]
num_worlds = 512

[train]
total_timesteps = 10_000_000
num_envs = 1
num_workers = 1
env_batch_size = 1
zero_copy = False
batch_size = 262_144
update_epochs = 5
minibatch_size = 32768
bptt_horizon = 4
anneal_lr = False
gae_lambda = 0.95
gamma = 0.99
clip_coef = 0.2
vf_coef = 0.5
vf_clip_coef = 0.2
max_grad_norm = 0.5
ent_coef = 0.00
learning_rate = 0.0003
checkpoint_interval = 1000
device = cuda

[sweep.metric]
goal = maximize
name = environment/goal_achieved

[sweep.parameters.train.parameters.batch_size]
distribution = uniform
min = 32768
max = 524288

[sweep.parameters.train.parameters.minibatch_size]
distribution = uniform
min = 2048
max = 32768





================================================
FILE: pufferlib/config/griddly.ini
================================================
[base]
package = griddly
env_name = spiders



================================================
FILE: pufferlib/config/gvgai.ini
================================================
[base]
package = gvgai
env_name = zelda
policy_name = Policy
rnn_name = Recurrent

[train]
total_timesteps = 1_000_000
checkpoint_interval = 1000
learning_rate = 0.00024290984560207393
num_envs = 96
num_workers = 24
env_batch_size = 32
update_epochs = 1
zero_copy = False
bptt_horizon = 16
batch_size = 4096
minibatch_size = 1024
compile = False
anneal_lr = False
device = cuda

[sweep.metric]
goal = maximize
name = environment/reward

[sweep.parameters.train.parameters.total_timesteps]
distribution = log_uniform_values
min = 500_000_000
max = 10_000_000_000

[sweep.parameters.train.parameters.batch_size]
values = [4096, 8192, 16384]

[sweep.parameters.train.parameters.minibatch_size]
values = [512, 1024, 2048, 4096]



================================================
FILE: pufferlib/config/kinetix.ini
================================================
[base]
package = kinetix
env_name = kinetix-pixels-discrete kinetix-symbolic-discrete
policy_name = PixelsPolicy
rnn_name = Recurrent

[env]
num_envs = 2048

[train]
total_timesteps = 100_000_000
checkpoint_interval = 200
num_envs = 1
num_workers = 1
env_batch_size = 1
update_epochs = 8
batch_size = 131072
minibatch_size = 4096
learning_rate = 0.0003
gamma = 0.995
gae_lambda = 0.9
ent_coef = 0.01

[sweep]
method = protein
name = sweep

[sweep.metric]
goal = maximize
name = GoalR 
min = 0
max = None

[sweep.train.total_timesteps]
distribution = log_normal
min = 5e6
max = 1e9
mean = 1e7
scale = auto

[sweep.train.batch_size]
distribution = uniform_pow2
min = 32768
max = 131072
mean = 65536
scale = auto

[sweep.train.minibatch_size]
distribution = uniform_pow2
min = 1024
max = 32768
mean = 8192
scale = auto



================================================
FILE: pufferlib/config/magent.ini
================================================
[base]
package = magent
env_name = battle_v4



================================================
FILE: pufferlib/config/mani_skill.ini
================================================

[base]
package = mani_skill
env_name = mani_pickcube mani_pushcube mani_stackcube mani_peginsertion
policy_name = Policy
rnn_name = Recurrent

[env]
num_envs = 4096
sim_steps_per_control = 5
control_freq = 100
solver_position_iterations = 15

[vec]
backend = PufferEnv
num_envs = 1

[train]
total_timesteps = 15_000_000
adam_beta1 = 0.9832254546070032
adam_beta2 = 0.9996089758513379
adam_eps = 0.0000024542110227211678
bptt_horizon = 64
clip_coef = 0.6609987983481933
ent_coef = 0.001194131610607018
gae_lambda = 0.968478898646462
gamma = 0.8880001899050386
learning_rate = 0.04729013902338006
max_grad_norm = 1.9301595176438802
minibatch_size = 32768
prio_alpha = 0.9531362058849446
prio_beta0 = 0.8285186322612919
vf_clip_coef = 0.2581908677409054
vf_coef = 2.6102252379894217
vtrace_c_clip = 2.008516783867587
vtrace_rho_clip = 0.7482202150166445

[sweep]
method = Protein 
metric = success_once
downsample = 0

[sweep.train.total_timesteps]
distribution = log_normal
min = 2e7
max = 5e7
mean = 4e7
scale = time

[sweep.env.sim_steps_per_control]
distribution = int_uniform
min = 1
max = 10
mean = 5
scale = auto

[sweep.env.control_freq]
distribution = int_uniform
min = 10
max = 100
mean = 20
scale = auto

[sweep.env.solver_position_iterations]
distribution = int_uniform
min = 4
max = 30
mean = 15
scale = auto



================================================
FILE: pufferlib/config/metta.ini
================================================
[base]
package = metta
env_name = metta 
policy_name = Policy
rnn_name = Recurrent

[vec]
num_envs = 64
num_workers = 16

[env]
render_mode = auto
ore_reward = 0.17088483842567775
battery_reward = 0.9882859711234822
heart_reward = 1.0

[train]
total_timesteps = 300_000_000
batch_size = auto
adam_beta1 = 0.8923106632311335
adam_beta2 = 0.9632470625784862
adam_eps = 1.3537431449843922e-7
clip_coef = 0.14919147162017737
ent_coef = 0.016700174334611493
gae_lambda = 0.8443676864928215
gamma = 0.997950174315581
learning_rate = 0.018470110879570414
max_grad_norm = 2.572849891206465
minibatch_size = 32768
bptt_horizon = 64
prio_alpha = 0.7918451491719373
prio_beta0 = 0.5852686803034238
vf_clip_coef = 0.1569624916309049
vf_coef = 3.2211333828684454
vtrace_c_clip = 2.134490283650365
vtrace_rho_clip = 2.296343917695581

[sweep]
metric = agent/heart.gained

[sweep.train.total_timesteps]
distribution = log_normal
min = 1e8
max = 5e8
mean = 3e8
scale = auto

[sweep.env.ore_reward]
distribution = uniform
min = 0.0
mean = 0.25
max = 1.0
scale = auto

[sweep.env.battery_reward]
distribution = uniform
min = 0.0
mean = 0.5
max = 1.0
scale = auto

[sweep.env.heart_reward]
distribution = uniform
min = 0.0
mean = 1.0
max = 1.0
scale = auto



================================================
FILE: pufferlib/config/microrts.ini
================================================
[base]
package = microrts
env_name = GlobalAgentCombinedRewardEnv



================================================
FILE: pufferlib/config/minerl.ini
================================================
[base]
package = minerl
env_name = MineRLNavigateDense-v0



================================================
FILE: pufferlib/config/minigrid.ini
================================================
[base]
package = minigrid
env_name = minigrid

[vec]
num_envs = 48
num_workers = 6
batch_size = 48

[train]
total_timesteps = 1_000_000
batch_size = 6144
minibatch_size = 768
update_epochs = 4
anneal_lr = False
gae_lambda = 0.95
gamma = 0.95
ent_coef = 0.025
learning_rate = 2.5e-4



================================================
FILE: pufferlib/config/minihack.ini
================================================
[base]
package = minihack
env_name = minihack

[vec]
num_envs = 128
num_workers = 16
batch_size = 64

[train]
batch_size = 8192
minibatch_size = 2048
update_epochs = 1
bptt_horizon = 64
total_timesteps = 10_000_000



================================================
FILE: pufferlib/config/mujoco.ini
================================================
[base]
package = mujoco
env_name = HalfCheetah-v4 Hopper-v4 Swimmer-v4 Walker2d-v4 Ant-v4 Humanoid-v4 Reacher-v4 InvertedPendulum-v4 InvertedDoublePendulum-v4 Pusher-v4 HumanoidStandup-v4
policy_name = Policy
rnn_name = Recurrent

[env]
render_mode = rgb_array

[vec]
num_envs = 512
num_workers = 16
batch_size = auto

[train]
total_timesteps = 5_000_000
learning_rate = 3e-4
gamma = 0.99
gae_lambda = 0.95
update_epochs = 10
clip_coef = 0.2
vf_coef = 0.5
vf_clip_coef = 0.2
max_grad_norm = 0.5
ent_coef = 0.0
checkpoint_interval = 200
batch_size = 32768
minibatch_size = 4096
bptt_horizon = 64



================================================
FILE: pufferlib/config/nethack.ini
================================================
[base]
package = nethack
env_name = nethack
policy_name = Policy
rnn_name = Recurrent

[vec]
num_envs = 8192
num_workers = 16
batch_size = 4096

[train]
total_timesteps = 90_000_000
adam_beta1 = 0.8946507418260217
adam_beta2 = 0.9
adam_eps = 0.0001
batch_size = auto
bptt_horizon = 64
clip_coef = 0.19696765958267629
ent_coef = 0.0005690816545012474
gae_lambda = 0.747650023961198
gamma = 0.9997053654668936
learning_rate = 0.044482546441415506
max_grad_norm = 2.2356112188495723
minibatch_size = 32768
prio_alpha = 0.98967001208896
prio_beta0 = 0.09999999999999998
vf_clip_coef = 2.178492167689251
vf_coef = 1.6832989594296321
vtrace_c_clip = 2.878171091654008
vtrace_rho_clip = 0.7876748061547312



================================================
FILE: pufferlib/config/nmmo.ini
================================================
[base]
package = nmmo
env_name = nmmo

[train]
num_envs = 4
env_batch_size = 4
num_workers = 4
batch_size = 4096
minibatch_size = 2048



================================================
FILE: pufferlib/config/open_spiel.ini
================================================
[base]
package = open_spiel
env_name = connect_four

[train]
num_envs = 32
batch_size = 4096



================================================
FILE: pufferlib/config/pokemon_red.ini
================================================
[base]
package = pokemon_red
env_name = pokemon_red

[train]
total_timesteps = 1_000_000
num_envs = 96
num_workers = 24
env_batch_size = 32
zero_copy = False
update_epochs = 3
gamma = 0.998
batch_size = 65536
minibatch_size = 2048
compile = True
learning_rate = 2.0e-4
anneal_lr = False



================================================
FILE: pufferlib/config/procgen.ini
================================================
[base]
package = procgen
env_name = bigfish bossfight caveflyer chaser climber coinrun dodgeball fruitbot heist jumper leaper maze miner ninja plunder starpilot

[vec]
num_envs = 128
num_workers = 16
batch_size = 64

[train]
batch_size = 8192
minibatch_size = 2048
update_epochs = 1
bptt_horizon = 64
total_timesteps = 25_000_000



================================================
FILE: pufferlib/config/slimevolley.ini
================================================
[base]
package = slimevolley
env_name = slimevolley

[train]
num_envs=1536
num_workers=24
env_batch_size=512
zero_copy=False
batch_size=65536
minibatch_size=8192
update_epochs=1



================================================
FILE: pufferlib/config/stable_retro.ini
================================================
[base]
package = stable_retro
env_name = Airstriker-Genesis



================================================
FILE: pufferlib/config/starcraft.ini
================================================
[base]
package = smac



================================================
FILE: pufferlib/config/trade_sim.ini
================================================
[base]
package = trade_sim
env_name = trade_sim
policy_name = Policy
rnn_name = Recurrent

[vec]
backend = Multiprocessing
num_envs = 1024
num_workers = 16
batch_size = 512

#[env]
#num_envs = 128 

[train]
total_timesteps = 100_000_000
gamma = 0.95
learning_rate = 0.05
minibatch_size = 32768

[sweep]
metric = final_capital

[sweep.train.total_timesteps]
distribution = log_normal
min = 2e7
max = 1e8
mean = 5e7
scale = auto



================================================
FILE: pufferlib/config/tribal_village.ini
================================================
[base]
package = tribal_village
env_name = tribal_village tribal-village
policy_name = Policy
rnn_name = Recurrent

[vec]
num_envs = 192 
num_workers = 12

[env]
render_mode = rgb_array
max_steps = 512
ore_per_battery = 3
batteries_per_heart = 2
enable_combat = True
clippy_spawn_rate = 0.1
clippy_damage = 10
heart_reward = 1.0
battery_reward = 0.5
ore_reward = 0.1
survival_penalty = -0.01
death_penalty = -1.0

[train]
total_timesteps = 50_000_000
batch_size = auto
bptt_horizon = 32
learning_rate = 0.001
ent_coef = 0.01
vf_coef = 0.5
max_grad_norm = 0.5
gamma = 0.99
gae_lambda = 0.95
clip_coef = 0.2
minibatch_size = 32768

[sweep]
metric = agent/heart.gained

[sweep.env.heart_reward]
distribution = uniform
min = 0.5
mean = 1.0
max = 2.0
scale = auto

[sweep.env.battery_reward]
distribution = uniform
min = 0.1
mean = 0.5
max = 1.0
scale = auto

[sweep.env.ore_reward]
distribution = uniform
min = 0.01
mean = 0.1
max = 0.5
scale = auto



================================================
FILE: pufferlib/config/ocean/asteroids.ini
================================================
[base]
package = ocean
env_name = puffer_asteroids
policy_name = Policy
rnn_name = Recurrent

[vec]
num_envs = 8

[env]
num_envs = 1024
size = 500

[train]
adam_beta1 = 0.975493290069733
adam_beta2 = 0.9999436458974764
adam_eps = 6.915036275112011e-08
anneal_lr = true
batch_size = auto
bptt_horizon = 64
checkpoint_interval = 200
clip_coef = 0.18588778503512546
ent_coef = 0.0016620361911332262
gae_lambda = 0.8400278040617952
gamma = 0.9998708818940873
learning_rate = 0.00502237062536979
max_grad_norm = 0.7306435358436453
max_minibatch_size = 32768
minibatch_size = 8192
prio_alpha = 0.9165093859993415
prio_beta0 = 0.8869674411376214
total_timesteps = 100_000_000
update_epochs = 1
vf_clip_coef = 0.1
vf_coef = 2.960148388519086
vtrace_c_clip = 1.0767718761515104
vtrace_rho_clip = 4.132507367126342



================================================
FILE: pufferlib/config/ocean/battle.ini
================================================
[base]
package = ocean
env_name = puffer_battle
policy_name = Policy
rnn_name = Recurrent

[policy]
hidden_size = 512

[rnn]
input_size = 512
hidden_size = 512

[vec]
num_envs = 16

[env]
num_envs = 4
num_agents = 128
num_armies = 2
size_x = 2
size_y = 1.0
size_z = 2

[train]
total_timesteps = 50_000_000

#adam_beta1 = 0.9672322418397323
#adam_beta2 = 0.9877607751795193
#adam_eps = 3.1721115738865995e-12
#clip_coef = 0.43568934504743784
#ent_coef = 0.0009836417478975427
#gae_lambda = 0.9668222538234107
#gamma = 0.990709789440733
#learning_rate = 0.006246420318636455
#max_grad_norm = 1.7919049246329588
#minibatch_size = 65536
#prio_alpha = 0.09999999999999998
#prio_beta0 = 0.7406397128300295
#vf_clip_coef = 1.6190073090306314
#vf_coef = 3.4918587292978454
#vtrace_c_clip = 0.5344573247342275
#vtrace_rho_clip = 1.2893540729776307

#learning_rate = 0.0015534438005054883
#gamma = 0.9923382806478448
#minibatch_size = 32768

#adam_beta1 = 0.5797997352318079
#adam_beta2 = 0.9001752474216785
#adam_eps = 6.121819610318236e-8
#clip_coef = 0.3616904471336408
#ent_coef = 0.020160134702951138
#gae_lambda = 0.5999999999999999
#gamma = 0.9923382806478448
#learning_rate = 0.0015534438005054883
#max_grad_norm = 0.5104995231814086
#minibatch_size = 65536
#prio_alpha = 0.5594506255128311
#prio_beta0 = 0.9852161239512259 
#vf_clip_coef = 0.1
#vf_coef = 1.8024088377114245
#vtrace_c_clip = 1.7578256946375268
#vtrace_rho_clip = 1.0041987439042879





================================================
FILE: pufferlib/config/ocean/blastar.ini
================================================
[base]
package = ocean
env_name = puffer_blastar 
policy_name = Policy
rnn_name = Recurrent

[env]
num_envs = 4096

[train]
total_timesteps = 200_000_000
gamma = 0.95
learning_rate = 0.05
minibatch_size = 32768

[sweep]
metric = environment/enemy_crossed_screen
goal = minimize

[sweep.parameters.train.parameters.total_timesteps]
distribution = uniform
min = 10_000_000
max = 100_000_000



================================================
FILE: pufferlib/config/ocean/boids.ini
================================================
[base]
package = ocean
env_name = puffer_boids 
policy_name = Boids
rnn_name = Recurrent
; rnn_name = None

[env]
num_envs = 64
num_boids = 64
; num_envs = 1
; num_boids = 1
margin_turn_factor = 0.0
centering_factor = 0.00
avoid_factor = 1.00
matching_factor = 1.00

[vec]
num_workers = 2
num_envs = 2
batch_size = auto

[train]
total_timesteps = 100_000_000
gamma = 0.95
learning_rate = 0.025
minibatch_size = 16384
; minibatch_size = 1

; [sweep]
; method = protein
; metric = episode_length

; [sweep.train.total_timesteps]
; distribution = log_normal
; min = 1e6
; max = 1e7
; mean = 5e6
; scale = 0.5

; [sweep.train.gamma]
; distribution = log_normal
; min = 0.9
; max = 0.999
; mean = 0.97

; [sweep.train.gae_lambda]
; distribution = log_normal
; min = 0.7
; max = 0.999
; mean = 0.95

; [sweep.train.learning_rate]
; distribution = log_normal
; min = 0.0001
; max = 0.001
; mean = 0.00025
; scale = 0.5

; [sweep.train.batch_size]
; min = 32768
; max = 131072
; mean = 65536
; scale = 0.5

; [sweep.train.minibatch_size]
; min = 512
; max = 2048
; mean = 1024
; scale = 0.5





================================================
FILE: pufferlib/config/ocean/breakout.ini
================================================
[base]
package = ocean
env_name = puffer_breakout
policy_name = Policy
rnn_name = Recurrent

[vec]
num_envs = 8

[env]
num_envs = 1024
frameskip = 4
width = 576
height = 330
paddle_width = 62
paddle_height = 8
ball_width = 32
ball_height = 32
brick_width = 32
brick_height = 12
brick_rows = 6
brick_cols = 18
initial_ball_speed = 256
max_ball_speed = 448
paddle_speed = 620
continuous = 0
 
[policy]
hidden_size = 128

[rnn]
input_size = 128
hidden_size = 128

[train]
total_timesteps = 90_000_000
adam_beta1 = 0.8946507418260217
adam_beta2 = 0.9
adam_eps = 0.0001
batch_size = auto
bptt_horizon = 64
clip_coef = 0.19696765958267629
ent_coef = 0.0005690816545012474
gae_lambda = 0.747650023961198
gamma = 0.9997053654668936
learning_rate = 0.044482546441415506
max_grad_norm = 2.2356112188495723
minibatch_size = 32768
prio_alpha = 0.98967001208896
prio_beta0 = 0.09999999999999998
vf_clip_coef = 2.178492167689251
vf_coef = 1.6832989594296321
vtrace_c_clip = 2.878171091654008
vtrace_rho_clip = 0.7876748061547312

[sweep.train.total_timesteps]
distribution = log_normal
min = 3e7
max = 2e8
mean = 8e7
scale = auto

[sweep.env.frameskip]
distribution = int_uniform
min = 1
max = 8
mean = 4
scale = 2.0



================================================
FILE: pufferlib/config/ocean/cartpole.ini
================================================
[base]
package = ocean
env_name = puffer_cartpole
policy_name = Policy
rnn_name = Recurrent

[env]
num_envs = 4096
cart_mass = 1.0
pole_mass = 0.1
pole_length = 0.5
gravity = 9.8
force_mag = 10.0
dt = 0.02

[train]
total_timesteps = 20_000_000
gamma = 0.95
learning_rate = 0.05
minibatch_size = 32768

[sweep]
method = protein
metric = episode_length

[sweep.train.total_timesteps]
distribution = log_normal
min = 1e6
max = 1e7
mean = 5e6
scale = 0.5

[sweep.train.gamma]
distribution = log_normal
min = 0.9
max = 0.999
mean = 0.97

[sweep.train.gae_lambda]
distribution = log_normal
min = 0.7
max = 0.999
mean = 0.95

[sweep.train.learning_rate]
distribution = log_normal
min = 0.0001
max = 0.001
mean = 0.00025
scale = 0.5

[sweep.train.batch_size]
min = 32768
max = 131072
mean = 65536
scale = 0.5

[sweep.train.minibatch_size]
min = 512
max = 2048
mean = 1024
scale = 0.5



================================================
FILE: pufferlib/config/ocean/chain_mdp.ini
================================================
[base]
package = ocean
env_name = puffer_chain_mdp
policy_name = Policy
; rnn_name = Recurrent

[vec]
num_envs = 8

[env]
num_envs = 512
size = 128

[policy]
hidden_size = 128

; [rnn]
; input_size = 128
; hidden_size = 128

[train]
total_timesteps = 5_000_000
bptt_horizon = 64
entropy_coef = 0.1


================================================
FILE: pufferlib/config/ocean/checkers.ini
================================================
[base]
package = ocean
env_name = puffer_checkers
policy_name = Policy
rnn_name = Recurrent

[env]
num_envs = 4096
size = 8

[vec]
num_envs = 8

[train]
total_timesteps = 1_000_000_000
minibatch_size = 65536
gamma = 0.95



================================================
FILE: pufferlib/config/ocean/connect4.ini
================================================
[base]
package = ocean
env_name = puffer_connect4
policy_name = Policy
rnn_name = Recurrent

[env]
num_envs = 1024

[vec]
num_envs = 8

[train]
total_timesteps = 22_000_000
adam_beta1 = 0.7332525176640032
adam_beta2 = 0.9992588002434659
adam_eps = 0.0001
clip_coef = 0.3344358533613167
ent_coef = 0.00004214003802569246
gae_lambda = 0.8969790930039623
gamma = 0.9945932652529774
learning_rate = 0.1
max_grad_norm = 1.0219144411399215
minibatch_size = 32768
prio_alpha = 0.9057091953725436
prio_beta0 = 0.6320607520016285
vf_clip_coef = 1.9948775471721416
vf_coef = 2.3734839181925462
vtrace_c_clip = 0.5659747235622431
vtrace_rho_clip = 1.4499061438546799

[sweep.train.total_timesteps]
distribution = log_normal
min = 1e7
max = 2e8
mean = 3e7
scale = 0.5



================================================
FILE: pufferlib/config/ocean/continuous.ini
================================================
[base]
package = ocean
env_name = puffer_continuous

[train]
total_timesteps = 1_000_000
anneal_lr = False
num_envs = 64
batch_size = 16384
minibatch_size = 4096
update_epochs = 1
gamma = 0.8
ent_coef = 0.05



================================================
FILE: pufferlib/config/ocean/convert.ini
================================================
[base]
package = ocean
env_name = puffer_convert
policy_name = Policy
rnn_name = Recurrent

[vec]
num_envs = 16

[env]
num_envs = 1
num_agents = 1024
num_factories = 32
num_resources = 8

[train]
total_timesteps = 100_000_000
gamma = 0.99
learning_rate = 0.015
minibatch_size = 32768
ent_coef = 0.02




================================================
FILE: pufferlib/config/ocean/convert_circle.ini
================================================
[base]
package = ocean
env_name = puffer_convert_circle
policy_name = Policy
rnn_name = Recurrent

[vec]
num_envs = 16

[env]
num_envs = 1
num_agents = 1024
num_factories = 32
num_resources = 8
equidistant = 1
radius = 400

[train]
total_timesteps = 100_000_000
gamma = 0.99
learning_rate = 0.015
minibatch_size = 32768
ent_coef = 0.02




================================================
FILE: pufferlib/config/ocean/drive.ini
================================================
[base]
package = ocean
env_name = puffer_drive
policy_name = Drive
rnn_name = Recurrent

[vec]
num_workers = 8
num_envs = 8
batch_size = 2
#backend = Serial

[policy]
input_size = 64
hidden_size = 256

[rnn]
input_size = 256
hidden_size = 256

[env]
num_agents = 1024
reward_vehicle_collision = -0.5
reward_offroad_collision = -0.2
spawn_immunity_timer = 50   
reward_goal_post_respawn = 0.25
reward_vehicle_collision_post_respawn = -0.5
resample_frequency = 910
num_maps = 80000

[train]
total_timesteps = 2_000_000_000
#learning_rate = 0.02
#gamma = 0.985
anneal_lr = True
batch_size = 745472
minibatch_size = 11648
max_minibatch_size = 11648
bptt_horizon = 91
adam_beta1 = 0.9
adam_beta2 = 0.999
adam_eps = 1e-8
clip_coef = 0.2
ent_coef = 0.001
gae_lambda = 0.95
gamma = 0.98
learning_rate = 0.001
max_grad_norm = 1
prio_alpha = 0.8499999999999999
prio_beta0 = 0.8499999999999999
update_epochs = 1
vf_clip_coef = 0.1999999999999999
vf_coef = 2
vtrace_c_clip = 1
vtrace_rho_clip = 1
checkpoint_interval = 1000



[sweep.train.total_timesteps]
distribution = log_normal
min = 1e8
max = 4e8
mean = 2e8
scale = time
 
[sweep.env.reward_vehicle_collision]
distribution = uniform
min = -1.0
max = 0.0
mean = -0.2
scale = auto 
 
[sweep.env.reward_offroad_collision]
distribution = uniform
min = -1.0
max = 0.0
mean = -0.2
scale = auto

[sweep.env.spawn_immunity_timer]
distribution = uniform
min = 1
max = 91
mean = 30
scale = auto

[sweep.env.reward_goal_post_respawn]
distribution = uniform
min = 0.0
max = 1.0
mean = 0.5
scale = auto

[sweep.env.reward_vehicle_collision_post_respawn]
distribution = uniform
min = -1.0
max = 0.0
mean = -0.2
scale = auto



================================================
FILE: pufferlib/config/ocean/drone_race.ini
================================================
[base]
package = ocean
env_name = puffer_drone_race
policy_name = Policy
rnn_name = Recurrent

[vec]
num_envs = 8

[env]
num_envs = 1024

[train]
adam_beta1 = 0.9610890980775877
adam_beta2 = 0.9999260775286266
adam_eps = 7.782906079040132e-10
anneal_lr = true
batch_size = auto
bptt_horizon = 64
checkpoint_interval = 200
clip_coef = 0.05982655642208556
ent_coef = 0.002465076521024325
gae_lambda = 0.9641173414828333
gamma = 0.997472126425902
learning_rate = 0.010933756713881205
max_grad_norm = 1.6317688647793107
max_minibatch_size = 32768
minibatch_size = 32768
prio_alpha = 0.8968873016577552
prio_beta0 = 0.8672928227817938
total_timesteps = 100_000_000
update_epochs = 1
#use_rnn = false
vf_clip_coef = 0.5869845581530236
vf_coef = 2.1319065538539963
vtrace_c_clip = 2.714930379733876
vtrace_rho_clip = 3.8183814893708057



================================================
FILE: pufferlib/config/ocean/drone_swarm.ini
================================================
[base]
package = ocean
env_name = puffer_drone_swarm
policy_name = Policy
rnn_name = Recurrent

[policy]
hidden_size = 128

[rnn]
input_size = 128
hidden_size = 128

[vec]
num_envs = 8

[env]
num_envs = 16
num_drones = 64
max_rings = 10

[train]
adam_beta1 = 0.9610890980775877
adam_beta2 = 0.9999260775286266
adam_eps = 7.782906079040132e-10
anneal_lr = true
batch_size = auto
bptt_horizon = 64
checkpoint_interval = 200
clip_coef = 0.05982655642208556
ent_coef = 0.002465076521024325
gae_lambda = 0.9641173414828333
gamma = 0.997472126425902
learning_rate = 0.010933756713881205
#learning_rate = 0.005
max_grad_norm = 1.6317688647793107
max_minibatch_size = 32768
minibatch_size = 32768
prio_alpha = 0.8968873016577552
prio_beta0 = 0.8672928227817938
total_timesteps = 500_000_000
update_epochs = 1
#use_rnn = false
vf_clip_coef = 0.5869845581530236
vf_coef = 2.1319065538539963
vtrace_c_clip = 2.714930379733876
vtrace_rho_clip = 3.8183814893708057

[sweep]
downsample = 0

[sweep.train.total_timesteps]
distribution = log_normal
min = 2e8
max = 4e8
mean = 2e8
scale = time



================================================
FILE: pufferlib/config/ocean/enduro.ini
================================================
[base]
package = ocean
env_name = puffer_enduro 
policy_name = Policy
rnn_name = Recurrent

[env]
num_envs = 1024

[vec]
num_envs = 1

[train]
total_timesteps = 400_000_000
adam_beta1 = 0.9602226117399812
adam_beta2 = 0.999983918771099
adam_eps = 2.109767652202695e-9
bptt_horizon = 64
clip_coef = 0.5716251062832933
ent_coef = 0.009778379693175061
gae_lambda = 0.9924829173144767
gamma = 0.9433427558493771
learning_rate = 0.014263349414255656
max_grad_norm = 0.42249653686869115
max_minibatch_size = 32768
minibatch_size = 65536
prio_alpha = 0.22253503344197678
prio_beta0 = 0.7866639848626998
vf_clip_coef = 0.01
vf_coef = 3.2952964839081016
vtrace_c_clip = 3.060525785199293
vtrace_rho_clip = 5

[sweep]
metric = days_completed

[sweep.train.total_timesteps]
distribution = log_normal
min = 5e7
max = 4e8
mean = 2e8
scale = auto



================================================
FILE: pufferlib/config/ocean/freeway.ini
================================================
[base]
package = ocean
env_name = puffer_freeway
policy_name = Policy
rnn_name = Recurrent

[vec]
num_envs = 8

[env]
num_envs = 1024
frameskip = 4
use_dense_rewards = True
env_randomization = True
difficulty = 0
level = -1

[train]
total_timesteps = 500_000_000
minibatch_size = 32768

[sweep.train.total_timesteps]
distribution = log_normal
min = 3e8
max = 4e8
mean = 3e8
scale = auto



================================================
FILE: pufferlib/config/ocean/g2048.ini
================================================
[base]
package = ocean
env_name = puffer_g2048
policy_name = Policy
rnn_name = Recurrent

[policy]
hidden_size = 128

[rnn]
input_size = 128
hidden_size = 128

[vec]
num_envs = 4

[env]
num_envs = 4096

[train]
total_timesteps = 5_000_000_000
adam_beta1 = 0.9529488439604378
adam_beta2 = 0.9993901829477296
adam_eps = 2.745365927413118e-7
bptt_horizon = 64
clip_coef = 0.596573170393339
ent_coef = 0.02107417730003862
gae_lambda = 0.9940613415815854
gamma = 0.9889857974154952
#learning_rate = 0.0032402460796988127
learning_rate = 0.001
max_grad_norm = 1.0752406726589745
minibatch_size = 16384
prio_alpha = 0.25297099593586336
prio_beta0 = 0.940606268942572
vf_clip_coef = 0.1
vf_coef = 1.6362878279900643
vtrace_c_clip = 0
vtrace_rho_clip = 1.2917509971869054
anneal_lr = False



================================================
FILE: pufferlib/config/ocean/go.ini
================================================
[base]
package = ocean
env_name = puffer_go
policy_name = Policy
rnn_name = Recurrent

[env]
num_envs = 1024
reward_move_pass = -0.6026362603175613
reward_move_valid = 0
reward_move_invalid = -0.5393516480382454
reward_opponent_capture = -0.3152783593705354
reward_player_capture = 0.42122681325442923
grid_size = 7

[vec]
num_envs = 8

[train]
total_timesteps = 100_000_000
adam_beta1 = 0.5686370767889766
adam_beta2 = 0.9999454817221638
adam_eps = 2.007252656207671e-12
bptt_horizon = 64
clip_coef = 0.17930104885238807
ent_coef = 0.0018946598458748304
gae_lambda = 0.9831319174802507
gamma = 0.9480351741863737
learning_rate = 0.031603809039284864
max_grad_norm = 1.320177349287771
minibatch_size = 8192
prio_alpha = 0.6979639079178326
prio_beta0 = 0.5614257332458639
vf_clip_coef = 1.1755607092687304
vf_coef = 1.6195967557187005
vtrace_c_clip = 0
vtrace_rho_clip = 4.060318960532289

[sweep.train.total_timesteps]
distribution = log_normal
min = 1e8
max = 5e8
mean = 2e8
scale = 0.25

[sweep.env.reward_move_invalid]
distribution = uniform
min = -1.0
max = 0.0
mean = -0.5
scale = 0.5

[sweep.env.reward_move_pass]
distribution = uniform
min = -1.0
max = 0.0
mean = -0.5
scale = 0.5

[sweep.env.reward_player_capture]
distribution = uniform
min = 0.0
max = 1.0
mean = 0.5
scale = 0.5

[sweep.env.reward_opponent_capture]
distribution = uniform
min = -1.0
max = 0.0
mean = -0.5
scale = 0.5



================================================
FILE: pufferlib/config/ocean/grid.ini
================================================
[base]
package = ocean
env_name = puffer_grid
policy_name = Policy
rnn_name = Recurrent

[policy]
hidden_size = 512

[rnn]
input_size = 512
hidden_size = 512

[vec]
#num_envs = 8
num_envs = 1

[env]
max_size = 47
num_envs = 1024
#num_envs = 4096
num_maps = 8192

[train]
# Best params
#total_timesteps = 435_000_000
#adam_beta1 = 0.9801350114303844
#adam_beta2 = 0.9931056135397744
#adam_eps = 6.024885743259763e-8
#clip_coef = 0.283658795325587
#ent_coef = 0.007885530106105381
#gae_lambda = 0.9574676436577135
#gamma = 0.9961782334639131
#learning_rate = 0.0007890771333884192
#max_grad_norm = 2.5271346931510053
#minibatch_size = 8192
#prio_alpha = 0.8735470630752789
#prio_beta0 = 0.6533958384978629
#vf_clip_coef = 1.9338563232919095
#vf_coef = 3.915248046963283
#vtrace_c_clip = 1.018588814067991
#vtrace_rho_clip = 2.4215244529216466

# New sweep best params
total_timesteps = 435_000_000
adam_beta1 = 0.9493079570168755
adam_beta2 = 0.9998213228757207
adam_eps = 2.16720639574209e-8
bptt_horizon = 64
clip_coef = 0.399530686596841
ent_coef = 0.0017271288609381147
gae_lambda = 0.9491722822649111
gamma = 0.9877360824574745
learning_rate = 0.0012892859713461897
max_grad_norm = 3.016348031602564
minibatch_size = 8192
prio_alpha = 0.8219794821639037
prio_beta0 = 0.9447478232810274
vf_clip_coef = 0.6051579400844748
vf_coef = 2.323141961227481
vtrace_c_clip = 1.2499497264614237
vtrace_rho_clip = 4.7398234531013985

[sweep]
downsample = 0

[sweep.train.total_timesteps]
distribution = log_normal
min = 3e8
max = 6e8
mean = 3e8
scale = time



================================================
FILE: pufferlib/config/ocean/impulse_wars.ini
================================================
[base]
package = ocean
env_name = puffer_impulse_wars
policy_name = ImpulseWarsPolicy
rnn_name = ImpulseWarsLSTM
max_suggestion_cost = 10_800

[policy]
cnn_channels = 64
input_size = 512
hidden_size = 512

# These must match what's set in env below
continuous = False
num_drones = 2
is_training = True

[vec]
num_envs = 16
num_workers = 16
batch_size = 4

[env]
num_envs = 256
num_drones = 2
num_agents = 1
enable_teams = False
sitting_duck = False
continuous = False
is_training = True

[train]
total_timesteps = 100_000_000
checkpoint_interval = 250

learning_rate = 0.005

compile = False
compile_mode = reduce-overhead
compile_fullgraph = False
device = cuda

[sweep.env.num_envs]
distribution = uniform_pow2
min = 16
max = 512
mean = 128
scale = auto

[sweep.train.total_timesteps]
distribution = log_normal
min = 250_000_000
max = 1_500_000_000
mean = 500_000_000
scale = time

[sweep.train.batch_size]
distribution = uniform_pow2
min = 65_536
max = 1_048_576
mean = 262_144
scale = auto

[sweep.train.bptt_horizon]
distribution = uniform_pow2
min = 64
max = 256
mean = 128
scale = auto

[sweep.train.minibatch_size]
distribution = uniform_pow2
min = 1024
max = 262_144
mean = 16_384
scale = auto

[sweep.train.learning_rate]
distribution = log_normal
min = 0.00001
mean = 0.001
max = 0.1
scale = 0.5

[sweep.train.ent_coef]
distribution = log_normal
min = 0.000001
mean = 0.001
max = 0.2
scale = auto

[sweep.train.gamma]
distribution = logit_normal
min = 0.8
mean = 0.98
max = 0.99999
scale = auto

[sweep.train.gae_lambda]
distribution = logit_normal
min = 0.6
mean = 0.93
max = 0.995
scale = auto

[sweep.train.vf_coef]
distribution = uniform
min = 0.0
max = 5.0
mean = 1.0
scale = auto

[sweep.train.max_grad_norm]
distribution = uniform
min = 0.0
mean = 1.0
max = 5.0
scale = auto



================================================
FILE: pufferlib/config/ocean/matsci.ini
================================================
[base]
package = ocean
env_name = puffer_matsci
policy_name = Policy

[vec]
num_envs = 8

[env]
num_envs = 8
num_atoms = 128

[train]
total_timesteps = 50_000_000
minibatch_size = 32768





================================================
FILE: pufferlib/config/ocean/memory.ini
================================================
[base]
package = ocean
env_name = puffer_memory
policy_name = Policy
rnn_name = Recurrent

[env]
num_envs = 1024

[vec]
num_envs = 8

[train]
total_timesteps = 50_000_000
minibatch_size = 32768



================================================
FILE: pufferlib/config/ocean/moba.ini
================================================
[base]
package = ocean
env_name = puffer_moba
policy_name = MOBA
rnn_name = Recurrent

[env]
reward_death = 0.0
reward_xp = 0.0016926873475313188
reward_distance = 0.0
reward_tower = 4.525112152099609
num_envs = 128

[vec]
num_envs = 8

[train]
total_timesteps = 150_000_000

[sweep.train.total_timesteps]
distribution = log_normal
min = 2e7
max = 2e8
mean = 1e8
scale = auto

[sweep.env.reward_death]
distribution = uniform
min = -1.0
max = 0
mean = 0
scale = auto

[sweep.env.reward_xp]
distribution = uniform
min = 0.0
max = 0.05
mean = 0.0015
scale = auto

[sweep.env.reward_tower]
distribution = uniform
min = 0.0
max = 1.0
mean = 1.0
scale = auto



================================================
FILE: pufferlib/config/ocean/nmmo3.ini
================================================
[base]
package = ocean
env_name = puffer_nmmo3
policy_name = NMMO3
rnn_name = NMMO3LSTM

[vec]
num_envs = 8

[env]
reward_combat_level = 1.0
reward_prof_level = 1.0
reward_item_level = 1.0
reward_market = 0.0
reward_death = -1.0
num_envs = 1

[train]
total_timesteps = 107000000000
checkpoint_interval = 1000
learning_rate = 0.0004573146765703167
gamma = 0.7647543366891623
gae_lambda = 0.996005622445478
ent_coef = 0.01210084358004069
max_grad_norm = 0.6075578331947327
vf_coef = 0.3979089612467003
bptt_horizon = 64
batch_size = 524288
minibatch_size = 32768
max_minibatch_size = 32768

[sweep]
metric = min_comb_prof

[sweep.env.num_envs]
distribution = uniform_pow2
min = 1
max = 8
mean = 4
scale = 0.5

[sweep.train.total_timesteps]
distribution = log_normal
min = 2e8
max = 1e9
mean = 5e8
scale = 0.5

[sweep.env.reward_combat_level]
distribution = uniform
min = 0.0
max = 1.0
mean = 0.5
scale = auto

[sweep.env.reward_prof_level]
distribution = uniform
min = 0.0
max = 1.0
mean = 0.5
scale = auto

[sweep.env.reward_item_level]
distribution = uniform
min = 0.0
max = 1.0
mean = 1.0
scale = auto

[sweep.env.reward_death]
distribution = uniform
min = -1.0
max = 0.0
mean = -1.0
scale = auto



================================================
FILE: pufferlib/config/ocean/oldgrid.ini
================================================
[base]
package = ocean
env_name = puffer_oldgrid
vec = multiprocessing
policy_name = Policy
rnn_name = Recurrent

#[policy]
#hidden_size = 512

#[rnn]
#input_size = 512
#hidden_size = 512

[env]
#map_size = 31
max_map_size = 31
num_envs = 512
num_maps = 8192
#num_maps = 1

[train]
total_timesteps = 180_000_000
checkpoint_interval = 1000
learning_rate = 0.0005978750098629419
gamma = 0.9944336976183826
gae_lambda = 0.9474288929489364
ent_coef = 0.00001
use_e3b = True

num_envs = 1
num_workers = 1
env_batch_size = 1
update_epochs = 4
bptt_horizon = 16
batch_size = 131072
minibatch_size = 16384
compile = False
device = cuda
e3b_coef = 0.01

[sweep]
method = protein
name = sweep

[sweep.metric]
goal = maximize
name = score 
min = 0
max = 1

[sweep.train.total_timesteps]
distribution = log_normal
min = 5e7
max = 2e8
mean = 1e8
scale = auto

[sweep.train.e3b_coef]
distribution = logit_normal
min = 0.0001
max = 0.99
mean = 0.001
scale = auto

[sweep.train.e3b_lambda]
distribution = log_normal
min = 0.01
max = 10.0
mean = 0.1
scale = auto

[sweep.train.e3b_norm]
distribution = log_normal
min = 0.0001
max = 0.1
mean = 0.001
scale = auto




================================================
FILE: pufferlib/config/ocean/onestateworld.ini
================================================
[base]
package = ocean
env_name = puffer_onestateworld
policy_name = Policy
rnn_name = None

[vec]
num_envs = 8

[env]
num_envs = 512
mean_left = 0.1
mean_right = 0.5
var_right = 10

[policy]
hidden_size = 128

[train]
total_timesteps = 5_000_000



================================================
FILE: pufferlib/config/ocean/pacman.ini
================================================
[base]
package = ocean
env_name = puffer_pacman
policy_name = Policy
rnn_name = Recurrent

[vec]
num_envs = 8

[env]
num_envs = 1024
randomize_starting_position = 1 

[train]
total_timesteps = 110_000_000
adam_beta1 = 0.9038605017693528
adam_beta2 = 0.9974184818428597
adam_eps = 0.000023302187415940045
bptt_horizon = 64
clip_coef = 0.08819998793159559
ent_coef = 0.003877776836171818
gae_lambda = 0.9548561279014964
gamma = 0.9808956918725869
learning_rate = 0.07834293253084383
max_grad_norm = 1.4336515067572169
minibatch_size = 32768
prio_alpha = 0.912262602688309
prio_beta0 = 0.8868847849454541
vf_clip_coef = 0.2143010707893266
vf_coef = 0.31518694995467555
vtrace_c_clip = 0.30575543665366217
vtrace_rho_clip = 1.5301756939690652




================================================
FILE: pufferlib/config/ocean/pong.ini
================================================
[base]
package = ocean
env_name = puffer_pong 
policy_name = Policy
rnn_name = Recurrent

[vec]
num_envs = 4

[env]
num_envs = 1024 
frameskip = 8

[train]
total_timesteps = 12_000_000
adam_beta1 = 0.9766295300012044
adam_beta2 = 0.9998113167362397
adam_eps = 6.301709731262074e-9
bptt_horizon = 64
clip_coef = 0.22131450913204256
ent_coef = 0.0020310049268479863
gae_lambda = 0.8854219852971792
gamma = 0.9608378504980243
learning_rate = 0.07109386062895108
max_grad_norm = 1.7820203601055993
minibatch_size = 32768
prio_alpha = 0.09999999999999998
prio_beta0 = 0.7475661360032159
vf_clip_coef = 2.7025841941932303
vf_coef = 1.9960893747329385
vtrace_c_clip = 1.0873122745787867
vtrace_rho_clip = 2.784150207139061

[sweep.train.total_timesteps]
distribution = log_normal
min = 1e7
max = 2e8
mean = 8e7
scale = auto

[sweep.env.frameskip]
distribution = int_uniform
min = 1
max = 8
mean = 4
scale = 2.0



================================================
FILE: pufferlib/config/ocean/pysquared.ini
================================================
[base]
package = ocean
env_name = puffer_pysquared
policy_name = Policy
rnn_name = Recurrent

# Set up to match our C version defaults for speed comparison
[vec]
num_envs = 8192
num_workers = 2

[train]
total_timesteps = 20_000_000
gamma = 0.95
learning_rate = 0.05
minibatch_size = 32768



================================================
FILE: pufferlib/config/ocean/rware.ini
================================================
[base]
package = ocean
env_name = puffer_rware
policy_name = Policy
rnn_name = Recurrent

[vec]
num_envs = 8

[env]
num_envs = 128
map_choice = 2
num_agents = 8
num_requested_shelves = 8

[train]
total_timesteps = 100_000_000
learning_rate = 0.05
minibatch_size = 32768

[sweep.train.total_timesteps]
distribution = log_normal
min = 3e7
max = 3e8
mean = 1e8
scale = 0.25



================================================
FILE: pufferlib/config/ocean/sanity.ini
================================================
[base]
package = ocean
env_name = puffer_bandit puffer_memory puffer_multiagent puffer_password puffer_spaces puffer_stochastic
policy_name = Policy
rnn_name = Recurrent

[train]
total_timesteps = 50_000
learning_rate = 0.017
num_envs = 8
num_workers = 2
env_batch_size = 8
batch_size = 1024
minibatch_size = 128
bptt_horizon = 4
device = cpu

[sweep.train.batch_size]
distribution = uniform
min = 512
max = 2048
mean = 1024
scale = 0.5

[sweep.train.minibatch_size]
distribution = uniform
min = 64
max = 512
mean = 128
scale = 0.5



================================================
FILE: pufferlib/config/ocean/shared_pool.ini
================================================
[base]
package = ocean
env_name = puffer_shared_pool
rnn_name = Recurrent

[env]
num_envs = 512
vision = 3
num_agents = [8]
report_interval=1
reward_food = 0.1
interactive_food_reward = 0.2
reward_move = +0.00
food_base_spawn_rate = 2e-3

[train]
total_timesteps = 60_000_000
bptt_horizon = 16
checkpoint_interval = 200
learning_rate = 0.0008524
gamma = 0.9989
gae_lambda = 0.99
vf_coef = 1
ent_coef = 0.01
adam_beta1 = 0.9
adam_beta2 = 0.999
adam_eps = 1e-12
max_grad_norm = 0.5
vf_clip_coef = 0.1
update_epochs = 1

[sweep.env.reward_food]
distribution = log_normal
min = 0.0001
max = 0.01
mean = 0.001
scale = auto 

[sweep.env.interactive_food_reward]
distribution = log_normal
min = 0.0001
max = 0.02
mean = 0.002
scale = auto

[sweep.train.total_timesteps]
distribution = log_normal
min = 50e6
max = 75e6
mean = 60e6
scale = time



================================================
FILE: pufferlib/config/ocean/slimevolley.ini
================================================
[base]
package = ocean
env_name = puffer_slimevolley
policy_name = Policy

[env]
; 1 for single-agent (vs bot), 2 for two-agent (self-play)
num_agents=2

[train]
learning_rate = 0.015
total_timesteps = 10_000_000
num_envs=128
num_workers=8
batch_size=1024
minibatch_size=128


================================================
FILE: pufferlib/config/ocean/snake.ini
================================================
[base]
package = ocean
env_name = puffer_snake
policy_name = Snake
#policy_name = Policy
rnn_name = Recurrent

[env]
num_envs = 4
width = 640
height = 360
num_snakes = 256
num_food = 4096
vision = 5
leave_corpse_on_death = True
reward_food = 0.1
reward_corpse = 0.1
reward_death = -1.0

[vec]
num_envs = 16

[train]
total_timesteps = 500_000_000
adam_beta1 = 0.6762060389098516
adam_beta2 = 0.9
adam_eps = 0.000002764249390410885
bptt_horizon = 64
clip_coef = 0.7379459916127813
ent_coef = 0.010507292602201058
gae_lambda = 0.6006253996849398
gamma = 0.9997067226101388
learning_rate = 0.016779905178021273
max_grad_norm = 0.6504710763256233
minibatch_size = 32768
prio_alpha = 0.6082618023318664
prio_beta0 = 0.447524297405661
vf_clip_coef = 2.830994746057568
vf_coef = 3.9655925817980053
vtrace_c_clip = 0
vtrace_rho_clip = 0.9285200248552337

[sweep.env.reward_food]
distribution = uniform
min = 0.0
max = 1.0
mean = 0.0
scale = auto

[sweep.env.reward_death]
distribution = uniform
min = -1.0
max = 0.0
mean = 0.0
scale = auto

[sweep.train.total_timesteps]
distribution = log_normal
min = 5e7
max = 2e8
mean = 1e8
scale = auto



================================================
FILE: pufferlib/config/ocean/squared.ini
================================================
[base]
package = ocean
env_name = puffer_squared
policy_name = Policy
rnn_name = Recurrent

[env]
num_envs = 4096

[train]
total_timesteps = 20_000_000
gamma = 0.95
learning_rate = 0.05
minibatch_size = 32768



================================================
FILE: pufferlib/config/ocean/tactical.ini
================================================
[base]
package = ocean
env_name = puffer_tactical



================================================
FILE: pufferlib/config/ocean/target.ini
================================================
[base]
package = ocean
env_name = puffer_target
policy_name = Policy
rnn_name = Recurrent

[env]
num_envs = 512
num_agents = 8
num_goals = 4

[train]
total_timesteps = 100_000_000
gamma = 0.99
learning_rate = 0.015
minibatch_size = 32768
ent_coef = 0.02




================================================
FILE: pufferlib/config/ocean/template.ini
================================================
[base]
package = ocean
env_name = puffer_template
policy_name = Policy

[env]
num_envs = 4096

[train]
total_timesteps = 10_000_000



================================================
FILE: pufferlib/config/ocean/terraform.ini
================================================
[base]
package = ocean
env_name = puffer_terraform
policy_name = Terraform
rnn_name = Recurrent

[vec]
num_envs = 8
#backend = Serial
[env]
num_envs =  1024
num_agents = 1
reset_frequency = 1024
reward_scale = 0.11

[policy]
hidden_size = 256

[rnn]
input_size = 256
hidden_size = 256

[train]
total_timesteps = 1_000_000_000
adam_beta1 = 0.8792313963264954
adam_beta2 = 0.9980457691558037
adam_eps = 0.0000060001757672174796
bptt_horizon = 64
ent_coef = 0.007047731279570716
gae_lambda = 0.95
gamma = 0.98
learning_rate = 0.005
max_grad_norm = 1.1870216773228415
minibatch_size = 32768
prio_alpha = 0.498348178927537
prio_beta0 = 0.7687009564385903
vf_clip_coef = 1.4509861770544443
vf_coef = 3.175722544969796
vtrace_c_clip = 0.937506506536413
vtrace_rho_clip = 1.208308436542831

[sweep.train.total_timesteps]
distribution = log_normal
min = 2e8
max = 6e8
mean = 4e8
scale = time

#[sweep.env.reset_frequency]
#distribution = int_uniform
#min = 1024
#max = 16384
#mean = 8192
#scale = auto

[sweep.env.reward_scale]
distribution = log_normal
min = 0.01
max = 1
mean = 0.5
scale = auto



================================================
FILE: pufferlib/config/ocean/tetris.ini
================================================
[base]
package = ocean
env_name = puffer_tetris
policy_name = Policy
rnn_name = Recurrent

[vec]
num_envs = 8

[env]
num_envs = 1024
deck_size = 3

[train]
total_timesteps = 2_000_000_000
batch_size = auto
bptt_horizon = 64
minibatch_size = 32768

[sweep]
metric = score
goal = maximize

[sweep.train.total_timesteps]
distribution = log_normal
min = 2e8
max = 4e8
mean = 3e8
scale = auto



================================================
FILE: pufferlib/config/ocean/tmaze.ini
================================================
[base]
package = ocean
env_name = puffer_tmaze
policy_name = Policy
rnn_name = Recurrent

[vec]
num_envs = 8

[env]
num_envs = 512
size = 16

[policy]
hidden_size = 128

[rnn]
input_size = 128
hidden_size = 128

[train]
total_timesteps = 5_000_000
bptt_horizon = 32
; entropy_coef = 0.01


================================================
FILE: pufferlib/config/ocean/tower_climb.ini
================================================
[base]
package = ocean
env_name = puffer_tower_climb
policy_name = TowerClimb
rnn_name = TowerClimbLSTM

[vec]
num_envs = 8

[env]
num_envs = 1024
num_maps = 50
reward_climb_row = 0.636873185634613
reward_fall_row = -0.15898257493972778
reward_illegal_move = -0.003928301855921745
reward_move_block = 0.235064297914505

[train]
total_timesteps = 150_000_000
#gamma = 0.98
#learning_rate = 0.05
minibatch_size = 32768

[sweep.train.total_timesteps]
distribution = uniform
min = 50_000_000
max = 200_000_000
mean = 100_000_000
scale = 0.5

[sweep.env.reward_climb_row]
distribution = uniform
min = 0.0
max = 1.0
mean = 0.5
scale = auto

[sweep.env.reward_fall_row]
distribution = uniform
min = -1.0
max = 0.0
mean = -0.5
scale = auto

[sweep.env.reward_illegal_move]
distribution = uniform
min = -1e-2
max = -1e-4
mean = -1e-3
scale = auto

[sweep.env.reward_move_block]
distribution = uniform
min = 0.0
max = 1.0
mean = 0.5
scale = auto



================================================
FILE: pufferlib/config/ocean/trash_pickup.ini
================================================
[base]
package = ocean
env_name = puffer_trash_pickup 
policy_name = TrashPickup
rnn_name = Recurrent

[vec]
num_envs = 8

[env]
num_envs = 128
grid_size = 20
num_agents = 8
num_trash = 40
num_bins = 2
max_steps = 500
report_interval = 32
agent_sight_range = 5

[train]
total_timesteps = 100_000_000

[sweep.train.total_timesteps]
distribution = log_normal
min = 3e7
max = 2e8
mean = 1e8
scale = 0.5



================================================
FILE: pufferlib/config/ocean/tripletriad.ini
================================================
[base]
package = ocean
env_name = puffer_tripletriad
policy_name = Policy
rnn_name = Recurrent

[env]
num_envs = 1024

[vec]
num_envs = 8

[train]
total_timesteps = 100_000_000

[sweep.train.total_timesteps]
distribution = log_normal
min = 5e7
max = 2e8
mean = 1e8
scale = 0.25



================================================
FILE: pufferlib/config/ocean/whisker_racer.ini
================================================
[base]
package = ocean
env_name = puffer_whisker_racer
policy_name = Policy
rnn_name = Recurrent

[vec]
num_envs = 8

[env]
num_envs = 1024
frameskip = 4
width = 1080
height = 720
track_width = 75
num_radial_sectors = 180
num_points = 16
bezier_resolution = 4
turn_pi_frac = 40
w_ang = 0.777 # 0.586 # 0.523
reward_yellow = 0.2
reward_green = -0.001
corner_thresh = 0.5 # dot product for hairpins
ftmp1 = 0.5 #0.9
ftmp2 = 3.0 #1.05
ftmp3 = 0.3 # 0.2
ftmp4 = 0.0
mode7 = 0
render_many = 0
rng = 6
method = 2

[policy]
hidden_size = 128

[rnn]
input_size = 128
hidden_size = 128

[train]
adam_beta1 = 0.9446160612709289
adam_beta2 = 0.9898294105500932
adam_eps = 3.599894131847621e-14
batch_size = auto
bptt_horizon = 64
clip_coef = 0.18182501031893042
ent_coef = 0.014660408908451323
gae_lambda = 0.9560790493173461
gamma = 0.9966518685154753
learning_rate = 0.009774918469042404
max_grad_norm = 1.1402446026380597
#max_minibatch_size = 32768
#min_minibatch_size = 32768
minibatch_size = 32768 #16384 32768
prio_alpha = 0.8186786991771018
prio_beta0 = 0.49639773186725333
total_timesteps = 200_000_000
vf_clip_coef = 1.1492700894337171
vf_coef = 1.2551354745222134
vtrace_c_clip = 0
vtrace_rho_clip = 2.2637237926128577

[sweep]
method = Protein
metric = score
goal = maximize
downsample = 10

[sweep.train.total_timesteps]
distribution = log_normal
min = 19_000_000
max = 20_000_000
mean = 19_500_000
scale = auto

[sweep.env.track_width]
distribution = log_normal
min = 30
max = 75
mean = 50
scale = auto

[sweep.train.learning_rate]
distribution = log_normal
min = 0.03
mean = 0.08
max = 0.15
scale = 0.5

[sweep.env.turn_pi_frac]
distribution = uniform
min = 30
max = 75
mean = 50
scale = auto

[sweep.env.num_radial_sectors]
distribution = uniform
min = 8
max = 100
mean = 18
scale = auto

[sweep.env.w_ang]
distribution = uniform
min = 0.523 # PI / 6
max = 0.785 # PI / 4
mean = 0.654
scale = auto

#[sweep.env.frameskip]
#distribution = categorical
#values = 1, 4, 8

[sweep.env.reward_yellow]
distribution = uniform
min = 0.20
max = 0.50
mean = 0.30
scale = auto

[sweep.env.reward_green]
distribution = uniform
min = -0.001
max = 0.0
mean = -0.0001
scale = auto

#[sweep.train.vtrace_rho_clip]
#distribution = uniform
#min = 1.0
#max = 5.0
#mean = 1.5
#scale = auto

#[sweep.train.max_grad_norm]
#distribution = uniform
#min = 1.0
#mean = 1.25
#max = 5.0
#scale = auto

#[sweep.train.vf_coef]
#distribution = uniform
#min = 0.0
#max = 1.0
#mean = 1.8
#scale = auto

#[sweep.train.gae_lambda]
#distribution = logit_normal
#min = 0.96
#mean = 0.984
#max = 0.9995
#scale = auto

#[sweep.train.vtrace_c_clip]
#distribution = uniform
#min = 0.0
#max = 4.0
#mean = 1.0
#scale = auto

[sweep.train.minibatch_size]
distribution = uniform_pow2
min = 8192
max = 32768
mean = 16384
scale = auto

#[sweep.train.ent_coef]
#distribution = log_normal
#min = 0.00001
#mean = 0.005
#max = 0.05
#scale = auto



================================================
FILE: pufferlib/environments/__init__.py
================================================
from pdb import set_trace as T
import pufferlib

def try_import(module_path, package_name=None):
    if package_name is None:
        package_name = module_path
    try:
       package = __import__(module_path)
    except ImportError as e:
        raise ImportError(
            f'{e.args[0]}\n\n'
            'This is probably an installation error. Try: '
            f'pip install pufferlib[{package_name}]. '

            'Note that some environments have non-python dependencies. '
            'These are included in PufferTank. Or, you can install '
            'manually by following the instructions provided by the '
            'environment meaintainers. But some are finicky, so we '
            'recommend using PufferTank.'
        ) from e
    return package



================================================
FILE: pufferlib/environments/atari/__init__.py
================================================
from .environment import env_creator

try:
    import torch
except ImportError:
    pass
else:
    from .torch import Policy
    try:
        from .torch import Recurrent
    except:
        Recurrent = None



================================================
FILE: pufferlib/environments/atari/environment.py
================================================
from pdb import set_trace as T
import numpy as np
import functools

import gymnasium as gym

import pufferlib
import pufferlib.emulation
import pufferlib.environments

def env_creator(name='breakout'):
    return functools.partial(make, name)

def make(name, obs_type='grayscale', frameskip=4,
        full_action_space=False, framestack=1,
        repeat_action_probability=0.0, render_mode='rgb_array',
        buf=None, seed=0):
    '''Atari creation function'''
    pufferlib.environments.try_import('ale_py', 'AtariEnv')

    ale_render_mode = render_mode
    if render_mode == 'human':
        ale_render_mode = 'rgb_array'
        obs_type = 'rgb'
        frameskip = 1
        full_action_space = True
        upscale = 4
    elif render_mode == 'raylib':
        ale_render_mode = 'rgb_array'
        upscale = 8

    from ale_py import AtariEnv
    env = AtariEnv(name, obs_type=obs_type, frameskip=frameskip,
        repeat_action_probability=repeat_action_probability,
        full_action_space=full_action_space,
        render_mode=ale_render_mode)

    action_set = env._action_set
                    
    if render_mode != 'human':
        env = pufferlib.ResizeObservation(env, downscale=2)

    if framestack > 1:
        env = gym.wrappers.FrameStack(env, framestack)

    if render_mode in ('human', 'raylib'):
        env = RaylibClient(env, action_set, frameskip, upscale)
    else:
        env = AtariPostprocessor(env) # Don't use standard postprocessor

    env = pufferlib.EpisodeStats(env)
    env = pufferlib.emulation.GymnasiumPufferEnv(env=env, buf=buf)
    return env

class AtariPostprocessor(gym.Wrapper):
    '''Atari breaks the normal PufferLib postprocessor because
    it sends terminal=True every live, not every episode'''
    def __init__(self, env):
        super().__init__(env)
        shape = env.observation_space.shape
        if len(shape) < 3:
            shape = (1, *shape)
        else:
            shape = (shape[2], shape[0], shape[1])

        self.observation_space = gym.spaces.Box(low=0, high=255,
            shape=shape, dtype=env.observation_space.dtype)

    def unsqueeze_transpose(self, obs):
        if len(obs.shape) == 3:
            return np.transpose(obs, (2, 0, 1))
        else:
            return np.expand_dims(obs, 0)

    def reset(self, seed=None, options=None):
        obs, _ = self.env.reset(seed=seed)
        return self.unsqueeze_transpose(obs), {}

    def step(self, action):
        obs, reward, terminal, truncated, _ = self.env.step(action)
        return self.unsqueeze_transpose(obs), reward, terminal, truncated, {}

class RaylibClient(gym.Wrapper):
    def __init__(self, env, action_set, frameskip=4, upscale=4):
        self.env = env

        self.keymap = {}
        for i, atn in enumerate(action_set):
            self.keymap[atn.value] = i

        obs_shape = env.observation_space.shape
        if len(obs_shape) == 2:
            height, width = obs_shape
            channels = 1
        else:
            height, width, channels = obs_shape

        height *= upscale
        width *= upscale
        from raylib import rl, colors
        rl.InitWindow(width, height, "Atari".encode())
        rl.SetTargetFPS(60//frameskip)
        self.rl = rl
        self.colors = colors


        import numpy as np
        rendered = np.zeros((width, height, 4), dtype=np.uint8)

        import pyray
        from cffi import FFI
        raylib_image = pyray.Image(FFI().from_buffer(rendered.data),
            width, height, 1, pyray.PIXELFORMAT_UNCOMPRESSED_R8G8B8)
        self.texture = rl.LoadTextureFromImage(raylib_image)
        self.action = 0

        self.upscale = upscale
        self.rescaler = np.ones((upscale, upscale, 1), dtype=np.uint8)

    def any_key_pressed(self, keys):
        for key in keys:
            if self.rl.IsKeyPressed(key):
                return True
        return False

    def any_key_down(self, keys):
        for key in keys:
            if self.rl.IsKeyDown(key):
                return True
        return False

    def down(self):
        return self.any_key_down([self.rl.KEY_S, self.rl.KEY_DOWN])

    def up(self):
        return self.any_key_down([self.rl.KEY_W, self.rl.KEY_UP])

    def left(self):
        return self.any_key_down([self.rl.KEY_A, self.rl.KEY_LEFT])

    def right(self):
        return self.any_key_down([self.rl.KEY_D, self.rl.KEY_RIGHT])

    def render(self):
        from ale_py import Action

        rl = self.rl
        if rl.IsKeyPressed(rl.KEY_ESCAPE):
            exit(0)

        elif rl.IsKeyDown(rl.KEY_SPACE):
            if self.left() and self.down():
                action = Action.DOWNLEFTFIRE.value
            elif self.right() and self.down():
                action = Action.DOWNRIGHTFIRE.value
            elif self.left() and self.up():
                action = Action.UPLEFTFIRE.value
            elif self.right() and self.up():
                action = Action.UPRIGHTFIRE.value
            elif self.left():
                action = Action.LEFTFIRE.value
            elif self.right():
                action = Action.RIGHTFIRE.value
            elif self.up():
                action = Action.UPFIRE.value
            elif self.down():
                action = Action.DOWNFIRE.value
            else:
                action = Action.FIRE.value
        elif self.left() and self.down():
            action = Action.DOWNLEFT.value
        elif self.right() and self.down():
            action = Action.DOWNRIGHT.value
        elif self.left() and self.up():
            action = Action.UPLEFT.value
        elif self.right() and self.up():
            action = Action.UPRIGHT.value
        elif self.left():
            action = Action.LEFT.value
        elif self.right():
            action = Action.RIGHT.value
        elif self.up():
            action = Action.UP.value
        else:
            action = Action.NOOP.value

        if action in self.keymap:
            self.action = self.keymap[action]
        else:
            self.action = Action.NOOP.value

        #frame = self.env.render()
        frame = self.frame
        if len(frame.shape) < 3:
            frame = np.expand_dims(frame, 2)
            frame = np.repeat(frame, 3, axis=2)

        if self.upscale > 1:
            frame = np.kron(frame, self.rescaler)

        rl.BeginDrawing()
        rl.ClearBackground(self.colors.BLACK)
        rl.UpdateTexture(self.texture, frame.tobytes())
        rl.DrawTextureEx(self.texture, (0, 0), 0, 1, self.colors.WHITE)
        rl.EndDrawing()

    def reset(self, seed=None, options=None):
        obs, info = self.env.reset(seed=seed, options=options)
        self.frame = obs
        return obs, info

    def step(self, action):
        obs, reward, terminal, truncated, info = self.env.step(self.action)
        self.frame = obs
        return obs, reward, terminal, truncated, info



================================================
FILE: pufferlib/environments/atari/torch.py
================================================
import pufferlib.models


class Recurrent(pufferlib.models.LSTMWrapper):
    def __init__(self, env, policy, input_size=512, hidden_size=512):
        super().__init__(env, policy, input_size, hidden_size)

class Policy(pufferlib.models.Convolutional):
    def __init__(self, env, input_size=512, hidden_size=512, output_size=512,
            framestack=1, flat_size=64*6*9, **kwargs):
        super().__init__(
            env=env,
            input_size=input_size,
            hidden_size=hidden_size,
            output_size=output_size,
            framestack=framestack,
            flat_size=flat_size,
        )



================================================
FILE: pufferlib/environments/box2d/__init__.py
================================================
from .environment import env_creator, make

try:
    import torch
except ImportError:
    pass
else:
    from .torch import Policy
    try:
        from .torch import Recurrent
    except:
        Recurrent = None



================================================
FILE: pufferlib/environments/box2d/environment.py
================================================
from pdb import set_trace as T

import gymnasium
import functools

import pufferlib.emulation
import pufferlib.environments
import pufferlib.postprocess


def env_creator(name='car-racing'):
    return functools.partial(make, name=name)

def make(name, domain_randomize=True, continuous=False, render_mode='rgb_array', buf=None):
    if name == 'car-racing':
        name = 'CarRacing-v2'

    env = gymnasium.make(name, render_mode=render_mode,
        domain_randomize=domain_randomize, continuous=continuous)
    env = pufferlib.postprocess.EpisodeStats(env)
    return pufferlib.emulation.GymnasiumPufferEnv(env=env, buf=buf)



================================================
FILE: pufferlib/environments/box2d/torch.py
================================================
from functools import partial
import torch

import pufferlib.models

class Recurrent(pufferlib.models.LSTMWrapper):
    def __init__(self, env, policy,
            input_size=128, hidden_size=128, num_layers=1):
        super().__init__(env, policy,
            input_size, hidden_size, num_layers)

class Policy(pufferlib.models.Convolutional):
    def __init__(self, env,
            input_size=128, hidden_size=128, output_size=128,
            framestack=3, flat_size=64*8*8):
        super().__init__(
            env=env,
            input_size=input_size,
            hidden_size=hidden_size,
            output_size=output_size,
            framestack=framestack,
            flat_size=flat_size,
            channels_last=True,
        )



================================================
FILE: pufferlib/environments/bsuite/__init__.py
================================================
from .environment import env_creator, make

try:
    import torch
except ImportError:
    pass
else:
    from .torch import Policy
    try:
        from .torch import Recurrent
    except:
        Recurrent = None



================================================
FILE: pufferlib/environments/bsuite/environment.py
================================================
from pdb import set_trace as T
import gym
import functools

import pufferlib.emulation
import pufferlib.wrappers

import bsuite
from bsuite.utils import gym_wrapper

def env_creator(name='bandit/0'):
    return functools.partial(make, name)

def make(name='bandit/0', results_dir='experiments/bsuite', overwrite=True, buf=None):
    '''BSuite environments'''
    bsuite = pufferlib.environments.try_import('bsuite')
    from bsuite.utils import gym_wrapper
    env = bsuite.load_and_record_to_csv(name, results_dir, overwrite=overwrite)
    env = gym_wrapper.GymFromDMEnv(env)
    env = BSuiteStopper(env)
    env = pufferlib.wrappers.GymToGymnasium(env)
    env = pufferlib.emulation.GymnasiumPufferEnv(env, buf=buf)
    return env

class BSuiteStopper:
    def __init__(self, env):
        self.env = env
        self.num_episodes = 0

        self.step = self.env.step
        self.render = self.env.render
        self.close = self.env.close
        self.observation_space = self.env.observation_space
        self.action_space = self.env.action_space

    def reset(self):
        '''Forces the environment to stop after the
        number of episodes required by bsuite'''
        self.num_episodes += 1

        if self.num_episodes >= self.env.bsuite_num_episodes:
            exit(0)

        return self.env.reset()



================================================
FILE: pufferlib/environments/bsuite/torch.py
================================================
from pufferlib.models import Default as Policy



================================================
FILE: pufferlib/environments/butterfly/__init__.py
================================================
from .environment import env_creator, make

try:
    import torch
except ImportError:
    pass
else:
    from .torch import Policy
    try:
        from .torch import Recurrent
    except:
        Recurrent = None



================================================
FILE: pufferlib/environments/butterfly/environment.py
================================================
from pdb import set_trace as T
from pettingzoo.utils.conversions import aec_to_parallel_wrapper
import functools

import pufferlib.emulation
import pufferlib.environments


def env_creator(name='cooperative_pong_v5'):
    return functools.partial(make, name)

def make(name, buf=None):
    pufferlib.environments.try_import('pettingzoo.butterfly', 'butterfly')
    if name == 'cooperative_pong_v5':
        from pettingzoo.butterfly import cooperative_pong_v5 as pong
        env_cls = pong.raw_env
    elif name == 'knights_archers_zombies_v10':
        from pettingzoo.butterfly import knights_archers_zombies_v10 as kaz
        env_cls = kaz.raw_env
    else:
        raise ValueError(f'Unknown environment: {name}')

    env = env_cls()
    env = aec_to_parallel_wrapper(env)
    return pufferlib.emulation.PettingZooPufferEnv(env=env, buf=buf)



================================================
FILE: pufferlib/environments/butterfly/torch.py
================================================
import pufferlib.models


class Policy(pufferlib.models.Convolutional):
    def __init__(
            self,
            env,
            flat_size=3520,
            channels_last=True,
            downsample=4,
            input_size=512,
            hidden_size=128,
            output_size=128,
            **kwargs
        ):
        super().__init__(
            env,
            framestack=3,
            flat_size=flat_size,
            channels_last=channels_last,
            downsample=downsample,
            input_size=input_size,
            hidden_size=hidden_size,
            output_size=output_size,
            **kwargs
        )



================================================
FILE: pufferlib/environments/classic_control/__init__.py
================================================
from .environment import env_creator, make

try:
    import torch
except ImportError:
    pass
else:
    from .torch import Policy
    try:
        from .torch import Recurrent
    except:
        Recurrent = None



================================================
FILE: pufferlib/environments/classic_control/environment.py
================================================
import gymnasium
from gymnasium.envs import classic_control
import functools
import numpy as np

import pufferlib
import pufferlib.emulation
import pufferlib.postprocess

ALIASES = {
    'cartpole': 'CartPole-v0',
    'mountaincar': 'MountainCar-v0',
}

def env_creator(name='cartpole'):
    return functools.partial(make, name)

def make(name, render_mode='rgb_array', buf=None):
    '''Create an environment by name'''

    if name in ALIASES:
        name = ALIASES[name]

    env = gymnasium.make(name, render_mode=render_mode)
    if name == 'MountainCar-v0':
        env = MountainCarWrapper(env)

    #env = gymnasium.wrappers.NormalizeObservation(env)
    env = gymnasium.wrappers.TransformObservation(env, lambda obs: np.clip(obs, -1, 1))
    #env = gymnasium.wrappers.NormalizeReward(env, gamma=gamma)
    env = gymnasium.wrappers.TransformReward(env, lambda reward: np.clip(reward, -1, 1))
    env = pufferlib.postprocess.EpisodeStats(env)
    return pufferlib.emulation.GymnasiumPufferEnv(env=env, buf=buf)

class MountainCarWrapper(gymnasium.Wrapper):
    def step(self, action):
        obs, reward, terminated, truncated, info = self.env.step(action)
        reward = abs(obs[0]+0.5)
        return obs, reward, terminated, truncated, info




================================================
FILE: pufferlib/environments/classic_control/torch.py
================================================
import pufferlib.models


class Policy(pufferlib.models.Default):
    def __init__(self, env, hidden_size=64):
        super().__init__(env, hidden_size)



================================================
FILE: pufferlib/environments/classic_control_continuous/__init__.py
================================================
from .environment import env_creator, make

try:
    import torch
except ImportError:
    pass
else:
    from .torch import Policy
    try:
        from .torch import Recurrent
    except:
        Recurrent = None



================================================
FILE: pufferlib/environments/classic_control_continuous/environment.py
================================================
import gymnasium
import functools

import pufferlib
import pufferlib.emulation
import pufferlib.postprocess


def env_creator(name='MountainCarContinuous-v0'):
    return functools.partial(make, name)

def make(name, render_mode='rgb_array', buf=None):
    '''Create an environment by name'''
    env = gymnasium.make(name, render_mode=render_mode)
    if name == 'MountainCarContinuous-v0':
        env = MountainCarWrapper(env)

    env = pufferlib.postprocess.ClipAction(env)
    env = pufferlib.postprocess.EpisodeStats(env)
    return pufferlib.emulation.GymnasiumPufferEnv(env=env, buf=buf)

class MountainCarWrapper(gymnasium.Wrapper):
    def step(self, action):
        obs, reward, terminated, truncated, info = self.env.step(action)
        reward = abs(obs[0]+0.5)
        return obs, reward, terminated, truncated, info




================================================
FILE: pufferlib/environments/classic_control_continuous/torch.py
================================================
import pufferlib.models


class Policy(pufferlib.models.Default):
    def __init__(self, env, hidden_size=64):
        super().__init__(env, hidden_size)



================================================
FILE: pufferlib/environments/craftax/__init__.py
================================================
from .environment import env_creator, make

try:
    import torch
except ImportError:
    pass
else:
    from .torch import Policy
    try:
        from .torch import Recurrent
    except:
        Recurrent = None



================================================
FILE: pufferlib/environments/craftax/environment.py
================================================
import functools

import pufferlib
import pufferlib.emulation

def env_creator(name='Craftax-Symbolic-v1'):
    return functools.partial(make, name)

def make(name, num_envs=2048, buf=None):
    from craftax.craftax_env import make_craftax_env_from_name
    env = make_craftax_env_from_name(name, auto_reset=True)
    env_params = env.default_params
    return pufferlib.emulation.GymnaxPufferEnv(env, env_params, num_envs=num_envs, buf=buf)



================================================
FILE: pufferlib/environments/craftax/torch.py
================================================
import torch
from torch import nn

import pufferlib.models

Recurrent = pufferlib.models.LSTMWrapper

'''
CRAFTAX_CHANNELS = 83

# Are these transposed?
CRAFTAX_ROWS = 11
CRAFTAX_COLS = 9

N_MAP = CRAFTAX_ROWS * CRAFTAX_COLS * CRAFTAX_CHANNELS
N_FLAT = 51
'''

CRAFTAX_ROWS = 7
CRAFTAX_COLS = 9
CRAFTAX_CHANNELS = 21
N_MAP = CRAFTAX_ROWS * CRAFTAX_COLS * CRAFTAX_CHANNELS
N_FLAT = 22


class Policy(nn.Module):
    def __init__(self, env, cnn_channels=32, hidden_size=128, **kwargs):
        super().__init__()
        self.map_encoder = nn.Sequential(
            pufferlib.pytorch.layer_init(
                nn.Conv2d(21, cnn_channels, 3, stride=2)),
            nn.ReLU(),
            pufferlib.pytorch.layer_init(
                nn.Conv2d(cnn_channels, cnn_channels, 3, stride=1)),
            nn.ReLU(),
            nn.Flatten(),
        )
        self.flat_encoder = nn.Sequential(
            pufferlib.pytorch.layer_init(nn.Linear(N_FLAT, hidden_size)),
            nn.ReLU(),
        )
        self.proj = nn.Sequential(
            pufferlib.pytorch.layer_init(nn.Linear(2*cnn_channels + hidden_size, hidden_size)),
            nn.ReLU(),
        )
        self.actor = pufferlib.pytorch.layer_init(
            nn.Linear(hidden_size, env.single_action_space.n), std=0.01)
        self.value_fn = pufferlib.pytorch.layer_init(
            nn.Linear(hidden_size, 1), std=1)

        self.is_continuous = False

    def forward(self, observations):
        hidden, lookup = self.encode_observations(observations)
        actions, value = self.decode_actions(hidden, lookup)
        return actions, value

    def encode_observations(self, observations):
        map_obs = observations[:, :N_MAP].view(
            -1, CRAFTAX_ROWS, CRAFTAX_COLS, CRAFTAX_CHANNELS
            ).permute(0, 3, 1, 2)
        map_obs = self.map_encoder(map_obs)
        flat_obs = observations[:, N_MAP:]
        flat_obs = self.flat_encoder(flat_obs)
        features = torch.cat([map_obs, flat_obs], dim=1)
        features = self.proj(features)
        return features, None

    def decode_actions(self, flat_hidden, lookup, concat=None):
        action = self.actor(flat_hidden)
        value = self.value_fn(flat_hidden)
        return action, value



================================================
FILE: pufferlib/environments/crafter/__init__.py
================================================
from .environment import env_creator, make

try:
    import torch
except ImportError:
    pass
else:
    from .torch import Policy
    try:
        from .torch import Recurrent
    except:
        Recurrent = None



================================================
FILE: pufferlib/environments/crafter/environment.py
================================================
from pdb import set_trace as T

import gym
import gymnasium
import shimmy
import functools

import pufferlib
import pufferlib.emulation
import pufferlib.environments
import pufferlib.postprocess
import pufferlib.utils


class TransposeObs(gym.Wrapper):
    def observation(self, observation):
        return observation.transpose(2, 0, 1)

def env_creator(name='crafter'):
    return functools.partial(make, name)

def make(name, buf=None):
    '''Crafter creation function'''
    if name == 'crafter':
        name = 'CrafterReward-v1'

    pufferlib.environments.try_import('crafter')
    env = gym.make(name)
    env.reset = pufferlib.utils.silence_warnings(env.reset)
    env = shimmy.GymV21CompatibilityV0(env=env)
    env = RenderWrapper(env)
    env = TransposeObs(env)
    env = pufferlib.postprocess.EpisodeStats(env)
    return pufferlib.emulation.GymnasiumPufferEnv(env=env, buf=buf)

class RenderWrapper(gym.Wrapper):
    def __init__(self, env):
        super().__init__(env)
        self.env = env

    @property
    def render_mode(self):
        return 'rgb_array'

    def render(self, *args, **kwargs):
        return self.env.unwrapped.env.unwrapped.render((256,256))



================================================
FILE: pufferlib/environments/crafter/torch.py
================================================
import pufferlib.models


class Policy(pufferlib.models.Convolutional):
    def __init__(
            self,
            env,
            flat_size=1024,
            channels_last=True,
            downsample=1,
            input_size=512,
            hidden_size=128,
            output_size=128,
            **kwargs
        ):
        super().__init__(
            env,
            framestack=3,
            flat_size=flat_size,
            channels_last=channels_last,
            downsample=downsample,
            input_size=input_size,
            hidden_size=hidden_size,
            output_size=output_size,
            **kwargs
        )



================================================
FILE: pufferlib/environments/dm_control/__init__.py
================================================
from .environment import env_creator, make

try:
    import torch
except ImportError:
    pass
else:
    from .torch import Policy
    try:
        from .torch import Recurrent
    except:
        Recurrent = None



================================================
FILE: pufferlib/environments/dm_control/environment.py
================================================
from pdb import set_trace as T

import gym
import shimmy
import functools

import pufferlib
import pufferlib.emulation
import pufferlib.environments


def env_creator(name='walker'):
    '''Deepmind Control environment creation function

    No support for bindings yet because PufferLib does
    not support continuous action spaces.'''
    return functools.partial(make, name)

def make(name, task_name='walk', buf=None):
    '''Untested. Let us know in Discord if you want to use dmc in PufferLib.'''
    dm_control = pufferlib.environments.try_import('dm_control.suite', 'dmc')
    env = dm_control.suite.load(name, task_name)
    env = shimmy.DmControlCompatibilityV0(env=env)
    return pufferlib.emulation.GymnasiumPufferEnv(env, buf=buf)



================================================
FILE: pufferlib/environments/dm_control/torch.py
================================================
from pufferlib.models import Default as Policy



================================================
FILE: pufferlib/environments/dm_lab/__init__.py
================================================
from .environment import env_creator, make

try:
    import torch
except ImportError:
    pass
else:
    from .torch import Policy
    try:
        from .torch import Recurrent
    except:
        Recurrent = None



================================================
FILE: pufferlib/environments/dm_lab/environment.py
================================================
from pdb import set_trace as T

import gym
import shimmy
import functools

import pufferlib
import pufferlib.emulation
import pufferlib.environments


def env_creator(name='seekavoid_arena_01'):
    '''Deepmind Lab binding creation function
    dm-lab requires extensive setup. Use PufferTank.'''
    return functools.partial(make, name=name)

def make(name, buf=None):
    '''Deepmind Lab binding creation function
    dm-lab requires extensive setup. Currently dropped frop PufferTank.
    Let us know if you need this for your work.'''
    dm_lab = pufferlib.environments.try_import('deepmind_lab', 'dm-lab')
    env = dm_lab.Lab(name, ['RGB_INTERLEAVED'])
    env = shimmy.DmLabCompatibilityV0(env=env)
    return pufferlib.emulation.GymnasiumPufferEnv(env=env, buf=buf)



================================================
FILE: pufferlib/environments/dm_lab/torch.py
================================================
import pufferlib.models


class Policy(pufferlib.models.Convolutional):
    def __init__(
            self,
            env,
            flat_size=3136,
            channels_last=True,
            downsample=1,
            input_size=512,
            hidden_size=128,
            output_size=128,
            **kwargs
        ):
        super().__init__(
            env,
            framestack=3,
            flat_size=flat_size,
            channels_last=channels_last,
            downsample=downsample,
            input_size=input_size,
            hidden_size=hidden_size,
            output_size=output_size,
            **kwargs
        )



================================================
FILE: pufferlib/environments/gpudrive/__init__.py
================================================
from .environment import env_creator

try:
    import torch
except ImportError:
    pass
else:
    from .torch import Policy
    try:
        from .torch import Recurrent
    except:
        Recurrent = None



================================================
FILE: pufferlib/environments/gpudrive/environment.py
================================================
import os
import numpy as np
from pathlib import Path
import torch
import gymnasium

from pygpudrive.env.config import EnvConfig, RenderConfig, SceneConfig, SelectionDiscipline
from pygpudrive.env.env_torch import GPUDriveTorchEnv

def env_creator(name='gpudrive'):
    return PufferGPUDrive

class PufferGPUDrive:
    def __init__(self, device='cuda', max_cont_agents=64, num_worlds=64, k_unique_scenes=1):
        self.device = device
        self.max_cont_agents = max_cont_agents
        self.num_worlds = num_worlds
        self.k_unique_scenes = k_unique_scenes
        self.total_agents = max_cont_agents * num_worlds

        # Set working directory to the base directory 'gpudrive'
        working_dir = os.path.join(Path.cwd(), '../gpudrive')
        os.chdir(working_dir)

        scene_config = SceneConfig(
            path='biggest_file/',
            num_scenes=num_worlds,
            discipline=SelectionDiscipline.K_UNIQUE_N,
            k_unique_scenes=k_unique_scenes,
        )

        env_config = EnvConfig(
            steer_actions = torch.round(
                torch.linspace(-1.0, 1.0, 3), decimals=3),
            accel_actions = torch.round(
                torch.linspace(-3, 3, 3), decimals=3
            )
        )

        render_config = RenderConfig(
            resolution=(512, 512), # Quality of the rendered images
        )

        self.env = GPUDriveTorchEnv(
            config=env_config,
            scene_config=scene_config,
            render_config=render_config,
            max_cont_agents=max_cont_agents,
            device=device,
        )

        self.obs_size = self.env.observation_space.shape[-1]
        self.action_space = self.env.action_space
        self.observation_space = self.env.observation_space
        self.observation_space = gymnasium.spaces.Box(
            low=0, high=255, shape=(self.obs_size,), dtype=np.float32)
        self.single_observation_space = self.observation_space
        self.single_action_space = self.action_space
        self.done = False
        self.emulated = None
        self.render_mode = 'rgb_array'
        self.num_live = []

        self.controlled_agent_mask = self.env.cont_agent_mask.clone()
        self.obs = self.env.reset()[self.controlled_agent_mask]
        self.num_controlled = self.controlled_agent_mask.sum().item()
        self.num_agents = self.obs.shape[0]
        self.env_id = np.array([i for i in range(self.num_agents)])
        self.mask = np.ones(self.num_agents, dtype=bool)
        self.actions = torch.zeros((num_worlds, max_cont_agents),
            dtype=torch.int64).to(self.device)

    def _obs_and_mask(self, obs):
        #self.buf.masks[:] = self.env.cont_agent_mask.numpy().ravel() * self.live_agent_mask
        #return np.asarray(obs).reshape(NUM_WORLDS*MAX_NUM_OBJECTS, self.obs_size)
        #return obs.numpy().reshape(NUM_WORLDS*MAX_NUM_OBJECTS, self.obs_size)[:, :6]
        return obs.view(self.total_agents, self.obs_size)

    def close(self):
        '''There is no point in closing the env because
        Madrona doesn't close correctly anyways. You will want
        to cache this copy for later use. Cuda errors if you don't'''
        pass 
        #self.env.close()
        #del self.env.sim

    def reset(self, seed=None, options=None):
        self.reward = torch.zeros(self.num_agents, dtype=torch.float32).to(self.device)
        self.terminal = torch.zeros(self.num_agents, dtype=torch.bool).to(self.device)
        self.truncated = torch.zeros(self.num_agents, dtype=torch.bool).to(self.device)

        self.episode_returns = torch.zeros(self.num_agents, dtype=torch.float32).to(self.device)
        self.episode_lengths = torch.zeros(self.num_agents, dtype=torch.float32).to(self.device)
        self.live_agent_mask = torch.ones((self.num_worlds, self.max_cont_agents), dtype=bool).to(self.device)
        return self.obs, self.reward, self.terminal, self.truncated, [], self.env_id, self.mask

    def step(self, action):
        action = torch.from_numpy(action).to(self.device)
        self.actions[self.controlled_agent_mask] = action
        self.env.step_dynamics(self.actions)

        obs = self.env.get_obs()[self.controlled_agent_mask]
        reward = self.env.get_rewards()[self.controlled_agent_mask]
        terminal = self.env.get_dones().bool()

        done_worlds = torch.where(
            (terminal.nan_to_num(0) * self.controlled_agent_mask).sum(dim=1)
             == self.controlled_agent_mask.sum(dim=1)
        )[0].cpu()

        self.episode_returns += reward
        self.episode_lengths += 1
        self.mask = self.live_agent_mask[self.controlled_agent_mask].cpu().numpy()
        self.live_agent_mask[terminal] = 0
        terminal = terminal[self.controlled_agent_mask]

        info = []
        self.num_live.append(self.mask.sum())

        if len(done_worlds) > 0:
            controlled_mask = self.controlled_agent_mask[done_worlds]
            info_tensor = self.env.get_infos()[done_worlds][controlled_mask]
            num_finished_agents = controlled_mask.sum().item()
            info.append({
                'off_road': info_tensor[:, 0].sum().item() / num_finished_agents,
                'veh_collisions': info_tensor[:, 1].sum().item() / num_finished_agents,
                'non_veh_collisions': info_tensor[:, 2].sum().item() / num_finished_agents,
                'goal_achieved': info_tensor[:, 3].sum().item() / num_finished_agents,
                'num_finished_agents': num_finished_agents,
                'episode_length': self.episode_lengths[done_worlds].mean().item(),
                'mean_reward_per_episode': self.episode_returns[done_worlds].mean().item(),
                'control_density': self.num_controlled / self.num_agents,
                'data_density': np.mean(self.num_live) / self.num_agents,
            })

            self.num_live = []
            for idx in done_worlds:
                self.env.sim.reset(idx)
                self.episode_returns[idx] = 0
                self.episode_lengths[idx] = 0
                self.live_agent_mask[idx] = self.controlled_agent_mask[idx]

        return obs, reward, terminal, self.truncated, info, self.env_id, self.mask

    def render(self, world_render_idx=0):
        return self.env.render(world_render_idx=world_render_idx)



================================================
FILE: pufferlib/environments/gpudrive/torch.py
================================================
from torch import nn
import torch
import torch.nn.functional as F

from functools import partial
import pufferlib.models

from pufferlib.models import Default as Policy
Recurrent = pufferlib.models.LSTMWrapper

EGO_STATE_DIM = 6
PARTNER_DIM = 10
ROAD_MAP_DIM = 13

MAX_CONTROLLED_VEHICLES = 32
ROADMAP_AGENT_FEAT_DIM = MAX_CONTROLLED_VEHICLES - 1
TOP_K_ROADPOINTS = 64 # Number of visible roadpoints from the road graph

def unpack_obs(obs_flat):
    """
    Unpack the flattened observation into the ego state and visible state.
    Args:
        obs_flat (torch.Tensor): flattened observation tensor of shape (batch_size, obs_dim)
    Return:
        ego_state, road_objects, stop_signs, road_graph (torch.Tensor).
    """
    # Unpack ego and visible state
    ego_state = obs_flat[:, :EGO_STATE_DIM]
    vis_state = obs_flat[:, EGO_STATE_DIM :]
                                                                                                                   # Visible state object order: road_objects, road_points
    # Find the ends of each section
    ro_end_idx = PARTNER_DIM * ROADMAP_AGENT_FEAT_DIM
    rg_end_idx = ro_end_idx + (ROAD_MAP_DIM * TOP_K_ROADPOINTS)
    
    # Unflatten and reshape to (batch_size, num_objects, object_dim)
    road_objects = (vis_state[:, :ro_end_idx]).reshape(
        -1, ROADMAP_AGENT_FEAT_DIM, PARTNER_DIM
    )
    road_graph = (vis_state[:, ro_end_idx:rg_end_idx]).reshape(
        -1,
        TOP_K_ROADPOINTS,
        ROAD_MAP_DIM,
    )
    return ego_state, road_objects, road_graph

class Policy(nn.Module):
    def __init__(self, env, input_size=64, hidden_size=128, **kwargs):
        super().__init__()
        self.ego_embed = nn.Sequential(
            pufferlib.pytorch.layer_init(nn.Linear(EGO_STATE_DIM, input_size)),
            torch.nn.ReLU(),
            pufferlib.pytorch.layer_init(nn.Linear(input_size, input_size)),
        )

        self.partner_embed = nn.Sequential(
            pufferlib.pytorch.layer_init(nn.Linear(PARTNER_DIM, input_size)),
            torch.nn.ReLU(),
            pufferlib.pytorch.layer_init(nn.Linear(input_size, input_size)),
        )

        self.road_map_embed = nn.Sequential(
            pufferlib.pytorch.layer_init(nn.Linear(ROAD_MAP_DIM, input_size)),
            torch.nn.ReLU(),
            pufferlib.pytorch.layer_init(nn.Linear(input_size, input_size)),
        )

        self.proj = pufferlib.pytorch.layer_init(nn.Linear(3*input_size, hidden_size))

        self.actor = pufferlib.pytorch.layer_init(
            nn.Linear(hidden_size, env.single_action_space.n), std=0.01)
        self.value_fn = pufferlib.pytorch.layer_init(
            nn.Linear(hidden_size, 1), std=1)

    def forward(self, observations):
        hidden, lookup = self.encode_observations(observations)
        actions, value = self.decode_actions(hidden, lookup)
        return actions, value

    def encode_observations(self, observations):
        ego_state, road_objects, road_graph = unpack_obs(observations)
        ego_embed = self.ego_embed(ego_state)
        partner_embed, _ = self.partner_embed(road_objects).max(dim=1)
        road_map_embed, _ = self.road_map_embed(road_graph).max(dim=1)
        embed = torch.cat([ego_embed, partner_embed, road_map_embed], dim=1)
        return self.proj(embed), None

    def decode_actions(self, flat_hidden, lookup, concat=None):
        action = self.actor(flat_hidden)
        value = self.value_fn(flat_hidden)
        return action, value



================================================
FILE: pufferlib/environments/griddly/__init__.py
================================================
from .environment import env_creator, make

try:
    import torch
except ImportError:
    pass
else:
    from .torch import Policy
    try:
        from .torch import Recurrent
    except:
        Recurrent = None



================================================
FILE: pufferlib/environments/griddly/environment.py
================================================
from pdb import set_trace as T

import gym
import shimmy
import functools

import pufferlib
import pufferlib.emulation
import pufferlib.environments
import pufferlib.postprocess

ALIASES = {
    'spiders': 'GDY-Spiders-v0',
}    

def env_creator(name='spiders'):
    return functools.partial(make, name)

# TODO: fix griddly
def make(name, buf=None):
    '''Griddly creation function

    Note that Griddly environments do not have observation spaces until
    they are created and reset'''
    if name in ALIASES:
        name = ALIASES[name]

    import warnings
    warnings.warn('Griddly has been segfaulting in the latest build and we do not know why. Submit a PR if you find a fix!')
    pufferlib.environments.try_import('griddly')
    with pufferlib.utils.Suppress():
        env = gym.make(name)
        env.reset() # Populate observation space

    env = shimmy.GymV21CompatibilityV0(env=env)
    env = pufferlib.postprocess.EpisodeStats(env)
    return pufferlib.emulation.GymnasiumPufferEnv(env, buf=buf)



================================================
FILE: pufferlib/environments/griddly/torch.py
================================================
from pufferlib.models import Default as Policy



================================================
FILE: pufferlib/environments/gvgai/environment.py
================================================
from pdb import set_trace as T
import numpy as np
import functools

import gym

import pufferlib
import pufferlib.emulation
import pufferlib.environments
import pufferlib.utils
import pufferlib.postprocess
import pufferlib.wrappers

def env_creator(name='zelda'):
    if name == 'zelda':
        name = 'gvgai-zelda-lvl0-v0'
    return functools.partial(make, name)

def make(name, obs_type='grayscale', frameskip=4, full_action_space=False,
        repeat_action_probability=0.0, render_mode='rgb_array', buf=None):
    '''Atari creation function'''
    pufferlib.environments.try_import('gym_gvgai')
    env = gym.make(name)
    env = pufferlib.wrappers.GymToGymnasium(env)
    env = pufferlib.postprocess.EpisodeStats(env)
    env = pufferlib.emulation.GymnasiumPufferEnv(env=env, buf=buf)
    return env




================================================
FILE: pufferlib/environments/kinetix/__init__.py
================================================
from .environment import env_creator, make

try:
    import torch
except ImportError:
    pass
else:
    from .torch import Policy
    try:
        from .torch import Recurrent
    except:
        Recurrent = None



================================================
FILE: pufferlib/environments/kinetix/environment.py
================================================
import functools
import numpy as np

import pufferlib

train_levels = [
    "s/h0_weak_thrust",
    "s/h7_unicycle_left",
    "s/h3_point_the_thruster",
    "s/h4_thrust_aim",
    "s/h1_thrust_over_ball",
    "s/h5_rotate_fall",
    "s/h9_explode_then_thrust_over",
    "s/h6_unicycle_right",
    "s/h8_unicycle_balance",
    "s/h2_one_wheel_car",
    "m/h0_unicycle",
    "m/h1_car_left",
    "m/h2_car_right",
    "m/h3_car_thrust",
    "m/h4_thrust_the_needle",
    "m/h5_angry_birds",
    "m/h6_thrust_over",
    "m/h7_car_flip",
    "m/h8_weird_vehicle",
    "m/h9_spin_the_right_way",
    "m/h10_thrust_right_easy",
    "m/h11_thrust_left_easy",
    "m/h12_thrustfall_left",
    "m/h13_thrustfall_right",
    "m/h14_thrustblock",
    "m/h15_thrustshoot",
    "m/h16_thrustcontrol_right",
    "m/h17_thrustcontrol_left",
    "m/h18_thrust_right_very_easy",
    "m/h19_thrust_left_very_easy",
    "m/arm_left",
    "m/arm_right",
    "m/arm_up",
    "m/arm_hard",
    "l/h0_angrybirds",
    "l/h1_car_left",
    "l/h2_car_ramp",
    "l/h3_car_right",
    "l/h4_cartpole",
    "l/h5_flappy_bird",
    "l/h6_lorry",
    "l/h7_maze_1",
    "l/h8_maze_2",
    "l/h9_morph_direction",
    "l/h10_morph_direction_2",
    "l/h11_obstacle_avoidance",
    "l/h12_platformer_1",
    "l/h13_platformer_2",
    "l/h14_simple_thruster",
    "l/h15_swing_up",
    "l/h16_thruster_goal",
    "l/h17_unicycle",
    "l/hard_beam_balance",
    "l/hard_cartpole_thrust",
    "l/hard_cartpole_wheels",
    "l/hard_lunar_lander",
    "l/hard_pinball",
    "l/grasp_hard",
    "l/grasp_easy",
    "l/mjc_half_cheetah",
    "l/mjc_half_cheetah_easy",
    "l/mjc_hopper",
    "l/mjc_hopper_easy",
    "l/mjc_swimmer",
    "l/mjc_walker",
    "l/mjc_walker_easy",
    "l/car_launch",
    "l/car_swing_around",
    "l/chain_lander",
    "l/chain_thrust",
    "l/gears",
    "l/lever_puzzle",
    "l/pr",
    "l/rail",
]


def env_creator(name="kinetix"):
    from kinetix.environment.env import ObservationType, ActionType

    _, obs, act = name.split("-")
    if obs == "symbolic": obs = "symbolic_flat"

    try:
        obs = ObservationType.from_string(obs)
    except ValueError:
        raise ValueError(f"Unknown observation type: {obs}.")
    try:
        act = ActionType.from_string(act)
    except ValueError:
        raise ValueError(f"Unknown action type: {act}.")

    return functools.partial(KinetixPufferEnv, observation_type=obs, action_type=act)


def make(name, *args, **kwargs):
    return KinetixPufferEnv(*args, **kwargs)


class KinetixPufferEnv(pufferlib.environment.PufferEnv):
    def __init__(self, observation_type, action_type, num_envs=1, buf=None):

        from kinetix.environment.env import make_kinetix_env, ObservationType, ActionType
        from kinetix.environment.env_state import EnvParams, StaticEnvParams
        from kinetix.environment.ued.ued_state import UEDParams
        from kinetix.environment.ued.ued import make_reset_fn_list_of_levels

        import jax
        from gymnax.environments.spaces import gymnax_space_to_gym_space

        self.observation_type = observation_type
        self.action_type = action_type

        # Use default parameters
        env_params = EnvParams()
        static_env_params = StaticEnvParams().replace()

        # Create the environment
        env = make_kinetix_env(
            observation_type=observation_type,  # ObservationType.PIXELS,
            action_type=action_type,  # ActionType.DISCRETE,
            reset_fn=make_reset_fn_list_of_levels(train_levels, static_env_params),
            env_params=env_params,
            static_env_params=static_env_params,
        )

        self.single_observation_space = gymnax_space_to_gym_space(env.observation_space(env_params))
        self.single_action_space = gymnax_space_to_gym_space(env.action_space(env_params))
        self.num_agents = num_envs

        self.env = env

        super().__init__(buf)
        self.env_params = env_params
        self.env = env

        self.reset_fn = jax.jit(jax.vmap(env.reset, in_axes=(0, None)))
        self.step_fn = jax.jit(jax.vmap(env.step, in_axes=(0, 0, 0, None)))
        self.rng = jax.random.PRNGKey(0)

    def reset(self, rng, params=None):
        import jax
        from torch.utils import dlpack as torch_dlpack

        self.rng, _rng = jax.random.split(self.rng)
        self.rngs = jax.random.split(_rng, self.num_agents)
        obs, self.state = self.reset_fn(self.rngs, params)
        obs = self._obs_to_tensor(obs)

        self.observations = torch_dlpack.from_dlpack(jax.dlpack.to_dlpack(obs))
        return self.observations, []

    def step(self, action):
        import jax
        from torch.utils import dlpack as torch_dlpack

        obs, self.state, reward, done, info = self.step_fn(self.rngs, self.state, action, self.env_params)
        obs = self._obs_to_tensor(obs)

        # Convert JAX array to DLPack, then to PyTorch tensor
        self.observations = torch_dlpack.from_dlpack(jax.dlpack.to_dlpack(obs))
        self.rewards = np.asarray(reward)
        self.terminals = np.asarray(done)
        infos = [{k: v.mean().item() for k, v in info.items()} | {"reward": self.rewards.mean()} ]
        return self.observations, self.rewards, self.terminals, self.terminals, infos

    def _obs_to_tensor(self, obs):
        from kinetix.environment.env import ObservationType
        if self.observation_type == ObservationType.PIXELS:
            return obs.image
        elif self.observation_type == ObservationType.SYMBOLIC_FLAT:
            return obs
        else:
            raise ValueError(f"Unknown observation type: {self.observation_type}.")



================================================
FILE: pufferlib/environments/kinetix/torch.py
================================================
import torch
from torch import nn

import pufferlib.models

Recurrent = pufferlib.models.LSTMWrapper

from pufferlib.models import Default as Policy
SymbolicPolicy = Policy

class PixelsPolicy(nn.Module):
    def __init__(self, env, cnn_channels=32, hidden_size=128, **kwargs):
        super().__init__()
        self.hidden_size = 128
        self.map_encoder = nn.Sequential(
            pufferlib.pytorch.layer_init(nn.Conv2d(3, cnn_channels, 8, stride=4)),
            nn.ReLU(),
            pufferlib.pytorch.layer_init(nn.Conv2d(cnn_channels, cnn_channels, 4, stride=2)),
            nn.ReLU(),
            nn.Flatten(),
        )
        self.proj = nn.Sequential(
            pufferlib.pytorch.layer_init(nn.Linear(14 * 14 * cnn_channels, hidden_size)),
            nn.ReLU(),
        )
        self.actor = pufferlib.pytorch.layer_init(nn.Linear(hidden_size, env.single_action_space.n), std=0.01)
        self.value_fn = pufferlib.pytorch.layer_init(nn.Linear(hidden_size, 1), std=1)

        self.is_continuous = False

    def forward(self, observations):
        hidden = self.encode_observations(observations)
        actions, value = self.decode_actions(hidden)
        return actions, value

    def encode_observations(self, observations):
        encoded = self.map_encoder(observations.permute(0, 3, 1, 2))
        features = self.proj(encoded)
        return features

    def decode_actions(self, flat_hidden, concat=None):
        action = self.actor(flat_hidden)
        value = self.value_fn(flat_hidden)
        return action, value



================================================
FILE: pufferlib/environments/links_awaken/__init__.py
================================================
from .environment import env_creator, make_env

try:
    import torch
except ImportError:
    pass
else:
    from .torch import Policy
    try:
        from .torch import Recurrent
    except:
        Recurrent = None



================================================
FILE: pufferlib/environments/links_awaken/environment.py
================================================
from pdb import set_trace as T

import gymnasium

from links_awaken import LinksAwakenV1 as env_creator

import pufferlib.emulation


def make_env(headless: bool = True, state_path=None, buf=None):
    '''Links Awakening'''
    env = env_creator(headless=headless, state_path=state_path)
    env = gymnasium.wrappers.ResizeObservation(env, shape=(72, 80))
    return pufferlib.emulation.GymnasiumPufferEnv(env=env,
        postprocessor_cls=pufferlib.emulation.BasicPostprocessor, buf=buf)



================================================
FILE: pufferlib/environments/links_awaken/torch.py
================================================
import pufferlib.models
from pufferlib.pytorch import LSTM


class Recurrent:
    input_size = 512
    hidden_size = 512
    num_layers = 1

class Policy(pufferlib.models.Convolutional):
    def __init__(self, env, input_size=512, hidden_size=512, output_size=512,
            framestack=3, flat_size=64*5*6):
        super().__init__(
            env=env,
            input_size=input_size,
            hidden_size=hidden_size,
            output_size=output_size,
            framestack=framestack,
            flat_size=flat_size,
            channels_last=True,
        )



================================================
FILE: pufferlib/environments/magent/__init__.py
================================================
from .environment import env_creator, make

try:
    import torch
except ImportError:
    pass
else:
    from .torch import Policy
    try:
        from .torch import Recurrent
    except:
        Recurrent = None



================================================
FILE: pufferlib/environments/magent/environment.py
================================================
from pdb import set_trace as T
from pettingzoo.utils.conversions import aec_to_parallel_wrapper
import functools

import pufferlib.emulation
import pufferlib.environments
import pufferlib.wrappers


def env_creator(name='battle_v4'):
    return functools.partial(make, name)
    pufferlib.environments.try_import('pettingzoo.magent', 'magent')

def make(name, buf=None):
    '''MAgent Battle V4 creation function'''
    if name == 'battle_v4':
        from pettingzoo.magent import battle_v4
        env_cls = battle_v4.env
    else:
        raise ValueError(f'Unknown environment name {name}')
 
    env = env_cls()
    env = aec_to_parallel_wrapper(env)
    env = pufferlib.wrappers.PettingZooTruncatedWrapper(env)
    return pufferlib.emulation.PettingZooPufferEnv(env=env, buf=buf)



================================================
FILE: pufferlib/environments/magent/torch.py
================================================
from torch import nn

import pufferlib.models


class Policy(pufferlib.models.Policy):
    '''Based off of the DQN policy in MAgent'''
    def __init__(self, env, hidden_size=256, output_size=256, kernel_num=32):
        '''The CleanRL default Atari policy: a stack of three convolutions followed by a linear layer
        
        Takes framestack as a mandatory keyword arguments. Suggested default is 1 frame
        with LSTM or 4 frames without.'''
        super().__init__(env)
        self.num_actions = self.action_space.n

        self.network = nn.Sequential(
            pufferlib.pytorch.layer_init(nn.Conv2d(5, kernel_num, 3)),
            nn.ReLU(),
            pufferlib.pytorch.layer_init(nn.Conv2d(kernel_num, kernel_num, 3)),
            nn.ReLU(),
            nn.Flatten(),
            pufferlib.pytorch.layer_init(nn.Linear(kernel_num*9*9, hidden_size)),
            nn.ReLU(),
            pufferlib.pytorch.layer_init(nn.Linear(hidden_size, hidden_size)),
            nn.ReLU(),
        )

        self.actor = pufferlib.pytorch.layer_init(nn.Linear(output_size, self.num_actions), std=0.01)
        self.value_function = pufferlib.pytorch.layer_init(nn.Linear(output_size, 1), std=1)

    def critic(self, hidden):
        return self.value_function(hidden)

    def encode_observations(self, observations):
        observations = observations.permute(0, 3, 1, 2)
        return self.network(observations), None

    def decode_actions(self, hidden, lookup):
        action = self.actor(hidden)
        value = self.value_function(hidden)
        return action, value



================================================
FILE: pufferlib/environments/mani_skill/__init__.py
================================================
from .environment import env_creator, make

try:
    import torch
except ImportError:
    pass
else:
    from .torch import Policy
    try:
        from .torch import Recurrent
    except:
        Recurrent = None



================================================
FILE: pufferlib/environments/mani_skill/environment.py
================================================
import functools
import numpy as np
from collections import defaultdict

import mani_skill.envs
from mani_skill.vector.wrappers.gymnasium import ManiSkillVectorEnv

import gymnasium as gym
import torch

import pufferlib

ALIASES = {
    'mani_pickcube': 'PickCube-v1',
    'mani_pushcube': 'PushCube-v1',
    'mani_stackcube': 'StackCube-v1',
    'mani_peginsertion': 'PegInsertionSide-v1',
}

def env_creator(name='PickCube-v1', **kwargs):
    return functools.partial(make, name)

def make(name, num_envs=1, render_mode='rgb_array', buf=None, seed=0, **kwargs):
    '''Create an environment by name'''

    if name in ALIASES:
        name = ALIASES[name]

    return ManiPufferEnv(name, num_envs=num_envs, render_mode=render_mode, buf=buf, seed=seed, **kwargs)

class ManiPufferEnv(pufferlib.PufferEnv):
    def __init__(self, name, num_envs=1, solver_position_iterations=15,
            sim_steps_per_control=5, control_freq=20, render_mode='rgb_array',
            log_interval=16, buf=None, seed=0):
        sim_freq = int(sim_steps_per_control * control_freq)
        sim_config = {
            'scene_config': {
                'solver_position_iterations': solver_position_iterations
            },
            'sim_freq': sim_freq,
            'control_freq': control_freq
        }
        self.env = gym.make(name, reward_mode='delta', num_envs=num_envs,
            render_mode=render_mode, sim_config=sim_config)
        self.env = ManiSkillVectorEnv(self.env, auto_reset=True, ignore_terminations=False, record_metrics=True)
        self.agents_per_batch = num_envs

        obs_space = self.env.observation_space
        self.single_observation_space = gym.spaces.Box(
            low=obs_space.low[0],
            high=obs_space.high[0],
            shape=obs_space.shape[1:],
            dtype=obs_space.dtype,
        )

        atn_space = self.env.action_space
        self.single_action_space = gym.spaces.Box(
            low=atn_space.low[0],
            high=atn_space.high[0],
            shape=atn_space.shape[1:],
            dtype=atn_space.dtype,
        )

        self.render_mode = render_mode
        self.num_agents = num_envs
        self.log_interval = log_interval
        self.tick = 0

        self.env_id = np.arange(num_envs)

        self.logs = defaultdict(list)
        
        super().__init__(buf)

    def _flatten_info(self, info):
        if "final_info" in info:
            mask = info["_final_info"]
            for k, v in info["final_info"]["episode"].items():
                self.logs[k].append(v[mask].float().mean().item())

    def reset(self, seed=0):
        obs, info = self.env.reset()
        #self.observations = torch.nan_to_num(obs)
        self.observations = torch.clamp(torch.nan_to_num(obs), -5, 5)
        self.observations = obs / 20.0
        self._flatten_info(info)
        return obs, []

    def step(self, actions):
        obs, reward, terminated, truncated, info = self.env.step(actions)
        collapsed = torch.where(torch.isnan(obs).sum(1) > 0)[0]
        if len(collapsed) > 0:
            obs, _ = self.env.reset(options={'env_idx': collapsed})

        self.observations = torch.clamp(torch.nan_to_num(obs), -5, 5)
        #self.observations = obs / 20.0 #torch.nan_to_num(obs)
        self.rewards = reward
        self.terminated = terminated
        self.truncated = truncated
        self._flatten_info(info)

        self.infos = []
        self.tick += 1
        if self.tick % self.log_interval == 0:
            info = {}
            for k, v in self.logs.items():
                info[k] = np.mean(v)

            self.logs = defaultdict(list)
            self.infos.append(info)

        return obs, reward, terminated, truncated, self.infos

    def render(self):
        return self.env.render()[0].cpu().numpy()

    def close(self):
        self.env.close()



================================================
FILE: pufferlib/environments/mani_skill/torch.py
================================================
import numpy as np

import torch
import torch.nn as nn

import pufferlib
from pufferlib.models import Default as Policy
from pufferlib.models import LSTMWrapper as Recurrent

class FakePolicy(nn.Module):
    '''Default PyTorch policy. Flattens obs and applies a linear layer.

    PufferLib is not a framework. It does not enforce a base class.
    You can use any PyTorch policy that returns actions and values.
    We structure our forward methods as encode_observations and decode_actions
    to make it easier to wrap policies with LSTMs. You can do that and use
    our LSTM wrapper or implement your own. To port an existing policy
    for use with our LSTM wrapper, simply put everything from forward() before
    the recurrent cell into encode_observations and put everything after
    into decode_actions.
    '''
    def __init__(self, env, hidden_size=256):
        super().__init__()
        self.hidden_size = hidden_size

        n_obs = np.prod(env.single_observation_space.shape)
        n_atn = env.single_action_space.shape[0]
        self.decoder_mean = nn.Sequential(
            pufferlib.pytorch.layer_init(nn.Linear(n_obs, 256)),
            nn.Tanh(),
            pufferlib.pytorch.layer_init(nn.Linear(256, 256)),
            nn.Tanh(),
            pufferlib.pytorch.layer_init(nn.Linear(256, 256)),
            nn.Tanh(),
            pufferlib.pytorch.layer_init(nn.Linear(256, n_atn), std=0.01),
        )
        self.decoder_logstd = nn.Parameter(torch.zeros(
            1, env.single_action_space.shape[0]))

        self.value = nn.Sequential(
            pufferlib.pytorch.layer_init(nn.Linear(n_obs, 256)),
            nn.Tanh(),
            pufferlib.pytorch.layer_init(nn.Linear(256, 256)),
            nn.Tanh(),
            pufferlib.pytorch.layer_init(nn.Linear(256, 256)),
            nn.Tanh(),
            pufferlib.pytorch.layer_init(nn.Linear(256, 1), std=1),
        )
 
    def forward_eval(self, observations, state=None):
        hidden = self.encode_observations(observations, state=state)
        logits, values = self.decode_actions(hidden)
        return logits, values

    def forward(self, observations, state=None):
        return self.forward_eval(observations, state)

    def encode_observations(self, observations, state=None):
        '''Encodes a batch of observations into hidden states. Assumes
        no time dimension (handled by LSTM wrappers).'''
        return observations

    def decode_actions(self, hidden):
        '''Decodes a batch of hidden states into (multi)discrete actions.
        Assumes no time dimension (handled by LSTM wrappers).'''
        mean = self.decoder_mean(hidden)
        logstd = self.decoder_logstd.expand_as(mean)
        std = torch.exp(logstd)
        logits = torch.distributions.Normal(mean, std)
        values = self.value(hidden)
        return logits, values



================================================
FILE: pufferlib/environments/metta/__init__.py
================================================
from .environment import env_creator, make

try:
    import torch
except ImportError:
    pass
else:
    from .torch import Policy
    try:
        from .torch import Recurrent
    except:
        Recurrent = None



================================================
FILE: pufferlib/environments/metta/environment.py
================================================
import functools
import numpy as np
import gymnasium

import pufferlib

from omegaconf import OmegaConf
from metta.mettagrid.mettagrid_env import MettaGridEnv
from metta.mettagrid.curriculum.core import SingleTaskCurriculum
from metta.mettagrid.replay_writer import ReplayWriter

def env_creator(name='metta'):
    return functools.partial(make, name)

def make(name, config='pufferlib/environments/metta/metta.yaml', render_mode='auto', buf=None, seed=0,
         ore_reward=0.17088483842567775, battery_reward=0.9882859711234822, heart_reward=1.0):
    '''Metta creation function'''
    
    OmegaConf.register_new_resolver("div", oc_divide, replace=True)
    cfg = OmegaConf.load(config)
    
    # Update rewards under the new structure: agent.rewards.inventory
    inventory_rewards = cfg['game']['agent']['rewards']['inventory']
    inventory_rewards['ore_red'] = float(ore_reward)
    inventory_rewards['heart'] = float(heart_reward)
    inventory_rewards['battery_red'] = float(battery_reward)
    
    curriculum = SingleTaskCurriculum('puffer', cfg)
    return MettaPuff(curriculum, render_mode=render_mode, buf=buf, seed=seed)

def oc_divide(a, b):
    """
    Divide a by b, returning an int if both inputs are ints and result is a whole number,
    otherwise return a float.
    """
    result = a / b
    # If both inputs are integers and the result is a whole number, return as int
    if isinstance(a, int) and isinstance(b, int) and result.is_integer():
        return int(result)
    return result

class MettaPuff(MettaGridEnv):
    def __init__(self, curriculum, render_mode='human', buf=None, seed=0):
        self.replay_writer = None
        #if render_mode == 'auto':
        #    self.replay_writer = ReplayWriter("metta/")

        super().__init__(
            curriculum=curriculum,
            render_mode=render_mode,
            buf=buf,
            replay_writer=self.replay_writer
        )
        self.action_space = pufferlib.spaces.joint_space(self.single_action_space, self.num_agents)
        self.actions = self.actions.astype(np.int32)

    @property
    def single_action_space(self):
        return gymnasium.spaces.MultiDiscrete(super().single_action_space.nvec, dtype=np.int32)

    def step(self, actions):
        obs, rew, term, trunc, info = super().step(actions)

        if all(term) or all(trunc):
            self.reset()
            if 'agent_raw' in info:
                del info['agent_raw']
            if 'episode_rewards' in info:
                info['score'] = info['episode_rewards']

        else:
            info = []

        return obs, rew, term, trunc, [info]



================================================
FILE: pufferlib/environments/metta/metta.yaml
================================================
name: "GDY-MettaGrid"

report_stats_interval: 100
normalize_rewards: false

sampling: 0
desync_episodes: true

game:
  # Required list of inventory items
  inventory_item_names:
    - ore_red
    - ore_blue
    - ore_green
    - battery_red
    - battery_blue
    - battery_green
    - heart
    - armor
    - laser
    - blueprint

  num_agents: 64
  obs_width: 11
  obs_height: 11
  num_observation_tokens: 200
  max_steps: 1000

  # Global observation tokens configuration
  global_obs:
    episode_completion_pct: true
    last_action: true
    last_reward: true
    resource_rewards: false

  # Show recipe inputs in observations for all converters
  recipe_details_obs: false

  agent:
    default_resource_limit: 10
    resource_limits:
      heart: 255
    freeze_duration: 10
    action_failure_penalty: 0.0
    
    rewards:
      inventory:
        heart: 1
      stats: {}

  groups:
    agent:
      id: 0
      sprite: 0
      props: {}

  objects:
    altar:
      type_id: 8
      input_resources:
        battery_red: 3
      output_resources:
        heart: 1
      max_output: 5
      conversion_ticks: 1
      cooldown: 10
      initial_resource_count: 0

    wall:
      type_id: 1
      swappable: false

    block:
      type_id: 14
      swappable: true

    mine_red:
      type_id: 2
      output_resources:
        ore_red: 1
      color: 0
      max_output: 5
      conversion_ticks: 1
      cooldown: 50
      initial_resource_count: 0

    mine_blue:
      type_id: 3
      color: 1
      output_resources:
        ore_blue: 1
      max_output: 5
      conversion_ticks: 1
      cooldown: 50
      initial_resource_count: 0

    mine_green:
      type_id: 4
      output_resources:
        ore_green: 1
      color: 2
      max_output: 5
      conversion_ticks: 1
      cooldown: 50
      initial_resource_count: 0

    generator_red:
      type_id: 5
      input_resources:
        ore_red: 1
      output_resources:
        battery_red: 1
      color: 0
      max_output: 5
      conversion_ticks: 1
      cooldown: 25
      initial_resource_count: 0

    generator_blue:
      type_id: 6
      input_resources:
        ore_blue: 1
      output_resources:
        battery_blue: 1
      color: 1
      max_output: 5
      conversion_ticks: 1
      cooldown: 25
      initial_resource_count: 0

    generator_green:
      type_id: 7
      input_resources:
        ore_green: 1
      output_resources:
        battery_green: 1
      color: 2
      max_output: 5
      conversion_ticks: 1
      cooldown: 25
      initial_resource_count: 0

    armory:
      type_id: 9
      input_resources:
        ore_red: 3
      output_resources:
        armor: 1
      max_output: 5
      conversion_ticks: 1
      cooldown: 10
      initial_resource_count: 0

    lasery:
      type_id: 10
      input_resources:
        ore_red: 1
        battery_red: 2
      output_resources:
        laser: 1
      max_output: 5
      conversion_ticks: 1
      cooldown: 10
      initial_resource_count: 0

    lab:
      type_id: 11
      input_resources:
        ore_red: 3
        battery_red: 3
      output_resources:
        blueprint: 1
      max_output: 5
      conversion_ticks: 1
      cooldown: 5
      initial_resource_count: 0

    factory:
      type_id: 12
      input_resources:
        blueprint: 1
        ore_red: 5
        battery_red: 5
      output_resources:
        armor: 5
        laser: 5
      max_output: 5
      conversion_ticks: 1
      cooldown: 5
      initial_resource_count: 0

    temple:
      type_id: 13
      input_resources:
        heart: 1
        blueprint: 1
      output_resources:
        heart: 5
      max_output: 5
      conversion_ticks: 1
      cooldown: 5
      initial_resource_count: 0

  actions:
    noop:
      enabled: true
    move:
      enabled: true
    rotate:
      enabled: true
    put_items:
      enabled: true
    get_items:
      enabled: true
    attack:
      enabled: true
      consumed_resources:
        laser: 1
      defense_resources:
        armor: 1
    swap:
      enabled: true
    change_color:
      enabled: false
    change_glyph:
      enabled: false
      number_of_glyphs: 4

  map_builder:
    _target_: metta.mettagrid.room.multi_room.MultiRoom
    num_rooms: 1
    border_width: 6

    room:
      _target_: metta.mettagrid.room.random.Random
      width: 64
      height: 64
      border_width: 0

      agents: 64

      objects:
        mine_red: 128
        generator_red: 64
        altar: 32
        armory: 0
        lasery: 0
        lab: 0
        factory: 0
        temple: 0
        wall: 0



================================================
FILE: pufferlib/environments/metta/torch.py
================================================
import numpy as np
import einops
import torch
from torch import nn
from torch.nn import functional as F

import pufferlib.models

class Recurrent(pufferlib.models.LSTMWrapper):
    def __init__(self, env, policy, input_size=512, hidden_size=512):
        super().__init__(env, policy, input_size, hidden_size)

class Policy(nn.Module):
    def __init__(self, env, cnn_channels=128, hidden_size=512, **kwargs):
        super().__init__()
        self.hidden_size = hidden_size
        self.is_continuous = False

        self.out_width = 11
        self.out_height = 11
        self.num_layers = 22

        self.network= nn.Sequential(
            pufferlib.pytorch.layer_init(
                nn.Conv2d(self.num_layers, cnn_channels, 5, stride=3)),
            nn.ReLU(),
            pufferlib.pytorch.layer_init(
                nn.Conv2d(cnn_channels, cnn_channels, 3, stride=1)),
            nn.ReLU(),
            nn.Flatten(),
            pufferlib.pytorch.layer_init(nn.Linear(cnn_channels, hidden_size//2)),
            nn.ReLU(),
        )

        self.self_encoder = nn.Sequential(
            pufferlib.pytorch.layer_init(nn.Linear(self.num_layers, hidden_size//2)),
            nn.ReLU(),
        )

        #max_vec = torch.tensor([  1.,   9.,   1.,  30.,   1.,   3., 255.,  26.,   1.,   1.,   1.,   1.,
        #  1.,  47.,   3.,   3.,   2.,   1.,   1.,   1.,   1., 1.])[None, :, None, None]
        max_vec = torch.tensor([9., 1., 1., 10., 3., 254., 1., 1., 235., 8., 9., 250., 29., 1., 1., 8., 1., 1., 6., 3., 1., 2.])[None, :, None, None]
        #max_vec = torch.ones(22)[None, :, None, None]
        self.register_buffer('max_vec', max_vec)

        action_nvec = env.single_action_space.nvec
        self.actor = nn.ModuleList([pufferlib.pytorch.layer_init(
            nn.Linear(hidden_size, n), std=0.01) for n in action_nvec])

        self.value = pufferlib.pytorch.layer_init(
            nn.Linear(hidden_size, 1), std=1)

    def forward(self, observations, state=None):
        hidden, lookup = self.encode_observations(observations)
        actions, value = self.decode_actions(hidden, lookup)
        return (actions, value), hidden

    def encode_observations(self, observations, state=None):

        token_observations = observations
        B = token_observations.shape[0]
        TT = 1
        if token_observations.dim() != 3:  # hardcoding for shape [B, M, 3]
            TT = token_observations.shape[1]
            token_observations = einops.rearrange(token_observations, "b t m c -> (b t) m c")

        assert token_observations.shape[-1] == 3, f"Expected 3 channels per token. Got shape {token_observations.shape}"
        token_observations[token_observations == 255] = 0

        # coords_byte contains x and y coordinates in a single byte (first 4 bits are x, last 4 bits are y)
        coords_byte = token_observations[..., 0].to(torch.uint8)

        # Extract x and y coordinate indices (0-15 range, but we need to make them long for indexing)
        x_coord_indices = ((coords_byte >> 4) & 0x0F).long()  # Shape: [B_TT, M]
        y_coord_indices = (coords_byte & 0x0F).long()  # Shape: [B_TT, M]
        atr_indices = token_observations[..., 1].long()  # Shape: [B_TT, M], ready for embedding
        atr_values = token_observations[..., 2].float()  # Shape: [B_TT, M]

        # In ObservationShaper we permute. Here, we create the observations pre-permuted.
        # We'd like to pre-create this as part of initialization, but we don't know the batch size or time steps at
        # that point.
        box_obs = torch.zeros(
            (B * TT, 22, self.out_width, self.out_height),
            dtype=atr_values.dtype,
            device=token_observations.device,
        )
        batch_indices = torch.arange(B * TT, device=token_observations.device).unsqueeze(-1).expand_as(atr_values)

        # Add bounds checking to prevent out-of-bounds access
        valid_tokens = coords_byte != 0xFF
        valid_tokens = valid_tokens & (x_coord_indices < self.out_width) & (y_coord_indices < self.out_height)
        valid_tokens = valid_tokens & (atr_indices < 22)  # Also check attribute indices
        
        box_obs[
            batch_indices[valid_tokens],
            atr_indices[valid_tokens],
            x_coord_indices[valid_tokens],
            y_coord_indices[valid_tokens],
        ] = atr_values[valid_tokens]

        observations = box_obs

        #max_vec = box_obs.max(0)[0].max(1)[0].max(1)[0]
        #self.max_vec = torch.maximum(self.max_vec, max_vec[None, :, None, None])
        #if (np.random.rand() < 0.001):
        #    breakpoint()

        features = observations / self.max_vec
        #mmax = features.max(0)[0].max(1)[0].max(1)[0]
        #self.max_vec = torch.maximum(self.max_vec, mmax[None, :, None, None])
        self_features = self.self_encoder(features[:, :, 5, 5])
        cnn_features = self.network(features)
        return torch.cat([self_features, cnn_features], dim=1)

    def decode_actions(self, hidden):
        #hidden = self.layer_norm(hidden)
        logits = [dec(hidden) for dec in self.actor]
        value = self.value(hidden)
        return logits, value



================================================
FILE: pufferlib/environments/microrts/__init__.py
================================================
from .environment import env_creator, make

try:
    import torch
except ImportError:
    pass
else:
    from .torch import Policy
    try:
        from .torch import Recurrent
    except:
        Recurrent = None



================================================
FILE: pufferlib/environments/microrts/environment.py
================================================
from pdb import set_trace as T
import numpy as np

import warnings
import shimmy
import functools

import pufferlib.emulation
import pufferlib.environments


def env_creator(name='GlobalAgentCombinedRewardEnv'):
    return functools.partial(make, name)

def make(name, buf=None):
    '''Gym MicroRTS creation function
    
    This library appears broken. Step crashes in Java.
    '''
    pufferlib.environments.try_import('gym_microrts')
    if name == 'GlobalAgentCombinedRewardEnv':
        from gym_microrts.envs import GlobalAgentCombinedRewardEnv
    else:
        raise ValueError(f'Unknown environment: {name}')

    with pufferlib.utils.Suppress():
        return GlobalAgentCombinedRewardEnv()

    env.reset = pufferlib.utils.silence_warnings(env.reset)
    env.step = pufferlib.utils.silence_warnings(env.step)

    env = MicroRTS(env)
    env = shimmy.GymV21CompatibilityV0(env=env)
    return pufferlib.emulation.GymnasiumPufferEnv(env=env, buf=buf)

class MicroRTS:
    def __init__(self, env):
        self.env = env
        self.observation_space = self.env.observation_space
        self.action_space = self.env.action_space
        self.render = self.env.render
        self.close = self.env.close
        self.seed = self.env.seed

    def reset(self):
        return self.env.reset().astype(np.int32)

    def step(self, action):
        o, r, d, i = self.env.step(action)
        return o.astype(np.int32), r, d, i



================================================
FILE: pufferlib/environments/microrts/torch.py
================================================
from pufferlib.models import Default as Policy



================================================
FILE: pufferlib/environments/minerl/__init__.py
================================================
from .environment import env_creator, make

try:
    import torch
except ImportError:
    pass
else:
    from .torch import Policy
    try:
        from .torch import Recurrent
    except:
        Recurrent = None



================================================
FILE: pufferlib/environments/minerl/environment.py
================================================
from pdb import set_trace as T

import gym
import shimmy
import functools

import pufferlib
import pufferlib.emulation
import pufferlib.environments
import pufferlib.utils


def env_creator(name='MineRLBasaltFindCave-v0'):
    return functools.partial(make, name=name)

def make(name, buf=None):
    '''Minecraft environment creation function'''

    pufferlib.environments.try_import('minerl')

    # Monkey patch to add .itmes to old gym.spaces.Dict
    #gym.spaces.Dict.items = lambda self: self.spaces.items()

    #with pufferlib.utils.Suppress():
    env = gym.make(name)

    env = shimmy.GymV21CompatibilityV0(env=env)
    return pufferlib.emulation.GymnasiumPufferEnv(env, buf=buf)



================================================
FILE: pufferlib/environments/minerl/torch.py
================================================
from pufferlib.models import Default as Policy



================================================
FILE: pufferlib/environments/minigrid/__init__.py
================================================
from .environment import env_creator, make

try:
    import torch
except ImportError:
    pass
else:
    from .torch import Policy
    try:
        from .torch import Recurrent
    except:
        Recurrent = None



================================================
FILE: pufferlib/environments/minigrid/environment.py
================================================
from pdb import set_trace as T

import gymnasium
import functools

import pufferlib.emulation
import pufferlib.environments

ALIASES = {
    'minigrid': 'MiniGrid-LavaGapS7-v0',
}


def env_creator(name='minigrid'):
    return functools.partial(make, name=name)

def make(name, render_mode='rgb_array', buf=None, seed=0):
    if name in ALIASES:
        name = ALIASES[name]

    minigrid = pufferlib.environments.try_import('minigrid')
    env = gymnasium.make(name, render_mode=render_mode)
    env = MiniGridWrapper(env)
    env = pufferlib.EpisodeStats(env)
    return pufferlib.emulation.GymnasiumPufferEnv(env=env, buf=buf)

class MiniGridWrapper:
    def __init__(self, env):
        self.env = env
        self.observation_space = gymnasium.spaces.Dict({
            k: v for k, v in self.env.observation_space.items() if
            k != 'mission'
        })
        self.action_space = self.env.action_space
        self.close = self.env.close
        self.render = self.env.render
        self.close = self.env.close
        self.render_mode = 'rgb_array'

    def reset(self, seed=None, options=None):
        self.tick = 0
        obs, info = self.env.reset(seed=seed)
        del obs['mission']
        return obs, info

    def step(self, action):
        obs, reward, done, truncated, info = self.env.step(action)
        del obs['mission']

        self.tick += 1
        if self.tick == 100:
            done = True

        return obs, reward, done, truncated, info



================================================
FILE: pufferlib/environments/minigrid/torch.py
================================================
from pufferlib.models import Default as Policy



================================================
FILE: pufferlib/environments/minihack/__init__.py
================================================
from .environment import env_creator, make

try:
    import torch
except ImportError:
    pass
else:
    from .torch import Policy
    try:
        from .torch import Recurrent
    except:
        Recurrent = None



================================================
FILE: pufferlib/environments/minihack/environment.py
================================================
from pdb import set_trace as T

import gym
import shimmy
import functools

import pufferlib
import pufferlib.emulation
import pufferlib.environments


EXTRA_OBS_KEYS = [
    'tty_chars',
    'tty_colors',
    'tty_cursor',
]

ALIASES = {
    'minihack': 'MiniHack-River-v0',
}

def env_creator(name='minihack'):
    return functools.partial(make, name)

def make(name, buf=None, seed=0):
    '''NetHack binding creation function'''
    if name in ALIASES:
        name = ALIASES[name]

    import minihack
    pufferlib.environments.try_import('minihack')
    obs_key = minihack.base.MH_DEFAULT_OBS_KEYS + EXTRA_OBS_KEYS
    env = gym.make(name, observation_keys=obs_key)
    env = shimmy.GymV21CompatibilityV0(env=env)
    env = MinihackWrapper(env)
    return pufferlib.emulation.GymnasiumPufferEnv(env=env, buf=buf)

class MinihackWrapper:
    def __init__(self, env):
        self.env = env
        self.observation_space = self.env.observation_space
        self.action_space = self.env.action_space
        self.close = self.env.close
        self.close = self.env.close
        self.render_mode = 'ansi'

    def reset(self, seed=None):
        obs, info = self.env.reset(seed=seed)
        self.obs = obs
        return obs, info

    def step(self, action):
        obs, reward, done, truncated, info = self.env.step(action)
        self.obs = obs
        return obs, reward, done, truncated, info

    def render(self):
        import nle
        chars = nle.nethack.tty_render(
            self.obs['tty_chars'], self.obs['tty_colors'], self.obs['tty_cursor'])
        return chars




================================================
FILE: pufferlib/environments/minihack/torch.py
================================================
from pdb import set_trace as T

import pufferlib.pytorch
from pufferlib.environments.nethack import Policy

class Recurrent(pufferlib.models.LSTMWrapper):
    def __init__(self, env, policy, input_size=512, hidden_size=512, num_layers=1):
        super().__init__(env, policy, input_size, hidden_size, num_layers)



================================================
FILE: pufferlib/environments/mujoco/__init__.py
================================================
from .environment import *

try:
    import torch
except ImportError:
    pass
else:
    from .torch import Policy
    try:
        from .torch import Recurrent
    except:
        Recurrent = None



================================================
FILE: pufferlib/environments/mujoco/environment.py
================================================

from pdb import set_trace as T

import functools

import numpy as np
import gymnasium

import pufferlib
import pufferlib.emulation
import pufferlib.environments


def single_env_creator(env_name, capture_video, gamma,
        run_name=None, idx=None, obs_norm=True, pufferl=False, render_mode='rgb_array', buf=None, seed=0):
    if capture_video and idx == 0:
        assert run_name is not None, "run_name must be specified when capturing videos"
        env = gymnasium.make(env_name, render_mode="rgb_array")
        env = gymnasium.wrappers.RecordVideo(env, f"videos/{run_name}")
    else:
        env = gymnasium.make(env_name, render_mode=render_mode)

    env = pufferlib.ClipAction(env)  # NOTE: this changed actions space
    env = pufferlib.EpisodeStats(env)

    if obs_norm:
        env = gymnasium.wrappers.NormalizeObservation(env)
        env = gymnasium.wrappers.TransformObservation(env, lambda obs: np.clip(obs, -10, 10), env.observation_space)

    env = gymnasium.wrappers.NormalizeReward(env, gamma=gamma)
    env = gymnasium.wrappers.TransformReward(env, lambda reward: np.clip(reward, -10, 10))

    if pufferl is True:
        env = pufferlib.emulation.GymnasiumPufferEnv(env=env, buf=buf)

    return env


def cleanrl_env_creator(env_name, run_name, capture_video, gamma, idx):
    kwargs = {
        "env_name": env_name,
        "run_name": run_name,
        "capture_video": capture_video,
        "gamma": gamma,
        "idx": idx,
        "pufferl": False,
    }
    return functools.partial(single_env_creator, **kwargs)


# Keep it simple for pufferl demo, for now
def env_creator(env_name="HalfCheetah-v4", gamma=0.99):
    default_kwargs = {
        "env_name": env_name,
        "capture_video": False,
        "gamma": gamma,
        "pufferl": True,
    }
    return functools.partial(single_env_creator, **default_kwargs)



================================================
FILE: pufferlib/environments/mujoco/torch.py
================================================

from pufferlib.models import LSTMWrapper as Recurrent
from pufferlib.models import Default as Policy



================================================
FILE: pufferlib/environments/nethack/__init__.py
================================================
from .environment import env_creator, make

try:
    import torch
except ImportError:
    pass
else:
    from .torch import Policy
    try:
        from .torch import Recurrent
    except:
        Recurrent = None



================================================
FILE: pufferlib/environments/nethack/environment.py
================================================
from pdb import set_trace as T

import shimmy
import gym
import functools

import pufferlib
import pufferlib.emulation
import pufferlib.environments
#from .wrapper import RenderCharImagesWithNumpyWrapper

# Copyright (c) Facebook, Inc. and its affiliates.
import enum
import logging
import os
import random
import sys
import tempfile
import time
import warnings
import weakref

import gymnasium as gym
import numpy as np

from nle import nethack

logger = logging.getLogger(__name__)

DUNGEON_SHAPE = nethack.DUNGEON_SHAPE


DEFAULT_MSG_PAD = 256
DEFAULT_INV_PAD = 55
DEFAULT_INVSTR_PAD = 80

ASCII_SPACE = ord(" ")
ASCII_y = ord("y")
ASCII_n = ord("n")
ASCII_ESC = nethack.C("[")

FULL_ACTIONS = nethack.USEFUL_ACTIONS

SKIP_EXCEPTIONS = (b"eat", b"attack", b"direction?", b"pray")

NLE_SPACE_ITEMS = (
    (
        "glyphs",
        gym.spaces.Box(
            low=0, high=nethack.MAX_GLYPH, **nethack.OBSERVATION_DESC["glyphs"]
        ),
    ),
    ("chars", gym.spaces.Box(low=0, high=255, **nethack.OBSERVATION_DESC["chars"])),
    ("colors", gym.spaces.Box(low=0, high=15, **nethack.OBSERVATION_DESC["colors"])),
    (
        "specials",
        gym.spaces.Box(low=0, high=255, **nethack.OBSERVATION_DESC["specials"]),
    ),
    (
        "blstats",
        gym.spaces.Box(
            low=np.iinfo(np.int32).min,
            high=np.iinfo(np.int32).max,
            **nethack.OBSERVATION_DESC["blstats"],
        ),
    ),
    (
        "message",
        gym.spaces.Box(
            low=np.iinfo(np.uint8).min,
            high=np.iinfo(np.uint8).max,
            **nethack.OBSERVATION_DESC["message"],
        ),
    ),
    (
        "program_state",
        gym.spaces.Box(
            low=np.iinfo(np.int32).min,
            high=np.iinfo(np.int32).max,
            **nethack.OBSERVATION_DESC["program_state"],
        ),
    ),
    (
        "internal",
        gym.spaces.Box(
            low=np.iinfo(np.int32).min,
            high=np.iinfo(np.int32).max,
            **nethack.OBSERVATION_DESC["internal"],
        ),
    ),
    (
        "inv_glyphs",
        gym.spaces.Box(
            low=0,
            high=nethack.MAX_GLYPH,
            **nethack.OBSERVATION_DESC["inv_glyphs"],
        ),
    ),
    (
        "inv_strs",
        gym.spaces.Box(low=0, high=255, **nethack.OBSERVATION_DESC["inv_strs"]),
    ),
    (
        "inv_letters",
        gym.spaces.Box(low=0, high=127, **nethack.OBSERVATION_DESC["inv_letters"]),
    ),
    (
        "inv_oclasses",
        gym.spaces.Box(
            low=0,
            high=nethack.MAXOCLASSES,
            **nethack.OBSERVATION_DESC["inv_oclasses"],
        ),
    ),
    (
        "screen_descriptions",
        gym.spaces.Box(
            low=0, high=127, **nethack.OBSERVATION_DESC["screen_descriptions"]
        ),
    ),
    (
        "tty_chars",
        gym.spaces.Box(low=0, high=255, **nethack.OBSERVATION_DESC["tty_chars"]),
    ),
    (
        "tty_colors",
        gym.spaces.Box(
            low=0,
            high=31,
            **nethack.OBSERVATION_DESC["tty_colors"],
        ),
    ),
    (
        "tty_cursor",
        gym.spaces.Box(low=0, high=255, **nethack.OBSERVATION_DESC["tty_cursor"]),
    ),
    (
        "misc",
        gym.spaces.Box(
            low=np.iinfo(np.int32).min,
            high=np.iinfo(np.int32).max,
            **nethack.OBSERVATION_DESC["misc"],
        ),
    ),
)


class NLE(gym.Env):
    """Standard NetHack Learning Environment.

    Implements a gym interface around `nethack.Nethack`.


    Examples:
        >>> env = NLE()
        >>> obs, reset_info = env.reset()
        >>> obs, reward, done, truncation, info = env.step(0)
        >>> env.render()
    """

    # Gymnasium expects an fps rate > 0 for render checks
    # but NetHack doesn't have any. Set it to 42, because
    # that is always the answer to life, the universe and
    # everything.
    metadata = {"render_modes": ["human", "ansi", "full"], "render_fps": 42}

    class StepStatus(enum.IntEnum):
        """Specifies the status of the terminal state.

        Note:
            User may redefine this class in subtasks to handle / categorize
            more terminal states.

            It is highly advised that, in such cases, the enums already defined
            in this object are replicated in some way. See `nle.env.tasks` for
            examples on how to do this right.
        """

        ABORTED = -1
        RUNNING = 0
        DEATH = 1

    def __init__(
        self,
        save_ttyrec_every=0,
        savedir=None,
        character="mon-hum-neu-mal",
        max_episode_steps=5000,
        observation_keys=(
            "glyphs",
            "chars",
            "colors",
            "specials",
            "blstats",
            "message",
            "inv_glyphs",
            "inv_strs",
            "inv_letters",
            "inv_oclasses",
            "screen_descriptions",
            "tty_chars",
            "tty_colors",
            "tty_cursor",
        ),
        actions=None,
        options=None,
        wizard=False,
        allow_all_yn_questions=False,
        allow_all_modes=False,
        spawn_monsters=True,
        render_mode="human",
    ):
        """Constructs a new NLE environment.

        Args:
            save_ttyrec_every: Integer, if 0, no ttyrecs (game recordings) will
                be saved. Otherwise, save a ttyrec every Nth episode.
            savedir (str or None): Path to save ttyrecs (game recordings) into,
                if save_ttyrec_every is nonzero. If nonempty string, interpreted
                as a path to a new or existing directory.
                If "" (empty string) or None, NLE choses a unique directory name.
            character (str): name of character. Defaults to "mon-hum-neu-mal".
            max_episode_steps (int): maximum amount of steps allowed before the
                game is forcefully quit. In such cases, ``info["end_status"]``
                will be equal to ``StepStatus.ABORTED``. Defaults to 5000.
            observation_keys (list): keys to use when creating the observation.
                Defaults to all.
            actions (list): list of actions. If None, the full action space will
                be used, i.e. ``nle.nethack.ACTIONS``. Defaults to None.
            options (list): list of game options to initialize Nethack. If None,
                Nethack will be initialized with the options found in
                ``nle.nethack.NETHACKOPTIONS`. Defaults to None.
            wizard (bool): activate wizard mode. Defaults to False.
            allow_all_yn_questions (bool):
                If set to True, no y/n questions in step() are declined.
                If set to False, only elements of SKIP_EXCEPTIONS are not declined.
                Defaults to False.
            allow_all_modes (bool):
                If set to True, do not decline menus, text input or auto 'MORE'.
                If set to False, only skip click through 'MORE' on death.
            spawn_monsters: If False, disables normal NetHack behavior to randomly
                create monsters.
            render_mode (str): mode used to render the screen. One of
                "human" | "ansi" | "full".
                Defaults to "human", i.e. what a human would see playing the game.
        """
        self.character = character
        self._max_episode_steps = max_episode_steps
        self._allow_all_yn_questions = allow_all_yn_questions
        self._allow_all_modes = allow_all_modes
        self._save_ttyrec_every = save_ttyrec_every
        self.render_mode = render_mode

        if actions is None:
            actions = FULL_ACTIONS
        self.actions = actions

        self.last_observation = ()

        try:
            if not save_ttyrec_every:
                self.savedir = None
            elif savedir:
                self.savedir = os.path.abspath(savedir)
                os.makedirs(self.savedir)
            else:  # Empty savedir: We create our unique savedir inside nle_data/.
                parent_dir = os.path.join(os.getcwd(), "nle_data")
                os.makedirs(parent_dir, exist_ok=True)
                self.savedir = tempfile.mkdtemp(
                    prefix=time.strftime("%Y%m%d-%H%M%S_"), dir=parent_dir
                )
        except FileExistsError:
            logger.info("Using existing savedir: %s", self.savedir)
        else:
            if self.savedir:
                logger.info("Created savedir: %s", self.savedir)
            else:
                logger.info("Not saving any NLE data.")

        self._observation_keys = list(observation_keys)

        if "internal" in self._observation_keys:
            logger.warn(
                "The 'internal' NLE observation was requested. "
                "This might contain data that shouldn't be available to agents."
            )

        # Observations we always need.
        for key in (
            "glyphs",
            "blstats",
            "tty_chars",
            "message",
            "program_state",
            "internal",
        ):
            if key not in self._observation_keys:
                self._observation_keys.append(key)

        self._glyph_index = self._observation_keys.index("glyphs")
        self._blstats_index = self._observation_keys.index("blstats")
        self._message_index = self._observation_keys.index("message")
        self._program_state_index = self._observation_keys.index("program_state")
        self._internal_index = self._observation_keys.index("internal")

        self._original_observation_keys = observation_keys
        self._original_indices = tuple(
            self._observation_keys.index(key) for key in observation_keys
        )
        self._info = {}

        if self.savedir:
            ttyrec_version = ".ttyrec%i.bz2" % nethack.TTYREC_VERSION
            ttyrec_prefix = "nle.%i.%%i" % os.getpid()
            self._ttyrec_pattern = os.path.join(
                self.savedir, ttyrec_prefix + ttyrec_version
            )
            ttyrec = self._ttyrec_pattern % 0
            # Create an xlogfile with the same format of name.
            scoreprefix = ttyrec.replace("0" + ttyrec_version, "")
        else:
            ttyrec = None
            scoreprefix = None

        self.nethack = nethack.Nethack(
            observation_keys=self._observation_keys,
            options=options,
            playername="Agent-" + self.character,
            ttyrec=ttyrec,
            wizard=wizard,
            spawn_monsters=spawn_monsters,
            scoreprefix=scoreprefix,
        )
        self._close_nethack = weakref.finalize(self, self.nethack.close)

        self._random = random.SystemRandom()

        # -1 so that it's 0-based on first reset
        self._episode = -1

        space_dict = dict(NLE_SPACE_ITEMS)
        self.observation_space = gym.spaces.Dict(
            {key: space_dict[key] for key in observation_keys}
        )

        self.action_space = gym.spaces.Discrete(len(self.actions))

    def _get_observation(self, observation):
        return {
            key: observation[i]
            for key, i in zip(self._original_observation_keys, self._original_indices)
        }

    def _get_end_status(self, observation, done):
        return done or self._check_abort(observation)

    def _get_information(self, end_status):
        info = {}
        info["end_status"] = end_status
        info["is_ascended"] = self.nethack.how_done() == nethack.ASCENDED
        return info

    def print_action_meanings(self):
        for a_idx, a in enumerate(self.actions):
            print(a_idx, a)

    def _check_abort(self, observation):
        return self._steps >= self._max_episode_steps

    def step(self, action: int):
        """Steps the environment.

        Args:
            action (int): action integer as defined by ``self.action_space``.

        Returns:
            (dict, float, bool, dict): a tuple containing
                - (*dict*): an observation of the state; this will contain the keys
                  specified by ``self.observation_space``.
                - (*float*): a reward; see ``self._reward_fn`` to see how it is
                  specified.
                - (*bool*): True if the state is terminal, False otherwise.
                - (*bool*): True if the episode is truncated, False otherwise.
                - (*dict*): a dictionary of extra information (such as
                  `end_status`, i.e. a status info -- death, task win, etc. --
                  for the terminal state).
        """
        # Careful: By default we re-use Numpy arrays, so copy before!
        old_score = self.last_observation[self._blstats_index][nethack.NLE_BL_SCORE]

        observation, done = self.nethack.step(self.actions[action])
        truncated = self._check_abort(observation)

        # is_game_over = observation[self._program_state_index][0] == 1
        # if is_game_over or not self._allow_all_modes:
        #     observation, done = self._perform_known_steps(
        #         observation, done, exceptions=True
        #     )

        self._steps += 1

        self.last_observation = observation

        end_status = self._get_end_status(observation, done)

        reward = float(
            self._reward_fn(old_score, action, observation, end_status)
        )

        if end_status and not done:
            # Try to end the game nicely.
            self._quit_game(observation, done)
            done = True

        return (
            self._get_observation(observation),
            reward,
            done,
            truncated,
            self._info,
        )

    def _in_moveloop(self, observation):
        program_state = observation[self._program_state_index]
        return program_state[3]  # in_moveloop

    def reset(self, seed=None, options=None):
        """Resets the environment.

        Note:
            We attempt to manually navigate the first few menus so that the
            first seen state is ready to be acted upon by the user. This might
            fail in case Nethack is initialized with some uncommon options.

        Returns:
            (tuple) (Observation of the state as
                defined by `self.observation_space`,
                Extra game state information)
        """
        super().reset(seed=seed, options=options)
        self._episode += 1
        if self.savedir and self._episode % self._save_ttyrec_every == 0:
            new_ttyrec = self._ttyrec_pattern % self._episode
        else:
            new_ttyrec = None
        self.last_observation = self.nethack.reset(new_ttyrec, options=options)

        self._steps = 0
        done = False

        for _ in range(1000):
            # Get past initial phase of game. This should make sure
            # all the observations are present.
            if self._in_moveloop(self.last_observation):
                break
            # This fails if the agent picks up a scroll of scare
            # monster at the 0th turn and gets asked to name it.
            # Hence the defensive iteration above.
            # TODO: Detect this 'in_getlin' situation and handle it.
            self.last_observation, done = self.nethack.step(ASCII_SPACE)
            assert not done, "Game ended unexpectedly"
        else:
            warnings.warn(
                "Not in moveloop after 1000 tries, aborting (ttyrec: %s)." % new_ttyrec,
                stacklevel=2,
            )
            return self.reset(seed=seed, options=options)

        return self._get_observation(self.last_observation), {}

    def close(self):
        self._close_nethack()
        super().close()

    def seed(self, core=None, disp=None, reseed=False, lgen=None):
        """Sets the state of the NetHack RNGs after the next reset.

        NetHack 3.6 uses two RNGs, core and disp. This is to prevent
        RNG-manipulation by e.g. running into walls or other no-ops on the
        actual game state. This is a measure against "tool-assisted
        speedruns" (TAS). NLE can run in both NetHack's default mode and in
        TAS-friendly "no reseeding" if `reseed` is set to False, see below.

        lgen allows the user to use a different RNG to generate the levels in
        NetHack so that the levels become fixed and independent from the in-game
        choices.

        Arguments:
            core [int or None]: Seed for the core RNG. If None, chose a random
                value.
            disp [int or None]: Seed for the disp (anti-TAS) RNG. If None, chose
                a random value.
            reseed [boolean]: As an Anti-TAS (automation) measure,
                NetHack 3.6 reseeds with true randomness every now and then. This
                flag enables or disables this behavior. If set to True, trajectories
                won't be reproducible.
            lgen [int or None]: Seed for the level generator, used for RNG when
                NetHack generates new levels.

        Returns:
            [tuple] The seeds supplied, in the form (core, disp, reseed, lgen).
        """
        if core is None:
            core = self._random.randrange(sys.maxsize)
        if disp is None:
            disp = self._random.randrange(sys.maxsize)
        self.nethack.set_initial_seeds(core, disp, reseed, lgen)
        return (core, disp, reseed, lgen)

    def get_seeds(self):
        """Returns current seeds.

        Returns:
            (tuple): Current NetHack (core, disp, reseed, lgen) state.
        """
        return self.nethack.get_current_seeds()

    def render(self):
        """Renders the state of the environment."""
        mode = self.render_mode

        if mode == "human":
            obs = self.last_observation
            tty_chars = obs[self._observation_keys.index("tty_chars")]
            tty_colors = obs[self._observation_keys.index("tty_colors")]
            tty_cursor = obs[self._observation_keys.index("tty_cursor")]
            print(nethack.tty_render(tty_chars, tty_colors, tty_cursor))
            return None

        if mode == "full":
            message_index = self._observation_keys.index("message")
            message = bytes(self.last_observation[message_index])
            print(message[: message.index(b"\0")])
            try:
                inv_strs_index = self._observation_keys.index("inv_strs")
                inv_letters_index = self._observation_keys.index("inv_letters")

                inv_strs = self.last_observation[inv_strs_index]
                inv_letters = self.last_observation[inv_letters_index]
                for letter, line in zip(inv_letters, inv_strs):
                    if np.all(line == 0):
                        break
                    print(
                        letter.tobytes().decode("utf-8"), line.tobytes().decode("utf-8")
                    )
            except ValueError:  # inv_strs/letters not used.
                pass

            chars = self.last_observation[self._observation_keys.index("chars")]
            colors = self.last_observation[self._observation_keys.index("colors")]
            print(nethack.tty_render(chars, colors))
            return None

        if mode in ("ansi", "string"):  # Misnomer: This is the least ANSI of them all.
            chars = self.last_observation[self._observation_keys.index("chars")]
            # TODO: Why return a string here but print in the other branches?
            return "\n".join([line.tobytes().decode("utf-8") for line in chars])

        return "\nInvalid render mode: " + mode

    def __repr__(self):
        return "<%s>" % self.__class__.__name__

    def _is_episode_end(self, observation):
        """Returns whether the episode has ended.

        Tasks may override this method to specify different conditions, so long
        as the return value has a well defined __int__ method (e.g. booleans,
        numerical types, enum.IntEnum) and that value is part of StepStatus.

        The return value will be stored into info["end_status"].
        """
        return self.StepStatus.RUNNING

    def _reward_fn(self, old_score, action, observation, end_status):
        """Reward function. Difference between previous score and new score."""

        if observation[self._blstats_index][0] == 0:
            # Before game started and after it ended blstats are zero.
            return 0.0

        score = observation[self._blstats_index][nethack.NLE_BL_SCORE]
        return score - old_score

    def _perform_known_steps(self, observation, done, exceptions=True):
        steps = 0
        while not done:
            steps += 1
            if observation[self._internal_index][3]:  # xwaitforspace
                observation, done = self.nethack.step(ASCII_SPACE)
                continue

            internal = observation[self._internal_index]
            in_yn_function = internal[1]
            in_getlin = internal[2]

            if in_getlin:  # Game asking for a line of text. We don't do that.
                observation, done = self.nethack.step(ASCII_ESC)
                continue

            if in_yn_function:  # Game asking for a single character.
                # Note: No auto-yes to final questions thanks to the disclose option.
                if exceptions:
                    # This causes an annoying unnecessary copy...
                    msg = bytes(observation[self._message_index])
                    # Do not skip some questions to allow agent to select
                    # stuff to eat, attack, and to select directions.

                    # do not skip if all allowed or the allowed message appears
                    if self._allow_all_yn_questions or any(
                        el in msg for el in SKIP_EXCEPTIONS
                    ):
                        break

                # Otherwise, auto-decline.
                observation, done = self.nethack.step(ASCII_ESC)

            break
        if steps > 10:
            print(steps, "steps to get out of menus and windows.")
        return observation, done

    def _quit_game(self, observation, done):
        """Smoothly quit a game."""
        # Get out of menus and windows.
        # observation, done = self._perform_known_steps(
        #     observation, done, exceptions=False
        # )

        if done:
            return

        # Quit the game.
        actions = [0x80 | ord("q"), ord("y")]  # M-q y
        for a in actions:
            observation, done = self.nethack.step(a)

        # Answer final questions.
        # observation, done = self._perform_known_steps(
        #     observation, done, exceptions=False
        # )

        if not done:
            # Somehow, the above logic failed us.
            warnings.warn(
                "Warning: smooth quitting of game failed, aborting.", stacklevel=2
            )

def env_creator(name='nethack'):
    return functools.partial(make, name)

def make(name, buf=None, seed=0):
    '''NetHack binding creation function'''
    if name == 'nethack':
        name = 'NetHackScore-v0'

    nle = pufferlib.environments.try_import('nle')
    from nle.env.tasks import NetHackScore
    #env = NetHackScore(observation_keys=['blstats', 'chars'])
    env = NLE(observation_keys=['blstats', 'chars'])
    #env = RenderCharImagesWithNumpyWrapper(env)
    #env = shimmy.GymV21CompatibilityV0(env=env)
    env = NethackWrapper(env)
    env = pufferlib.EpisodeStats(env)
    return pufferlib.emulation.GymnasiumPufferEnv(env=env, buf=buf)

class NethackWrapper:
    def __init__(self, env):
        self.env = env
        self.observation_space = self.env.observation_space
        self.action_space = self.env.action_space
        self.close = self.env.close
        self.close = self.env.close
        self.render_mode = 'ansi'

    def reset(self, seed=None):
        obs, info = self.env.reset(seed=seed)
        self.obs = obs
        return obs, info

    def step(self, action):
        obs, reward, done, truncated, info = self.env.step(action)
        self.obs = obs
        return obs, reward, done, truncated, info

    def render(self):
        import nle
        chars = nle.nethack.tty_render(
            self.obs['tty_chars'], self.obs['tty_colors'], self.obs['tty_cursor'])
        return chars



================================================
FILE: pufferlib/environments/nethack/torch.py
================================================
from pdb import set_trace as T

import torch
import torch.nn as nn
import torch.nn.functional as F

import pufferlib.models
import pufferlib.pytorch
from pufferlib.pytorch import layer_init

from pufferlib.models import LSTMWrapper as Recurrent
from pufferlib.models import Default as Policy


'''
class Recurrent(pufferlib.models.LSTMWrapper):
    def __init__(self, env, policy, input_size=256, hidden_size=256, num_layers=1):
        super().__init__(env, policy, input_size, hidden_size, num_layers)

class Policy(nn.Module):
    def __init__(self, env):
        super().__init__()
        self.dtype = pufferlib.pytorch.nativize_dtype(env.emulated)

        self.blstats_net = nn.Sequential(
            nn.Embedding(256, 32),
            nn.Flatten(),
        )

        self.char_embed = nn.Embedding(256, 32)
        self.chars_net = nn.Sequential(
            layer_init(nn.Conv2d(32, 32, 5, stride=(2, 3))),
            nn.ReLU(),
            layer_init(nn.Conv2d(32, 64, 5, stride=(1, 3))),
            nn.ReLU(),
            layer_init(nn.Conv2d(64, 64, 3, stride=1)),
            nn.ReLU(),
            nn.Flatten(),
        )

        self.proj = nn.Linear(864+960, 256)
        self.actor = layer_init(nn.Linear(256, 8), std=0.01)
        self.critic = layer_init(nn.Linear(256, 1), std=1)

    def forward_eval(self, x, state=None):
        hidden = self.encode_observations(x)
        actions, value = self.decode_actions(hidden)
        return actions, value

    def forward(self, x, state=None):
        hidden = self.encode_observations(x)
        actions, value = self.decode_actions(hidden)
        return actions, value

    def encode_observations(self, x):
        x = x.type(torch.uint8) # Undo bad cleanrl cast
        x = pufferlib.pytorch.nativize_tensor(x, self.dtype)

        blstats = torch.clip(x['blstats'] + 1, 0, 255).int()
        blstats = self.blstats_net(blstats)

        chars = self.char_embed(x['chars'].int())
        chars = torch.permute(chars, (0, 3, 1, 2))
        chars = self.chars_net(chars)

        concat = torch.cat([blstats, chars], dim=1)
        return self.proj(concat)

    def decode_actions(self, hidden):
        value = self.critic(hidden)
        action = self.actor(hidden)
        return action, value
'''



================================================
FILE: pufferlib/environments/nethack/wrapper.py
================================================
"""Taken & adapted from Chaos Dwarf in Nethack Challenge Starter Kit:
https://github.com/Miffyli/nle-sample-factory-baseline


MIT License

Copyright (c) 2021 Anssi

Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files (the "Software"), to deal
in the Software without restriction, including without limitation the rights
to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the Software, and to permit persons to whom the Software is
furnished to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in all
copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
SOFTWARE.
"""

import os

import cv2
import gym
import numpy as np
from numba import njit
from nle import nethack
from PIL import Image
from PIL import ImageDraw
from PIL import ImageFont

#import render_utils

SMALL_FONT_PATH = os.path.join(__package__.replace(".", "/"), "Hack-Regular.ttf")

# Mapping of 0-15 colors used.
# Taken from bottom image here. It seems about right
# https://i.stack.imgur.com/UQVe5.png
COLORS = [
    "#000000",
    "#800000",
    "#008000",
    "#808000",
    "#000080",
    "#800080",
    "#008080",
    "#808080",  # - flipped these ones around
    "#C0C0C0",  # | the gray-out dull stuff
    "#FF0000",
    "#00FF00",
    "#FFFF00",
    "#0000FF",
    "#FF00FF",
    "#00FFFF",
    "#FFFFFF",
]


@njit
def _tile_characters_to_image(
    out_image,
    chars,
    colors,
    output_height_chars,
    output_width_chars,
    char_array,
    offset_h,
    offset_w,
):
    """
    Build an image using cached images of characters in char_array to out_image
    """
    char_height = char_array.shape[3]
    char_width = char_array.shape[4]
    for h in range(output_height_chars):
        h_char = h + offset_h
        # Stuff outside boundaries is not visible, so
        # just leave it black
        if h_char < 0 or h_char >= chars.shape[0]:
            continue
        for w in range(output_width_chars):
            w_char = w + offset_w
            if w_char < 0 or w_char >= chars.shape[1]:
                continue
            char = chars[h_char, w_char]
            color = colors[h_char, w_char]
            h_pixel = h * char_height
            w_pixel = w * char_width
            out_image[
                :, h_pixel : h_pixel + char_height, w_pixel : w_pixel + char_width
            ] = char_array[char, color]


def _initialize_char_array(font_size, rescale_font_size):
    """Draw all characters in PIL and cache them in numpy arrays

    if rescale_font_size is given, assume it is (width, height)

    Returns a np array of (num_chars, num_colors, char_height, char_width, 3)
    """
    font = ImageFont.truetype(SMALL_FONT_PATH, font_size)
    dummy_text = "".join(
        [(chr(i) if chr(i).isprintable() else " ") for i in range(256)]
    )
    _, _, image_width, image_height = font.getbbox(dummy_text)
    # Above can not be trusted (or its siblings)....
    image_width = int(np.ceil(image_width / 256) * 256)

    char_width = rescale_font_size[0]
    char_height = rescale_font_size[1]

    char_array = np.zeros((256, 16, char_height, char_width, 3), dtype=np.uint8)
    image = Image.new("RGB", (image_width, image_height))
    image_draw = ImageDraw.Draw(image)
    for color_index in range(16):
        image_draw.rectangle((0, 0, image_width, image_height), fill=(0, 0, 0))
        image_draw.text((0, 0), dummy_text, fill=COLORS[color_index], spacing=0)

        arr = np.array(image).copy()
        arrs = np.array_split(arr, 256, axis=1)
        for char_index in range(256):
            char = arrs[char_index]
            if rescale_font_size:
                char = cv2.resize(char, rescale_font_size, interpolation=cv2.INTER_AREA)
            char_array[char_index, color_index] = char
    return char_array


class RenderCharImagesWithNumpyWrapper(gym.Wrapper):
    """
    Render characters as images, using PIL to render characters like we humans see on screen
    but then some caching and numpy stuff to speed up things.

    To speed things up, crop image around the player.
    """

    def __init__(
        self,
        env,
        font_size=9,
        crop_size=12,
        rescale_font_size=(6, 6),
        blstats_cursor=False,
    ):
        super().__init__(env)
        self.char_array = _initialize_char_array(font_size, rescale_font_size)
        self.char_height = self.char_array.shape[2]
        self.char_width = self.char_array.shape[3]
        # Transpose for CHW
        self.char_array = self.char_array.transpose(0, 1, 4, 2, 3)

        self.crop_size = crop_size
        self.blstats_cursor = blstats_cursor

        self.half_crop_size = crop_size // 2
        self.output_height_chars = crop_size
        self.output_width_chars = crop_size
        self.chw_image_shape = (
            3,
            self.output_height_chars * self.char_height,
            self.output_width_chars * self.char_width,
        )

        self.observation_space = gym.spaces.Box(
            low=0, high=255, shape=self.chw_image_shape, dtype=np.uint8
        )
 
        '''
        obs_spaces = {
            "screen_image": gym.spaces.Box(
                low=0, high=255, shape=self.chw_image_shape, dtype=np.uint8
            )
        }
        obs_spaces.update(
            [
                (k, self.env.observation_space[k])
                for k in self.env.observation_space
                if k not in ["tty_chars", "tty_colors"]
            ]
        )
        self.observation_space = gym.spaces.Dict(obs_spaces)
        '''

        self.render_mode = 'rgb_array'

    def _render_text_to_image(self, obs):
        chars = obs["tty_chars"]
        colors = obs["tty_colors"]
        offset_w = 0
        offset_h = 0
        if self.crop_size:
            # Center around player
            if self.blstats_cursor:
                center_x, center_y = obs["blstats"][:2]
            else:
                center_y, center_x = obs["tty_cursor"]
            offset_h = center_y - self.half_crop_size
            offset_w = center_x - self.half_crop_size

        out_image = np.zeros(self.chw_image_shape, dtype=np.uint8)

        _tile_characters_to_image(
            out_image=out_image,
            chars=chars,
            colors=colors,
            output_height_chars=self.output_height_chars,
            output_width_chars=self.output_width_chars,
            char_array=self.char_array,
            offset_h=offset_h,
            offset_w=offset_w,
        )

        return out_image
        obs["screen_image"] = out_image
        del obs["tty_chars"]
        del obs["tty_colors"]
        return obs

    def step(self, action):
        obs, reward, done, info = self.env.step(action)
        self.obs = obs
        obs = self._render_text_to_image(obs)
        return obs, reward, done, info

    def reset(self):
        obs = self.env.reset()
        self.obs = obs
        obs = self._render_text_to_image(obs)
        return obs

    def render(self, mode='rgb_array'):
        return self.obs


class RenderCharImagesWithNumpyWrapperV2(gym.Wrapper):
    """
    Same as V1, but simpler and faster.
    """

    def __init__(
        self,
        env,
        font_size=9,
        crop_size=12,
        rescale_font_size=(6, 6),
    ):
        super().__init__(env)
        self.char_array = _initialize_char_array(font_size, rescale_font_size)
        self.char_height = self.char_array.shape[2]
        self.char_width = self.char_array.shape[3]
        # Transpose for CHW
        self.char_array = self.char_array.transpose(0, 1, 4, 2, 3)
        self.char_array = np.ascontiguousarray(self.char_array)
        self.crop_size = crop_size

        crop_rows = crop_size or nethack.nethack.TERMINAL_SHAPE[0]
        crop_cols = crop_size or nethack.nethack.TERMINAL_SHAPE[1]

        self.chw_image_shape = (
            3,
            crop_rows * self.char_height,
            crop_cols * self.char_width,
        )

        obs_spaces = {
            "screen_image": gym.spaces.Box(
                low=0, high=255, shape=self.chw_image_shape, dtype=np.uint8
            )
        }
        obs_spaces.update(
            [
                (k, self.env.observation_space[k])
                for k in self.env.observation_space
                if k not in ["tty_chars", "tty_colors"]
            ]
        )
        self.observation_space = gym.spaces.Dict(obs_spaces)

    def _populate_obs(self, obs):
        screen = np.zeros(self.chw_image_shape, order="C", dtype=np.uint8)
        render_utils.render_crop(
            obs["tty_chars"],
            obs["tty_colors"],
            obs["tty_cursor"],
            self.char_array,
            screen,
            crop_size=self.crop_size,
        )
        obs["screen_image"] = screen

    def step(self, action):
        obs, reward, done, info = self.env.step(action)
        self._populate_obs(obs)
        return obs, reward, done, info

    def reset(self):
        obs = self.env.reset()
        self._populate_obs(obs)
        return obs



================================================
FILE: pufferlib/environments/nmmo/__init__.py
================================================
from .environment import env_creator, make

try:
    import torch
except ImportError:
    pass
else:
    from .torch import Policy
    try:
        from .torch import Recurrent
    except:
        Recurrent = None



================================================
FILE: pufferlib/environments/nmmo/environment.py
================================================
from pdb import set_trace as T
import numpy as np
import functools

import pufferlib
import pufferlib.emulation
import pufferlib.environments
import pufferlib.wrappers
import pufferlib.postprocess


def env_creator(name='nmmo'):
    return functools.partial(make, name)

def make(name, *args, buf=None, **kwargs):
    '''Neural MMO creation function'''
    nmmo = pufferlib.environments.try_import('nmmo')
    env = nmmo.Env(*args, **kwargs)
    env = NMMOWrapper(env)
    env = pufferlib.postprocess.MultiagentEpisodeStats(env)
    env = pufferlib.postprocess.MeanOverAgents(env)
    return pufferlib.emulation.PettingZooPufferEnv(env=env, buf=buf)

class NMMOWrapper(pufferlib.postprocess.PettingZooWrapper):
    '''Remove task spam'''
    @property
    def render_mode(self):
        return 'rgb_array'
    
    def render(self):
        '''Quick little renderer for NMMO'''
        tiles = self.env.tile_map[:, :, 2].astype(np.uint8)
        render = np.zeros((tiles.shape[0], tiles.shape[1], 3), dtype=np.uint8)
        BROWN = (136, 69, 19)
        render[tiles == 1] = (0, 0, 255)
        render[tiles == 2] = (0, 255, 0)
        render[tiles == 3] = BROWN
        render[tiles == 4] = (64, 255, 64)
        render[tiles == 5] = (128, 128, 128)
        render[tiles == 6] = BROWN
        render[tiles == 7] = (255, 128, 128)
        render[tiles == 8] = BROWN
        render[tiles == 9] = (128, 255, 128)
        render[tiles == 10] = BROWN
        render[tiles == 11] = (128, 128, 255)
        render[tiles == 12] = BROWN
        render[tiles == 13] = (192, 255, 192)
        render[tiles == 14] = (0, 0, 255)
        render[tiles == 15] = (64, 64, 255)

        for agent in self.env.realm.players.values():
            agent_r = agent.row.val
            agent_c = agent.col.val
            render[agent_r, agent_c, :] = (255, 255, 0)

        for npc in self.env.realm.npcs.values():
            agent_r = npc.row.val
            agent_c = npc.col.val
            render[agent_r, agent_c, :] = (255, 0, 0)

        return render

    def reset(self, seed=None):
        obs, infos = self.env.reset(seed=seed)
        self.obs = obs
        return obs, infos

    def step(self, actions):
        obs, rewards, dones, truncateds, infos = self.env.step(actions)
        infos = {k: list(v['task'].values())[0] for k, v in infos.items()}
        self.obs = obs
        return obs, rewards, dones, truncateds, infos

    def close(self):
        return self.env.close()

    



================================================
FILE: pufferlib/environments/nmmo/torch.py
================================================
from pdb import set_trace as T

import torch
import torch.nn.functional as F

import pufferlib
import pufferlib.emulation
import pufferlib.models
import pufferlib.pytorch
from pufferlib.environments import try_import

try_import("nmmo")
from nmmo.entity.entity import EntityState


class Recurrent(pufferlib.models.LSTMWrapper):
    def __init__(self, env, policy, input_size=256, hidden_size=256, num_layers=1):
        super().__init__(env, policy, input_size, hidden_size, num_layers)

class Policy(torch.nn.Module):
  NUM_ATTRS = 34
  EntityId = EntityState.State.attr_name_to_col["id"]
  tile_offset = torch.tensor([i*256 for i in range(3)])
  entity_offset = torch.tensor([i*256 for i in range(3, 34)])

  def __init__(self, env, input_size=256, hidden_size=256, output_size=256):
      super().__init__()
      self.dtype = pufferlib.pytorch.nativize_dtype(env.emulated)

      # A dumb example encoder that applies a linear layer to agent self features
      self.embedding = torch.nn.Embedding(self.NUM_ATTRS*256, 32)
      self.tile_conv_1 = torch.nn.Conv2d(96, 32, 3)
      self.tile_conv_2 = torch.nn.Conv2d(32, 8, 3)
      self.tile_fc = torch.nn.Linear(8*11*11, input_size)

      self.entity_fc = torch.nn.Linear(31*32, input_size)

      self.proj_fc = torch.nn.Linear(2*input_size, input_size)

      self.decoders = torch.nn.ModuleList([torch.nn.Linear(hidden_size, n)
              for n in env.single_action_space.nvec])
      self.value_head = torch.nn.Linear(hidden_size, 1)

  def forward(self, x):
      hidden, lookup = self.encode_observations(x)
      actions, value = self.decode_actions(hidden, lookup)
      return actions, value

  def encode_observations(self, env_outputs):
    env_outputs = pufferlib.pytorch.nativize_tensor(env_outputs, self.dtype)

    tile = env_outputs['Tile']
    # Center on player
    # This is cursed without clone??
    tile[:, :, :2] -= tile[:, 112:113, :2].clone() 
    tile[:, :, :2] += 7
    tile = self.embedding(
        tile.long().clip(0, 255) + self.tile_offset.to(tile.device)
    )

    agents, tiles, features, embed = tile.shape
    tile = tile.view(agents, tiles, features*embed).transpose(1, 2).view(agents, features*embed, 15, 15)

    tile = self.tile_conv_1(tile)
    tile = F.relu(tile)
    tile = self.tile_conv_2(tile)
    tile = F.relu(tile)
    tile = tile.contiguous().view(agents, -1)
    tile = self.tile_fc(tile)
    tile = F.relu(tile)

    # Pull out rows corresponding to the agent
    agentEmb = env_outputs["Entity"]
    my_id = env_outputs["AgentId"][:,0]
    entity_ids = agentEmb[:,:,self.EntityId]
    mask = (entity_ids == my_id.unsqueeze(1)) & (entity_ids != 0)
    mask = mask.int()
    row_indices = torch.where(mask.any(dim=1), mask.argmax(dim=1), torch.zeros_like(mask.sum(dim=1)))
    entity = agentEmb[torch.arange(agentEmb.shape[0]), row_indices]

    entity = self.embedding(
        entity.long().clip(0, 255) + self.entity_offset.to(entity.device)
    )
    agents, attrs, embed = entity.shape
    entity = entity.view(agents, attrs*embed)

    entity = self.entity_fc(entity)
    entity = F.relu(entity)

    obs = torch.cat([tile, entity], dim=-1)
    return self.proj_fc(obs), None

  def decode_actions(self, hidden, lookup, concat=True):
      value = self.value_head(hidden)
      actions = [dec(hidden) for dec in self.decoders]
      return actions, value



================================================
FILE: pufferlib/environments/open_spiel/__init__.py
================================================
from .environment import env_creator, make

try:
    import torch
except ImportError:
    pass
else:
    from .torch import Policy
    try:
        from .torch import Recurrent
    except:
        Recurrent = None



================================================
FILE: pufferlib/environments/open_spiel/environment.py
================================================
from pdb import set_trace as T
import numpy as np
import functools

import pufferlib
from pufferlib import namespace
import pufferlib.emulation
import pufferlib.environments


def env_creator(name='connect_four'):
    '''OpenSpiel creation function'''
    return functools.partial(make, name)

def make(
        name,
        multiplayer=False,
        n_rollouts=5,
        max_simulations=10,
        min_simulations=None,
        buf=None
    ):
    '''OpenSpiel creation function'''
    pyspiel = pufferlib.environments.try_import('pyspiel', 'open_spiel')
    env = pyspiel.load_game(name)

    if min_simulations is None:
        min_simulations = max_simulations

    from pufferlib.environments.open_spiel.gymnasium_environment import (
        OpenSpielGymnasiumEnvironment
    )
    from pufferlib.environments.open_spiel.pettingzoo_environment import (
        OpenSpielPettingZooEnvironment
    )

    kwargs = dict(
        env=env,
        n_rollouts=int(n_rollouts),
        min_simulations=int(min_simulations),
        max_simulations=int(max_simulations),
    )
 
    if multiplayer:
        env = OpenSpielPettingZooEnvironment(**kwargs)
        wrapper_cls = pufferlib.emulation.PettingZooPufferEnv
    else:
        env = OpenSpielGymnasiumEnvironment(**kwargs)
        wrapper_cls = pufferlib.emulation.GymnasiumPufferEnv

    return wrapper_cls(
        env=env,
        postprocessor_cls=pufferlib.emulation.BasicPostprocessor,
        buf=buf,
    )




================================================
FILE: pufferlib/environments/open_spiel/gymnasium_environment.py
================================================
from pdb import set_trace as T
import numpy as np

from open_spiel.python.algorithms import mcts

import pufferlib
from pufferlib import namespace
from pufferlib.environments.open_spiel.utils import (
    solve_chance_nodes,
    get_obs_and_infos,
    observation_space,
    action_space,
    init,
    render,
    close,
)


def create_bots(state, seed):
    assert seed is not None, 'seed must be set'
    rnd_state = np.random.RandomState(seed)

    evaluator = mcts.RandomRolloutEvaluator(
        n_rollouts=state.n_rollouts,
        random_state=rnd_state
    )

    return [mcts.MCTSBot(
        game=state.env,
        uct_c=2,
        max_simulations=a,
        evaluator=evaluator,
        random_state=rnd_state, 
        child_selection_fn=mcts.SearchNode.puct_value,
        solve=True,
    ) for a in range(state.min_simulations, state.max_simulations + 1)]
    
def reset(state, seed = None, options = None):
    state.state = state.env.new_initial_state()

    if not state.has_reset:
        state.has_reset = True
        state.seed_value = seed
        np.random.seed(seed)
        state.all_bots = create_bots(state, seed)

    state.bot = np.random.choice(state.all_bots)

    if np.random.rand() < 0.5:
        bot_atn = state.bot.step(state.state)
        state.state.apply_action(bot_atn)
    
    obs, infos = get_obs_and_infos(state)
    player = state.state.current_player()
    return obs[player], infos[player]

def step(state, action):
    player = state.state.current_player()
    solve_chance_nodes(state)
    state.state.apply_action(action)

    # Take other move with a bot
    if not state.state.is_terminal():
        bot_atn = state.bot.step(state.state)
        solve_chance_nodes(state)
        state.state.apply_action(bot_atn)

    # Now that we have applied all actions, get the next obs.
    obs, all_infos = get_obs_and_infos(state)
    reward = state.state.returns()[player]
    info = all_infos[player]

    # Are we done?
    terminated = state.state.is_terminal()
    if terminated:
        key = f'win_mcts_{state.bot.max_simulations}'
        info[key] = int(reward==1)

    return obs[player], reward, terminated, False, info

class OpenSpielGymnasiumEnvironment:
    __init__ = init
    step = step
    reset = reset
    observation_space = property(observation_space)
    action_space = property(action_space)
    render = render
    close = close



================================================
FILE: pufferlib/environments/open_spiel/pettingzoo_environment.py
================================================
from pdb import set_trace as T
import numpy as np

import pufferlib
from pufferlib import namespace

from pufferlib.environments.open_spiel.utils import (
    solve_chance_nodes,
    get_obs_and_infos,
    observation_space,
    action_space,
    init,
    render,
    close,
)

def agents(state):
    return state.agents

def possible_agents(state):
    return list(range(state.env.num_players()))

def pz_observation_space(state, agent):
    return observation_space(state)

def pz_action_space(state, agent):
    return action_space(state)

def reset(state, seed = None, options = None):
    state.state = state.env.new_initial_state()
    obs, infos = get_obs_and_infos(state)
    state.agents = state.possible_agents

    if not state.has_reset:
        state.has_reset = True
        state.seed_value = seed
        np.random.seed(seed)

    return obs, infos

def step(state, actions):
    curr_player = state.state.current_player()
    solve_chance_nodes(state)
    state.state.apply_action(actions[curr_player])
    obs, infos = get_obs_and_infos(state)
    rewards = {ag: r for ag, r in enumerate(state.state.returns())}

    # Are we done?
    is_terminated = state.state.is_terminal()
    terminateds = {a: False for a in obs}
    truncateds = {a: False for a in obs}

    if is_terminated:
        terminateds = {a: True for a in state.possible_agents}
        state.agents = []

    return obs, rewards, terminateds, truncateds, infos

class OpenSpielPettingZooEnvironment:
    __init__ = init
    step = step
    reset = reset
    agents = lambda state: state.agents
    possible_agents = property(possible_agents)
    observation_space = pz_observation_space
    action_space = pz_action_space
    render = render
    close = close



================================================
FILE: pufferlib/environments/open_spiel/torch.py
================================================
from pdb import set_trace as T
import numpy as np

import torch
from torch import nn

import pufferlib.emulation
from pufferlib.models import Policy as Base

class Policy(Base):
    def __init__(self, env, input_size=128, hidden_size=128):
        '''Default PyTorch policy, meant for debugging.
        This should run with any environment but is unlikely to learn anything.
        
        Uses a single linear layer + relu to encode observations and a list of
        linear layers to decode actions. The value function is a single linear layer.
        '''
        super().__init__(env)

        self.flat_observation_space = env.flat_observation_space
        self.flat_observation_structure = env.flat_observation_structure

        self.encoder = nn.Linear(np.prod(
            env.structured_observation_space['obs'].shape), hidden_size)
        self.decoder = nn.Linear(hidden_size, self.action_space.n)

        self.value_head = nn.Linear(hidden_size, 1)

    def encode_observations(self, observations):
        '''Linear encoder function'''
        observations = pufferlib.emulation.unpack_batched_obs(observations,
            self.flat_observation_space, self.flat_observation_structure)
        obs = observations['obs'].view(observations['obs'].shape[0], -1)
        self.action_mask = observations['action_mask']

        hidden = torch.relu(self.encoder(obs))
        return hidden, None

    def decode_actions(self, hidden, lookup, concat=True):
        '''Concatenated linear decoder function'''
        value = self.value_head(hidden)
        action = self.decoder(hidden)
        action = action.masked_fill(self.action_mask == 0, -1e9)
        return action, value


================================================
FILE: pufferlib/environments/open_spiel/utils.py
================================================
from pdb import set_trace as T
import numpy as np

import gymnasium

from pufferlib import namespace


def init(self, 
    env,
    n_rollouts,
    min_simulations,
    max_simulations
    ):
    #state.num_agents = state.env.num_players()
    return namespace(self,
        env=env,
        type=env.get_type(),
        n_rollouts=n_rollouts,
        min_simulations=min_simulations,
        max_simulations=max_simulations,
        state=None,
        agents=[],
        has_reset=False,
    )

def observation_space(state):
    return gymnasium.spaces.Dict({
        'obs': gymnasium.spaces.Box(
            low=0.0,
            high=1.0,
            shape=(state.env.observation_tensor_size(),),
            dtype=np.float32,
        ),
        'action_mask': gymnasium.spaces.Box(
            low=0,
            high=1,
            shape=(action_space(state).n,),
            dtype=np.int8
        )
    })

def action_space(state):
    return gymnasium.spaces.Discrete(
        state.env.num_distinct_actions())

def render(state, mode=None) -> None:
    if mode == "human":
        print(state.state)

def close(state):
    pass

def act(state, action):
    solve_chance_nodes(state)
    state.state.apply_action(action)

def get_obs_and_infos(state):
    # Before calculating an observation, there could be chance nodes
    # (that may have an effect on the actual observations).
    # E.g. After reset, figure out initial (random) positions of the
    # agents.
    solve_chance_nodes(state)

    if state.state.is_terminal():
        return (
            state.last_obs, 
            {player: {} for player in range(state.env.num_players())},
        )

    # Sequential game:
    curr_player = state.state.current_player()
    mask = state.state.legal_actions(curr_player)
    np_mask = np.zeros(action_space(state).n)
    np_mask[mask] = 1

    state.last_obs = {player: {
        'obs': np.reshape(state.state.observation_tensor(),
            [-1]).astype(np.float32),
        'action_mask': np_mask.astype(np.int8),
    } for player in range(state.env.num_players())}

    state.last_info = {curr_player: {}}

    return (
        {curr_player: state.last_obs[curr_player]},
        state.last_info,
    )

def solve_chance_nodes(state):
    # Before applying action(s), there could be chance nodes.
    # E.g. if env has to figure out, which agent's action should get
    # resolved first in a simultaneous node.
    # Chance node(s): Sample a (non-player) action and apply.
    while state.state.is_chance_node():
        assert state.state.current_player() == -1
        actions, probs = zip(*state.state.chance_outcomes())
        action = np.random.choice(actions, p=probs)
        state.state.apply_action(action)



================================================
FILE: pufferlib/environments/pokemon_red/__init__.py
================================================
from .environment import env_creator, make

try:
    import torch
except ImportError:
    pass
else:
    from .torch import Policy
    try:
        from .torch import Recurrent
    except:
        Recurrent = None



================================================
FILE: pufferlib/environments/pokemon_red/environment.py
================================================
from pdb import set_trace as T

import gymnasium
import functools

from pokegym import Environment

import pufferlib.emulation
import pufferlib.postprocess


def env_creator(name='pokemon_red'):
    return functools.partial(make, name)

def make(name, headless: bool = True, state_path=None, buf=None, seed=0):
    '''Pokemon Red'''
    env = Environment(headless=headless, state_path=state_path)
    env = RenderWrapper(env)
    env = pufferlib.postprocess.EpisodeStats(env)
    return pufferlib.emulation.GymnasiumPufferEnv(env=env, buf=buf, seed=seed)

class RenderWrapper(gymnasium.Wrapper):
    def __init__(self, env):
        self.env = env

    @property
    def render_mode(self):
        return 'rgb_array'

    def render(self):
        return self.env.screen.screen_ndarray()



================================================
FILE: pufferlib/environments/pokemon_red/torch.py
================================================
from functools import partial
import torch

import pufferlib.models


class Recurrent(pufferlib.models.LSTMWrapper):
    def __init__(self, env, policy,
            input_size=512, hidden_size=512, num_layers=1):
        super().__init__(env, policy,
            input_size, hidden_size, num_layers)

class Policy(pufferlib.models.Convolutional):
    def __init__(self, env,
            input_size=512, hidden_size=512, output_size=512,
            framestack=4, flat_size=64*5*6):
        super().__init__(
            env=env,
            input_size=input_size,
            hidden_size=hidden_size,
            output_size=output_size,
            framestack=framestack,
            flat_size=flat_size,
            channels_last=True,
        )


'''
class Policy(pufferlib.models.ProcgenResnet):
    def __init__(self, env, cnn_width=16, mlp_width=512):
        super().__init__(
            env=env,
            cnn_width=cnn_width,
            mlp_width=mlp_width,
        )
'''



================================================
FILE: pufferlib/environments/procgen/__init__.py
================================================
from .environment import env_creator, make

try:
    import torch
except ImportError:
    pass
else:
    from .torch import Policy
    try:
        from .torch import Recurrent
    except:
        Recurrent = None



================================================
FILE: pufferlib/environments/procgen/environment.py
================================================
from pdb import set_trace as T
import numpy as np

import gym
import gymnasium
import shimmy
import functools

import pufferlib
import pufferlib.emulation
import pufferlib.environments

from stable_baselines3.common.atari_wrappers import (
    MaxAndSkipEnv,
)

def env_creator(name='bigfish'):
    return functools.partial(make, name)

def make(name, num_envs=1, num_levels=0, start_level=0,
        distribution_mode='easy', render_mode=None, buf=None, seed=0):
    '''Atari creation function with default CleanRL preprocessing based on Stable Baselines3 wrappers'''
    assert int(num_envs) == float(num_envs), "num_envs must be an integer"
    num_envs = int(num_envs)

    procgen = pufferlib.environments.try_import('procgen') 
    envs = procgen.ProcgenEnv(
        env_name=name,
        num_envs=num_envs,
        num_levels=num_levels,
        start_level=start_level,
        distribution_mode=distribution_mode,
        render_mode=render_mode,
    )
    envs = gym.wrappers.TransformObservation(envs, lambda obs: obs["rgb"])
    envs.single_action_space = envs.action_space
    envs.single_observation_space = envs.observation_space["rgb"]
    envs.is_vector_env = True
    envs = gym.wrappers.RecordEpisodeStatistics(envs)
    envs = gym.wrappers.NormalizeReward(envs)
    envs = gym.wrappers.TransformReward(envs, lambda reward: np.clip(reward, -10, 10))
    assert isinstance(envs.single_action_space, gym.spaces.Discrete), "only discrete action space is supported"
    envs = ProcgenWrapper(envs)
    envs = shimmy.GymV21CompatibilityV0(env=envs, render_mode=render_mode)
    #envs = gymnasium.wrappers.GrayScaleObservation(envs)
    #envs = gymnasium.wrappers.FrameStack(envs, 4)#, framestack)
    #envs = MaxAndSkipEnv(envs, skip=2)
    envs = pufferlib.EpisodeStats(envs)
    return pufferlib.emulation.GymnasiumPufferEnv(env=envs, buf=buf)

class ProcgenWrapper:
    def __init__(self, env):
        self.env = env
        self.observation_space = self.env.observation_space['rgb']
        self.action_space = self.env.action_space

    @property
    def render_mode(self):
        return 'rgb_array'

    def reset(self, seed=None):
        obs = self.env.reset()[0]
        return obs

    def render(self, mode=None):
        return self.env.env.env.env.env.env.get_info()[0]['rgb']

    def close(self):
        return self.env.close()

    def step(self, actions):
        actions = np.asarray(actions).reshape(1)
        obs, rewards, dones, infos = self.env.step(actions)
        return obs[0], rewards[0], dones[0], infos[0]



================================================
FILE: pufferlib/environments/procgen/torch.py
================================================
from pdb import set_trace as T
from torch import nn
import pufferlib.models

# This policy ended up being useful broadly
# so I included it in the defaults

class Recurrent(pufferlib.models.LSTMWrapper):
    def __init__(self, env, policy, input_size=256, hidden_size=256, num_layers=1):
        super().__init__(env, policy, input_size, hidden_size, num_layers)

class Policy (nn.Module):
    def __init__(self, env, *args, input_size=256, hidden_size=256,
            output_size=256, **kwargs):
        '''The CleanRL default NatureCNN policy used for Atari.
        It's just a stack of three convolutions followed by a linear layer
        
        Takes framestack as a mandatory keyword argument. Suggested default is 1 frame
        with LSTM or 4 frames without.'''
        super().__init__()

        self.network= nn.Sequential(
            pufferlib.pytorch.layer_init(nn.Conv2d(3, 16, 8, stride=4)),
            nn.ReLU(),
            pufferlib.pytorch.layer_init(nn.Conv2d(16, 32, 4, stride=2)),
            nn.ReLU(),
            nn.Flatten(),
            pufferlib.pytorch.layer_init(nn.Linear(1152, hidden_size)),
            nn.ReLU(),
        )
        self.actor = pufferlib.pytorch.layer_init(
            nn.Linear(hidden_size, env.single_action_space.n), std=0.01)
        self.value_fn = pufferlib.pytorch.layer_init(
            nn.Linear(output_size, 1), std=1)

    def forward(self, observations):
        hidden, lookup = self.encode_observations(observations)
        actions, value = self.decode_actions(hidden, lookup)
        return actions, value

    def encode_observations(self, observations):
        observations = observations.permute(0, 3, 1, 2)
        return self.network(observations.float() / 255.0), None

    def decode_actions(self, flat_hidden, lookup, concat=None):
        action = self.actor(flat_hidden)
        value = self.value_fn(flat_hidden)
        return action, value

Policy = pufferlib.models.ProcgenResnet



================================================
FILE: pufferlib/environments/slimevolley/__init__.py
================================================
from .environment import env_creator

try:
    import torch
except ImportError:
    pass
else:
    from .torch import Policy
    try:
        from .torch import Recurrent
    except:
        Recurrent = None



================================================
FILE: pufferlib/environments/slimevolley/environment.py
================================================
from pdb import set_trace as T
import numpy as np
import functools

import gym
import shimmy

import pufferlib
import pufferlib.emulation
import pufferlib.environments
import pufferlib.utils
import pufferlib.postprocess


def env_creator(name='SlimeVolley-v0'):
    return functools.partial(make, name)

def make(name, render_mode='rgb_array', buf=None):
    if name == 'slimevolley':
        name = 'SlimeVolley-v0'

    from slimevolleygym import SlimeVolleyEnv
    SlimeVolleyEnv.atari_mode = True
    env = SlimeVolleyEnv()
    env.policy.predict = lambda obs: np.random.randint(0, 2, 3)
    env = SlimeVolleyMultiDiscrete(env)
    env = SkipWrapper(env, repeat_count=4)
    env = shimmy.GymV21CompatibilityV0(env=env)
    env = pufferlib.postprocess.EpisodeStats(env)
    return pufferlib.emulation.GymnasiumPufferEnv(env=env, buf=buf)

class SlimeVolleyMultiDiscrete(gym.Wrapper):
    def __init__(self, env):
        super().__init__(env)
        #self.action_space = gym.spaces.MultiDiscrete(
        #    [2 for _ in range(env.action_space.n)])

    def reset(self, seed=None):
        return self.env.reset().astype(np.float32)

    def step(self, action):
        obs, reward, done, info = self.env.step(action)
        return obs.astype(np.float32), reward, done, info

class SkipWrapper(gym.Wrapper):
    """
        Generic common frame skipping wrapper
        Will perform action for `x` additional steps
    """
    def __init__(self, env, repeat_count):
        super(SkipWrapper, self).__init__(env)
        self.repeat_count = repeat_count
        self.stepcount = 0

    def step(self, action):
        done = False
        total_reward = 0
        current_step = 0
        while current_step < (self.repeat_count + 1) and not done:
            self.stepcount += 1
            obs, reward, done, info = self.env.step(action)
            total_reward += reward
            current_step += 1

        return obs, total_reward, done, info

    def reset(self):
        self.stepcount = 0
        return self.env.reset()




================================================
FILE: pufferlib/environments/slimevolley/torch.py
================================================
import pufferlib.models

Recurrent = pufferlib.models.LSTMWrapper
Policy = pufferlib.models.Default



================================================
FILE: pufferlib/environments/smac/__init__.py
================================================
from .environment import env_creator, make

try:
    import torch
except ImportError:
    pass
else:
    from .torch import Policy
    try:
        from .torch import Recurrent
    except:
        Recurrent = None



================================================
FILE: pufferlib/environments/smac/environment.py
================================================
import functools

import pufferlib
import pufferlib.emulation
import pufferlib.environments
import pufferlib.wrappers


def env_creator(name='smac'):
    return functools.partial(make, name)

def make(name, buf=None):
    '''Starcraft Multiagent Challenge creation function

    Support for SMAC is WIP because environments do not function without
    an action-masked baseline policy.'''
    pufferlib.environments.try_import('smac')
    from smac.env.pettingzoo.StarCraft2PZEnv import _parallel_env as smac_env

    env = smac_env(1000)
    env = pufferlib.wrappers.PettingZooTruncatedWrapper(env)
    env = pufferlib.emulation.PettingZooPufferEnv(env, buf=buf)
    return env



================================================
FILE: pufferlib/environments/smac/torch.py
================================================
from pufferlib.models import Default as Policy



================================================
FILE: pufferlib/environments/stable_retro/__init__.py
================================================
from .environment import env_creator, make

try:
    import torch
except ImportError:
    pass
else:
    from .torch import Policy
    try:
        from .torch import Recurrent
    except:
        Recurrent = None



================================================
FILE: pufferlib/environments/stable_retro/environment.py
================================================
from pdb import set_trace as T
import numpy as np

import gymnasium as gym
import functools

import pufferlib
import pufferlib.emulation
import pufferlib.environments


def env_creator(name='Airstriker-Genesis'):
    return functools.partial(make, name)

def make(name='Airstriker-Genesis', framestack=4, buf=None):
    '''Atari creation function with default CleanRL preprocessing based on Stable Baselines3 wrappers'''
    retro = pufferlib.environments.try_import('retro', 'stable-retro')

    from stable_baselines3.common.atari_wrappers import (
        ClipRewardEnv,
        EpisodicLifeEnv,
        FireResetEnv,
        MaxAndSkipEnv,
    )
    with pufferlib.utils.Suppress():
        env = retro.make(name)

    env = gym.wrappers.RecordEpisodeStatistics(env)
    env = MaxAndSkipEnv(env, skip=4)
    env = ClipRewardEnv(env)
    env = gym.wrappers.ResizeObservation(env, (84, 84))
    env = gym.wrappers.GrayScaleObservation(env)
    env = gym.wrappers.FrameStack(env, framestack)
    return pufferlib.emulation.GymnasiumPufferEnv(
        env=env, postprocessor_cls=AtariFeaturizer, buf=buf)

class AtariFeaturizer(pufferlib.emulation.Postprocessor):
    def reset(self, obs):
        self.epoch_return = 0
        self.epoch_length = 0
        self.done = False

    #@property
    #def observation_space(self):
    #    return gym.spaces.Box(0, 255, (1, 84, 84), dtype=np.uint8)

    def observation(self, obs):
        return np.array(obs)
        return np.array(obs[1], dtype=np.float32)

    def reward_done_truncated_info(self, reward, done, truncated, info):
        return reward, done, truncated, info
        if 'lives' in info:
            if info['lives'] == 0 and done:
                info['return'] = info['episode']['r']
                info['length'] = info['episode']['l']
                info['time'] = info['episode']['t']
                return reward, True, info
            return reward, False, info

        if self.done:
            return reward, done, info

        if done:
            info['return'] = self.epoch_return
            info['length'] = self.epoch_length
            self.done = True
        else:
            self.epoch_length += 1
            self.epoch_return += reward

        return reward, done, info



================================================
FILE: pufferlib/environments/stable_retro/torch.py
================================================
import pufferlib.models


class Recurrent:
    input_size = 512
    hidden_size = 512
    num_layers = 1

class Policy(pufferlib.models.Convolutional):
    def __init__(self, env, input_size=512, hidden_size=512, output_size=512,
            framestack=4, flat_size=64*7*7):
        super().__init__(
            env=env,
            input_size=input_size,
            hidden_size=hidden_size,
            output_size=output_size,
            framestack=framestack,
            flat_size=flat_size,
        )



================================================
FILE: pufferlib/environments/test/__init__.py
================================================
from .environment import (
    GymnasiumPerformanceEnv,
    PettingZooPerformanceEnv,
    GymnasiumTestEnv, 
    PettingZooTestEnv,
    make_all_mock_environments,
    MOCK_OBSERVATION_SPACES,
    MOCK_ACTION_SPACES,
)

from .mock_environments import MOCK_SINGLE_AGENT_ENVIRONMENTS
from .mock_environments import MOCK_MULTI_AGENT_ENVIRONMENTS

try:
    import torch
except ImportError:
    pass
else:
    from .torch import Policy
    try:
        from .torch import Recurrent
    except:
        Recurrent = None



================================================
FILE: pufferlib/environments/test/environment.py
================================================
from pdb import set_trace as T
import numpy as np

import time
import hashlib

import gym
import gymnasium
from gymnasium.spaces import Box, Discrete, Dict, Tuple
from pufferlib import spaces
from pettingzoo.utils.env import ParallelEnv

import pufferlib
import pufferlib.emulation
import pufferlib.utils


HIGH = 100
LOW = 0

MOCK_OBSERVATION_SPACES = [
    # Atari space
    Box(low=0, high=255, shape=(4, 84, 84), dtype=np.uint8),

    # NetHack space
    Dict({
        'blstats': Box(-2147483648, 2147483647, (27,), 'int64'),
        'chars': Box(0, 255, (21, 79), 'uint8'),
        'colors': Box(0, 15, (21, 79), 'uint8'),
        'glyphs': Box(0, 5976, (21, 79), 'int16'),
        'inv_glyphs': Box(0, 5976, (55,), 'int16'),
        'inv_letters': Box(0, 127, (55,), 'uint8'),
        'inv_oclasses': Box(0, 18, (55,), 'uint8'),
        'inv_strs': Box(0, 255, (55, 80), 'uint8'),
        'message': Box(0, 255, (256,), 'uint8'),
        'screen_descriptions': Box(0, 127, (21, 79, 80), 'uint8'),
        'specials': Box(0, 255, (21, 79), 'uint8'),
        'tty_chars': Box(0, 255, (24, 80), 'uint8'),
        'tty_colors': Box(0, 31, (24, 80), 'int8'),
        'tty_cursor': Box(0, 255, (2,), 'uint8'),
    }),
    
    # Neural MMO space
    Dict({
        'ActionTargets': Dict({
            'Attack': Dict({
                'Style': Box(0, 1, (3,), 'int8'),
                'Target': Box(0, 1, (100,), 'int8'),
            }),
            'Buy': Dict({
                'MarketItem': Box(0, 1, (1024,), 'int8'),
            }),
            'Comm': Dict({
                'Token': Box(0, 1, (50,), 'int8'),
            }),
            'Destroy': Dict({
                'InventoryItem': Box(0, 1, (12,), 'int8'),
            }),
            'Give': Dict({
                'InventoryItem': Box(0, 1, (12,), 'int8'),
                'Target': Box(0, 1, (100,), 'int8'),
            }),
            'GiveGold': Dict({
                'Price': Box(0, 1, (99,), 'int8'),
                'Target': Box(0, 1, (100,), 'int8'),
            }),
            'Move': Dict({
                'Direction': Box(0, 1, (5,), 'int8'),
            }),
            'Sell': Dict({
                'InventoryItem': Box(0, 1, (12,), 'int8'),
                'Price': Box(0, 1, (99,), 'int8'),
            }),
            'Use': Dict({
                'InventoryItem': Box(0, 1, (12,), 'int8'),
            })
        }),
        'AgentId': Discrete(129),
        'CurrentTick': Discrete(1025),
        'Entity': Box(-32768, 32767, (100, 23), 'int16'),
        'Inventory': Box(-32768, 32767, (12, 16), 'int16'),
        'Market': Box(-32768, 32767, (1024, 16), 'int16'),
        'Task': Box(-32770.0, 32770.0, (1024,), 'float16'),
        'Tile': Box(-32768, 32767, (225, 3), 'int16'),
    }),

    # Simple spaces
    Discrete(5),
    Box(low=LOW, high=HIGH, shape=(4,), dtype=np.float32),

    # Nested spaces
    Dict({
        "foo": Box(low=LOW, high=HIGH, shape=(2,), dtype=np.float32),
        "bar": Box(low=LOW, high=HIGH, shape=(2,), dtype=np.float32),
    }),
    Tuple((Discrete(3), Discrete(4))),
    Tuple((
        Box(low=LOW, high=HIGH, shape=(2,), dtype=np.float32),
        Discrete(3),
        Dict({
            "baz": Box(low=LOW, high=HIGH, shape=(1,), dtype=np.float32),
            "qux": Box(low=LOW, high=HIGH, shape=(1,), dtype=np.float32),
        }),
    )),
    Dict({
        "foo": Tuple((
            Box(low=LOW, high=HIGH, shape=(2,), dtype=np.float32),
            Discrete(3),
        )),
        "bar": Dict({
            "baz": Discrete(2),
            "qux": Discrete(4),
        }),
    }),
]

MOCK_ACTION_SPACES = [
    # NetHack action space
    Discrete(5),

    # Neural MMO action space
    Dict({
        'Attack': Dict({
            'Style': Discrete(3),
            'Target': Discrete(100),
        }),
        'Buy': Dict({
            'MarketItem': Discrete(1024),
        }),
        'Comm': Dict({
            'Token': Discrete(50),
        }),
        'Destroy': Dict({
            'InventoryItem': Discrete(12),
        }),
        'Give': Dict({
            'InventoryItem': Discrete(12),
            'Target': Discrete(100),
        }),
        'GiveGold': Dict({
            'Price': Discrete(99),
            'Target': Discrete(100),
        }),
        'Move': Dict({
            'Direction': Discrete(5),
        }),
        'Sell': Dict({
            'InventoryItem': Discrete(12),
            'Price': Discrete(99),
        }),
        'Use': Dict({
            'InventoryItem': Discrete(12),
        })
    }),

    # Nested spaces
    Tuple((Discrete(2), Discrete(3))),
    Dict({
        "foo": Discrete(4),
        "bar": Discrete(2),
    }),
    Tuple((
        Discrete(4),
        Dict({
            "baz": Discrete(2),
            "qux": Discrete(2),
        }),
    )),
    Dict({
        "foo": Tuple((
            Discrete(2),
            Discrete(3),
        )),
        "bar": Dict({
            "baz": Discrete(2),
            "qux": Discrete(4),
        }),
    }),
]

MOCK_TEAMS = {
    'None': None,
    'single': {
        'team_1': ['agent_1'],
        'team_2': ['agent_2'],
        'team_3': ['agent_3'],
        'team_4': ['agent_4'],
        'team_5': ['agent_5'],
        'team_6': ['agent_6'],
        'team_7': ['agent_7'],
        'team_8': ['agent_8'],
        'team_9': ['agent_9'],
        'team_10': ['agent_10'],
        'team_11': ['agent_11'],
        'team_12': ['agent_12'],
        'team_13': ['agent_13'],
        'team_14': ['agent_14'],
        'team_15': ['agent_15'],
        'team_16': ['agent_16'],
    },
    'pairs': {
        'team_1': ['agent_1', 'agent_2'],
        'team_2': ['agent_3', 'agent_4'],
        'team_3': ['agent_5', 'agent_6'],
        'team_4': ['agent_7', 'agent_8'],
        'team_5': ['agent_9', 'agent_10'],
        'team_6': ['agent_11', 'agent_12'],
        'team_7': ['agent_13', 'agent_14'],
        'team_8': ['agent_15', 'agent_16'],
    },
    'mixed': {
        'team_1': ['agent_1', 'agent_2'],
        'team_2': ['agent_3', 'agent_4', 'agent_5', 'agent_6'],
        'team_3': ['agent_7', 'agent_8', 'agent_9'],
        'team_4': ['agent_10', 'agent_11', 'agent_12', 'agent_13', 'agent_14'],
        'team_5': ['agent_15', 'agent_16'],
    },
}

DEFAULT_OBSERVATION_SPACE = gymnasium.spaces.Box(
    low=-2**20, high=2**20,
    shape=(1,), dtype=np.float32
)
DEFAULT_ACTION_SPACE = gymnasium.spaces.Discrete(2)
 

def make_all_mock_environments():
    mock_single_agent_environments = []
    mock_multi_agent_environments = []
    for obs_space in MOCK_OBSERVATION_SPACES:
        for act_space in MOCK_ACTION_SPACES:
            mock_single_agent_environments.append(
                GymnasiumTestEnv(
                    observation_space=obs_space,
                    action_space=act_space,
                )
            )
     
            mock_multi_agent_environments.append(
                PettingZooTestEnv(
                    observation_space=obs_space,
                    action_space=act_space,
                    initial_agents=16,
                    max_agents=16,
                    spawn_per_tick=0,
                    death_per_tick=1,
                )
            )
    return mock_single_agent_environments, mock_multi_agent_environments

def do_work(delay_mean, delay_std):
    start, idx = time.process_time(), 0
    target_time = delay_mean + delay_std*np.random.randn()
    while time.process_time() - start < target_time:
        idx += 1
    return

class GymnasiumPerformanceEnv:
    def __init__(self, delay_mean=0, delay_std=0):
        self.observation_space = DEFAULT_OBSERVATION_SPACE
        self.action_space = DEFAULT_ACTION_SPACE
        self.observation = self.observation_space.sample()

        self.delay_mean = delay_mean
        self.delay_std = delay_std

        # Test performance independent of PufferLib seeding
        np.random.seed(time.time_ns() % 2**32)

    def reset(self, seed=None):
        return self.observation, {}

    def step(self, action):
        do_work(self.delay_mean, self.delay_std)
        return self.observation, 0, False, False, {}

    def close(self):
        pass

class PettingZooPerformanceEnv:
    def __init__(self, delay_mean, delay_std):
        self.possible_agents = [1]
        self.agents = [1]
        self.done = False

        self.delay_mean = delay_mean
        self.delay_std = delay_std

    def observation_space(self, agent):
        return DEFAULT_OBSERVATION_SPACE

    def action_space(self, agent):
        return DEFAULT_ACTION_SPACE
 
    def reset(self, seed=None):
        return {1: self.observation_space(1).sample()}, {1: {}}

    def step(self, actions):
        obs = {1: np.array([0], dtype=np.float32)}
        rewards = {1: 1}
        dones = {1: False}
        truncateds = {1: False}
        infos = {1: {}}

        do_work(self.delay_mean, self.delay_std)

        return obs, rewards, dones, truncateds, infos

    def close(self):
        pass

class GymnasiumTestEnv(gym.Env):
    def __init__(self,
            observation_space=DEFAULT_OBSERVATION_SPACE,
            action_space=DEFAULT_ACTION_SPACE):
        self.observation_space = observation_space
        self.action_space = action_space

    def reset(self, seed=None):
        self.tick = 0
        self.rng = pufferlib.utils.RandomState(seed)

        ob = _sample_space('agent_1', self.tick, self.observation_space)
        return ob, {}

    def step(self, actions):
        reward = self.tick
        done = self.tick < 10
        self.tick += 1

        ob = _sample_space('agent_1', self.tick, self.observation_space)
        return ob, reward, done, False, {'dead': done}

    def close(self):
        pass

class PettingZooTestEnv(ParallelEnv):
    def __init__(self,
            observation_space=DEFAULT_OBSERVATION_SPACE,
            action_space=DEFAULT_ACTION_SPACE,
            initial_agents=16, max_agents=16,
            spawn_per_tick=0, death_per_tick=1,
            homogeneous_spaces=True):
        self._observation_space = observation_space
        self._action_space = action_space
        self.initial_agents = initial_agents
        self.max_agents = max_agents
        self.spawn_per_tick = spawn_per_tick
        self.death_per_tick = death_per_tick
        self.homogeneous_spaces = homogeneous_spaces

        self.possible_agents = [f'agent_{i+1}' for i in range(max_agents)]
        self.agents = []

    def reset(self, seed=None):
        self.tick = 0
        self.agents = self.possible_agents[:self.initial_agents]

        obs = {a: _sample_space(a, self.tick, self._observation_space)
            for a in self.agents}
        infos = {a: {} for a in self.agents}
        return obs, infos

    def step(self, actions):
        obs, rewards, dones, truncateds, infos = {}, {}, {}, {}, {}
        self.tick += 1

        dead  = self.agents[:self.death_per_tick]
        for kill in dead:
            self.agents.remove(kill)
            # TODO: Make pufferlib work without pad obs
            # but still require rewards, dones, and optionally infos
            obs[kill] = _sample_space(kill, self.tick,
                self._observation_space, zero=True)
            rewards[kill] = -1
            dones[kill] = True
            truncateds[kill] = False
            infos[kill] = {'dead': True}

        # TODO: Fix this
        assert self.spawn_per_tick == 0
        for spawn in range(self.spawn_per_tick):
            # TODO: Make pufferlib check if an agent respawns on the
            # Same tick as it dies (is this good or bad?)
            spawn = self.rng.choice(self.possible_agents)
            if spawn not in self.agents + dead:
                self.agents.append(spawn)

        for agent in self.agents:
            obs[agent] = _sample_space(agent, self.tick, self._observation_space)
            rewards[agent] = 0.1 * _agent_str_to_int(agent)
            dones[agent] = False
            truncateds[agent] = False
            infos[agent] = {'dead': False}

        return obs, rewards, dones, truncateds, infos

    def observation_space(self, agent) -> gym.Space:
        return self._observation_space

    def action_space(self, agent) -> gym.Space:
        return self._action_space

    def render(self, mode='human'):
        pass

    def close(self):
        pass

### Other Mock environments and utilities
def _agent_str_to_int(agent):
    return int(agent.split('_')[-1])

def _sample_space(agent, tick, space, zero=False):
    if isinstance(agent, str):
        agent = float(agent.split('_')[-1])

    if isinstance(space, spaces.Discrete):
        if zero:
            return 0
        return int((10*agent + tick) % space.n)
    elif isinstance(space, spaces.Box):
        if zero:
            return np.zeros(space.shape, dtype=space.dtype)

        # Try to make a relatively unique data pattern
        # without using RNG
        nonce = 10*agent + tick
        low = space.low
        high = space.high
        sample = low + np.arange(low.size).reshape(space.shape) + nonce
        sample = (sample % high).astype(space.dtype)
        return sample
    elif isinstance(space, spaces.Tuple):
        return tuple(_sample_space(agent, tick, s, zero) for s in space.spaces)
    elif isinstance(space, spaces.Dict):
        return {k: _sample_space(agent, tick, v, zero) for k, v in space.spaces.items()}
    else:
        raise ValueError(f"Invalid space type: {type(space)}")



================================================
FILE: pufferlib/environments/test/mock_environments.py
================================================
from pdb import set_trace as T
import numpy as np

import time
import hashlib
from functools import partial

import gymnasium as gym
from gymnasium.spaces import Box, Discrete, Dict, Tuple
from pettingzoo.utils.env import ParallelEnv

import pufferlib
import pufferlib.emulation
import pufferlib.utils


HIGH = 100
LOW = 0

def make_performance_env(delay=0, bandwidth=1):
    return pufferlib.emulation.PettingZooPufferEnv(
        env_creator=PerformanceEnv,
        env_args=[delay, bandwidth],
    )

class PerformanceEnv:
    def __init__(self, delay=0, bandwith=1):
        self.agents = [1]
        self.possible_agents = [1]
        self.done = False

        self.delay = delay
        assert bandwith > 0
        self.bandwidth = bandwith

    def reset(self, seed=None):
        return {1: self.observation_space(1).sample()}, {1: {}}

    def step(self, actions):
        obs = {1: np.array([0], dtype=np.float32)}
        rewards = {1: 1}
        dones = {1: False}
        truncateds = {1: False}
        infos = {1: {}}

        # Busy wait so process does not swap on sleep
        end = time.perf_counter() + self.delay
        while time.perf_counter() < end:
            pass

        return obs, rewards, dones, truncateds, infos

    def observation_space(self, agent):
        return Box(
            low=-2**20, high=2**20,
            shape=(self.bandwidth,), dtype=np.float32
        )

    def action_space(self, agent):
        return Discrete(2)
    

### Other Mock environments and utilities
def _agent_str_to_int(agent):
    return int(agent.split('_')[-1])


def _sample_space(agent, tick, space, zero=False):
    if isinstance(agent, str):
        agent = float(agent.split('_')[-1])

    if isinstance(space, Discrete):
        if zero:
            return 0
        return int((10*agent + tick) % space.n)
    elif isinstance(space, Box):
        if zero:
            return np.zeros(space.shape, dtype=space.dtype)

        # Try to make a relatively unique data pattern
        # without using RNG
        nonce = 10*agent + tick
        low = space.low
        high = space.high
        sample = low + np.arange(low.size).reshape(space.shape) + nonce
        sample = (sample % high).astype(space.dtype)
        return sample
    elif isinstance(space, Tuple):
        return tuple(_sample_space(agent, tick, s, zero) for s in space.spaces)
    elif isinstance(space, Dict):
        return {k: _sample_space(agent, tick, v, zero) for k, v in space.spaces.items()}
    else:
        raise ValueError(f"Invalid space type: {type(space)}")

class GymnasiumTestEnv(gym.Env):
    def __init__(self, observation_space, action_space):
        self.observation_space = observation_space
        self.action_space = action_space

    def reset(self, seed=None):
        self.tick = 0
        self.rng = pufferlib.utils.RandomState(seed)

        ob = _sample_space('agent_1', self.tick, self.observation_space)
        return ob, {}

    def step(self, actions):
        reward = self.tick
        done = self.tick < 10
        self.tick += 1

        ob = _sample_space('agent_1', self.tick, self.observation_space)
        return ob, reward, done, False, {'dead': done}


def make_mock_singleagent_env(observation_space, action_space):
    return partial(
        GymnasiumTestEnv,
        observation_space=observation_space,
        action_space=action_space,
    )

class TestEnv(ParallelEnv):
    def __init__(self, observation_space, action_space, initial_agents,
            max_agents, spawn_per_tick, death_per_tick):
        self.single_observation_space = observation_space
        self.single_action_space = action_space
        self.initial_agents = initial_agents
        self.max_agents = max_agents
        self.spawn_per_tick = spawn_per_tick
        self.death_per_tick = death_per_tick

        self.possible_agents = [f'agent_{i+1}' for i in range(max_agents)]
        self.agents = []

    def reset(self, seed=None):
        self.tick = 0
        self.agents = self.possible_agents[:self.initial_agents]

        obs = {a: _sample_space(a, self.tick, self.single_observation_space)
            for a in self.agents}
        infos = {a: {} for a in self.agents}
        return obs, infos

    def step(self, actions):
        obs, rewards, dones, truncateds, infos = {}, {}, {}, {}, {}
        self.tick += 1

        dead  = self.agents[:self.death_per_tick]
        for kill in dead:
            self.agents.remove(kill)
            # TODO: Make pufferlib work without pad obs
            # but still require rewards, dones, and optionally infos
            obs[kill] = _sample_space(kill, self.tick, self.single_observation_space, zero=True)
            rewards[kill] = -1
            dones[kill] = True
            truncateds[kill] = False
            infos[kill] = {'dead': True}

        # TODO: Fix this
        assert self.spawn_per_tick == 0
        for spawn in range(self.spawn_per_tick):
            # TODO: Make pufferlib check if an agent respawns on the
            # Same tick as it dies (is this good or bad?)
            spawn = self.rng.choice(self.possible_agents)
            if spawn not in self.agents + dead:
                self.agents.append(spawn)

        for agent in self.agents:
            obs[agent] = _sample_space(agent, self.tick, self.single_observation_space)
            rewards[agent] = 0.1 * _agent_str_to_int(agent)
            dones[agent] = False
            truncateds[agent] = False
            infos[agent] = {'dead': False}

        return obs, rewards, dones, truncateds, infos

    def observation_space(self, agent) -> gym.Space:
        return self.single_observation_space

    def action_space(self, agent) -> gym.Space:
        return self.single_action_space

    def render(self, mode='human'):
        pass

    def close(self):
        pass

def make_mock_multiagent_env(
        observation_space,
        action_space,
        initial_agents,
        max_agents,
        spawn_per_tick,
        death_per_tick,
        homogeneous_spaces=True):
    return partial(
        TestEnv,
        observation_space=observation_space,
        action_space=action_space,
        initial_agents=initial_agents,
        max_agents=max_agents,
        spawn_per_tick=spawn_per_tick,
        death_per_tick=death_per_tick,
    )


MOCK_OBSERVATION_SPACES = [
    # Atari space
    Box(low=0, high=255, shape=(4, 84, 84), dtype=np.uint8),

    # NetHack space
    Dict({
        'blstats': Box(-2147483648, 2147483647, (27,), 'int64'),
        'chars': Box(0, 255, (21, 79), 'uint8'),
        'colors': Box(0, 15, (21, 79), 'uint8'),
        'glyphs': Box(0, 5976, (21, 79), 'int16'),
        'inv_glyphs': Box(0, 5976, (55,), 'int16'),
        'inv_letters': Box(0, 127, (55,), 'uint8'),
        'inv_oclasses': Box(0, 18, (55,), 'uint8'),
        'inv_strs': Box(0, 255, (55, 80), 'uint8'),
        'message': Box(0, 255, (256,), 'uint8'),
        'screen_descriptions': Box(0, 127, (21, 79, 80), 'uint8'),
        'specials': Box(0, 255, (21, 79), 'uint8'),
        'tty_chars': Box(0, 255, (24, 80), 'uint8'),
        'tty_colors': Box(0, 31, (24, 80), 'int8'),
        'tty_cursor': Box(0, 255, (2,), 'uint8'),
    }),
    
    # Neural MMO space
    Dict({
        'ActionTargets': Dict({
            'Attack': Dict({
                'Style': Box(0, 1, (3,), 'int8'),
                'Target': Box(0, 1, (100,), 'int8'),
            }),
            'Buy': Dict({
                'MarketItem': Box(0, 1, (1024,), 'int8'),
            }),
            'Comm': Dict({
                'Token': Box(0, 1, (50,), 'int8'),
            }),
            'Destroy': Dict({
                'InventoryItem': Box(0, 1, (12,), 'int8'),
            }),
            'Give': Dict({
                'InventoryItem': Box(0, 1, (12,), 'int8'),
                'Target': Box(0, 1, (100,), 'int8'),
            }),
            'GiveGold': Dict({
                'Price': Box(0, 1, (99,), 'int8'),
                'Target': Box(0, 1, (100,), 'int8'),
            }),
            'Move': Dict({
                'Direction': Box(0, 1, (5,), 'int8'),
            }),
            'Sell': Dict({
                'InventoryItem': Box(0, 1, (12,), 'int8'),
                'Price': Box(0, 1, (99,), 'int8'),
            }),
            'Use': Dict({
                'InventoryItem': Box(0, 1, (12,), 'int8'),
            })
        }),
        'AgentId': Discrete(129),
        'CurrentTick': Discrete(1025),
        'Entity': Box(-32768, 32767, (100, 23), 'int16'),
        'Inventory': Box(-32768, 32767, (12, 16), 'int16'),
        'Market': Box(-32768, 32767, (1024, 16), 'int16'),
        'Task': Box(-32770.0, 32770.0, (1024,), 'float16'),
        'Tile': Box(-32768, 32767, (225, 3), 'int16'),
    }),

    # Simple spaces
    Discrete(5),
    Box(low=LOW, high=HIGH, shape=(4,), dtype=np.float32),

    # Nested spaces
    Dict({
        "foo": Box(low=LOW, high=HIGH, shape=(2,), dtype=np.float32),
        "bar": Box(low=LOW, high=HIGH, shape=(2,), dtype=np.float32),
    }),
    Tuple((Discrete(3), Discrete(4))),
    Tuple((
        Box(low=LOW, high=HIGH, shape=(2,), dtype=np.float32),
        Discrete(3),
        Dict({
            "baz": Box(low=LOW, high=HIGH, shape=(1,), dtype=np.float32),
            "qux": Box(low=LOW, high=HIGH, shape=(1,), dtype=np.float32),
        }),
    )),
    Dict({
        "foo": Tuple((
            Box(low=LOW, high=HIGH, shape=(2,), dtype=np.float32),
            Discrete(3),
        )),
        "bar": Dict({
            "baz": Discrete(2),
            "qux": Discrete(4),
        }),
    }),
]


MOCK_ACTION_SPACES = [
    # NetHack action space
    Discrete(5),

    # Neural MMO action space
    Dict({
        'Attack': Dict({
            'Style': Discrete(3),
            'Target': Discrete(100),
        }),
        'Buy': Dict({
            'MarketItem': Discrete(1024),
        }),
        'Comm': Dict({
            'Token': Discrete(50),
        }),
        'Destroy': Dict({
            'InventoryItem': Discrete(12),
        }),
        'Give': Dict({
            'InventoryItem': Discrete(12),
            'Target': Discrete(100),
        }),
        'GiveGold': Dict({
            'Price': Discrete(99),
            'Target': Discrete(100),
        }),
        'Move': Dict({
            'Direction': Discrete(5),
        }),
        'Sell': Dict({
            'InventoryItem': Discrete(12),
            'Price': Discrete(99),
        }),
        'Use': Dict({
            'InventoryItem': Discrete(12),
        })
    }),

    # Nested spaces
    Tuple((gym.spaces.Discrete(2), gym.spaces.Discrete(3))),
    Dict({
        "foo": Discrete(4),
        "bar": Discrete(2),
    }),
    Tuple((
        Discrete(4),
        Dict({
            "baz": Discrete(2),
            "qux": Discrete(2),
        }),
    )),
    Dict({
        "foo": Tuple((
            Discrete(2),
            Discrete(3),
        )),
        "bar": Dict({
            "baz": Discrete(2),
            "qux": Discrete(4),
        }),
    }),
]

MOCK_TEAMS = {
    'None': None,
    'single': {
        'team_1': ['agent_1'],
        'team_2': ['agent_2'],
        'team_3': ['agent_3'],
        'team_4': ['agent_4'],
        'team_5': ['agent_5'],
        'team_6': ['agent_6'],
        'team_7': ['agent_7'],
        'team_8': ['agent_8'],
        'team_9': ['agent_9'],
        'team_10': ['agent_10'],
        'team_11': ['agent_11'],
        'team_12': ['agent_12'],
        'team_13': ['agent_13'],
        'team_14': ['agent_14'],
        'team_15': ['agent_15'],
        'team_16': ['agent_16'],
    },
    'pairs': {
        'team_1': ['agent_1', 'agent_2'],
        'team_2': ['agent_3', 'agent_4'],
        'team_3': ['agent_5', 'agent_6'],
        'team_4': ['agent_7', 'agent_8'],
        'team_5': ['agent_9', 'agent_10'],
        'team_6': ['agent_11', 'agent_12'],
        'team_7': ['agent_13', 'agent_14'],
        'team_8': ['agent_15', 'agent_16'],
    },
    'mixed': {
        'team_1': ['agent_1', 'agent_2'],
        'team_2': ['agent_3', 'agent_4', 'agent_5', 'agent_6'],
        'team_3': ['agent_7', 'agent_8', 'agent_9'],
        'team_4': ['agent_10', 'agent_11', 'agent_12', 'agent_13', 'agent_14'],
        'team_5': ['agent_15', 'agent_16'],
    },
}

MOCK_SINGLE_AGENT_ENVIRONMENTS = []
MOCK_MULTI_AGENT_ENVIRONMENTS = []
for obs_space in MOCK_OBSERVATION_SPACES:
    for act_space in MOCK_ACTION_SPACES:
        MOCK_SINGLE_AGENT_ENVIRONMENTS.append(
            make_mock_singleagent_env(
                observation_space=obs_space,
                action_space=act_space,
            )
        )
 
        MOCK_MULTI_AGENT_ENVIRONMENTS.append(
            make_mock_multiagent_env(
                observation_space=obs_space,
                action_space=act_space,
                initial_agents=16,
                max_agents=16,
                spawn_per_tick=0,
                death_per_tick=1,
            )
        )



================================================
FILE: pufferlib/environments/test/torch.py
================================================
from pufferlib.models import Default as Policy



================================================
FILE: pufferlib/environments/trade_sim/__init__.py
================================================
from .environment import env_creator, make

try:
    import torch
except ImportError:
    pass
else:
    from .torch import Policy
    try:
        from .torch import Recurrent
    except:
        Recurrent = None



================================================
FILE: pufferlib/environments/trade_sim/environment.py
================================================
import functools
import numpy as np

import pufferlib

from nof1.simulation.env import TradingEnvironment

def env_creator(name='metta'):
    return functools.partial(make, name)

def make(name, config_path='../nof1-trading-sim/config/experiment_cv.yaml', render_mode='human', buf=None, seed=1):
    '''Crafter creation function'''
    from nof1.utils.config_manager import ConfigManager
    from nof1.data_ingestion.historical_data_reader import HistoricalDataReader

    config_manager = ConfigManager(config_path)
    config = config_manager.config
    data_reader = HistoricalDataReader(config_manager)
    states, prices, atrs, timestamps = data_reader.preprocess_data()
    
    # Create environment
    env = TradingEnvironmentPuff(config_manager.config, states=states, prices=prices, atrs=atrs, timestamps=timestamps)
    return pufferlib.emulation.GymnasiumPufferEnv(env, buf=buf)

class TradingEnvironmentPuff(TradingEnvironment):
    def reset(self):
        obs, info = super().reset()
        return obs.astype(np.float32), info

    def step(self, action):
        obs, reward, terminated, truncated, info = super().step(action)

        if not terminated and not truncated:
            info = {}

        return obs.astype(np.float32), reward, terminated, truncated, info




================================================
FILE: pufferlib/environments/trade_sim/torch.py
================================================
import pufferlib.models

Policy = pufferlib.models.Default
Recurrent = pufferlib.models.LSTMWrapper



================================================
FILE: pufferlib/environments/tribal_village/__init__.py
================================================
from .environment import env_creator, make

try:
    import torch
except ImportError:
    pass
else:
    from .torch import Policy
    try:
        from .torch import Recurrent
    except:
        Recurrent = None


================================================
FILE: pufferlib/environments/tribal_village/environment.py
================================================
"""
Tribal Village Environment PufferLib Integration.

Prefers a local tribal-village checkout for rapid iteration; falls back to the
installed package if not present.
"""

import functools
import sys
from pathlib import Path
from typing import Any, Dict, Optional

import pufferlib


def _import_tribal_village_env():
    """Prefer local tribal-village checkout if present; else import package."""
    repo_root = Path(__file__).resolve().parents[2]
    fallback_dir = repo_root.parent / 'tribal-village'
    if fallback_dir.exists():
        if str(fallback_dir) not in sys.path:
            sys.path.insert(0, str(fallback_dir))
        try:
            from tribal_village_env.environment import TribalVillageEnv  # type: ignore
            return TribalVillageEnv
        except ImportError:
            pass

    try:
        from tribal_village_env.environment import TribalVillageEnv  # type: ignore
        return TribalVillageEnv
    except ImportError as exc:
        raise ImportError("""Failed to import tribal-village environment. Install the package with
  pip install pufferlib[tribal-village] --no-build-isolation
or keep a checkout at ../tribal-village containing tribal_village_env/.""") from exc


def env_creator(name='tribal_village'):
    return functools.partial(make, name=name)


def make(name='tribal_village', config=None, buf=None, **kwargs):
    """Create a tribal village PufferLib environment instance."""
    TribalVillageEnv = _import_tribal_village_env()

    # Merge config with kwargs
    if config is None:
        config = {}
    config = {**config, **kwargs}

    # Create the environment
    env = TribalVillageEnv(config=config, buf=buf)
    return env



================================================
FILE: pufferlib/environments/tribal_village/torch.py
================================================
"""
Ultra-minimal PyTorch policy for Tribal Village - optimized for maximum SPS.
"""

import torch
import torch.nn as nn
import pufferlib.pytorch
import pufferlib.models


class Policy(nn.Module):
    """Ultra-minimal policy optimized for speed over sophistication."""

    def __init__(self, env, hidden_size=64, **kwargs):
        super().__init__()
        self.hidden_size = hidden_size
        self.is_continuous = False

        # Dense observation processing - 21 layers -> hidden_size
        self.layer_proj = pufferlib.pytorch.layer_init(nn.Linear(21, hidden_size))

        # Action heads
        action_space = env.single_action_space
        self.actor = nn.ModuleList([
            pufferlib.pytorch.layer_init(nn.Linear(hidden_size, n), std=0.01)
            for n in action_space.nvec
        ])

        # Value head
        self.value = pufferlib.pytorch.layer_init(nn.Linear(hidden_size, 1), std=1.0)

    def forward(self, observations: torch.Tensor, state=None):
        hidden = self.encode_observations(observations, state)
        actions, value = self.decode_actions(hidden)
        return (actions, value), hidden

    def forward_eval(self, observations: torch.Tensor, state=None):
        hidden = self.encode_observations(observations, state)
        return self.decode_actions(hidden)

    def encode_observations(self, observations: torch.Tensor, state=None) -> torch.Tensor:
        """Ultra-fast dense observation processing."""
        # observations shape: (batch, 21, 11, 11) - direct dense format from Nim

        # Global average pooling across spatial dimensions
        features = observations.float().mean(dim=(2, 3))  # (batch, 21) - one value per layer

        # Use the pre-initialized layer projection
        hidden = torch.relu(self.layer_proj(features))  # (batch, hidden_size)
        return hidden

    def decode_actions(self, hidden: torch.Tensor):
        """Simple action decoding."""
        logits = [head(hidden) for head in self.actor]
        values = self.value(hidden)
        return logits, values


class Recurrent(pufferlib.models.LSTMWrapper):
    """Minimal LSTM wrapper."""

    def __init__(self, env, policy, input_size=64, hidden_size=64):
        super().__init__(env, policy, input_size, hidden_size)


================================================
FILE: pufferlib/environments/vizdoom/__init__.py
================================================
from .environment import env_creator

try:
    import torch
except ImportError:
    pass
else:
    from .torch import Policy
    try:
        from .torch import Recurrent
    except:
        Recurrent = None



================================================
FILE: pufferlib/environments/vizdoom/environment.py
================================================
from pdb import set_trace as T
import numpy as np
import functools

import gymnasium as gym

import pufferlib
import pufferlib.emulation
import pufferlib.environments
import pufferlib.utils
import pufferlib.postprocess


def env_creator(name='doom'):
    return functools.partial(make, name)

def make(name, framestack=1, render_mode='rgb_array', buf=None):
    '''Atari creation function with default CleanRL preprocessing based on Stable Baselines3 wrappers'''
    if name == 'doom':
        name = 'VizdoomHealthGatheringSupreme-v0'

    #pufferlib.environments.try_import('vizdoom', 'gymnasium_wrapper')
    from stable_baselines3.common.atari_wrappers import (
        ClipRewardEnv,
        EpisodicLifeEnv,
        FireResetEnv,
        MaxAndSkipEnv,
        NoopResetEnv,
    )
    # Make does not work without this imported
    # TODO: Fix try_import
    from vizdoom import gymnasium_wrapper
    with pufferlib.utils.Suppress():
        env = gym.make(name, render_mode=render_mode)

    env = DoomWrapper(env) # Don't use standard postprocessor

    #env = gym.wrappers.RecordEpisodeStatistics(env)
    #env = NoopResetEnv(env, noop_max=30)
    #env = MaxAndSkipEnv(env, skip=4)
    #env = EpisodicLifeEnv(env)
    #if "FIRE" in env.unwrapped.get_action_meanings():
    #    env = FireResetEnv(env)
    #env = ClipRewardEnv(env)
    #env = gym.wrappers.GrayScaleObservation(env)
    #env = gym.wrappers.FrameStack(env, framestack)
    return pufferlib.emulation.GymnasiumPufferEnv(env=env, buf=buf)

class DoomWrapper(gym.Wrapper):
    '''Gymnasium env does not expose proper options for screen scale and
    render format. This is slow. So we do it ourselves. Not it is fast. Yay!'''
    def __init__(self, env):
        super().__init__(env.unwrapped)
        if env.observation_space['screen'].shape[0] != 120:
            raise ValueError('Wrong screen resolution. Doom does not provide '
                'a way to change this. You must edit scenarios/<env_name>.cfg'
                'This is inside your local ViZDoom installation. Likely in python system packages'
                'Set screen resolution to RES_160X120 and screen format to GRAY8')

        self.observation_space = gym.spaces.Box(
            low=0, high=255, shape=(60, 80, 1), dtype=np.uint8)

    def reset(self, seed=None, options=None):
        obs, info = self.env.reset(seed=seed, options=options)
        return obs['screen'][::2, ::2], {}

    def step(self, action):
        obs, reward, terminal, truncated, info = self.env.step(action)
        return obs['screen'][::2, ::2], reward, terminal, truncated, info



================================================
FILE: pufferlib/environments/vizdoom/torch.py
================================================
import pufferlib.models


class Recurrent(pufferlib.models.LSTMWrapper):
    def __init__(self, env, policy, input_size=512, hidden_size=512, num_layers=1):
        super().__init__(env, policy, input_size, hidden_size, num_layers)

class Policy(pufferlib.models.Convolutional):
    def __init__(self, env, input_size=512, hidden_size=512, output_size=512,
            framestack=1, flat_size=64*4*6):
        super().__init__(
            env=env,
            input_size=input_size,
            hidden_size=hidden_size,
            output_size=output_size,
            framestack=framestack,
            flat_size=flat_size,
            channels_last=True
        )



================================================
FILE: pufferlib/extensions/extensions.pyx
================================================
# distutils: define_macros=NPY_NO_DEPRECATED_API=NPY_1_7_API_VERSION
# cython: language_level=3
# cython: boundscheck=False
# cython: initializedcheck=False
# cython: wraparound=False
# cython: nonecheck=False

'''Cythonized implementations of PufferLib's emulation functions

emulate is about 2x faster than Python. Nativize is only slightly faster.
'''

import numpy as np
cimport numpy as cnp

from pufferlib.spaces import Tuple, Dict, Discrete


def emulate(cnp.ndarray np_struct, object sample):
    cdef str k
    cdef int i

    if isinstance(sample, dict):
        for k, v in sample.items():
            emulate(np_struct[k], v)
    elif isinstance(sample, tuple):
        for i, v in enumerate(sample):
            emulate(np_struct[f'f{i}'], v)
    else:
        np_struct[()] = sample

cdef object _nativize(np_struct, object space):
    cdef str k
    cdef int i

    if isinstance(space, Discrete):
        return np_struct.item()
    elif isinstance(space, Tuple):
        return tuple(_nativize(np_struct[f'f{i}'], elem)
            for i, elem in enumerate(space))
    elif isinstance(space, Dict):
        return {k: _nativize(np_struct[k], value)
            for k, value in space.items()}
    else:
        return np_struct

def nativize(arr, object space, cnp.dtype struct_dtype):
    np_struct = np.asarray(arr).view(struct_dtype)[0]
    return _nativize(np_struct, space)



================================================
FILE: pufferlib/extensions/pufferlib.cpp
================================================
#include <Python.h>
#include <ATen/Operators.h>
#include <torch/all.h>
#include <torch/library.h>
#include <vector>

extern "C" {
  /* Creates a dummy empty _C module that can be imported from Python.
     The import from Python will load the .so consisting of this file
     in this extension, so that the TORCH_LIBRARY static initializers
     below are run. */
  PyObject* PyInit__C(void)
  {
      static struct PyModuleDef module_def = {
          PyModuleDef_HEAD_INIT,
          "_C",   /* name of module */
          NULL,   /* module documentation, may be NULL */
          -1,     /* size of per-interpreter state of the module,
                     or -1 if the module keeps state in global variables. */
          NULL,   /* methods */
      };
      return PyModule_Create(&module_def);
  }
}

namespace pufferlib {

void puff_advantage_row(float* values, float* rewards, float* dones,
        float* importance, float* advantages, float gamma, float lambda,
        float rho_clip, float c_clip, int horizon) {
    float lastpufferlam = 0;
    for (int t = horizon-2; t >= 0; t--) {
        int t_next = t + 1;
        float nextnonterminal = 1.0 - dones[t_next];
        float rho_t = fminf(importance[t], rho_clip);
        float c_t = fminf(importance[t], c_clip);
        float delta = rho_t*(rewards[t_next] + gamma*values[t_next]*nextnonterminal - values[t]);
        lastpufferlam = delta + gamma*lambda*c_t*lastpufferlam*nextnonterminal;
        advantages[t] = lastpufferlam;
    }
}

void vtrace_check(torch::Tensor values, torch::Tensor rewards,
        torch::Tensor dones, torch::Tensor importance, torch::Tensor advantages,
        int num_steps, int horizon) {

    // Validate input tensors
    torch::Device device = values.device();
    for (const torch::Tensor& t : {values, rewards, dones, importance, advantages}) {
        TORCH_CHECK(t.dim() == 2, "Tensor must be 2D");
        TORCH_CHECK(t.device() == device, "All tensors must be on same device");
        TORCH_CHECK(t.size(0) == num_steps, "First dimension must match num_steps");
        TORCH_CHECK(t.size(1) == horizon, "Second dimension must match horizon");
        TORCH_CHECK(t.dtype() == torch::kFloat32, "All tensors must be float32");
        if (!t.is_contiguous()) {
            t.contiguous();
        }
    }
}


// [num_steps, horizon]
void puff_advantage(float* values, float* rewards, float* dones, float* importance,
        float* advantages, float gamma, float lambda, float rho_clip, float c_clip,
        int num_steps, const int horizon){
    for (int offset = 0; offset < num_steps*horizon; offset+=horizon) {
        puff_advantage_row(values + offset, rewards + offset,
            dones + offset, importance + offset, advantages + offset,
            gamma, lambda, rho_clip, c_clip, horizon
        );
    }
}


void compute_puff_advantage_cpu(torch::Tensor values, torch::Tensor rewards,
        torch::Tensor dones, torch::Tensor importance, torch::Tensor advantages,
        double gamma, double lambda, double rho_clip, double c_clip) {
    int num_steps = values.size(0);
    int horizon = values.size(1);
    vtrace_check(values, rewards, dones, importance, advantages, num_steps, horizon);
    puff_advantage(values.data_ptr<float>(), rewards.data_ptr<float>(),
        dones.data_ptr<float>(), importance.data_ptr<float>(), advantages.data_ptr<float>(),
        gamma, lambda, rho_clip, c_clip, num_steps, horizon
    );
}

TORCH_LIBRARY(pufferlib, m) {
   m.def("compute_puff_advantage(Tensor(a!) values, Tensor(b!) rewards, Tensor(c!) dones, Tensor(d!) importance, Tensor(e!) advantages, float gamma, float lambda, float rho_clip, float c_clip) -> ()");
 }

TORCH_LIBRARY_IMPL(pufferlib, CPU, m) {
  m.impl("compute_puff_advantage", &compute_puff_advantage_cpu);
}

}



================================================
FILE: pufferlib/extensions/puffernet.h
================================================
#include <stdio.h>
#include <stdbool.h>
#include <string.h>
#include <math.h>
#include <assert.h>

typedef struct {
    void* data;
    size_t capacity;
    size_t used;
} Arena;

Arena* make_allocator(size_t total_size) {
    void* buffer = calloc(1, total_size + sizeof(Arena));
    Arena* allocator = (Arena*)buffer;
    allocator->data = (void*)((char*)buffer + sizeof(Arena));
    allocator->capacity = total_size;
    allocator->used = 0;
    return allocator;
}

void* alloc(Arena* allocator, size_t size) {
    void* ptr = (void*)((char*)allocator->data + allocator->used);
    if (allocator->used + size > allocator->capacity) {
        return NULL;
    }
    allocator->used += size;
    return ptr;
}

// File format is obained by flattening and concatenating all pytorch layers
typedef struct Weights Weights;
struct Weights {
    float* data;
    int size;
    int idx;
};

void _load_weights(const char* filename, float* weights, size_t num_weights) {
    FILE* file = fopen(filename, "rb");
    if (!file) {
        perror("Error opening file");
    }
    fseek(file, 0, SEEK_END);
    rewind(file);
    size_t read_size = fread(weights, sizeof(float), num_weights, file);
    fclose(file);
    if (read_size != num_weights) {
        perror("Error reading file");
    }
}

Weights* load_weights(const char* filename, size_t num_weights) {
    Weights* weights = calloc(1, sizeof(Weights) + num_weights*sizeof(float));
    weights->data = (float*)(weights + 1);
    _load_weights(filename, weights->data, num_weights);
    weights->size = num_weights;
    weights->idx = 0;
    return weights;
}

float* get_weights(Weights* weights, int num_weights) {
    float* data = &weights->data[weights->idx];
    weights->idx += num_weights;
    assert(weights->idx <= weights->size);
    return data;
}

// PufferNet implementation of PyTorch functions
// These are tested against the PyTorch implementation
void _relu(float* input, float* output, int size) {
    for (int i = 0; i < size; i++) {
        output[i] = fmaxf(0.0f, input[i]);
    }
}

void _gelu(float* input, float* output, int size) {
    for (int i = 0; i < size; i++) {
        output[i] = 0.5f*input[i]*(1 + tanhf(0.6628526501011142 * (input[i] + 0.044715f*input[i]*input[i]*input[i])));
    }
}

float _sigmoid(float x);
inline float _sigmoid(float x) {
    return 1.0f / (1.0f + expf(-x));
}

void _linear(float* input, float* weights, float* bias, float* output,
        int batch_size, int input_dim, int output_dim) {
    for (int b = 0; b < batch_size; b++) {
        for (int o = 0; o < output_dim; o++) {
            float sum = 0.0f;
            for (int i = 0; i < input_dim; i++)
                sum += input[b*input_dim + i] * weights[o*input_dim + i];
            output[b*output_dim + o] = sum + bias[o];
        }
    }
}

void _linear_accumulate(float* input, float* weights, float* bias, float* output,
        int batch_size, int input_dim, int output_dim) {
    for (int b = 0; b < batch_size; b++) {
        for (int o = 0; o < output_dim; o++) {
            float sum = 0.0f;
            for (int i = 0; i < input_dim; i++)
                sum += input[b*input_dim + i] * weights[o*input_dim + i];
            output[b*output_dim + o] += sum + bias[o];
        }
    }
}

void _conv2d(float* input, float* weights, float* bias,
        float* output, int batch_size, int in_width, int in_height,
        int in_channels, int out_channels, int kernel_size, int stride) {
    int h_out = (in_height - kernel_size)/stride + 1;
    int w_out = (in_width - kernel_size)/stride + 1;
    for (int b = 0; b < batch_size; b++) {
        for (int oc = 0; oc < out_channels; oc++) {
            for (int h = 0; h < h_out; h++) {
                for (int w = 0; w < w_out; w++) {
                    int out_adr = (
                        b*out_channels*h_out*w_out
                        + oc*h_out*w_out+ 
                        + h*w_out
                        + w
                    );
                    output[out_adr] = bias[oc];
                    for (int ic = 0; ic < in_channels; ic++) {
                        for (int kh = 0; kh < kernel_size; kh++) {
                            for (int kw = 0; kw < kernel_size; kw++) {
                                int in_adr = (
                                    b*in_channels*in_height*in_width
                                    + ic*in_height*in_width
                                    + (h*stride + kh)*in_width
                                    + (w*stride + kw)
                                );
                                int weight_adr = (
                                    oc*in_channels*kernel_size*kernel_size
                                    + ic*kernel_size*kernel_size
                                    + kh*kernel_size
                                    + kw
                                );
                                output[out_adr] += input[in_adr]*weights[weight_adr];
                            }
                        }
                    }
               }
            }
        }
    }
}

void _conv3d(float* input, float* weights, float* bias,
        float* output, int batch_size, int in_width, int in_height, int in_depth,
        int in_channels, int out_channels, int kernel_size, int stride) {
    int d_out = (in_depth - kernel_size)/stride + 1;
    int h_out = (in_height - kernel_size)/stride + 1;
    int w_out = (in_width - kernel_size)/stride + 1;
    for (int b = 0; b < batch_size; b++) {
        for (int oc = 0; oc < out_channels; oc++) {
            for (int d = 0; d < d_out; d++) {
                for (int h = 0; h < h_out; h++) {
                    for (int w = 0; w < w_out; w++) {
                        int out_adr = (
                            b*out_channels*d_out*h_out*w_out
                            + oc*d_out*h_out*w_out
                            + d*h_out*w_out
                            + h*w_out
                            + w
                        );
                        output[out_adr] = bias[oc];
                        for (int ic = 0; ic < in_channels; ic++) {
                            for (int kd = 0; kd < kernel_size; kd++) {
                                for (int kh = 0; kh < kernel_size; kh++) {
                                    for (int kw = 0; kw < kernel_size; kw++) {
                                        int in_adr = (
                                            b*in_channels*in_depth*in_height*in_width
                                            + ic*in_depth*in_height*in_width
                                            + (d*stride + kd)*in_height*in_width
                                            + (h*stride + kh)*in_width
                                            + (w*stride + kw)
                                        );
                                        int weight_adr = (
                                            oc*in_channels*kernel_size*kernel_size*kernel_size
                                            + ic*kernel_size*kernel_size*kernel_size
                                            + kd*kernel_size*kernel_size
                                            + kh*kernel_size
                                            + kw
                                        );
                                        output[out_adr] += input[in_adr]*weights[weight_adr];
                                    }
                                }
                            }
                        }
                    }
                }
            }
        }
    }
}

void _lstm(float* input, float* state_h, float* state_c, float* weights_input,
        float* weights_state, float* bias_input, float*bias_state,
        float *buffer, int batch_size, int input_size, int hidden_size) {
    _linear(input, weights_input, bias_input, buffer, batch_size, input_size, 4*hidden_size);
    _linear_accumulate(state_h, weights_state, bias_state, buffer, batch_size, hidden_size, 4*hidden_size);

    // Activation functions
    for (int b=0; b<batch_size; b++) {
        int b_offset = 4*b*hidden_size;
        for (int i=0; i<2*hidden_size; i++) {
            int buf_adr = b_offset + i;
            buffer[buf_adr] = _sigmoid(buffer[buf_adr]);
        }
        for (int i=2*hidden_size; i<3*hidden_size; i++) {
            int buf_adr = b_offset + i;
            buffer[buf_adr] = tanh(buffer[buf_adr]);
        }
        for (int i=3*hidden_size; i<4*hidden_size; i++) {
            int buf_adr = b_offset + i;
            buffer[buf_adr] = _sigmoid(buffer[buf_adr]);
        }
    }

    // Gates
    for (int b=0; b<batch_size; b++) {
        int hidden_offset = b*hidden_size;
        int b_offset = 4*b*hidden_size;
        for (int i=0; i<hidden_size; i++) {
            state_c[hidden_offset + i] = (
                buffer[b_offset + hidden_size + i] * state_c[hidden_offset + i]
                + buffer[b_offset + i] * buffer[b_offset + 2*hidden_size + i]
            );
            state_h[hidden_offset + i] = (
                buffer[b_offset + 3*hidden_size + i] * tanh(state_c[hidden_offset + i])
            );
        }
    }
}

void _embedding(int* input, float* weights, float* output, int batch_size, int num_embeddings, int embedding_dim) {
    for (int b = 0; b < batch_size; b++) {
        memcpy(output + b*embedding_dim, weights + input[b]*embedding_dim, embedding_dim*sizeof(float));
    }
}

void _layernorm(float* input, float* weights, float* bias, float* output, int batch_size, int input_dim) {
    for (int b = 0; b < batch_size; b++) {
        float mean = 0.0f;
        for (int i = 0; i < input_dim; i++) {
            mean += input[b*input_dim + i];
        }
        mean /= (float)input_dim;

        float variance = 0.0f;
        for (int i = 0; i < input_dim; i++) {
            float diff = input[b*input_dim + i] - mean;
            variance += diff*diff;
        }
        variance /= (float)input_dim;

        float denom = sqrtf(variance + 1e-5f);
        for (int i = 0; i < input_dim; i++) {
            float norm = (input[b*input_dim + i] - mean)/denom;
            output[b*input_dim + i] = norm*weights[i] + bias[i];
        }
    }
}

void _one_hot(int* input, int* output, int batch_size, int input_size, int num_classes) {
    for (int b = 0; b < batch_size; b++) {
        for (int i = 0; i < input_size; i++) {
            int in_adr = b*input_size + i;
            int out_adr = (
                b*input_size*num_classes
                + i*num_classes
                + input[in_adr]
            );
            output[out_adr] = 1.0f;
        }
    }
}

void _cat_dim1(float* x, float* y, float* output, int batch_size, int x_size, int y_size) {
    for (int b = 0; b < batch_size; b++) {
        for (int i = 0; i < x_size; i++) {
            int x_adr = b*x_size + i;
            int out_adr = b*(x_size + y_size) + i;
            output[out_adr] = x[x_adr];
        }
        for (int i = 0; i < y_size; i++) {
            int y_adr = b*y_size + i;
            int out_adr = b*(x_size + y_size) + x_size + i;
            output[out_adr] = y[y_adr];
        }
    }
}

void _argmax_multidiscrete(float* input, int* output, int batch_size, int logit_sizes[], int num_actions) {
    int in_adr = 0;
    for (int b = 0; b < batch_size; b++) {
        for (int a = 0; a < num_actions; a++) {
            int out_adr = b*num_actions + a;
            float max_logit = input[in_adr];
            output[out_adr] = 0;
            int num_action_types = logit_sizes[a];
            for (int i = 1; i < num_action_types; i++) {
                float out = input[in_adr + i];
                if (out > max_logit) {
                    max_logit = out;
                    output[out_adr] = i;
                }
            }
            in_adr += num_action_types;
        }
    }
}

void _softmax_multidiscrete(float* input, int* output, int batch_size, int logit_sizes[], int num_actions) {
    int in_adr = 0;
    for (int b = 0; b < batch_size; b++) {
        for (int a = 0; a < num_actions; a++) {
            int out_adr = b*num_actions + a;
            float logit_exp_sum = 0;
            int num_action_types = logit_sizes[a];
            for (int i = 0; i < num_action_types; i++) {
                logit_exp_sum += expf(input[in_adr + i]);
            }
            float prob = rand() / (float)RAND_MAX;
            float logit_prob = 0;
            output[out_adr] = 0;
            for (int i = 0; i < num_action_types; i++) {
                logit_prob += expf(input[in_adr + i]) / logit_exp_sum;
                if (prob < logit_prob) {
                    output[out_adr] = i;
                    break;
                }
            }
            in_adr += num_action_types;
        }
    }
}

void _max_dim1(float* input, float* output, int batch_size, int seq_len, int feature_dim) {
    for (int b = 0; b < batch_size; b++) {
        for (int f = 0; f < feature_dim; f++) {
            float max_val = input[b*seq_len*feature_dim + f];
            for (int s = 1; s < seq_len; s++) {
                float val = input[b*seq_len*feature_dim + s*feature_dim + f];
                if (val > max_val) {
                    max_val = val;
                }
            }
            output[b*feature_dim + f] = max_val;
        }
    }
}

// User API. Provided to help organize layers
typedef struct Linear Linear;
struct Linear {
    float* output;
    float* weights;
    float* bias;
    int batch_size;
    int input_dim;
    int output_dim;
};

Linear* make_linear(Weights* weights, int batch_size, int input_dim, int output_dim) {
    size_t buffer_size = batch_size*output_dim*sizeof(float);
    Linear* layer = calloc(1, sizeof(Linear) + buffer_size);
    *layer = (Linear){
        .output = (float*)(layer + 1),
        .weights = get_weights(weights, output_dim*input_dim),
        .bias = get_weights(weights, output_dim),
        .batch_size = batch_size,
        .input_dim = input_dim,
        .output_dim = output_dim,
    };
    return layer;
}

void linear(Linear* layer, float* input) {
    _linear(input, layer->weights, layer->bias, layer->output,
        layer->batch_size, layer->input_dim, layer->output_dim);
}

void linear_accumulate(Linear* layer, float* input) {
    _linear_accumulate(input, layer->weights, layer->bias, layer->output,
        layer->batch_size, layer->input_dim, layer->output_dim);
}

typedef struct ReLU ReLU;
struct ReLU {
    float* output;
    int batch_size;
    int input_dim;
};

ReLU* make_relu(int batch_size, int input_dim) {
    size_t buffer_size = batch_size*input_dim*sizeof(float);
    ReLU* layer = calloc(1, sizeof(ReLU) + buffer_size);
    *layer = (ReLU){
        .output = (float*)(layer + 1),
        .batch_size = batch_size,
        .input_dim = input_dim,
    };
    return layer;
}

void relu(ReLU* layer, float* input) {
    _relu(input, layer->output, layer->batch_size*layer->input_dim);
}

typedef struct GELU GELU;
struct GELU {
    float* output;
    int batch_size;
    int input_dim;
};

GELU* make_gelu(int batch_size, int input_dim) {
    size_t buffer_size = batch_size*input_dim*sizeof(float);
    GELU* layer = calloc(1, sizeof(GELU) + buffer_size);
    *layer = (GELU){
        .output = (float*)(layer + 1),
        .batch_size = batch_size,
        .input_dim = input_dim,
    };
    return layer;
}

void gelu(GELU* layer, float* input) {
    _gelu(input, layer->output, layer->batch_size*layer->input_dim);
}

typedef struct MaxDim1 MaxDim1;
struct MaxDim1 {
    float* output;
    int batch_size;
    int seq_len;
    int feature_dim;
};

MaxDim1* make_max_dim1(int batch_size, int seq_len, int feature_dim) {
    size_t buffer_size = batch_size*feature_dim*sizeof(float);
    MaxDim1* layer = calloc(1, sizeof(MaxDim1) + buffer_size);
    *layer = (MaxDim1){
        .output = (float*)(layer + 1),
        .batch_size = batch_size,
        .seq_len = seq_len,
        .feature_dim = feature_dim,
    };
    return layer;
}

void max_dim1(MaxDim1* layer, float* input) {
    _max_dim1(input, layer->output, layer->batch_size, layer->seq_len, layer->feature_dim);
}

typedef struct Conv2D Conv2D;
struct Conv2D {
    float* output;
    float* weights;
    float* bias;
    int batch_size;
    int in_width;
    int in_height;
    int in_channels;
    int out_channels;
    int kernel_size;
    int stride;
};

Conv2D* make_conv2d(Weights* weights, int batch_size, int in_width, int in_height,
        int in_channels, int out_channels, int kernel_size, int stride) {
    size_t buffer_size = batch_size*out_channels*in_height*in_width*sizeof(float);
    int num_weights = out_channels*in_channels*kernel_size*kernel_size;
    Conv2D* layer = calloc(1, sizeof(Conv2D) + buffer_size);
    *layer = (Conv2D){
        .output = (float*)(layer + 1),
        .weights = get_weights(weights, num_weights),
        .bias = get_weights(weights, out_channels),
        .batch_size = batch_size,
        .in_width = in_width,
        .in_height = in_height,
        .in_channels = in_channels,
        .out_channels = out_channels,
        .kernel_size = kernel_size,
        .stride = stride,
    };
    return layer;
}

void conv2d(Conv2D* layer, float* input) {
    _conv2d(input, layer->weights, layer->bias, layer->output,
        layer->batch_size, layer->in_width, layer->in_height,
        layer->in_channels, layer->out_channels, layer->kernel_size, layer->stride);
}

typedef struct Conv3D Conv3D;
struct Conv3D {
    float* output;
    float* weights;
    float* bias;
    int batch_size;
    int in_width;
    int in_height;
    int in_depth;
    int in_channels;
    int out_channels;
    int kernel_size;
    int stride;
};

Conv3D* make_conv3d(Weights* weights, int batch_size, int in_width, int in_height, int in_depth,
        int in_channels, int out_channels, int kernel_size, int stride) {
    
    size_t buffer_size = batch_size*out_channels*in_depth*in_height*in_width*sizeof(float);
    int num_weights = out_channels*in_channels*kernel_size*kernel_size*kernel_size;
    Conv3D* layer = calloc(1, sizeof(Conv3D) + buffer_size);
    *layer = (Conv3D){
        .output = (float*)(layer + 1),
        .weights = get_weights(weights, num_weights),
        .bias = get_weights(weights, out_channels),
        .batch_size = batch_size,
        .in_width = in_width,
        .in_height = in_height,
        .in_depth = in_depth,
        .in_channels = in_channels,
        .out_channels = out_channels,
        .kernel_size = kernel_size,
        .stride = stride,
    };
    return layer;
}

void conv3d(Conv3D* layer, float* input) {
    _conv3d(input, layer->weights, layer->bias, layer->output,
        layer->batch_size, layer->in_width, layer->in_height, layer->in_depth,
        layer->in_channels, layer->out_channels, layer->kernel_size, layer->stride);
}

typedef struct LSTM LSTM;
struct LSTM {
    float* state_h;
    float* state_c;
    float* weights_input;
    float* weights_state;
    float* bias_input;
    float*bias_state;
    float *buffer;
    int batch_size;
    int input_size;
    int hidden_size;
};

LSTM* make_lstm(Weights* weights, int batch_size, int input_size, int hidden_size) {
    int state_size = batch_size*hidden_size;
    LSTM* layer = calloc(1, sizeof(LSTM) + 6*state_size*sizeof(float));
    float* buffer = (float*)(layer + 1);
    *layer = (LSTM){
        .state_h = buffer,
        .state_c = buffer + state_size,
        .weights_input = get_weights(weights, 4*hidden_size*input_size),
        .weights_state = get_weights(weights, 4*hidden_size*hidden_size),
        .bias_input = get_weights(weights, 4*hidden_size),
        .bias_state = get_weights(weights, 4*hidden_size),
        .buffer = buffer + 2*state_size,
        .batch_size = batch_size,
        .input_size = input_size,
        .hidden_size = hidden_size,

    };
    return layer;
}

void lstm(LSTM* layer, float* input) {
    _lstm(input, layer->state_h, layer->state_c, layer->weights_input,
        layer->weights_state, layer->bias_input, layer->bias_state,
        layer->buffer, layer->batch_size, layer->input_size, layer->hidden_size);
}

typedef struct Embedding Embedding;
struct Embedding {
    float* output;
    float* weights;
    int batch_size;
    int num_embeddings;
    int embedding_dim;
};

Embedding* make_embedding(Weights* weights, int batch_size, int num_embeddings, int embedding_dim) {
    size_t output_size = batch_size*embedding_dim*sizeof(float);
    Embedding* layer = (Embedding*)calloc(1, sizeof(Embedding) + batch_size + output_size);
    *layer = (Embedding){
        .output = (float*)(layer + 1),
        .weights = get_weights(weights, num_embeddings*embedding_dim),
        .batch_size = batch_size,
        .num_embeddings = num_embeddings,
        .embedding_dim = embedding_dim,
    };
    return layer;
}

void embedding(Embedding* layer, int* input) {
    _embedding(input, layer->weights, layer->output, layer->batch_size, layer->num_embeddings, layer->embedding_dim);
}

typedef struct LayerNorm LayerNorm;
struct LayerNorm {
    float* output;
    float* weights;
    float* bias;
    int batch_size;
    int input_dim;
};

LayerNorm* make_layernorm(Weights* weights, int batch_size, int input_dim) {
    size_t output_size = batch_size*input_dim*sizeof(float);
    LayerNorm* layer = calloc(1, sizeof(LayerNorm) + output_size);
    *layer = (LayerNorm){
        .output = (float*)(layer + 1),
        .weights = get_weights(weights, input_dim),
        .bias = get_weights(weights, input_dim),
        .batch_size = batch_size,
        .input_dim = input_dim,
    };
    return layer;
}
    
void layernorm(LayerNorm* layer, float* input) {
    _layernorm(input, layer->weights, layer->bias, layer->output,
        layer->batch_size, layer->input_dim);
}

typedef struct OneHot OneHot;
struct OneHot {
    int* output;
    int batch_size;
    int input_size;
    int num_classes;
};

OneHot* make_one_hot(int batch_size, int input_size, int num_classes) {
    size_t buffer_size = batch_size*input_size*num_classes*sizeof(int);
    OneHot* layer = calloc(1, sizeof(OneHot) + buffer_size);
    *layer = (OneHot){
        .output = (int*)(layer + 1),
        .batch_size = batch_size,
        .input_size = input_size,
        .num_classes = num_classes,
    };
    return layer;
}

void one_hot(OneHot* layer, int* input) {
    _one_hot(input, layer->output, layer->batch_size, layer->input_size, layer->num_classes);
}

typedef struct CatDim1 CatDim1;
struct CatDim1 {
    float* output;
    int batch_size;
    int x_size;
    int y_size;
};

CatDim1* make_cat_dim1(int batch_size, int x_size, int y_size) {
    size_t buffer_size = batch_size*(x_size + y_size)*sizeof(float);
    CatDim1* layer = calloc(1, sizeof(CatDim1) + buffer_size);
    *layer = (CatDim1){
        .output = (float*)(layer + 1),
        .batch_size = batch_size,
        .x_size = x_size,
        .y_size = y_size,
    };
    return layer;
}

void cat_dim1(CatDim1* layer, float* x, float* y) {
    _cat_dim1(x, y, layer->output, layer->batch_size, layer->x_size, layer->y_size);
}

typedef struct Multidiscrete Multidiscrete;
struct Multidiscrete {
    int batch_size;
    int logit_sizes[32];
    int num_actions;
};

Multidiscrete* make_multidiscrete(int batch_size, int logit_sizes[], int num_actions) {
    Multidiscrete* layer = calloc(1, sizeof(Multidiscrete));
    layer->batch_size = batch_size;
    layer->num_actions = num_actions;
    memcpy(layer->logit_sizes, logit_sizes, num_actions*sizeof(int));
    return layer;
}

void argmax_multidiscrete(Multidiscrete* layer, float* input, int* output) {
    _argmax_multidiscrete(input, output, layer->batch_size, layer->logit_sizes, layer->num_actions);
}

void softmax_multidiscrete(Multidiscrete* layer, float* input, int* output) {
    _softmax_multidiscrete(input, output, layer->batch_size, layer->logit_sizes, layer->num_actions);
}

// Default models

typedef struct Default Default;
struct Default {
    int num_agents;
    float* obs;
    Linear* encoder;
    ReLU* relu1;
    Linear* actor;
    Linear* value_fn;
    Multidiscrete* multidiscrete;
};

Default* make_default(Weights* weights, int num_agents, int input_dim, int hidden_dim, int action_dim) {
    Default* net = calloc(1, sizeof(Default));
    net->num_agents = num_agents;
    net->obs = (float*)calloc(num_agents*input_dim, sizeof(float));
    net->encoder = make_linear(weights, num_agents, input_dim, hidden_dim);
    net->relu1 = make_relu(num_agents, hidden_dim);
    net->actor = make_linear(weights, num_agents, hidden_dim, action_dim);
    net->value_fn = make_linear(weights, num_agents, hidden_dim, 1);
    int logit_sizes[1] = {action_dim};
    net->multidiscrete = make_multidiscrete(num_agents, logit_sizes, 1);
    return net;
}

void free_default(Default* net) {
    free(net->obs);
    free(net->encoder);
    free(net->relu1);
    free(net->actor);
    free(net->value_fn);
    free(net->multidiscrete);
    free(net);
}

void forward_default(Default* net, float* observations, int* actions) {
    linear(net->encoder, observations);
    relu(net->relu1, net->encoder->output);
    linear(net->actor, net->relu1->output);
    linear(net->value_fn, net->relu1->output);
    softmax_multidiscrete(net->multidiscrete, net->actor->output, actions);
}

typedef struct LinearLSTM LinearLSTM;
struct LinearLSTM {
    int num_agents;
    float* obs;
    Linear* encoder;
    GELU* gelu1;
    LSTM* lstm;
    Linear* actor;
    Linear* value_fn;
    Multidiscrete* multidiscrete;
};

LinearLSTM* make_linearlstm(Weights* weights, int num_agents, int input_dim, int logit_sizes[], int num_actions) {
    LinearLSTM* net = calloc(1, sizeof(LinearLSTM));
    net->num_agents = num_agents;
    net->obs = calloc(num_agents*input_dim, sizeof(float));
    net->encoder = make_linear(weights, num_agents, input_dim, 128);
    net->gelu1 = make_gelu(num_agents, 128);
    int atn_sum = 0;
    for (int i = 0; i < num_actions; i++) {
        atn_sum += logit_sizes[i];
    }
    net->actor = make_linear(weights, num_agents, 128, atn_sum);
    net->value_fn = make_linear(weights, num_agents, 128, 1);
    net->lstm = make_lstm(weights, num_agents, 128, 128);
    net->multidiscrete = make_multidiscrete(num_agents, logit_sizes, num_actions);
    return net;
}

void free_linearlstm(LinearLSTM* net) {
    free(net->obs);
    free(net->encoder);
    free(net->gelu1);
    free(net->actor);
    free(net->value_fn);
    free(net->lstm);
    free(net->multidiscrete);
    free(net);
}

void forward_linearlstm(LinearLSTM* net, float* observations, int* actions) {
    linear(net->encoder, observations);
    gelu(net->gelu1, net->encoder->output);
    lstm(net->lstm, net->gelu1->output);
    linear(net->actor, net->lstm->state_h);
    linear(net->value_fn, net->lstm->state_h);
    softmax_multidiscrete(net->multidiscrete, net->actor->output, actions);
}

typedef struct ConvLSTM ConvLSTM; struct ConvLSTM {
    int num_agents;
    float* obs;
    Conv2D* conv1;
    ReLU* relu1;
    Conv2D* conv2;
    ReLU* relu2;
    Linear* linear;
    LSTM* lstm;
    Linear* actor;
    Linear* value_fn;
    Multidiscrete* multidiscrete;
};

ConvLSTM* make_convlstm(Weights* weights, int num_agents, int input_dim,
        int input_channels, int cnn_channels, int hidden_dim, int action_dim) {
    ConvLSTM* net = calloc(1, sizeof(ConvLSTM));
    net->num_agents = num_agents;
    net->obs = calloc(num_agents*input_dim*input_dim*input_channels, sizeof(float));
    net->conv1 = make_conv2d(weights, num_agents, input_dim,
        input_dim, input_channels, cnn_channels, 5, 3);
    net->relu1 = make_relu(num_agents, hidden_dim*3*3);
    net->conv2 = make_conv2d(weights, num_agents, 3, 3, cnn_channels, cnn_channels, 3, 1);
    net->relu2 = make_relu(num_agents, hidden_dim);
    net->linear = make_linear(weights, num_agents, cnn_channels, hidden_dim);
    net->actor = make_linear(weights, num_agents, hidden_dim, action_dim);
    net->value_fn = make_linear(weights, num_agents, hidden_dim, 1);
    net->lstm = make_lstm(weights, num_agents, hidden_dim, hidden_dim);
    int logit_sizes[1] = {action_dim};
    net->multidiscrete = make_multidiscrete(num_agents, logit_sizes, 1);
    return net;
}

void free_convlstm(ConvLSTM* net) {
    free(net->obs);
    free(net->conv1);
    free(net->relu1);
    free(net->conv2);
    free(net->relu2);
    free(net->linear);
    free(net->actor);
    free(net->value_fn);
    free(net->lstm);
    free(net->multidiscrete);
    free(net);
}

void forward_convlstm(ConvLSTM* net, float* observations, int* actions) {
    conv2d(net->conv1, observations);
    relu(net->relu1, net->conv1->output);
    conv2d(net->conv2, net->relu1->output);
    relu(net->relu2, net->conv2->output);
    linear(net->linear, net->relu2->output);
    lstm(net->lstm, net->linear->output);
    linear(net->actor, net->lstm->state_h);
    linear(net->value_fn, net->lstm->state_h);
    softmax_multidiscrete(net->multidiscrete, net->actor->output, actions);
}



================================================
FILE: pufferlib/extensions/puffernet.pyx
================================================
# distutils: define_macros=NPY_NO_DEPRECATED_API=NPY_1_7_API_VERSION
# cython: language_level=3
# cython: boundscheck=False
# cython: initializedcheck=False
# cython: wraparound=False
# cython: cdivision=True
# cython: nonecheck=False
# cython: profile=False

from libc.stdlib cimport rand, RAND_MAX
from libc.math cimport sqrtf
from cpython.list cimport PyList_GET_ITEM
cimport cython
cimport numpy as cnp
import numpy as np

cdef extern from "puffernet.h":
    void _linear(float* input, float* weights, float* bias, float* output,
        int batch_size, int input_dim, int output_dim)
    void _relu(float* input, float* output,int size)
    float _sigmoid(float x)
    void _conv2d(float* input, float* weights, float* bias,
        float* output, int batch_size, int in_width, int in_height,
        int in_channels, int out_channels, int kernel_size, int stride)
    void _conv3d(float* input, float* weights, float* bias,
        float* output, int batch_size, int in_width, int in_height, int in_depth,
        int in_channels, int out_channels, int kernel_size, int stride)
    void _embedding(int* input, float* weights, float* output,
        int batch_size, int num_embeddings, int embedding_dim)
    void _lstm(float* input, float* state_h, float* state_c, float* weights_input,
        float* weights_state, float* bias_input, float*bias_state,
        float *buffer, int batch_size, int input_size, int hidden_size)
    void _layernorm(float* input, float* weights, float* bias, float* output,
        int batch_size, int input_size)
    void _one_hot(int* input, int* output, int batch_size,
        int input_size, int num_classes)
    void _cat_dim1(float* x, float* y, float* output,
        int batch_size, int x_size, int y_size)
    void _argmax_multidiscrete(float* input, int* output,
        int batch_size, int logit_sizes[], int num_actions)

def puf_linear_layer(cnp.ndarray input, cnp.ndarray weights, cnp.ndarray bias, cnp.ndarray output,
        int batch_size, int input_dim, int output_dim):
    _linear(<float*> input.data, <float*> weights.data, <float*> bias.data,
        <float*> output.data, batch_size, input_dim, output_dim)

def puf_relu(cnp.ndarray input, cnp.ndarray output, int size):
    _relu(<float*> input.data, <float*> output.data, size)

def puf_sigmoid(float x):
    return _sigmoid(x)

def puf_convolution_layer(cnp.ndarray input, cnp.ndarray weights, cnp.ndarray bias,
        cnp.ndarray output, int batch_size, int in_width, int in_height,
        int in_channels, int out_channels, int kernel_size, int stride):
    _conv2d(<float*> input.data, <float*> weights.data, <float*> bias.data,
        <float*> output.data, batch_size, in_width, in_height, in_channels, out_channels,
        kernel_size, stride)

def puf_convolution_3d_layer(cnp.ndarray input, cnp.ndarray weights, cnp.ndarray bias,
        cnp.ndarray output, int batch_size, int in_width, int in_height, int in_depth,
        int in_channels, int out_channels, int kernel_size, int stride):
    _conv3d(<float*> input.data, <float*> weights.data, <float*> bias.data,
        <float*> output.data, batch_size, in_width, in_height, in_depth, in_channels, out_channels,
        kernel_size, stride)

def puf_lstm(cnp.ndarray input, cnp.ndarray state_h, cnp.ndarray state_c, cnp.ndarray weights_input,
        cnp.ndarray weights_state, cnp.ndarray bias_input, cnp.ndarray bias_state,
        cnp.ndarray buffer, int batch_size, int input_size, int hidden_size):
    _lstm(<float*> input.data, <float*> state_h.data, <float*> state_c.data,
        <float*> weights_input.data, <float*> weights_state.data, <float*> bias_input.data,
        <float*> bias_state.data, <float*> buffer.data, batch_size, input_size, hidden_size)

def puf_embedding(cnp.ndarray input, cnp.ndarray weights, cnp.ndarray output,
        int batch_size, int num_embeddings, int embedding_dim):
    _embedding(<int*> input.data, <float*> weights.data, <float*> output.data,
        batch_size, num_embeddings, embedding_dim)

def puf_layernorm(cnp.ndarray input, cnp.ndarray weights, cnp.ndarray bias,
        cnp.ndarray output, int batch_size, int input_size):
    _layernorm(<float*> input.data, <float*> weights.data, <float*> bias.data,
        <float*> output.data, batch_size, input_size)

def puf_one_hot(cnp.ndarray input, cnp.ndarray output, int batch_size, int input_size, int num_classes):
    _one_hot(<int*> input.data, <int*> output.data, batch_size, input_size, num_classes)

def puf_cat_dim1(cnp.ndarray x, cnp.ndarray y, cnp.ndarray output, int batch_size, int x_size, int y_size):
    _cat_dim1(<float*> x.data, <float*> y.data, <float*> output.data, batch_size, x_size, y_size)

def puf_argmax_multidiscrete(cnp.ndarray input, cnp.ndarray output,
        int batch_size, cnp.ndarray logit_sizes, int num_actions):
    _argmax_multidiscrete(<float*> input.data, <int*> output.data,
        batch_size, <int*> logit_sizes.data, num_actions)




================================================
FILE: pufferlib/extensions/cuda/pufferlib.cu
================================================
#include <torch/extension.h>
#include <cuda.h>
#include <cuda_runtime.h>

namespace pufferlib {

__host__ __device__ void puff_advantage_row_cuda(float* values, float* rewards, float* dones,
        float* importance, float* advantages, float gamma, float lambda,
        float rho_clip, float c_clip, int horizon) {
    float lastpufferlam = 0;
    for (int t = horizon-2; t >= 0; t--) {
        int t_next = t + 1;
        float nextnonterminal = 1.0 - dones[t_next];
        float rho_t = fminf(importance[t], rho_clip);
        float c_t = fminf(importance[t], c_clip);
        float delta = rho_t*(rewards[t_next] + gamma*values[t_next]*nextnonterminal - values[t]);
        lastpufferlam = delta + gamma*lambda*c_t*lastpufferlam*nextnonterminal;
        advantages[t] = lastpufferlam;
    }
}

void vtrace_check_cuda(torch::Tensor values, torch::Tensor rewards,
        torch::Tensor dones, torch::Tensor importance, torch::Tensor advantages,
        int num_steps, int horizon) {

    // Validate input tensors
    torch::Device device = values.device();
    for (const torch::Tensor& t : {values, rewards, dones, importance, advantages}) {
        TORCH_CHECK(t.dim() == 2, "Tensor must be 2D");
        TORCH_CHECK(t.device() == device, "All tensors must be on same device");
        TORCH_CHECK(t.size(0) == num_steps, "First dimension must match num_steps");
        TORCH_CHECK(t.size(1) == horizon, "Second dimension must match horizon");
        TORCH_CHECK(t.dtype() == torch::kFloat32, "All tensors must be float32");
        if (!t.is_contiguous()) {
            t.contiguous();
        }
    }
}

 // [num_steps, horizon]
__global__ void puff_advantage_kernel(float* values, float* rewards,
        float* dones, float* importance, float* advantages, float gamma,
        float lambda, float rho_clip, float c_clip, int num_steps, int horizon) {
    int row = blockIdx.x*blockDim.x + threadIdx.x;
    if (row >= num_steps) {
        return;
    }
    int offset = row*horizon;
    puff_advantage_row_cuda(values + offset, rewards + offset, dones + offset,
        importance + offset, advantages + offset, gamma, lambda, rho_clip, c_clip, horizon);
}

void compute_puff_advantage_cuda(torch::Tensor values, torch::Tensor rewards,
        torch::Tensor dones, torch::Tensor importance, torch::Tensor advantages,
        double gamma, double lambda, double rho_clip, double c_clip) {
    int num_steps = values.size(0);
    int horizon = values.size(1);
    vtrace_check_cuda(values, rewards, dones, importance, advantages, num_steps, horizon);
    TORCH_CHECK(values.is_cuda(), "All tensors must be on GPU");

    int threads_per_block = 256;
    int blocks = (num_steps + threads_per_block - 1) / threads_per_block;

    puff_advantage_kernel<<<blocks, threads_per_block>>>(
        values.data_ptr<float>(),
        rewards.data_ptr<float>(),
        dones.data_ptr<float>(),
        importance.data_ptr<float>(),
        advantages.data_ptr<float>(),
        gamma,
        lambda,
        rho_clip,
        c_clip,
        num_steps,
        horizon
    );

    cudaError_t err = cudaGetLastError();
    if (err != cudaSuccess) {
        throw std::runtime_error(cudaGetErrorString(err));
    }
}

TORCH_LIBRARY_IMPL(pufferlib, CUDA, m) {
  m.impl("compute_puff_advantage", &compute_puff_advantage_cuda);
}

}



================================================
FILE: pufferlib/ocean/__init__.py
================================================
from .environment import *

try:
    import torch
except ImportError:
    pass
else:
    from .torch import Policy
    try:
        from .torch import Recurrent
    except:
        Recurrent = None



================================================
FILE: pufferlib/ocean/env_binding.h
================================================
#include <Python.h>
#include <numpy/arrayobject.h>

// Forward declarations for env-specific functions supplied by user
static int my_log(PyObject* dict, Log* log);
static int my_init(Env* env, PyObject* args, PyObject* kwargs);

static PyObject* my_shared(PyObject* self, PyObject* args, PyObject* kwargs);
#ifndef MY_SHARED
static PyObject* my_shared(PyObject* self, PyObject* args, PyObject* kwargs) {
    return NULL;
}
#endif

static PyObject* my_get(PyObject* dict, Env* env);
#ifndef MY_GET
static PyObject* my_get(PyObject* dict, Env* env) {
    return NULL;
}
#endif

static int my_put(Env* env, PyObject* args, PyObject* kwargs);
#ifndef MY_PUT
static int my_put(Env* env, PyObject* args, PyObject* kwargs) {
    return 0;
}
#endif

#ifndef MY_METHODS
#define MY_METHODS {NULL, NULL, 0, NULL}
#endif

static Env* unpack_env(PyObject* args) {
    PyObject* handle_obj = PyTuple_GetItem(args, 0);
    if (!PyObject_TypeCheck(handle_obj, &PyLong_Type)) {
        PyErr_SetString(PyExc_TypeError, "env_handle must be an integer");
        return NULL;
    }

    Env* env = (Env*)PyLong_AsVoidPtr(handle_obj);
    if (!env) {
        PyErr_SetString(PyExc_ValueError, "Invalid env handle");
        return NULL;
    }

    return env;
}

// Python function to initialize the environment
static PyObject* env_init(PyObject* self, PyObject* args, PyObject* kwargs) {
    if (PyTuple_Size(args) != 6) {
        PyErr_SetString(PyExc_TypeError, "Environment requires 5 arguments");
        return NULL;
    }

    Env* env = (Env*)calloc(1, sizeof(Env));
    if (!env) {
        PyErr_SetString(PyExc_MemoryError, "Failed to allocate environment");
        return NULL;
    }

    PyObject* obs = PyTuple_GetItem(args, 0);
    if (!PyObject_TypeCheck(obs, &PyArray_Type)) {
        PyErr_SetString(PyExc_TypeError, "Observations must be a NumPy array");
        return NULL;
    }
    PyArrayObject* observations = (PyArrayObject*)obs;
    if (!PyArray_ISCONTIGUOUS(observations)) {
        PyErr_SetString(PyExc_ValueError, "Observations must be contiguous");
        return NULL;
    }
    env->observations = PyArray_DATA(observations);

    PyObject* act = PyTuple_GetItem(args, 1);
    if (!PyObject_TypeCheck(act, &PyArray_Type)) {
        PyErr_SetString(PyExc_TypeError, "Actions must be a NumPy array");
        return NULL;
    }
    PyArrayObject* actions = (PyArrayObject*)act;
    if (!PyArray_ISCONTIGUOUS(actions)) {
        PyErr_SetString(PyExc_ValueError, "Actions must be contiguous");
        return NULL;
    }
    env->actions = PyArray_DATA(actions);
    if (PyArray_ITEMSIZE(actions) == sizeof(double)) {
        PyErr_SetString(PyExc_ValueError, "Action tensor passed as float64 (pass np.float32 buffer)");
        return NULL;
    }

    PyObject* rew = PyTuple_GetItem(args, 2);
    if (!PyObject_TypeCheck(rew, &PyArray_Type)) {
        PyErr_SetString(PyExc_TypeError, "Rewards must be a NumPy array");
        return NULL;
    }
    PyArrayObject* rewards = (PyArrayObject*)rew;
    if (!PyArray_ISCONTIGUOUS(rewards)) {
        PyErr_SetString(PyExc_ValueError, "Rewards must be contiguous");
        return NULL;
    }
    if (PyArray_NDIM(rewards) != 1) {
        PyErr_SetString(PyExc_ValueError, "Rewards must be 1D");
        return NULL;
    }
    env->rewards = PyArray_DATA(rewards);

    PyObject* term = PyTuple_GetItem(args, 3);
    if (!PyObject_TypeCheck(term, &PyArray_Type)) {
        PyErr_SetString(PyExc_TypeError, "Terminals must be a NumPy array");
        return NULL;
    }
    PyArrayObject* terminals = (PyArrayObject*)term;
    if (!PyArray_ISCONTIGUOUS(terminals)) {
        PyErr_SetString(PyExc_ValueError, "Terminals must be contiguous");
        return NULL;
    }
    if (PyArray_NDIM(terminals) != 1) {
        PyErr_SetString(PyExc_ValueError, "Terminals must be 1D");
        return NULL;
    }
    env->terminals = PyArray_DATA(terminals);

    PyObject* trunc = PyTuple_GetItem(args, 4);
    if (!PyObject_TypeCheck(trunc, &PyArray_Type)) {
        PyErr_SetString(PyExc_TypeError, "Truncations must be a NumPy array");
        return NULL;
    }
    PyArrayObject* truncations = (PyArrayObject*)trunc;
    if (!PyArray_ISCONTIGUOUS(truncations)) {
        PyErr_SetString(PyExc_ValueError, "Truncations must be contiguous");
        return NULL;
    }
    if (PyArray_NDIM(truncations) != 1) {
        PyErr_SetString(PyExc_ValueError, "Truncations must be 1D");
        return NULL;
    }
    // env->truncations = PyArray_DATA(truncations);
    
    
    PyObject* seed_arg = PyTuple_GetItem(args, 5);
    if (!PyObject_TypeCheck(seed_arg, &PyLong_Type)) {
        PyErr_SetString(PyExc_TypeError, "seed must be an integer");
        return NULL;
    }
    int seed = PyLong_AsLong(seed_arg);
 
    // Assumes each process has the same number of environments
    srand(seed);

    // If kwargs is NULL, create a new dictionary
    if (kwargs == NULL) {
        kwargs = PyDict_New();
    } else {
        Py_INCREF(kwargs);  // We need to increment the reference since we'll be modifying it
    }

    // Add the seed to kwargs
    PyObject* py_seed = PyLong_FromLong(seed);
    if (PyDict_SetItemString(kwargs, "seed", py_seed) < 0) {
        PyErr_SetString(PyExc_RuntimeError, "Failed to set seed in kwargs");
        Py_DECREF(py_seed);
        Py_DECREF(kwargs);
        return NULL;
    }
    Py_DECREF(py_seed);

    PyObject* empty_args = PyTuple_New(0);
    my_init(env, empty_args, kwargs);
    Py_DECREF(kwargs);
    if (PyErr_Occurred()) {
        return NULL;
    }

    return PyLong_FromVoidPtr(env);
}

// Python function to reset the environment
static PyObject* env_reset(PyObject* self, PyObject* args) {
    if (PyTuple_Size(args) != 2) {
        PyErr_SetString(PyExc_TypeError, "env_reset requires 2 arguments");
        return NULL;
    }

    Env* env = unpack_env(args);
    if (!env){
        return NULL;
    }
    c_reset(env);
    Py_RETURN_NONE;
}

// Python function to step the environment
static PyObject* env_step(PyObject* self, PyObject* args) {
    int num_args = PyTuple_Size(args);
    if (num_args != 1) {
        PyErr_SetString(PyExc_TypeError, "vec_render requires 1 argument");
        return NULL;
    }

    Env* env = unpack_env(args);
    if (!env){
        return NULL;
    }
    c_step(env);
    Py_RETURN_NONE;
}

// Python function to step the environment
static PyObject* env_render(PyObject* self, PyObject* args) {
    Env* env = unpack_env(args);
    if (!env){
        return NULL;
    }
    c_render(env);
    Py_RETURN_NONE;
}

// Python function to close the environment
static PyObject* env_close(PyObject* self, PyObject* args) {
    Env* env = unpack_env(args);
    if (!env){
        return NULL;
    }
    c_close(env);
    free(env);
    Py_RETURN_NONE;
}

static PyObject* env_get(PyObject* self, PyObject* args) {
    Env* env = unpack_env(args);
    if (!env){
        return NULL;
    }
    PyObject* dict = PyDict_New();
    my_get(dict, env);
    if (PyErr_Occurred()) {
        return NULL;
    }
    return dict;
}

static PyObject* env_put(PyObject* self, PyObject* args, PyObject* kwargs) {
    int num_args = PyTuple_Size(args);
    if (num_args != 1) {
        PyErr_SetString(PyExc_TypeError, "env_put requires 1 positional argument");
        return NULL;
    }

    Env* env = unpack_env(args);
    if (!env){
        return NULL;
    }

    PyObject* empty_args = PyTuple_New(0);
    my_put(env, empty_args, kwargs);
    if (PyErr_Occurred()) {
        return NULL;
    }

    Py_RETURN_NONE;
}

typedef struct {
    Env** envs;
    int num_envs;
} VecEnv;

static VecEnv* unpack_vecenv(PyObject* args) {
    PyObject* handle_obj = PyTuple_GetItem(args, 0);
    if (!PyObject_TypeCheck(handle_obj, &PyLong_Type)) {
        PyErr_SetString(PyExc_TypeError, "env_handle must be an integer");
        return NULL;
    }

    VecEnv* vec = (VecEnv*)PyLong_AsVoidPtr(handle_obj);
    if (!vec) {
        PyErr_SetString(PyExc_ValueError, "Missing or invalid vec env handle");
        return NULL;
    }

    if (vec->num_envs <= 0) {
        PyErr_SetString(PyExc_ValueError, "Missing or invalid vec env handle");
        return NULL;
    }

    return vec;
}

static PyObject* vec_init(PyObject* self, PyObject* args, PyObject* kwargs) {
    if (PyTuple_Size(args) != 7) {
        PyErr_SetString(PyExc_TypeError, "vec_init requires 6 arguments");
        return NULL;
    }

    VecEnv* vec = (VecEnv*)calloc(1, sizeof(VecEnv));
    if (!vec) {
        PyErr_SetString(PyExc_MemoryError, "Failed to allocate vec env");
        return NULL;
    }
    PyObject* num_envs_arg = PyTuple_GetItem(args, 5);
    if (!PyObject_TypeCheck(num_envs_arg, &PyLong_Type)) {
        PyErr_SetString(PyExc_TypeError, "num_envs must be an integer");
        return NULL;
    }
    int num_envs = PyLong_AsLong(num_envs_arg);
    if (num_envs <= 0) {
        PyErr_SetString(PyExc_TypeError, "num_envs must be greater than 0");
        return NULL;
    }
    vec->num_envs = num_envs;
    vec->envs = (Env**)calloc(num_envs, sizeof(Env*));
    if (!vec->envs) {
        PyErr_SetString(PyExc_MemoryError, "Failed to allocate vec env");
        return NULL;
    }

    PyObject* seed_obj = PyTuple_GetItem(args, 6);
    if (!PyObject_TypeCheck(seed_obj, &PyLong_Type)) {
        PyErr_SetString(PyExc_TypeError, "seed must be an integer");
        return NULL;
    }
    int seed = PyLong_AsLong(seed_obj);

    PyObject* obs = PyTuple_GetItem(args, 0);
    if (!PyObject_TypeCheck(obs, &PyArray_Type)) {
        PyErr_SetString(PyExc_TypeError, "Observations must be a NumPy array");
        return NULL;
    }
    PyArrayObject* observations = (PyArrayObject*)obs;
    if (!PyArray_ISCONTIGUOUS(observations)) {
        PyErr_SetString(PyExc_ValueError, "Observations must be contiguous");
        return NULL;
    }
    if (PyArray_NDIM(observations) < 2) {
        PyErr_SetString(PyExc_ValueError, "Batched Observations must be at least 2D");
        return NULL;
    }

    PyObject* act = PyTuple_GetItem(args, 1);
    if (!PyObject_TypeCheck(act, &PyArray_Type)) {
        PyErr_SetString(PyExc_TypeError, "Actions must be a NumPy array");
        return NULL;
    }
    PyArrayObject* actions = (PyArrayObject*)act;
    if (!PyArray_ISCONTIGUOUS(actions)) {
        PyErr_SetString(PyExc_ValueError, "Actions must be contiguous");
        return NULL;
    }
    if (PyArray_ITEMSIZE(actions) == sizeof(double)) {
        PyErr_SetString(PyExc_ValueError, "Action tensor passed as float64 (pass np.float32 buffer)");
        return NULL;
    }

    PyObject* rew = PyTuple_GetItem(args, 2);
    if (!PyObject_TypeCheck(rew, &PyArray_Type)) {
        PyErr_SetString(PyExc_TypeError, "Rewards must be a NumPy array");
        return NULL;
    }
    PyArrayObject* rewards = (PyArrayObject*)rew;
    if (!PyArray_ISCONTIGUOUS(rewards)) {
        PyErr_SetString(PyExc_ValueError, "Rewards must be contiguous");
        return NULL;
    }
    if (PyArray_NDIM(rewards) != 1) {
        PyErr_SetString(PyExc_ValueError, "Rewards must be 1D");
        return NULL;
    }

    PyObject* term = PyTuple_GetItem(args, 3);
    if (!PyObject_TypeCheck(term, &PyArray_Type)) {
        PyErr_SetString(PyExc_TypeError, "Terminals must be a NumPy array");
        return NULL;
    }
    PyArrayObject* terminals = (PyArrayObject*)term;
    if (!PyArray_ISCONTIGUOUS(terminals)) {
        PyErr_SetString(PyExc_ValueError, "Terminals must be contiguous");
        return NULL;
    }
    if (PyArray_NDIM(terminals) != 1) {
        PyErr_SetString(PyExc_ValueError, "Terminals must be 1D");
        return NULL;
    }

    PyObject* trunc = PyTuple_GetItem(args, 4);
    if (!PyObject_TypeCheck(trunc, &PyArray_Type)) {
        PyErr_SetString(PyExc_TypeError, "Truncations must be a NumPy array");
        return NULL;
    }
    PyArrayObject* truncations = (PyArrayObject*)trunc;
    if (!PyArray_ISCONTIGUOUS(truncations)) {
        PyErr_SetString(PyExc_ValueError, "Truncations must be contiguous");
        return NULL;
    }
    if (PyArray_NDIM(truncations) != 1) {
        PyErr_SetString(PyExc_ValueError, "Truncations must be 1D");
        return NULL;
    }

    // If kwargs is NULL, create a new dictionary
    if (kwargs == NULL) {
        kwargs = PyDict_New();
    } else {
        Py_INCREF(kwargs);  // We need to increment the reference since we'll be modifying it
    }

    for (int i = 0; i < num_envs; i++) {
        Env* env = (Env*)calloc(1, sizeof(Env));
        if (!env) {
            PyErr_SetString(PyExc_MemoryError, "Failed to allocate environment");
            Py_DECREF(kwargs);
            return NULL;
        }
        vec->envs[i] = env;
        
        // // Make sure the log is initialized to 0
        memset(&env->log, 0, sizeof(Log));
        
        env->observations = (void*)((char*)PyArray_DATA(observations) + i*PyArray_STRIDE(observations, 0));
        env->actions = (void*)((char*)PyArray_DATA(actions) + i*PyArray_STRIDE(actions, 0));
        env->rewards = (void*)((char*)PyArray_DATA(rewards) + i*PyArray_STRIDE(rewards, 0));
        env->terminals = (void*)((char*)PyArray_DATA(terminals) + i*PyArray_STRIDE(terminals, 0));
        // env->truncations = (void*)((char*)PyArray_DATA(truncations) + i*PyArray_STRIDE(truncations, 0));

        // Assumes each process has the same number of environments
        int env_seed = i + seed*vec->num_envs;
        srand(env_seed);
 
        // Add the seed to kwargs for this environment
        PyObject* py_seed = PyLong_FromLong(env_seed);
        if (PyDict_SetItemString(kwargs, "seed", py_seed) < 0) {
            PyErr_SetString(PyExc_RuntimeError, "Failed to set seed in kwargs");
            Py_DECREF(py_seed);
            Py_DECREF(kwargs);
            return NULL;
        }
        Py_DECREF(py_seed);

        PyObject* empty_args = PyTuple_New(0);
        my_init(env, empty_args, kwargs);
        if (PyErr_Occurred()) {
            return NULL;
        }
    }

    Py_DECREF(kwargs);
    return PyLong_FromVoidPtr(vec);
}


// Python function to close the environment
static PyObject* vectorize(PyObject* self, PyObject* args) {
    int num_envs = PyTuple_Size(args);
    if (num_envs == 0) {
        PyErr_SetString(PyExc_TypeError, "make_vec requires at least 1 env id");
        return NULL;
    }

    VecEnv* vec = (VecEnv*)calloc(1, sizeof(VecEnv));
    if (!vec) {
        PyErr_SetString(PyExc_MemoryError, "Failed to allocate vec env");
        return NULL;
    }

    vec->envs = (Env**)calloc(num_envs, sizeof(Env*));
    if (!vec->envs) {
        PyErr_SetString(PyExc_MemoryError, "Failed to allocate vec env");
        return NULL;
    }

    vec->num_envs = num_envs;
    for (int i = 0; i < num_envs; i++) {
        PyObject* handle_obj = PyTuple_GetItem(args, i);
        if (!PyObject_TypeCheck(handle_obj, &PyLong_Type)) {
            PyErr_SetString(PyExc_TypeError, "Env ids must be integers. Pass them as separate args with *env_ids, not as a list.");
            return NULL;
        }
        vec->envs[i] = (Env*)PyLong_AsVoidPtr(handle_obj);
    }

    return PyLong_FromVoidPtr(vec);
}

static PyObject* vec_reset(PyObject* self, PyObject* args) {
    if (PyTuple_Size(args) != 2) {
        PyErr_SetString(PyExc_TypeError, "vec_reset requires 2 arguments");
        return NULL;
    }

    VecEnv* vec = unpack_vecenv(args);
    if (!vec) {
        return NULL;
    }

    PyObject* seed_arg = PyTuple_GetItem(args, 1);
    if (!PyObject_TypeCheck(seed_arg, &PyLong_Type)) {
        PyErr_SetString(PyExc_TypeError, "seed must be an integer");
        return NULL;
    }
    int seed = PyLong_AsLong(seed_arg);
 
    for (int i = 0; i < vec->num_envs; i++) {
        // Assumes each process has the same number of environments
        srand(i + seed*vec->num_envs);
        c_reset(vec->envs[i]);
    }
    Py_RETURN_NONE;
}

static PyObject* vec_step(PyObject* self, PyObject* arg) {
    int num_args = PyTuple_Size(arg);
    if (num_args != 1) {
        PyErr_SetString(PyExc_TypeError, "vec_step requires 1 argument");
        return NULL;
    }

    VecEnv* vec = unpack_vecenv(arg);
    if (!vec) {
        return NULL;
    }

    for (int i = 0; i < vec->num_envs; i++) {
        c_step(vec->envs[i]);
    }
    Py_RETURN_NONE;
}

static PyObject* vec_render(PyObject* self, PyObject* args) {
    int num_args = PyTuple_Size(args);
    if (num_args != 2) {
        PyErr_SetString(PyExc_TypeError, "vec_render requires 2 arguments");
        return NULL;
    }

    VecEnv* vec = (VecEnv*)PyLong_AsVoidPtr(PyTuple_GetItem(args, 0));
    if (!vec) {
        PyErr_SetString(PyExc_ValueError, "Invalid vec_env handle");
        return NULL;
    }

    PyObject* env_id_arg = PyTuple_GetItem(args, 1);
    if (!PyObject_TypeCheck(env_id_arg, &PyLong_Type)) {
        PyErr_SetString(PyExc_TypeError, "env_id must be an integer");
        return NULL;
    }
    int env_id = PyLong_AsLong(env_id_arg);
 
    c_render(vec->envs[env_id]);
    Py_RETURN_NONE;
}

static int assign_to_dict(PyObject* dict, char* key, float value) {
    PyObject* v = PyFloat_FromDouble(value);
    if (v == NULL) {
        PyErr_SetString(PyExc_TypeError, "Failed to convert log value");
        return 1;
    }
    if(PyDict_SetItemString(dict, key, v) < 0) {
        PyErr_SetString(PyExc_TypeError, "Failed to set log value");
        return 1;
    }
    Py_DECREF(v);
    return 0;
}

static PyObject* vec_log(PyObject* self, PyObject* args) {
    VecEnv* vec = unpack_vecenv(args);
    if (!vec) {
        return NULL;
    }

    // Iterates over logs one float at a time. Will break
    // horribly if Log has non-float data.
    Log aggregate = {0};
    int num_keys = sizeof(Log) / sizeof(float);
    for (int i = 0; i < vec->num_envs; i++) {
        Env* env = vec->envs[i];
        for (int j = 0; j < num_keys; j++) {
            ((float*)&aggregate)[j] += ((float*)&env->log)[j];
            ((float*)&env->log)[j] = 0.0f;
        }
    }

    PyObject* dict = PyDict_New();
    if (aggregate.n == 0.0f) {
        return dict;
    }

    // Average
    float n = aggregate.n;
    for (int i = 0; i < num_keys; i++) {
        ((float*)&aggregate)[i] /= n;
    }

    // User populates dict
    my_log(dict, &aggregate);
    assign_to_dict(dict, "n", n);

    return dict;
}

static PyObject* vec_close(PyObject* self, PyObject* args) {
    VecEnv* vec = unpack_vecenv(args);
    if (!vec) {
        return NULL;
    }

    for (int i = 0; i < vec->num_envs; i++) {
        c_close(vec->envs[i]);
        free(vec->envs[i]);
    }
    free(vec->envs);
    free(vec);
    Py_RETURN_NONE;
}

static double unpack(PyObject* kwargs, char* key) {
    PyObject* val = PyDict_GetItemString(kwargs, key);
    if (val == NULL) {
        char error_msg[100];
        snprintf(error_msg, sizeof(error_msg), "Missing required keyword argument '%s'", key);
        PyErr_SetString(PyExc_TypeError, error_msg);
        return 1;
    }
    if (PyLong_Check(val)) {
        long out = PyLong_AsLong(val);
        if (out > INT_MAX || out < INT_MIN) {
            char error_msg[100];
            snprintf(error_msg, sizeof(error_msg), "Value %ld of integer argument %s is out of range", out, key);
            PyErr_SetString(PyExc_TypeError, error_msg);
            return 1;
        }
        // Cast on return. Safe because double can represent all 32-bit ints exactly
        return out;
    }
    if (PyFloat_Check(val)) {
        return PyFloat_AsDouble(val);
    }
    char error_msg[100];
    snprintf(error_msg, sizeof(error_msg), "Failed to unpack keyword %s as int", key);
    PyErr_SetString(PyExc_TypeError, error_msg);
    return 1;
}

// Method table
static PyMethodDef methods[] = {
    {"env_init", (PyCFunction)env_init, METH_VARARGS | METH_KEYWORDS, "Init environment with observation, action, reward, terminal, truncation arrays"},
    {"env_reset", env_reset, METH_VARARGS, "Reset the environment"},
    {"env_step", env_step, METH_VARARGS, "Step the environment"},
    {"env_render", env_render, METH_VARARGS, "Render the environment"},
    {"env_close", env_close, METH_VARARGS, "Close the environment"},
    {"env_get", env_get, METH_VARARGS, "Get the environment state"},
    {"env_put", (PyCFunction)env_put, METH_VARARGS | METH_KEYWORDS, "Put stuff into env"},
    {"vectorize", vectorize, METH_VARARGS, "Make a vector of environment handles"},
    {"vec_init", (PyCFunction)vec_init, METH_VARARGS | METH_KEYWORDS, "Initialize a vector of environments"},
    {"vec_reset", vec_reset, METH_VARARGS, "Reset the vector of environments"},
    {"vec_step", vec_step, METH_VARARGS, "Step the vector of environments"},
    {"vec_log", vec_log, METH_VARARGS, "Log the vector of environments"},
    {"vec_render", vec_render, METH_VARARGS, "Render the vector of environments"},
    {"vec_close", vec_close, METH_VARARGS, "Close the vector of environments"},
    {"shared", (PyCFunction)my_shared, METH_VARARGS | METH_KEYWORDS, "Shared state"},
    MY_METHODS,
    {NULL, NULL, 0, NULL}
};

// Module definition
static PyModuleDef module = {
    PyModuleDef_HEAD_INIT,
    "binding",
    NULL,
    -1,
    methods
};

PyMODINIT_FUNC PyInit_binding(void) {
    import_array();
    return PyModule_Create(&module);
}



================================================
FILE: pufferlib/ocean/environment.py
================================================
import importlib
import pufferlib.emulation

def lazy_import(module_path, attr):
    """
    Returns a callable that, when called with any arguments, will
    import the module, retrieve the attribute (usually a class or factory)
    and then call it with the given arguments.
    """
    return lambda *args, **kwargs: getattr(__import__(module_path, fromlist=[attr]), attr)(*args, **kwargs)

def make_foraging(width=1080, height=720, num_agents=4096, horizon=512,
        discretize=True, food_reward=0.1, render_mode='rgb_array'):
    from .grid import grid
    init_fn = grid.init_foraging
    reward_fn = grid.reward_foraging
    return grid.PufferGrid(width, height, num_agents,
        horizon, discretize=discretize, food_reward=food_reward, init_fn=init_fn, reward_fn=reward_fn, render_mode=render_mode)

def make_predator_prey(width=1080, height=720, num_agents=4096, horizon=512,
        discretize=True, food_reward=0.1, render_mode='rgb_array'):
    from .grid import grid
    init_fn = grid.init_predator_prey
    reward_fn = grid.reward_predator_prey
    return grid.PufferGrid(width, height, num_agents,
        horizon, discretize=discretize, food_reward=food_reward,
        init_fn=init_fn, reward_fn=reward_fn,
        render_mode=render_mode)

def make_group(width=1080, height=720, num_agents=4096, horizon=512,
        discretize=True, food_reward=0.1, render_mode='rgb_array'):
    from .grid import grid
    init_fn = grid.init_group
    reward_fn = grid.reward_group
    return grid.PufferGrid(width, height, num_agents,
        horizon, discretize=discretize, food_reward=food_reward,
        init_fn=init_fn, reward_fn=reward_fn,
        render_mode=render_mode)

def make_puffer(width=1080, height=720, num_agents=4096, horizon=512,
        discretize=True, food_reward=0.1, render_mode='rgb_array'):
    from .grid import grid
    init_fn = grid.init_puffer
    reward_fn = grid.reward_puffer
    return grid.PufferGrid(width, height, num_agents,
        horizon, discretize=discretize, food_reward=food_reward,
        init_fn=init_fn, reward_fn=reward_fn,
        render_mode=render_mode)

def make_puffergrid(render_mode='raylib', vision_range=5,
        num_envs=4096, num_maps=1000, max_map_size=9,
        report_interval=128, buf=None):
    return PufferGrid(render_mode, vision_range, num_envs,
        num_maps, max_map_size, report_interval, buf)

def make_continuous(discretize=False, buf=None, **kwargs):
    from . import sanity
    env = sanity.Continuous(discretize=discretize)
    if not discretize:
        env = pufferlib.ClipAction(env)
    env = pufferlib.EpisodeStats(env)
    return pufferlib.emulation.GymnasiumPufferEnv(env=env, buf=buf)

def make_squared(distance_to_target=3, num_targets=1, buf=None, **kwargs):
    from . import sanity
    env = sanity.Squared(distance_to_target=distance_to_target, num_targets=num_targets, **kwargs)
    env = pufferlib.EpisodeStats(env)
    return pufferlib.emulation.GymnasiumPufferEnv(env=env, buf=buf, **kwargs)

def make_bandit(num_actions=10, reward_scale=1, reward_noise=1, buf=None):
    from . import sanity
    env = sanity.Bandit(num_actions=num_actions, reward_scale=reward_scale,
        reward_noise=reward_noise)
    env = pufferlib.EpisodeStats(env)
    return pufferlib.emulation.GymnasiumPufferEnv(env=env, buf=buf)

def make_memory(mem_length=2, mem_delay=2, buf=None, **kwargs):
    from . import sanity
    env = sanity.Memory(mem_length=mem_length, mem_delay=mem_delay)
    env = pufferlib.EpisodeStats(env)
    return pufferlib.emulation.GymnasiumPufferEnv(env=env, buf=buf)

def make_password(password_length=5, buf=None, **kwargs):
    from . import sanity
    env = sanity.Password(password_length=password_length)
    env = pufferlib.EpisodeStats(env)
    return pufferlib.emulation.GymnasiumPufferEnv(env=env, buf=buf)

def make_performance(delay_mean=0, delay_std=0, bandwidth=1, buf=None, **kwargs):
    from . import sanity
    env = sanity.Performance(delay_mean=delay_mean, delay_std=delay_std, bandwidth=bandwidth)
    env = pufferlib.EpisodeStats(env)
    return pufferlib.emulation.GymnasiumPufferEnv(env=env, buf=buf)

def make_performance_empiric(count_n=0, count_std=0, bandwidth=1, buf=None, **kwargs):
    from . import sanity
    env = sanity.PerformanceEmpiric(count_n=count_n, count_std=count_std, bandwidth=bandwidth)
    env = pufferlib.EpisodeStats(env)
    return pufferlib.emulation.GymnasiumPufferEnv(env=env, buf=buf)

def make_stochastic(p=0.7, horizon=100, buf=None, **kwargs):
    from . import sanity
    env = sanity.Stochastic(p=p, horizon=100)
    env = pufferlib.EpisodeStats(env)
    return pufferlib.emulation.GymnasiumPufferEnv(env=env, buf=buf)

def make_spaces(buf=None, **kwargs):
    from . import sanity
    env = sanity.Spaces()
    env = pufferlib.EpisodeStats(env)
    return pufferlib.emulation.GymnasiumPufferEnv(env=env, buf=buf, **kwargs)

def make_multiagent(buf=None, **kwargs):
    from . import sanity
    env = sanity.Multiagent()
    env = pufferlib.MultiagentEpisodeStats(env)
    return pufferlib.emulation.PettingZooPufferEnv(env=env, buf=buf)

MAKE_FUNCTIONS = {
    'battle': 'Battle',
    'breakout': 'Breakout',
    'blastar': 'Blastar',
    'convert': 'Convert',
    'convert_circle': 'ConvertCircle',
    'pong': 'Pong',
    'freeway': 'Freeway',
    'enduro': 'Enduro',
    'tetris': 'Tetris',
    'cartpole': 'Cartpole',
    'moba': 'Moba',
    'matsci': 'Matsci',
    'memory': 'Memory',
    'boids': 'Boids',
    'drone_race': 'DroneRace',
    'drone_swarm': 'DroneSwarm',
    'nmmo3': 'NMMO3',
    'snake': 'Snake',
    'squared': 'Squared',
    'pysquared': 'PySquared',
    'connect4': 'Connect4',
    'g2048': 'G2048',
    'terraform': 'Terraform',
    'template': 'Template',
    'tripletriad': 'TripleTriad',
    'tactical': 'Tactical',
    'target': 'Target',
    'go': 'Go',
    'rware': 'Rware',
    'trash_pickup': 'TrashPickupEnv',
    'tower_climb': 'TowerClimb',
    'grid': 'Grid',
    'shared_pool': 'PyCPR',
    'impulse_wars': 'ImpulseWars',
    'drive': 'Drive',
    'pacman': 'Pacman',
    'tmaze': 'TMaze',
    'checkers': 'Checkers',
    'asteroids': 'Asteroids',
    'whisker_racer': 'WhiskerRacer',
    'onestateworld': 'World',
    'chain_mdp': 'Chain',
    'spaces': make_spaces,
    'multiagent': make_multiagent,
    'slimevolley': 'SlimeVolley',
}

def env_creator(name='squared', *args, **kwargs):
    if 'puffer_' not in name:
        raise pufferlib.APIUsageError(f'Invalid environment name: {name}')

    # TODO: Robust sanity / ocean imports
    name = name.replace('puffer_', '')
    try:
        module = importlib.import_module(f'pufferlib.ocean.{name}.{name}')
        return getattr(module, MAKE_FUNCTIONS[name])
    except ModuleNotFoundError:
        return MAKE_FUNCTIONS[name]



================================================
FILE: pufferlib/ocean/render.py
================================================
import numpy as np
import os

from cffi import FFI
from raylib import rl, colors
import pyray

PUFF_BACKGROUND = [6, 24, 24, 255]
PUFF_TEXT = [0, 187, 187, 255]

ANSI_COLORS = [30, 34, 36, 90, 31, 97, 91, 37]

COLORS = np.array([
    [6, 24, 24 ],     # Background
    [0, 0, 255],     # Food
    [0, 128, 255],   # Corpse
    [128, 128, 128], # Wall
    [255, 0, 0],     # Snake
    [255, 255, 255], # Snake
    [255, 85, 85],     # Snake
    [170, 170, 170], # Snake
], dtype=np.uint8)


def any_key_down(keys):
    for key in keys:
        if rl.IsKeyDown(key):
            return True
    return False

def any_key_pressed(keys):
    for key in keys:
        if rl.IsKeyPressed(key):
            return True
    return False

def cdata_to_numpy():
    image = rl.LoadImageFromScreen()
    data_pointer = image.data
    width = image.width
    height = image.height
    channels = 4
    data_size = width * height * channels
    cdata = FFI().buffer(data_pointer, data_size)
    return np.frombuffer(cdata, dtype=np.uint8
        ).reshape((height, width, channels))

def make_texture(width, height):
    rendered = np.zeros((height, width, 4), dtype=np.uint8)
    raylib_image = pyray.Image(FFI().from_buffer(rendered.data),
        width, height, 1, pyray.PIXELFORMAT_UNCOMPRESSED_R8G8B8)
    return rl.LoadTextureFromImage(raylib_image)

class AnsiRender:
    def __init__(self, colors=None):
        self.colors = colors
        if colors is None:
            self.colors = ANSI_COLORS

    def render(self, grid):
        frame = ''
        for v in range(grid.shape[0]):
            lines = []
            for line in grid[v-1:-v, v-1:-v]:
                lines.append(''.join([
                    f'\033[{ANSI_COLORS[val]}m██\033[0m' for val in line]))

            frame = '\n'.join(lines)

        return frame
 
class RGBArrayRender:
    def __init__(self, colors=None, upscale=1):
        self.colors = colors
        if colors is None:
            self.colors = COLORS

        self.rescaler = np.ones((upscale, upscale, 1), dtype=np.uint8)
        self.upscale = upscale

    def render(self, grid):
        frame = self.colors[grid]

        if self.upscale > 1:
            frame = np.kron(frame, self.rescaler)

        return frame

class GridRender:
    def __init__(self, width, height, screen_width=1080, screen_height=720,
            colors=None, fps=60, name='PufferLib Raylib Renderer'):
        self.width = width
        self.height = height
        self.fps = fps

        self.colors = colors
        if colors is None:
            self.colors = COLORS

        rl.InitWindow(screen_width, screen_height, name.encode())
        rl.SetTargetFPS(fps)
        self.width = width
        self.height = height

        camera = pyray.Camera2D()
        camera.target= (width/2, height/2)
        camera.rotation = 0.0 
        camera.zoom = min(screen_width/width, screen_height/height)
        self.camera = camera

        self.speed = min(screen_width, screen_height) / 100
        self.texture = make_texture(width, height)

        self.show_help = False
        self.screen_width = screen_width
        self.screen_height = screen_height

    def render(self, grid, *args, end_drawing=True):
        assert grid.shape[0] == self.height
        assert grid.shape[1] == self.width
        rendered = self.colors[grid]

        if rl.IsKeyDown(rl.KEY_ESCAPE):
            exit(0)

        screen_width = rl.GetScreenWidth()
        screen_height = rl.GetScreenHeight()

        camera = self.camera
        camera.offset.x = screen_width/2
        camera.offset.y = screen_height/2

        fps = rl.GetFPS() or self.fps
        fps_mul = self.fps / fps
        speed = self.speed * fps_mul
        zoom_speed = 0.01 * fps_mul

        if any_key_down([rl.KEY_SPACE]):
            camera.zoom = min(screen_width/self.width, screen_height/self.height)
            camera.target.x = self.width/2
            camera.target.y = self.height/2

        if any_key_down([rl.KEY_LEFT_SHIFT]):
            speed *= 3
            zoom_speed *= 3

        speed = speed / camera.zoom

        if any_key_down([rl.KEY_UP, rl.KEY_W]):
            camera.target.y -= speed
        if any_key_down([rl.KEY_DOWN, rl.KEY_S]):
            camera.target.y += speed
        if any_key_down([rl.KEY_LEFT, rl.KEY_A]):
            camera.target.x -= speed
        if any_key_down([rl.KEY_RIGHT, rl.KEY_D]):
            camera.target.x += speed
        if any_key_down([rl.KEY_Q, rl.KEY_MINUS]):
            camera.zoom /= 1 + zoom_speed
        if any_key_down([rl.KEY_E, rl.KEY_EQUAL]):
            camera.zoom *= 1 + zoom_speed

        if any_key_pressed([rl.KEY_TAB, rl.KEY_GRAVE]):
            self.show_help = not self.show_help

        rl.BeginDrawing()
        rl.BeginMode2D(self.camera)
        rl.ClearBackground(PUFF_BACKGROUND)
        rl.UpdateTexture(self.texture, rendered.tobytes())
        rl.DrawTextureEx(self.texture, (0, 0), 0, 1, colors.WHITE)
        rl.EndMode2D()
        if self.show_help:
            # Stats
            rl.DrawText(f'FPS: {fps}'.encode(), 10, 10, 20, PUFF_TEXT)
            rl.DrawText(f'Zoom: {camera.zoom:.2f}'.encode(), 10, 30, 20, PUFF_TEXT)
            rl.DrawText(f'X: {camera.offset.x:.2f}'.encode(), 10, 50, 20, PUFF_TEXT)
            rl.DrawText(f'Y: {camera.offset.y:.2f}'.encode(), 10, 70, 20, PUFF_TEXT)
            rl.DrawText(f'Speed: {speed:.2f}'.encode(), 10, 90, 20, PUFF_TEXT)

            # Controls
            rl.DrawText('Move: WASD/HJKL'.encode(), 10, 120, 20, PUFF_TEXT)
            rl.DrawText('Zoom: QE/-+'.encode(), 10, 140, 20, PUFF_TEXT)
            rl.DrawText('Turbo: Shift'.encode(), 10, 160, 20, PUFF_TEXT)
            rl.DrawText('Help: Tab/~'.encode(), 10, 180, 20, PUFF_TEXT)
            rl.DrawText('Reset: Space'.encode(), 10, 200, 20, PUFF_TEXT)

        if end_drawing:
            rl.EndDrawing()

        return cdata_to_numpy()

class GameRender:
    def __init__(self, width, height, screen_width=1080, screen_height=720,
            colors=None, name='PufferLib Raylib Game'):
        self.client = GridRender(width, height,
            screen_width, screen_height, colors, name)

    def render(self, grid, x, y):
        self.client.camera.target.x = x
        self.client.camera.target.y = y
        return self.client.render(grid)

class TestGameRender:
    def __init__(self, width, height, colors=None,
            tile_size=16, name='PufferLib Raylib Game'):
        assert width % tile_size == 0
        assert height % tile_size == 0
        assert (width // tile_size) % 2 == 1
        assert (height // tile_size) % 2 == 1

        self.width = width
        self.height = height

        self.colors = colors
        if colors is None:
            self.colors = COLORS

        self.x_tiles = width // tile_size
        self.y_tiles = height // tile_size

        rl.InitWindow(width, height, name.encode())
        rl.SetTargetFPS(60)

    def render(self, grid, agent_positions):
        action = None
        if rl.IsKeyDown(rl.KEY_UP) or rl.IsKeyDown(rl.KEY_W):
            action = 0
        elif rl.IsKeyDown(rl.KEY_DOWN) or rl.IsKeyDown(rl.KEY_S):
            action = 1
        elif rl.IsKeyDown(rl.KEY_LEFT) or rl.IsKeyDown(rl.KEY_A):
            action = 2
        elif rl.IsKeyDown(rl.KEY_RIGHT) or rl.IsKeyDown(rl.KEY_D):
            action = 3

        rl.BeginDrawing()
        rl.ClearBackground(PUFF_BACKGROUND)

        main_y, main_x = agent_positions[0]
        dx = self.x_tiles // 2
        dy = self.y_tiles // 2


if __name__ == '__main__':
    renderer = GridRender(256, 256)
    grid = np.random.randint(0, 3, (256, 256), dtype=np.uint8)
    while True:
        frame = renderer.render(grid)




================================================
FILE: pufferlib/ocean/sanity.py
================================================
import gymnasium
import pettingzoo
import numpy as np
import random
import time


class Bandit(gymnasium.Env):
    '''Pufferlib Bandit environment

    Simulates a classic multiarmed bandit problem.

    Observation space: Box(0, 1, (1,)). The observation is always 1.
    Action space: Discrete(num_actions). Which arm to pull.
    Args:
        num_actions: The number of bandit arms
        reward_scale: The scale of the reward
        reward_noise: The standard deviation of the reward signal
        hard_fixed_seed: All instances of the environment should share the same seed.
    '''
    def __init__(self, num_actions=4, reward_scale=1,
            reward_noise=0, hard_fixed_seed=42):
        self.num_actions = num_actions
        self.reward_scale = reward_scale
        self.reward_noise = reward_noise
        self.hard_fixed_seed = hard_fixed_seed
        self.observation=np.ones(1, dtype=np.float32)
        self.observation_space=gymnasium.spaces.Box(
            low=-1, high=1, shape=(1,))
        self.action_space=gymnasium.spaces.Discrete(num_actions)
        self.render_mode = 'ansi'

    def reset(self, seed=None):
        # Bandit problem requires a single fixed seed
        # for all environments
        seed = self.hard_fixed_seed

        if seed is not None:
            random.seed(seed)
            np.random.seed(seed)

        self.solution_idx = np.random.randint(0, self.num_actions)

        return self.observation, {}

    def step(self, action):
        assert action == int(action) and action >= 0 and action < self.num_actions

        correct = False
        reward = 0
        if action == self.solution_idx:
            correct = True
            reward = 1

        reward_noise = 0
        if self.reward_noise != 0:
            reward_noise = np.random.randn() * self.reward_scale

        # Couples reward noise to scale
        reward = (reward + reward_noise) * self.reward_scale

        return self.observation, reward, True, False, {'score': correct}

class Memory(gymnasium.Env):
    '''Pufferlib Memory environment

    Repeat the observed sequence after a delay. It is randomly generated upon every reset. This is a test of memory length and capacity. It starts requiring credit assignment if you make the sequence too long.

    The sequence is presented one digit at a time, followed by a string of 0. The agent should output 0s for the first mem_length + mem_delay steps, then output the sequence.

    Observation space: Box(0, 1, (1,)). The current digit.
    Action space: Discrete(2). Your guess for the next digit.

    Args:
        mem_length: The length of the sequence
        mem_delay: The number of 0s between the sequence and the agent's response
    '''
    def __init__(self, mem_length=1, mem_delay=0):
        self.mem_length = mem_length
        self.mem_delay = mem_delay
        self.horizon = 2 * mem_length + mem_delay
        self.observation_space=gymnasium.spaces.Box(
            low=-1, high=1, shape=(1,))
        self.action_space=gymnasium.spaces.Discrete(2)
        self.render_mode = 'ansi'

    def reset(self, seed=None):
        if seed is not None:
            random.seed(seed)
            np.random.seed(seed)

        self.solution = np.random.randint(0, 2, size=self.horizon).astype(np.float32)
        self.solution[-(self.mem_length + self.mem_delay):] = -1
        self.submission = np.zeros(self.horizon) - 1
        self.tick = 1

        return self.solution[0], {}

    def step(self, action):
        assert self.tick < self.horizon
        assert action in (0, 1)

        ob = reward = 0.0

        if self.tick < self.mem_length:
            ob = self.solution[self.tick]
            reward = float(action == 0)

        if self.tick >= self.mem_length + self.mem_delay:
            idx = self.tick - self.mem_length - self.mem_delay
            sol = self.solution[idx]
            reward = float(action == sol)
            self.submission[self.tick] = action

        self.tick += 1
        terminal = self.tick == self.horizon

        info = {}
        if terminal:
            info['score'] = np.all(
                self.solution[:self.mem_length] == self.submission[-self.mem_length:])

        return ob, reward, terminal, False, info

    def render(self):
        def _render(val):
            if val == 1:
                c = 94
            elif val == 0:
                c = 91
            else:
                c = 90
            return f'\033[{c}m██\033[0m'

        chars = []
        for val in self.solution:
            c = _render(val)
            chars.append(c)
        chars.append(' Solution\n')

        for val in self.submission:
            c = _render(val)
            chars.append(c)
        chars.append(' Prediction\n')

        return ''.join(chars)


class Multiagent(pettingzoo.ParallelEnv):
    '''Pufferlib Multiagent environment

    Agent 1 must pick action 0 and Agent 2 must pick action 1

    Observation space: Box(0, 1, (1,)). 0 for Agent 1 and 1 for Agent 2
    Action space: Discrete(2). Which action to take.
    '''
    def __init__(self):
        self.observation = {
            1: np.zeros(1, dtype=np.float32),
            2: np.ones(1, dtype=np.float32),
        }
        self.terminal = {
            1: True,
            2: True,
        }
        self.truncated = {
            1: False,
            2: False,
        }
        self.possible_agents=[1, 2]
        self.agents=[1, 2]
        self.render_mode = 'ansi'

    def observation_space(self, agent):
        return gymnasium.spaces.Box(
            low=0, high=1, shape=(1,))

    def action_space(self, agent):
        return gymnasium.spaces.Discrete(2)

    def reset(self, seed=None):
        # Reallocating is faster than zeroing
        self.view=np.zeros((2, 5), dtype=np.float32)
        return self.observation, {}

    def step(self, action):
        reward = {}
        assert 1 in action and action[1] in (0, 1)
        if action[1] == 0:
            self.view[0, 2] = 1
            reward[1] = 1
        else:
            self.view[0, 0] = 1
            reward[1] = 0

        assert 2 in action and action[2] in (0, 1)
        if action[2] == 1:
            self.view[1, 2] = 1
            reward[2] = 1
        else:
            self.view[1, 4] = 1
            reward[2] = 0

        info = {
            1: {'score': reward[1]},
            2: {'score': reward[2]},
        }
        return self.observation, reward, self.terminal, self.truncated, info

    def render(self):
        def _render(val):
            if val == 1:
                c = 94
            elif val == 0:
                c = 90
            else:
                c = 90
            return f'\033[{c}m██\033[0m'

        chars = []
        for row in self.view:
            for val in row:
                c = _render(val)
                chars.append(c)
            chars.append('\n')
        return ''.join(chars)

class Password(gymnasium.Env):
    '''Pufferlib Password environment

    Guess the password, which is a static binary string. Your policy has to
    not determinize before it happens to get the reward, and it also has to
    latch onto the reward within a few instances of getting it. 

    Observation space: Box(0, 1, (password_length,)). A binary vector containing your guesses so far, so that the environment will be solvable without memory.
    Action space: Discrete(2). Your guess for the next digit.

    Args:
        password_length: The number of binary digits in the password.
        hard_fixed_seed: A fixed seed for the environment. It should be the same for all instances. This environment does not make sense when randomly generated.
    '''
 
    def __init__(self, password_length=5, hard_fixed_seed=42):
        self.password_length = password_length
        self.hard_fixed_seed = hard_fixed_seed
        self.observation_space=gymnasium.spaces.Box(
            low=-1, high=1, shape=(password_length,))
        self.action_space=gymnasium.spaces.Discrete(2)
        self.render_mode = 'ansi'

    def reset(self, seed=None):
        # Bandit problem requires a single fixed seed
        # for all environments
        seed = self.hard_fixed_seed
        if seed is not None:
            random.seed(seed)
            np.random.seed(seed)

        self.observation = np.zeros(self.password_length, dtype=np.float32) - 1
        self.solution = np.random.randint(
            0, 2, size=self.password_length).astype(np.float32)
        self.tick = 0

        return self.observation, {}

    def step(self, action):
        assert self.tick < self.password_length
        assert action in (0, 1)

        self.observation[self.tick] = action
        self.tick += 1

        reward = 0
        terminal = self.tick == self.password_length
        info = {}

        if terminal:
            reward = float(np.all(self.observation == self.solution))
            info['score'] = reward

        return self.observation, reward, terminal, False, info

    def render(self):
        def _render(val):
            if val == 1:
                c = 94
            elif val == 0:
                c = 91
            else:
                c = 90
            return f'\033[{c}m██\033[0m'

        chars = []
        for val in self.solution:
            c = _render(val)
            chars.append(c)
        chars.append(' Solution\n')

        for val in self.observation:
            c = _render(val)
            chars.append(c)
        chars.append(' Prediction\n')

        return ''.join(chars)

class Performance(gymnasium.Env):
    def __init__(self, delay_mean=0, delay_std=0, bandwidth=1):
        np.random.seed(time.time_ns() % 2**32)

        self.observation_space = gymnasium.spaces.Box(
            low=-2**20, high=2**20,
            shape=(bandwidth,), dtype=np.float32
        )
        self.action_space = gymnasium.spaces.Discrete(2)
        self.observation = self.observation_space.sample()
        self.render_mode = 'ansi'

    def reset(self, seed=None):
        return self.observation, {}

    def step(self, action):
        start = time.process_time()
        idx = 0
        target_time = self.delay_mean + self.delay_std*np.random.randn()
        while time.process_time() - start < target_time:
            idx += 1

        return self.observation, 0, False, False, {}

class PerformanceEmpiric(gymnasium.Env):
    def __init__(self, count_n=0, count_std=0, bandwidth=1):
        np.random.seed(time.time_ns() % 2**32)

        self.observation_space = gymnasium.spaces.Box(
            low=-2**20, high=2**20,
            shape=(bandwidth,), dtype=np.float32
        )
        self.action_space = gymnasium.spaces.Discrete(2)
        self.observation = self.observation_space.sample()
        self.count_n = count_n
        self.count_std = count_std
        self.bandwidth = bandwidth
        self.render_mode = 'ansi'

    def reset(self, seed=None):
        return self.observation, {}

    def step(self, action):
        idx = 0
        target = self.count_n  +  self.count_std * np.random.randn()
        while idx < target:
            idx += 1

        return self.observation, 0, False, False, {}

class Spaces(gymnasium.Env):
    '''Pufferlib Spaces environment

    A simple environment with hierarchical observation and action spaces

    The image action should be 1 if the sum of the image is positive, 0 otherwise
    The flat action should be 1 if the sum of the flat obs is positive, 0 otherwise

    0.5 reward is given for each correct action

    Does not provide rendering
    '''
    def __init__(self):
        self.observation_space = gymnasium.spaces.Dict({
            'image': gymnasium.spaces.Box(
                low=0, high=1, shape=(5, 5), dtype=np.float32),
            'flat': gymnasium.spaces.Box(
                low=-1, high=1, shape=(5,), dtype=np.int8),
        })
        self.action_space = gymnasium.spaces.Dict({
            'image': gymnasium.spaces.Discrete(2),
            'flat': gymnasium.spaces.Discrete(2),
        })
        self.render_mode = 'ansi'

    def reset(self, seed=None):
        self.observation = {
            'image': np.random.rand(5, 5).astype(np.float32),
            'flat': np.random.randint(-1, 2, (5,), dtype=np.int8),
        }
        self.image_sign = np.sum(self.observation['image']) > 0
        self.flat_sign = np.sum(self.observation['flat']) > 0

        return self.observation, {}

    def step(self, action):
        assert isinstance(action, dict)
        assert 'image' in action and action['image'] in (0, 1)
        assert 'flat' in action and action['flat'] in (0, 1)

        reward = 0
        if self.image_sign == action['image']:
            reward += 0.5

        if self.flat_sign == action['flat']:
            reward += 0.5

        info = dict(score=reward)
        return self.observation, reward, True, False, info

class Squared(gymnasium.Env):
    '''Pufferlib Squared environment

    Agent starts at the center of a square grid.
    Targets are placed on the perimeter of the grid.
    Reward is 1 minus the L-inf distance to the closest target.
    This means that reward varies from -1 to 1.
    Reward is not given for targets that have already been hit.

    Observation space: Box(-1, 1, (grid_size, grid_size)). The map.
    Action space: Discrete(8). Which direction to move.

    Args:
        distance_to_target: The distance from the center to the closest target.
        num_targets: The number of targets to randomly generate.
 
    '''

    MOVES = [(0, -1), (0, 1), (-1, 0), (1, 0), (1, -1), (-1, -1), (1, 1), (-1, 1)]

    def __init__(self,
        distance_to_target=1,
        num_targets=-1,
        ):
        grid_size = 2 * distance_to_target + 1
        if num_targets == -1:
            num_targets = 4 * distance_to_target

        self.distance_to_target = distance_to_target
        self.possible_targets = self._all_possible_targets(grid_size)
        self.num_targets = num_targets
        self.grid_size = grid_size
        self.max_ticks = num_targets * distance_to_target
        self.observation_space = gymnasium.spaces.Box(
            low=-1, high=1, shape=(grid_size, grid_size))
        self.action_space = gymnasium.spaces.Discrete(8)
        self.render_mode = 'ansi'

    def _all_possible_targets(self, grid_size):
        return [(x, y) for x in range(grid_size) for y in range(grid_size)
                if x == 0 or y == 0 or x == grid_size - 1 or y == grid_size - 1]

    def reset(self, seed=None):
        if seed is not None:
            random.seed(seed)
            np.random.seed(seed)

        # Allocating a new grid is faster than resetting an old one
        self.grid = np.zeros((self.grid_size, self.grid_size), dtype=np.float32)
        self.grid[self.distance_to_target, self.distance_to_target] = -1
        self.agent_pos = (self.distance_to_target, self.distance_to_target)
        self.tick = 0

        self.targets = random.sample(self.possible_targets, self.num_targets)
        for x, y in self.targets:
            self.grid[x, y] = 1

        return self.grid, {}

    def step(self, action):
        x, y = self.agent_pos
        self.grid[x, y] = 0

        dx, dy = Squared.MOVES[action]
        x += dx
        y += dy

        min_dist = min([max(abs(x-tx), abs(y-ty)) for tx, ty in self.targets])
        # This reward function will return 0.46 average reward for an unsuccessful
        # episode with distance_to_target=4 and num_targets=1 (0.5 for solve)
        # It looks reasonable but is not very discriminative
        reward = 1 - min_dist / self.distance_to_target

        # This reward function will return 1 when the agent moves in the right direction
        # (plus an adjustment for the 0 reset reward) to average 1 for success
        # It is not much better than the previous one.
        #reward = state.distance_to_target - min_dist - state.tick + 1/state.max_ticks

        # This function will return 0, 0.2, 0.4, ... 1 for successful episodes (n=5)
        # And will drop rewards to 0 or less as soon as an error is made
        # Somewhat smoother but actually worse than the previous ones
        # reward = (state.distance_to_target - min_dist - state.tick) / (state.max_ticks - state.tick)


        # This one nicely tracks the task completed metric but does not optimize well
        #if state.distance_to_target - min_dist - state.tick  == 1:
        #    reward = 1
        #else:
        #    reward = -state.tick

        if (x, y) in self.targets:
            self.targets.remove((x, y))
            #state.grid[x, y] = 0

        dist_from_origin = max(abs(x-self.distance_to_target), abs(y-self.distance_to_target))
        if dist_from_origin >= self.distance_to_target:
            self.agent_pos = self.distance_to_target, self.distance_to_target
        else:
            self.agent_pos = x, y
        
        self.grid[self.agent_pos] = -1
        self.tick += 1

        done = self.tick >= self.max_ticks
        score = (self.num_targets - len(self.targets)) / self.num_targets
        info = {'score': score} if done else {}

        return self.grid, reward, done, False, info

    def render(self):
        chars = []
        for row in self.grid:
            for val in row:
                if val == 1:
                    color = 94
                elif val == -1:
                    color = 91
                else:
                    color = 90
                chars.append(f'\033[{color}m██\033[0m')
            chars.append('\n')
        return ''.join(chars)

class Stochastic(gymnasium.Env):
    '''Pufferlib Stochastic environment

    The optimal policy is to play action 0 < p % of the time and action 1 < (1 - p) %
    This is a test of whether your algorithm can learn a nontrivial stochastic policy.
    Do not use a policy with memory, as that will trivialize the problem.

    Observation space: Box(0, 1, (1,)). The observation is always 0.
    Action space: Discrete(2). Select action 0 or action 1.

    Args:
        p: The optimal probability for action 0
        horizon: How often the environment should reset
    '''
    def __init__(self, p=0.75, horizon=1000):
        self.p = p
        self.horizon = horizon
        self.observation_space = gymnasium.spaces.Box(
            low=0, high=1, shape=(1,))
        self.action_space = gymnasium.spaces.Discrete(2)
        self.render_mode = 'ansi'

    def reset(self, seed=None):
        if seed is not None:
            random.seed(seed)
            np.random.seed(seed)

        self.tick = 0
        self.count = 0
        self.action = 0

        return np.zeros(1, dtype=np.float32), {}

    def step(self, action):
        assert self.tick < self.horizon
        assert action in (0, 1)

        self.tick += 1
        self.count += action == 0
        self.action = action

        terminal = self.tick == self.horizon
        atn0_frac = self.count / self.tick
        proximity_to_p = 1 - (self.p - atn0_frac)**2

        reward = proximity_to_p if (
            (action == 0 and atn0_frac < self.p) or
            (action == 1 and atn0_frac >= self.p)) else 0

        info = {}
        if terminal:
            info['score'] = proximity_to_p

        return np.zeros(1, dtype=np.float32), reward, terminal, False, info

    def render(self):
        def _render(val):
            if val == 1:
                c = 94
            elif val == 0:
                c = 91
            else:
                c = 90
            return f'\033[{c}m██\033[0m'
        chars = []
        if self.tick == 0:
            solution = 0
        else:
            solution = 0 if self.count / self.tick < self.p else 1
        chars.append(_render(solution))
        chars.append(' Solution\n')

        chars.append(_render(self.action))
        chars.append(' Prediction\n')

        return ''.join(chars)

class Continuous(gymnasium.Env):
    def __init__(self, discretize=False):
        self.observation_space=gymnasium.spaces.Box(
            low=-1, high=1, shape=(6,))
        self.discretize = discretize
        if discretize:
            self.action_space=gymnasium.spaces.Discrete(4)
        else:
            self.action_space=gymnasium.spaces.Box(
                low=-1, high=1, shape=(2,))

        self.render_mode = 'human'
        self.client = None

    def reset(self, seed=None, options=None):
        # pos_x, pos_y, vel_x, vel_y, target_x, target_y
        self.state = 2*np.random.rand(6)-1
        self.state[2:4] = 0
        self.tick = 0

        return self.state, {}

    def step(self, action):
        if self.discretize:
            accel_x, accel_y = 0, 0
            if action == 0:
                accel_x = -0.1
            elif action == 1:
                accel_x = 0.1
            elif action == 2:
                accel_y = -0.1
            elif action == 3:
                accel_y = 0.1
        else:
            accel_x, accel_y = 0.1*action

        self.state[2] += accel_x
        self.state[3] += accel_y
        self.state[0] += self.state[2]
        self.state[1] += self.state[3]

        pos_x, pos_y, vel_x, vel_y, target_x, target_y = self.state

        if pos_x < -1 or pos_x > 1 or pos_y < -1 or pos_y > 1:
            return self.state, -1, True, False, {'score': 0}

        dist = np.sqrt((pos_x - target_x)**2 + (pos_y - target_y)**2)
        reward = 0.02 * (1 - dist)

        self.tick += 1
        done = dist < 0.1
        truncated = self.tick >= 100

        # TODO: GAE implementation making agent not hit target
        # without a big reward here
        info = {}
        if done:
            reward = 5.0
            info = {'score': 1}
        elif truncated:
            reward = 0.0
            info = {'score': 0}

        return self.state, reward, done, truncated, info

    def render(self):
        if self.client is None:
            self.client = RaylibClient()

        pos_x, pos_y, vel_x, vel_y, target_x, target_y = self.state
        frame, atn = self.client.render(pos_x, pos_y, target_x, target_y)
        return frame

class RaylibClient:
    def __init__(self, width=1080, height=720, size=20):
        self.width = width
        self.height = height
        self.size = size

        from raylib import rl
        rl.InitWindow(width, height,
            "PufferLib Simple Continuous".encode())
        rl.SetTargetFPS(10)
        self.rl = rl

        from cffi import FFI
        self.ffi = FFI()

    def _cdata_to_numpy(self):
        image = self.rl.LoadImageFromScreen()
        width, height, channels = image.width, image.height, 4
        cdata = self.ffi.buffer(image.data, width*height*channels)
        return np.frombuffer(cdata, dtype=np.uint8
            ).reshape((height, width, channels))[:, :, :3]

    def render(self, pos_x, pos_y, target_x, target_y):
        rl = self.rl
        action = None
        if rl.IsKeyDown(rl.KEY_UP) or rl.IsKeyDown(rl.KEY_W):
            action = 0
        elif rl.IsKeyDown(rl.KEY_DOWN) or rl.IsKeyDown(rl.KEY_S):
            action = 1
        elif rl.IsKeyDown(rl.KEY_LEFT) or rl.IsKeyDown(rl.KEY_A):
            action = 2
        elif rl.IsKeyDown(rl.KEY_RIGHT) or rl.IsKeyDown(rl.KEY_D):
            action = 3

        rl.BeginDrawing()
        rl.ClearBackground([6, 24, 24, 255])

        pos_x = int((0.5+pos_x/2) * self.width)
        pos_y = int((0.5+pos_y/2) * self.height)
        target_x = int((0.5+target_x/2) * self.width)
        target_y = int((0.5+target_y/2) * self.height)

        rl.DrawCircle(pos_x, pos_y, self.size, [255, 0, 0, 255])
        rl.DrawCircle(target_x, target_y, self.size, [0, 0, 255, 255])

        rl.EndDrawing()
        return self._cdata_to_numpy(), action



================================================
FILE: pufferlib/ocean/torch.py
================================================
from types import SimpleNamespace
from typing import Any, Tuple

from gymnasium import spaces

from torch import nn
import torch
from torch.distributions.normal import Normal
from torch import nn
import torch.nn.functional as F

import pufferlib
import pufferlib.models

from pufferlib.models import Default as Policy
from pufferlib.models import Convolutional as Conv
Recurrent = pufferlib.models.LSTMWrapper
from pufferlib.pytorch import layer_init, _nativize_dtype, nativize_tensor
import numpy as np


class Boids(nn.Module):
    def __init__(self, env, cnn_channels=32, hidden_size=128, **kwargs):
        super().__init__()
        self.hidden_size = hidden_size
        self.is_continuous = False
        self.network = nn.Sequential(
            pufferlib.pytorch.layer_init(nn.Linear(4, hidden_size)),
            nn.GELU(),
            pufferlib.pytorch.layer_init(nn.Linear(hidden_size, hidden_size)),
        )
        self.action_vec = tuple(env.single_action_space.nvec)
        self.actor = pufferlib.pytorch.layer_init(
            nn.Linear(hidden_size, sum(self.action_vec)), std=0.01)
        self.value_fn = pufferlib.pytorch.layer_init(
            nn.Linear(hidden_size, 1), std=1)

    def forward(self, observations, state=None):
        hidden = self.encode_observations(observations)
        actions, value = self.decode_actions(hidden)
        return actions, value

    def forward_train(self, x, state=None):
        return self.forward(x, state)

    def encode_observations(self, observations, state=None):
        batch, n, = observations.shape
        return self.network(observations.reshape(batch, n//4, 4)).max(dim=1)[0]

    def decode_actions(self, flat_hidden, state=None):
        value = self.value_fn(flat_hidden)
        action = self.actor(flat_hidden).split(self.action_vec, dim=1)
        return action, value

class NMMO3LSTM(pufferlib.models.LSTMWrapper):
    def __init__(self, env, policy, input_size=512, hidden_size=512):
        super().__init__(env, policy, input_size, hidden_size)

class NMMO3(nn.Module):
    def __init__(self, env, hidden_size=512, output_size=512, **kwargs):
        super().__init__()
        self.hidden_size = hidden_size
        #self.dtype = pufferlib.pytorch.nativize_dtype(env.emulated)
        self.num_actions = env.single_action_space.n
        self.factors = np.array([4, 4, 17, 5, 3, 5, 5, 5, 7, 4])
        offsets = torch.tensor([0] + list(np.cumsum(self.factors)[:-1])).view(1, -1, 1, 1)
        self.register_buffer('offsets', offsets)
        self.cum_facs = np.cumsum(self.factors)

        self.multihot_dim = self.factors.sum()
        self.is_continuous = False

        self.map_2d = nn.Sequential(
            pufferlib.pytorch.layer_init(nn.Conv2d(self.multihot_dim, 128, 5, stride=3)),
            nn.ReLU(),
            pufferlib.pytorch.layer_init(nn.Conv2d(128, 128, 3, stride=1)),
            nn.Flatten(),
        )

        self.player_discrete_encoder = nn.Sequential(
            nn.Embedding(128, 32),
            nn.Flatten(),
        )
        self.proj = nn.Sequential(
            pufferlib.pytorch.layer_init(nn.Linear(1817, hidden_size)),
            nn.ReLU(),
        )

        self.layer_norm = nn.LayerNorm(hidden_size)
        self.actor = pufferlib.pytorch.layer_init(
            nn.Linear(output_size, self.num_actions), std=0.01)
        self.value_fn = pufferlib.pytorch.layer_init(nn.Linear(output_size, 1), std=1)

    def forward(self, x, state=None):
        hidden = self.encode_observations(x)
        actions, value = self.decode_actions(hidden)
        return actions, value

    def forward_train(self, x, state=None):
        return self.forward(x, state)

    def encode_observations(self, observations, state=None):
        batch = observations.shape[0]
        ob_map = observations[:, :11*15*10].view(batch, 11, 15, 10)
        ob_player = observations[:, 11*15*10:-10]
        ob_reward = observations[:, -10:]

        batch = ob_map.shape[0]
        map_buf = torch.zeros(batch, 59, 11, 15, dtype=torch.float32, device=observations.device)
        codes = ob_map.permute(0, 3, 1, 2) + self.offsets
        map_buf.scatter_(1, codes, 1)
        ob_map = self.map_2d(map_buf)

        player_discrete = self.player_discrete_encoder(ob_player.int())

        obs = torch.cat([ob_map, player_discrete, ob_player.to(ob_map.dtype), ob_reward], dim=1)
        obs = self.proj(obs)
        return obs

    def decode_actions(self, flat_hidden):
        flat_hidden = self.layer_norm(flat_hidden)
        action = self.actor(flat_hidden)
        value = self.value_fn(flat_hidden)
        return action, value

class Terraform(nn.Module):
    def __init__(self, env, cnn_channels=32, hidden_size=128, **kwargs):
        super().__init__()
        self.hidden_size = hidden_size
        self.is_continuous = False

        self.local_net_2d = nn.Sequential(
            pufferlib.pytorch.layer_init(
                nn.Conv2d(2, cnn_channels, 5, stride=3)),
            nn.ReLU(),
            pufferlib.pytorch.layer_init(
                nn.Conv2d(cnn_channels, cnn_channels, 3, stride=1)),
            nn.ReLU(),
            nn.Flatten(),
        )

        self.global_net_2d = nn.Sequential(
            pufferlib.pytorch.layer_init(
                nn.Conv2d(2, cnn_channels, 3, stride=1)),
            nn.ReLU(),
            pufferlib.pytorch.layer_init(
                nn.Conv2d(cnn_channels, cnn_channels, 3, stride=1)),
            nn.ReLU(),
            nn.Flatten(),
        )

        self.net_1d = nn.Sequential(
            pufferlib.pytorch.layer_init(
                nn.Linear(5, hidden_size)),
            nn.Flatten(),
        )
        self.proj = nn.Sequential(
            pufferlib.pytorch.layer_init(nn.Linear(hidden_size + cnn_channels*5, hidden_size)),
            nn.ReLU(),
        )
        self.atn_dim = env.single_action_space.nvec.tolist()
        self.actor = pufferlib.pytorch.layer_init(nn.Linear(hidden_size, sum(self.atn_dim)), std=0.01)
        self.value = pufferlib.pytorch.layer_init(
                nn.Linear(hidden_size, 1), std=1)

    def forward(self, observations, state=None):
        hidden = self.encode_observations(observations, state)
        actions, value = self.decode_actions(hidden)
        return actions, value

    def forward_train(self, x, state=None):
        return self.forward(x, state)

    def encode_observations(self, observations, state=None):
        # breakpoint()
        obs_2d = observations[:, :242].reshape(-1, 2, 11, 11).float()
        obs_1d = observations[:, 242:247].reshape(-1, 5).float()
        location_2d = observations[:, 247:].reshape(-1,2, 6, 6).float()
        hidden_local_2d = self.local_net_2d(obs_2d)
        hidden_global_2d = self.global_net_2d(location_2d)
        hidden_1d = self.net_1d(obs_1d)
        hidden = torch.cat([hidden_local_2d, hidden_global_2d, hidden_1d], dim=1)
        return self.proj(hidden)

    def decode_actions(self, hidden):
        action = self.actor(hidden)
        action = torch.split(action, self.atn_dim, dim=1)
        #action = [head(hidden) for head in self.actor]
        value = self.value(hidden)
        return action, value


class G2048(nn.Module):
    def __init__(self, env, cnn_channels=32, hidden_size=128):
        super().__init__()
        self.hidden_size = hidden_size
        self.is_continuous = False

        self.cnn = nn.Sequential(
            pufferlib.pytorch.layer_init(
                nn.Conv2d(1, cnn_channels, 2, stride=1)),
            nn.GELU(),
            pufferlib.pytorch.layer_init(
                nn.Conv2d(cnn_channels, cnn_channels, 2, stride=1)),
            nn.Flatten(),
            nn.GELU(),
            pufferlib.pytorch.layer_init(
            nn.Linear(128, hidden_size), std=0.01),
        )

        self.decoder = pufferlib.pytorch.layer_init(
            nn.Linear(hidden_size, env.single_action_space.n), std=0.01)
        self.value = pufferlib.pytorch.layer_init(
            nn.Linear(hidden_size, 1), std=1)

    def forward_eval(self, observations, state=None):
        hidden = self.encode_observations(observations)
        actions, value = self.decode_actions(hidden)
        return actions, value

    def forward(self, x, state=None):
        return self.forward_eval(x, state)

    def encode_observations(self, observations, state=None):
        #observations = F.one_hot(observations.long(), 16).view(-1, 16, 4, 4).float()
        observations = observations.float().view(-1, 1, 4, 4)
        return self.cnn(observations)

    def decode_actions(self, hidden):
        action = self.decoder(hidden)
        value = self.value(hidden)
        return action, value

class Snake(nn.Module):
    def __init__(self, env, cnn_channels=32, hidden_size=128):
        super().__init__()
        self.hidden_size = hidden_size
        self.is_continuous = False

        encode_dim = cnn_channels

        '''
        self.network= nn.Sequential(
            pufferlib.pytorch.layer_init(
                nn.Conv2d(8, cnn_channels, 5, stride=3)),
            nn.ReLU(),
            pufferlib.pytorch.layer_init(
                nn.Conv2d(cnn_channels, cnn_channels, 3, stride=1)),
            nn.ReLU(),
            nn.Flatten(),
        )
        self.proj = nn.Sequential(
            pufferlib.pytorch.layer_init(nn.Linear(encode_dim, hidden_size)),
            nn.ReLU(),
        )
 
        '''
        self.encoder= torch.nn.Sequential(
            nn.Linear(8*np.prod(env.single_observation_space.shape), hidden_size),
            nn.GELU(),
        )
        self.decoder = pufferlib.pytorch.layer_init(
            nn.Linear(hidden_size, env.single_action_space.n), std=0.01)
        self.value = pufferlib.pytorch.layer_init(
            nn.Linear(hidden_size, 1), std=1)

    def forward(self, observations, state=None):
        #observations = F.one_hot(observations.long(), 8).permute(0, 3, 1, 2).float()
        hidden = self.encode_observations(observations)
        actions, value = self.decode_actions(hidden)
        return actions, value

    def forward_train(self, x, state=None):
        return self.forward(x, state)

    def encode_observations(self, observations, state=None):
        observations = F.one_hot(observations.long(), 8).view(-1, 11*11*8).float()
        return self.encoder(observations)

    def decode_actions(self, hidden):
        action = self.decoder(hidden)
        value = self.value(hidden)
        return action, value

'''
class Snake(pufferlib.models.Default):
    def __init__(self, env, hidden_size=128):
        super().__init__()

    def encode_observations(self, observations, state=None):
        observations = F.one_hot(observations.long(), 8).view(-1, 11*11*8).float()
        super().encode_observations(observations, state)
'''

class Grid(nn.Module):
    def __init__(self, env, cnn_channels=32, hidden_size=128, **kwargs):
        super().__init__()
        self.hidden_size = hidden_size
        self.network = nn.Sequential(
            pufferlib.pytorch.layer_init(
                nn.Conv2d(32, cnn_channels, 5, stride=3)),
            nn.ReLU(),
            pufferlib.pytorch.layer_init(
                nn.Conv2d(cnn_channels, cnn_channels, 3, stride=1)),
            nn.Flatten(),
            nn.ReLU(),
            pufferlib.pytorch.layer_init(nn.Linear(cnn_channels, hidden_size)),
            nn.ReLU(),
        )

        self.is_continuous = isinstance(env.single_action_space, pufferlib.spaces.Box)
        if self.is_continuous:
            self.decoder_mean = pufferlib.pytorch.layer_init(
                nn.Linear(hidden_size, env.single_action_space.shape[0]), std=0.01)
            self.decoder_logstd = nn.Parameter(torch.zeros(
                1, env.single_action_space.shape[0]))
        else:
            num_actions = env.single_action_space.n
            self.actor = pufferlib.pytorch.layer_init(
                nn.Linear(hidden_size, num_actions), std=0.01)

        self.value_fn = pufferlib.pytorch.layer_init(
            nn.Linear(hidden_size, 1), std=1)

    def forward(self, observations, state=None):
        hidden = self.encode_observations(observations)
        actions, value = self.decode_actions(hidden)
        return actions, value

    def forward_train(self, x, state=None):
        return self.forward(x, state)

    def encode_observations(self, observations, state=None):
        hidden = observations.view(-1, 11, 11).long()
        hidden = F.one_hot(hidden, 32).permute(0, 3, 1, 2).float()
        hidden = self.network(hidden)
        return hidden

    def decode_actions(self, flat_hidden, state=None):
        value = self.value_fn(flat_hidden)
        if self.is_continuous:
            mean = self.decoder_mean(flat_hidden)
            logstd = self.decoder_logstd.expand_as(mean)
            std = torch.exp(logstd)
            probs = torch.distributions.Normal(mean, std)
            batch = flat_hidden.shape[0]
            return probs, value
        else:
            action = self.actor(flat_hidden)
            return action, value

class Go(nn.Module):
    def __init__(self, env, cnn_channels=64, hidden_size=128, **kwargs):
        super().__init__()
        self.hidden_size = hidden_size
        self.is_continuous = False
        # 3 categories 2 boards. 
        # categories = player, opponent, empty
        # boards = current, previous
        self.cnn = nn.Sequential(
            pufferlib.pytorch.layer_init(
                nn.Conv2d(2, cnn_channels, 3, stride=1)),
            nn.ReLU(),
            pufferlib.pytorch.layer_init(
                nn.Conv2d(cnn_channels, cnn_channels, 3, stride = 1)),
            nn.Flatten(),
        )

        obs_size = env.single_observation_space.shape[0]
        self.grid_size = int(np.sqrt((obs_size-2)/2))
        output_size = self.grid_size - 4
        cnn_flat_size = cnn_channels * output_size * output_size
        
        self.flat = pufferlib.pytorch.layer_init(nn.Linear(2,32))
        
        self.proj = pufferlib.pytorch.layer_init(nn.Linear(cnn_flat_size + 32, hidden_size))

        self.actor = pufferlib.pytorch.layer_init(
                nn.Linear(hidden_size, env.single_action_space.n), std=0.01)

        self.value_fn = pufferlib.pytorch.layer_init(
                nn.Linear(hidden_size, 1), std=1)
   
    def forward(self, observations, state=None):
        hidden = self.encode_observations(observations)
        actions, value = self.decode_actions(hidden)
        return actions, value

    def forward_train(self, x, state=None):
        return self.forward(x, state)

    def encode_observations(self, observations, state=None):
        grid_size = int(np.sqrt((observations.shape[1] - 2) / 2))
        full_board = grid_size * grid_size 
        black_board = observations[:, :full_board].view(-1,1, grid_size,grid_size).float()
        white_board = observations[:, full_board:-2].view(-1,1, grid_size, grid_size).float()
        board_features = torch.cat([black_board, white_board],dim=1)
        flat_feature1 = observations[:, -2].unsqueeze(1).float()
        flat_feature2 = observations[:, -1].unsqueeze(1).float()
        # Pass board through cnn
        cnn_features = self.cnn(board_features)
        # Pass extra feature
        flat_features = torch.cat([flat_feature1, flat_feature2],dim=1)
        flat_features = self.flat(flat_features)
        # pass all features
        features = torch.cat([cnn_features, flat_features], dim=1)
        features = F.relu(self.proj(features))

        return features

    def decode_actions(self, flat_hidden, state=None):
        value = self.value_fn(flat_hidden)
        action = self.actor(flat_hidden)
        return action, value
    
class MOBA(nn.Module):
    def __init__(self, env, cnn_channels=128, hidden_size=128, **kwargs):
        super().__init__()
        self.hidden_size = hidden_size
        self.cnn = nn.Sequential(
            pufferlib.pytorch.layer_init(
                nn.Conv2d(16 + 3, cnn_channels, 5, stride=3)),
            nn.ReLU(),
            pufferlib.pytorch.layer_init(
                nn.Conv2d(cnn_channels, cnn_channels, 3, stride=1)),
            nn.Flatten(),
        )
        self.flat = pufferlib.pytorch.layer_init(nn.Linear(26, 128))
        self.proj = pufferlib.pytorch.layer_init(nn.Linear(128+cnn_channels, hidden_size))

        self.is_continuous = isinstance(env.single_action_space, pufferlib.spaces.Box)
        if self.is_continuous:
            self.decoder_mean = pufferlib.pytorch.layer_init(
                nn.Linear(hidden_size, env.single_action_space.shape[0]), std=0.01)
            self.decoder_logstd = nn.Parameter(torch.zeros(
                1, env.single_action_space.shape[0]))
        else:
            self.atn_dim = env.single_action_space.nvec.tolist()
            self.actor = pufferlib.pytorch.layer_init(
                nn.Linear(hidden_size, sum(self.atn_dim)), std=0.01)

        self.value_fn = pufferlib.pytorch.layer_init(
            nn.Linear(hidden_size, 1), std=1)

    def forward(self, observations, state=None):
        hidden = self.encode_observations(observations)
        actions, value = self.decode_actions(hidden)
        return actions, value

    def forward_train(self, x, state=None):
        return self.forward(x, state)

    def encode_observations(self, observations, state=None):
        cnn_features = observations[:, :-26].view(-1, 11, 11, 4).long()
        map_features = F.one_hot(cnn_features[:, :, :, 0], 16).permute(0, 3, 1, 2).float()
        extra_map_features = (cnn_features[:, :, :, -3:].float() / 255).permute(0, 3, 1, 2)
        cnn_features = torch.cat([map_features, extra_map_features], dim=1)
        #print('observations 2d: ', map_features[0].cpu().numpy().tolist())
        cnn_features = self.cnn(cnn_features)
        #print('cnn features: ', cnn_features[0].detach().cpu().numpy().tolist())

        flat_features = observations[:, -26:].float() / 255.0
        #print('observations 1d: ', flat_features[0, 0])
        flat_features = self.flat(flat_features)
        #print('flat features: ', flat_features[0].detach().cpu().numpy().tolist())

        features = torch.cat([cnn_features, flat_features], dim=1)
        features = F.relu(self.proj(F.relu(features)))
        #print('features: ', features[0].detach().cpu().numpy().tolist())
        return features

    def decode_actions(self, flat_hidden):
        #print('lstm: ', flat_hidden[0].detach().cpu().numpy().tolist())
        value = self.value_fn(flat_hidden)
        if self.is_continuous:
            mean = self.decoder_mean(flat_hidden)
            logstd = self.decoder_logstd.expand_as(mean)
            std = torch.exp(logstd)
            probs = torch.distributions.Normal(mean, std)
            batch = flat_hidden.shape[0]
            return probs, value
        else:
            action = self.actor(flat_hidden)
            action = torch.split(action, self.atn_dim, dim=1)

            #argmax_samples = [torch.argmax(a, dim=1).detach().cpu().numpy().tolist() for a in action]
            #print('argmax samples: ', argmax_samples)

            return action, value

class TrashPickup(nn.Module):
    def __init__(self, env, cnn_channels=32, hidden_size=128, **kwargs):
        super().__init__()
        self.hidden_size = hidden_size
        self.is_continuous = False
        self.network= nn.Sequential(
            pufferlib.pytorch.layer_init(
                nn.Conv2d(5, cnn_channels, 5, stride=3)),
            nn.ReLU(),
            pufferlib.pytorch.layer_init(
                nn.Conv2d(cnn_channels, cnn_channels, 3, stride=1)),
            nn.ReLU(),
            nn.Flatten(),
            pufferlib.pytorch.layer_init(nn.Linear(cnn_channels, hidden_size)),
        )
        self.actor = pufferlib.pytorch.layer_init(
            nn.Linear(hidden_size, env.single_action_space.n), std=0.01)
        self.value_fn = pufferlib.pytorch.layer_init(
            nn.Linear(hidden_size, 1), std=1)

    def forward(self, observations, state=None):
        hidden = self.encode_observations(observations)
        actions, value = self.decode_actions(hidden)
        return actions, value

    def forward_train(self, x, state=None):
        return self.forward(x, state)

    def encode_observations(self, observations, state=None):
        observations = observations.view(-1, 5, 11, 11).float()
        return self.network(observations)

    def decode_actions(self, flat_hidden):
        action = self.actor(flat_hidden)
        value = self.value_fn(flat_hidden)
        return action, value

class TowerClimbLSTM(pufferlib.models.LSTMWrapper):
    def __init__(self, env, policy, input_size = 256, hidden_size = 256):
        super().__init__(env, policy, input_size, hidden_size)

class TowerClimb(nn.Module):
    def __init__(self, env, cnn_channels=16, hidden_size = 256, **kwargs):
        self.hidden_size = hidden_size
        self.is_continuous = False
        super().__init__()
        self.network = nn.Sequential(
                pufferlib.pytorch.layer_init(
                    nn.Conv3d(1, cnn_channels, 3, stride = 1)),
                nn.ReLU(),
                pufferlib.pytorch.layer_init(
                    nn.Conv3d(cnn_channels, cnn_channels, 3, stride=1)),
                nn.Flatten()       
        )
        cnn_flat_size = cnn_channels * 1 * 1 * 5

        # Process player obs
        self.flat = pufferlib.pytorch.layer_init(nn.Linear(3,16))

        # combine
        self.proj = pufferlib.pytorch.layer_init(
                nn.Linear(cnn_flat_size + 16, hidden_size))
        self.actor = pufferlib.pytorch.layer_init(
                nn.Linear(hidden_size, env.single_action_space.n), std = 0.01)
        self.value_fn = pufferlib.pytorch.layer_init(
                nn.Linear(hidden_size, 1 ), std=1)

    def forward(self, observations, state=None):
        hidden = self.encode_observations(observations)
        actions, value = self.decode_actions(hidden)
        return actions, value

    def forward_train(self, x, state=None):
        return self.forward(x, state)

    def encode_observations(self, observations, state=None):
        board_state = observations[:,:225]
        player_info = observations[:, -3:] 
        board_features = board_state.view(-1, 1, 5,5,9).float()
        cnn_features = self.network(board_features)
        flat_features = self.flat(player_info.float())
        
        features = torch.cat([cnn_features,flat_features],dim = 1)
        features = self.proj(features)
        return features
    
    def decode_actions(self, flat_hidden):
        action = self.actor(flat_hidden)
        value = self.value_fn(flat_hidden)
        
        return action, value


class ImpulseWarsLSTM(Recurrent):
    def __init__(self, env: pufferlib.PufferEnv, policy: nn.Module, input_size: int = 512, hidden_size: int = 512):
        super().__init__(env, policy, input_size, hidden_size)


class ImpulseWarsPolicy(nn.Module):
    def __init__(
        self,
        env: pufferlib.PufferEnv,
        cnn_channels: int = 64,
        weapon_type_embedding_dims: int = 2,
        input_size: int = 512,
        hidden_size: int = 512,
        batch_size: int = 131_072,
        num_drones: int = 2,
        continuous: bool = False,
        is_training: bool = True,
        device: str = "cuda",
        **kwargs,
    ):
        super().__init__()
        self.hidden_size = hidden_size

        self.is_continuous = continuous

        self.numDrones = num_drones
        self.isTraining = is_training
        from pufferlib.ocean.impulse_wars import binding
        self.obsInfo = SimpleNamespace(**binding.get_consts(self.numDrones))

        self.discreteFactors = np.array(
            [self.obsInfo.wallTypes] * self.obsInfo.numNearWallObs
            + [self.obsInfo.wallTypes + 1] * self.obsInfo.numFloatingWallObs
            + [self.numDrones + 1] * self.obsInfo.numProjectileObs,
        )
        discreteOffsets = torch.tensor([0] + list(np.cumsum(self.discreteFactors)[:-1]), device=device).view(
            1, -1
        )
        self.register_buffer("discreteOffsets", discreteOffsets, persistent=False)
        self.discreteMultihotDim = self.discreteFactors.sum()

        multihotBuffer = torch.zeros(batch_size, self.discreteMultihotDim, device=device)
        self.register_buffer("multihotOutput", multihotBuffer, persistent=False)

        # most of the observation is a 2D array of bytes, but the end
        # contains around 200 floats; this allows us to treat the end
        # of the observation as a float array
        _, *self.dtype = _nativize_dtype(
            np.dtype((np.uint8, (self.obsInfo.continuousObsBytes,))),
            np.dtype((np.float32, (self.obsInfo.continuousObsSize,))),
        )
        self.dtype = tuple(self.dtype)

        self.weaponTypeEmbedding = nn.Embedding(self.obsInfo.weaponTypes, weapon_type_embedding_dims)

        # each byte in the map observation contains 4 values:
        # - 2 bits for wall type
        # - 1 bit for is floating wall
        # - 1 bit for is weapon pickup
        # - 3 bits for drone index
        self.register_buffer(
            "unpackMask",
            torch.tensor([0x60, 0x10, 0x08, 0x07], dtype=torch.uint8),
            persistent=False,
        )
        self.register_buffer("unpackShift", torch.tensor([5, 4, 3, 0], dtype=torch.uint8), persistent=False)

        self.mapObsInputChannels = (self.obsInfo.wallTypes + 1) + 1 + 1 + self.numDrones
        self.mapCNN = nn.Sequential(
            layer_init(
                nn.Conv2d(
                    self.mapObsInputChannels,
                    cnn_channels,
                    kernel_size=5,
                    stride=3,
                )
            ),
            nn.ReLU(),
            layer_init(nn.Conv2d(cnn_channels, cnn_channels, kernel_size=3, stride=1)),
            nn.ReLU(),
            nn.Flatten(),
        )
        cnnOutputSize = self._computeCNNShape()

        featuresSize = (
            cnnOutputSize
            + (self.obsInfo.numNearWallObs * (self.obsInfo.wallTypes + self.obsInfo.nearWallPosObsSize))
            + (
                self.obsInfo.numFloatingWallObs
                * (self.obsInfo.wallTypes + 1 + self.obsInfo.floatingWallInfoObsSize)
            )
            + (
                self.obsInfo.numWeaponPickupObs
                * (weapon_type_embedding_dims + self.obsInfo.weaponPickupPosObsSize)
            )
            + (
                self.obsInfo.numProjectileObs
                * (weapon_type_embedding_dims + self.obsInfo.projectileInfoObsSize + self.numDrones + 1)
            )
            + ((self.numDrones - 1) * (weapon_type_embedding_dims + self.obsInfo.enemyDroneObsSize))
            + (self.obsInfo.droneObsSize + weapon_type_embedding_dims)
            + self.obsInfo.miscObsSize
        )

        self.encoder = nn.Sequential(
            layer_init(nn.Linear(featuresSize, input_size)),
            nn.ReLU(),
        )

        if self.is_continuous:
            self.actorMean = layer_init(nn.Linear(hidden_size, env.single_action_space.shape[0]), std=0.01)
            self.actorLogStd = nn.Parameter(torch.zeros(1, env.single_action_space.shape[0]))
        else:
            self.actionDim = env.single_action_space.nvec.tolist()
            self.actor = layer_init(nn.Linear(hidden_size, sum(self.actionDim)), std=0.01)

        self.critic = layer_init(nn.Linear(hidden_size, 1), std=1.0)

    def forward(self, obs: torch.Tensor, state = None) -> Tuple[torch.Tensor, torch.Tensor]:
        hidden = self.encode_observations(obs)
        actions, value = self.decode_actions(hidden)
        return actions, value

    def unpack(self, batchSize: int, obs: torch.Tensor) -> torch.Tensor:
        # prepare map obs to be unpacked
        mapObs = obs[:, : self.obsInfo.mapObsSize].reshape((batchSize, -1, 1))
        # unpack wall types, weapon pickup types, and drone indexes
        mapObs = (mapObs & self.unpackMask) >> self.unpackShift
        # reshape so channels are first, required for torch conv2d
        return mapObs.permute(0, 2, 1).reshape(
            (batchSize, 4, self.obsInfo.mapObsRows, self.obsInfo.mapObsColumns)
        )

    def encode_observations(self, obs: torch.Tensor, state: Any = None) -> torch.Tensor:
        batchSize = obs.shape[0]

        mapObs = self.unpack(batchSize, obs)

        # one hot encode wall types
        wallTypeObs = mapObs[:, 0, :, :].long()
        wallTypes = F.one_hot(wallTypeObs, self.obsInfo.wallTypes + 1).permute(0, 3, 1, 2).float()

        # unsqueeze floating wall booleans (is wall a floating wall)
        floatingWallObs = mapObs[:, 1, :, :].unsqueeze(1)

        # unsqueeze map pickup booleans (does map tile contain a weapon pickup)
        mapPickupObs = mapObs[:, 2, :, :].unsqueeze(1)

        # one hot drone indexes
        droneIndexObs = mapObs[:, 3, :, :].long()
        droneIndexes = F.one_hot(droneIndexObs, self.numDrones).permute(0, 3, 1, 2).float()

        # combine all map observations and feed through CNN
        mapObs = torch.cat((wallTypes, floatingWallObs, mapPickupObs, droneIndexes), dim=1)
        map = self.mapCNN(mapObs)

        # process discrete observations
        multihotInput = (
            obs[:, self.obsInfo.nearWallTypesObsOffset : self.obsInfo.projectileTypesObsOffset]
            + self.discreteOffsets
        )
        multihotOutput = self.multihotOutput[:batchSize].zero_()
        multihotOutput.scatter_(1, multihotInput.long(), 1)

        weaponTypeObs = obs[:, self.obsInfo.projectileTypesObsOffset : self.obsInfo.discreteObsSize].int()
        weaponTypes = self.weaponTypeEmbedding(weaponTypeObs).float()
        weaponTypes = torch.flatten(weaponTypes, start_dim=1, end_dim=-1)

        # process continuous observations
        continuousObs = nativize_tensor(obs[:, self.obsInfo.continuousObsOffset :], self.dtype)
        # combine all observations and feed through final linear encoder
        features = torch.cat((map, multihotOutput, weaponTypes, continuousObs), dim=-1)

        return self.encoder(features)

    def decode_actions(self, hidden: torch.Tensor):
        if self.is_continuous:
            actionMean = self.actorMean(hidden)
            if self.isTraining:
                actionLogStd = self.actorLogStd.expand_as(actionMean)
                actionStd = torch.exp(actionLogStd)
                action = Normal(actionMean, actionStd)
            else:
                action = actionMean
        else:
            action = self.actor(hidden)
            action = torch.split(action, self.actionDim, dim=1)

        value = self.critic(hidden)

        return action, value

    def _computeCNNShape(self) -> int:
        mapSpace = spaces.Box(
            low=0,
            high=1,
            shape=(self.mapObsInputChannels, self.obsInfo.mapObsRows, self.obsInfo.mapObsColumns),
            dtype=np.float32,
        )

        with torch.no_grad():
            t = torch.as_tensor(mapSpace.sample()[None])
            return self.mapCNN(t).shape[1]

class Drive(nn.Module):
    def __init__(self, env, input_size=128, hidden_size=128, **kwargs):
        super().__init__()
        self.hidden_size = hidden_size
        self.ego_encoder = nn.Sequential(
            pufferlib.pytorch.layer_init(
                nn.Linear(7, input_size)),
            nn.LayerNorm(input_size),
            # nn.ReLU(),
            pufferlib.pytorch.layer_init(
                nn.Linear(input_size, input_size))
        )
        max_road_objects = 13
        self.road_encoder = nn.Sequential(
            pufferlib.pytorch.layer_init(
                nn.Linear(max_road_objects, input_size)),
            nn.LayerNorm(input_size),
            # nn.ReLU(),
            pufferlib.pytorch.layer_init(
                nn.Linear(input_size, input_size))
        )
        max_partner_objects = 7
        self.partner_encoder = nn.Sequential(
            pufferlib.pytorch.layer_init(
                nn.Linear(max_partner_objects, input_size)),
            nn.LayerNorm(input_size),
            # nn.ReLU(),
            pufferlib.pytorch.layer_init(
                nn.Linear(input_size, input_size))
        )


        self.shared_embedding = nn.Sequential(
            nn.GELU(),
            pufferlib.pytorch.layer_init(nn.Linear(3*input_size,  hidden_size)),
        )
        self.is_continuous = isinstance(env.single_action_space, pufferlib.spaces.Box)

        self.atn_dim = env.single_action_space.nvec.tolist()
        self.actor = pufferlib.pytorch.layer_init(
                nn.Linear(hidden_size, sum(self.atn_dim)), std = 0.01)
        self.value_fn = pufferlib.pytorch.layer_init(
                nn.Linear(hidden_size, 1 ), std=1)
    
    def forward(self, observations, state=None):
        hidden = self.encode_observations(observations)
        actions, value = self.decode_actions(hidden)
        return actions, value

    def forward_train(self, x, state=None):
        return self.forward(x, state)
   
    def encode_observations(self, observations, state=None):
        ego_dim = 7
        partner_dim = 63 * 7
        road_dim = 200*7
        ego_obs = observations[:, :ego_dim]
        partner_obs = observations[:, ego_dim:ego_dim+partner_dim]
        road_obs = observations[:, ego_dim+partner_dim:ego_dim+partner_dim+road_dim]
        
        partner_objects = partner_obs.view(-1, 63, 7)
        road_objects = road_obs.view(-1, 200, 7)
        road_continuous = road_objects[:, :, :6]  # First 6 features
        road_categorical = road_objects[:, :, 6]
        road_onehot = F.one_hot(road_categorical.long(), num_classes=7)  # Shape: [batch, 200, 7]
        road_objects = torch.cat([road_continuous, road_onehot], dim=2)
        ego_features = self.ego_encoder(ego_obs)
        partner_features, _ = self.partner_encoder(partner_objects).max(dim=1)
        road_features, _ = self.road_encoder(road_objects).max(dim=1)
        
        concat_features = torch.cat([ego_features, road_features, partner_features], dim=1)
        
        # Pass through shared embedding
        embedding = F.relu(self.shared_embedding(concat_features))
        # embedding = self.shared_embedding(concat_features)
        return embedding
    
    def decode_actions(self, flat_hidden):
        action = self.actor(flat_hidden)
        action = torch.split(action, self.atn_dim, dim=1)
        value = self.value_fn(flat_hidden)
        return action, value

class Tetris(nn.Module):
    def __init__(
        self, 
        env, 
        cnn_channels=32,
        input_size=128,
        hidden_size=128,
        **kwargs
    ):
        super().__init__()
        self.hidden_size = hidden_size
        self.cnn_channels =  cnn_channels   
        self.n_cols = env.n_cols
        self.n_rows = env.n_rows
        self.scalar_input_size = (6 + 7 * (env.deck_size + 1))
        self.flat_conv_size = cnn_channels * 3 * 10
        self.is_continuous = isinstance(env.single_action_space, pufferlib.spaces.Box)

        self.conv_grid = nn.Sequential(
            pufferlib.pytorch.layer_init(nn.Conv2d(2, cnn_channels, kernel_size=(5, 3), stride=(2,1), padding=(2,1))),
            nn.ReLU(),
            pufferlib.pytorch.layer_init(nn.Conv2d(cnn_channels, cnn_channels, kernel_size=(5, 3), stride=(2,1), padding=(2,1))),
            nn.ReLU(),
            pufferlib.pytorch.layer_init(nn.Conv2d(cnn_channels, cnn_channels, kernel_size=(5, 5), stride=(2,1), padding=(2,2))),
            nn.ReLU(),
            nn.Flatten(),
            pufferlib.pytorch.layer_init(nn.Linear(self.flat_conv_size, input_size)),
        )

        self.fc_scalar = nn.Sequential(
            pufferlib.pytorch.layer_init(nn.Linear(self.scalar_input_size, input_size)),
            nn.ReLU(),
        )

        self.proj = nn.Sequential(
            pufferlib.pytorch.layer_init(nn.Linear(2 * input_size, hidden_size)),
            nn.ReLU(),
        )

        self.actor = nn.Sequential(
            pufferlib.pytorch.layer_init(nn.Linear(hidden_size, 7), std=0.01),
            nn.Flatten()
        )

        self.value_fn = nn.Sequential(
            pufferlib.pytorch.layer_init(nn.Linear(hidden_size, 1)),
            nn.ReLU(),
        )

    def forward(self, observations, state=None):
        hidden = self.encode_observations(observations) 
        actions, value = self.decode_actions(hidden)
        return actions, value

    def forward_train(self, x, state=None):
        return self.forward(x, state)

    def encode_observations(self, observations, state=None):
        B = observations.shape[0]
        grid_info = observations[:, 0:(self.n_cols * self.n_rows)].view(B, self.n_rows, self.n_cols)  # (B, n_rows, n_cols)
        grid_info = torch.stack([(grid_info == 1).float(), (grid_info == 2).float()], dim=1)  # (B, 2, n_rows, n_cols)
        scalar_info = observations[:, (self.n_cols * self.n_rows):(self.n_cols * self.n_rows + self.scalar_input_size)].float()

        grid_feat = self.conv_grid(grid_info)  # (B, input_size)
        scalar_feat = self.fc_scalar(scalar_info)  # (B, input_size)

        combined = torch.cat([grid_feat, scalar_feat], dim=-1)  # (B, 2 * input_size)
        features = self.proj(combined)  # (B, hidden_size)
        return features

    def decode_actions(self, hidden):
        action = self.actor(hidden)  # (B, 4 * n_cols)
        value = self.value_fn(hidden)  # (B, 1)
        return action, value

class Drone(nn.Module):
    ''' Drone policy. Flattens obs and applies a linear layer.
    '''
    def __init__(self, env, hidden_size=128):
        super().__init__()
        self.hidden_size = hidden_size
        self.is_multidiscrete = isinstance(env.single_action_space,
                pufferlib.spaces.MultiDiscrete)
        self.is_continuous = isinstance(env.single_action_space,
                pufferlib.spaces.Box)
        try:
            self.is_dict_obs = isinstance(env.env.observation_space, pufferlib.spaces.Dict) 
        except:
            self.is_dict_obs = isinstance(env.observation_space, pufferlib.spaces.Dict) 

        if self.is_dict_obs:
            self.dtype = pufferlib.pytorch.nativize_dtype(env.emulated)
            input_size = int(sum(np.prod(v.shape) for v in env.env.observation_space.values()))
            self.encoder = nn.Linear(input_size, self.hidden_size)
        else:
            self.encoder = torch.nn.Sequential(
                nn.Linear(np.prod(env.single_observation_space.shape), hidden_size),
                nn.GELU(),
            )

        if self.is_multidiscrete:
            self.action_nvec = tuple(env.single_action_space.nvec)
            self.decoder = pufferlib.pytorch.layer_init(
                    nn.Linear(hidden_size, sum(self.action_nvec)), std=0.01)
        elif not self.is_continuous:
            self.decoder = pufferlib.pytorch.layer_init(
                nn.Linear(hidden_size, env.single_action_space.n), std=0.01)
        else:
            self.decoder_mean = pufferlib.pytorch.layer_init(
                nn.Linear(hidden_size, env.single_action_space.shape[0]), std=0.01)
            self.decoder_logstd = nn.Parameter(torch.zeros(
                1, env.single_action_space.shape[0]))

        self.value = pufferlib.pytorch.layer_init(
            nn.Linear(hidden_size, 1), std=1)

    def forward_eval(self, observations, state=None):
        hidden = self.encode_observations(observations, state=state)
        logits, values = self.decode_actions(hidden)
        return logits, values

    def forward(self, observations, state=None):
        return self.forward_eval(observations, state)

    def encode_observations(self, observations, state=None):
        '''Encodes a batch of observations into hidden states. Assumes
        no time dimension (handled by LSTM wrappers).'''
        batch_size = observations.shape[0]
        if self.is_dict_obs:
            observations = pufferlib.pytorch.nativize_tensor(observations, self.dtype)
            observations = torch.cat([v.view(batch_size, -1) for v in observations.values()], dim=1)
        else: 
            observations = observations.view(batch_size, -1)
        return self.encoder(observations.float())

    def decode_actions(self, hidden):
        '''Decodes a batch of hidden states into (multi)discrete actions.
        Assumes no time dimension (handled by LSTM wrappers).'''
        if self.is_multidiscrete:
            logits = self.decoder(hidden).split(self.action_nvec, dim=1)
        elif self.is_continuous:
            mean = self.decoder_mean(hidden)
            logstd = self.decoder_logstd.expand_as(mean)
            std = torch.exp(logstd)
            logits = torch.distributions.Normal(mean, std)
        else:
            logits = self.decoder(hidden)

        values = self.value(hidden)
        return logits, values



================================================
FILE: pufferlib/ocean/asteroids/asteroids.c
================================================
#include "asteroids.h"

int main() {
  Asteroids env = {.size = 500, .frameskip = 1};
  env.observations = (float *)calloc(4 + 2 * 50, sizeof(float));
  env.actions = (int *)calloc(1, sizeof(int));
  env.rewards = (float *)calloc(1, sizeof(float));
  env.terminals = (unsigned char *)calloc(1, sizeof(unsigned char));

  c_reset(&env);
  c_render(&env);
  while (!WindowShouldClose()) {
    if (IsKeyDown(KEY_LEFT_SHIFT)) {
      if (IsKeyDown(KEY_W) || IsKeyDown(KEY_UP)) {
        env.actions[0] = 0;
      } else if (IsKeyDown(KEY_A) || IsKeyDown(KEY_LEFT)) {
        env.actions[0] = 1;
      } else if (IsKeyDown(KEY_D) || IsKeyDown(KEY_RIGHT)) {
        env.actions[0] = 2;
      } else if (IsKeyDown(KEY_SPACE)) {
        env.actions[0] = 3;
      } else {
        env.actions[0] = -1;
      }
    } else {
      env.actions[0] = rand() % 4;
    }
    c_step(&env);
    c_render(&env);
  }
  free(env.observations);
  free(env.actions);
  free(env.rewards);
  free(env.terminals);
  c_close(&env);
}



================================================
FILE: pufferlib/ocean/asteroids/asteroids.h
================================================
#pragma once

#include "raylib.h"
#include <math.h>
#include <stdio.h>
#include <stdlib.h>
#include <string.h>

#define MAX_PARTICLES 10
#define MAX_ASTEROIDS 20

const unsigned char FORWARD = 0;
const unsigned char TURN_LEFT = 1;
const unsigned char TURN_RIGHT = 2;
const unsigned char SHOOT = 3;

const float FRICTION = 0.95f;
const float SPEED = 0.6f;
const float PARTICLE_SPEED = 7.0f;
const float ROTATION_SPEED = 0.1f;
const float ASTEROID_SPEED = 3.0f;
const int SHOOT_DELAY = 18;

const int MAX_TICK = 3600;

const int DEBUG = 0;

// for render only game over state
static int global_game_over_timer = 0;
static int global_game_over_started = 0;
static int global_render_flag = 0;

typedef struct {
  float perf;
  float score;
  float episode_return;
  float episode_length;
  float n;
} Log;

typedef struct {
  Vector2 position;
  Vector2 velocity;
} Particle;

typedef struct {
  Vector2 position;
  Vector2 velocity;
  int radius;
  int radius_sq;
  Vector2 shape[12];
  int num_vertices;
} Asteroid;

typedef struct {
  Asteroid asteroid;
  float distance;
} AsteroidDistance;

typedef struct {
  Log log;
  float *observations;
  int *actions;
  float *rewards;
  unsigned char *terminals;
  int size;
  Vector2 player_position;
  Vector2 player_vel;
  float player_angle;
  int player_radius;
  int thruster_on;
  Particle particles[MAX_PARTICLES];
  int particle_index;
  Asteroid asteroids[MAX_ASTEROIDS];
  int asteroid_index;
  int last_shot;
  int tick;
  int score;
  float episode_return;
  int frameskip;
} Asteroids;

float random_float(float low, float high) {
  return low + (high - low) * ((float)rand() / (float)RAND_MAX);
}

void generate_asteroid_shape(Asteroid *as) {
  as->num_vertices = 8 + (as->radius / 10);

  for (int v = 0; v < as->num_vertices; v++) {
    float angle = (2.0f * PI * v) / as->num_vertices;
    float radius_variation =
        as->radius * (0.7f + 0.6f * random_float(0.0f, 1.0f));
    as->shape[v].x = cosf(angle) * radius_variation;
    as->shape[v].y = sinf(angle) * radius_variation;
  }
}

float clamp(float val, float low, float high) {
  return fmin(fmax(val, low), high);
}

Vector2 rotate_vector(Vector2 point, Vector2 center, float angle) {
  float s = sinf(angle);
  float c = cosf(angle);

  // Translate point back to origin:
  point.x -= center.x;
  point.y -= center.y;

  // Rotate point
  float xnew = point.x * c - point.y * s;
  float ynew = point.x * s + point.y * c;

  // Translate point back:
  point.x = xnew + center.x;
  point.y = ynew + center.y;
  return point;
}

Vector2 get_direction_vector(Asteroids *env) {
  float px = env->player_position.x;
  float py = env->player_position.y;
  Vector2 dir = (Vector2){px, py - 1};
  dir = rotate_vector(dir, env->player_position, env->player_angle);
  return (Vector2){dir.x - px, dir.y - py};
}

void move_particles(Asteroids *env) {
  Particle p;
  for (int i = 0; i < MAX_PARTICLES; i++) {
    p = env->particles[i];
    p.position.x += p.velocity.x * PARTICLE_SPEED;
    p.position.y += p.velocity.y * PARTICLE_SPEED;
    env->particles[i] = p;
  }
}

void move_asteroids(Asteroids *env) {
  Asteroid *as;
  for (int i = 0; i < MAX_ASTEROIDS; i++) {
    as = &env->asteroids[i];
    if (as->radius == 0)
      continue;

    as->position.x += as->velocity.x * ASTEROID_SPEED;
    as->position.y += as->velocity.y * ASTEROID_SPEED;
  }
}

Vector2 angle_to_vector(float angle) {
  Vector2 v;
  v.x = cosf(angle);
  v.y = sinf(angle);
  return v;
}

void spawn_asteroids(Asteroids *env) {
  float px, py;
  float angle;
  if (rand() % 10 == 0) {
    switch (rand() % 4) {
    case 0:
      // left edge
      px = 0;
      py = rand() % env->size;
      angle = random_float(-PI / 2, PI / 2);
      break;
    case 1:
      // right edge
      px = env->size;
      py = rand() % env->size;
      angle = random_float(PI / 2, 3 * PI / 2);
      break;
    case 2:
      // top edge
      px = rand() % env->size;
      py = 0;
      angle = random_float(PI, 2 * PI);
      break;
    default:
      // bottom edge
      px = rand() % env->size;
      py = env->size;
      angle = random_float(0, PI);
      break;
    }

    Vector2 direction = angle_to_vector(angle);
    Vector2 start_pos = (Vector2){px, py};
    Asteroid as;
    switch (rand() % 3) {
    case 0:
      // small
      as = (Asteroid){start_pos, direction, 10, 100};
      break;
    case 1:
      // medium
      as = (Asteroid){start_pos, direction, 20, 400};
      break;
    default:
      // big
      as = (Asteroid){start_pos, direction, 40, 1600};
      break;
    }
    env->asteroid_index = (env->asteroid_index + 1) % MAX_ASTEROIDS;
    env->asteroids[env->asteroid_index] = as;
    if (global_render_flag)
      generate_asteroid_shape(&env->asteroids[env->asteroid_index]);
  }
}

int particle_asteroid_collision(Asteroids *env, Particle *p, Asteroid *as) {
  float dx = p->position.x - as->position.x;
  float dy = p->position.y - as->position.y;
  return as->radius_sq > dx * dx + dy * dy;
}

void split_asteroid(Asteroids *env, Asteroid *as) {
  int new_radius = as->radius == 40 ? 20 : 10;

  float original_angle = atan2f(as->velocity.y, as->velocity.x);

  float offset1 = random_float(-PI / 4, PI / 4);
  float offset2 = random_float(-PI / 4, PI / 4);

  float angle1 = original_angle + offset1;
  float angle2 = original_angle + offset2;

  Vector2 direction1 = angle_to_vector(angle1);
  Vector2 direction2 = angle_to_vector(angle2);

  float len1 = sqrtf(direction1.x * direction1.x + direction1.y * direction1.y);
  float len2 = sqrtf(direction2.x * direction2.x + direction2.y * direction2.y);
  if (len1 > 0) {
    direction1.x /= len1;
    direction1.y /= len1;
  }
  if (len2 > 0) {
    direction2.x /= len2;
    direction2.y /= len2;
  }

  Vector2 start_pos = (Vector2){as->position.x, as->position.y};

  int new_index1 = (env->asteroid_index + 1) % MAX_ASTEROIDS;
  int new_index2 = (new_index1 + 1) % MAX_ASTEROIDS;

  as->position = start_pos;
  as->velocity = direction1;
  as->radius = new_radius;
  as->radius_sq = new_radius * new_radius;
  env->asteroids[new_index1] = (Asteroid){start_pos, direction2, new_radius};
  env->asteroid_index = new_index2;

  // Generate shapes for the new asteroids
  generate_asteroid_shape(as);
  generate_asteroid_shape(&env->asteroids[new_index1]);
}

void check_particle_asteroid_collision(Asteroids *env) {
  Particle *p;
  Asteroid *as;
  for (int i = 0; i < MAX_PARTICLES; i++) {
    p = &env->particles[i];
    if (p->position.x == 0 && p->position.y == 0)
      continue;

    for (int j = 0; j < MAX_ASTEROIDS; j++) {
      as = &env->asteroids[j];
      if (as->radius == 0)
        continue;

      if (particle_asteroid_collision(env, p, as)) {
        memset(p, 0, sizeof(*p));
        env->score += 1;
        env->rewards[0] += 1.0f;

        switch (as->radius) {
        case 10:
          memset(as, 0, sizeof(*as));
          break;
        case 20:
          split_asteroid(env, as);
          break;
        default:
          split_asteroid(env, as);
          break;
        }
        break;
      }
    }
  }
}

void check_player_asteroid_collision(Asteroids *env) {
  float min_dist;
  float dx, dy;
  Asteroid *as;
  for (int i = 0; i < MAX_ASTEROIDS; i++) {
    as = &env->asteroids[i];
    if (as->radius == 0)
      continue;

    min_dist = env->player_radius + as->radius;
    dx = env->player_position.x - as->position.x;
    dy = env->player_position.y - as->position.y;
    if (min_dist * min_dist > dx * dx + dy * dy) {
      env->terminals[0] = 1;
      env->rewards[0] = -1.0f;
      return;
    }
  }
}

void compute_observations(Asteroids *env) {
  int observation_indx = 0;
  env->observations[observation_indx++] = env->player_position.x / env->size;
  env->observations[observation_indx++] = env->player_position.y / env->size;
  env->observations[observation_indx++] = env->player_vel.x;
  env->observations[observation_indx++] = env->player_vel.y;
  
  // Create temporary array to store asteroids with their distances
  AsteroidDistance asteroid_distances[MAX_ASTEROIDS];
  
  int num_active_asteroids = 0;
  
  // Calculate distances and store active asteroids
  for (int i = 0; i < MAX_ASTEROIDS; i++) {
    Asteroid as = env->asteroids[i];
    if (as.radius == 0)
      continue;
    
    float dx = as.position.x - env->player_position.x;
    float dy = as.position.y - env->player_position.y;
    float distance = dx * dx + dy * dy;
    
    asteroid_distances[num_active_asteroids].asteroid = as;
    asteroid_distances[num_active_asteroids].distance = distance;
    num_active_asteroids++;
  }
  
  // Sort asteroids by distance (bubble sort for simplicity)
  for (int i = 0; i < num_active_asteroids - 1; i++) {
    for (int j = 0; j < num_active_asteroids - i - 1; j++) {
      if (asteroid_distances[j].distance > asteroid_distances[j + 1].distance) {
        AsteroidDistance temp = asteroid_distances[j];
        asteroid_distances[j] = asteroid_distances[j + 1];
        asteroid_distances[j + 1] = temp;
      }
    }
  }
  
  // Output sorted asteroids to observations (up to MAX_ASTEROIDS)
  for (int i = 0; i < MAX_ASTEROIDS; i++) {
    if (i < num_active_asteroids) {
      Asteroid as = asteroid_distances[i].asteroid;
      env->observations[observation_indx++] =
          (as.position.x - env->player_position.x) / env->size;
      env->observations[observation_indx++] =
          (as.position.y - env->player_position.y) / env->size;
      env->observations[observation_indx++] = as.velocity.x;
      env->observations[observation_indx++] = as.velocity.y;
      env->observations[observation_indx++] = (float)as.radius / 40;
    } else {
      // Pad with zeros for missing asteroids to ensure fixed observation size
      env->observations[observation_indx++] = 0.0f; // relative x
      env->observations[observation_indx++] = 0.0f; // relative y
      env->observations[observation_indx++] = 0.0f; // velocity x
      env->observations[observation_indx++] = 0.0f; // velocity y
      env->observations[observation_indx++] = 0.0f; // radius
    }
  }
}

void add_log(Asteroids *env) {
  env->log.perf += env->score / 100.0f;
  env->log.score += env->score;
  env->log.episode_length += env->tick;
  env->log.episode_return += env->episode_return;
  env->log.n++;
}

void c_reset(Asteroids *env) {
  env->player_position = (Vector2){env->size / 2.0f, env->size / 2.0f};
  env->player_angle = 0.0f;
  env->player_radius = 12;
  env->player_vel = (Vector2){0, 0};
  env->thruster_on = 0;
  memset(env->particles, 0, sizeof(Particle) * MAX_PARTICLES);
  memset(env->asteroids, 0, sizeof(Asteroid) * MAX_ASTEROIDS);
  env->particle_index = 0;
  env->asteroid_index = 0;
  env->tick = 0;
  env->score = 0;
  env->episode_return = 0;
  env->last_shot = 0;
}

void step_frame(Asteroids *env, int action) {
  // slow down each step
  env->player_vel.x *= FRICTION;
  env->player_vel.y *= FRICTION;

  Vector2 dir = get_direction_vector(env);

  if (action == TURN_LEFT)
    env->player_angle -= ROTATION_SPEED;
  if (action == TURN_RIGHT)
    env->player_angle += ROTATION_SPEED;
  if (action == FORWARD) {
    env->player_vel.x += dir.x * SPEED;
    env->player_vel.y += dir.y * SPEED;
    env->thruster_on = 1;
  }

  int elapsed = env->tick - env->last_shot;

  if (action == SHOOT && elapsed >= SHOOT_DELAY) {
    env->last_shot = env->tick;
    env->particle_index = (env->particle_index + 1) % MAX_PARTICLES;
    Vector2 start_pos = (Vector2){env->player_position.x + 20 * dir.x,
                                  env->player_position.y + 20 * dir.y};
    env->particles[env->particle_index] = (Particle){start_pos, dir};
  }

  // Explicit Euler
  env->player_position.x += env->player_vel.x;
  env->player_position.y += env->player_vel.y;

  move_particles(env);
  spawn_asteroids(env);
  move_asteroids(env);
  check_particle_asteroid_collision(env);
  check_player_asteroid_collision(env);

  if (env->player_position.x < 0)
    env->player_position.x = env->size;
  if (env->player_position.y < 0)
    env->player_position.y = env->size;
  if (env->player_position.x > env->size)
    env->player_position.x = 0;
  if (env->player_position.y > env->size)
    env->player_position.y = 0;
}

void c_step(Asteroids *env) {
  env->rewards[0] = 0;
  env->terminals[0] = 0;
  env->thruster_on = 0;

  // only when rendering
  if (global_game_over_timer > 0)
    return;

  int action = env->actions[0];
  for (int i = 0; i < env->frameskip; i++) {
    env->tick += 1;
    step_frame(env, action);
  }

  env->episode_return += env->rewards[0];
  if (env->terminals[0] == 1 || env->tick > MAX_TICK) {
    env->terminals[0] = 1;
    add_log(env);
    c_reset(env);
    return;
  }

  // env->rewards[0] = env->score;
  compute_observations(env);
}

const Color PUFF_RED = (Color){187, 0, 0, 255};
const Color PUFF_CYAN = (Color){0, 187, 187, 255};
const Color PUFF_WHITE = (Color){241, 241, 241, 241};
const Color PUFF_BACKGROUND = (Color){6, 24, 24, 255};

void draw_player(Asteroids *env) {
  if (global_game_over_timer > 0)
    return;

  float px = env->player_position.x;
  float py = env->player_position.y;

  if (DEBUG) {
    DrawPixel(px, py, RED);
    Vector2 dir = get_direction_vector(env);
    dir = (Vector2){dir.x * 10.0f, dir.y * 10.f};
    Vector2 t = (Vector2){dir.x + px, dir.y + py};
    DrawLineV(env->player_position, t, RED);
    DrawCircleLines(px, py, env->player_radius, RED);
  }

  Vector2 ps[8];

  // ship
  ps[0] = (Vector2){px - 10, py + 10};
  ps[1] = (Vector2){px + 10, py + 10};
  ps[2] = (Vector2){px, py - 20};
  ps[3] = (Vector2){px - 9, py + 6};
  ps[4] = (Vector2){px + 9, py + 6};
  ps[5] = (Vector2){px - 5, py + 6};
  ps[6] = (Vector2){px + 5, py + 6};
  ps[7] = (Vector2){px, py + 14};

  for (int i = 0; i < 8; i++)
    ps[i] = rotate_vector(ps[i], env->player_position, env->player_angle);

  DrawLineV(ps[0], ps[2], PUFF_RED);
  DrawLineV(ps[1], ps[2], PUFF_RED);

  DrawLineV(ps[3], ps[4], PUFF_RED);

  if (env->thruster_on) {
    DrawLineV(ps[5], ps[7], PUFF_RED);
    DrawLineV(ps[6], ps[7], PUFF_RED);
  }
}

void draw_particles(Asteroids *env) {
  for (int i = 0; i < MAX_PARTICLES; i++) {
    DrawCircle(env->particles[i].position.x, env->particles[i].position.y, 2, PUFF_RED);
  }
}

void draw_asteroids(Asteroids *env) {
  Asteroid as;
  for (int i = 0; i < MAX_ASTEROIDS; i++) {
    as = env->asteroids[i];
    if (as.radius == 0)
      continue;

    if (DEBUG)
      DrawCircleLines(as.position.x, as.position.y, as.radius, RED);

    for (int v = 0; v < as.num_vertices; v++) {
      int next_v = (v + 1) % as.num_vertices;
      Vector2 pos1 = {as.position.x + as.shape[v].x,
                      as.position.y + as.shape[v].y};
      Vector2 pos2 = {as.position.x + as.shape[next_v].x,
                      as.position.y + as.shape[next_v].y};
      DrawLineV(pos1, pos2, PUFF_CYAN);
    }
  }
}

void c_render(Asteroids *env) {
  if (!IsWindowReady()) {
    InitWindow(env->size, env->size, "PufferLib Asteroids");
    SetConfigFlags(FLAG_MSAA_4X_HINT);
    SetTargetFPS(60);
    global_render_flag = 1;
  }

  if (IsKeyDown(KEY_ESCAPE)) {
    exit(0);
  }

  if (env->terminals[0] == 1 && !global_game_over_started) {
    global_game_over_started = 1;
    global_game_over_timer = 120;
  }

  if (global_game_over_timer > 0) {
    global_game_over_timer--;
  } else {
    global_game_over_started = 0;
  }

  BeginDrawing();
  ClearBackground(PUFF_BACKGROUND);
  draw_player(env);
  draw_particles(env);
  draw_asteroids(env);

  DrawText(TextFormat("Score: %d", env->score), 10, 10, 20, PUFF_WHITE);
  DrawText(TextFormat("%d s", (int)(env->tick / 60)), env->size - 40, 10, 20, PUFF_WHITE);

  if (global_game_over_timer > 0) {
    const char *game_over_text = "GAME OVER";
    int text_width = MeasureText(game_over_text, 40);
    int x = (env->size - text_width) / 2;
    int y = env->size / 2 - 20;

    float alpha = (float)global_game_over_timer / 120.0f;
    int alpha_value = (int)(alpha * 255);

    Color text_color = ColorAlpha(PUFF_RED, alpha_value);
    DrawTextEx(GetFontDefault(), game_over_text, (Vector2){x, y}, 40, 2, text_color);
  }

  EndDrawing();
}

void c_close(Asteroids *env) {
  if (IsWindowReady()) {
    CloseWindow();
  }
}



================================================
FILE: pufferlib/ocean/asteroids/asteroids.py
================================================
import gymnasium
import numpy as np

import pufferlib
from pufferlib.ocean.asteroids import binding

class Asteroids(pufferlib.PufferEnv):
    def __init__(self, num_envs=1, render_mode=None, log_interval=128, buf=None, seed=0, size=500, frameskip=4):
        obs_shape = 4 + 5 * 20  # player pos, player vel, [asteroid pos, asteroid vel, asteroid size] x num asteroids
        self.single_observation_space = gymnasium.spaces.Box(low=-5, high=5,
            shape=(obs_shape,), dtype=np.float32)
        self.single_action_space = gymnasium.spaces.Discrete(4)  # forward, left, right, shoot
        self.render_mode = render_mode
        self.num_agents = num_envs

        super().__init__(buf)
        self.c_envs = binding.vec_init(self.observations, self.actions, self.rewards,
            self.terminals, self.truncations, num_envs, seed, size=size, frameskip=frameskip)
 
    def reset(self, seed=0):
        binding.vec_reset(self.c_envs, seed)
        return self.observations, []

    def step(self, actions):
        self.actions[:] = actions
        binding.vec_step(self.c_envs)
        info = [binding.vec_log(self.c_envs)]
        return (self.observations, self.rewards,
            self.terminals, self.truncations, info)

    def render(self):
        binding.vec_render(self.c_envs, 0)

    def close(self):
        binding.vec_close(self.c_envs)

if __name__ == '__main__':
    N = 4096
    env = Asteroids(num_envs=N)
    env.reset()
    steps = 0

    CACHE = 1024
    actions = np.random.randint(0, 5, (CACHE, N))

    import time
    start = time.time()
    while time.time() - start < 10:
        env.step(actions[steps % CACHE])
        steps += 1

    sps = int(env.num_agents*steps / (time.time() - start))
    print(f'Asteroids SPS: {sps:,}')



================================================
FILE: pufferlib/ocean/asteroids/binding.c
================================================
#include "asteroids.h"

#define Env Asteroids
#include "../env_binding.h"

static int my_init(Env *env, PyObject *args, PyObject *kwargs) {
  env->size = unpack(kwargs, "size");
  env->frameskip = unpack(kwargs, "frameskip");
  return 0;
}

static int my_log(PyObject *dict, Log *log) {
  assign_to_dict(dict, "perf", log->perf);
  assign_to_dict(dict, "score", log->score);
  assign_to_dict(dict, "episode_return", log->episode_return);
  assign_to_dict(dict, "episode_length", log->episode_length);
  return 0;
}



================================================
FILE: pufferlib/ocean/battle/battle.c
================================================
/* Pure C demo file for Battle. Build it with:
 * bash scripts/build_ocean.sh battle local (debug)
 * bash scripts/build_ocean.sh battle fast
 * We suggest building and debugging your env in pure C first. You
 * get faster builds and better error messages
 */
#include "battle.h"

/* Puffernet is our lightweight cpu inference library that
 * lets you load basic PyTorch model architectures so that
 * you can run them in pure C or on the web via WASM
 */
#include "puffernet.h"

int main() {
    // Weights are exported by running puffer export
    //Weights* weights = load_weights("resources/puffer_battle_weights.bin", 137743);

    //int logit_sizes[2] = {9, 5};
    //LinearLSTM* net = make_linearlstm(weights, num_agents, num_obs, logit_sizes, 2);

    Battle env = {
        .width = 1980,
        .height = 1020,
        .size_x = 8,
        .size_y = 2.0,
        .size_z = 8,
        .num_agents = 1024,
        .num_armies = 8,
    };
    init(&env);

    // Allocate these manually since they aren't being passed from Python
    int num_obs = 3*env.num_armies + 4*16 + 22 + 8;
    env.observations = calloc(env.num_agents*num_obs, sizeof(float));
    env.actions = calloc(3*env.num_agents, sizeof(int));
    env.rewards = calloc(env.num_agents, sizeof(float));
    env.terminals = calloc(env.num_agents, sizeof(unsigned char));

    // Always call reset and render first
    c_reset(&env);
    c_render(&env);

    int ctrl = 0;

    while (!WindowShouldClose()) {
        /*
        for (int i=0; i<env.num_agents; i++) {
            Entity* agent = &env.agents[i];
            int army = agent->army;
            float vx = env.observations[num_obs*i + 3*army];
            float vz = env.observations[num_obs*i + 3*army + 2];
            float yaw = env.observations[num_obs*i + 3*army + 3];
            float pitch = env.observations[num_obs*i + 3*army + 4];
            float x = env.observations[num_obs*i + 3*army + 6];
            float y = env.observations[num_obs*i + 3*army + 7];
            float z = env.observations[num_obs*i + 3*army + 8];

            if (agent->unit == INFANTRY || agent->unit == TANK || agent->unit == ARTILLERY) {
                env.actions[3*i] = (vx > 0.0f) ? 6 : 2;
                env.actions[3*i + 2] = (vz > 0.0f) ? 6 : 2;
            } else {
                float desired_pitch = atan2f(-y, sqrt(x*x + z*z));
                float pitch_error = desired_pitch - pitch;
                if (pitch_error > 0) {
                    env.actions[3*i] = 6; // pitch up
                } else if (pitch_error < 0) {
                    env.actions[3*i] = 2; // pitch down
                }
                env.actions[3*i] = 4;

                env.actions[3*i + 1] = 4;

                // Roll control
                float desired_yaw = atan2f(-x, -z); // Direction to origin
                float yaw_error = desired_yaw - yaw;

                // Normalize yaw_error to [-PI, PI]
                if (yaw_error > PI) yaw_error -= 2*PI;
                if (yaw_error < -PI) yaw_error += 2*PI;

                //printf("%f %f\n", yaw_error, yaw);

                if (yaw_error > 0.1f) {
                    env.actions[3*i + 1] = 2; // roll left
                } else if (yaw_error < -0.1f) {
                    env.actions[3*i + 1] = 6; // roll right
                } else {
                    env.actions[3*i + 1] = 4; // neutral roll (assuming 0 is valid)
                }

            }
            //env.actions[3*i] = 4;
            //env.actions[3*i + 1] = 4;
            //env.actions[3*i + 2] = 4;
            //float dpitch = atan2f(dz, sqrtf(dx*dx + dy*dy));
            //float droll = asinf(dz/sqrtf(dx*dx + dy*dy + dz*dz));
            //env.actions[3*i] = 6;
            //env.actions[3*i + 1] = (dpitch > 0.0f) ? 6 : 2;
            //env.actions[3*i + 2] = (droll > 0.0f) ? 6 : 2;
            //env.actions[3*i] = rand() % 9;
            //env.actions[3*i + 1] = rand() % 9;
            //env.actions[3*i + 2] = rand() % 9;
            //env.actions[3*i] = 4.0f;
            //env.actions[3*i + 1] = 4.0f;
            //env.actions[3*i + 2] = 4.0f;
        }
        */

        if (IsKeyDown(KEY_LEFT_SHIFT)) {
            if (IsKeyPressed(KEY_TAB)) {
                ctrl = (ctrl + 1) % env.num_agents;
            }
            int i = ctrl;
            float x = env.observations[num_obs*i + 3*env.num_armies + 6];
            float y = env.observations[num_obs*i + 3*env.num_armies + 7];
            float z = env.observations[num_obs*i + 3*env.num_armies + 8];

            Camera3D* camera = &(env.client->camera);
            camera->target = (Vector3){x, y, z};


            Entity* agent = &env.agents[i];
            Vector3 forward = Vector3RotateByQuaternion((Vector3){0, 0, 1}, agent->orientation);

            Vector3 local_up = Vector3RotateByQuaternion((Vector3){0, 1, 0}, agent->orientation);
            local_up = Vector3Normalize(local_up);

            camera->target = (Vector3){agent->x, agent->y, agent->z};

            camera->position = (Vector3){
                agent->x + 0.5*(- forward.x),
                agent->y + 0.5*(- forward.y) + 0.5f,
                agent->z + 0.5*(- forward.z)
            };


            /*
            Entity* agent = &env.agents[i];
            Vec3 forward = quat_rotate(agent->orientation, (Vec3){0, 0, -1}); // Ship's local forward
            vec3_normalize(&forward);

            Vec3 local_up = quat_rotate(agent->orientation, (Vec3){0, 1, 0}); // Ship's local up
            vec3_normalize(&local_up);

            // Compute negative forward and negative local up
            Vec3 neg_forward = {-forward.x, -forward.y, -forward.z};
            Vec3 neg_local_up = {-local_up.x, -local_up.y, -local_up.z};

            // Compute the vector 45 degrees between neg_forward and neg_local_up
            // Since forward and local_up are orthogonal, averaging gives a 45-degree angle
            Vec3 camera_dir = {
                neg_forward.x + neg_local_up.x,
                neg_forward.y + neg_local_up.y,
                neg_forward.z + neg_local_up.z
            };
            vec3_normalize(&camera_dir);
            printf("camera_dir: %f %f %f\n", camera_dir.x, camera_dir.y, camera_dir.z);

            // Scale by desired distance and offset from ship's position
            camera->position = (Vector3){
                agent->x + 1.0f*camera_dir.x,
                agent->y + 1.0f*camera_dir.y,
                agent->z + 1.0f*camera_dir.z
            };

            float dd = sqrtf(vx*vx + vy*vy + vz*vz);
            float forward_x = vx / dd;
            float forward_y = vy / dd;
            float forward_z = vz / dd;

            float dist = 0.5f;
            camera->position = (Vector3){
                x - dist*forward_x,
                y - dist*forward_y + 0.5f,
                z - dist*forward_z
            };
            */

            env.actions[3*i] = 4;
            if (IsKeyDown(KEY_W)) {
                env.actions[3*i] = 6;
            } else if (IsKeyDown(KEY_S)) {
                env.actions[3*i] = 2;
            }

            env.actions[3*i + 1] = 4;
            if (IsKeyDown(KEY_A)) {
                env.actions[3*i + 1] = 2;
            } else if (IsKeyDown(KEY_D)) {
                env.actions[3*i + 1] = 6;
            }
        }

        //forward_linearlstm(net, env.observations, env.actions);
        compute_observations(&env);
        c_step(&env);
        c_render(&env);
    }

    // Try to clean up after yourself
    //free_linearlstm(net);
    free(env.observations);
    free(env.actions);
    free(env.rewards);
    free(env.terminals);
    c_close(&env);
}




================================================
FILE: pufferlib/ocean/battle/battle.h
================================================
/* Battle: a sample multiagent env about puffers eating stars.
 * Use this as a tutorial and template for your own multiagent envs.
 * We suggest starting with the Squared env for a simpler intro.
 * Star PufferLib on GitHub to support. It really, really helps!
 */

#include <stdlib.h>
#include <string.h>
#include <math.h>
#include <stdio.h>
#include <float.h>
#include <assert.h>
#include "raylib.h"
#include "raymath.h"
#include "rlgl.h"
#include "simplex.h"

#define RLIGHTS_IMPLEMENTATION
#include "rlights.h"


#if defined(PLATFORM_DESKTOP)
    #define GLSL_VERSION 330
#else
    #define GLSL_VERSION 100
#endif

#define MAX_SPEED 0.01f
#define MAX_FACTORY_SPEED 0.001f

#define DRONE 0
#define MOTHERSHIP 1
#define FIGHTER 2
#define BOMBER 3
#define INFANTRY 4
#define TANK 5
#define ARTILLERY 6
#define BASE 7

#define AGENT_OBS 16

static inline float clampf(float v, float min, float max) {
  if (v < min)
    return min;
  if (v > max)
    return max;
  return v;
}

float clip(float val, float min, float max) {
    if (val < min) {
        return min;
    } else if (val > max) {
        return max;
    }
    return val;
}

float clip_angle(float theta) {
    if (theta < -PI) {
        return theta + 2.0f*PI;
    } else if (theta > PI) {
        return theta - 2.0f*PI;
    }
    return theta;
}

float randf(float min, float max) {
    return min + (max - min)*(float)rand()/(float)RAND_MAX;
}

float randi(int min, int max) {
    return min + (max - min)*(float)rand()/(float)RAND_MAX;
}

typedef struct {
    float perf;
    float score;
    float collision_rate;
    float oob_rate;
    float episode_return;
    float episode_length;
    float n;
} Log;

typedef struct {
    Camera3D camera;
    Light light;
    Model models[8];
    Mesh* mesh;
    Model model;
    Shader light_shader;
    Shader terrain_shader;
    Texture2D terrain_texture;
    Texture2D vehicle_texture;
    int terrain_shader_loc;
    unsigned char *terrain_data;
} Client;

typedef struct {
    float x;
    float y;
    float z;
    float vx;
    float vy;
    float vz;
    float speed;
    float health;
    float max_turn;
    float max_speed;
    float attack_damage;
    float attack_range;
    Quaternion orientation;
    int army;
    int unit;
    int target;
    int episode_length;
    float episode_return;
} Entity;

typedef struct {
    Log log;
    Client* client;
    Entity* agents;
    Entity* bases;
    float* observations;
    float* actions;
    float* rewards;
    unsigned char* terminals;
    int width;
    int height;
    float size_x;
    float size_y;
    float size_z;
    int terrain_width;
    int terrain_height;
    int num_agents;
    int num_armies;
    float* terrain;
} Battle;

int map_idx(Battle* env, float x, float y) {
    return env->terrain_width*(int)y + (int)x;
}

float ground_height(Battle* env, float x, float z) {
    int agent_map_x = 128*x + 128*env->size_x;
    int agent_map_z = 128*z + 128*env->size_z;
    if (agent_map_x == 256*env->size_x) {
        agent_map_x -= 1;
    }
    if (agent_map_z == 256*env->size_z) {
        agent_map_z -= 1;
    }
    int idx = map_idx(env, agent_map_x, agent_map_z);
    float terrain_height = env->terrain[idx];
    return (terrain_height - 128.0f*env->size_y) / 128.0f;
}

void perlin_noise(float* map, int width, int height,
        float base_frequency, int octaves, int offset_x, int offset_y, float glob_scale) {
    float frequencies[octaves];
    for (int i = 0; i < octaves; i++) {
        frequencies[i] = base_frequency*pow(2, i);
    }

    float min_value = FLT_MAX;
    float max_value = FLT_MIN;
    for (int r = 0; r < height; r++) {
        for (int c = 0; c < width; c++) {
            int adr = r*width + c;
            for (int oct = 0; oct < octaves; oct++) {
                float freq = frequencies[oct];
                map[adr] += (1.0/pow(2, oct))*noise2(freq*c + offset_x, freq*r + offset_y);
            }
            float val = map[adr];
            if (val < min_value) {
                min_value = val;
            }
            if (val > max_value) {
                max_value = val;
            }
        }
    }

    float scale = 1.0/(max_value - min_value);
    for (int r = 0; r < height; r++) {
        for (int c = 0; c < width; c++) {
            int adr = r*width + c;
            map[adr] = glob_scale * scale * (map[adr] - min_value);
            if (map[adr] < 16.0f) {
                map[adr] = 0.0f;
            } else {
                map[adr] -= 16.0f;
            }
        }
    }
}

void init(Battle* env) {
    env->agents = calloc(env->num_agents, sizeof(Entity));
    env->bases = calloc(env->num_armies, sizeof(Entity));
    env->terrain_width = 256*env->size_x;
    env->terrain_height = 256*env->size_z;
    env->terrain = calloc(env->terrain_width*env->terrain_height, sizeof(float));
    perlin_noise(env->terrain, env->terrain_width, env->terrain_height, 1.0/2048.0, 8, 0, 0, 256);
}

void update_abilities(Entity* agent) {
    if (agent->unit == DRONE) {
        agent->health = 0.4f;
        agent->attack_damage = 0.1f;
        agent->attack_range = 0.15f;
        agent->max_turn = 2.0f;
        agent->max_speed = 1.0f;
    } else if (agent->unit == FIGHTER) {
        agent->health = 1.0f;
        agent->attack_damage = 0.5f;
        agent->attack_range = 0.25f;
        agent->max_turn = 1.0f;
        agent->max_speed = 0.75f;
    } else if (agent->unit == MOTHERSHIP) {
        agent->health = 10.0f;
        agent->attack_damage = 2.0f;
        agent->attack_range = 0.4f;
        agent->max_turn = 0.5f;
        agent->max_speed = 0.5f;
    } else if (agent->unit == BOMBER) {
        agent->health = 1.0f;
        agent->attack_damage = 1.0f;
        agent->attack_range = 0.1f;
        agent->max_turn = 0.5f;
        agent->max_speed = 0.5f;
    } else if (agent->unit == INFANTRY) {
        agent->health = 0.2f;
        agent->attack_damage = 0.2f;
        agent->attack_range = 0.2f;
        agent->max_turn = 2.0f;
        agent->max_speed = 0.25f;
    } else if (agent->unit == TANK) {
        agent->health = 2.0f;
        agent->attack_damage = 0.5f;
        agent->attack_range = 0.25f;
        agent->max_turn = 0.25f;
        agent->max_speed = 0.75f;
    } else if (agent->unit == ARTILLERY) {
        agent->health = 2.0f;
        agent->attack_damage = 2.0f;
        agent->attack_range = 0.7f;
        agent->max_turn = 0.5f;
        agent->max_speed = 0.25f;
    }
}

void respawn(Battle* env, int idx) {
    Entity* agent = &env->agents[idx];
    int army = agent->army;
    agent->orientation = QuaternionIdentity();
    agent->vx = 0;
    agent->vy = 0;
    agent->vz = 0;

    if (agent->unit == DRONE) {
        int team_mothership_idx = 64*(idx / 64); // Hardcoded per army
        agent->x = env->agents[team_mothership_idx].x;
        agent->y = env->agents[team_mothership_idx].y;
        agent->z = env->agents[team_mothership_idx].z;
        if (agent->unit == INFANTRY || agent->unit == TANK || agent->unit == ARTILLERY) {
            agent->y = ground_height(env, agent->x, agent->z);
        }
        return;
    }

    Entity* base = &env->bases[army];
    agent->x = base->x;
    agent->z = base->z;
    float height = ground_height(env, agent->x, agent->z);
    if (agent->unit == INFANTRY || agent->unit == TANK || agent->unit == ARTILLERY) {
        agent->y = height;
    } else {
        agent->y = clampf(height + 0.2f, -env->size_y, env->size_y);
    }

    return;
}


bool attack_air(Entity *agent, Entity *target) {
    float dx = target->x - agent->x;
    float dy = target->y - agent->y;
    float dz = target->z - agent->z;
    float dd = sqrtf(dx*dx + dy*dy + dz*dz);

    if (dd > agent->attack_range) {
        return false;
    }

    Vector3 forward = Vector3RotateByQuaternion((Vector3){0, 0, 1}, agent->orientation);
    forward = Vector3Normalize(forward);

    // Unit vec to target
    Vector3 to_target = {dx, dy, dz};
    to_target = Vector3Normalize(to_target);

    float angle = Vector3Angle(forward, to_target);
    if (angle < PI/6.0f) {
        return true;
    }
    return false;
}

bool attack_ground(Entity *agent, Entity *target) {
    if (target->unit == FIGHTER) {
        return false;
    }
    if (target->unit == MOTHERSHIP) {
        return false;
    }
    if (target->unit == BOMBER) {
        return false;
    }
    if (target->unit == DRONE) {
        return false;
    }

    float dx = target->x - agent->x;
    float dz = target->z - agent->z;
    float dd = sqrtf(dx*dx + dz*dz);

    if (dd > agent->attack_range) {
        return false;
    }

    Vector3 forward = Vector3RotateByQuaternion((Vector3){0, 0, 1}, agent->orientation);
    forward = Vector3Normalize(forward);

    // Unit vec to target
    Vector3 to_target = {dx, 0, dz};
    to_target = Vector3Normalize(to_target);

    float angle = Vector3Angle(forward, to_target);
    if (angle < PI/6) {
        return true;
    }
    return false;
}

bool attack_bomber(Entity *agent, Entity *target) {
    if (target->unit == DRONE) {
        return false;
    }
    if (target->unit == FIGHTER) {
        return false;
    }
    if (target->unit == MOTHERSHIP) {
        return false;
    }
    if (target->unit == BOMBER) {
        return false;
    }

    float dx = target->x - agent->x;
    float dz = target->z - agent->z;
    float dd = sqrtf(dx*dx + dz*dz);

    if (dd > agent->attack_range) {
        return false;
    }

    return true;
}

bool attack_aa(Entity *agent, Entity *target) {
    if (target->unit == INFANTRY) {
        return false;
    }
    if (target->unit == TANK) {
        return false;
    }
    if (target->unit == ARTILLERY) {
        return false;
    }

    float dx = target->x - agent->x;
    float dy = target->y - agent->y;
    float dz = target->z - agent->z;
    float dd = sqrtf(dx*dx + dz*dz);

    if (dd > agent->attack_range) {
        return false;
    }

    Vector3 forward = Vector3RotateByQuaternion((Vector3){0, 0, 1}, agent->orientation);
    forward = Vector3Normalize(forward);

    // Unit vec to target
    Vector3 to_target = {dx, dy, dz};
    to_target = Vector3Normalize(to_target);

    float angle = Vector3Angle(forward, to_target);
    if (angle < PI/6) {
        return true;
    }
}

void move_basic(Battle* env, Entity* agent, float* actions) {
    float d_vx = actions[0]/100.0f;
    float d_vy = actions[1]/100.0f;
    float d_vz = actions[2]/100.0f;

    agent->vx += d_vx;
    agent->vy += d_vy;
    agent->vz += d_vz;

    agent->vx = clip(agent->vx, -MAX_SPEED, MAX_SPEED);
    agent->vy = clip(agent->vy, -MAX_SPEED, MAX_SPEED);
    agent->vz = clip(agent->vz, -MAX_SPEED, MAX_SPEED);

    agent->x += agent->vx;
    agent->y += agent->vy;
    agent->z += agent->vz;

    agent->x = clip(agent->x, -env->size_x, env->size_x);
    agent->y = clip(agent->y, -env->size_y, env->size_y);
    agent->z = clip(agent->z, -env->size_z, env->size_z);
}

void move_ground(Battle* env, Entity* agent, float* actions) {
    float d_theta = -actions[1]/10.0f;

    // Update speed and clamp
    agent->speed = agent->max_speed * MAX_SPEED;

    Quaternion q_y = QuaternionFromAxisAngle((Vector3){0, 1, 0}, d_theta);
    agent->orientation = QuaternionMultiply(q_y, agent->orientation);

    Vector3 forward = Vector3RotateByQuaternion((Vector3){0, 0, 1}, agent->orientation);
    forward = Vector3Normalize(forward);

    agent->speed = agent->max_speed * MAX_SPEED;
    agent->vx = agent->speed * forward.x;
    agent->vz = agent->speed * forward.z;
    agent->x += agent->vx;
    agent->z += agent->vz;

    agent->x = clip(agent->x, -env->size_x, env->size_x);
    agent->z = clip(agent->z, -env->size_z, env->size_z);
    agent->y = ground_height(env, agent->x, agent->z);
}

Entity* nearest_enemy(Battle* env, Entity* agent) {
    Entity* nearest = NULL;
    float nearest_dist = 999999;
    for (int i=0; i<env->num_agents; i++) {
        Entity* other = &env->agents[i];
        if (other->army == agent->army) {
            continue;
        }
        float dx = other->x - agent->x;
        float dy = other->y - agent->y;
        float dz = other->z - agent->z;
        float dd = dx*dx + dy*dy + dz*dz;
        if (dd < nearest_dist) {
            nearest_dist = dd;
            nearest = other;
        }
    }
    return nearest;
}

// Cheats physics and moves directly to the nearest enemy
void scripted_move(Battle* env, Entity* agent, bool is_air) {
    Entity* target = nearest_enemy(env, agent);
    if (target == NULL) {
        return;
    }
    float dx = target->x - agent->x;
    float dy = target->y - agent->y;
    float dz = target->z - agent->z;

    // Add some noise
    dx += randf(-0.1f, 0.1f);
    dy += randf(-0.1f, 0.1f);
    dz += randf(-0.1f, 0.1f);

    float dd = dx*dx + dz*dz;
    if (is_air) {
        dd += dy*dy;
    }

    dd = sqrtf(dd);
    dx /= dd;
    dy /= dd;
    dz /= dd;

    
    float target_x;
    float target_y;
    float target_z;
    if (dd > 0.05f) {
        target_x = agent->x + dx*agent->max_speed*MAX_SPEED;
        target_y = agent->y + dy*agent->max_speed*MAX_SPEED;
        target_z = agent->z + dz*agent->max_speed*MAX_SPEED;
    } else {
        target_x = agent->x - dx*agent->max_speed*MAX_SPEED;
        target_y = agent->y - dy*agent->max_speed*MAX_SPEED;
        target_z = agent->z - dz*agent->max_speed*MAX_SPEED;
    }
    
    float height = ground_height(env, target_x, target_z);
    if (is_air) {
        if (target_y < height + 0.5f) {
            target_y = height + 0.5f;
        }
    } else {
        target_y = height;
    }

    agent->x = target_x;
    agent->y = target_y;
    agent->z = target_z;

    agent->x = clip(agent->x, -env->size_x, env->size_x);
    agent->y = clip(agent->y, -env->size_y, env->size_y);
    agent->z = clip(agent->z, -env->size_z, env->size_z);

    // Update orientation to target
    Vector3 target_forward = {dx, 0, dz};
    if (is_air) {
        target_forward.y = dy;
    }
    target_forward = Vector3Normalize(target_forward);

    Vector3 current_forward = Vector3RotateByQuaternion((Vector3){0, 0, 1}, agent->orientation);
    current_forward = Vector3Normalize(current_forward);

    Quaternion q = QuaternionFromVector3ToVector3(current_forward, target_forward);
    agent->orientation = QuaternionMultiply(q, agent->orientation);
}

void move_ship(Battle* env, Entity* agent, float* actions, int i) {
    // Compute deltas from actions (same as original)
    float d_pitch = agent->max_turn * actions[0] / 10.0f;
    float d_roll = agent->max_turn * actions[1] / 10.0f;

    // Update speed and clamp
    agent->speed = agent->max_speed * MAX_SPEED;

    Vector3 forward = Vector3RotateByQuaternion((Vector3){0, 0, 1}, agent->orientation);
    forward = Vector3Normalize(forward);

    Vector3 local_up = Vector3RotateByQuaternion((Vector3){0, 1, 0}, agent->orientation);
    local_up = Vector3Normalize(local_up);

    Vector3 right = Vector3CrossProduct(forward, local_up); // Ship's local right
    right = Vector3Normalize(right);

    // Create rotation quaternions
    /*
    if (i == 0) {
        printf("actions: %d %d %d\n", actions[0], actions[1], actions[2]);
        printf("orientation: %f %f %f %f\n", agent->orientation.w, agent->orientation.x, agent->orientation.y, agent->orientation.z);
        printf("Local up: %f %f %f\n", local_up.x, local_up.y, local_up.z);
        printf("Forward: %f %f %f\n", forward.x, forward.y, forward.z);
        printf("Right: %f %f %f\n", right.x, right.y, right.z);
        printf("d_pitch: %f\n, d_roll: %f\n", d_pitch, d_roll);
    }
    */

    float d_yaw = 0.0;
    Quaternion q_yaw = QuaternionFromAxisAngle(local_up, d_yaw);
    Quaternion q_roll = QuaternionFromAxisAngle(forward, d_roll);
    Quaternion q_pitch = QuaternionFromAxisAngle(right, d_pitch);

    /*
    if (i == 0) {
        printf("q_yaw: %f %f %f %f\n", q_yaw.w, q_yaw.x, q_yaw.y, q_yaw.z);
        printf("q_roll: %f %f %f %f\n", q_roll.w, q_roll.x, q_roll.y, q_roll.z);
        printf("q_pitch: %f %f %f %f\n", q_pitch.w, q_pitch.x, q_pitch.y, q_pitch.z);
    }
    */

    Quaternion q = QuaternionMultiply(q_roll, QuaternionMultiply(q_pitch, q_yaw));
    q = QuaternionNormalize(q);

    forward = Vector3RotateByQuaternion(forward, q);
    forward = Vector3Normalize(forward);

    agent->orientation = QuaternionMultiply(q, agent->orientation);

    // Jank plane physics
    Vector3 v = {
        agent->speed * (forward.x + local_up.x),
        agent->speed * (forward.y + local_up.y - 1.0f),
        agent->speed * (forward.z + local_up.z)
    };

    agent->x += v.x;
    agent->y += v.y;
    agent->z += v.z;

    // Just for visualization
    agent->vx = v.x;
    agent->vy = v.y;
    agent->vz = v.z;

    // Clamp position to environment bounds
    agent->x = clampf(agent->x, -env->size_x, env->size_x);
    agent->y = clampf(agent->y, -env->size_y, env->size_y);
    agent->z = clampf(agent->z, -env->size_z, env->size_z);
}

typedef struct {
    float distance;
    float dx;
    float dy;
    float dz;
    float same_team;
    int idx;
} AgentObs;

int compare_agent_obs(const void* a, const void* b) {
    AgentObs* oa = (AgentObs*)a;
    AgentObs* ob = (AgentObs*)b;
    if (oa->distance < ob->distance) {
        return -1;
    } else if (oa->distance > ob->distance) {
        return 1;
    }
    return 0;
}

void compute_observations(Battle* env) {
    AgentObs agent_obs[env->num_agents];

    int obs_idx = 0;
    for (int a=0; a<env->num_agents/2; a++) {
        assert(obs_idx == a*(6*env->num_armies + 19 + 8));

        // Distance to each base
        Entity* agent = &env->agents[a];
        int team = agent->army;
        float dists[env->num_armies];
        for (int i=0; i<env->num_armies; i++) {
            dists[i] = 999999;
        }
        for (int f=0; f<env->num_armies; f++) {
            Entity* base = &env->bases[f];
            float dx = base->x - agent->x;
            float dy = base->y - agent->y;
            float dz = base->z - agent->z;
            float dd = dx*dx + dy*dy + dz*dz;
            int type = f % env->num_armies;
            if (dd < dists[type]) {
                dists[type] = dd;
                env->observations[obs_idx + 3*type] = dx;
                env->observations[obs_idx + 3*type + 1] = dy;
                env->observations[obs_idx + 3*type + 2] = dz;
            }
        }
        obs_idx += 3*env->num_armies;


        // Distance to each agent. Slow O(n^2) naive implementation
        float x = agent->x;
        float y = agent->y;
        float z = agent->z;
        for (int i=0; i<env->num_agents; i++) {
            Entity* other = &env->agents[i];
            float dx = other->x - x;
            float dy = other->y - y;
            float dz = other->z - z;
            float distance = dx*dx + dy*dy + dz*dz;
            AgentObs* o = &agent_obs[i];
            o->dx = dx;
            o->dy = dy;
            o->dz = dz;
            if (other->army == agent->army) {
                o->same_team = 1.0f;
                o->distance = 99999.0f;
            } else {
                o->same_team = 0.0f;
                o->distance = distance;
            }
            o->idx = i;
        }
        qsort(agent_obs, env->num_agents, sizeof(AgentObs), compare_agent_obs);

        for (int i=0; i<AGENT_OBS; i++) {
            env->observations[obs_idx++] = agent_obs[i].dx;
            env->observations[obs_idx++] = agent_obs[i].dy;
            env->observations[obs_idx++] = agent_obs[i].dz;
            env->observations[obs_idx++] = agent_obs[i].same_team;
        }

        // Individual agent stats
        env->observations[obs_idx++] = agent->vx/MAX_SPEED;
        env->observations[obs_idx++] = agent->vy/MAX_SPEED;
        env->observations[obs_idx++] = agent->vz/MAX_SPEED;
        env->observations[obs_idx++] = agent->orientation.w;
        env->observations[obs_idx++] = agent->orientation.x;
        env->observations[obs_idx++] = agent->orientation.y;
        env->observations[obs_idx++] = agent->orientation.z;
        env->observations[obs_idx++] = agent->x;
        env->observations[obs_idx++] = agent->y;
        env->observations[obs_idx++] = agent->z;
        env->observations[obs_idx++] = agent->y - ground_height(env, agent->x, agent->z);
        env->observations[obs_idx++] = abs(agent->x) - 0.95f*env->size_x;
        env->observations[obs_idx++] = abs(agent->z) - 0.95f*env->size_z;
        env->observations[obs_idx++] = abs(agent->y) - 0.95f*env->size_y;
        env->observations[obs_idx++] = agent->speed;
        env->observations[obs_idx++] = agent->health;
        env->observations[obs_idx++] = agent->max_turn;
        env->observations[obs_idx++] = agent->max_speed;
        env->observations[obs_idx++] = agent->attack_damage;
        env->observations[obs_idx++] = agent->attack_range;
        env->observations[obs_idx++] = env->rewards[a];
        env->observations[obs_idx++] = env->terminals[a];

        // Hardcoded 8 unit types
        memset(&env->observations[obs_idx], 0, 8*sizeof(float));
        env->observations[obs_idx + agent->unit] = 1.0f;
        obs_idx += 8;
    }
}

// Required function
void c_reset(Battle* env) {
    int agents_per_army = env->num_agents / env->num_armies;
    for (int i=0; i<env->num_armies; i++) {
        bool spawn = false;
        Entity* base = &env->bases[i];
        while (!spawn) {
            base->x = randf(0.5 - env->size_x, env->size_x - 0.5);
            base->z = randf(0.5 - env->size_z, env->size_z - 0.5);
            base->y = ground_height(env, base->x, base->z);
            base->army = i;
            spawn = true;

            for (int j=0; j<i; j++) {
                Entity* other = &env->bases[j];
                float dx = other->x - base->x;
                float dz = other->z - base->z;
                float dd = sqrtf(dx*dx + dz*dz);
                if (dd < 2.0f) {
                    spawn = false;
                    break;
                }
            }
        }
    }

    for (int army=0; army<env->num_armies; army++) {
        for (int i=0; i<agents_per_army; i++) {
            int idx = army*agents_per_army + i;
            Entity* agent = &env->agents[idx];
            if (i % 64 == 0) {
                agent->unit = MOTHERSHIP;
            } else if (i % 64 <= 4) {
                agent->unit = TANK;
            } else if (i % 64 <= 6) {
                agent->unit = ARTILLERY;
            } else if (i % 64 <= 10) {
                agent->unit = BOMBER;
            } else if (i % 64 <= 14) {
                agent->unit = FIGHTER;
            } else if (i % 64 <= 32) {
                agent->unit = INFANTRY;
            } else {
                agent->unit = DRONE;
            }

            agent->army = army;
            agent->orientation = QuaternionIdentity();
            agent->episode_length = 0;
            agent->target = -1;
            update_abilities(agent);
            respawn(env, idx);
        }
    }
    compute_observations(env);
}

void c_step(Battle* env) {
    memset(env->rewards, 0, env->num_agents/2*sizeof(float));
    memset(env->terminals, 0, env->num_agents/2*sizeof(unsigned char));

    for (int i=0; i<env->num_agents; i++) {
        Entity* agent = &env->agents[i];
        agent->episode_length += 1;
        agent->target = -1;

        bool done = false;
        float collision = 0.0f;
        float oob = 0.0f;
        float reward = 0.0f;
        if (agent->health <= 0) {
            done = true;
            reward = 0.0f;
        }
        if (agent->unit == DRONE || agent->unit == FIGHTER || agent->unit == BOMBER || agent->unit == MOTHERSHIP) {
            // Crash into terrain
            float terrain_height = ground_height(env, agent->x, agent->z);
            if (agent->y < terrain_height) {
                collision = 1.0f;
                done = true;
                reward = -1.0f;
            }
        }
        if (
            agent->x < -0.95f*env->size_x || agent->x > 0.95f*env->size_x ||
            agent->z < -0.95f*env->size_z || agent->z > 0.95f*env->size_z ||
            agent->y > 0.95f*env->size_y
        ) {
            done = true;
            reward = -1.0f;
            oob = 1.0f;
        }

        if (done) {
            update_abilities(agent);
            respawn(env, i);
            agent->episode_return += reward;
            if (i < env->num_agents/2) {
                env->rewards[i] = reward;
                env->terminals[i] = 1;
                env->log.score = env->log.episode_return;
                env->log.episode_length += agent->episode_length;
                env->log.episode_return += agent->episode_return;
                env->log.collision_rate += collision;
                env->log.oob_rate += oob;
                env->log.n++;

            }
            agent->episode_length = 0;
            agent->episode_return = 0;
        }

        //move_basic(env, agent, env->actions + 3*i);
        if (agent->unit == INFANTRY || agent->unit == TANK || agent->unit == ARTILLERY) {
            if (i < env->num_agents/2) {
                move_ground(env, agent, env->actions + 3*i);
            } else {
                scripted_move(env, agent, false);
            }
        } else {
            if (i < env->num_agents/2) {
                move_ship(env, agent, env->actions + 3*i, i);
            } else {
                scripted_move(env, agent, true);
            }
        }
    }

    for (int i=0; i<env->num_agents; i++) {
        Entity* agent = &env->agents[i];
        for (int j=0; j<env->num_agents; j++) {
            if (j == i) {
                continue;
            }
            Entity* target = &env->agents[j];
            if (agent->army == target->army) {
                continue;
            }
            bool can_attack = false;
            if (agent->unit == INFANTRY || agent->unit == TANK) {
                can_attack = attack_ground(agent, target);
            } else if (agent->unit == ARTILLERY) {
                can_attack = attack_aa(agent, target);
            } else if (agent->unit == BOMBER) {
                can_attack = attack_bomber(agent, target);
            } else {
                can_attack = attack_air(agent, target);
            }
            if (!can_attack) {
                continue;
            }
            agent->target = j;
            if (i < env->num_agents/2) {
                env->rewards[i] += 0.25f;
                agent->episode_return += 0.25f;
            }
            target->health -= agent->attack_damage;
            break;
        }
    }

    if (rand() % 9000 == 0) {
        c_reset(env);
    }

    compute_observations(env);
}

Color COLORS[8] = {
    (Color){0, 255, 255, 255},
    (Color){255, 0, 0, 255},
    (Color){0, 255, 0, 255},
    (Color){255, 255, 0, 255},
    (Color){255, 0, 255, 255},
    (Color){0, 0, 255, 255},
    (Color){128, 255, 0, 255},
    (Color){255, 128, 0, 255},
};

Mesh* create_heightmap_mesh(float* heightMap, Vector3 size) {
    int mapX = size.x;
    int mapZ = size.z;

    // NOTE: One vertex per pixel
    Mesh* mesh = (Mesh*)calloc(1, sizeof(Mesh));
    mesh->triangleCount = (mapX - 1)*(mapZ - 1)*2;    // One quad every four pixels

    mesh->vertexCount = mesh->triangleCount*3;

    mesh->vertices = (float *)RL_MALLOC(mesh->vertexCount*3*sizeof(float));
    mesh->normals = (float *)RL_MALLOC(mesh->vertexCount*3*sizeof(float));
    mesh->texcoords = (float *)RL_MALLOC(mesh->vertexCount*2*sizeof(float));
    mesh->colors = NULL;
    UploadMesh(mesh, false);
    return mesh;
}

void update_heightmap_mesh(Mesh* mesh, float* heightMap, Vector3 size) {
    int mapX = size.x;
    int mapZ = size.z;

    int vCounter = 0;       // Used to count vertices float by float
    int tcCounter = 0;      // Used to count texcoords float by float
    int nCounter = 0;       // Used to count normals float by float

    //Vector3 scaleFactor = { size.x/(mapX - 1), 1.0f, size.z/(mapZ - 1) };
    Vector3 scaleFactor = { 1.0f, 1.0f, 1.0f};

    Vector3 vA = { 0 };
    Vector3 vB = { 0 };
    Vector3 vC = { 0 };
    Vector3 vN = { 0 };

    for (int z = 0; z < mapZ-1; z++)
    {
        for (int x = 0; x < mapX-1; x++)
        {
            // Fill vertices array with data
            //----------------------------------------------------------

            // one triangle - 3 vertex
            mesh->vertices[vCounter] = (float)x*scaleFactor.x;
            mesh->vertices[vCounter + 1] = heightMap[x + z*mapX]*scaleFactor.y;
            mesh->vertices[vCounter + 2] = (float)z*scaleFactor.z;

            mesh->vertices[vCounter + 3] = (float)x*scaleFactor.x;
            mesh->vertices[vCounter + 4] = heightMap[x + (z + 1)*mapX]*scaleFactor.y;
            mesh->vertices[vCounter + 5] = (float)(z + 1)*scaleFactor.z;

            mesh->vertices[vCounter + 6] = (float)(x + 1)*scaleFactor.x;
            mesh->vertices[vCounter + 7] = heightMap[(x + 1) + z*mapX]*scaleFactor.y;
            mesh->vertices[vCounter + 8] = (float)z*scaleFactor.z;

            // Another triangle - 3 vertex
            mesh->vertices[vCounter + 9] = mesh->vertices[vCounter + 6];
            mesh->vertices[vCounter + 10] = mesh->vertices[vCounter + 7];
            mesh->vertices[vCounter + 11] = mesh->vertices[vCounter + 8];

            mesh->vertices[vCounter + 12] = mesh->vertices[vCounter + 3];
            mesh->vertices[vCounter + 13] = mesh->vertices[vCounter + 4];
            mesh->vertices[vCounter + 14] = mesh->vertices[vCounter + 5];

            mesh->vertices[vCounter + 15] = (float)(x + 1)*scaleFactor.x;
            mesh->vertices[vCounter + 16] = heightMap[(x + 1) + (z + 1)*mapX]*scaleFactor.y;
            mesh->vertices[vCounter + 17] = (float)(z + 1)*scaleFactor.z;
            vCounter += 18;     // 6 vertex, 18 floats

            // Fill texcoords array with data
            //--------------------------------------------------------------
            mesh->texcoords[tcCounter] = (float)x/(mapX - 1);
            mesh->texcoords[tcCounter + 1] = (float)z/(mapZ - 1);

            mesh->texcoords[tcCounter + 2] = (float)x/(mapX - 1);
            mesh->texcoords[tcCounter + 3] = (float)(z + 1)/(mapZ - 1);

            mesh->texcoords[tcCounter + 4] = (float)(x + 1)/(mapX - 1);
            mesh->texcoords[tcCounter + 5] = (float)z/(mapZ - 1);

            mesh->texcoords[tcCounter + 6] = mesh->texcoords[tcCounter + 4];
            mesh->texcoords[tcCounter + 7] = mesh->texcoords[tcCounter + 5];

            mesh->texcoords[tcCounter + 8] = mesh->texcoords[tcCounter + 2];
            mesh->texcoords[tcCounter + 9] = mesh->texcoords[tcCounter + 3];

            mesh->texcoords[tcCounter + 10] = (float)(x + 1)/(mapX - 1);
            mesh->texcoords[tcCounter + 11] = (float)(z + 1)/(mapZ - 1);
            tcCounter += 12;    // 6 texcoords, 12 floats

            // Fill normals array with data
            //--------------------------------------------------------------
            for (int i = 0; i < 18; i += 9)
            {
                vA.x = mesh->vertices[nCounter + i];
                vA.y = mesh->vertices[nCounter + i + 1];
                vA.z = mesh->vertices[nCounter + i + 2];

                vB.x = mesh->vertices[nCounter + i + 3];
                vB.y = mesh->vertices[nCounter + i + 4];
                vB.z = mesh->vertices[nCounter + i + 5];

                vC.x = mesh->vertices[nCounter + i + 6];
                vC.y = mesh->vertices[nCounter + i + 7];
                vC.z = mesh->vertices[nCounter + i + 8];

                vN = Vector3Normalize(Vector3CrossProduct(Vector3Subtract(vB, vA), Vector3Subtract(vC, vA)));

                mesh->normals[nCounter + i] = vN.x;
                mesh->normals[nCounter + i + 1] = vN.y;
                mesh->normals[nCounter + i + 2] = vN.z;

                mesh->normals[nCounter + i + 3] = vN.x;
                mesh->normals[nCounter + i + 4] = vN.y;
                mesh->normals[nCounter + i + 5] = vN.z;

                mesh->normals[nCounter + i + 6] = vN.x;
                mesh->normals[nCounter + i + 7] = vN.y;
                mesh->normals[nCounter + i + 8] = vN.z;
            }

            nCounter += 18;     // 6 vertex, 18 floats
        }
    }

    // Upload vertex data to GPU (static mesh)
    UpdateMeshBuffer(*mesh, 0, mesh->vertices, mesh->vertexCount * 3 * sizeof(float), 0); // Update vertices
    UpdateMeshBuffer(*mesh, 2, mesh->normals, mesh->vertexCount * 3 * sizeof(float), 0); // Update normals
}


// Required function. Should handle creating the client on first call
void c_render(Battle* env) {
    if (env->client == NULL) {
        SetConfigFlags(FLAG_MSAA_4X_HINT);
        InitWindow(env->width, env->height, "PufferLib Battle");
        SetTargetFPS(30);
        Client* client = (Client*)calloc(1, sizeof(Client));
        env->client = client;
        client->models[DRONE] = LoadModel("resources/battle/drone.glb");
        client->models[FIGHTER] = LoadModel("resources/battle/fighter.glb");
        client->models[MOTHERSHIP] = LoadModel("resources/battle/mothership.glb");
        client->models[BOMBER] = LoadModel("resources/battle/bomber.glb");
        client->models[INFANTRY] = LoadModel("resources/battle/car.glb");
        client->models[TANK] = LoadModel("resources/battle/tank.glb");
        client->models[ARTILLERY] = LoadModel("resources/battle/artillery.glb");
        client->models[BASE] = LoadModel("resources/battle/base.glb");
        //env->client->ship = LoadModel("resources/puffer.glb");
        
        char vsPath[256];
        char fsPath[256];
        sprintf(vsPath, "resources/tower_climb/shaders/gls%i/lighting.vs", GLSL_VERSION);
        sprintf(fsPath, "resources/tower_climb/shaders/gls%i/lighting.fs", GLSL_VERSION);
        client->light_shader = LoadShader(vsPath, fsPath);
        client->light = CreateLight(LIGHT_DIRECTIONAL, 
            (Vector3){ 0.0f, 10.0f, 0.0f },    // High above for top lighting
            (Vector3){ 0.5f, -1.0f, 0.3f },    // Direction: down and slightly forward
            (Color){ 180, 180, 190, 255 },    // Softer warm white for tops
            client->light_shader);

        for (int i = 0; i < 8; i++) {
            Model* m = &client->models[i];
            for (int j = 0; j < m->materialCount; j++) {
                //m->materials[j].maps[MATERIAL_MAP_DIFFUSE].texture = client->vehicle_texture;
                m->materials[j].shader = client->light_shader;
            }
        }
 
        Camera3D camera = { 0 };
        camera.up = (Vector3){ 0.0f, 1.0f, 0.0f };          // Camera up vector (rotation towards target)
        camera.fovy = 45.0f;                                // Camera field-of-view Y
        camera.projection = CAMERA_PERSPECTIVE;             // Camera projection type
        camera.position = (Vector3){ 0, 5*env->size_y, -3*env->size_z};
        camera.target = (Vector3){ 0, 0, 0};
        client->camera = camera;

        client->mesh = create_heightmap_mesh(env->terrain, (Vector3){env->terrain_width, 1, env->terrain_height});
        client->model = LoadModelFromMesh(*client->mesh);
        update_heightmap_mesh(client->mesh, env->terrain, (Vector3){env->terrain_width, 1, env->terrain_height});

        client->terrain_shader = LoadShader(
            TextFormat("resources/battle/shader_%i.vs", GLSL_VERSION),
            TextFormat("resources/battle/shader_%i.fs", GLSL_VERSION)
        );

        Image img = GenImageColor(env->terrain_width, env->terrain_height, WHITE);
        ImageFormat(&img, PIXELFORMAT_UNCOMPRESSED_R8G8B8A8);
        client->terrain_texture = LoadTextureFromImage(img);
        UnloadImage(img);

        client->terrain_shader_loc = GetShaderLocation(client->terrain_shader, "terrain");
        SetShaderValueTexture(client->terrain_shader, client->terrain_shader_loc, client->terrain_texture);

        client->terrain_data = calloc(4*env->terrain_width*env->terrain_height, sizeof(unsigned char));
        for (int i = 0; i < env->terrain_width*env->terrain_height; i++) {
            client->terrain_data[4*i] = env->terrain[i];
            client->terrain_data[4*i+3] = 255;
        }
        UpdateTexture(client->terrain_texture, client->terrain_data);
        SetShaderValueTexture(client->terrain_shader, client->terrain_shader_loc, client->terrain_texture);

        int shader_width_loc = GetShaderLocation(client->terrain_shader, "width");
        SetShaderValue(client->terrain_shader, shader_width_loc, &env->terrain_width, SHADER_UNIFORM_INT);

        int shader_height_loc = GetShaderLocation(client->terrain_shader, "height");
        SetShaderValue(client->terrain_shader, shader_height_loc, &env->terrain_height, SHADER_UNIFORM_INT);
 
    }

    // Standard across our envs so exiting is always the same
    if (IsKeyDown(KEY_ESCAPE)) {
        exit(0);
    }

    Client* client = env->client;
    UpdateCamera(&client->camera, CAMERA_THIRD_PERSON);
    //UpdateLightValues(client->light);
    BeginDrawing();
    ClearBackground((Color){6, 24, 24, 255});
    BeginMode3D(client->camera);

        //BeginShaderMode(client->terrain_shader);
        client->model.materials[0].shader = client->terrain_shader;
        Vector3 pos = {-env->size_x, -env->size_y, -env->size_z};
        DrawModel(client->model, pos, 1.0/128.0f, (Color){156, 50, 20, 255});
        //EndShaderMode();


        for (int f=0; f<env->num_armies; f++) {
            Entity* base = &env->bases[f];
            float y = ground_height(env, base->x, base->z);
            DrawModel(client->models[BASE], (Vector3){base->x, y, base->z}, 0.05f, COLORS[base->army]);
        }

        for (int i=0; i<env->num_agents; i++) {
            Entity* agent = &env->agents[i];

            Vector3 pos = {agent->x, agent->y, agent->z};
            Matrix transform = QuaternionToMatrix(agent->orientation);
            Model model = client->models[agent->unit];
            model.transform = transform;

            Vector3 scale = (Vector3){0.01f, 0.01f, 0.01f};
            if (agent->unit == DRONE) {
                scale = (Vector3){0.01f, 0.01f, 0.01f};
            } else if (agent->unit == MOTHERSHIP) {
                scale = (Vector3){0.03f, 0.03f, 0.03f};
            } else if (agent->unit == FIGHTER) {
                scale = (Vector3){0.015f, 0.015f, 0.015f};
            } else if (agent->unit == BOMBER) {
                scale = (Vector3){0.015f, 0.015f, 0.015f};
            } else if (agent->unit == INFANTRY) {
                scale = (Vector3){0.005f, 0.005f, 0.005f};
            } else if (agent->unit == TANK) {
                scale = (Vector3){0.01f, 0.01f, 0.01f};
            } else if (agent->unit == ARTILLERY) {
                scale = (Vector3){0.02f, 0.02f, 0.02f};
            }

            Color color = COLORS[agent->army];
            Vector3 rot = {0.0f, 1.0f, 0.0f};
            DrawModelEx(model, pos, rot, 0, scale, color);

            if (agent->target >= 0) {
                Entity* target = &env->agents[agent->target];
                DrawLine3D(
                    (Vector3){agent->x, agent->y, agent->z},
                    (Vector3){target->x, target->y, target->z},
                    COLORS[agent->army]
                );
            }
        }

        DrawCubeWires(
            (Vector3){0, 0, 0},
            2*env->size_x, 2*env->size_y, 2*env->size_z,
            (Color){0, 255, 255, 128}
        );

    EndMode3D();
    EndDrawing();
}

// Required function. Should clean up anything you allocated
// Do not free env->observations, actions, rewards, terminals
void c_close(Battle* env) {
    free(env->agents);
    free(env->bases);
    if (env->client != NULL) {
        Client* client = env->client;
        //UnloadTexture(client->sprites);
        CloseWindow();
        free(client);
    }
}



================================================
FILE: pufferlib/ocean/battle/battle.py
================================================
'''A simple sample environment. Use this as a template for your own envs.'''

import gymnasium
import numpy as np

import pufferlib
from pufferlib.ocean.battle import binding

class Battle(pufferlib.PufferEnv):
    def __init__(self, num_envs=1, width=1920, height=1080, size_x=1.0,
            size_y=1.0, size_z=1.0, num_agents=1024, num_factories=32,
            num_armies=4, render_mode=None, log_interval=128, buf=None, seed=0):
        self.single_observation_space = gymnasium.spaces.Box(low=0, high=1,
            shape=(num_armies*3 + 4*16 + 22 + 8,), dtype=np.float32)
        self.single_action_space = gymnasium.spaces.Box(
                low=-1, high=1, shape=(3,), dtype=np.float32)
        self.render_mode = render_mode
        self.num_agents = num_envs*num_agents
        self.log_interval = log_interval

        if num_armies < 1 or num_armies > 8:
            raise pufferlib.APIUsageError('num_armies must be in [1, 8]')
        if num_agents % num_armies != 0:
            raise pufferlib.APIUsageError('num_agents must be a multiple of num_armies')

        super().__init__(buf)
        c_envs = []
        for i in range(num_envs):
            c_env = binding.env_init(
                self.observations[i*num_agents:(i+1)*num_agents],
                self.actions[i*num_agents:(i+1)*num_agents],
                self.rewards[i*num_agents:(i+1)*num_agents],
                self.terminals[i*num_agents:(i+1)*num_agents],
                self.truncations[i*num_agents:(i+1)*num_agents],
                seed, width=width, height=height, size_x=size_x, size_y=size_y, size_z=size_z,
                num_agents=num_agents*2, num_factories=num_factories,
                num_armies=num_armies)
            c_envs.append(c_env)

        self.c_envs = binding.vectorize(*c_envs)

    def reset(self, seed=0):
        binding.vec_reset(self.c_envs, seed)
        self.tick = 0
        return self.observations, []

    def step(self, actions):
        self.tick += 1
        self.actions[:] = actions
        binding.vec_step(self.c_envs)

        info = []
        if self.tick % self.log_interval == 0:
            log = binding.vec_log(self.c_envs)
            if log:
                info.append(log)

        return (self.observations, self.rewards,
            self.terminals, self.truncations, info)

    def render(self):
        binding.vec_render(self.c_envs, 0)

    def close(self):
        binding.vec_close(self.c_envs)

if __name__ == '__main__':
    N = 512

    env = Battle(num_envs=N)
    env.reset()
    steps = 0

    CACHE = 1024
    actions = np.random.randint(env.single_action_space.nvec, size=(CACHE, 2))

    i = 0
    import time
    start = time.time()
    while time.time() - start < 10:
        env.step(actions[i % CACHE])
        steps += env.num_agents
        i += 1

    print('Battle SPS:', int(steps / (time.time() - start)))



================================================
FILE: pufferlib/ocean/battle/binding.c
================================================
#include "battle.h"

#define Env Battle
#include "../env_binding.h"

static int my_init(Env* env, PyObject* args, PyObject* kwargs) {
    env->width = unpack(kwargs, "width");
    env->height = unpack(kwargs, "height");
    env->size_x = unpack(kwargs, "size_x");
    env->size_y = unpack(kwargs, "size_y");
    env->size_z = unpack(kwargs, "size_z");
    env->num_agents = unpack(kwargs, "num_agents");
    env->num_armies = unpack(kwargs, "num_armies");
    init(env);
    return 0;
}

static int my_log(PyObject* dict, Log* log) {
    assign_to_dict(dict, "perf", log->perf);
    assign_to_dict(dict, "score", log->score);
    assign_to_dict(dict, "collision_rate", log->collision_rate);
    assign_to_dict(dict, "oob_rate", log->oob_rate);
    assign_to_dict(dict, "episode_return", log->episode_return);
    assign_to_dict(dict, "episode_length", log->episode_length);
    return 0;
}



================================================
FILE: pufferlib/ocean/battle/rlights.h
================================================
/**********************************************************************************************
*
*   raylib.lights - Some useful functions to deal with lights data
*
*   CONFIGURATION:
*
*   #define RLIGHTS_IMPLEMENTATION
*       Generates the implementation of the library into the included file.
*       If not defined, the library is in header only mode and can be included in other headers 
*       or source files without problems. But only ONE file should hold the implementation.
*
*   LICENSE: zlib/libpng
*
*   Copyright (c) 2017-2024 Victor Fisac (@victorfisac) and Ramon Santamaria (@raysan5)
*
*   This software is provided "as-is", without any express or implied warranty. In no event
*   will the authors be held liable for any damages arising from the use of this software.
*
*   Permission is granted to anyone to use this software for any purpose, including commercial
*   applications, and to alter it and redistribute it freely, subject to the following restrictions:
*
*     1. The origin of this software must not be misrepresented; you must not claim that you
*     wrote the original software. If you use this software in a product, an acknowledgment
*     in the product documentation would be appreciated but is not required.
*
*     2. Altered source versions must be plainly marked as such, and must not be misrepresented
*     as being the original software.
*
*     3. This notice may not be removed or altered from any source distribution.
*
**********************************************************************************************/

#ifndef RLIGHTS_H
#define RLIGHTS_H

//----------------------------------------------------------------------------------
// Defines and Macros
//----------------------------------------------------------------------------------
#define MAX_LIGHTS  4         // Max dynamic lights supported by shader

//----------------------------------------------------------------------------------
// Types and Structures Definition
//----------------------------------------------------------------------------------

// Light data
typedef struct {   
    int type;
    bool enabled;
    Vector3 position;
    Vector3 target;
    Color color;
    float attenuation;
    
    // Shader locations
    int enabledLoc;
    int typeLoc;
    int positionLoc;
    int targetLoc;
    int colorLoc;
    int attenuationLoc;
} Light;

// Light type
typedef enum {
    LIGHT_DIRECTIONAL = 0,
    LIGHT_POINT
} LightType;

#ifdef __cplusplus
extern "C" {            // Prevents name mangling of functions
#endif

//----------------------------------------------------------------------------------
// Module Functions Declaration
//----------------------------------------------------------------------------------
Light CreateLight(int type, Vector3 position, Vector3 target, Color color, Shader shader);   // Create a light and get shader locations
void UpdateLightValues(Shader shader, Light light);         // Send light properties to shader

#ifdef __cplusplus
}
#endif

#endif // RLIGHTS_H


/***********************************************************************************
*
*   RLIGHTS IMPLEMENTATION
*
************************************************************************************/

#if defined(RLIGHTS_IMPLEMENTATION)

#include "raylib.h"

//----------------------------------------------------------------------------------
// Defines and Macros
//----------------------------------------------------------------------------------
// ...

//----------------------------------------------------------------------------------
// Types and Structures Definition
//----------------------------------------------------------------------------------
// ...

//----------------------------------------------------------------------------------
// Global Variables Definition
//----------------------------------------------------------------------------------
static int lightsCount = 0;    // Current amount of created lights

//----------------------------------------------------------------------------------
// Module specific Functions Declaration
//----------------------------------------------------------------------------------
// ...

//----------------------------------------------------------------------------------
// Module Functions Definition
//----------------------------------------------------------------------------------

// Create a light and get shader locations
Light CreateLight(int type, Vector3 position, Vector3 target, Color color, Shader shader)
{
    Light light = { 0 };

    if (lightsCount < MAX_LIGHTS)
    {
        light.enabled = true;
        light.type = type;
        light.position = position;
        light.target = target;
        light.color = color;

        // NOTE: Lighting shader naming must be the provided ones
        light.enabledLoc = GetShaderLocation(shader, TextFormat("lights[%i].enabled", lightsCount));
        light.typeLoc = GetShaderLocation(shader, TextFormat("lights[%i].type", lightsCount));
        light.positionLoc = GetShaderLocation(shader, TextFormat("lights[%i].position", lightsCount));
        light.targetLoc = GetShaderLocation(shader, TextFormat("lights[%i].target", lightsCount));
        light.colorLoc = GetShaderLocation(shader, TextFormat("lights[%i].color", lightsCount));

        UpdateLightValues(shader, light);
        
        lightsCount++;
    }

    return light;
}

// Send light properties to shader
// NOTE: Light shader locations should be available 
void UpdateLightValues(Shader shader, Light light)
{
    // Send to shader light enabled state and type
    SetShaderValue(shader, light.enabledLoc, &light.enabled, SHADER_UNIFORM_INT);
    SetShaderValue(shader, light.typeLoc, &light.type, SHADER_UNIFORM_INT);

    // Send to shader light position values
    float position[3] = { light.position.x, light.position.y, light.position.z };
    SetShaderValue(shader, light.positionLoc, position, SHADER_UNIFORM_VEC3);

    // Send to shader light target position values
    float target[3] = { light.target.x, light.target.y, light.target.z };
    SetShaderValue(shader, light.targetLoc, target, SHADER_UNIFORM_VEC3);

    // Send to shader light color values
    float color[4] = { (float)light.color.r/(float)255, (float)light.color.g/(float)255, 
                       (float)light.color.b/(float)255, (float)light.color.a/(float)255 };
    SetShaderValue(shader, light.colorLoc, color, SHADER_UNIFORM_VEC4);
}

#endif // RLIGHTS_IMPLEMENTATION



================================================
FILE: pufferlib/ocean/battle/simplex.h
================================================
// Original work (noise library) Copyright (c) 2008 Casey Duncan
// Modified work (vec_noise library) Copyright (c) 2017 Zev Benjamin
// Single-file C port (this file) Copyright (c) 2024 Joseph Suarez
// Distributed under the MIT license. This is a simple copy-paste job.
// I did this because the original code mixed Python bindings into the
// C source, so there wasn't a clean way to use it as a C standalone.

#include <math.h>
const float GRAD3[][3] = {
    {1,1,0},{-1,1,0},{1,-1,0},{-1,-1,0},
    {1,0,1},{-1,0,1},{1,0,-1},{-1,0,-1},
    {0,1,1},{0,-1,1},{0,1,-1},{0,-1,-1},
    {1,0,-1},{-1,0,-1},{0,-1,1},{0,1,1}};

const float GRAD4[][4] = {
    {0,1,1,1}, {0,1,1,-1}, {0,1,-1,1}, {0,1,-1,-1},
    {0,-1,1,1}, {0,-1,1,-1}, {0,-1,-1,1}, {0,-1,-1,-1},
    {1,0,1,1}, {1,0,1,-1}, {1,0,-1,1}, {1,0,-1,-1},
    {-1,0,1,1}, {-1,0,1,-1}, {-1,0,-1,1}, {-1,0,-1,-1},
    {1,1,0,1}, {1,1,0,-1}, {1,-1,0,1}, {1,-1,0,-1},
    {-1,1,0,1}, {-1,1,0,-1}, {-1,-1,0,1}, {-1,-1,0,-1},
    {1,1,1,0}, {1,1,-1,0}, {1,-1,1,0}, {1,-1,-1,0},
    {-1,1,1,0}, {-1,1,-1,0}, {-1,-1,1,0}, {-1,-1,-1,0}};

// At the possible cost of unaligned access, we use char instead of
// int here to try to ensure that this table fits in L1 cache
const unsigned char PERM[] = {
  151, 160, 137, 91, 90, 15, 131, 13, 201, 95, 96, 53, 194, 233, 7, 225, 140,
  36, 103, 30, 69, 142, 8, 99, 37, 240, 21, 10, 23, 190, 6, 148, 247, 120,
  234, 75, 0, 26, 197, 62, 94, 252, 219, 203, 117, 35, 11, 32, 57, 177, 33,
  88, 237, 149, 56, 87, 174, 20, 125, 136, 171, 168, 68, 175, 74, 165, 71,
  134, 139, 48, 27, 166, 77, 146, 158, 231, 83, 111, 229, 122, 60, 211, 133,
  230, 220, 105, 92, 41, 55, 46, 245, 40, 244, 102, 143, 54, 65, 25, 63, 161,
  1, 216, 80, 73, 209, 76, 132, 187, 208, 89, 18, 169, 200, 196, 135, 130,
  116, 188, 159, 86, 164, 100, 109, 198, 173, 186, 3, 64, 52, 217, 226, 250,
  124, 123, 5, 202, 38, 147, 118, 126, 255, 82, 85, 212, 207, 206, 59, 227,
  47, 16, 58, 17, 182, 189, 28, 42, 223, 183, 170, 213, 119, 248, 152, 2, 44,
  154, 163, 70, 221, 153, 101, 155, 167, 43, 172, 9, 129, 22, 39, 253, 19, 98,
  108, 110, 79, 113, 224, 232, 178, 185, 112, 104, 218, 246, 97, 228, 251, 34,
  242, 193, 238, 210, 144, 12, 191, 179, 162, 241, 81, 51, 145, 235, 249, 14,
  239, 107, 49, 192, 214, 31, 181, 199, 106, 157, 184, 84, 204, 176, 115, 121,
  50, 45, 127, 4, 150, 254, 138, 236, 205, 93, 222, 114, 67, 29, 24, 72, 243,
  141, 128, 195, 78, 66, 215, 61, 156, 180, 151, 160, 137, 91, 90, 15, 131,
  13, 201, 95, 96, 53, 194, 233, 7, 225, 140, 36, 103, 30, 69, 142, 8, 99, 37,
  240, 21, 10, 23, 190, 6, 148, 247, 120, 234, 75, 0, 26, 197, 62, 94, 252,
  219, 203, 117, 35, 11, 32, 57, 177, 33, 88, 237, 149, 56, 87, 174, 20, 125,
  136, 171, 168, 68, 175, 74, 165, 71, 134, 139, 48, 27, 166, 77, 146, 158,
  231, 83, 111, 229, 122, 60, 211, 133, 230, 220, 105, 92, 41, 55, 46, 245,
  40, 244, 102, 143, 54, 65, 25, 63, 161, 1, 216, 80, 73, 209, 76, 132, 187,
  208, 89, 18, 169, 200, 196, 135, 130, 116, 188, 159, 86, 164, 100, 109, 198,
  173, 186, 3, 64, 52, 217, 226, 250, 124, 123, 5, 202, 38, 147, 118, 126,
  255, 82, 85, 212, 207, 206, 59, 227, 47, 16, 58, 17, 182, 189, 28, 42, 223,
  183, 170, 213, 119, 248, 152, 2, 44, 154, 163, 70, 221, 153, 101, 155, 167,
  43, 172, 9, 129, 22, 39, 253, 19, 98, 108, 110, 79, 113, 224, 232, 178, 185,
  112, 104, 218, 246, 97, 228, 251, 34, 242, 193, 238, 210, 144, 12, 191, 179,
  162, 241, 81, 51, 145, 235, 249, 14, 239, 107, 49, 192, 214, 31, 181, 199,
  106, 157, 184, 84, 204, 176, 115, 121, 50, 45, 127, 4, 150, 254, 138, 236,
  205, 93, 222, 114, 67, 29, 24, 72, 243, 141, 128, 195, 78, 66, 215, 61, 156,
  180};

const unsigned char SIMPLEX[][4] = {
    {0,1,2,3},{0,1,3,2},{0,0,0,0},{0,2,3,1},{0,0,0,0},{0,0,0,0},{0,0,0,0},
    {1,2,3,0},{0,2,1,3},{0,0,0,0},{0,3,1,2},{0,3,2,1},{0,0,0,0},{0,0,0,0},
    {0,0,0,0},{1,3,2,0},{0,0,0,0},{0,0,0,0},{0,0,0,0},{0,0,0,0},{0,0,0,0},
    {0,0,0,0},{0,0,0,0},{0,0,0,0},{1,2,0,3},{0,0,0,0},{1,3,0,2},{0,0,0,0},
    {0,0,0,0},{0,0,0,0},{2,3,0,1},{2,3,1,0},{1,0,2,3},{1,0,3,2},{0,0,0,0},
    {0,0,0,0},{0,0,0,0},{2,0,3,1},{0,0,0,0},{2,1,3,0},{0,0,0,0},{0,0,0,0},
    {0,0,0,0},{0,0,0,0},{0,0,0,0},{0,0,0,0},{0,0,0,0},{0,0,0,0},{2,0,1,3},
    {0,0,0,0},{0,0,0,0},{0,0,0,0},{3,0,1,2},{3,0,2,1},{0,0,0,0},{3,1,2,0},
    {2,1,0,3},{0,0,0,0},{0,0,0,0},{0,0,0,0},{3,1,0,2},{0,0,0,0},{3,2,0,1},
    {3,2,1,0}};

#define fastfloor(n) (int)(n) - (((n) < 0.0f) & ((n) != (int)(n)))

// Fast sine/cosine functions from
// http://devmaster.net/forums/topic/4648-fast-and-accurate-sinecosine/page__st__80
// Note the input to these functions is not radians
// instead x = [0, 2] for r = [0, 2*PI]

inline float fast_sin(float x)
{
    // Convert the input value to a range of -1 to 1
    // x = x * (1.0f / PI);

    // Wrap around
    volatile float z = (x + 25165824.0f);
    x = x - (z - 25165824.0f);

    #if LOW_SINE_PRECISION
        return 4.0f * (x - x * fabsf(x));
    #else
    {
        float y = x - x * fabsf(x);
        const float Q = 3.1f;
        const float P = 3.6f;
        return y * (Q + P * fabsf(y));
    }
    #endif
}

inline float fast_cos(float x)
{
    return fast_sin(x + 0.5f);
}

// 2D simplex skew factors
#define F2 0.3660254037844386f  // 0.5 * (sqrt(3.0) - 1.0)
#define G2 0.21132486540518713f // (3.0 - sqrt(3.0)) / 6.0

float
noise2(float x, float y)
{
    int i1, j1, II, JJ, c;
    float s = (x + y) * F2;
    float i = floorf(x + s);
    float j = floorf(y + s);
    float t = (i + j) * G2;

    float xx[3], yy[3], f[3];
    float noise[3] = {0.0f, 0.0f, 0.0f};
    int g[3];

    xx[0] = x - (i - t);
    yy[0] = y - (j - t);

    i1 = xx[0] > yy[0];
    j1 = xx[0] <= yy[0];

    xx[2] = xx[0] + G2 * 2.0f - 1.0f;
    yy[2] = yy[0] + G2 * 2.0f - 1.0f;
    xx[1] = xx[0] - i1 + G2;
    yy[1] = yy[0] - j1 + G2;

    II = (int) i & 255;
    JJ = (int) j & 255;
    g[0] = PERM[II + PERM[JJ]] % 12;
    g[1] = PERM[II + i1 + PERM[JJ + j1]] % 12;
    g[2] = PERM[II + 1 + PERM[JJ + 1]] % 12;

    for (c = 0; c <= 2; c++)
        f[c] = 0.5f - xx[c]*xx[c] - yy[c]*yy[c];

    for (c = 0; c <= 2; c++)
        if (f[c] > 0)
            noise[c] = f[c]*f[c]*f[c]*f[c] * (GRAD3[g[c]][0]*xx[c] + GRAD3[g[c]][1]*yy[c]);

    return (noise[0] + noise[1] + noise[2]) * 70.0f;
}



================================================
FILE: pufferlib/ocean/blastar/binding.c
================================================
#include "blastar.h"
#define Env Blastar
#include "../env_binding.h"

static int my_init(Env* env, PyObject* args, PyObject* kwargs) {
    env->num_obs = unpack(kwargs, "num_obs");
    init(env, env->num_obs);
    return 0;
}

static int my_log(PyObject* dict, Log* log) {
    assign_to_dict(dict, "perf", log->perf);
    assign_to_dict(dict, "score", log->score);
    assign_to_dict(dict, "episode_return", log->episode_return);
    assign_to_dict(dict, "episode_length", log->episode_length);
    assign_to_dict(dict, "lives", log->lives);
    assign_to_dict(dict, "vertical_closeness_rew", log->vertical_closeness_rew);
    assign_to_dict(dict, "fired_bullet_rew", log->fired_bullet_rew);
    assign_to_dict(dict, "kill_streak", log->kill_streak);
    assign_to_dict(dict, "hit_enemy_with_bullet_rew", log->hit_enemy_with_bullet_rew);
    assign_to_dict(dict, "avg_score_difference", log->avg_score_difference);
    return 0;
}



================================================
FILE: pufferlib/ocean/blastar/blastar.c
================================================
#include "blastar.h"
#include <assert.h>
#include <stdio.h>
#include <stdlib.h>
#include <time.h>
#include "puffernet.h"

const char* WEIGHTS_PATH = "resources/blastar/blastar_weights.bin";
#define OBSERVATIONS_SIZE 10
#define ACTIONS_SIZE 6
#define NUM_WEIGHTS 134407

void get_input(Blastar* env) {
    if (IsKeyDown(KEY_LEFT) || IsKeyDown(KEY_A)) {
        env->actions[0] = 1;  // Left
    } else if (IsKeyDown(KEY_RIGHT) || IsKeyDown(KEY_D)) {
        env->actions[0] = 2;  // Right
    } else if (IsKeyDown(KEY_UP) || IsKeyDown(KEY_W)) {
        env->actions[0] = 3;  // Up
    } else if (IsKeyDown(KEY_DOWN) || IsKeyDown(KEY_S)) {
        env->actions[0] = 4;  // Down
    } else if (IsKeyDown(KEY_SPACE)) {
        env->actions[0] = 5;  // Fire
    } else {
        env->actions[0] = 0;  // Noop
    }
}

int demo() {
    Weights* weights = load_weights(WEIGHTS_PATH, NUM_WEIGHTS);
    int logit_sizes[1] = {ACTIONS_SIZE};
    LinearLSTM* net = make_linearlstm(weights, 1, OBSERVATIONS_SIZE, logit_sizes, 1);
    Blastar env = {
        .num_obs = OBSERVATIONS_SIZE,
    };
    allocate(&env, env.num_obs);
    Client* client = make_client(&env);
    unsigned int seed = 12345;
    srand(seed);
    c_reset(&env);
    int running = 1;
    while (running) {
        if (IsKeyDown(KEY_LEFT_SHIFT)) {
            get_input(&env);  // Human input
        } else {
            forward_linearlstm(net, env.observations, env.actions);  // AI input
        }
        c_step(&env);
        c_render(&env);
        if (WindowShouldClose() || env.game_over) {
            running = 0;
        }
    }
    free_linearlstm(net);
    free(weights);
    close_client(client);
    free_allocated(&env);
    return 0;
}

void perftest(float test_time) {
    Blastar env = {
        .num_obs = OBSERVATIONS_SIZE,
    };
    allocate(&env, env.num_obs);
    unsigned int seed = 12345;
    srand(seed);
    c_reset(&env);
    int start = time(NULL);
    int steps = 0;
    while (time(NULL) - start < test_time) {
        env.actions[0] = rand() % ACTIONS_SIZE;  // Random actions
        c_step(&env);
        steps++;
    }
    int end = time(NULL);
    printf("Steps per second: %f\n", steps / (float)(end - start));
    free_allocated(&env);
}

int main() {
    demo();
    // perftest(10.0f);
    return 0;
}



================================================
FILE: pufferlib/ocean/blastar/blastar.h
================================================
#include <math.h>
#include <stdbool.h>
#include <stdio.h>
#include <stdlib.h>
#include <string.h>

#include "raylib.h"

#define SCREEN_WIDTH 640
#define SCREEN_HEIGHT 480
#define PLAYER_MAX_LIVES 10
#define ENEMY_SPAWN_Y 50
#define ENEMY_SPAWN_X -30
#define INIT_BULLET_SPEED 3.0f
#define MAX_SCORE (5 * PLAYER_MAX_LIVES)
#define BULLET_SPEED (INIT_BULLET_SPEED * SPEED_SCALE)

static const float SPEED_SCALE = 4.0f;
static const int ENEMY_WIDTH = 16;
static const int ENEMY_HEIGHT = 17;
static const int PLAYER_WIDTH = 17;
static const int PLAYER_HEIGHT = 17;
static const int PLAYER_BULLET_WIDTH = 17;
static const int PLAYER_BULLET_HEIGHT = 6;

typedef struct Log {
    float perf;
    float score;
    float episode_return;
    float episode_length;
    float lives;
    float vertical_closeness_rew;
    float fired_bullet_rew;
    float kill_streak;
    float hit_enemy_with_bullet_rew;
    float avg_score_difference;
    float n;
} Log;

typedef struct Bullet {
    float x;
    float y;
    bool active;
} Bullet;

typedef struct Enemy {
    float x;
    float y;
    float enemy_speed;
    bool active;
    bool attacking;
    int crossed_screen;
    Bullet bullet;
} Enemy;

typedef struct Player {
    float x;
    float y;
    float player_speed;
    int score;
    int lives;
    Bullet bullet;
    bool bullet_fired;
    bool player_stuck;
} Player;

typedef struct Client {
    Texture2D player_texture;
    Texture2D enemy_texture;
    Texture2D player_bullet_texture;
    Texture2D enemy_bullet_texture;
    Texture2D explosion_texture;
} Client;

typedef struct Blastar {
    Client* client;
    int reset_count;
    int num_obs;
    bool game_over;
    int tick;
    int player_explosion_timer;
    int enemy_explosion_timer;
    int kill_streak;
    int enemy_respawns;
    Player player;
    Enemy enemy;
    float* observations;
    int* actions;
    float* rewards;
    unsigned char* terminals;
    Log log;
} Blastar;

void add_log(Blastar* env) {
    env->log.episode_length += env->tick;
    env->log.lives = env->player.lives;
    env->log.score = env->player.score;
    env->log.perf = env->player.score / MAX_SCORE;
    env->log.kill_streak = env->kill_streak;
    env->log.n += 1;
}

static inline void scale_speeds(Blastar* env) {
    env->player.player_speed *= SPEED_SCALE;
    env->enemy.enemy_speed *= SPEED_SCALE;
}

void c_reset(Blastar* env) {
    env->game_over = false;
    env->tick = 0;
    env->player_explosion_timer = 0;
    env->enemy_explosion_timer = 0;
    env->player.player_speed = 2.0f;
    env->enemy.enemy_speed = 1.0f;
    scale_speeds(env);
    env->player.x = (float)(rand() % (SCREEN_WIDTH - PLAYER_WIDTH));
    env->player.y = (float)(rand() % (SCREEN_HEIGHT - PLAYER_HEIGHT));
    env->player.score = 0;
    env->player.lives = PLAYER_MAX_LIVES;
    env->player.bullet_fired = false;
    env->player.player_stuck = false;
    env->player.bullet.active = false;
    env->player.bullet.x = env->player.x;
    env->player.bullet.y = env->player.y;
    env->kill_streak = 0;
    env->enemy.x = ENEMY_SPAWN_X;
    env->enemy.y = ENEMY_SPAWN_Y;
    env->enemy.active = true;
    env->enemy.attacking = false;
    if (env->reset_count < 1) {
        env->enemy_respawns = 0;
        env->enemy.crossed_screen = 0;
    }
    env->enemy.bullet.active = false;
    env->enemy.bullet.x = env->enemy.x;
    env->enemy.bullet.y = env->enemy.y;
    env->reset_count++;

    env->log = (Log){0};
}

void c_close(Blastar* env) {
}

void init(Blastar* env, int num_obs) {
    env->reset_count = 0;
    env->num_obs = num_obs;
    c_reset(env);
}

void allocate(Blastar* env, int num_obs) {
    init(env, num_obs);
    env->observations = (float*)calloc(env->num_obs, sizeof(float));
    env->actions = (int*)calloc(1, sizeof(int));
    env->rewards = (float*)calloc(1, sizeof(float));
    env->terminals = (unsigned char*)calloc(1, sizeof(unsigned char));
}

void free_allocated(Blastar* env) {
    free(env->observations);
    free(env->actions);
    free(env->rewards);
    free(env->terminals);
}

static inline void calculate_center(float x, float y, int width, int height, float* center_x, float* center_y) {
    *center_x = x + width / 2.0f;
    *center_y = y + height / 2.0f;
}

void compute_observations(Blastar* env) {
    env->log.lives = env->player.lives;
    env->log.score = env->player.score;

    memset(env->observations, 0, env->num_obs * sizeof(float));
    env->observations[0] = env->player.x / SCREEN_WIDTH;
    env->observations[1] = env->player.y / SCREEN_HEIGHT;
    env->observations[2] = env->enemy.x / SCREEN_WIDTH;
    env->observations[3] = env->enemy.y / SCREEN_HEIGHT;
    if (env->player.bullet.active) {
        env->observations[4] = env->player.bullet.x / SCREEN_WIDTH;
        env->observations[5] = env->player.bullet.y / SCREEN_HEIGHT;
        env->observations[6] = 1.0f;
    }
    if (env->enemy.bullet.active) {
        env->observations[7] = env->enemy.bullet.x / SCREEN_WIDTH;
        env->observations[8] = env->enemy.bullet.y / SCREEN_HEIGHT;
        env->observations[9] = 1.0f;
    }
}

bool check_collision(float x1, float y1, float w1, float h1, float x2, float y2, float w2, float h2) {
    if (x1 < x2 + w2 && x1 + w1 > x2 && y1 < y2 + h2 && y1 + h1 > y2) {
        return true;
    }
    return false;
}

void c_step(Blastar* env) {
    if (env->game_over) {
        if (env->terminals) env->terminals[0] = 1;
        add_log(env);
        c_reset(env);
        return;
    }

    env->tick++;
    env->log.episode_length += 1;
    float rew = 0.0f;
    env->rewards[0] = rew;
    float fired_bullet_rew = 0.0f;
    float vertical_closeness_rew = 0.0f;
    float hit_enemy_with_bullet_rew = 0.0f;
    int crossed_screen = 0;
    int action = env->actions[0];

    if (env->player_explosion_timer > 0) {
        env->player_explosion_timer--;
        env->kill_streak = 0;
        if (env->player_explosion_timer == 0) {
            env->player.player_stuck = false;
            env->player.bullet.active = false;
        }
        compute_observations(env);
        add_log(env);
        return;
    }

    if (env->enemy_explosion_timer > 0) {
        env->enemy_explosion_timer--;
        if (env->enemy_explosion_timer == 0) {
            env->enemy.crossed_screen = 0;
            float respawn_bias = 0.1f;
            if ((float)rand() / (float)RAND_MAX > respawn_bias) {
                env->enemy.x = -ENEMY_WIDTH;
                env->enemy.y = rand() % (SCREEN_HEIGHT - ENEMY_HEIGHT);
                env->enemy_respawns += 1;
            }
            env->enemy.active = true;
            env->enemy.attacking = false;
        }
        compute_observations(env);
        add_log(env);
        return;
    }

    if (env->enemy.y > (SCREEN_HEIGHT - (ENEMY_HEIGHT * 3.5f))) {
        env->enemy.y = (SCREEN_HEIGHT - (ENEMY_HEIGHT * 3.5f));
    }

    if (!env->player.player_stuck) {
        if (action == 1 && env->player.x > 0) env->player.x -= env->player.player_speed;
        if (action == 2 && env->player.x < SCREEN_WIDTH - PLAYER_WIDTH) env->player.x += env->player.player_speed;
        if (action == 3 && env->player.y > 0) env->player.y -= env->player.player_speed;
        if (action == 4 && env->player.y < SCREEN_HEIGHT - PLAYER_HEIGHT) env->player.y += env->player.player_speed;
    }

    if (action == 5 && (!env->enemy.bullet.active)) {
        if (env->player.bullet.active) {
            env->player.bullet.active = false;
        } else {
            fired_bullet_rew += 0.0005f;
        }
        env->player.bullet.active = true;
        env->player.bullet.x = env->player.x + PLAYER_WIDTH / 2 - PLAYER_BULLET_WIDTH / 2;
        env->player.bullet.y = env->player.y;
    }

    if (env->player.bullet.active) {
        env->player.bullet.y -= BULLET_SPEED;
        if (env->player.bullet.y < 0) {
            env->player.bullet.active = false;
        }
    }

    float player_center_x;
    float enemy_center_x;
    float dummy;
    calculate_center(env->player.x, env->player.y, PLAYER_WIDTH, PLAYER_HEIGHT, &player_center_x, &dummy);
    calculate_center(env->enemy.x, env->enemy.y, ENEMY_WIDTH, ENEMY_HEIGHT, &enemy_center_x, &dummy);

    if (!env->enemy.attacking) {
        env->enemy.x += env->enemy.enemy_speed;
        if (env->enemy.x > SCREEN_WIDTH) {
            env->enemy.x = -ENEMY_WIDTH;
            crossed_screen += 1;
        }
    }

    if (fabs(player_center_x - enemy_center_x) < SPEED_SCALE &&
        !env->enemy.attacking && env->enemy.active &&
        env->enemy.y < env->player.y - (ENEMY_HEIGHT / 2)) {
        if (rand() % 2 == 0) {
            env->enemy.attacking = true;
            if (!env->enemy.bullet.active) {
                env->enemy.bullet.active = true;
                calculate_center(env->enemy.x, env->enemy.y, ENEMY_WIDTH, ENEMY_HEIGHT, &enemy_center_x, &dummy);
                env->enemy.bullet.x = enemy_center_x - 5.0f;
                env->enemy.bullet.y = env->enemy.y + ENEMY_HEIGHT;
                env->player.bullet.active = false;
                env->player.player_stuck = true;
            }
        } else {
            env->enemy.attacking = false;
            env->enemy.x += env->enemy.enemy_speed;
        }
    }

    if (env->enemy.bullet.active) {
        env->enemy.bullet.y += BULLET_SPEED;
        if (env->enemy.bullet.y > SCREEN_HEIGHT) {
            env->enemy.bullet.active = false;
            env->player.player_stuck = false;
            env->enemy.attacking = false;
        }
    }

    if (check_collision(env->player.x, env->player.y, PLAYER_WIDTH, PLAYER_HEIGHT, 
                        env->enemy.x, env->enemy.y, ENEMY_WIDTH, ENEMY_HEIGHT)) {
        env->player.lives--;
        env->enemy.active = false;
        env->enemy_explosion_timer = 30;
        env->enemy.x = -ENEMY_WIDTH;
        env->enemy.y = rand() % (SCREEN_HEIGHT - ENEMY_HEIGHT);
        env->player_explosion_timer = 30;
        env->player.player_stuck = false;

        if (env->player.lives <= 0) {
            env->player.lives = 0;
            env->game_over = true;
            if (env->terminals) env->terminals[0] = 1;
            add_log(env);
            compute_observations(env);
            c_reset(env);
        }
        compute_observations(env);
        add_log(env);
        return;
    }

    if (env->player.bullet.active && env->player.y > env->enemy.y + ENEMY_HEIGHT &&
        check_collision(env->player.bullet.x, env->player.bullet.y, PLAYER_BULLET_WIDTH, PLAYER_BULLET_HEIGHT,
                        env->enemy.x, env->enemy.y, ENEMY_WIDTH, ENEMY_HEIGHT) &&
        env->enemy.active) {
        env->player.bullet.active = false;
        env->enemy.active = false;
        env->kill_streak += 1;
        fired_bullet_rew += 1.5f;
        env->player.score += 1;
        env->log.score += 1.0f;
        env->enemy_explosion_timer = 30;
        float enemy_x_normalized = 1.0f - (env->enemy.x / SCREEN_WIDTH);
        hit_enemy_with_bullet_rew += (crossed_screen == 0) ? (4.5f * enemy_x_normalized)
                                                          : (3.5f * enemy_x_normalized);
    }

    if (env->enemy.bullet.active &&
        check_collision(env->enemy.bullet.x, env->enemy.bullet.y, 10, 12, 
                        env->player.x, env->player.y, PLAYER_WIDTH, PLAYER_HEIGHT)) {
        env->enemy.bullet.active = false;
        env->player.lives--;
        env->player_explosion_timer = 30;
        env->player.player_stuck = false;
        env->enemy.attacking = false;
        env->enemy.x = -ENEMY_WIDTH;
        env->enemy.y = rand() % (SCREEN_HEIGHT - ENEMY_HEIGHT);

        if (env->player.lives <= 0) {
            env->player.lives = 0;
            env->game_over = true;
            if (env->terminals) {
                env->terminals[0] = 1;
            }
            compute_observations(env);
            add_log(env);
            c_reset(env);
        }
    }

    if (!(env->player.y > env->enemy.y + ENEMY_HEIGHT)) {
        vertical_closeness_rew = 0.0f;
        fired_bullet_rew = 0.0f;
        hit_enemy_with_bullet_rew = 0.0f;
    } else {
        float v_delta_distance = env->player.y - env->enemy.y;
        v_delta_distance = 2.0f - (v_delta_distance / SCREEN_HEIGHT);
        vertical_closeness_rew = 0.01f * v_delta_distance;
    }

    float avg_score_difference = 0.0f;
    if (env->player.score > 0) {
        avg_score_difference = (float)env->player.score / (env->tick + 1);
    }

    env->log.avg_score_difference = avg_score_difference;
    env->log.fired_bullet_rew = fired_bullet_rew;
    env->log.kill_streak = env->kill_streak;
    env->log.hit_enemy_with_bullet_rew = hit_enemy_with_bullet_rew;
    env->log.vertical_closeness_rew = vertical_closeness_rew;
    env->enemy.crossed_screen = crossed_screen;

    rew += fired_bullet_rew + vertical_closeness_rew + hit_enemy_with_bullet_rew + avg_score_difference;
    rew *= (1.0f + env->kill_streak * 0.1f);

    if (!(env->player.y > env->enemy.y + ENEMY_HEIGHT && fabs(player_center_x - enemy_center_x) > ENEMY_WIDTH * 0.3f)) {
        rew = fminf(rew, 0.0f);
    }

    if (env->player.x > SCREEN_WIDTH / 2.0f) {
        env->log.episode_return = 0;
        rew = 0.0f;
    }

    env->rewards[0] = rew;
    env->log.episode_return += rew;

    if (env->player.score > MAX_SCORE) {
        env->game_over = true;
        env->terminals[0] = 1;
        compute_observations(env);
        add_log(env);
        c_reset(env);
    }

    compute_observations(env);
}

const Color PUFF_RED = (Color){187, 0, 0, 255};
const Color PUFF_CYAN = (Color){0, 187, 187, 255}; 
const Color PUFF_WHITE = (Color){241, 241, 241, 241};
const Color PUFF_BACKGROUND = (Color){6, 24, 24, 255};

Client* make_client(Blastar* env) {
    InitWindow(SCREEN_WIDTH, SCREEN_HEIGHT, "Blastar");
    Client* client = (Client*)malloc(sizeof(Client));
    SetTargetFPS(60);
    client->player_texture = LoadTexture("resources/blastar/player_ship.png");
    client->enemy_texture = LoadTexture("resources/blastar/enemy_ship.png");
    client->player_bullet_texture = LoadTexture("resources/blastar/player_bullet.png");
    client->enemy_bullet_texture = LoadTexture("resources/blastar/enemy_bullet.png");
    client->explosion_texture = LoadTexture("resources/blastar/player_death_explosion.png");
    env->client = client;
    return client;
}

void close_client(Client* client) {
    CloseWindow();
    free(client);
}

void c_render(Blastar* env) {
    if (env->client == NULL) {
        make_client(env);
    }

    if (IsKeyDown(KEY_ESCAPE)) {
        exit(0);
    }

    Client* client = env->client;

    if (WindowShouldClose()) {
        env->game_over = true;
        close_client(client);
        env->client = NULL;
        exit(0);
    }

    BeginDrawing();
    ClearBackground(PUFF_BACKGROUND);

    if (env->game_over && env->player.lives <=0) {
        DrawText("GAME OVER", SCREEN_WIDTH / 2 - MeasureText("GAME OVER", 30) / 2, SCREEN_HEIGHT / 2 - 15, 30, PUFF_RED);
        DrawText(TextFormat("FINAL SCORE: %d", env->player.score), SCREEN_WIDTH / 2 - MeasureText(TextFormat("FINAL SCORE: %d", env->player.score), 20)/2, SCREEN_HEIGHT / 2 + 25, 20, PUFF_CYAN);
    } else {
        if (env->player_explosion_timer > 0) {
            DrawTexture(client->explosion_texture, env->player.x, env->player.y, WHITE);
        } else if (env->player.lives > 0) {
            DrawTexture(client->player_texture, env->player.x, env->player.y, WHITE);
        }
        if (env->enemy_explosion_timer > 0) {
            DrawTexture(client->explosion_texture, env->enemy.x, env->enemy.y, WHITE);
        } else if (env->enemy.active) {
            DrawTexture(client->enemy_texture, env->enemy.x, env->enemy.y, WHITE);
        }
        if (env->player.bullet.active) {
            DrawTexture(client->player_bullet_texture, env->player.bullet.x, env->player.bullet.y, WHITE);
        }
        if (env->enemy.bullet.active) {
            DrawTexture(client->enemy_bullet_texture, env->enemy.bullet.x, env->enemy.bullet.y, WHITE);
        }
        if (env->player.player_stuck) {
            DrawText("Status Beam", SCREEN_WIDTH - MeasureText("Status Beam", 20) - 10, SCREEN_HEIGHT / 3, 20, PUFF_RED);
        }
        DrawText(TextFormat("SCORE: %d", env->player.score), 10, 10, 20, PUFF_CYAN);
        DrawText(TextFormat("LIVES: %d", env->player.lives), SCREEN_WIDTH - MeasureText(TextFormat("LIVES: %d", env->player.lives), 20) - 10, 10, 20, PUFF_CYAN);
    }
    EndDrawing();
}



================================================
FILE: pufferlib/ocean/blastar/blastar.py
================================================
import numpy as np
import gymnasium
import pufferlib
from pufferlib.ocean.blastar import binding

class Blastar(pufferlib.PufferEnv):
    def __init__(self, num_envs=1, render_mode=None, buf=None, seed=0):
        self.single_observation_space = gymnasium.spaces.Box(
            low=0, high=1, shape=(10,), dtype=np.float32
        )
        self.single_action_space = gymnasium.spaces.Discrete(6)
        self.render_mode = render_mode
        self.num_agents = num_envs
        self.num_obs = self.single_observation_space.shape[0]
        self.tick = 0
        self.log_interval = 1
        
        super().__init__(buf)
        self.c_envs = binding.vec_init(
            self.observations,
            self.actions,
            self.rewards,
            self.terminals,
            self.truncations,
            num_envs,
            seed,
            num_obs=self.num_obs
        )

    def reset(self, seed=None):
        self.tick = 0
        binding.vec_reset(self.c_envs, seed)
        return self.observations, []

    def step(self, actions):
        self.actions[:] = actions
        self.tick += 1
        binding.vec_step(self.c_envs)

        info = []
        if self.tick % self.log_interval == 0:
            info.append(binding.vec_log(self.c_envs))
            
        return (self.observations, self.rewards,
                self.terminals, self.truncations, info)

    def render(self):
        binding.vec_render(self.c_envs, 0)

    def close(self):
        binding.vec_close(self.c_envs)

def test_performance(timeout=10, atn_cache=1024):
    env = Blastar(num_envs=1000)
    env.reset(0)
    tick = 0

    rng = np.random.default_rng()
    actions = rng.integers(0, 6, (atn_cache, env.num_agents))
 
    import time
    start = time.time()
    while time.time() - start < timeout:
        atn = actions[tick % atn_cache]
        env.step(atn)
        tick += 1

    print('SPS:', env.num_agents * tick / (time.time() - start))

if __name__ == '__main__':
    test_performance()



================================================
FILE: pufferlib/ocean/boids/binding.c
================================================
#include "boids.h"

#define Env Boids
#include "../env_binding.h"

static int my_init(Env* env, PyObject* args, PyObject* kwargs) {
    env->num_boids = unpack(kwargs, "num_boids");
    env->report_interval = unpack(kwargs, "report_interval");
    env->margin_turn_factor = unpack(kwargs, "margin_turn_factor");
    env->centering_factor = unpack(kwargs, "centering_factor");
    env->avoid_factor = unpack(kwargs, "avoid_factor");
    env->matching_factor = unpack(kwargs, "matching_factor");
    init(env);
    return 0;
}

static int my_log(PyObject* dict, Log* log) {
    assign_to_dict(dict, "perf", log->perf);
    assign_to_dict(dict, "score", log->score);
    assign_to_dict(dict, "episode_return", log->episode_return);
    assign_to_dict(dict, "episode_length", log->episode_length);
    assign_to_dict(dict, "n", log->n);
    return 0;
}



================================================
FILE: pufferlib/ocean/boids/boids.c
================================================
// Standalone C demo for Boids environment
// Compile using: ./scripts/build_ocean.sh boids [local|fast]
// Run with: ./boids

#include <time.h>
#include "boids.h"

// --- Demo Configuration ---
#define NUM_BOIDS_DEMO 20   // Number of boids for the standalone demo
#define MAX_STEPS_DEMO 500 // Max steps per episode in the demo
#define ACTION_SCALE 3.0f   // Corresponds to action space [-3.0, 3.0]

// Dummy action generation: random velocity changes for each boid
void generate_dummy_actions(Boids* env) {
    for (unsigned int i = 0; i < env->num_boids; ++i) {
        // Generate random floats in [-1, 1] range
        float rand_vx = ((float)rand() / (float)RAND_MAX) * 2.0f - 1.0f;
        float rand_vy = ((float)rand() / (float)RAND_MAX) * 2.0f - 1.0f;
        
        // Scale to the action space [-ACTION_SCALE, ACTION_SCALE]
        env->actions[i * 2 + 0] = rand_vx * ACTION_SCALE;
        env->actions[i * 2 + 1] = rand_vy * ACTION_SCALE;
    }
}

void demo() {
    // Initialize Boids environment struct
    Boids env = {0}; 
    env.num_boids = NUM_BOIDS_DEMO;
    
    // In the Python binding, these pointers are assigned from NumPy arrays.
    // Here, we need to allocate them explicitly.
    size_t obs_size = env.num_boids * 4; // num_boids * (x, y, vx, vy)
    size_t act_size = env.num_boids * 2; // num_boids * (dvx, dvy)
    env.observations = (float*)calloc(obs_size, sizeof(float));
    env.actions = (float*)calloc(act_size, sizeof(float));
    env.rewards = (float*)calloc(env.num_boids, sizeof(float)); // Env-level reward
    
    if (!env.observations || !env.actions || !env.rewards) {
        fprintf(stderr, "ERROR: Failed to allocate memory for demo buffers.\n");
        free(env.observations); free(env.actions); free(env.rewards);
        return;
    }

    init(&env); 
    Client* client = make_client(&env);

    if (client == NULL) {
        fprintf(stderr, "ERROR: Failed to create rendering client during initial setup.\n");
        c_close(&env);
        free(env.observations); free(env.actions); free(env.rewards);
        return;
    }
    env.client = client;
    
    // Initial reset
    c_reset(&env);
    int total_steps = 0;

    printf("Starting Boids demo with %d boids. Press ESC to exit.\n", env.num_boids);

    while (!WindowShouldClose() && total_steps < MAX_STEPS_DEMO) { // Raylib function to check if ESC is pressed or window closed
        generate_dummy_actions(&env);
        c_step(&env);
        c_render(&env);
        total_steps++;
    }

    c_close(&env);
    free(env.observations);
    free(env.actions);
    free(env.rewards);
    // ----------------------------------------
}

int main() {
    srand(time(NULL)); // Seed random number generator
    demo();
    return 0;
}



================================================
FILE: pufferlib/ocean/boids/boids.h
================================================
#include <stdlib.h>
#include <stdbool.h>
#include <stdio.h>
#include <math.h>
#include <string.h>
#include <limits.h>
#include <stdbool.h>

#include "raylib.h"

#define TOP_MARGIN 50
#define BOTTOM_MARGIN 50
#define LEFT_MARGIN 50
#define RIGHT_MARGIN 50
#define VELOCITY_CAP 5
#define VISUAL_RANGE 20
#define PROTECTED_RANGE 100
#define WIDTH 1080
#define HEIGHT 720
#define BOID_WIDTH 32
#define BOID_HEIGHT 32
#define BOID_TEXTURE_PATH "./resources/puffers_128.png"

typedef struct {
    float perf;
    float score;
    float episode_return;
    float episode_length;
    float n;
} Log;

typedef struct {
    float x;
    float y;
} Velocity;

typedef struct {
    float x;
    float y;
    Velocity velocity;
} Boid;

typedef struct Client Client;
typedef struct {
    // an array of shape (num_boids, 4) with the 4 values correspoinding to (x, y, velocity x, velocity y)
    float* observations;
    // an array of shape (num_boids, 2) with the 2 values correspoinding to (velocity x, velocity y)
    float* actions;
    // an array of shape (1) with the summed up reward for all boids
    float* rewards;
    unsigned char* terminals; // Not being used but is required by env_binding.h
    Boid* boids;
    unsigned int num_boids;
    float margin_turn_factor;
    float centering_factor;
    float avoid_factor;
    float matching_factor;
    unsigned tick;
    Log log;
    Log* boid_logs;
    unsigned report_interval;
    Client* client;

} Boids;

static inline float flmax(float a, float b) { return a > b ? a : b; }
static inline float flmin(float a, float b) { return a > b ? b : a; }
static inline float flclip(float x,float lo,float hi) { return flmin(hi,flmax(lo,x)); }
static inline float rndf(float lo,float hi) { return lo + (float)rand()/(float)RAND_MAX*(hi-lo); }

static void respawn_boid(Boids *env, unsigned int i) {
    env->boids[i].x = rndf(LEFT_MARGIN, WIDTH  - RIGHT_MARGIN);
    env->boids[i].y = rndf(BOTTOM_MARGIN, HEIGHT - TOP_MARGIN);
    env->boids[i].velocity.x = 0;
    env->boids[i].velocity.y = 0;
    env->boid_logs[i]       = (Log){0};
}

void init(Boids *env) {
    env->boids = (Boid*)calloc(env->num_boids, sizeof(Boid));
    env->boid_logs = (Log*)calloc(env->num_boids, sizeof(Log));
    env->log = (Log){0};
    env->tick = 0;

    for (unsigned current_indx = 0; current_indx < env->num_boids; current_indx++) {
        env->boids[current_indx].x = rndf(LEFT_MARGIN, WIDTH  - RIGHT_MARGIN);
        env->boids[current_indx].y = rndf(BOTTOM_MARGIN, HEIGHT - TOP_MARGIN);
        env->boids[current_indx].velocity.x = 0;
        env->boids[current_indx].velocity.y = 0;
    }
}


static void compute_observations(Boids *env) {
    unsigned base_indx;

    int idx = 0;
    for (unsigned i=0; i<env->num_boids; i++) {
        for (unsigned j=0; j<env->num_boids; j++) {
            env->observations[idx++] = (env->boids[j].x - env->boids[i].x) / WIDTH;
            env->observations[idx++] = (env->boids[j].y - env->boids[i].y) / HEIGHT;
            env->observations[idx++] = (env->boids[j].velocity.x - env->boids[i].velocity.x) / VELOCITY_CAP;
            env->observations[idx++] = (env->boids[j].velocity.y - env->boids[i].velocity.y) / VELOCITY_CAP;
        }
    }
}

void c_reset(Boids *env) {
    env->log = (Log){0};
    env->tick = 0;
    for (unsigned boid_indx = 0; boid_indx < env->num_boids; boid_indx++) {
        respawn_boid(env, boid_indx);
    }
    compute_observations(env);
}

void c_step(Boids *env) {
    Boid* current_boid;
    Boid observed_boid;
    float vis_vx_sum, vis_vy_sum, vis_x_sum, vis_y_sum, vis_x_avg, vis_y_avg, vis_vx_avg, vis_vy_avg;
    float diff_x, diff_y, dist, protected_dist_sum, current_boid_reward;
    unsigned visual_count, protected_count;
    bool manual_control = IsKeyDown(KEY_LEFT_SHIFT);
    float mouse_x = (float)GetMouseX();
    float mouse_y = (float)GetMouseY();

    env->tick++;
    env->rewards[0] = 0;
    env->log.score = 0;
    for (unsigned current_indx = 0; current_indx < env->num_boids; current_indx++) {
        // apply action
        current_boid = &env->boids[current_indx];
        if (manual_control) {
            current_boid->velocity.x = flclip(current_boid->velocity.x + (mouse_x - current_boid->x), -VELOCITY_CAP, VELOCITY_CAP);
            current_boid->velocity.y = flclip(current_boid->velocity.y + (mouse_y - current_boid->y), -VELOCITY_CAP, VELOCITY_CAP);
        } else {
            current_boid->velocity.x = flclip(current_boid->velocity.x + 2*env->actions[current_indx * 2 + 0], -VELOCITY_CAP, VELOCITY_CAP);
            current_boid->velocity.y = flclip(current_boid->velocity.y + 2*env->actions[current_indx * 2 + 1], -VELOCITY_CAP, VELOCITY_CAP);
        }
        current_boid->x = flclip(current_boid->x + current_boid->velocity.x, 0, WIDTH  - BOID_WIDTH);
        current_boid->y = flclip(current_boid->y + current_boid->velocity.y, 0, HEIGHT - BOID_HEIGHT);

        // reward calculation
        current_boid_reward = 0.0f, protected_dist_sum = 0.0f, protected_count = 0.0f;
        visual_count = 0.0f, vis_vx_sum = 0.0f, vis_vy_sum = 0.0f, vis_x_sum = 0.0f, vis_y_sum = 0.0f;
        for (unsigned observed_indx = 0; observed_indx < env->num_boids; observed_indx++) {
            if (current_indx == observed_indx) continue;
            observed_boid = env->boids[observed_indx];
            diff_x = current_boid->x - observed_boid.x;
            diff_y = current_boid->y - observed_boid.y;
            dist = sqrtf(diff_x*diff_x + diff_y*diff_y);
            if (dist < PROTECTED_RANGE) {
                protected_dist_sum += (PROTECTED_RANGE - dist);
                protected_count++;
            } else if (dist < VISUAL_RANGE) {
                vis_x_sum += observed_boid.x;
                vis_y_sum += observed_boid.y;
                vis_vx_sum += observed_boid.velocity.x;
                vis_vy_sum += observed_boid.velocity.y;
                visual_count++;
            }
        }
        if (protected_count > 0) {
            //current_boid_reward -= fabsf(protected_dist_sum / protected_count) * env->avoid_factor;
            current_boid_reward -= flclip(protected_count/5.0, 0.0f, 1.0f) * env->avoid_factor;
        }
        if (visual_count) {
            vis_x_avg  = vis_x_sum  / visual_count;
            vis_y_avg  = vis_y_sum  / visual_count;
            vis_vx_avg = vis_vx_sum / visual_count;
            vis_vy_avg = vis_vy_sum / visual_count;

            current_boid_reward -= fabsf(vis_vx_avg - current_boid->velocity.x) * env->matching_factor;
            current_boid_reward -= fabsf(vis_vy_avg - current_boid->velocity.y) * env->matching_factor;
            current_boid_reward -= fabsf(vis_x_avg  - current_boid->x) * env->centering_factor;
            current_boid_reward -= fabsf(vis_y_avg  - current_boid->y) * env->centering_factor;
        }
        if (current_boid->y < TOP_MARGIN || current_boid->y > HEIGHT - BOTTOM_MARGIN) {
            current_boid_reward -= env->margin_turn_factor;
        } else {
            current_boid_reward += env->margin_turn_factor;
        }
        if (current_boid->x < LEFT_MARGIN || current_boid->x > WIDTH  - RIGHT_MARGIN) {
            current_boid_reward -= env->margin_turn_factor;
        } else {
            current_boid_reward += env->margin_turn_factor;
        }
        // Normalization
        // env->rewards[current_indx] = current_boid_reward / 15.0f;
        // printf("current_boid_reward: %f\n", current_boid_reward);
        env->rewards[current_indx] = current_boid_reward / 2.0f;

        //log updates
        if (env->tick == env->report_interval) {
            env->log.score          += env->rewards[current_indx];
            env->log.n              += 1.0f;

            /* clear per-boid log for next episode */
            // env->boid_logs[boid_indx] = (Log){0};
            env->tick = 0;
        }
    }
    //env->log.score /= env->num_boids;

    compute_observations(env);
}

typedef struct Client Client;
struct Client {
    float width;
    float height;
    Texture2D boid_texture;
};

void c_close_client(Client* client) {
    UnloadTexture(client->boid_texture);
    CloseWindow();
    free(client);
}

void c_close(Boids* env) {
    free(env->boids);
    free(env->boid_logs);
    if (env->client != NULL) {
        c_close_client(env->client);
    }
}

Client* make_client(Boids* env) {
    Client* client = (Client*)calloc(1, sizeof(Client));
    
    client->width = WIDTH;
    client->height = HEIGHT;
    
    InitWindow(WIDTH, HEIGHT, "PufferLib Boids");
    SetTargetFPS(60);
    
    if (!IsWindowReady()) {
        TraceLog(LOG_ERROR, "Window failed to initialize\n");
        free(client);
        return NULL;
    }
    
    client->boid_texture = LoadTexture(BOID_TEXTURE_PATH);
    if (client->boid_texture.id == 0) {
        TraceLog(LOG_ERROR, "Failed to load texture: %s", BOID_TEXTURE_PATH);
        c_close_client(client);
        return NULL;
    }
    
    return client;
}

void c_render(Boids* env) {
    if (env->client == NULL) {
        env->client = make_client(env);
        if (env->client == NULL) {
            TraceLog(LOG_ERROR, "Failed to initialize client for rendering\n");
            return;
        }
    }
    
    if (!WindowShouldClose() && IsWindowReady()) {
        if (IsKeyDown(KEY_ESCAPE)) {
            exit(0);
        }

        BeginDrawing();
        ClearBackground((Color){6, 24, 24, 255});

        for (unsigned boid_indx = 0; boid_indx < env->num_boids; boid_indx++) {
            DrawTexturePro(
                env->client->boid_texture,
                (Rectangle){
                    (env->boids[boid_indx].velocity.x > 0) ? 0 : 128,
                    0,
                    128,
                    128,
                },
                (Rectangle){
                    env->boids[boid_indx].x,
                    env->boids[boid_indx].y,
                    BOID_WIDTH,
                    BOID_HEIGHT
                },
                (Vector2){0, 0},
                0,
                WHITE
            );
        }

        EndDrawing();
    } else {
        TraceLog(LOG_WARNING, "Window is not ready or should close");
    }
}



================================================
FILE: pufferlib/ocean/boids/boids.py
================================================
'''
High-perf Boids
Inspired by https://people.ece.cornell.edu/land/courses/ece4760/labs/s2021/Boids/Boids.html
'''

import numpy as np
import gymnasium

import pufferlib
from pufferlib.ocean.boids import binding

class Boids(pufferlib.PufferEnv):
    def __init__(
        self,
        num_envs=1,
        buf=None,
        render_mode=None,
        seed=0,
        report_interval=1,
        num_boids=1,
        margin_turn_factor=1.0,
        centering_factor=0.0,
        avoid_factor=0.0,
        matching_factor=0.0
    ):
        ACTION_SPACE_SIZE = 2
        self.num_agents = num_envs * num_boids
        self.num_boids = num_boids

        self.single_observation_space = gymnasium.spaces.Box(
            -1000.0, 1000.0, shape=(num_boids*4,), dtype=np.float32
        )
        
        #self.single_action_space = gymnasium.spaces.Box(
        #    -np.inf, np.inf, shape=(ACTION_SPACE_SIZE,), dtype=np.float32
        #)
        self.single_action_space = gymnasium.spaces.MultiDiscrete([5, 5])

        self.render_mode = render_mode
        self.report_interval = report_interval

        super().__init__(buf)
        self.actions = self.actions.astype(np.float32)

        # Create C binding with flattened action buffer
        # We need to manually create a flattened action buffer to pass to C
        #self.flat_actions = np.zeros((self.num_agents * ACTION_SPACE_SIZE), dtype=np.float32)
        
        c_envs = []
        for env_num in range(num_envs):
            c_envs.append(binding.env_init(
                self.observations[env_num*num_boids:(env_num+1)*num_boids],
                #self.flat_actions[env_num*num_boids*ACTION_SPACE_SIZE:(env_num+1)*num_boids*ACTION_SPACE_SIZE],
                self.actions[env_num*num_boids:(env_num+1)*num_boids],
                self.rewards[env_num*num_boids:(env_num+1)*num_boids],
                self.terminals[env_num*num_boids:(env_num+1)*num_boids],
                self.truncations[env_num*num_boids:(env_num+1)*num_boids],
                seed,
                num_boids=num_boids,
                report_interval=self.report_interval,
                margin_turn_factor=margin_turn_factor,
                centering_factor=centering_factor,
                avoid_factor=avoid_factor,
                matching_factor=matching_factor,
            ))
        
        self.c_envs = binding.vectorize(*c_envs)

    def reset(self, seed=0):
        self.tick = 0
        binding.vec_reset(self.c_envs, seed)
        return self.observations, []

    def step(self, actions):
        # Clip actions to valid range
        clipped_actions = (actions.astype(np.float32) - 2.0) / 4.0
        #clipped_actions = np.clip(actions, -1.0, 1.0)
        
        # Copy the clipped actions to our flat actions buffer for C binding
        # Flatten from [num_agents, num_boids, 2] to a 1D array for C
        # TODO: Check if I even need this? its not like I'm using the actions anywhere else
        #self.flat_actions[:] = clipped_actions.reshape(-1)
        
        # Save the original actions for the experience buffer
        # TODO: Same thing with this
        self.actions[:] = clipped_actions
        
        self.tick += 1
        binding.vec_step(self.c_envs)

        info = []
        if self.tick % self.report_interval == 0:
            log_data = binding.vec_log(self.c_envs)
            if log_data:
                info.append(log_data)

        # print(f"OBSERVATIONS: {self.observations}")
        return (self.observations, self.rewards,
            self.terminals, self.truncations, info)

    def render(self):
        binding.vec_render(self.c_envs, 0)

    def close(self):
        binding.vec_close(self.c_envs)

def test_performance(timeout=10, atn_cache=1024):
    env = Boids(num_envs=1000)
    env.reset()
    tick = 0

    # Generate random actions with proper shape: [cache_size, num_agents, action_dim]
    actions = np.random.uniform(-3.0, 3.0, (atn_cache, env.num_agents, 2))

    import time
    start = time.time()
    while time.time() - start < timeout:
        atn = actions[tick % atn_cache]
        env.step(atn)
        tick += 1

    print(f'SPS: {env.num_agents * tick / (time.time() - start)}')


if __name__ == '__main__':
    test_performance()



================================================
FILE: pufferlib/ocean/breakout/binding.c
================================================
#include "breakout.h"

#define Env Breakout
#include "../env_binding.h"

static int my_init(Env* env, PyObject* args, PyObject* kwargs) {
    env->frameskip = unpack(kwargs, "frameskip");
    env->width = unpack(kwargs, "width");
    env->height = unpack(kwargs, "height");
    env->initial_paddle_width = unpack(kwargs, "paddle_width");
    env->paddle_height = unpack(kwargs, "paddle_height");
    env->ball_width = unpack(kwargs, "ball_width");
    env->ball_height = unpack(kwargs, "ball_height");
    env->brick_width = unpack(kwargs, "brick_width");
    env->brick_height = unpack(kwargs, "brick_height");
    env->brick_rows = unpack(kwargs, "brick_rows");
    env->brick_cols = unpack(kwargs, "brick_cols");
    env->initial_ball_speed = unpack(kwargs, "initial_ball_speed");
    env->max_ball_speed = unpack(kwargs, "max_ball_speed");
    env->paddle_speed = unpack(kwargs, "paddle_speed");
    env->continuous = unpack(kwargs, "continuous");
    init(env);
    return 0;
}

static int my_log(PyObject* dict, Log* log) {
    assign_to_dict(dict, "perf", log->perf);
    assign_to_dict(dict, "score", log->score);
    assign_to_dict(dict, "episode_return", log->episode_return);
    assign_to_dict(dict, "episode_length", log->episode_length);
    return 0;
}



================================================
FILE: pufferlib/ocean/breakout/breakout.c
================================================
#include <time.h>
#include "breakout.h"
#include "puffernet.h"

void demo() {
    Weights* weights = load_weights("resources/breakout/breakout_weights.bin", 147844);
    int logit_sizes[1] = {3};
    LinearLSTM* net = make_linearlstm(weights, 1, 118, logit_sizes, 1);

    Breakout env = {
        .frameskip = 1,
        .width = 576,
        .height = 330,
        .paddle_width = 62,
        .paddle_height = 8,
        .ball_width = 32,
        .ball_height = 32,
        .brick_width = 32,
        .brick_height = 12,
        .brick_rows = 6,
        .brick_cols = 18,
        .initial_ball_speed = 256,
        .max_ball_speed = 448,
        .paddle_speed = 620,
        .continuous = 0,
    };
    allocate(&env);

    env.client = make_client(&env);

    c_reset(&env);
    int frame = 0;
    SetTargetFPS(60);
    while (!WindowShouldClose()) {
        // User can take control of the paddle
        if (IsKeyDown(KEY_LEFT_SHIFT)) {
            if(env.continuous) {
                float move = GetMouseWheelMove();
                float clamped_wheel = fmaxf(-1.0f, fminf(1.0f, move));
                env.actions[0] = clamped_wheel;
            } else {
                env.actions[0] = 0.0;
                if (IsKeyDown(KEY_LEFT)  || IsKeyDown(KEY_A)) env.actions[0] = 1;
                if (IsKeyDown(KEY_RIGHT) || IsKeyDown(KEY_D)) env.actions[0] = 2;
            }
        } else if (frame % 4 == 0) {
            // Apply frameskip outside the env for smoother rendering
            int* actions = (int*)env.actions;
            forward_linearlstm(net, env.observations, actions);
            env.actions[0] = actions[0];
        }

        frame = (frame + 1) % 4;
        c_step(&env);
        c_render(&env);
    }
    free_linearlstm(net);
    free(weights);
    free_allocated(&env);
    close_client(env.client);
}

void test_performance(int timeout) {
    Breakout env = {
        .width = 512,
        .height = 512,
        .paddle_width = 20,
        .paddle_height = 70,
        .ball_width = 10,
        .ball_height = 15,
        .brick_width = 10,
        .brick_height = 10,
        .brick_rows = 5,
        .brick_cols = 10,
        .continuous = 0,
    };
    allocate(&env);
    c_reset(&env);

    int start = time(NULL);
    int num_steps = 0;
    while (time(NULL) - start < timeout) {
        env.actions[0] = rand() % 3;
        c_step(&env);
        num_steps++;
    }

    int end = time(NULL);
    float sps = num_steps / (end - start);
    printf("Test Environment SPS: %f\n", sps);
    free_allocated(&env);
}

int main() {
    demo();
    //test_performance(10);
}



================================================
FILE: pufferlib/ocean/breakout/breakout.h
================================================
#include <stdlib.h>
#include <math.h>
#include <assert.h>
#include <unistd.h>
#include <limits.h>
#include <string.h>
#include "raylib.h"

#define NOOP 0
#define LEFT 1
#define RIGHT 2
#define HALF_PADDLE_WIDTH 31
#define Y_OFFSET 50
#define TICK_RATE 1.0f/60.0f

#define BRICK_INDEX_NO_COLLISION -4
#define BRICK_INDEX_SIDEWALL_COLLISION -3
#define BRICK_INDEX_BACKWALL_COLLISION -2
#define BRICK_INDEX_PADDLE_COLLISION -1

typedef struct Log {
    float perf;
    float score;
    float episode_return;
    float episode_length;
    float n;
} Log;

typedef struct Client {
    float width;
    float height;
    float paddle_width;
    float paddle_height;
    float ball_width;
    float ball_height;    
    Texture2D ball;
} Client;

typedef struct Breakout {
    Client* client;
    Log log;
    float* observations;
    float* actions;
    float* rewards;
    unsigned char* terminals;
    int score;
    float paddle_x;
    float paddle_y;
    float ball_x;
    float ball_y;
    float ball_vx;
    float ball_vy;
    float* brick_x;
    float* brick_y;
    float* brick_states;
    int balls_fired;
    float initial_paddle_width;
    float paddle_width;
    float paddle_height;
    float paddle_speed;
    float ball_speed;
    float initial_ball_speed;
    float max_ball_speed;
    int hits;
    int width;
    int height;
    int num_bricks;
    int brick_rows;
    int brick_cols;
    int ball_width;
    int ball_height;
    int brick_width;
    int brick_height;
    int num_balls;
    int max_score;
    int half_max_score;
    int tick;
    int frameskip;
    unsigned char hit_brick;
    int continuous;
} Breakout;

typedef struct CollisionInfo CollisionInfo;
struct CollisionInfo {
    float t;
    float overlap;
    float x;
    float y;
    float vx; 
    float vy;
    int brick_index;
};

void generate_brick_positions(Breakout* env) {
    env->half_max_score=0;
    for (int row = 0; row < env->brick_rows; row++) {
        for (int col = 0; col < env->brick_cols; col++) {
            int idx = row * env->brick_cols + col;
            env->brick_x[idx] = col*env->brick_width;
            env->brick_y[idx] = row*env->brick_height + Y_OFFSET;
            env->half_max_score += 7 - 3 * (idx / env->brick_cols / 2);
        }
    }
    env->max_score=2*env->half_max_score;
}

void init(Breakout* env) {
    env->tick = 0;
    env->num_bricks = env->brick_rows * env->brick_cols;
    assert(env->num_bricks > 0);

    env->brick_x = (float*)calloc(env->num_bricks, sizeof(float));
    env->brick_y = (float*)calloc(env->num_bricks, sizeof(float));
    env->brick_states = (float*)calloc(env->num_bricks, sizeof(float));
    env->num_balls = -1;
    generate_brick_positions(env);
}

void allocate(Breakout* env) {
    init(env);
    env->observations = (float*)calloc(11 + env->num_bricks, sizeof(float));
    env->actions = (float*)calloc(1, sizeof(float));
    env->rewards = (float*)calloc(1, sizeof(float));
    env->terminals = (unsigned char*)calloc(1, sizeof(unsigned char));
}

void c_close(Breakout* env) {
    free(env->brick_x);
    free(env->brick_y);
    free(env->brick_states);
}

void free_allocated(Breakout* env) {
    free(env->actions);
    free(env->observations);
    free(env->terminals);
    free(env->rewards);
    c_close(env);
}

void add_log(Breakout* env) {
    env->log.episode_length += env->tick;
    env->log.episode_return += env->score;
    env->log.score += env->score;
    env->log.perf += env->score / (float)env->max_score;
    env->log.n += 1;
}

void compute_observations(Breakout* env) {
    env->observations[0] = env->paddle_x / env->width;
    env->observations[1] = env->paddle_y / env->height;
    env->observations[2] = env->ball_x / env->width;
    env->observations[3] = env->ball_y / env->height;
    env->observations[4] = env->ball_vx / 512.0f;
    env->observations[5] = env->ball_vy / 512.0f;
    env->observations[6] = env->balls_fired / 5.0f;
    env->observations[7] = env->score / 864.0f;
    env->observations[8] = env->num_balls / 5.0f;
    env->observations[9] = env->paddle_width / (2.0f * HALF_PADDLE_WIDTH);
    for (int i = 0; i < env->num_bricks; i++) {
        env->observations[10 + i] = env->brick_states[i];
    }
}

// Collision of a stationary vertical line segment (xw,yw) to (xw,yw+hw)
// with a moving line segment (x+vx*t,y+vy*t) to (x+vx*t,y+vy*t+h).
static inline bool calc_vline_collision(float xw, float yw, float hw, float x,
        float y, float vx, float vy, float h, CollisionInfo* col) {
    float t_new = (xw - x) / vx;
    float topmost = fmin(yw + hw, y + h + vy * t_new);
    float botmost = fmax(yw, y + vy * t_new);
    float overlap_new = topmost - botmost;

    // Collision finds the smallest time of collision with the greatest overlap
    // between the ball and the wall.
    if (overlap_new > 0.0f && t_new > 0.0f && t_new <= 1.0f  && 
        (t_new < col->t || (t_new == col->t && overlap_new > col->overlap))) {
        col->t = t_new;
        col->overlap = overlap_new;
        col->x = xw;
        col->y = y + vy * t_new;
        col->vx = -vx;
        col->vy = vy;
        return true;
    }
    return false;
}
static inline bool calc_hline_collision(float xw, float yw, float ww,
        float x, float y, float vx, float vy, float w, CollisionInfo* col) {
    float t_new = (yw - y) / vy;
    float rightmost = fminf(xw + ww, x + w + vx * t_new);
    float leftmost = fmaxf(xw, x + vx * t_new);
    float overlap_new = rightmost - leftmost;

    // Collision finds the smallest time of collision with the greatest overlap between the ball and the wall.
    if (overlap_new > 0.0f && t_new > 0.0f && t_new <= 1.0f && 
        (t_new < col->t || (t_new == col->t && overlap_new > col->overlap))) {
        col->t = t_new;
        col->overlap = overlap_new;
        col->x = x + vx * t_new;
        col->y = yw;
        col->vx = vx;
        col->vy = -vy;
        return true;
    }
    return false;
}
static inline void calc_brick_collision(Breakout* env, int idx, 
        CollisionInfo* collision_info) {
    bool collision = false;
    // Brick left wall collides with ball right side
    if (env->ball_vx > 0) {
        if (calc_vline_collision(env->brick_x[idx], env->brick_y[idx], env->brick_height,
                env->ball_x + env->ball_width, env->ball_y, env->ball_vx, env->ball_vy, env->ball_height, collision_info)) {
            collision = true;
            collision_info->x -= env->ball_width;
        }
    }

    // Brick right wall collides with ball left side
    if (env->ball_vx < 0) {
        if (calc_vline_collision(env->brick_x[idx] + env->brick_width, env->brick_y[idx], env->brick_height,
                env->ball_x, env->ball_y, env->ball_vx, env->ball_vy, env->ball_height, collision_info)) {
            collision = true;
        }
    }

    // Brick top wall collides with ball bottom side
    if (env->ball_vy > 0) {
        if (calc_hline_collision(env->brick_x[idx], env->brick_y[idx], env->brick_width,
                env->ball_x, env->ball_y + env->ball_height, env->ball_vx, env->ball_vy, env->ball_width, collision_info)) {
            collision = true;
            collision_info->y -= env->ball_height;
        }
    }

    // Brick bottom wall collides with ball top side
    if (env->ball_vy < 0) {
        if (calc_hline_collision(env->brick_x[idx], env->brick_y[idx] + env->brick_height, env->brick_width,
                env->ball_x, env->ball_y, env->ball_vx, env->ball_vy, env->ball_width, collision_info)) {
            collision = true;
        }
    }
    if (collision) {
        collision_info->brick_index = idx;
    }
}
static inline int column_index(Breakout* env, float x) {
    return (int)(floorf(x / env->brick_width));
}
static inline int row_index(Breakout* env, float y) {
    return (int)(floorf((y - Y_OFFSET) / env->brick_height));
}

void calc_all_brick_collisions(Breakout* env, CollisionInfo* collision_info) {
    int column_from = column_index(env, fminf(env->ball_x + env->ball_vx, env->ball_x));
    column_from = fmaxf(column_from, 0);
    int column_to = column_index(env, fmaxf(env->ball_x + env->ball_width + env->ball_vx, env->ball_x + env->ball_width));
    column_to = fminf(column_to, env->brick_cols - 1);
    int row_from = row_index(env, fminf(env->ball_y + env->ball_vy, env->ball_y));
    row_from = fmaxf(row_from, 0);
    int row_to = row_index(env, fmaxf(env->ball_y + env->ball_height + env->ball_vy, env->ball_y + env->ball_height));
    row_to = fminf(row_to, env->brick_rows - 1);

    for (int row = row_from; row <= row_to; row++) {
        for (int column = column_from; column <= column_to; column++) {
            int brick_index = row * env->brick_cols + column;
            if (env->brick_states[brick_index] == 0.0)
                calc_brick_collision(env, brick_index, collision_info);
        }
    }
}

bool calc_paddle_ball_collisions(Breakout* env, CollisionInfo* collision_info) {
    float base_angle = M_PI / 4.0f;

    // Check if ball is above the paddle
    if (env->ball_y + env->ball_height + env->ball_vy < env->paddle_y) {
        return false;
    }

    // Check for collision
    // If we've found another collision (eg the ball hits the wall before the paddle)
    // this correctly skips the paddle collision.
    if (!calc_hline_collision(env->paddle_x, env->paddle_y, env->paddle_width,
          env->ball_x, env->ball_y + env->ball_height, env->ball_vx, env->ball_vy, env->ball_width,
          collision_info) || collision_info->t > 1.0f) {
        return false;
    }

    collision_info->y -= env->ball_height;
    collision_info->brick_index = BRICK_INDEX_PADDLE_COLLISION;

    env->hit_brick = false;
    float relative_intersection = (
        (env->ball_x + env->ball_width / 2) - env->paddle_x) / env->paddle_width;
    float angle = -base_angle + relative_intersection * 2 * base_angle;
    env->ball_vx = sin(angle) * env->ball_speed * TICK_RATE;
    env->ball_vy = -cos(angle) * env->ball_speed * TICK_RATE;
    env->hits += 1;
    if (env->hits % 4 == 0 && env->ball_speed < env->max_ball_speed) {
        env->ball_speed += 64;
    }
    if (env->score == env->half_max_score) {
        for (int i = 0; i < env->num_bricks; i++) {
            env->brick_states[i] = 0.0;
        }
    }
    return true;
}

void calc_all_wall_collisions(Breakout* env, CollisionInfo* collision_info) {
    if (env->ball_vx < 0) {
        if (calc_vline_collision(0, 0, env->height,
                env->ball_x, env->ball_y, env->ball_vx, env->ball_vy, env->ball_height,
                collision_info)) {
            collision_info->brick_index = BRICK_INDEX_SIDEWALL_COLLISION;
        }
    }
    if (env->ball_vx > 0) {
        if (calc_vline_collision(env->width, 0, env->height,
                 env->ball_x + env->ball_width, env->ball_y, env->ball_vx, env->ball_vy, env->ball_height,
                 collision_info)) {
            collision_info->x -= env->ball_width;
            collision_info->brick_index = BRICK_INDEX_SIDEWALL_COLLISION;
        }
    }
    if (env->ball_vy < 0) {
        if (calc_hline_collision(0, 0, env->width,
                 env->ball_x, env->ball_y, env->ball_vx, env->ball_vy, env->ball_width,
                 collision_info)) {
            collision_info->brick_index = BRICK_INDEX_BACKWALL_COLLISION;
        }
    }
}

// With rare floating point conditions, the ball could escape the bounds.
// Let's handle that explicitly.
void check_wall_bounds(Breakout* env) {
    float offset = env->max_ball_speed * 1.1f * TICK_RATE;
    if (env->ball_x < 0) {
        env->ball_x += offset;
    }
    if (env->ball_x > env->width) {
        env->ball_x -= offset;
    }
    if (env->ball_y < 0) {
        env->ball_y += offset;
    }
}

void destroy_brick(Breakout* env, int brick_idx) {
    float gained_points = 7 - 3 * ((brick_idx / env->brick_cols) / 2);

    env->score += gained_points;
    env->brick_states[brick_idx] = 1.0;

    env->rewards[0] += gained_points;

    if (brick_idx / env->brick_cols < 3) {
        env->ball_speed = env->max_ball_speed;
    }
}

bool handle_collisions(Breakout* env) {
    CollisionInfo collision_info = {
        .t = 2.0f,
        .overlap = -1.0f,
        .x = 0.0f,
        .y = 0.0f,
        .vx = 0.0f,
        .vy = 0.0f,
        .brick_index = BRICK_INDEX_NO_COLLISION,
    };

    check_wall_bounds(env);

    calc_all_brick_collisions(env, &collision_info);
    calc_all_wall_collisions(env, &collision_info);
    calc_paddle_ball_collisions(env, &collision_info);
    if (collision_info.brick_index != BRICK_INDEX_PADDLE_COLLISION 
            && collision_info.t <= 1.0f) {
        env->ball_x = collision_info.x;
        env->ball_y = collision_info.y;
        env->ball_vx = collision_info.vx;
        env->ball_vy = collision_info.vy;
        if (collision_info.brick_index >= 0) {
            destroy_brick(env, collision_info.brick_index);
        }
        if (collision_info.brick_index == BRICK_INDEX_BACKWALL_COLLISION) {
            env->paddle_width = HALF_PADDLE_WIDTH;
        }
    }
    return collision_info.brick_index != BRICK_INDEX_NO_COLLISION;
}

void reset_round(Breakout* env) {
    env->balls_fired = 0;
    env->hit_brick = false;
    env->hits = 0;
    env->ball_speed = env->initial_ball_speed;
    env->paddle_width = env->initial_paddle_width;

    env->paddle_x = env->width / 2.0 - env->paddle_width / 2;
    env->paddle_y = env->height - env->paddle_height - 10;

    env->ball_x = env->paddle_x + (env->paddle_width / 2 - env->ball_width / 2);
    env->ball_y = env->height / 2 - 30;

    env->ball_vx = 0.0;
    env->ball_vy = 0.0;
}

void c_reset(Breakout* env) {
    env->score = 0;
    env->num_balls = 5;
    for (int i = 0; i < env->num_bricks; i++) {
        env->brick_states[i] = 0.0;
    }
    reset_round(env);
    env->tick = 0;
    compute_observations(env);
}

void step_frame(Breakout* env, float action) {
    float act = 0.0;
    if (env->balls_fired == 0) {
        env->balls_fired = 1;
        float direction = M_PI / 3.25f;

        env->ball_vy = cos(direction) * env->ball_speed * TICK_RATE;
        env->ball_vx = sin(direction) * env->ball_speed * TICK_RATE;
        if (rand() % 2 == 0) {
            env->ball_vx = -env->ball_vx;
        }
    }   
     else if (action == LEFT) {
        act = -1.0;
    } else if (action == RIGHT) {
        act = 1.0;
    }
    if (env->continuous){
        act = action;
    }
    env->paddle_x += act * env->paddle_speed * TICK_RATE;
    if (env->paddle_x <= 0){
        env->paddle_x = fmaxf(0, env->paddle_x);
    } else {
        env->paddle_x = fminf(env->width - env->paddle_width, env->paddle_x);
    }

    //Handle collisions. 
    //Regular timestepping is done only if there are no collisions.
    if(!handle_collisions(env)){
        env->ball_x += env->ball_vx;
        env->ball_y += env->ball_vy;
    }

    if (env->ball_y >= env->paddle_y + env->paddle_height) {
        env->num_balls -= 1;
        reset_round(env);
    }
    if (env->num_balls < 0 || env->score == env->max_score) {
        env->terminals[0] = 1;
        add_log(env);
        c_reset(env);
    }
}

void c_step(Breakout* env) {
    env->terminals[0] = 0;
    env->rewards[0] = 0.0;

    float action = env->actions[0];
    for (int i = 0; i < env->frameskip; i++) {
        env->tick += 1;
        step_frame(env, action);
    }

    compute_observations(env);
}

Color BRICK_COLORS[6] = {RED, ORANGE, YELLOW, GREEN, SKYBLUE, BLUE};

Client* make_client(Breakout* env) {
    Client* client = (Client*)calloc(1, sizeof(Client));
    client->width = env->width;
    client->height = env->height;
    client->paddle_width = env->paddle_width;
    client->paddle_height = env->paddle_height;
    client->ball_width = env->ball_width;
    client->ball_height = env->ball_height;

    InitWindow(env->width, env->height, "PufferLib Breakout");
    SetTargetFPS(60 / env->frameskip);

    client->ball = LoadTexture("resources/shared/puffers_128.png");
    return client;
}

void close_client(Client* client) {
    CloseWindow();
    free(client);
}

void c_render(Breakout* env) {
    if (env->client == NULL) {
        env->client = make_client(env);
    }

    Client* client = env->client;

    if (IsKeyDown(KEY_ESCAPE)) {
        exit(0);
    }
    if (IsKeyPressed(KEY_TAB)) {
        ToggleFullscreen();
    }

    BeginDrawing();
    ClearBackground((Color){6, 24, 24, 255});

    DrawRectangle(env->paddle_x, env->paddle_y,
        env->paddle_width, env->paddle_height, (Color){0, 255, 255, 255});

    // Draw ball
    DrawTexturePro(
        client->ball,
        (Rectangle){
            (env->ball_vx > 0) ? 0 : 128,
            0, 128, 128,
        },
        (Rectangle){
            env->ball_x,
            env->ball_y,
            env->ball_width,
            env->ball_height
        },
        (Vector2){0, 0},
        0,
        WHITE
    );

    for (int row = 0; row < env->brick_rows; row++) {
        for (int col = 0; col < env->brick_cols; col++) {
            int brick_idx = row * env->brick_cols + col;
            if (env->brick_states[brick_idx] == 1) {
                continue;
            }
            int x = env->brick_x[brick_idx];
            int y = env->brick_y[brick_idx];
            Color brick_color = BRICK_COLORS[row];
            DrawRectangle(x, y, env->brick_width, env->brick_height, brick_color);
        }
    }

    DrawText(TextFormat("Score: %i", env->score), 10, 10, 20, WHITE);
    DrawText(TextFormat("Balls: %i", env->num_balls), client->width - 80, 10, 20, WHITE);
    EndDrawing();

    //PlaySound(client->sound);
}



================================================
FILE: pufferlib/ocean/breakout/breakout.py
================================================
import numpy as np
import gymnasium

import pufferlib
from pufferlib.ocean.breakout import binding

class Breakout(pufferlib.PufferEnv):
    def __init__(self, num_envs=1, render_mode=None,
            frameskip=4, width=576, height=330,
            paddle_width=62, paddle_height=8,
            ball_width=32, ball_height=32,
            brick_width=32, brick_height=12,
            brick_rows=6, brick_cols=18,
            initial_ball_speed=256, max_ball_speed=448,
            paddle_speed=620,
            continuous=False, log_interval=128,
            buf=None, seed=0):
        self.single_observation_space = gymnasium.spaces.Box(low=0, high=1,
            shape=(10 + brick_rows*brick_cols,), dtype=np.float32)
        self.render_mode = render_mode
        self.num_agents = num_envs
        self.continuous = continuous
        self.log_interval = log_interval
        self.tick = 0
        
        if continuous:
            self.single_action_space = gymnasium.spaces.Box(low=-1, high=1,
                shape=(1,), dtype=np.float32)
        else:
            self.single_action_space = gymnasium.spaces.Discrete(3)
            
        super().__init__(buf)
        if continuous:
            self.actions = self.actions.flatten()
        else:
            self.actions = self.actions.astype(np.float32)
            
        self.c_envs = binding.vec_init(self.observations, self.actions, self.rewards,
            self.terminals, self.truncations, num_envs, seed, frameskip=frameskip,
            width=width, height=height, paddle_width=paddle_width,
            paddle_height=paddle_height, ball_width=ball_width,
            ball_height=ball_height, brick_width=brick_width,
            brick_height=brick_height, brick_rows=brick_rows,
            brick_cols=brick_cols, initial_ball_speed=initial_ball_speed,
            max_ball_speed=max_ball_speed, paddle_speed=paddle_speed,
            continuous=continuous
        )

    def reset(self, seed=0):
        binding.vec_reset(self.c_envs, seed)
        self.tick = 0
        return self.observations, []

    def step(self, actions):
        if self.continuous:
            self.actions[:] = np.clip(actions.flatten(), -1.0, 1.0)
        else:
            self.actions[:] = actions
            
        self.tick += 1
        binding.vec_step(self.c_envs)

        info = []
        if self.tick % self.log_interval == 0:
            info.append(binding.vec_log(self.c_envs))

        return (self.observations, self.rewards,
            self.terminals, self.truncations, info)

    def render(self):
        binding.vec_render(self.c_envs, 0)

    def close(self):
        binding.vec_close(self.c_envs)

def test_performance(timeout=10, atn_cache=1024):
    env = Breakout(num_envs=1000)
    env.reset()
    tick = 0

    actions = np.random.randint(0, 2, (atn_cache, env.num_agents))

    import time
    start = time.time()
    while time.time() - start < timeout:
        atn = actions[tick % atn_cache]
        env.step(atn)
        tick += 1

    print(f'SPS: %f', env.num_agents * tick / (time.time() - start))

if __name__ == '__main__':
    test_performance()



================================================
FILE: pufferlib/ocean/cartpole/binding.c
================================================
#include "cartpole.h"
#define Env Cartpole
#include "../env_binding.h"

static int my_init(Env* env, PyObject* args, PyObject* kwargs) {   
    env->cart_mass = unpack(kwargs, "cart_mass");
    env->pole_mass = unpack(kwargs, "pole_mass");
    env->pole_length = unpack(kwargs, "pole_length");
    env->gravity = unpack(kwargs, "gravity");
    env->force_mag = unpack(kwargs, "force_mag");
    env->tau = unpack(kwargs, "dt");
    env->continuous = unpack(kwargs, "continuous");
    init(env);
    return 0;
}

static int my_log(PyObject* dict, Log* log) {
    assign_to_dict(dict, "score", log->score);
    assign_to_dict(dict, "perf", log->perf);
    assign_to_dict(dict, "episode_length", log->episode_length);
    assign_to_dict(dict, "x_threshold_termination", log->x_threshold_termination);
    assign_to_dict(dict, "pole_angle_termination", log->pole_angle_termination);
    assign_to_dict(dict, "max_steps_termination", log->max_steps_termination);
    assign_to_dict(dict, "n", log->n);
    return 0;
}



================================================
FILE: pufferlib/ocean/cartpole/cartpole.c
================================================
// local compile/eval implemented for discrete actions only
// eval with python demo.py --mode eval --env puffer_cartpole --eval-mode-path <path to model>

#include <math.h>
#include <stdlib.h>
#include <stdio.h>
#include <time.h>
#include "cartpole.h"
#include "puffernet.h"

#define NUM_WEIGHTS 133123
#define OBSERVATIONS_SIZE 4
#define ACTIONS_SIZE 2
#define CONTINUOUS 0

const char* WEIGHTS_PATH = "resources/cartpole/cartpole_weights.bin";

float movement(int discrete_action, int userControlMode) {
    if (userControlMode) {
        return (IsKeyDown(KEY_RIGHT) || IsKeyDown(KEY_D)) ? 1.0f : -1.0f;
    } else {
        return (discrete_action == 1) ? 1.0f : -1.0f;
    }
}

void demo() {
    Weights* weights = load_weights(WEIGHTS_PATH, NUM_WEIGHTS);
    LinearLSTM* net;
    
    int logit_sizes[1] = {ACTIONS_SIZE};
    net = make_linearlstm(weights, 1, OBSERVATIONS_SIZE, logit_sizes, 1);
    Cartpole env = {0};
    env.continuous = CONTINUOUS;
    allocate(&env);
    c_reset(&env);
    c_render(&env);

    SetTargetFPS(60);

    while (!WindowShouldClose()) {
        int userControlMode = IsKeyDown(KEY_LEFT_SHIFT);

        if (!userControlMode) {
            int action_value;
            forward_linearlstm(net, env.observations, &action_value);
            env.actions[0] = movement(action_value, 0);
        } else {
            env.actions[0] = movement(env.actions[0], userControlMode);
        }   

        c_step(&env);

        BeginDrawing();
        ClearBackground(RAYWHITE);
        c_render(&env);
        EndDrawing();

        if (env.terminals[0]) {
            c_reset(&env);
        }
    }

    free_linearlstm(net);
    free(weights);
    free_allocated(&env);
}

int main() {
    srand(time(NULL));
    demo();
    return 0;
}



================================================
FILE: pufferlib/ocean/cartpole/cartpole.h
================================================
#include <stdlib.h>
#include <string.h>
#include <stdio.h>
#include <stdbool.h>
#include <math.h>
#include <time.h>
#include "raylib.h"

#define X_THRESHOLD 2.4f
#define THETA_THRESHOLD_RADIANS (12 * 2 * M_PI / 360)
#define MAX_STEPS 200
#define WIDTH 600
#define HEIGHT 200
#define SCALE 100

typedef struct Log Log;
struct Log {
    float perf;
    float episode_length;
    float x_threshold_termination;
    float pole_angle_termination;
    float max_steps_termination;
    float n;
    float score;
};

typedef struct Client Client;
struct Client {
};

typedef struct Cartpole Cartpole;
struct Cartpole {
    float* observations;
    float* actions;
    float* rewards;
    unsigned char* terminals;
    unsigned char* truncations;
    Log log;
    Client* client;
    float x;
    float x_dot;
    float theta;
    float theta_dot;
    int tick;
    float cart_mass;
    float pole_mass;
    float pole_length;
    float gravity;
    float force_mag;
    float tau;
    int continuous;
    float episode_return;
};

void add_log(Cartpole* env) {
    if (env->episode_return > 0) {
        env->log.perf = env->episode_return / MAX_STEPS;
    } else {
        env->log.perf = 0.0f;
    }
    env->log.episode_length += env->tick;
    env->log.score += env->tick;
    env->log.x_threshold_termination += (env->x < -X_THRESHOLD || env->x > X_THRESHOLD);
    env->log.pole_angle_termination += (env->theta < -THETA_THRESHOLD_RADIANS || env->theta > THETA_THRESHOLD_RADIANS);
    env->log.max_steps_termination += (env->tick >= MAX_STEPS);
    env->log.n += 1;
}

void init(Cartpole* env) {
    env->tick = 0;
    memset(&env->log, 0, sizeof(Log));
}

void allocate(Cartpole* env) {
    init(env);
    env->observations = (float*)calloc(4, sizeof(float));
    env->actions = (float*)calloc(1, sizeof(float));
    env->rewards = (float*)calloc(1, sizeof(float));
    env->terminals = (unsigned char*)calloc(1, sizeof(unsigned char));
}

void free_allocated(Cartpole* env) {
    free(env->observations);
    free(env->actions);
    free(env->rewards);
    free(env->terminals);
}

void c_close(Cartpole* env) {
}

const Color PUFF_RED = (Color){187, 0, 0, 255};
const Color PUFF_CYAN = (Color){0, 187, 187, 255};
const Color PUFF_WHITE = (Color){241, 241, 241, 241};
const Color PUFF_BACKGROUND = (Color){6, 24, 24, 255};

Client* make_client(Cartpole* env) {
    Client* client = (Client*)calloc(1, sizeof(Client));
    InitWindow(WIDTH, HEIGHT, "puffer Cartpole");
    SetTargetFPS(60);
    return client;
}

void close_client(Client* client) {
    CloseWindow();
    free(client);
}

void c_render(Cartpole* env) {
    if (IsKeyDown(KEY_ESCAPE))
        exit(0);
    if (IsKeyPressed(KEY_TAB))
        ToggleFullscreen();

    if (env->client == NULL) {
        env->client = make_client(env);
    }

    BeginDrawing();
    ClearBackground(PUFF_BACKGROUND);
    DrawLine(0, HEIGHT / 1.5, WIDTH, HEIGHT / 1.5, PUFF_CYAN);
    float cart_x = WIDTH / 2 + env->x * SCALE;
    float cart_y = HEIGHT / 1.6;
    DrawRectangle((int)(cart_x - 20), (int)(cart_y - 10), 40, 20, PUFF_CYAN);
    float pole_length = 2.0f * 0.5f * SCALE;
    float pole_x2 = cart_x + sinf(env->theta) * pole_length;
    float pole_y2 = cart_y - cosf(env->theta) * pole_length;
    DrawLineEx((Vector2){cart_x, cart_y}, (Vector2){pole_x2, pole_y2}, 5, PUFF_RED);
    DrawText(TextFormat("Steps: %i", env->tick), 10, 10, 20, PUFF_WHITE);
    DrawText(TextFormat("Cart Position: %.2f", env->x), 10, 40, 20, PUFF_WHITE);
    DrawText(TextFormat("Pole Angle: %.2f", env->theta * 180.0f / M_PI), 10, 70, 20, PUFF_WHITE);
    EndDrawing();
}

void compute_observations(Cartpole* env) {
    env->observations[0] = env->x;
    env->observations[1] = env->x_dot;
    env->observations[2] = env->theta;
    env->observations[3] = env->theta_dot;
}

void c_reset(Cartpole* env) {
    env->episode_return = 0.0f;
    env->x = ((float)rand() / (float)RAND_MAX) * 0.08f - 0.04f;
    env->x_dot = ((float)rand() / (float)RAND_MAX) * 0.08f - 0.04f;
    env->theta = ((float)rand() / (float)RAND_MAX) * 0.08f - 0.04f;
    env->theta_dot = ((float)rand() / (float)RAND_MAX) * 0.08f - 0.04f;
    env->tick = 0;
    
    compute_observations(env);
}

void c_step(Cartpole* env) {  
    // float force = 0.0;
    // if (env->continuous) {
    //     force = env->actions[0] * FORCE_MAG;
    // } else {
    //     force = (env->actions[0] > 0.5f) ? FORCE_MAG : -FORCE_MAG; 
    // }

    float a = env->actions[0];

    /* ===== runtime sanity check –– delete after debugging ===== */
    if (!isfinite(a) || a < -1.0001f || a > 1.0001f) {
        fprintf(stderr,
                "[BAD ACTION] tick=%d  raw=%.6f\n",
                env->tick, a);
        fflush(stderr);
    }
    /* ========================================================== */

    if (!isfinite(a)) {
        a = 0.0f;
    }
    a = fminf(fmaxf(a, -1.0f), 1.0f);
    env->actions[0] = a;

    float force = env->continuous ? a * env->force_mag
                                  : (a > 0.5f ? env->force_mag: -env->force_mag);

    float costheta = cosf(env->theta);
    float sintheta = sinf(env->theta);

    float total_mass = env->cart_mass + env->pole_mass;
    float polemass_length = total_mass + env->pole_mass;
    float temp = (force + polemass_length * env->theta_dot * env->theta_dot * sintheta) / total_mass;
    float thetaacc = (env->gravity * sintheta - costheta * temp) / 
                     (env->pole_length * (4.0f / 3.0f - total_mass * costheta * costheta / total_mass));
    float xacc = temp - polemass_length * thetaacc * costheta / total_mass;

    env->x += env->tau * env->x_dot;
    env->x_dot += env->tau * xacc;
    env->theta += env->tau * env->theta_dot;
    env->theta_dot += env->tau * thetaacc;

    env->tick += 1;
    
    bool terminated = env->x < -X_THRESHOLD || env->x > X_THRESHOLD ||
                env->theta < -THETA_THRESHOLD_RADIANS || env->theta > THETA_THRESHOLD_RADIANS;
    bool truncated = env->tick >= MAX_STEPS;
    bool done = terminated || truncated;

    env->rewards[0] = done ? 0.0f : 1.0f;
    env->episode_return += env->rewards[0];
    env->terminals[0] = terminated ? 1 : 0;

    if (done) {
        add_log(env);
        c_reset(env);
    }

    compute_observations(env);
}



================================================
FILE: pufferlib/ocean/cartpole/cartpole.py
================================================
import numpy as np
import gymnasium
import pufferlib
from pufferlib.ocean.cartpole import binding

class Cartpole(pufferlib.PufferEnv):
    def __init__(self, num_envs=1, cart_mass=1.0, pole_mass=0.1,
            pole_length=0.5, gravity=9.8, force_mag=10.0, dt=0.02,
            render_mode='human', report_interval=1, continuous=False,
            buf=None, seed=0):
        self.render_mode = render_mode
        self.num_agents = num_envs
        self.report_interval = report_interval
        self.tick = 0
        self.continuous = continuous
        self.human_action = None

        self.num_obs = 4
        self.single_observation_space = gymnasium.spaces.Box(
            low=-np.inf, high=np.inf, shape=(self.num_obs,), dtype=np.float32
        )
        if self.continuous:
            self.single_action_space = gymnasium.spaces.Box(
                low=-1.0, high=1.0, shape=(1,)
            )
            
        else:
            self.single_action_space = gymnasium.spaces.Discrete(2)

        super().__init__(buf)
        self.actions = np.zeros(num_envs, dtype=np.float32)

        self.c_envs = binding.vec_init(
            self.observations,
            self.actions,
            self.rewards,
            self.terminals,
            self.truncations,
            num_envs,
            seed,
            cart_mass=cart_mass,
            pole_mass=pole_mass,
            pole_length=pole_length,
            gravity=gravity,
            force_mag=force_mag,
            dt=dt,
            continuous=int(self.continuous),
        )
   
    def reset(self, seed=None):
        self.tick = 0      
        if seed is None:
            binding.vec_reset(self.c_envs, 0)
        else:
            binding.vec_reset(self.c_envs, seed)
        return self.observations, []
   
    def step(self, actions):
        if self.continuous:
            self.actions[:] = np.clip(actions.flatten(), -1.0, 1.0)
        else:
            self.actions[:] = actions
        
        self.tick += 1    
        binding.vec_step(self.c_envs)
        
        info = []
        if self.tick % self.report_interval == 0:
            info.append(binding.vec_log(self.c_envs))
        
        return (
            self.observations,
            self.rewards,
            self.terminals,
            self.truncations,
            info
        )
   
    def render(self):
        binding.vec_render(self.c_envs, 0)
   
    def close(self):
        binding.vec_close(self.c_envs)

def test_performance(timeout=10, atn_cache=8192, continuous=True):
    """Benchmark environment performance."""
    num_envs = 4096
    env = Cartpole(num_envs=num_envs, continuous=continuous)
    env.reset()
    tick = 0

    if env.continuous:
        actions = np.random.uniform(-1, 1, (atn_cache, num_envs, 1)).astype(np.float32)
    else:
        actions = np.random.randint(0, env.single_action_space.n, (atn_cache, num_envs)).astype(np.int8)

    import time
    start = time.time()
    while time.time() - start < timeout:
        atn = actions[tick % atn_cache]
        env.step(atn)
        tick += 1
    sps = num_envs * tick / (time.time() - start)
    print(f'SPS: {sps:,}')

if __name__ == '__main__':
    test_performance()
    



================================================
FILE: pufferlib/ocean/chain_mdp/README.md
================================================
The Chain MDP [Osband et al., 2016] is a classic benchmark environment designed to test whether reinforcement learning algorithms are capable of systematic exploration rather than falling into short-sighted local optima. 

The environment is composed of $N$ states arranged in a linear chain. At each state $s_i (1 \leq i \leq N)$ the agent can take one of two actions: left or right. The transitions are deterministic: moving left shifts the agent to $s_{i-1}$, while moving right shifts it to $s_{i+1}$. The two boundary states, $s_1$ and $s_N$, are absorbing, meaning moving left while in state $s_1$ keeps you in that state. 

Rewards are sparse and asymmetric: reaching $s_1$ yields a small reward of 1/1000, while reaching $s_N$ yields a much larger reward of 1; all other states give zero reward. Each episode lasts exactly $N+9$ steps, and the agent always starts at $s_2$. 

This setup creates a strong exploration challenge: the nearby absorbing state $s_1$ provides an easy but suboptimal payoff, while the optimal long-term strategy requires consistently moving right until $s_N$ is reached. Uniformly random exploration is highly inefficient here—on average it takes $2^{N-2}$ steps before the agent reaches $s_N$ — so for large $N$, it is virtually impossible to discover the optimal strategy by chance within a single episode.


================================================
FILE: pufferlib/ocean/chain_mdp/binding.c
================================================
#include "chain_mdp.h"

#define Env Chain
#include "../env_binding.h"

static int my_init(Env* env, PyObject* args, PyObject* kwargs) {
    env->size = unpack(kwargs, "size");
    return 0;
}

static int my_log(PyObject* dict, Log* log) {
    assign_to_dict(dict, "perf", log->perf);
    assign_to_dict(dict, "score", log->score);
    assign_to_dict(dict, "episode_return", log->episode_return);
    assign_to_dict(dict, "episode_length", log->episode_length);
    return 0;
}



================================================
FILE: pufferlib/ocean/chain_mdp/chain_mdp.c
================================================
#include "chain_mdp.h"

int main() {
    Chain env = {.size = 64};
    allocate_chain(&env);

    c_reset(&env);
    c_render(&env);
    while (!WindowShouldClose()) {
        if (IsKeyDown(KEY_LEFT_SHIFT)) {
            env.actions[0] = 0;
            if (IsKeyDown(KEY_LEFT)  || IsKeyDown(KEY_A)) env.actions[0] = LEFT;
            if (IsKeyDown(KEY_RIGHT) || IsKeyDown(KEY_D)) env.actions[0] = RIGHT;
        } else {
            env.actions[0] = rand() % 2;
        }
        c_step(&env);
        c_render(&env);
    }
    free_allocated(&env);
}




================================================
FILE: pufferlib/ocean/chain_mdp/chain_mdp.h
================================================
#include <stdlib.h>
#include <string.h>
#include <stdio.h>
#include "raylib.h"

const unsigned char LEFT = 0;
const unsigned char RIGHT = 1;

const float STATE1_REWARD = 0.001;
const float STATEN_REWARD = 1.0;

#define MAX(a,b) ((a) > (b) ? (a) : (b))
#define MIN(a,b) ((a) < (b) ? (a) : (b))

typedef struct {
    float perf; // Recommended 0-1 normalized single real number perf metric
    float score; // Recommended unnormalized single real number perf metric
    float episode_return; // Recommended metric: sum of agent rewards over episode
    float episode_length; // Recommended metric: number of steps of agent episode
    // Any extra fields you add here may be exported to Python in binding.c
    float n; // Required as the last field 
} Log;

// Required that you have some struct for your env
// Recommended that you name it the same as the env file
typedef struct {
    Log log; // Required field. Env binding code uses this to aggregate logs
    unsigned char* observations; // Required. You can use any obs type, but make sure it matches in Python!
    int* actions; // Required. int* for discrete/multidiscrete, float* for box
    float* rewards; // Required
    unsigned char* terminals; // Required. We don't yet have truncations as standard yet
    int size;
    int tick;
    unsigned char state; 

    Texture2D puffer; 

} Chain;

Chain* allocate_chain(Chain *env) {
    env->observations = calloc(1, sizeof(unsigned char));
    env->actions = calloc(1, sizeof(float));
    env->rewards = calloc(1, sizeof(float));
    env->terminals = calloc(1, sizeof(unsigned char));
    return env;
}

void free_allocated(Chain* env) {
    free(env->observations);
    free(env->actions);
    free(env->rewards);
    free(env->terminals);
    free(env);
}

void add_log(Chain* env) {
    env->log.perf += (env->rewards[0] == STATEN_REWARD) ? 1 : 0;
    env->log.score += env->rewards[0];
    env->log.episode_length += env->tick;
    env->log.episode_return += env->rewards[0];
    env->log.n++;
}

// Required function
void c_reset(Chain* env) {
    env->observations[0] = 1;
    env->state = 1;
    env->tick = 0;
}

// Required function
void c_step(Chain* env) {
    env->tick += 1;

    // Clear previous buffers
    env->terminals[0] = 0;
    env->rewards[0] = 0;

    int action = env->actions[0];
    action = action*2 -1; // Map 0,1 to -1,1
    env->state = MIN(MAX(env->state + action, 0), env->size -1);
    env->observations[0] = env->state;

    // States 0 and N are absorbing and rewarding
    if (env->state == 0) {
        env->rewards[0] = STATE1_REWARD;
    } else if (env->state == env->size -1) {
        env->rewards[0] = STATEN_REWARD;
    }

    // Episode is over at N+9 steps
    if (env->tick == env->size + 9) {
        env->terminals[0] = 1;
        add_log(env);
        c_reset(env);
        return;
    }
}

// Required function. Should handle creating the client on first call
void c_render(Chain* env) {
    int px = MAX(8, 1024.0/env->size);

    if (!IsWindowReady()) {
        InitWindow(px*env->size, px*5, "PufferLib Chain MDP");
        SetTargetFPS(4);
        env->puffer = LoadTexture("resources/shared/puffers_128.png");
    }

    // Standard across our envs so exiting is always the same
    if (IsKeyDown(KEY_ESCAPE)) {
        exit(0);
    }

    BeginDrawing();
    ClearBackground((Color){6, 24, 24, 255});

    int agent_pos = env->observations[0];
    for (int i = 0; i < env->size; i++) {
        Color color = 
            (i == agent_pos) ? (Color){0, 255, 255, 255} :
            (i == 0) ? (Color){204, 204, 0, 255} : 
            (i == env->size-1) ? (Color){255, 255, 51, 255} :
            (Color){224, 224, 224, 255};

        if (i == agent_pos) {
            int starting_sprite_x = 0;
            float rotation = env->actions[0];
            if (rotation == -1) {
            starting_sprite_x = 128;
            rotation = 0;
            }
            Rectangle source_rect = (Rectangle){starting_sprite_x, 0, 128, 128};
            Rectangle dest_rect = (Rectangle){i*px, 2*px, px, px};        
            DrawTexturePro(env->puffer, source_rect, dest_rect,
                        (Vector2){0, 0}, 0, color);
        } else {
            DrawRectangle(i*px, 2*px, px, px, color);
        }

    }

    EndDrawing();
}

// Required function. Should clean up anything you allocated
// Do not free env->observations, actions, rewards, terminals
void c_close(Chain* env) {
    if (IsWindowReady()) {
        CloseWindow();
    }
}



================================================
FILE: pufferlib/ocean/chain_mdp/chain_mdp.py
================================================
'''A simple sample environment. Use this as a template for your own envs.'''

import gymnasium
import numpy as np

import pufferlib
from pufferlib.ocean.chain_mdp import binding

class Chain(pufferlib.PufferEnv):
    def __init__(self, num_envs=1, render_mode=None, log_interval=128, size=11, buf=None, seed=0):
        self.single_observation_space = gymnasium.spaces.Box(low=0, high=1,
            shape=(1,), dtype=np.uint8)
        self.single_action_space = gymnasium.spaces.Discrete(2)
        self.render_mode = render_mode
        self.num_agents = num_envs
        self.log_interval = log_interval

        super().__init__(buf)
        self.c_envs = binding.vec_init(self.observations, self.actions, self.rewards,
            self.terminals, self.truncations, num_envs, seed, size=size)
 
    def reset(self, seed=0):
        binding.vec_reset(self.c_envs, seed)
        self.tick = 0
        return self.observations, []

    def step(self, actions):
        self.tick += 1

        self.actions[:] = actions
        binding.vec_step(self.c_envs)

        info = []
        if self.tick % self.log_interval == 0:
            info.append(binding.vec_log(self.c_envs))

        return (self.observations, self.rewards,
            self.terminals, self.truncations, info)

    def render(self):
        binding.vec_render(self.c_envs, 0)

    def close(self):
        binding.vec_close(self.c_envs)

if __name__ == '__main__':
    size = 10

    env = Chain(size=size)
    env.reset()
    steps = 0

    CACHE = 1024
    actions = np.random.randint(0, 2, (CACHE,))

    i = 0
    import time
    start = time.time()
    while time.time() - start < 10:
        env.step(actions[i % CACHE])
        steps += 1
        i += 1

    print('Chain MDP SPS:', int(steps / (time.time() - start)))



================================================
FILE: pufferlib/ocean/checkers/binding.c
================================================
#include "checkers.h"

#define Env Checkers
#include "../env_binding.h"

static int my_init(Env *env, PyObject *args, PyObject *kwargs) {
  env->size = unpack(kwargs, "size");
  return 0;
}

static int my_log(PyObject *dict, Log *log) {
  assign_to_dict(dict, "perf", log->perf);
  assign_to_dict(dict, "score", log->score);
  assign_to_dict(dict, "episode_return", log->episode_return);
  assign_to_dict(dict, "episode_length", log->episode_length);
  assign_to_dict(dict, "winrate", log->winrate);
  return 0;
}



================================================
FILE: pufferlib/ocean/checkers/checkers.c
================================================
#include "checkers.h"

int main() {
  Checkers env = {.size = 8};
  env.observations =
      (unsigned char *)calloc(env.size * env.size, sizeof(unsigned char));
  env.actions = (int *)calloc(1, sizeof(int));
  env.rewards = (float *)calloc(1, sizeof(float));
  env.terminals = (unsigned char *)calloc(1, sizeof(unsigned char));

  c_reset(&env);
  c_render(&env);
  while (!WindowShouldClose()) {
    if (IsKeyDown(KEY_LEFT_SHIFT)) {
      env.actions[0] = 0;
      if (IsKeyDown(KEY_UP) || IsKeyDown(KEY_W))
        env.actions[0] = 1;
      if (IsKeyDown(KEY_DOWN) || IsKeyDown(KEY_S))
        env.actions[0] = 2;
      if (IsKeyDown(KEY_LEFT) || IsKeyDown(KEY_A))
        env.actions[0] = 3;
      if (IsKeyDown(KEY_RIGHT) || IsKeyDown(KEY_D))
        env.actions[0] = 4;
    } else {
      env.actions[0] = rand() % 5;
    }
    c_step(&env);
    c_render(&env);
  }
  free(env.observations);
  free(env.actions);
  free(env.rewards);
  free(env.terminals);
  c_close(&env);
}



================================================
FILE: pufferlib/ocean/checkers/checkers.h
================================================
#pragma once

#include "raylib.h"
#include <math.h>
#include <stdio.h>
#include <stdlib.h>
#include <string.h>
#include <time.h>

#define EMPTY 0
#define AGENT 1
#define OPPONENT 3
#define AGENT_PAWN 1
#define AGENT_KING 2
#define OPPONENT_PAWN 3
#define OPPONENT_KING 4

// Required struct. Only use floats!
typedef struct {
  float perf;
  float score;
  float episode_return;
  float episode_length;
  float winrate;
  float n;
} Log;

// Required that you have some struct for your env
// Recommended that you name it the same as the env file
typedef struct {
  Log log;
  unsigned char *observations;
  int *actions;
  float *rewards;
  unsigned char *terminals;
  int size;
  int tick;
  int current_player;
  int agent_pieces;
  int opponent_pieces;
  int capture_available_cache;
  int capture_available_valid;
  int game_over_cache;
  int game_over_valid;
} Checkers;

typedef struct {
  int r;
  int c;
} Position;

typedef struct {
  Position from;
  Position to;
} Move;

float clamp(float val, float low, float high) {
  return fmin(fmax(val, low), high);
}

Move decode_action(Checkers *env, int action) {
  int num_move_types = 8;
  int pos = action / num_move_types;
  int move_type = action % num_move_types;

  Move m;
  m.from.r = pos / env->size;
  m.from.c = pos % env->size;
  m.to.r = m.from.r;
  m.to.c = m.from.c;

  switch (move_type) {
  case 0:
    m.to.r = m.from.r - 1;
    m.to.c = m.from.c - 1;
    break;
  case 1:
    m.to.r = m.from.r - 1;
    m.to.c = m.from.c + 1;
    break;
  case 2:
    m.to.r = m.from.r + 1;
    m.to.c = m.from.c - 1;
    break;
  case 3:
    m.to.r = m.from.r + 1;
    m.to.c = m.from.c + 1;
    break;
  case 4:
    m.to.r = m.from.r - 2;
    m.to.c = m.from.c - 2;
    break;
  case 5:
    m.to.r = m.from.r - 2;
    m.to.c = m.from.c + 2;
    break;
  case 6:
    m.to.r = m.from.r + 2;
    m.to.c = m.from.c - 2;
    break;
  case 7:
    m.to.r = m.from.r + 2;
    m.to.c = m.from.c + 2;
    break;
  }

  return m;
}

int p2i(Checkers *env, Position p) { return p.r * env->size + p.c; }

int check_in_bounds(Checkers *env, Position p) {
  return 0 <= p.r && p.r < env->size && 0 <= p.c && p.c < env->size;
}

int get_piece(Checkers *env, Position p) {
  if (!check_in_bounds(env, p)) {
    return EMPTY;
  }
  return env->observations[p2i(env, p)];
}

int get_piece_type(Checkers *env, Position p) {
  int piece = get_piece(env, p);
  if (piece == AGENT_PAWN || piece == AGENT_KING)
    return AGENT;
  if (piece == OPPONENT_PAWN || piece == OPPONENT_KING)
    return OPPONENT;
  return EMPTY;
}

int get_move_direction(Checkers *env, Move m) {
  return m.to.r > m.from.r ? 1 : -1;
}

int valid_move_direction(Checkers *env, Move m) {
  int piece = get_piece(env, m.from);
  if (piece == AGENT_PAWN)
    return get_move_direction(env, m) == 1 ? 1 : 0;
  if (piece == OPPONENT_PAWN)
    return get_move_direction(env, m) == -1 ? 1 : 0;
  return 1;
}

int is_diagonal_move(Move m) {
  int dr = m.to.r - m.from.r;
  int dc = m.to.c - m.from.c;
  return (dr == dc) || (dr == -dc);
}
int move_size(Move m) { return abs(m.from.r - m.to.r); }

int is_valid_move_no_capture(Checkers *env, Move m) {
  if (!check_in_bounds(env, m.from) || !check_in_bounds(env, m.to))
    return 0;

  if (get_piece_type(env, m.from) != env->current_player)
    return 0;

  if (get_piece(env, m.to) != EMPTY)
    return 0;

  if (!valid_move_direction(env, m))
    return 0;

  if (!is_diagonal_move(m))
    return 0;

  if (move_size(m) != 1 && move_size(m) != 2)
    return 0;

  if (move_size(m) == 2) {
    int other_player = env->current_player == AGENT ? OPPONENT : AGENT;
    Position between_pos =
        (Position){(m.from.r + m.to.r) / 2, (m.from.c + m.to.c) / 2};
    if (get_piece_type(env, between_pos) != other_player)
      return 0;
  }

  return 1;
}

int capture_available(Checkers *env) {
  if (env->capture_available_valid) {
    return env->capture_available_cache;
  }

  int current_pawn = env->current_player == AGENT ? AGENT_PAWN : OPPONENT_PAWN;
  int current_king = env->current_player == AGENT ? AGENT_KING : OPPONENT_KING;

  for (int i = 0; i < env->size * env->size; i++) {
    int piece = env->observations[i];
    if (piece != current_pawn && piece != current_king)
      continue;

    int r = i / env->size;
    int c = i % env->size;

    int directions[4][2] = {{-2, -2}, {-2, 2}, {2, -2}, {2, 2}};
    for (int d = 0; d < 4; d++) {
      int new_r = r + directions[d][0];
      int new_c = c + directions[d][1];

      if (new_r < 0 || new_r >= env->size || new_c < 0 || new_c >= env->size)
        continue;

      if (env->observations[new_r * env->size + new_c] != EMPTY)
        continue;

      int mid_r = r + directions[d][0] / 2;
      int mid_c = c + directions[d][1] / 2;
      int mid_piece = env->observations[mid_r * env->size + mid_c];

      int opponent_pawn =
          env->current_player == AGENT ? OPPONENT_PAWN : AGENT_PAWN;
      int opponent_king =
          env->current_player == AGENT ? OPPONENT_KING : AGENT_KING;

      if (mid_piece == opponent_pawn || mid_piece == opponent_king) {
        int move_dir = directions[d][0] > 0 ? 1 : -1;
        int valid_dir = env->current_player == AGENT ? 1 : -1;
        if (move_dir != valid_dir)
          continue;

        env->capture_available_cache = 1;
        env->capture_available_valid = 1;
        return 1;
      }
    }
  }

  env->capture_available_cache = 0;
  env->capture_available_valid = 1;
  return 0;
}

int is_valid_move(Checkers *env, Move m) {
  if (capture_available(env) && move_size(m) != 2)
    return 0;
  return is_valid_move_no_capture(env, m);
}

int num_legal_moves(Checkers *env) {
  int res = 0;
  int current_pawn = env->current_player == AGENT ? AGENT_PAWN : OPPONENT_PAWN;
  int current_king = env->current_player == AGENT ? AGENT_KING : OPPONENT_KING;
  int has_captures = capture_available(env);

  for (int i = 0; i < env->size * env->size; i++) {
    int piece = env->observations[i];
    if (piece != current_pawn && piece != current_king)
      continue;

    int r = i / env->size;
    int c = i % env->size;

    int directions[8][2] = {{-1, -1}, {-1, 1}, {1, -1}, {1, 1},
                            {-2, -2}, {-2, 2}, {2, -2}, {2, 2}};

    for (int d = 0; d < 8; d++) {
      int new_r = r + directions[d][0];
      int new_c = c + directions[d][1];

      if (new_r < 0 || new_r >= env->size || new_c < 0 || new_c >= env->size)
        continue;

      if (env->observations[new_r * env->size + new_c] != EMPTY)
        continue;

      int move_size = abs(directions[d][0]);

      if (has_captures && move_size != 2)
        continue;

      if (piece == current_pawn) {
        int move_dir = directions[d][0] > 0 ? 1 : -1;
        int valid_dir = env->current_player == AGENT ? 1 : -1;
        if (move_dir != valid_dir)
          continue;
      }

      if (move_size == 2) {
        int mid_r = r + directions[d][0] / 2;
        int mid_c = c + directions[d][1] / 2;
        int mid_piece = env->observations[mid_r * env->size + mid_c];

        int opponent_pawn =
            env->current_player == AGENT ? OPPONENT_PAWN : AGENT_PAWN;
        int opponent_king =
            env->current_player == AGENT ? OPPONENT_KING : AGENT_KING;

        if (mid_piece != opponent_pawn && mid_piece != opponent_king)
          continue;
      }

      res++;
    }
  }

  return res;
}

int num_pieces_by_player(Checkers *env, int player) {
  if (player == AGENT) {
    return env->agent_pieces;
  } else {
    return env->opponent_pieces;
  }
}

int try_make_king(Checkers *env) {
  int promoted = 0;

  for (int i = 0; i < env->size; i++) {
    if (env->observations[i] == OPPONENT_PAWN) {
      env->observations[i] = OPPONENT_KING;
      promoted = 1;
    }
  }
  for (int i = 0; i < env->size; i++) {
    if (env->observations[env->size * (env->size - 1) + i] == AGENT_PAWN) {
      env->observations[env->size * (env->size - 1) + i] = AGENT_KING;
      promoted = 1;
    }
  }

  if (promoted) {
    env->capture_available_valid = 0;
    env->game_over_valid = 0;
  }

  return promoted;
}

int is_game_over(Checkers *env) {
  if (env->game_over_valid) {
    return env->game_over_cache;
  }

  int current_player_pieces = num_pieces_by_player(env, env->current_player);
  int other_player = env->current_player == AGENT ? OPPONENT : AGENT;
  int other_player_pieces = num_pieces_by_player(env, other_player);

  if (current_player_pieces == 0 || other_player_pieces == 0) {
    env->game_over_cache = 1;
    env->game_over_valid = 1;
    return 1;
  }

  int has_captures = capture_available(env);
  if (has_captures) {
    env->game_over_cache = 0;
    env->game_over_valid = 1;
    return 0;
  }

  int current_pawn = env->current_player == AGENT ? AGENT_PAWN : OPPONENT_PAWN;
  int current_king = env->current_player == AGENT ? AGENT_KING : OPPONENT_KING;

  for (int i = 0; i < env->size * env->size; i++) {
    int piece = env->observations[i];
    if (piece != current_pawn && piece != current_king)
      continue;

    int r = i / env->size;
    int c = i % env->size;

    int directions[4][2] = {{-1, -1}, {-1, 1}, {1, -1}, {1, 1}};
    for (int d = 0; d < 4; d++) {
      int new_r = r + directions[d][0];
      int new_c = c + directions[d][1];

      if (new_r < 0 || new_r >= env->size || new_c < 0 || new_c >= env->size)
        continue;
      if (env->observations[new_r * env->size + new_c] != EMPTY)
        continue;

      if (piece == current_pawn) {
        int move_dir = directions[d][0] > 0 ? 1 : -1;
        int valid_dir = env->current_player == AGENT ? 1 : -1;
        if (move_dir != valid_dir)
          continue;
      }

      env->game_over_cache = 0;
      env->game_over_valid = 1;
      return 0;
    }
  }

  env->game_over_cache = 1;
  env->game_over_valid = 1;
  return 1;
}

int get_winner(Checkers *env) {
  if (env->agent_pieces == 0) {
    return OPPONENT;
  }

  if (env->opponent_pieces == 0) {
    return AGENT;
  }

  if (is_game_over(env)) {
    return env->current_player == AGENT ? OPPONENT : AGENT;
  }

  return EMPTY;
}

void make_move(Checkers *env, int action) {
  Move m = decode_action(env, action);
  if (!is_valid_move(env, m)) {
    env->rewards[0] = -1.0f; // reward for invalid move
    return;
  }

  int moving_piece = get_piece(env, m.from);
  env->observations[p2i(env, m.from)] = EMPTY;
  env->observations[p2i(env, m.to)] = moving_piece;

  int capture_occurred = 0;
  float reward = 0.0f;

  if (move_size(m) == 2) {
    Position between_pos =
        (Position){(m.from.r + m.to.r) / 2, (m.from.c + m.to.c) / 2};
    int captured_piece = env->observations[p2i(env, between_pos)];
    env->observations[p2i(env, between_pos)] = EMPTY;
    capture_occurred = 1;

    if (captured_piece == AGENT_PAWN || captured_piece == AGENT_KING) {
      env->agent_pieces--;
      reward -= 0.05f; // reward for losing pieces
    } else if (captured_piece == OPPONENT_PAWN ||
               captured_piece == OPPONENT_KING) {
      env->opponent_pieces--;
    }
  }

  env->capture_available_valid = 0;
  env->game_over_valid = 0;

  int promotion_occurred = try_make_king(env);

  if (capture_occurred && env->current_player == OPPONENT) {
    reward += 0.1f; // reward for capturing
  } else if (env->current_player == AGENT) {
    reward += 0.01f; // reward for successful moves
  }

  if (move_size(m) == 1 || !capture_available(env)) {
    int other_player = env->current_player == AGENT ? OPPONENT : AGENT;
    env->current_player = other_player;
  }

  if (promotion_occurred) {
    for (int i = 0; i < env->size; i++) {
      if (env->observations[env->size * (env->size - 1) + i] == AGENT_KING) {
        reward += 0.05f; // reward for promotion
        break;
      }
    }
  }

  if (is_game_over(env)) {
    env->terminals[0] = 1;
    int winner = get_winner(env);
    reward = winner == AGENT ? 1.0f : -1.0f;
  }

  env->rewards[0] = clamp(reward, -1.0f, 1.0f);
}

void scripted_first_move(Checkers *env) {
  int current_pawn = env->current_player == AGENT ? AGENT_PAWN : OPPONENT_PAWN;
  int current_king = env->current_player == AGENT ? AGENT_KING : OPPONENT_KING;
  int has_captures = capture_available(env);

  for (int i = 0; i < env->size * env->size; i++) {
    int piece = env->observations[i];
    if (piece != current_pawn && piece != current_king)
      continue;

    int r = i / env->size;
    int c = i % env->size;

    int directions[8][2] = {{-1, -1}, {-1, 1}, {1, -1}, {1, 1},
                            {-2, -2}, {-2, 2}, {2, -2}, {2, 2}};

    for (int d = 0; d < 8; d++) {
      int new_r = r + directions[d][0];
      int new_c = c + directions[d][1];

      if (new_r < 0 || new_r >= env->size || new_c < 0 || new_c >= env->size)
        continue;
      if (env->observations[new_r * env->size + new_c] != EMPTY)
        continue;

      int move_size = abs(directions[d][0]);

      if (has_captures && move_size != 2)
        continue;

      if (piece == current_pawn) {
        int move_dir = directions[d][0] > 0 ? 1 : -1;
        int valid_dir = env->current_player == AGENT ? 1 : -1;
        if (move_dir != valid_dir)
          continue;
      }

      if (move_size == 2) {
        int mid_r = r + directions[d][0] / 2;
        int mid_c = c + directions[d][1] / 2;
        int mid_piece = env->observations[mid_r * env->size + mid_c];

        int opponent_pawn =
            env->current_player == AGENT ? OPPONENT_PAWN : AGENT_PAWN;
        int opponent_king =
            env->current_player == AGENT ? OPPONENT_KING : AGENT_KING;

        if (mid_piece != opponent_pawn && mid_piece != opponent_king)
          continue;
      }

      int action = i * 8 + d;
      make_move(env, action);
      return;
    }
  }
}

// Helper function to evaluate position value
float evaluate_position(Checkers *env) {
  float score = 0.0f;

  for (int i = 0; i < env->size * env->size; i++) {
    int piece = env->observations[i];
    int r = i / env->size;

    if (piece == AGENT_PAWN) {
      score += 1.0f + (r * 0.1f); // Pawns are worth more as they advance
    } else if (piece == AGENT_KING) {
      score += 2.0f;
    } else if (piece == OPPONENT_PAWN) {
      score -= 1.0f + ((env->size - 1 - r) * 0.1f);
    } else if (piece == OPPONENT_KING) {
      score -= 2.0f;
    }
  }

  return score;
}

void scripted_random_move(Checkers *env) {
  int current_pawn = env->current_player == AGENT ? AGENT_PAWN : OPPONENT_PAWN;
  int current_king = env->current_player == AGENT ? AGENT_KING : OPPONENT_KING;
  int has_captures = capture_available(env);

  srand(time(NULL));
  int i;
  int num_positions = env->size * env->size;
  while (1) {
    i = random() % num_positions;
    int piece = env->observations[i];
    if (piece != current_pawn && piece != current_king)
      continue;

    int r = i / env->size;
    int c = i % env->size;

    int directions[8][2] = {{-1, -1}, {-1, 1}, {1, -1}, {1, 1},
                            {-2, -2}, {-2, 2}, {2, -2}, {2, 2}};

    for (int d = 0; d < 8; d++) {
      int new_r = r + directions[d][0];
      int new_c = c + directions[d][1];

      if (new_r < 0 || new_r >= env->size || new_c < 0 || new_c >= env->size)
        continue;
      if (env->observations[new_r * env->size + new_c] != EMPTY)
        continue;

      int move_size = abs(directions[d][0]);

      if (has_captures && move_size != 2)
        continue;

      if (piece == current_pawn) {
        int move_dir = directions[d][0] > 0 ? 1 : -1;
        int valid_dir = env->current_player == AGENT ? 1 : -1;
        if (move_dir != valid_dir)
          continue;
      }

      if (move_size == 2) {
        int mid_r = r + directions[d][0] / 2;
        int mid_c = c + directions[d][1] / 2;
        int mid_piece = env->observations[mid_r * env->size + mid_c];

        int opponent_pawn =
            env->current_player == AGENT ? OPPONENT_PAWN : AGENT_PAWN;
        int opponent_king =
            env->current_player == AGENT ? OPPONENT_KING : AGENT_KING;

        if (mid_piece != opponent_pawn && mid_piece != opponent_king)
          continue;
      }

      int action = i * 8 + d;
      make_move(env, action);
      return;
    }
  }
}

void scripted_step(Checkers *env, int difficulty) {
  switch (difficulty) {
  case 0:
    scripted_first_move(env);
    break;
  case 1:
    scripted_random_move(env);
    break;
  default:
    scripted_random_move(env);
    break;
  }
}

void update_piece_counts(Checkers *env) {
  env->agent_pieces = 0;
  env->opponent_pieces = 0;

  for (int i = 0; i < env->size * env->size; i++) {
    int piece = env->observations[i];
    if (piece == AGENT_PAWN || piece == AGENT_KING) {
      env->agent_pieces++;
    } else if (piece == OPPONENT_PAWN || piece == OPPONENT_KING) {
      env->opponent_pieces++;
    }
  }

  env->capture_available_valid = 0;
  env->game_over_valid = 0;
}

void add_log(Checkers *env) {
  env->log.perf += (env->rewards[0] > 0) ? 1 : 0;
  env->log.score += evaluate_position(env);
  env->log.episode_length += env->tick;
  env->log.episode_return += env->rewards[0];
  if (env->terminals[0] == 1)
    env->log.winrate += get_winner(env) == AGENT ? 1.0f : 0.0f;
  env->log.n += 1;
}

// Required function
void c_reset(Checkers *env) {
  env->tick = 0;
  env->terminals[0] = 0;
  env->rewards[0] = 0.0f;

  int tiles = env->size * env->size;
  for (int i = 0; i < tiles; i++)
    env->observations[i] = EMPTY;
  for (int i = 0; i < 3; i++) {
    for (int j = 0; j < env->size; j++) {
      if ((i + j) % 2)
        env->observations[i * env->size + j] = AGENT_PAWN;
    }
  }
  for (int i = env->size - 3; i < env->size; i++) {
    for (int j = 0; j < env->size; j++) {
      if ((i + j) % 2)
        env->observations[i * env->size + j] = OPPONENT_PAWN;
    }
  }

  env->current_player = AGENT;

  update_piece_counts(env);
}

// Required function
void c_step(Checkers *env) {
  env->tick += 1;
  int action = env->actions[0];
  env->rewards[0] = 0.0f;
  env->terminals[0] = 0;

  make_move(env, action);

  env->rewards[0] = clamp(env->rewards[0], -1.0f, 1.0f);
  if (env->terminals[0] == 1) {
    add_log(env);
    c_reset(env);
    return;
  }

  scripted_step(env, 1);
  if (env->terminals[0] == 1) {
    add_log(env);
    c_reset(env);
    return;
  }
}

// Required function. Should handle creating the client on first call
void c_render(Checkers *env) {
  const Color BG1 = (Color){27, 27, 27, 255};
  const Color BG2 = (Color){13, 13, 13, 255};

  int cell_size = 64;
  int window_width = cell_size * env->size;
  int window_height = cell_size * env->size;
  int radius = cell_size / 3;
  int king_offset = 14;

  if (!IsWindowReady()) {
    SetConfigFlags(FLAG_MSAA_4X_HINT);
    InitWindow(window_width, window_height, "Puffer Checkers");
    SetTargetFPS(30);
  } else if (GetScreenWidth() != window_width ||
             GetScreenHeight() != window_height) {
    SetWindowSize(window_width, window_height);
  }

  if (IsKeyDown(KEY_ESCAPE)) {
    CloseWindow();
    exit(0);
    return;
  }

  BeginDrawing();
  ClearBackground(BG1);

  Color piece_color;
  for (int i = 0; i < env->size; i++) {
    for (int j = 0; j < env->size; j++) {
      int piece = env->observations[i * env->size + j];
      if ((i + j) % 2 == 0)
        DrawRectangle(j * cell_size - 1, i * cell_size - 1, cell_size + 1,
                      cell_size + 1, BG2);
      if (piece == EMPTY)
        continue;

      int center_x = j * cell_size + cell_size / 2;
      int center_y = i * cell_size + cell_size / 2;

      switch (piece) {
      case AGENT_PAWN:
        piece_color = BLUE;
        DrawCircle(center_x, center_y, radius, piece_color);
        DrawCircleGradient(center_x - radius / 3, center_y - radius / 3,
                           radius / 3, (Color){255, 255, 255, 80},
                           (Color){255, 255, 255, 10});
        DrawCircleGradient(center_x, center_y, radius,
                           (Color){255, 255, 255, 50},
                           (Color){255, 255, 255, 5});
        break;

      case AGENT_KING:
        piece_color = BLUE;
        DrawCircle(center_x, center_y, radius, piece_color);
        DrawCircleGradient(center_x, center_y, radius,
                           (Color){255, 255, 255, 50},
                           (Color){255, 255, 255, 5});

        DrawCircleGradient(center_x, center_y - king_offset / 2, radius,
                           (Color){20, 20, 20, 60}, (Color){20, 20, 20, 30});
        DrawCircle(center_x, center_y - king_offset, radius, piece_color);
        DrawCircleGradient(
            center_x - radius / 3, center_y - radius / 3 - king_offset,
            radius / 3, (Color){255, 255, 255, 80}, (Color){255, 255, 255, 10});
        DrawCircleGradient(center_x, center_y - king_offset, radius,
                           (Color){255, 255, 255, 50},
                           (Color){255, 255, 255, 5});
        break;

      case OPPONENT_PAWN:
        piece_color = RED;
        DrawCircle(center_x, center_y, radius, piece_color);
        DrawCircleGradient(center_x - radius / 3, center_y - radius / 3,
                           radius / 3, (Color){255, 255, 255, 80},
                           (Color){255, 255, 255, 10});
        DrawCircleGradient(center_x, center_y, radius,
                           (Color){255, 255, 255, 50},
                           (Color){255, 255, 255, 5});
        break;

      case OPPONENT_KING:
        piece_color = RED;
        DrawCircle(center_x, center_y, radius, piece_color);
        DrawCircleGradient(center_x, center_y, radius,
                           (Color){255, 255, 255, 50},
                           (Color){255, 255, 255, 5});

        DrawCircleGradient(center_x, center_y - king_offset / 2, radius,
                           (Color){20, 20, 20, 60}, (Color){20, 20, 20, 30});
        DrawCircle(center_x, center_y - king_offset, radius, piece_color);
        DrawCircleGradient(
            center_x - radius / 3, center_y - radius / 3 - king_offset,
            radius / 3, (Color){255, 255, 255, 80}, (Color){255, 255, 255, 10});
        DrawCircleGradient(center_x, center_y - king_offset, radius,
                           (Color){255, 255, 255, 50},
                           (Color){255, 255, 255, 5});
        break;

      default:
        break;
      }
    }
  }

  EndDrawing();
}

// Required function. Should clean up anything you allocated
// Do not free env->observations, actions, rewards, terminals
void c_close(Checkers *env) {
  if (IsWindowReady()) {
    CloseWindow();
  }
}



================================================
FILE: pufferlib/ocean/checkers/checkers.py
================================================
import gymnasium
import numpy as np

import pufferlib
from pufferlib.ocean.checkers import binding

class Checkers(pufferlib.PufferEnv):
    def __init__(self, num_envs=1, render_mode=None, log_interval=128, size=8, buf=None, seed=0):
        self.single_observation_space = gymnasium.spaces.Box(low=0, high=1,
            shape=(size*size,), dtype=np.uint8)
        num_move_types = 8  # Move types are: NW, NE, SW, SE, 2*NW, 2*NE, 2*SW, 2*SE,
        action_space_size = size * size * num_move_types
        self.single_action_space = gymnasium.spaces.Discrete(action_space_size)
        self.render_mode = render_mode
        self.num_agents = num_envs
        self.log_interval = log_interval

        super().__init__(buf)
        self.c_envs = binding.vec_init(self.observations, self.actions, self.rewards,
            self.terminals, self.truncations, num_envs, seed, size=size)
 
    def reset(self, seed=0):
        binding.vec_reset(self.c_envs, seed)
        self.tick = 0
        return self.observations, []

    def step(self, actions):
        self.tick += 1

        self.actions[:] = actions
        binding.vec_step(self.c_envs)

        info = []
        if self.tick % self.log_interval == 0:
            info.append(binding.vec_log(self.c_envs))

        return (self.observations, self.rewards,
            self.terminals, self.truncations, info)

    def render(self):
        binding.vec_render(self.c_envs, 0)

    def close(self):
        binding.vec_close(self.c_envs)

if __name__ == '__main__':
    N = 4096
    size = 3

    env = Checkers(num_envs=N, size=size)
    env.reset()
    steps = 0

    CACHE = 1024
    actions = np.random.randint(0, size * size * 6, (CACHE, N))

    i = 0
    import time
    start = time.time()
    while time.time() - start < 10:
        env.step(actions[i % CACHE])
        steps += N
        i += 1

    print('Checkers SPS:', int(steps / (time.time() - start)))



================================================
FILE: pufferlib/ocean/connect4/binding.c
================================================
#include "connect4.h"
#define Env CConnect4
#include "../env_binding.h"

static int my_init(Env* env, PyObject* args, PyObject* kwargs) {
    init(env);
    return 0;
}

static int my_log(PyObject* dict, Log* log) {
    assign_to_dict(dict, "perf", log->perf);
    assign_to_dict(dict, "score", log->score);
    assign_to_dict(dict, "episode_return", log->episode_return);
    assign_to_dict(dict, "episode_length", log->episode_length);
    assign_to_dict(dict, "n", log->n);
    return 0;
}



================================================
FILE: pufferlib/ocean/connect4/connect4.c
================================================
#include "connect4.h"
#include "puffernet.h"
#include "time.h"

const unsigned char NOOP = 8;

void interactive() {
    Weights* weights = load_weights("resources/connect4/connect4_weights.bin", 138632);
    int logit_sizes[] = {7};
    LinearLSTM* net = make_linearlstm(weights, 1, 42, logit_sizes, 1);

    CConnect4 env = {
    };
    allocate_cconnect4(&env);
    c_reset(&env);
 
    env.client = make_client();
    float observations[42] = {0};
    int actions[1] = {0};

    int tick = 0;
    while (!WindowShouldClose()) {
        env.actions[0] = NOOP;
        // user inputs 1 - 7 key pressed
        if (IsKeyDown(KEY_LEFT_SHIFT)) {
            if(IsKeyPressed(KEY_ONE)) env.actions[0] = 0;
            if(IsKeyPressed(KEY_TWO)) env.actions[0] = 1;
            if(IsKeyPressed(KEY_THREE)) env.actions[0] = 2;
            if(IsKeyPressed(KEY_FOUR)) env.actions[0] = 3;
            if(IsKeyPressed(KEY_FIVE)) env.actions[0] = 4;
            if(IsKeyPressed(KEY_SIX)) env.actions[0] = 5;
            if(IsKeyPressed(KEY_SEVEN)) env.actions[0] = 6;
        } else if (tick % 30 == 0) {
            for (int i = 0; i < 42; i++) {
                observations[i] = env.observations[i];
            }
            forward_linearlstm(net, (float*)&observations, (int*)&actions);
            env.actions[0] = actions[0];
        }

        tick = (tick + 1) % 60;
        if (env.actions[0] >= 0 && env.actions[0] <= 6) {
            c_step(&env);
        }

        c_render(&env);
    }
    free_linearlstm(net);
    free(weights);
    close_client(env.client);
    free_allocated_cconnect4(&env);
}

void performance_test() {
    long test_time = 10;
    CConnect4 env = {
    };
    allocate_cconnect4(&env);
    c_reset(&env);
 
    long start = time(NULL);
    int i = 0;
    while (time(NULL) - start < test_time) {
        env.actions[0] = rand() % 7;
        c_step(&env);
        i++;
    }
    long end = time(NULL);
    printf("SPS: %ld\n", i / (end - start));
    free_allocated_cconnect4(&env);
}

int main() {
    // performance_test();
    interactive();
    return 0;
}



================================================
FILE: pufferlib/ocean/connect4/connect4.h
================================================
#include <stdlib.h>
#include <math.h>
#include <stdio.h>
#include <stdint.h>
#include "raylib.h"

#define WIN_CONDITION 4
const int PLAYER_WIN = 1.0;
const int ENV_WIN = -1.0;
const unsigned char DONE = 1;
const unsigned char NOT_DONE = 0;
const int ROWS = 6;
const int COLUMNS = 7;
const int WIDTH = 672;
const int HEIGHT = 576;
const int PIECE_WIDTH = 96;
const int PIECE_HEIGHT = 96;

const float MAX_VALUE = 31;
const float WIN_VALUE = 30;
const float DRAW_VALUE = 0;

typedef struct Log Log;
struct Log {
    float perf;
    float score;
    float episode_return;
    float episode_length;
    float n;
};

typedef struct Client Client;
typedef struct CConnect4 CConnect4;
struct CConnect4 {
    // Pufferlib inputs / outputs
    float* observations;
    int* actions;
    float* rewards;
    unsigned char* terminals;
    Log log;
    Client* client;

    // Bit string representation from:
    //  https://towardsdatascience.com/creating-the-perfect-connect-four-ai-bot-c165115557b0
    //  & http://blog.gamesolver.org/solving-connect-four/01-introduction/
    uint64_t player_pieces;
    uint64_t env_pieces;

    int tick;
};

void allocate_cconnect4(CConnect4* env) {
    env->observations = (float*)calloc(42, sizeof(float));
    env->actions = (int*)calloc(1, sizeof(int));
    env->terminals = (unsigned char*)calloc(1, sizeof(unsigned char));
    env->rewards = (float*)calloc(1, sizeof(float));
}

void free_allocated_cconnect4(CConnect4* env) {
    free(env->actions);
    free(env->observations);
    free(env->terminals);
    free(env->rewards);
}

void c_close(CConnect4* env) {
}

void add_log(CConnect4* env) {
    env->log.perf += (float)(env->rewards[0] == PLAYER_WIN);
    env->log.score += env->rewards[0];
    env->log.episode_return += env->rewards[0];
    env->log.episode_length += env->log.episode_length;
    env->log.n += 1;
}

void init(CConnect4* env) {
    env->log = (Log){0};
    env->tick = 0;
}

// Get the bit at the top of 'column'. Column can be played if bit is 0
uint64_t top_mask(uint64_t column) {
    return (UINT64_C(1) << (ROWS - 1)) << column * (ROWS + 1);
}

// Get a bit mask for where a piece played at 'column' would end up.
uint64_t bottom_mask(uint64_t column) {
    return UINT64_C(1) << column * (ROWS + 1);
}

// A bit mask used to create unique representation of the game state.
uint64_t c_bottom() {
    return UINT64_C(1) << (COLUMNS - 1) * (ROWS + 1);
}

bool invalid_move(int column, uint64_t mask) {
    return (mask & top_mask(column)) != 0;
}

uint64_t play(int column, uint64_t mask,  uint64_t other_pieces) {
    mask |= mask + bottom_mask(column); // Somehow faster than |= bottom_mask(column)
    return other_pieces ^ mask;
}

// A full board has this specifc value
bool draw(uint64_t mask) {
    return mask == 4432406249472;
}

// Determine if 'pieces' contains at least one line of connected pieces.
bool won(uint64_t pieces) {
    // Horizontal 
    uint64_t m = pieces & (pieces >> (ROWS + 1));
    if(m & (m >> (2 * (ROWS + 1)))) {
        return true;
    }

    // Diagonal 1
    m = pieces & (pieces >> ROWS);
    if(m & (m >> (2 * ROWS))) {
        return true;
    }

    // Diagonal 2 
    m = pieces & (pieces >> (ROWS + 2));
    if(m & (m >> (2 * (ROWS + 2)))) {
        return true;
    }

    // Vertical;
    m = pieces & (pieces >> 1);
    if(m & (m >> 2)) {
        return true;
    }

    return false;
}

// https://en.wikipedia.org/wiki/Negamax#Negamax_variant_with_no_color_parameter
float negamax(uint64_t pieces, uint64_t other_pieces, int depth) {
    uint64_t piece_mask = pieces | other_pieces;
    if (won(other_pieces)) {
        return pow(10, depth);
    }
    if (won(pieces)) {
        return 0;
    }

    if (depth == 0 || draw(piece_mask)) {
        return 0;
    }

    float value = 0;
    for (uint64_t column = 0; column < 7; column ++) {
        if (invalid_move(column, piece_mask)) {
            continue;
        }
        uint64_t child_pieces = play(column, piece_mask, other_pieces);
        value -= negamax(other_pieces, child_pieces, depth - 1);
    }
    return value;
}

int compute_env_move(CConnect4* env) {
    uint64_t piece_mask = env->player_pieces | env->env_pieces;
    uint64_t hash = env->player_pieces + piece_mask + c_bottom();

    // Hard coded opening book to handle some early game traps
    // TODO: Add more opening book moves
    switch (hash) {
        case 4398050705408:
            // Respond to _ _ _ o _ _ _
            // with       _ _ x o _ _ _
            return 2;
        case 4398583382016:
            // Respond to _ _ _ _ o _ _
            // with       _ _ _ x o _ _
            return 3;
    }

    float best_value = 9999;
    float values[7];
    for (int i = 0; i < 7; i++) {
        values[i] = 9999;
    }
    for (uint64_t column = 0; column < 7; column ++) {
        if (invalid_move(column, piece_mask)) {
            continue;
        }
        uint64_t child_env_pieces = play(column, piece_mask, env->player_pieces);
        if (won(child_env_pieces)) {
            return column;
        }
        float val = -negamax(env->player_pieces, child_env_pieces, 3);
        values[column] = val;
        if (val < best_value) {
            best_value = val;
        }
    }
    int num_ties = 0;
    for (uint64_t column = 0; column < 7; column ++) {
        if (values[column] == best_value) {
            num_ties++;
        }
    }
    //printf("Values: %f, %f, %f, %f, %f, %f, %f\n", values[0], values[1], values[2], values[3], values[4], values[5], values[6]);
    int best_tie = rand() % num_ties;
    for (uint64_t column = 0; column < 7; column ++) {
        if (values[column] == best_value) {
            if (best_tie == 0) {
                return column;

            }
            best_tie--;
        }
    }
    return 0;
}

void compute_observation(CConnect4* env) {
    // Populate observations from bitstring game representation
    // http://blog.gamesolver.org/solving-connect-four/06-bitboard/
    uint64_t player_pieces = env->player_pieces;
    uint64_t env_pieces = env->env_pieces;

    int obs_idx = 0;
    for (int i = 0; i < 49; i++) {
        // Skip the sentinel row
        if ((i + 1) % 7 == 0) {
            continue;
        }

        int p0_bit = (player_pieces >> i) & 1;
        if (p0_bit == 1) {
            env->observations[obs_idx] = PLAYER_WIN;
        }
        int p1_bit = (env_pieces >> i) & 1;
        if (p1_bit == 1) {
            env->observations[obs_idx] = ENV_WIN;
        }
        obs_idx += 1;
    }
}

void c_reset(CConnect4* env) {
    env->log = (Log){0};
    env->terminals[0] = NOT_DONE;
    env->player_pieces = 0;
    env->env_pieces = 0;
    for (int i = 0; i < 42; i ++) {
        env->observations[i] = 0.0;
    }
}

void finish_game(CConnect4* env, float reward) {
    env->rewards[0] = reward;
    env->terminals[0] = DONE;
    add_log(env);
    compute_observation(env);
}

void c_step(CConnect4* env) {
    env->log.episode_length += 1;
    env->rewards[0] = 0.0;

    if (env->terminals[0] == DONE) {
        c_reset(env);
        return;
    }

    // Player action (PLAYER_WIN)
    uint64_t column = env->actions[0];
    uint64_t piece_mask = env->player_pieces | env->env_pieces;
    if (invalid_move(column, piece_mask)) {
        finish_game(env, ENV_WIN);
        return;
    }

    env->player_pieces = play(column, piece_mask, env->env_pieces);
    if (won(env->player_pieces)) {
        finish_game(env, PLAYER_WIN);
        return;
    }

    // Environment action (ENV_WIN)
    column = compute_env_move(env);
    piece_mask = env->player_pieces | env->env_pieces;
    if (invalid_move(column, piece_mask)) {
        finish_game(env, PLAYER_WIN);
        return;
    }

    env->env_pieces = play(column, piece_mask, env->player_pieces);
    if (won(env->env_pieces)) {
        finish_game(env, ENV_WIN);
        return;
    }

    compute_observation(env);
}

const Color PUFF_RED = (Color){187, 0, 0, 255};
const Color PUFF_CYAN = (Color){0, 187, 187, 255};
const Color PUFF_WHITE = (Color){241, 241, 241, 241};
const Color PUFF_BACKGROUND = (Color){6, 24, 24, 255};

typedef struct Client Client;
struct Client {
    float width;
    float height;
    Texture2D puffers;
};

Client* make_client() {
    Client* client = (Client*)calloc(1, sizeof(Client));
    client->width = WIDTH;
    client->height = HEIGHT;

    InitWindow(WIDTH, HEIGHT, "PufferLib Ray Connect4");
    SetTargetFPS(60);

    client->puffers = LoadTexture("resources/shared/puffers_128.png");
    return client;
}

void c_render(CConnect4* env) {
    if (IsKeyDown(KEY_ESCAPE)) {
        exit(0);
    }

    if (env->client == NULL) {
        env->client = make_client();
    }

    Client* client = env->client;

    BeginDrawing();
    ClearBackground(PUFF_BACKGROUND);
    
    int y_offset = client->height - PIECE_HEIGHT;
    int obs_idx = 0;
    for (int i = 0; i < 49; i++) {
        // TODO: Simplify this by iterating over the observation more directly
        if ((i + 1) % 7 == 0) {
            continue;
        }

        int row = i % (ROWS + 1);
        int column = i / (ROWS + 1);
        int y = y_offset - row * PIECE_HEIGHT;
        int x = column * PIECE_WIDTH;

        Color piece_color=PURPLE;
        int color_idx = 0;
        if (env->observations[obs_idx] == 0.0) {
            piece_color = BLACK;
        } else if (env->observations[obs_idx]  == PLAYER_WIN) {
            piece_color = PUFF_CYAN;
            color_idx = 1;
        } else if (env->observations[obs_idx]  == ENV_WIN) {
            piece_color = PUFF_RED;
            color_idx = 2;
        }

        obs_idx += 1;
        Color board_color = (Color){0, 80, 80, 255};
        DrawRectangle(x , y , PIECE_WIDTH, PIECE_WIDTH, board_color);
        DrawCircle(x + PIECE_WIDTH/2, y + PIECE_WIDTH/2, PIECE_WIDTH/2, piece_color);
        if (color_idx == 0) {
            continue;
        }

        DrawTexturePro(
            client->puffers,
            (Rectangle){
                (color_idx == 1) ? 0 : 128,
                0, 128, 128,
            },
            (Rectangle){x+16, y+16, PIECE_WIDTH-32, PIECE_WIDTH-32},
            (Vector2){0, 0},
            0,
            WHITE
        );
    }
    EndDrawing();
}

void close_client(Client* client) {
    CloseWindow();
    free(client);
}



================================================
FILE: pufferlib/ocean/connect4/connect4.py
================================================
'''High-perf Pong

Inspired from https://gist.github.com/Yttrmin/18ecc3d2d68b407b4be1
& https://jair.org/index.php/jair/article/view/10819/25823
& https://www.youtube.com/watch?v=PSQt5KGv7Vk
'''

import numpy as np
import gymnasium

import pufferlib
from pufferlib.ocean.connect4 import binding


class Connect4(pufferlib.PufferEnv):
    def __init__(self, num_envs=1, render_mode=None, report_interval=128,
             buf=None, seed=0):

        self.single_observation_space = gymnasium.spaces.Box(low=0, high=1,
            shape=(42,), dtype=np.float32)
        self.single_action_space = gymnasium.spaces.Discrete(7)
        self.report_interval = report_interval
        self.render_mode = render_mode
        self.num_agents = num_envs

        super().__init__(buf=buf)
        self.c_envs = binding.vec_init(self.observations, self.actions, self.rewards,
            self.terminals, self.truncations, num_envs, seed)

    def reset(self, seed=None):
        self.tick = 0
        if seed is None:
            binding.vec_reset(self.c_envs, 0)
        else:
            binding.vec_reset(self.c_envs, seed)
        return self.observations, []

    def step(self, actions):
        self.actions[:] = actions
        binding.vec_step(self.c_envs)
        self.tick += 1

        info = []
        if self.tick % self.report_interval == 0:
            log = binding.vec_log(self.c_envs)
            if log['episode_length'] > 0:
                info.append(log)

        return (self.observations, self.rewards,
            self.terminals, self.truncations, info)

    def render(self):
        binding.vec_render(self.c_envs, 0)

    def close(self):
        binding.vec_close(self.c_envs)


def test_performance(timeout=10, atn_cache=1024, num_envs=1024):
    import time

    env = Connect4(num_envs=num_envs)
    env.reset()
    tick = 0

    actions = np.random.randint(
        0,
        env.single_action_space.n + 1,
        (atn_cache, num_envs),
    )

    start = time.time()
    while time.time() - start < timeout:
        atn = actions[tick % atn_cache]         
        env.step(atn)
        tick += 1

    print(f'SPS: {num_envs * tick / (time.time() - start)}')


if __name__ == '__main__':
    test_performance()



================================================
FILE: pufferlib/ocean/convert/binding.c
================================================
#include "convert.h"

#define Env Convert
#include "../env_binding.h"

static int my_init(Env* env, PyObject* args, PyObject* kwargs) {
    env->width = unpack(kwargs, "width");
    env->height = unpack(kwargs, "height");
    env->num_agents = unpack(kwargs, "num_agents");
    env->num_factories = unpack(kwargs, "num_factories");
    env->num_resources = unpack(kwargs, "num_resources");
    init(env);
    return 0;
}

static int my_log(PyObject* dict, Log* log) {
    assign_to_dict(dict, "perf", log->perf);
    assign_to_dict(dict, "score", log->score);
    assign_to_dict(dict, "episode_return", log->episode_return);
    assign_to_dict(dict, "episode_length", log->episode_length);
    return 0;
}



================================================
FILE: pufferlib/ocean/convert/convert.c
================================================
#include "convert.h"
#include "puffernet.h"

int main() {
    Convert env = {
        .width = 1920,
        .height = 1080,
        .num_agents = 1024,
        .num_factories = 32,
        .num_resources = 8,
    };
    init(&env);

    int num_obs = 2*env.num_resources + 4 + env.num_resources;
    env.observations = calloc(env.num_agents*num_obs, sizeof(float));
    env.actions = calloc(2*env.num_agents, sizeof(int));
    env.rewards = calloc(env.num_agents, sizeof(float));
    env.terminals = calloc(env.num_agents, sizeof(unsigned char));

    Weights* weights = load_weights("resources/convert/convert_weights.bin", 137743);
    int logit_sizes[2] = {9, 5};
    LinearLSTM* net = make_linearlstm(weights, env.num_agents, num_obs, logit_sizes, 2);

    c_reset(&env);
    c_render(&env);

    while (!WindowShouldClose()) {
        for (int i=0; i<env.num_agents; i++) {
            env.actions[2*i] = rand() % 9;
            env.actions[2*i + 1] = rand() % 5;
        }

        forward_linearlstm(net, env.observations, env.actions);
        compute_observations(&env);
        c_step(&env);
        c_render(&env);
    }

    free_linearlstm(net);
    free(env.observations);
    free(env.actions);
    free(env.rewards);
    free(env.terminals);
    c_close(&env);
}




================================================
FILE: pufferlib/ocean/convert/convert.h
================================================
/* Convert: a sample multiagent env about puffers eating stars.
 * Use this as a tutorial and template for your own multiagent envs.
 * We suggest starting with the Squared env for a simpler intro.
 * Star PufferLib on GitHub to support. It really, really helps!
 */

#include <stdlib.h>
#include <string.h>
#include <math.h>
#include "raylib.h"

typedef struct {
    float perf;
    float score;
    float episode_return;
    float episode_length;
    float n;
} Log;

typedef struct {
    Texture2D sprites;
} Client;

typedef struct {
    float x;
    float y;
    float heading;
    float speed;
    int item;
    int episode_length;
} Agent;

typedef struct {
    float x;
    float y;
    float heading;
    int item;
} Factory;

typedef struct {
    Log log;
    Client* client;
    Agent* agents;
    Factory* factories;
    float* observations;
    int* actions;
    float* rewards;
    unsigned char* terminals;
    int width;
    int height;
    int num_agents;
    int num_factories;
    int num_resources;
} Convert;

void init(Convert* env) {
    env->agents = calloc(env->num_agents, sizeof(Agent));
    env->factories = calloc(env->num_factories, sizeof(Factory));
}

int compare_floats(const void* a, const void* b) {
    return (*(float*)a - *(float*)b) > 0;
}

void compute_observations(Convert* env) {
    int obs_idx = 0;
    for (int a=0; a<env->num_agents; a++) {
        Agent* agent = &env->agents[a];
        float dists[env->num_resources];
        for (int i=0; i<env->num_resources; i++) {
            dists[i] = 999999;
        }
        for (int f=0; f<env->num_factories; f++) {
            Factory* factory = &env->factories[f];
            float dx = factory->x - agent->x;
            float dy = factory->y - agent->y;
            float dd = dx*dx + dy*dy;
            int type = f % env->num_resources;
            if (dd < dists[type]) {
                dists[type] = dd;
                env->observations[obs_idx + 2*type] = dx/env->width;
                env->observations[obs_idx + 2*type + 1] = dy/env->height;
            }
        }
        obs_idx += 2*env->num_resources;
        env->observations[obs_idx++] = agent->heading/(2*PI);
        env->observations[obs_idx++] = env->rewards[a];
        env->observations[obs_idx++] = agent->x/env->width;
        env->observations[obs_idx++] = agent->y/env->height;
        memset(&env->observations[obs_idx], 0, env->num_resources*sizeof(float));
        env->observations[obs_idx + agent->item] = 1.0f;
        obs_idx += env->num_resources;
    }
}

void c_reset(Convert* env) {
    for (int i=0; i<env->num_agents; i++) {
        env->agents[i].x = 16 + rand()%(env->width-16);
        env->agents[i].y = 16 + rand()%(env->height-16);
        env->agents[i].item = rand() % env->num_resources;
        env->agents[i].episode_length = 0;
    }
    for (int i=0; i<env->num_factories; i++) {
        env->factories[i].x = 16 + rand()%(env->width-16);
        env->factories[i].y = 16 + rand()%(env->height-16);
        env->factories[i].item = i % env->num_resources;
        env->factories[i].heading = (rand() % 360)*PI/180.0f;
    }
    compute_observations(env);
}

float clip(float val, float min, float max) {
    if (val < min) {
        return min;
    } else if (val > max) {
        return max;
    }
    return val;
}

void c_step(Convert* env) {
    for (int i=0; i<env->num_agents; i++) {
        env->terminals[i] = 0;
        env->rewards[i] = 0;
        Agent* agent = &env->agents[i];
        agent->episode_length += 1;

        agent->heading += ((float)env->actions[2*i] - 4.0f)/12.0f;
        agent->heading = clip(agent->heading, 0, 2*PI);

        agent->speed += 1.0f*((float)env->actions[2*i + 1] - 2.0f);
        agent->speed = clip(agent->speed, -20.0f, 20.0f);

        agent->x += agent->speed*cosf(agent->heading);
        agent->x = clip(agent->x, 16, env->width-16);

        agent->y += agent->speed*sinf(agent->heading);
        agent->y = clip(agent->y, 16, env->height-16);

        if (rand() % env->num_agents == 0) {
            env->agents[i].x = rand() % env->width;
            env->agents[i].y = rand() % env->height;
        }

        for (int f=0; f<env->num_factories; f++) {
            Factory* factory = &env->factories[f];
            float dx = (factory->x - agent->x);
            float dy = (factory->y - agent->y);
            float dist = sqrt(dx*dx + dy*dy);
            if (dist > 32) {
                continue;
            }
            if (factory->item == agent->item) {
                agent->item = (agent->item + 1) % env->num_resources;
                env->log.perf += 1.0f;
                env->log.score += 1.0f;
                env->log.episode_length += agent->episode_length;
                env->log.n++;
                env->rewards[i] = 1.0f;
                agent->episode_length = 0;
            }
        }
    }
    for (int f=0; f<env->num_factories; f++) {
        Factory* factory = &env->factories[f];
        factory->x += 2.0f*cosf(factory->heading);
        factory->y += 2.0f*sinf(factory->heading);

        float factory_x = clip(factory->x, 16, env->width-16);
        float factory_y = clip(factory->y, 16, env->height-16);

        if (factory_x != factory->x || factory_y != factory->y) {
            factory->heading = (rand() % 360)*PI/180.0f;
            factory->x = factory_x;
            factory->y = factory_y;
        }
    }
    compute_observations(env);
}

void c_render(Convert* env) {
    if (env->client == NULL) {
        InitWindow(env->width, env->height, "PufferLib Convert");
        SetTargetFPS(30);
        env->client = (Client*)calloc(1, sizeof(Client));
        env->client->sprites = LoadTexture("resources/shared/puffers.png");
    }

    if (IsKeyDown(KEY_ESCAPE)) {
        exit(0);
    }

    BeginDrawing();
    ClearBackground((Color){6, 24, 24, 255});

    for (int f=0; f<env->num_factories; f++) {
        Factory* factory = &env->factories[f];
        DrawTexturePro(
            env->client->sprites,
            (Rectangle){
                64*factory->item, 512, 64, 64,
            },
            (Rectangle){
                factory->x - 32,
                factory->y - 32,
                64,
                64 
            },
            (Vector2){0, 0},
            0,
            WHITE
        );
    }

    for (int i=0; i<env->num_agents; i++) {
        Agent* agent = &env->agents[i];
        float heading = agent->heading;
        int y = 576;
        if (heading < PI/2 || heading > 3*PI/2) {
            y += 32;
        }
        DrawTexturePro(
            env->client->sprites,
            (Rectangle){
                32*agent->item, y, 32, 32,
            },
            (Rectangle){
                agent->x - 16,
                agent->y - 16,
                32,
                32
            },
            (Vector2){0, 0},
            0,
            WHITE
        );
    }

    EndDrawing();
}

void c_close(Convert* env) {
    free(env->agents);
    free(env->factories);
    if (env->client != NULL) {
        Client* client = env->client;
        UnloadTexture(client->sprites);
        CloseWindow();
        free(client);
    }
}



================================================
FILE: pufferlib/ocean/convert/convert.py
================================================
'''A simple sample environment. Use this as a template for your own envs.'''

import gymnasium
import numpy as np

import pufferlib
from pufferlib.ocean.convert import binding

class Convert(pufferlib.PufferEnv):
    def __init__(self, num_envs=1, width=1920, height=1080, num_agents=1024, num_factories=32,
            num_resources=8, render_mode=None, log_interval=128, buf=None, seed=0):
        self.single_observation_space = gymnasium.spaces.Box(low=0, high=1,
            shape=(2*num_resources + 4 + num_resources,), dtype=np.float32)
        self.single_action_space = gymnasium.spaces.MultiDiscrete([9, 5])

        self.render_mode = render_mode
        self.num_agents = num_envs*num_agents
        self.log_interval = log_interval

        if num_resources < 1 or num_resources > 8:
            raise pufferlib.APIUsageError('num_resources must be in [1, 8]')

        super().__init__(buf)
        c_envs = []
        for i in range(num_envs):
            c_env = binding.env_init(
                self.observations[i*num_agents:(i+1)*num_agents],
                self.actions[i*num_agents:(i+1)*num_agents],
                self.rewards[i*num_agents:(i+1)*num_agents],
                self.terminals[i*num_agents:(i+1)*num_agents],
                self.truncations[i*num_agents:(i+1)*num_agents],
                seed, width=width, height=height,
                num_agents=num_agents, num_factories=num_factories,
                num_resources=num_resources)
            c_envs.append(c_env)

        self.c_envs = binding.vectorize(*c_envs)

    def reset(self, seed=0):
        binding.vec_reset(self.c_envs, seed)
        self.tick = 0
        return self.observations, []

    def step(self, actions):
        self.tick += 1
        self.actions[:] = actions
        binding.vec_step(self.c_envs)

        info = []
        if self.tick % self.log_interval == 0:
            log = binding.vec_log(self.c_envs)
            if log:
                info.append(log)

        return (self.observations, self.rewards,
            self.terminals, self.truncations, info)

    def render(self):
        binding.vec_render(self.c_envs, 0)

    def close(self):
        binding.vec_close(self.c_envs)

if __name__ == '__main__':
    N = 512

    env = Convert(num_envs=N)
    env.reset()
    steps = 0

    CACHE = 1024
    actions = np.random.randint(env.single_action_space.nvec, size=(CACHE, 2))

    i = 0
    import time
    start = time.time()
    while time.time() - start < 10:
        env.step(actions[i % CACHE])
        steps += env.num_agents
        i += 1

    print('Convert SPS:', int(steps / (time.time() - start)))



================================================
FILE: pufferlib/ocean/convert_circle/binding.c
================================================
#include "convert_circle.h"

#define Env ConvertCircle
#include "../env_binding.h"

static int my_init(Env *env, PyObject *args, PyObject *kwargs) {
  env->width = unpack(kwargs, "width");
  env->height = unpack(kwargs, "height");
  env->num_agents = unpack(kwargs, "num_agents");
  env->num_factories = unpack(kwargs, "num_factories");
  env->num_resources = unpack(kwargs, "num_resources");
  env->equidistant = unpack(kwargs, "equidistant");
  env->radius = unpack(kwargs, "radius");
  init(env);
  return 0;
}

static int my_log(PyObject *dict, Log *log) {
  assign_to_dict(dict, "perf", log->perf);
  assign_to_dict(dict, "score", log->score);
  assign_to_dict(dict, "episode_return", log->episode_return);
  assign_to_dict(dict, "episode_length", log->episode_length);
  return 0;
}



================================================
FILE: pufferlib/ocean/convert_circle/convert_circle.c
================================================
#include "convert_circle.h"
#include "puffernet.h"
#include <stdlib.h>
#include <time.h>

int main() {
  ConvertCircle env = {
      .width = 1920,
      .height = 1080,
      .num_agents = 128,
      .num_factories = 16,
      .num_resources = 6,
      .equidistant = 1,
      .radius = 400,
  };
  srand(time(NULL));
  init(&env);

  int num_obs = 2 * env.num_resources + 4 + env.num_resources;
  env.observations = calloc(env.num_agents * num_obs, sizeof(float));
  env.actions = calloc(2 * env.num_agents, sizeof(int));
  env.rewards = calloc(env.num_agents, sizeof(float));
  env.terminals = calloc(env.num_agents, sizeof(unsigned char));

  Weights *weights =
      load_weights("resources/convert/convert_weights.bin", 137743);
  int logit_sizes[2] = {9, 5};
  LinearLSTM *net =
      make_linearlstm(weights, env.num_agents, num_obs, logit_sizes, 2);

  c_reset(&env);
  c_render(&env);

  while (!WindowShouldClose()) {
    for (int i = 0; i < env.num_agents; i++) {
      env.actions[2 * i] = rand() % 9;
      env.actions[2 * i + 1] = rand() % 5;
    }

    forward_linearlstm(net, env.observations, env.actions);
    compute_observations(&env);
    c_step(&env);
    c_render(&env);
  }

  free_linearlstm(net);
  free(env.observations);
  free(env.actions);
  free(env.rewards);
  free(env.terminals);
  c_close(&env);
}



================================================
FILE: pufferlib/ocean/convert_circle/convert_circle.h
================================================
/* ConvertCircle: a sample multiagent env about puffers eating stars.
 * Use this as a tutorial and template for your own multiagent envs.
 * We suggest starting with the Squared env for a simpler intro.
 * Star PufferLib on GitHub to support. It really, really helps!
 */

#include "raylib.h"
#include <math.h>
#include <stdlib.h>
#include <string.h>

typedef struct {
  float perf;
  float score;
  float episode_return;
  float episode_length;
  float n;
} Log;

typedef struct {
  Texture2D sprites;
} Client;

typedef struct {
  float x;
  float y;
  float heading;
  float speed;
  int item;
  int episode_length;
} Agent;

typedef struct {
  float x;
  float y;
  float heading;
  int item;
} Factory;

typedef struct {
  Log log;
  Client *client;
  Agent *agents;
  Factory *factories;
  float *observations;
  int *actions;
  float *rewards;
  unsigned char *terminals;
  int width;
  int height;
  int num_agents;
  int num_factories;
  int num_resources;
  int equidistant;
  int radius;
} ConvertCircle;

static inline float random_float(float low, float high) {
  return low + (high - low) * ((float)rand() / (float)RAND_MAX);
}

void init(ConvertCircle *env) {
  env->agents = calloc(env->num_agents, sizeof(Agent));
  env->factories = calloc(env->num_factories, sizeof(Factory));
}

int compare_floats(const void *a, const void *b) {
  return (*(float *)a - *(float *)b) > 0;
}

void compute_observations(ConvertCircle *env) {
  int obs_idx = 0;
  for (int a = 0; a < env->num_agents; a++) {
    Agent *agent = &env->agents[a];
    float dists[env->num_resources];
    for (int i = 0; i < env->num_resources; i++) {
      dists[i] = 999999;
    }
    for (int f = 0; f < env->num_factories; f++) {
      Factory *factory = &env->factories[f];
      float dx = factory->x - agent->x;
      float dy = factory->y - agent->y;
      float dd = dx * dx + dy * dy;
      int type = f % env->num_resources;
      if (dd < dists[type]) {
        dists[type] = dd;
        env->observations[obs_idx + 2 * type] = dx / env->width;
        env->observations[obs_idx + 2 * type + 1] = dy / env->height;
      }
    }
    obs_idx += 2 * env->num_resources;
    env->observations[obs_idx++] = agent->heading / (2 * PI);
    env->observations[obs_idx++] = env->rewards[a];
    env->observations[obs_idx++] = agent->x / env->width;
    env->observations[obs_idx++] = agent->y / env->height;
    memset(&env->observations[obs_idx], 0, env->num_resources * sizeof(float));
    env->observations[obs_idx + agent->item] = 1.0f;
    obs_idx += env->num_resources;
  }
}

void c_reset(ConvertCircle *env) {
  for (int i = 0; i < env->num_agents; i++) {
    env->agents[i].x = env->width / 2.0f + random_float(-10.0f, 10.0f);
    env->agents[i].y = env->height / 2.0f + random_float(-10.0f, 10.0f);
    env->agents[i].item = rand() % env->num_resources;
    env->agents[i].episode_length = 0;
  }
  float angle;
  float delta_angle = 2.0f * PI / env->num_factories;
  for (int i = 0; i < env->num_factories; i++) {
    if (env->equidistant) {
      angle = i * delta_angle;
    } else {
      angle = random_float(0, 2.0f * PI);
    }
    env->factories[i].x = env->width / 2.0f + env->radius * cosf(angle);
    env->factories[i].y = env->height / 2.0f + env->radius * sinf(angle);
    env->factories[i].item = i % env->num_resources;
    env->factories[i].heading = (rand() % 360) * PI / 180.0f;
  }
  compute_observations(env);
}

float clip(float val, float min, float max) {
  if (val < min) {
    return min;
  } else if (val > max) {
    return max;
  }
  return val;
}

void c_step(ConvertCircle *env) {
  for (int i = 0; i < env->num_agents; i++) {
    env->terminals[i] = 0;
    env->rewards[i] = 0;
    Agent *agent = &env->agents[i];
    agent->episode_length += 1;

    agent->heading += ((float)env->actions[2 * i] - 4.0f) / 12.0f;
    agent->heading = clip(agent->heading, 0, 2 * PI);

    agent->speed += 1.0f * ((float)env->actions[2 * i + 1] - 2.0f);
    agent->speed = clip(agent->speed, -20.0f, 20.0f);

    agent->x += agent->speed * cosf(agent->heading);
    agent->x = clip(agent->x, 16, env->width - 16);

    agent->y += agent->speed * sinf(agent->heading);
    agent->y = clip(agent->y, 16, env->height - 16);

    if (rand() % env->num_agents == 0) {
      env->agents[i].x = env->width / 2.0f + random_float(-10.0f, 10.0f);
      env->agents[i].y = env->height / 2.0f + random_float(-10.0f, 10.0f);
    }

    for (int f = 0; f < env->num_factories; f++) {
      Factory *factory = &env->factories[f];
      float dx = (factory->x - agent->x);
      float dy = (factory->y - agent->y);
      float dist = sqrt(dx * dx + dy * dy);
      if (dist > 32) {
        continue;
      }
      if (factory->item == agent->item) {
        agent->item = (agent->item + 1) % env->num_resources;
        env->log.perf += 1.0f;
        env->log.score += 1.0f;
        env->log.episode_length += agent->episode_length;
        env->log.n++;
        env->rewards[i] = 1.0f;
        agent->episode_length = 0;
      }
    }
  }
  for (int f = 0; f < env->num_factories; f++) {
    Factory *factory = &env->factories[f];
    factory->x += 0.0f * cosf(factory->heading);
    factory->y += 0.0f * sinf(factory->heading);

    float factory_x = clip(factory->x, 16, env->width - 16);
    float factory_y = clip(factory->y, 16, env->height - 16);

    if (factory_x != factory->x || factory_y != factory->y) {
      factory->heading = (rand() % 360) * PI / 180.0f;
      factory->x = factory_x;
      factory->y = factory_y;
    }
  }
  compute_observations(env);
}

void c_render(ConvertCircle *env) {
  if (env->client == NULL) {
    InitWindow(env->width, env->height, "PufferLib ConvertCircle");
    SetTargetFPS(30);
    env->client = (Client *)calloc(1, sizeof(Client));
    env->client->sprites = LoadTexture("resources/shared/puffers.png");
  }

  if (IsKeyDown(KEY_ESCAPE)) {
    exit(0);
  }

  BeginDrawing();
  ClearBackground((Color){6, 24, 24, 255});

  for (int f = 0; f < env->num_factories; f++) {
    Factory *factory = &env->factories[f];
    DrawTexturePro(env->client->sprites,
                   (Rectangle){
                       64 * factory->item,
                       512,
                       64,
                       64,
                   },
                   (Rectangle){factory->x - 32, factory->y - 32, 64, 64},
                   (Vector2){0, 0}, 0, WHITE);
  }

  for (int i = 0; i < env->num_agents; i++) {
    Agent *agent = &env->agents[i];
    float heading = agent->heading;
    int y = 576;
    if (heading < PI / 2 || heading > 3 * PI / 2) {
      y += 32;
    }
    DrawTexturePro(env->client->sprites,
                   (Rectangle){
                       32 * agent->item,
                       y,
                       32,
                       32,
                   },
                   (Rectangle){agent->x - 16, agent->y - 16, 32, 32},
                   (Vector2){0, 0}, 0, WHITE);
  }

  EndDrawing();
}

void c_close(ConvertCircle *env) {
  free(env->agents);
  free(env->factories);
  if (env->client != NULL) {
    Client *client = env->client;
    UnloadTexture(client->sprites);
    CloseWindow();
    free(client);
  }
}



================================================
FILE: pufferlib/ocean/convert_circle/convert_circle.py
================================================
'''A simple sample environment. Use this as a template for your own envs.'''

import gymnasium
import numpy as np

import pufferlib
from pufferlib.ocean.convert_circle import binding

class ConvertCircle(pufferlib.PufferEnv):
    def __init__(self, num_envs=1, width=1920, height=1080, num_agents=1024, num_factories=32,
            num_resources=8, equidistant=0, radius=30, render_mode=None, log_interval=128, buf=None, seed=0):
        self.single_observation_space = gymnasium.spaces.Box(low=0, high=1,
            shape=(2*num_resources + 4 + num_resources,), dtype=np.float32)
        self.single_action_space = gymnasium.spaces.MultiDiscrete([9, 5])

        self.render_mode = render_mode
        self.num_agents = num_envs*num_agents
        self.log_interval = log_interval

        if num_resources < 1 or num_resources > 8:
            raise pufferlib.APIUsageError('num_resources must be in [1, 8]')

        super().__init__(buf)
        c_envs = []
        for i in range(num_envs):
            c_env = binding.env_init(
                self.observations[i*num_agents:(i+1)*num_agents],
                self.actions[i*num_agents:(i+1)*num_agents],
                self.rewards[i*num_agents:(i+1)*num_agents],
                self.terminals[i*num_agents:(i+1)*num_agents],
                self.truncations[i*num_agents:(i+1)*num_agents],
                seed, width=width, height=height,
                num_agents=num_agents, num_factories=num_factories,
                num_resources=num_resources, equidistant=equidistant,
                radius=radius)
            c_envs.append(c_env)

        self.c_envs = binding.vectorize(*c_envs)

    def reset(self, seed=0):
        binding.vec_reset(self.c_envs, seed)
        self.tick = 0
        return self.observations, []

    def step(self, actions):
        self.tick += 1
        self.actions[:] = actions
        binding.vec_step(self.c_envs)

        info = []
        if self.tick % self.log_interval == 0:
            log = binding.vec_log(self.c_envs)
            if log:
                info.append(log)

        return (self.observations, self.rewards,
            self.terminals, self.truncations, info)

    def render(self):
        binding.vec_render(self.c_envs, 0)

    def close(self):
        binding.vec_close(self.c_envs)

if __name__ == '__main__':
    N = 512

    env = ConvertCircle(num_envs=N)
    env.reset()
    steps = 0

    CACHE = 1024
    actions = np.random.randint(env.single_action_space.nvec, size=(CACHE, 2))

    i = 0
    import time
    start = time.time()
    while time.time() - start < 10:
        env.step(actions[i % CACHE])
        steps += env.num_agents
        i += 1

    print('ConvertCircle SPS:', int(steps / (time.time() - start)))



================================================
FILE: pufferlib/ocean/drive/binding.c
================================================
#include "drive.h"
#define Env Drive
#define MY_SHARED
#define MY_PUT
#include "../env_binding.h"

static int my_put(Env* env, PyObject* args, PyObject* kwargs) {
    PyObject* obs = PyDict_GetItemString(kwargs, "observations");
    if (!PyObject_TypeCheck(obs, &PyArray_Type)) {
        PyErr_SetString(PyExc_TypeError, "Observations must be a NumPy array");
        return 1;
    }
    PyArrayObject* observations = (PyArrayObject*)obs;
    if (!PyArray_ISCONTIGUOUS(observations)) {
        PyErr_SetString(PyExc_ValueError, "Observations must be contiguous");
        return 1;
    }
    env->observations = PyArray_DATA(observations);

    PyObject* act = PyDict_GetItemString(kwargs, "actions");
    if (!PyObject_TypeCheck(act, &PyArray_Type)) {
        PyErr_SetString(PyExc_TypeError, "Actions must be a NumPy array");
        return 1;
    }
    PyArrayObject* actions = (PyArrayObject*)act;
    if (!PyArray_ISCONTIGUOUS(actions)) {
        PyErr_SetString(PyExc_ValueError, "Actions must be contiguous");
        return 1;
    }
    env->actions = PyArray_DATA(actions);
    if (PyArray_ITEMSIZE(actions) == sizeof(double)) {
        PyErr_SetString(PyExc_ValueError, "Action tensor passed as float64 (pass np.float32 buffer)");
        return 1;
    }

    PyObject* rew = PyDict_GetItemString(kwargs, "rewards");
    if (!PyObject_TypeCheck(rew, &PyArray_Type)) {
        PyErr_SetString(PyExc_TypeError, "Rewards must be a NumPy array");
        return 1;
    }
    PyArrayObject* rewards = (PyArrayObject*)rew;
    if (!PyArray_ISCONTIGUOUS(rewards)) {
        PyErr_SetString(PyExc_ValueError, "Rewards must be contiguous");
        return 1;
    }
    if (PyArray_NDIM(rewards) != 1) {
        PyErr_SetString(PyExc_ValueError, "Rewards must be 1D");
        return 1;
    }
    env->rewards = PyArray_DATA(rewards);

    PyObject* term = PyDict_GetItemString(kwargs, "terminals");
    if (!PyObject_TypeCheck(term, &PyArray_Type)) {
        PyErr_SetString(PyExc_TypeError, "Terminals must be a NumPy array");
        return 1;
    }
    PyArrayObject* terminals = (PyArrayObject*)term;
    if (!PyArray_ISCONTIGUOUS(terminals)) {
        PyErr_SetString(PyExc_ValueError, "Terminals must be contiguous");
        return 1;
    }
    if (PyArray_NDIM(terminals) != 1) {
        PyErr_SetString(PyExc_ValueError, "Terminals must be 1D");
        return 1;
    }
    env->terminals = PyArray_DATA(terminals);
    return 0;
}

static PyObject* my_shared(PyObject* self, PyObject* args, PyObject* kwargs) {
    int num_agents = unpack(kwargs, "num_agents");
    int num_maps = unpack(kwargs, "num_maps");
    clock_gettime(CLOCK_REALTIME, &ts);
    srand(ts.tv_nsec);
    int total_agent_count = 0;
    int env_count = 0;
    int max_envs = num_agents;
    PyObject* agent_offsets = PyList_New(max_envs+1);
    PyObject* map_ids = PyList_New(max_envs);
    // getting env count
    while(total_agent_count < num_agents && env_count < max_envs){
        char map_file[100];
        int map_id = rand() % num_maps;
        Drive* env = calloc(1, sizeof(Drive));
        sprintf(map_file, "resources/drive/binaries/map_%03d.bin", map_id);
        env->entities = load_map_binary(map_file, env);
        set_active_agents(env);
        // Store map_id
        PyObject* map_id_obj = PyLong_FromLong(map_id);
        PyList_SetItem(map_ids, env_count, map_id_obj);
        // Store agent offset
        PyObject* offset = PyLong_FromLong(total_agent_count);
        PyList_SetItem(agent_offsets, env_count, offset);
        total_agent_count += env->active_agent_count;
        env_count++;
        for(int j=0;j<env->num_entities;j++) {
            free_entity(&env->entities[j]);
        }
        free(env->entities);
        free(env->active_agent_indices);
        free(env->static_car_indices);
        free(env->expert_static_car_indices);
        free(env);
    }
    if(total_agent_count >= num_agents){
        total_agent_count = num_agents;
    }
    PyObject* final_total_agent_count = PyLong_FromLong(total_agent_count);
    PyList_SetItem(agent_offsets, env_count, final_total_agent_count);
    PyObject* final_env_count = PyLong_FromLong(env_count);
    // resize lists
    PyObject* resized_agent_offsets = PyList_GetSlice(agent_offsets, 0, env_count + 1);
    PyObject* resized_map_ids = PyList_GetSlice(map_ids, 0, env_count);
    //
    //Py_DECREF(agent_offsets);
    //Py_DECREF(map_ids);
    // create a tuple
    PyObject* tuple = PyTuple_New(3);
    PyTuple_SetItem(tuple, 0, resized_agent_offsets);
    PyTuple_SetItem(tuple, 1, resized_map_ids);
    PyTuple_SetItem(tuple, 2, final_env_count);
    return tuple;

    //Py_DECREF(num);
    /*
    for(int i = 0;i<num_envs; i++) {
        for(int j=0;j<temp_envs[i].num_entities;j++) {
            free_entity(&temp_envs[i].entities[j]);
        }
        free(temp_envs[i].entities);
        free(temp_envs[i].active_agent_indices);
        free(temp_envs[i].static_car_indices);
    }
    free(temp_envs);
    */
    // return agent_offsets;
}

static int my_init(Env* env, PyObject* args, PyObject* kwargs) {
    env->human_agent_idx = unpack(kwargs, "human_agent_idx");
    env->reward_vehicle_collision = unpack(kwargs, "reward_vehicle_collision");
    env->reward_offroad_collision = unpack(kwargs, "reward_offroad_collision");
    env->reward_goal_post_respawn = unpack(kwargs, "reward_goal_post_respawn");
    env->reward_vehicle_collision_post_respawn = unpack(kwargs, "reward_vehicle_collision_post_respawn");
    env->spawn_immunity_timer = unpack(kwargs, "spawn_immunity_timer");
    int map_id = unpack(kwargs, "map_id");
    int max_agents = unpack(kwargs, "max_agents");

    char map_file[100];
    sprintf(map_file, "resources/drive/binaries/map_%03d.bin", map_id);
    env->num_agents = max_agents;
    env->map_name = strdup(map_file);
    init(env);
    return 0;
}

static int my_log(PyObject* dict, Log* log) {
    assign_to_dict(dict, "perf", log->perf);
    assign_to_dict(dict, "score", log->score);
    assign_to_dict(dict, "episode_return", log->episode_return);
    assign_to_dict(dict, "episode_length", log->episode_length);
    assign_to_dict(dict, "offroad_rate", log->offroad_rate);
    assign_to_dict(dict, "collision_rate", log->collision_rate);
    assign_to_dict(dict, "dnf_rate", log->dnf_rate);
    assign_to_dict(dict, "n", log->n);
    assign_to_dict(dict, "completion_rate", log->completion_rate);
    assign_to_dict(dict, "clean_collision_rate", log->clean_collision_rate);
    return 0;
}



================================================
FILE: pufferlib/ocean/drive/drive.c
================================================
#include <time.h>
#include <unistd.h>
#include "drive.h"
#include "puffernet.h"

typedef struct DriveNet DriveNet;
struct DriveNet {
    int num_agents;
    float* obs_self;
    float* obs_partner;
    float* obs_road;
    float* partner_linear_output;
    float* road_linear_output;
    float* partner_layernorm_output;
    float* road_layernorm_output;
    float* partner_linear_output_two;
    float* road_linear_output_two;
    Linear* ego_encoder;
    Linear* road_encoder;
    Linear* partner_encoder;
    LayerNorm* ego_layernorm;
    LayerNorm* road_layernorm;
    LayerNorm* partner_layernorm;
    Linear* ego_encoder_two;
    Linear* road_encoder_two;
    Linear* partner_encoder_two;
    MaxDim1* partner_max;
    MaxDim1* road_max;
    CatDim1* cat1;
    CatDim1* cat2;
    GELU* gelu;
    Linear* shared_embedding;
    ReLU* relu;
    LSTM* lstm;
    Linear* actor;
    Linear* value_fn;
    Multidiscrete* multidiscrete;
};

DriveNet* init_drivenet(Weights* weights, int num_agents) {
    DriveNet* net = calloc(1, sizeof(DriveNet));
    int hidden_size = 256;
    int input_size = 64;

    net->num_agents = num_agents;
    net->obs_self = calloc(num_agents*7, sizeof(float)); // 7 features
    net->obs_partner = calloc(num_agents*63*7, sizeof(float)); // 63 objects, 7 features
    net->obs_road = calloc(num_agents*200*13, sizeof(float)); // 200 objects, 13 features
    net->partner_linear_output = calloc(num_agents*63*input_size, sizeof(float));
    net->road_linear_output = calloc(num_agents*200*input_size, sizeof(float));
    net->partner_linear_output_two = calloc(num_agents*63*input_size, sizeof(float));
    net->road_linear_output_two = calloc(num_agents*200*input_size, sizeof(float));
    net->partner_layernorm_output = calloc(num_agents*63*input_size, sizeof(float));
    net->road_layernorm_output = calloc(num_agents*200*input_size, sizeof(float));
    net->ego_encoder = make_linear(weights, num_agents, 7, input_size);
    net->ego_layernorm = make_layernorm(weights, num_agents, input_size);
    net->ego_encoder_two = make_linear(weights, num_agents, input_size, input_size);
    net->road_encoder = make_linear(weights, num_agents, 13, input_size);
    net->road_layernorm = make_layernorm(weights, num_agents, input_size);
    net->road_encoder_two = make_linear(weights, num_agents, input_size, input_size);
    net->partner_encoder = make_linear(weights, num_agents, 7, input_size);
    net->partner_layernorm = make_layernorm(weights, num_agents, input_size);
    net->partner_encoder_two = make_linear(weights, num_agents, input_size, input_size);
    net->partner_max = make_max_dim1(num_agents, 63, input_size);
    net->road_max = make_max_dim1(num_agents, 200, input_size);
    net->cat1 = make_cat_dim1(num_agents, input_size, input_size);
    net->cat2 = make_cat_dim1(num_agents, input_size + input_size, input_size);
    net->gelu = make_gelu(num_agents, 3*input_size);
    net->shared_embedding = make_linear(weights, num_agents, input_size*3, hidden_size);
    net->relu = make_relu(num_agents, hidden_size);
    net->actor = make_linear(weights, num_agents, hidden_size, 20); 
    net->value_fn = make_linear(weights, num_agents, hidden_size, 1);
    net->lstm = make_lstm(weights, num_agents, hidden_size, 256);
    memset(net->lstm->state_h, 0, num_agents*256*sizeof(float));
    memset(net->lstm->state_c, 0, num_agents*256*sizeof(float));
    int logit_sizes[2] = {7, 13};
    net->multidiscrete = make_multidiscrete(num_agents, logit_sizes, 2);
    return net;
}

void free_drivenet(DriveNet* net) {
    free(net->obs_self);
    free(net->obs_partner);
    free(net->obs_road);
    free(net->partner_linear_output);
    free(net->road_linear_output);
    free(net->partner_linear_output_two);
    free(net->road_linear_output_two);
    free(net->ego_encoder);
    free(net->road_encoder);
    free(net->partner_encoder); 
    free(net->ego_layernorm);
    free(net->road_layernorm);
    free(net->partner_layernorm);
    free(net->ego_encoder_two);
    free(net->road_encoder_two);
    free(net->partner_encoder_two);
    free(net->partner_max);
    free(net->road_max);
    free(net->cat1);
    free(net->cat2);
    free(net->gelu);
    free(net->shared_embedding);
    free(net->relu);
    free(net->multidiscrete);
    free(net->actor);
    free(net->value_fn);
    free(net->lstm);
    free(net);
}

void forward(DriveNet* net, float* observations, int* actions) {
    // Clear previous observations
    memset(net->obs_self, 0, net->num_agents * 7 * sizeof(float));
    memset(net->obs_partner, 0, net->num_agents * 63 * 7 * sizeof(float));
    memset(net->obs_road, 0, net->num_agents * 200 * 13 * sizeof(float));
    
    // Reshape observations into 2D boards and additional features
    float (*obs_self)[7] = (float (*)[7])net->obs_self;
    float (*obs_partner)[63][7] = (float (*)[63][7])net->obs_partner;
    float (*obs_road)[200][13] = (float (*)[200][13])net->obs_road;
    
    for (int b = 0; b < net->num_agents; b++) {
        int b_offset = b * (7 + 63*7 + 200*7);  // offset for each batch
        int partner_offset = b_offset + 7;
        int road_offset = b_offset + 7 + 63*7;
        // Process self observation
        for(int i = 0; i < 7; i++) {
            obs_self[b][i] = observations[b_offset + i];
        }

        // Process partner observation
        for(int i = 0; i < 63; i++) {
            for(int j = 0; j < 7; j++) {
                obs_partner[b][i][j] = observations[partner_offset + i*7 + j];
            }
        }

        // Process road observation
        for(int i = 0; i < 200; i++) {
            for(int j = 0; j < 7; j++) {
                obs_road[b][i][j] = observations[road_offset + i*7 + j];
            }
            for(int j = 0; j < 7; j++) {
                if(j == observations[road_offset+i*7 + 6]) {
                    obs_road[b][i][6 + j] = 1.0f;
                } else {
                    obs_road[b][i][6 + j] = 0.0f;
                }
            }
        }
    }

    // Forward pass through the network
    linear(net->ego_encoder, net->obs_self);
    layernorm(net->ego_layernorm, net->ego_encoder->output);
    linear(net->ego_encoder_two, net->ego_layernorm->output);
    for (int b = 0; b < net->num_agents; b++) {
        for (int obj = 0; obj < 63; obj++) {
            // Get the 7 features for this object
            float* obj_features = &net->obs_partner[b*63*7 + obj*7];
            // Apply linear layer to this object
            _linear(obj_features, net->partner_encoder->weights, net->partner_encoder->bias,
                   &net->partner_linear_output[b*63*64 + obj*64], 1, 7, 64);
        }
    }

    for (int b = 0; b < net->num_agents; b++) {
        for (int obj = 0; obj < 63; obj++) {
            float* after_first = &net->partner_linear_output[b*63*64 + obj*64];
            _layernorm(after_first, net->partner_layernorm->weights, net->partner_layernorm->bias,
                        &net->partner_layernorm_output[b*63*64 + obj*64], 1, 64);
        }
    }
    for (int b = 0; b < net->num_agents; b++) {
        for (int obj = 0; obj < 63; obj++) {
            // Get the 7 features for this object
            float* obj_features = &net->partner_layernorm_output[b*63*64 + obj*64];
            // Apply linear layer to this object
            _linear(obj_features, net->partner_encoder_two->weights, net->partner_encoder_two->bias,
                   &net->partner_linear_output_two[b*63*64 + obj*64], 1, 64, 64);
            
        }
    }
    
    // Process road objects: apply linear to each object individually  
    for (int b = 0; b < net->num_agents; b++) {
        for (int obj = 0; obj < 200; obj++) {
            // Get the 13 features for this object
            float* obj_features = &net->obs_road[b*200*13 + obj*13];
            // Apply linear layer to this object
            _linear(obj_features, net->road_encoder->weights, net->road_encoder->bias,
                   &net->road_linear_output[b*200*64 + obj*64], 1, 13, 64);
        }
    }
    
    // Apply layer norm and second linear to each road object
    for (int b = 0; b < net->num_agents; b++) {
        for (int obj = 0; obj < 200; obj++) {
            float* after_first = &net->road_linear_output[b*200*64 + obj*64];
            _layernorm(after_first, net->road_layernorm->weights, net->road_layernorm->bias,
                        &net->road_layernorm_output[b*200*64 + obj*64], 1, 64);
        }
    }
    for (int b = 0; b < net->num_agents; b++) {
        for (int obj = 0; obj < 200; obj++) {
            float* after_first = &net->road_layernorm_output[b*200*64 + obj*64];
            _linear(after_first, net->road_encoder_two->weights, net->road_encoder_two->bias,
                    &net->road_linear_output_two[b*200*64 + obj*64], 1, 64, 64);
        }
    }
    
    max_dim1(net->partner_max, net->partner_linear_output_two);
    max_dim1(net->road_max, net->road_linear_output_two);
    cat_dim1(net->cat1, net->ego_encoder_two->output, net->road_max->output);
    cat_dim1(net->cat2, net->cat1->output, net->partner_max->output);
    gelu(net->gelu, net->cat2->output);
    linear(net->shared_embedding, net->gelu->output);
    relu(net->relu, net->shared_embedding->output);
    lstm(net->lstm, net->relu->output);
    linear(net->actor, net->lstm->state_h);
    linear(net->value_fn, net->lstm->state_h);

    // Get action by taking argmax of actor output
    softmax_multidiscrete(net->multidiscrete, net->actor->output, actions);
}
void demo() {

    Drive env = {
        .dynamics_model = CLASSIC,
        .human_agent_idx = 0,
        .reward_vehicle_collision = -0.1f,
        .reward_offroad_collision = -0.1f,
	    .map_name = "resources/drive/binaries/map_942.bin",
        .spawn_immunity_timer = 50
    };
    allocate(&env);
    c_reset(&env);
    c_render(&env);
    Weights* weights = load_weights("resources/drive/puffer_drive_weights.bin", 595925);
    DriveNet* net = init_drivenet(weights, env.active_agent_count);
    //Client* client = make_client(&env);
    int accel_delta = 2;
    int steer_delta = 4;
    while (!WindowShouldClose()) {
        // Handle camera controls
        int (*actions)[2] = (int(*)[2])env.actions;
        forward(net, env.observations, env.actions);
        if (IsKeyDown(KEY_LEFT_SHIFT)) {
            actions[env.human_agent_idx][0] = 3;
            actions[env.human_agent_idx][1] = 6;
            if(IsKeyDown(KEY_UP) || IsKeyDown(KEY_W)){
                actions[env.human_agent_idx][0] += accel_delta;
                // Cap acceleration to maximum of 6
                if(actions[env.human_agent_idx][0] > 6) {
                    actions[env.human_agent_idx][0] = 6;
                }
            }
            if(IsKeyDown(KEY_DOWN) || IsKeyDown(KEY_S)){
                actions[env.human_agent_idx][0] -= accel_delta;
                // Cap acceleration to minimum of 0
                if(actions[env.human_agent_idx][0] < 0) {
                    actions[env.human_agent_idx][0] = 0;
                }
            }
            if(IsKeyDown(KEY_LEFT) || IsKeyDown(KEY_A)){
                actions[env.human_agent_idx][1] += steer_delta;
                // Cap steering to minimum of 0
                if(actions[env.human_agent_idx][1] < 0) {
                    actions[env.human_agent_idx][1] = 0;
                }
            }
            if(IsKeyDown(KEY_RIGHT) || IsKeyDown(KEY_D)){
                actions[env.human_agent_idx][1] -= steer_delta;
                // Cap steering to maximum of 12
                if(actions[env.human_agent_idx][1] > 12) {
                    actions[env.human_agent_idx][1] = 12;
                }
            }   
            if(IsKeyPressed(KEY_TAB)){
                env.human_agent_idx = (env.human_agent_idx + 1) % env.active_agent_count;
            }
        }
        c_step(&env);
        c_render(&env);
    }

    close_client(env.client);
    free_allocated(&env);
    free_drivenet(net);
    free(weights);
}

void performance_test() {
    long test_time = 10;
    Drive env = {
        .dynamics_model = CLASSIC,
        .human_agent_idx = 0,
	    .map_name = "resources/drive/binaries/map_942.bin"
    };
    clock_t start_time, end_time;
    double cpu_time_used;
    start_time = clock();
    allocate(&env);
    c_reset(&env);
    end_time = clock();
    cpu_time_used = ((double) (end_time - start_time)) / CLOCKS_PER_SEC;
    printf("Init time: %f\n", cpu_time_used);

    long start = time(NULL);
    int i = 0;
    int (*actions)[2] = (int(*)[2])env.actions;
    
    while (time(NULL) - start < test_time) {
        // Set random actions for all agents
        for(int j = 0; j < env.active_agent_count; j++) {
            int accel = rand() % 7;
            int steer = rand() % 13;
            actions[j][0] = accel;  // -1, 0, or 1
            actions[j][1] = steer;  // Random steering
        }
        
        c_step(&env);
        i++;
    }
    long end = time(NULL);
    printf("SPS: %ld\n", (i*env.active_agent_count) / (end - start));
    free_allocated(&env);
}

int main() {
    demo();
    // performance_test();
    return 0;
}



================================================
FILE: pufferlib/ocean/drive/drive.py
================================================
import numpy as np
import gymnasium
import json
import struct
import os
import random
import pufferlib
from pufferlib.ocean.drive import binding

class Drive(pufferlib.PufferEnv):
    def __init__(self, render_mode=None, report_interval=1,
            width=1280, height=1024,
            human_agent_idx=0,
            reward_vehicle_collision=-0.1,
            reward_offroad_collision=-0.1,
            reward_goal_post_respawn=0.5,
            reward_vehicle_collision_post_respawn=-0.25,
            spawn_immunity_timer=30,
            resample_frequency = 91,
            num_maps=100,
            num_agents=512,
            buf = None,
            seed=1):

        # env
        self.render_mode = render_mode
        self.num_maps = num_maps
        self.report_interval = report_interval
        self.reward_vehicle_collision = reward_vehicle_collision
        self.reward_offroad_collision = reward_offroad_collision
        self.reward_goal_post_respawn = reward_goal_post_respawn
        self.reward_vehicle_collision_post_respawn = reward_vehicle_collision_post_respawn
        self.spawn_immunity_timer = spawn_immunity_timer
        self.human_agent_idx = human_agent_idx
        self.resample_frequency = resample_frequency
        self.num_obs = 7 + 63*7 + 200*7
        self.single_observation_space = gymnasium.spaces.Box(low=-1, high=1,
            shape=(self.num_obs,), dtype=np.float32)
        self.single_action_space = gymnasium.spaces.MultiDiscrete([7, 13])
        # self.single_action_space = gymnasium.spaces.Box(
        #     low=-1, high=1, shape=(2,), dtype=np.float32
        # )
        # Check if resources directory exists
        binary_path = "resources/drive/binaries/map_000.bin"
        if not os.path.exists(binary_path):
            raise FileNotFoundError(f"Required directory {binary_path} not found. Please ensure the Drive maps are downloaded and installed correctly per docs.")
        agent_offsets, map_ids, num_envs = binding.shared(num_agents=num_agents, num_maps=num_maps)
        self.num_agents = num_agents
        self.agent_offsets = agent_offsets
        self.map_ids = map_ids
        self.num_envs = num_envs
        super().__init__(buf=buf)
        env_ids = []
        for i in range(num_envs):
            cur = agent_offsets[i]
            nxt = agent_offsets[i+1]
            env_id = binding.env_init(
                self.observations[cur:nxt],
                self.actions[cur:nxt],
                self.rewards[cur:nxt],
                self.terminals[cur:nxt],
                self.truncations[cur:nxt],
                seed,
                human_agent_idx=human_agent_idx,
                reward_vehicle_collision=reward_vehicle_collision,
                reward_offroad_collision=reward_offroad_collision,
                reward_goal_post_respawn=reward_goal_post_respawn,
                reward_vehicle_collision_post_respawn=reward_vehicle_collision_post_respawn,
                spawn_immunity_timer=spawn_immunity_timer,
                map_id=map_ids[i],
                max_agents = nxt-cur
            )
            env_ids.append(env_id)

        self.c_envs = binding.vectorize(*env_ids)

    def reset(self, seed=0):
        binding.vec_reset(self.c_envs, seed)
        self.tick = 0
        return self.observations, []

    def step(self, actions):
        self.terminals[:] = 0
        self.actions[:] = actions
        binding.vec_step(self.c_envs)
        self.tick+=1
        info = []
        if self.tick % self.report_interval == 0:
            log = binding.vec_log(self.c_envs)
            if log:
                info.append(log)
                #print(log)
        if(self.tick > 0 and self.resample_frequency > 0 and self.tick % self.resample_frequency == 0):
            self.tick = 0
            will_resample = 1
            if will_resample:
                binding.vec_close(self.c_envs)
                agent_offsets, map_ids, num_envs = binding.shared(num_agents=self.num_agents, num_maps=self.num_maps)
                env_ids = []
                seed = np.random.randint(0, 2**32-1)
                for i in range(num_envs):
                    cur = agent_offsets[i]
                    nxt = agent_offsets[i+1]
                    env_id = binding.env_init(
                        self.observations[cur:nxt],
                        self.actions[cur:nxt],
                        self.rewards[cur:nxt],
                        self.terminals[cur:nxt],
                        self.truncations[cur:nxt],
                        seed,
                        human_agent_idx=self.human_agent_idx,
                        reward_vehicle_collision=self.reward_vehicle_collision,
                        reward_offroad_collision=self.reward_offroad_collision,
                        reward_goal_post_respawn=self.reward_goal_post_respawn,
                        reward_vehicle_collision_post_respawn=self.reward_vehicle_collision_post_respawn,
                        spawn_immunity_timer=self.spawn_immunity_timer,
                        map_id=map_ids[i],
                        max_agents = nxt-cur
                    )
                    env_ids.append(env_id)
                self.c_envs = binding.vectorize(*env_ids)

                binding.vec_reset(self.c_envs, seed)
                self.terminals[:] = 1
        return (self.observations, self.rewards,
            self.terminals, self.truncations, info)

    def render(self):
        binding.vec_render(self.c_envs, 0)
        
    def close(self):
        binding.vec_close(self.c_envs)

def calculate_area(p1, p2, p3):
    # Calculate the area of the triangle using the determinant method
    return 0.5 * abs((p1['x'] - p3['x']) * (p2['y'] - p1['y']) - (p1['x'] - p2['x']) * (p3['y'] - p1['y']))

def simplify_polyline(geometry, polyline_reduction_threshold):
    """Simplify the given polyline using a method inspired by Visvalingham-Whyatt, optimized for Python."""
    num_points = len(geometry)
    if num_points < 3:
        return geometry  # Not enough points to simplify

    skip = [False] * num_points
    skip_changed = True

    while skip_changed:
        skip_changed = False
        k = 0
        while k < num_points - 1:
            k_1 = k + 1
            while k_1 < num_points - 1 and skip[k_1]:
                k_1 += 1
            if k_1 >= num_points - 1:
                break

            k_2 = k_1 + 1
            while k_2 < num_points and skip[k_2]:
                k_2 += 1
            if k_2 >= num_points:
                break

            point1 = geometry[k]
            point2 = geometry[k_1]
            point3 = geometry[k_2]
            area = calculate_area(point1, point2, point3)

            if area < polyline_reduction_threshold:
                skip[k_1] = True
                skip_changed = True
                k = k_2
            else:
                k = k_1

    return [geometry[i] for i in range(num_points) if not skip[i]]

def save_map_binary(map_data, output_file):
    trajectory_length = 91
    """Saves map data in a binary format readable by C"""
    with open(output_file, 'wb') as f:
        # Count total entities
        print(len(map_data.get('objects', [])))
        print(len(map_data.get('roads', [])))
        num_objects = len(map_data.get('objects', []))
        num_roads = len(map_data.get('roads', []))
        # num_entities = num_objects + num_roads
        f.write(struct.pack('i', num_objects))
        f.write(struct.pack('i', num_roads))
        # f.write(struct.pack('i', num_entities))
        # Write objects
        for obj in map_data.get('objects', []):
            # Write base entity data
            obj_type = obj.get('type', 1)
            if(obj_type =='vehicle'):
                obj_type = 1
            elif(obj_type == 'pedestrian'):
                obj_type = 2;
            elif(obj_type == 'cyclist'):
                obj_type = 3;
            f.write(struct.pack('i', obj_type))  # type
            # f.write(struct.pack('i', obj.get('id', 0)))   # id  
            f.write(struct.pack('i', trajectory_length))                  # array_size
            # Write position arrays
            positions = obj.get('position', [])
            for i in range(trajectory_length):
                pos = positions[i] if i < len(positions) else {'x': 0.0, 'y': 0.0, 'z': 0.0}
                f.write(struct.pack('f', float(pos.get('x', 0.0))))
            for i in range(trajectory_length):
                pos = positions[i] if i < len(positions) else {'x': 0.0, 'y': 0.0, 'z': 0.0}
                f.write(struct.pack('f', float(pos.get('y', 0.0))))
            for i in range(trajectory_length):
                pos = positions[i] if i < len(positions) else {'x': 0.0, 'y': 0.0, 'z': 0.0}
                f.write(struct.pack('f', float(pos.get('z', 0.0))))

            # Write velocity arrays
            velocities = obj.get('velocity', [])
            for arr, key in [(velocities, 'x'), (velocities, 'y'), (velocities, 'z')]:
                for i in range(trajectory_length):
                    vel = arr[i] if i < len(arr) else {'x': 0.0, 'y': 0.0, 'z': 0.0}
                    f.write(struct.pack('f', float(vel.get(key, 0.0))))
            
            # Write heading and valid arrays
            headings = obj.get('heading', [])
            f.write(struct.pack(f'{trajectory_length}f', *[float(headings[i]) if i < len(headings) else 0.0 for i in range(trajectory_length)]))
            
            valids = obj.get('valid', [])
            f.write(struct.pack(f'{trajectory_length}i', *[int(valids[i]) if i < len(valids) else 0 for i in range(trajectory_length)]))
            
            # Write scalar fields
            f.write(struct.pack('f', float(obj.get('width', 0.0))))
            f.write(struct.pack('f', float(obj.get('length', 0.0))))
            f.write(struct.pack('f', float(obj.get('height', 0.0))))
            goal_pos = obj.get('goalPosition', {'x': 0, 'y': 0, 'z': 0})  # Get goalPosition object with default
            f.write(struct.pack('f', float(goal_pos.get('x', 0.0))))  # Get x value
            f.write(struct.pack('f', float(goal_pos.get('y', 0.0))))  # Get y value
            f.write(struct.pack('f', float(goal_pos.get('z', 0.0))))  # Get z value
            f.write(struct.pack('i', obj.get('mark_as_expert', 0)))
        
        # Write roads
        for idx, road in enumerate(map_data.get('roads', [])):
            geometry = road.get('geometry', [])
            road_type = road.get('map_element_id', 0)
            road_type_word = road.get('type', 0)
            if(road_type_word == "lane"):
                road_type = 2
            elif(road_type_word == "road_edge"):
                road_type = 15
            # breakpoint()
            if(len(geometry) > 10 and road_type <=16):
                geometry = simplify_polyline(geometry, .1)
            size = len(geometry)
            # breakpoint()
            if(road_type >=0 and road_type <=3):
                road_type = 4
            elif(road_type >=5 and road_type <=13):
                road_type = 5
            elif(road_type >=14 and road_type <=16):
                road_type = 6
            elif(road_type == 17):
                road_type = 7
            elif(road_type == 18):
                road_type = 8
            elif(road_type == 19):
                road_type = 9
            elif(road_type == 20):
                road_type = 10
            # Write base entity data
            f.write(struct.pack('i', road_type))  # type
            # f.write(struct.pack('i', road.get('id', 0)))    # id
            f.write(struct.pack('i', size))                 # array_size
            
            # Write position arrays
            for coord in ['x', 'y', 'z']:
                for point in geometry:
                    f.write(struct.pack('f', float(point.get(coord, 0.0))))
            # Write scalar fields
            f.write(struct.pack('f', float(road.get('width', 0.0))))
            f.write(struct.pack('f', float(road.get('length', 0.0))))
            f.write(struct.pack('f', float(road.get('height', 0.0))))
            goal_pos = road.get('goalPosition', {'x': 0, 'y': 0, 'z': 0})  # Get goalPosition object with default
            f.write(struct.pack('f', float(goal_pos.get('x', 0.0))))  # Get x value
            f.write(struct.pack('f', float(goal_pos.get('y', 0.0))))  # Get y value
            f.write(struct.pack('f', float(goal_pos.get('z', 0.0))))  # Get z value
            f.write(struct.pack('i', road.get('mark_as_expert', 0)))

def load_map(map_name, binary_output=None):
    """Loads a JSON map and optionally saves it as binary"""
    with open(map_name, 'r') as f:
        map_data = json.load(f)
    
    if binary_output:
        save_map_binary(map_data, binary_output)

def process_all_maps():
    """Process all maps and save them as binaries"""
    import os
    from pathlib import Path

    # Create the binaries directory if it doesn't exist
    binary_dir = Path("resources/drive/binaries")
    binary_dir.mkdir(parents=True, exist_ok=True)

    # Path to the training data
    data_dir = Path("data/processed_big/training")
    
    # Get all JSON files in the training directory
    json_files = sorted(data_dir.glob("*.json"))
    
    print(f"Found {len(json_files)} JSON files")
    
    # Process each JSON file
    for i, map_path in enumerate(json_files[:10000]):
        binary_file = f"map_{i:03d}.bin"  # Use zero-padded numbers for consistent sorting
        binary_path = binary_dir / binary_file
        
        print(f"Processing {map_path.name} -> {binary_file}")
        # try:
        load_map(str(map_path), str(binary_path))
        # except Exception as e:
        #     print(f"Error processing {map_path.name}: {e}")

def test_performance(timeout=10, atn_cache=1024, num_agents=1024):
    import time

    env = Drive(num_agents=num_agents)
    env.reset()
    tick = 0
    num_agents = 1024
    actions = np.stack([
        np.random.randint(0, space.n + 1, (atn_cache, num_agents))
        for space in env.single_action_space
    ], axis=-1)

    start = time.time()
    while time.time() - start < timeout:
        atn = actions[tick % atn_cache]         
        env.step(atn)
        tick += 1

    print(f'SPS: {num_agents * tick / (time.time() - start)}')
    env.close()
if __name__ == '__main__':
    # test_performance()
    process_all_maps()



================================================
FILE: pufferlib/ocean/drone_race/binding.c
================================================
#include "drone_race.h"

#define Env DroneRace
#include "../env_binding.h"

static int my_init(Env *env, PyObject *args, PyObject *kwargs) {
    env->max_rings = unpack(kwargs, "max_rings");
    env->max_moves = unpack(kwargs, "max_moves");
    init(env);
    return 0;
}

static int my_log(PyObject *dict, Log *log) {
    assign_to_dict(dict, "perf", log->perf);
    assign_to_dict(dict, "score", log->score);
    assign_to_dict(dict, "collision_rate", log->collision_rate);
    assign_to_dict(dict, "oob", log->oob);
    assign_to_dict(dict, "timeout", log->timeout);
    assign_to_dict(dict, "episode_return", log->episode_return);
    assign_to_dict(dict, "episode_length", log->episode_length);
    assign_to_dict(dict, "n", log->n);
    return 0;
}



================================================
FILE: pufferlib/ocean/drone_race/drone_race.c
================================================
// Standalone C demo for DroneRace environment
// Compile using: ./scripts/build_ocean.sh drone [local|fast]
// Run with: ./drone

#include "drone_race.h"
#include "puffernet.h"
#include <time.h>

#ifdef __EMSCRIPTEN__
#include <emscripten.h>
#endif

double randn(double mean, double std) {
    static int has_spare = 0;
    static double spare;

    if (has_spare) {
        has_spare = 0;
        return mean + std * spare;
    }

    has_spare = 1;
    double u, v, s;
    do {
        u = 2.0 * rand() / RAND_MAX - 1.0;
        v = 2.0 * rand() / RAND_MAX - 1.0;
        s = u * u + v * v;
    } while (s >= 1.0 || s == 0.0);

    s = sqrt(-2.0 * log(s) / s);
    spare = v * s;
    return mean + std * (u * s);
}

typedef struct LinearContLSTM LinearContLSTM;
struct LinearContLSTM {
    int num_agents;
    float *obs;
    float *log_std;
    Linear *encoder;
    GELU *gelu1;
    LSTM *lstm;
    Linear *actor;
    Linear *value_fn;
    int num_actions;
};

LinearContLSTM *make_linearcontlstm(Weights *weights, int num_agents, int input_dim,
                                    int logit_sizes[], int num_actions) {
    LinearContLSTM *net = calloc(1, sizeof(LinearContLSTM));
    net->num_agents = num_agents;
    net->obs = calloc(num_agents * input_dim, sizeof(float));
    net->num_actions = logit_sizes[0];
    net->log_std = weights->data;
    weights->idx += net->num_actions;
    net->encoder = make_linear(weights, num_agents, input_dim, 128);
    net->gelu1 = make_gelu(num_agents, 128);
    int atn_sum = 0;
    for (int i = 0; i < num_actions; i++) {
        atn_sum += logit_sizes[i];
    }
    net->actor = make_linear(weights, num_agents, 128, atn_sum);
    net->value_fn = make_linear(weights, num_agents, 128, 1);
    net->lstm = make_lstm(weights, num_agents, 128, 128);
    return net;
}

void free_linearcontlstm(LinearContLSTM *net) {
    free(net->obs);
    free(net->encoder);
    free(net->gelu1);
    free(net->actor);
    free(net->value_fn);
    free(net->lstm);
    free(net);
}

void forward_linearcontlstm(LinearContLSTM *net, float *observations, float *actions) {
    linear(net->encoder, observations);
    gelu(net->gelu1, net->encoder->output);
    lstm(net->lstm, net->gelu1->output);
    linear(net->actor, net->lstm->state_h);
    linear(net->value_fn, net->lstm->state_h);
    for (int i = 0; i < net->num_actions; i++) {
        float std = expf(net->log_std[i]);
        float mean = net->actor->output[i];
        actions[i] = randn(mean, std);
    }
}

void generate_dummy_actions(DroneRace *env) {
    // Generate random floats in [-1, 1] range
    env->actions[0] = ((float)rand() / (float)RAND_MAX) * 2.0f - 1.0f;
    env->actions[1] = ((float)rand() / (float)RAND_MAX) * 2.0f - 1.0f;
    env->actions[2] = ((float)rand() / (float)RAND_MAX) * 2.0f - 1.0f;
    env->actions[3] = ((float)rand() / (float)RAND_MAX) * 2.0f - 1.0f;
}

#ifdef __EMSCRIPTEN__
typedef struct {
    DroneRace *env;
    LinearContLSTM *net;
    Weights *weights;
} WebRenderArgs;

void emscriptenStep(void *e) {
    WebRenderArgs *args = (WebRenderArgs *)e;
    DroneRace *env = args->env;
    LinearContLSTM *net = args->net;

    forward_linearcontlstm(net, env->observations, env->actions);
    c_step(env);
    c_render(env);
    return;
}

WebRenderArgs *web_args = NULL;
#endif

int main() {
    srand(time(NULL)); // Seed random number generator

    DroneRace *env = calloc(1, sizeof(DroneRace));
    env->max_moves = 1000;
    env->max_rings = 10;

    size_t obs_size = 25;
    size_t act_size = 4;
    env->observations = (float *)calloc(obs_size, sizeof(float));
    env->actions = (float *)calloc(act_size, sizeof(float));
    env->rewards = (float *)calloc(1, sizeof(float));
    env->terminals = (unsigned char *)calloc(1, sizeof(float));

    Weights *weights = load_weights("resources/drone/drone_weights.bin", 136073);
    int logit_sizes[1] = {4};
    LinearContLSTM *net = make_linearcontlstm(weights, 1, 25, logit_sizes, 1);

    if (!env->observations || !env->actions || !env->rewards) {
        fprintf(stderr, "ERROR: Failed to allocate memory for demo buffers.\n");
        free(env->observations);
        free(env->actions);
        free(env->rewards);
        free(env->terminals);
        free(env);
        return 0;
    }

    init(env);
    c_reset(env);

#ifdef __EMSCRIPTEN__
    WebRenderArgs *args = calloc(1, sizeof(WebRenderArgs));
    args->env = env;
    args->net = net;
    args->weights = weights;
    web_args = args;

    emscripten_set_main_loop_arg(emscriptenStep, args, 0, true);
#else
    c_render(env);

    while (!WindowShouldClose()) {
        forward_linearcontlstm(net, env->observations, env->actions);
        c_step(env);
        c_render(env);
    }

    c_close(env);
    free_linearcontlstm(net);
    free(env->observations);
    free(env->actions);
    free(env->rewards);
    free(env->terminals);
    free(env);
#endif

    return 0;
}



================================================
FILE: pufferlib/ocean/drone_race/drone_race.h
================================================
// Originally made by Sam Turner and Finlay Sanders, 2025.
// Included in pufferlib under the original project's MIT license.
// https://github.com/stmio/drone

#include <float.h>
#include <math.h>
#include <stdbool.h>
#include <stdio.h>
#include <stdlib.h>
#include <string.h>
#include <time.h>

#include "raylib.h"
#include "dronelib.h"

typedef struct Client Client;
struct Client {
    Camera3D camera;
    float width;
    float height;

    float camera_distance;
    float camera_azimuth;
    float camera_elevation;
    bool is_dragging;
    Vector2 last_mouse_pos;

    Trail trail;
};

typedef struct DroneRace DroneRace;
struct DroneRace {
    float *observations;
    float *actions;
    float *rewards;
    unsigned char *terminals;

    Log log;
    int tick;
    int report_interval;
    int score;
    float episodic_return;

    int max_rings;
    int ring_idx;
    Ring *ring_buffer;

    int max_moves;
    int moves_left;

    Drone drone;
    Client *client;
};

void init(DroneRace *env) {
    env->log = (Log){0};
    env->tick = 0;
    env->ring_buffer = (Ring*)malloc((env->max_rings) * sizeof(Ring));
}

void add_log(DroneRace *env, float oob, float collision, float timeout) {
    env->log.score += env->score;
    env->log.episode_return += env->episodic_return;
    env->log.episode_length += env->tick;
    env->log.perf += (float)env->ring_idx / (float)env->max_rings;
    env->log.oob += oob;
    env->log.collision_rate += collision;
    env->log.timeout += timeout;
    env->log.n += 1.0f;
}

void compute_observations(DroneRace *env) {
    Drone *drone = &env->drone;

    Quat q_inv = quat_inverse(drone->state.quat);
    Ring curr_ring = env->ring_buffer[env->ring_idx];
    Ring next_ring = env->ring_buffer[env->ring_idx % env->max_rings];

    Vec3 to_curr_ring = quat_rotate(q_inv, sub3(curr_ring.pos, drone->state.pos));
    Vec3 to_next_ring = quat_rotate(q_inv, sub3(next_ring.pos, drone->state.pos));

    Vec3 curr_ring_norm = quat_rotate(q_inv, curr_ring.normal);
    Vec3 next_ring_norm = quat_rotate(q_inv, next_ring.normal);

    Vec3 linear_vel_body = quat_rotate(q_inv, drone->state.vel);
    Vec3 drone_up_world = quat_rotate(drone->state.quat, (Vec3){0.0f, 0.0f, 1.0f});

    env->observations[0] = to_curr_ring.x / GRID_X;
    env->observations[1] = to_curr_ring.y / GRID_Y;
    env->observations[2] = to_curr_ring.z / GRID_Z;

    env->observations[3] = curr_ring_norm.x;
    env->observations[4] = curr_ring_norm.y;
    env->observations[5] = curr_ring_norm.z;

    env->observations[6] = to_next_ring.x / GRID_X;
    env->observations[7] = to_next_ring.y / GRID_Y;
    env->observations[8] = to_next_ring.z / GRID_Z;

    env->observations[9] = next_ring_norm.x;
    env->observations[10] = next_ring_norm.y;
    env->observations[11] = next_ring_norm.z;

    env->observations[12] = linear_vel_body.x / drone->params.max_vel;
    env->observations[13] = linear_vel_body.y / drone->params.max_vel;
    env->observations[14] = linear_vel_body.z / drone->params.max_vel;

    env->observations[15] = drone->state.omega.x / drone->params.max_omega;
    env->observations[16] = drone->state.omega.y / drone->params.max_omega;
    env->observations[17] = drone->state.omega.z / drone->params.max_omega;

    env->observations[18] = drone_up_world.x;
    env->observations[19] = drone_up_world.y;
    env->observations[20] = drone_up_world.z;

    env->observations[21] = drone->state.quat.w;
    env->observations[22] = drone->state.quat.x;
    env->observations[23] = drone->state.quat.y;
    env->observations[24] = drone->state.quat.z;

    env->observations[25] = drone->state.rpms[0] / drone->params.max_rpm;
    env->observations[26] = drone->state.rpms[1] / drone->params.max_rpm;
    env->observations[27] = drone->state.rpms[2] / drone->params.max_rpm;
    env->observations[28] = drone->state.rpms[3] / drone->params.max_rpm;
}

void c_reset(DroneRace *env) {
    env->tick = 0;
    env->score = 0;
    env->episodic_return = 0.0f;
    env->moves_left = env->max_moves;

    // creates rings
    env->ring_idx = 0;
    float ring_radius = 2.0f;
    reset_rings(env->ring_buffer, env->max_rings, ring_radius);

    // creates drone
    Drone *drone = &env->drone;
    float size = rndf(0.05f, 0.8f);
    init_drone(drone, size, 0.1f);

    do {
        drone->state.pos = (Vec3){
            rndf(-MARGIN_X, MARGIN_X), 
            rndf(-MARGIN_Y, MARGIN_Y), 
            rndf(-MARGIN_Z, MARGIN_Z)
        };
    } while (norm3(sub3(drone->state.pos, env->ring_buffer[0].pos)) < 2.0f*ring_radius);

    drone->prev_pos = drone->state.pos;

    compute_observations(env);
}

void c_step(DroneRace *env) {
    env->tick++;
    env->rewards[0] = 0;
    env->terminals[0] = 0;
    env->log.score = 0;

    Drone *drone = &env->drone;
    move_drone(drone, env->actions);

    // check out of bounds
    bool out_of_bounds = drone->state.pos.x < -GRID_X || drone->state.pos.x > GRID_X ||
                         drone->state.pos.y < -GRID_Y || drone->state.pos.y > GRID_Y ||
                         drone->state.pos.z < -GRID_Z || drone->state.pos.z > GRID_Z;

    if (out_of_bounds) {
        env->rewards[0] -= 1;
        env->episodic_return -= 1;
        env->terminals[0] = 1;
        add_log(env, 1.0f, 0.0f, 0.0f);
        c_reset(env);
        compute_observations(env);
        return;
    }

    // check for passing ring
    Ring *ring = &env->ring_buffer[env->ring_idx];
    float reward = check_ring(drone, ring);
    env->rewards[0] += reward;
    env->episodic_return += reward;

    if (reward > 0) {
        env->score++;
        env->ring_idx++;
    } else if (reward < 0) {
        env->terminals[0] = 1;
        add_log(env, 0.0f, 1.0f, 0.0f);
        c_reset(env);
        return;
    }

    // truncate
    env->moves_left -= 1;
    if (env->moves_left == 0 || env->ring_idx == env->max_rings) {
        env->terminals[0] = 1;
        add_log(env, 0.0f, 0.0f, env->moves_left == 0 ? 1.0f : 0.0f);
        c_reset(env);
        return;
    }

    drone->prev_pos = drone->state.pos;

    compute_observations(env);
}

void c_close_client(Client *client) {
    CloseWindow();
    free(client);
}

void c_close(DroneRace *env) {
    free(env->ring_buffer);

    if (env->client != NULL) {
        c_close_client(env->client);
    }
}

static void update_camera_position(Client *c) {
    float r = c->camera_distance;
    float az = c->camera_azimuth;
    float el = c->camera_elevation;

    float x = r * cosf(el) * cosf(az);
    float y = r * cosf(el) * sinf(az);
    float z = r * sinf(el);

    c->camera.position = (Vector3){x, y, z};
    c->camera.target = (Vector3){0, 0, 0};
}

void handle_camera_controls(Client *client) {
    Vector2 mouse_pos = GetMousePosition();

    if (IsMouseButtonPressed(MOUSE_BUTTON_LEFT)) {
        client->is_dragging = true;
        client->last_mouse_pos = mouse_pos;
    }

    if (IsMouseButtonReleased(MOUSE_BUTTON_LEFT)) {
        client->is_dragging = false;
    }

    if (client->is_dragging && IsMouseButtonDown(MOUSE_BUTTON_LEFT)) {
        Vector2 mouse_delta = {mouse_pos.x - client->last_mouse_pos.x,
                               mouse_pos.y - client->last_mouse_pos.y};

        float sensitivity = 0.005f;

        client->camera_azimuth -= mouse_delta.x * sensitivity;

        client->camera_elevation += mouse_delta.y * sensitivity;
        client->camera_elevation =
            clampf(client->camera_elevation, -PI / 2.0f + 0.1f, PI / 2.0f - 0.1f);

        client->last_mouse_pos = mouse_pos;

        update_camera_position(client);
    }

    float wheel = GetMouseWheelMove();
    if (wheel != 0) {
        client->camera_distance -= wheel * 2.0f;
        client->camera_distance = clampf(client->camera_distance, 5.0f, 50.0f);
        update_camera_position(client);
    }
}

Client *make_client(DroneRace *env) {
    Client *client = (Client *)calloc(1, sizeof(Client));

    client->width = WIDTH;
    client->height = HEIGHT;

    SetConfigFlags(FLAG_MSAA_4X_HINT); // antialiasing
    InitWindow(WIDTH, HEIGHT, "PufferLib DroneRace");

#ifndef __EMSCRIPTEN__
    SetTargetFPS(60);
#endif

    if (!IsWindowReady()) {
        TraceLog(LOG_ERROR, "Window failed to initialize\n");
        free(client);
        return NULL;
    }

    client->camera_distance = 40.0f;
    client->camera_azimuth = 0.0f;
    client->camera_elevation = PI / 10.0f;
    client->is_dragging = false;
    client->last_mouse_pos = (Vector2){0.0f, 0.0f};

    client->camera.up = (Vector3){0.0f, 0.0f, 1.0f};
    client->camera.fovy = 45.0f;
    client->camera.projection = CAMERA_PERSPECTIVE;

    update_camera_position(client);

    client->trail.index = 0;
    client->trail.count = 0;
    for (int j = 0; j < TRAIL_LENGTH; j++) {
        client->trail.pos[j] = env->drone.state.pos;
    }

    return client;
}

void DrawRing3D(Ring ring, float thickness, Color entryColor, Color exitColor) {
    float half_thick = thickness / 2.0f;

    Vector3 center_pos = {ring.pos.x, ring.pos.y, ring.pos.z};

    Vector3 entry_start_pos = {center_pos.x - half_thick * ring.normal.x,
                               center_pos.y - half_thick * ring.normal.y,
                               center_pos.z - half_thick * ring.normal.z};

    DrawCylinderWiresEx(entry_start_pos, center_pos, ring.radius, ring.radius, 32, entryColor);

    Vector3 exit_end_pos = {center_pos.x + half_thick * ring.normal.x,
                            center_pos.y + half_thick * ring.normal.y,
                            center_pos.z + half_thick * ring.normal.z};

    DrawCylinderWiresEx(center_pos, exit_end_pos, ring.radius, ring.radius, 32, exitColor);
}

void c_render(DroneRace *env) {
    Drone *drone = &env->drone;
    if (env->client == NULL) {
        env->client = make_client(env);
        if (env->client == NULL) {
            TraceLog(LOG_ERROR, "Failed to initialize client for rendering\n");
            return;
        }
    }

    if (WindowShouldClose()) {
        c_close(env);
        exit(0);
    }

    if (IsKeyDown(KEY_ESCAPE)) {
        c_close(env);
        exit(0);
    }

    handle_camera_controls(env->client);

    Client *client = env->client;

    client->trail.pos[client->trail.index] = env->drone.state.pos;
    client->trail.index = (client->trail.index + 1) % TRAIL_LENGTH;
    if (client->trail.count < TRAIL_LENGTH) {
        client->trail.count++;
    }
    if (env->terminals[0]) {
        client->trail.index = 0;
        client->trail.count = 0;
    }    

    BeginDrawing();
    ClearBackground((Color){6, 24, 24, 255});

    BeginMode3D(client->camera);

    // draws bounding cube
    DrawCubeWires((Vector3){0.0f, 0.0f, 0.0f}, GRID_X * 2.0f, GRID_Y * 2.0f, GRID_Z * 2.0f,
                  WHITE);

    // draws drone body
    float r = drone->params.arm_len;
    DrawSphere((Vector3){drone->state.pos.x, drone->state.pos.y, drone->state.pos.z}, r/2.0f, RED);

    // draws rotors according to thrust
    float T[4];
    for (int i = 0; i < 4; i++) {
        float rpm = (env->actions[i] + 1.0f) * 0.5f * drone->params.max_rpm;
        T[i] = drone->params.k_thrust * rpm * rpm;
    }

    const float rotor_radius = r / 4.0f;
    const float visual_arm_len = 1.0f * drone->params.arm_len;

    Vec3 rotor_offsets_body[4] = {{+r, 0.0f, 0.0f},
                                  {-r, 0.0f, 0.0f},
                                  {0.0f, +r, 0.0f},
                                  {0.0f, -r, 0.0f}};

    Color base_colors[4] = {ORANGE, PURPLE, LIME, SKYBLUE};

    for (int i = 0; i < 4; i++) {
        Vec3 world_off = quat_rotate(drone->state.quat, rotor_offsets_body[i]);

        Vector3 rotor_pos = {drone->state.pos.x + world_off.x, drone->state.pos.y + world_off.y,
                             drone->state.pos.z + world_off.z};

        float rpm = (env->actions[i] + 1.0f) * 0.5f * drone->params.max_rpm;
        float intensity = 0.75f + 0.25f * (rpm / drone->params.max_rpm);

        Color rotor_color = (Color){(unsigned char)(base_colors[i].r * intensity),
                                    (unsigned char)(base_colors[i].g * intensity),
                                    (unsigned char)(base_colors[i].b * intensity), 255};

        DrawSphere(rotor_pos, rotor_radius, rotor_color);

        DrawCylinderEx((Vector3){drone->state.pos.x, drone->state.pos.y, drone->state.pos.z}, rotor_pos, 0.02f, 0.02f, 8,
                       BLACK);
    }

    // draws line with direction and magnitude of velocity / 10
    if (norm3(drone->state.vel) > 0.1f) {
        DrawLine3D((Vector3){drone->state.pos.x, drone->state.pos.y, drone->state.pos.z},
                   (Vector3){drone->state.pos.x + drone->state.vel.x * 0.1f, drone->state.pos.y + drone->state.vel.y * 0.1f,
                             drone->state.pos.z + drone->state.vel.z * 0.1f},
                   MAGENTA);
    }

    if (client->trail.count > 2) {
        for (int j = 0; j < client->trail.count - 1; j++) {
            int idx0 = (client->trail.index - j - 1 + TRAIL_LENGTH) % TRAIL_LENGTH;
            int idx1 = (client->trail.index - j - 2 + TRAIL_LENGTH) % TRAIL_LENGTH;
            float alpha = (float)(TRAIL_LENGTH - j) / (float)client->trail.count * 0.8f; // fade out
            Color trail_color = ColorAlpha((Color){0, 187, 187, 255}, alpha);
            DrawLine3D((Vector3){client->trail.pos[idx0].x, client->trail.pos[idx0].y, client->trail.pos[idx0].z},
                        (Vector3){client->trail.pos[idx1].x, client->trail.pos[idx1].y, client->trail.pos[idx1].z},
                        trail_color);
        }
    }

    // draws current and previous ring
    float ring_thickness = 0.2f;
    DrawRing3D(env->ring_buffer[env->ring_idx], ring_thickness, GREEN, BLUE);
    if (env->ring_idx > 0) {
        DrawRing3D(env->ring_buffer[env->ring_idx - 1], ring_thickness, GREEN, BLUE);
    }

    EndMode3D();

    // Draw 2D stats
    DrawText(TextFormat("Targets left: %d", env->max_rings - env->ring_idx), 10, 10, 20, WHITE);
    DrawText(TextFormat("Moves left: %d", env->moves_left), 10, 40, 20, WHITE);
    DrawText(TextFormat("Episode Return: %.2f", env->episodic_return), 10, 70, 20, WHITE);

    DrawText("Motor Thrusts:", 10, 110, 20, WHITE);
    DrawText(TextFormat("Front: %.3f", T[0]), 10, 135, 18, ORANGE);
    DrawText(TextFormat("Back:  %.3f", T[1]), 10, 155, 18, PURPLE);
    DrawText(TextFormat("Right: %.3f", T[2]), 10, 175, 18, LIME);
    DrawText(TextFormat("Left:  %.3f", T[3]), 10, 195, 18, SKYBLUE);

    DrawText(TextFormat("Pos: (%.1f, %.1f, %.1f)", drone->state.pos.x, drone->state.pos.y, drone->state.pos.z), 10, 225, 18,
             WHITE);
    DrawText(TextFormat("Vel: %.2f m/s", norm3(drone->state.vel)), 10, 245, 18, WHITE);

    DrawText("Left click + drag: Rotate camera", 10, 275, 16, LIGHTGRAY);
    DrawText("Mouse wheel: Zoom in/out", 10, 295, 16, LIGHTGRAY);

    EndDrawing();
}



================================================
FILE: pufferlib/ocean/drone_race/drone_race.py
================================================
import numpy as np
import gymnasium

import pufferlib
from pufferlib.ocean.drone_race import binding

class DroneRace(pufferlib.PufferEnv):
    def __init__(
        self,
        num_envs=16,
        render_mode=None,
        report_interval=1,
        buf=None,
        seed=0,
        max_rings=10,
        max_moves=1000,
    ):
        self.single_observation_space = gymnasium.spaces.Box(
            low=-1,
            high=1,
            shape=(29,),
            dtype=np.float32,
        )

        self.single_action_space = gymnasium.spaces.Box(
            low=-1, high=1, shape=(4,), dtype=np.float32
        )

        self.num_agents = num_envs
        self.render_mode = render_mode
        self.report_interval = report_interval
        self.tick = 0

        super().__init__(buf)
        self.actions = self.actions.astype(np.float32)

        c_envs = []
        for env_num in range(num_envs):
            c_envs.append(binding.env_init(
                self.observations[env_num:(env_num+1)],
                self.actions[env_num:(env_num+1)],
                self.rewards[env_num:(env_num+1)],
                self.terminals[env_num:(env_num+1)],
                self.truncations[env_num:(env_num+1)],
                env_num,
                report_interval=self.report_interval,
                max_rings=max_rings,
                max_moves=max_moves,
            ))

        self.c_envs = binding.vectorize(*c_envs)

    def reset(self, seed=None):
        self.tick = 0
        binding.vec_reset(self.c_envs, seed)
        return self.observations, []

    def step(self, actions):
        self.actions[:] = actions

        self.tick += 1
        binding.vec_step(self.c_envs)

        info = []
        if self.tick % self.report_interval == 0:
            log_data = binding.vec_log(self.c_envs)
            if log_data:
                info.append(log_data)

        return (self.observations, self.rewards, self.terminals, self.truncations, info)

    def render(self):
        binding.vec_render(self.c_envs, 0)

    def close(self):
        binding.vec_close(self.c_envs)

def test_performance(timeout=10, atn_cache=1024):
    env = DroneRace(num_envs=1000)
    env.reset()
    tick = 0

    actions = [env.action_space.sample() for _ in range(atn_cache)]

    import time
    start = time.time()
    while time.time() - start < timeout:
        atn = actions[tick % atn_cache]
        env.step(atn)
        tick += 1

    print(f"SPS: {env.num_agents * tick / (time.time() - start)}")

if __name__ == "__main__":
    test_performance()



================================================
FILE: pufferlib/ocean/drone_race/dronelib.h
================================================
// Originally made by Sam Turner and Finlay Sanders, 2025.
// Included in pufferlib under the original project's MIT license.
// https://github.com/stmio/drone

#include <float.h>
#include <math.h>
#include <stdbool.h>
#include <stdio.h>
#include <stdlib.h>
#include <string.h>
#include <time.h>

#include "raylib.h"

// Visualisation properties
#define WIDTH 1080
#define HEIGHT 720
#define TRAIL_LENGTH 50
#define HORIZON 1024

// Physical constants for the drone
#define BASE_MASS 1.0f       // kg
#define BASE_IXX 0.01f       // kgm^2
#define BASE_IYY 0.01f       // kgm^2
#define BASE_IZZ 0.02f       // kgm^2
#define BASE_ARM_LEN 0.1f    // m
#define BASE_K_THRUST 3e-5f  // thrust coefficient
#define BASE_K_ANG_DAMP 0.2f // angular damping coefficient
#define BASE_K_DRAG 1e-6f    // drag (torque) coefficient
#define BASE_B_DRAG 0.1f     // linear drag coefficient
#define BASE_GRAVITY 9.81f   // m/s^2
#define BASE_MAX_RPM 750.0f  // rad/s
#define BASE_MAX_VEL 50.0f   // m/s
#define BASE_MAX_OMEGA 50.0f // rad/s
#define BASE_K_MOT 0.1f      // s (Motor lag constant)
#define BASE_J_MOT 1e-5f     // kgm^2 (Motor rotational inertia)

// Simulation properties
#define GRID_X 10.0f
#define GRID_Y 10.0f
#define GRID_Z 10.0f
#define MARGIN_X (GRID_X - 1)
#define MARGIN_Y (GRID_Y - 1)
#define MARGIN_Z (GRID_Z - 1)
#define V_TARGET 0.05f
#define DT 0.05f
#define DT_RNG 0.0f

// Corner to corner distance
#define MAX_DIST sqrtf((2*GRID_X)*(2*GRID_X) + (2*GRID_Y)*(2*GRID_Y) + (2*GRID_Z)*(2*GRID_Z))

typedef struct Log Log;
struct Log {
    float episode_return;
    float episode_length;
    float rings_passed;
    float collision_rate;
    float oob;
    float timeout;
    float score;
    float perf;
    float n;
};

typedef struct {
    float w, x, y, z;
} Quat;

typedef struct {
    float x, y, z;
} Vec3;

static inline float clampf(float v, float min, float max) {
    if (v < min)
        return min;
    if (v > max)
        return max;
    return v;
}

static inline float rndf(float a, float b) {
    return a + ((float)rand() / (float)RAND_MAX) * (b - a);
}

static inline Vec3 add3(Vec3 a, Vec3 b) { return (Vec3){a.x + b.x, a.y + b.y, a.z + b.z}; }

static inline Quat add_quat(Quat a, Quat b) { return (Quat){a.w + b.w, a.x + b.x, a.y + b.y, a.z + b.z}; }

static inline Vec3 sub3(Vec3 a, Vec3 b) { return (Vec3){a.x - b.x, a.y - b.y, a.z - b.z}; }

static inline Vec3 scalmul3(Vec3 a, float b) { return (Vec3){a.x * b, a.y * b, a.z * b}; }

static inline Quat scalmul_quat(Quat a, float b) { return (Quat){a.w * b, a.x * b, a.y * b, a.z * b}; }

static inline float dot3(Vec3 a, Vec3 b) { return a.x * b.x + a.y * b.y + a.z * b.z; }

static inline float norm3(Vec3 a) { return sqrtf(dot3(a, a)); }

static inline void clamp3(Vec3 *vec, float min, float max) {
    vec->x = clampf(vec->x, min, max);
    vec->y = clampf(vec->y, min, max);
    vec->z = clampf(vec->z, min, max);
}

static inline void clamp4(float a[4], float min, float max) {
    a[0] = clampf(a[0], min, max);
    a[1] = clampf(a[1], min, max);
    a[2] = clampf(a[2], min, max);
    a[3] = clampf(a[3], min, max);
}

static inline Quat quat_mul(Quat q1, Quat q2) {
    Quat out;
    out.w = q1.w * q2.w - q1.x * q2.x - q1.y * q2.y - q1.z * q2.z;
    out.x = q1.w * q2.x + q1.x * q2.w + q1.y * q2.z - q1.z * q2.y;
    out.y = q1.w * q2.y - q1.x * q2.z + q1.y * q2.w + q1.z * q2.x;
    out.z = q1.w * q2.z + q1.x * q2.y - q1.y * q2.x + q1.z * q2.w;
    return out;
}

static inline void quat_normalize(Quat *q) {
    float n = sqrtf(q->w * q->w + q->x * q->x + q->y * q->y + q->z * q->z);
    if (n > 0.0f) {
        q->w /= n;
        q->x /= n;
        q->y /= n;
        q->z /= n;
    }
}

static inline Vec3 quat_rotate(Quat q, Vec3 v) {
    Quat qv = {0.0f, v.x, v.y, v.z};
    Quat tmp = quat_mul(q, qv);
    Quat q_conj = {q.w, -q.x, -q.y, -q.z};
    Quat res = quat_mul(tmp, q_conj);
    return (Vec3){res.x, res.y, res.z};
}

static inline Quat quat_inverse(Quat q) { return (Quat){q.w, -q.x, -q.y, -q.z}; }

Quat rndquat() {
    float u1 = rndf(0.0f, 1.0f);
    float u2 = rndf(0.0f, 1.0f);
    float u3 = rndf(0.0f, 1.0f);

    float sqrt_1_minus_u1 = sqrtf(1.0f - u1);
    float sqrt_u1 = sqrtf(u1);

    float pi_2_u2 = 2.0f * M_PI * u2;
    float pi_2_u3 = 2.0f * M_PI * u3;

    Quat q;
    q.w = sqrt_1_minus_u1 * sinf(pi_2_u2);
    q.x = sqrt_1_minus_u1 * cosf(pi_2_u2);
    q.y = sqrt_u1 * sinf(pi_2_u3);
    q.z = sqrt_u1 * cosf(pi_2_u3);

    return q;
}

typedef struct {
    Vec3 pos;
    Quat orientation;
    Vec3 normal;
    float radius;
} Ring;

Ring rndring(float radius) {
    Ring ring;

    ring.pos.x = rndf(-GRID_X + 2*radius, GRID_X - 2*radius);
    ring.pos.y = rndf(-GRID_Y + 2*radius, GRID_Y - 2*radius);
    ring.pos.z = rndf(-GRID_Z + 2*radius, GRID_Z - 2*radius);

    ring.orientation = rndquat();

    Vec3 base_normal = {0.0f, 0.0f, 1.0f};
    ring.normal = quat_rotate(ring.orientation, base_normal);

    ring.radius = radius;

    return ring;
}

typedef struct {
    Vec3 pos[TRAIL_LENGTH];
    int index;
    int count;
} Trail;

typedef struct {
    Vec3 pos;      // global position (x, y, z)
    Vec3 vel;      // linear velocity (u, v, w)
    Quat quat;     // roll/pitch/yaw (phi/theta/psi) as a quaternion
    Vec3 omega;    // angular velocity (p, q, r)
    float rpms[4]; // motor RPMs
} State;

typedef struct {
    Vec3 vel;       // Derivative of position
    Vec3 v_dot;       // Derivative of velocity
    Quat q_dot;       // Derivative of quaternion
    Vec3 w_dot;       // Derivative of angular velocity
    float rpm_dot[4]; // Derivative of motor RPMs
} StateDerivative;

typedef struct {
    // Physical properties. Modeled as part of the drone
    // to make domain randomization easier.
    float mass; // kg
    float ixx; // kgm^2
    float iyy; // kgm^2
    float izz; // kgm^2
    float arm_len; // m
    float k_thrust; // thrust coefficient
    float k_ang_damp; // angular damping coefficient
    float k_drag; // drag (torque) coefficient
    float b_drag; // linear drag coefficient
    float gravity; // m/s^2
    float max_rpm; // rad/s
    float max_vel; // m/s
    float max_omega; // rad/s
    float k_mot; // s
    float j_mot; // kgm^2
} Params;

typedef struct {
    // core state and parameters
    State state;
    Params params;

    // helpers for ring/swarm logic
    Vec3 spawn_pos;
    Vec3 prev_pos;
    Vec3 target_pos;
    Vec3 target_vel;

    // logging utils
    float last_abs_reward;
    float last_target_reward;
    float last_collision_reward;
    float episode_return;
    float collisions;
    int episode_length;
    float score;
    int ring_idx;
} Drone;


void init_drone(Drone* drone, float size, float dr) {
    drone->params.arm_len = size / 2.0f;

    // m ~ x^3
    float mass_scale = powf(drone->params.arm_len, 3.0f) / powf(BASE_ARM_LEN, 3.0f);
    drone->params.mass = BASE_MASS * mass_scale * rndf(1.0f - dr, 1.0f + dr);

    // I ~ mx^2
    float base_Iscale = BASE_MASS * BASE_ARM_LEN * BASE_ARM_LEN;
    float I_scale = drone->params.mass * powf(drone->params.arm_len, 2.0f) / base_Iscale;
    drone->params.ixx = BASE_IXX * I_scale * rndf(1.0f - dr, 1.0f + dr);
    drone->params.iyy = BASE_IYY * I_scale * rndf(1.0f - dr, 1.0f + dr);
    drone->params.izz = BASE_IZZ * I_scale * rndf(1.0f - dr, 1.0f + dr);

    // k_thrust ~ m/l
    float k_thrust_scale = (drone->params.mass * drone->params.arm_len) / (BASE_MASS * BASE_ARM_LEN);
    drone->params.k_thrust = BASE_K_THRUST * k_thrust_scale * rndf(1.0f - dr, 1.0f + dr);

    // k_ang_damp ~ I
    float base_avg_inertia = (BASE_IXX + BASE_IYY + BASE_IZZ) / 3.0f;
    float avg_inertia = (drone->params.ixx + drone->params.iyy + drone->params.izz) / 3.0f;
    float avg_inertia_scale = avg_inertia / base_avg_inertia;
    drone->params.k_ang_damp = BASE_K_ANG_DAMP * avg_inertia_scale * rndf(1.0f - dr, 1.0f + dr);

    // drag ~ x^2
    float drag_scale = powf(drone->params.arm_len, 2.0f) / powf(BASE_ARM_LEN, 2.0f);
    drone->params.k_drag = BASE_K_DRAG * drag_scale * rndf(1.0f - dr, 1.0f + dr);
    drone->params.b_drag = BASE_B_DRAG * drag_scale * rndf(1.0f - dr, 1.0f + dr);

    // Small gravity randomization
    drone->params.gravity = BASE_GRAVITY * rndf(0.99f, 1.01f);

    // RPM ~ 1/x
    float rpm_scale = (BASE_ARM_LEN) / (drone->params.arm_len);
    drone->params.max_rpm = BASE_MAX_RPM * rpm_scale * rndf(1.0f - dr, 1.0f + dr);

    drone->params.max_vel = BASE_MAX_VEL;
    drone->params.max_omega = BASE_MAX_OMEGA;

    drone->params.k_mot = BASE_K_MOT * rndf(1.0f - dr, 1.0f + dr);
    drone->params.j_mot = BASE_J_MOT * I_scale * rndf(1.0f - dr, 1.0f + dr);
    
    for (int i = 0; i < 4; i++) {
        drone->state.rpms[i] = 0.0f;
    }
    drone->state.pos = (Vec3){0.0f, 0.0f, 0.0f};
    drone->prev_pos = drone->state.pos;
    drone->state.vel = (Vec3){0.0f, 0.0f, 0.0f};
    drone->state.omega = (Vec3){0.0f, 0.0f, 0.0f};
    drone->state.quat = (Quat){1.0f, 0.0f, 0.0f, 0.0f};
}

void compute_derivatives(State* state, Params* params, float* actions, StateDerivative* derivatives) {
    // first order rpm lag
    float target_rpms[4];
    for (int i = 0; i < 4; i++) {
        target_rpms[i] = (actions[i] + 1.0f) * 0.5f * params->max_rpm;
    }

    // rpm rates
    float rpm_dot[4];
    for (int i = 0; i < 4; i++) {
        rpm_dot[i] = (1.0f / params->k_mot) * (target_rpms[i] - state->rpms[i]);
    }

    // motor thrusts
    float T[4];
    for (int i = 0; i < 4; i++) {
        T[i] = params->k_thrust * powf(state->rpms[i], 2.0f);
    }

    // body frame net force
    Vec3 F_prop_body = {0.0f, 0.0f, T[0] + T[1] + T[2] + T[3]};

    // body frame force -> world frame force
    Vec3 F_prop = quat_rotate(state->quat, F_prop_body);

    // world frame linear drag
    Vec3 F_aero;
    F_aero.x = -params->b_drag * state->vel.x;
    F_aero.y = -params->b_drag * state->vel.y;
    F_aero.z = -params->b_drag * state->vel.z;

    // velocity rates, a = F/m
    Vec3 v_dot;
    v_dot.x = (F_prop.x + F_aero.x) / params->mass;
    v_dot.y = (F_prop.y + F_aero.y) / params->mass;
    v_dot.z = ((F_prop.z + F_aero.z) / params->mass) - params->gravity;

    // quaternion rates
    Quat omega_q = {0.0f, state->omega.x, state->omega.y, state->omega.z};
    Quat q_dot = quat_mul(state->quat, omega_q);
    q_dot.w *= 0.5f;
    q_dot.x *= 0.5f;
    q_dot.y *= 0.5f;
    q_dot.z *= 0.5f;

    // body frame torques
    Vec3 Tau_prop;
    Tau_prop.x = params->arm_len*(T[1] - T[3]);
    Tau_prop.y = params->arm_len*(T[2] - T[0]);
    Tau_prop.z = params->k_drag*(T[0] - T[1] + T[2] - T[3]);

    // torque from chaging motor speeds
    float Tau_mot_z = params->j_mot * (rpm_dot[0] - rpm_dot[1] + rpm_dot[2] - rpm_dot[3]);

    // torque from angular damping
    Vec3 Tau_aero;
    Tau_aero.x = -params->k_ang_damp * state->omega.x;
    Tau_aero.y = -params->k_ang_damp * state->omega.y;
    Tau_aero.z = -params->k_ang_damp * state->omega.z;
    
    // gyroscopic torque
    Vec3 Tau_iner;
    Tau_iner.x = (params->iyy - params->izz) * state->omega.y * state->omega.z;
    Tau_iner.y = (params->izz - params->ixx) * state->omega.z * state->omega.x;
    Tau_iner.z = (params->ixx - params->iyy) * state->omega.x * state->omega.y;

    // angular velocity rates
    Vec3 w_dot;
    w_dot.x = (Tau_prop.x + Tau_aero.x + Tau_iner.x) / params->ixx;
    w_dot.y = (Tau_prop.y + Tau_aero.y + Tau_iner.y) / params->iyy;
    w_dot.z = (Tau_prop.z + Tau_aero.z + Tau_iner.z + Tau_mot_z) / params->izz;

    derivatives->vel = state->vel;
    derivatives->v_dot = v_dot;
    derivatives->q_dot = q_dot;
    derivatives->w_dot = w_dot;
    for (int i = 0; i < 4; i++) {
        derivatives->rpm_dot[i] = rpm_dot[i];
    }
}

static void step(State* initial, StateDerivative* deriv, float dt, State* output) {
    output->pos = add3(initial->pos, scalmul3(deriv->vel, dt));
    output->vel = add3(initial->vel, scalmul3(deriv->v_dot, dt));
    output->quat = add_quat(initial->quat, scalmul_quat(deriv->q_dot, dt));
    output->omega = add3(initial->omega, scalmul3(deriv->w_dot, dt));
    for (int i = 0; i < 4; i++) {
        output->rpms[i] = initial->rpms[i] + deriv->rpm_dot[i] * dt;
    }
    quat_normalize(&output->quat);
}

void rk4_step(State* state, Params* params, float* actions, float dt) {
    StateDerivative k1, k2, k3, k4;
    State temp_state;

    compute_derivatives(state, params, actions, &k1);

    step(state, &k1, dt * 0.5f, &temp_state);
    compute_derivatives(&temp_state, params, actions, &k2);

    step(state, &k2, dt * 0.5f, &temp_state);
    compute_derivatives(&temp_state, params, actions, &k3);

    step(state, &k3, dt, &temp_state);
    compute_derivatives(&temp_state, params, actions, &k4);

    float dt_6 = dt / 6.0f;

    state->pos.x += (k1.vel.x + 2.0f * k2.vel.x + 2.0f * k3.vel.x + k4.vel.x) * dt_6;
    state->pos.y += (k1.vel.y + 2.0f * k2.vel.y + 2.0f * k3.vel.y + k4.vel.y) * dt_6;
    state->pos.z += (k1.vel.z + 2.0f * k2.vel.z + 2.0f * k3.vel.z + k4.vel.z) * dt_6;

    state->vel.x += (k1.v_dot.x + 2.0f * k2.v_dot.x + 2.0f * k3.v_dot.x + k4.v_dot.x) * dt_6;
    state->vel.y += (k1.v_dot.y + 2.0f * k2.v_dot.y + 2.0f * k3.v_dot.y + k4.v_dot.y) * dt_6;
    state->vel.z += (k1.v_dot.z + 2.0f * k2.v_dot.z + 2.0f * k3.v_dot.z + k4.v_dot.z) * dt_6;

    state->quat.w += (k1.q_dot.w + 2.0f * k2.q_dot.w + 2.0f * k3.q_dot.w + k4.q_dot.w) * dt_6;
    state->quat.x += (k1.q_dot.x + 2.0f * k2.q_dot.x + 2.0f * k3.q_dot.x + k4.q_dot.x) * dt_6;
    state->quat.y += (k1.q_dot.y + 2.0f * k2.q_dot.y + 2.0f * k3.q_dot.y + k4.q_dot.y) * dt_6;
    state->quat.z += (k1.q_dot.z + 2.0f * k2.q_dot.z + 2.0f * k3.q_dot.z + k4.q_dot.z) * dt_6;

    state->omega.x += (k1.w_dot.x + 2.0f * k2.w_dot.x + 2.0f * k3.w_dot.x + k4.w_dot.x) * dt_6;
    state->omega.y += (k1.w_dot.y + 2.0f * k2.w_dot.y + 2.0f * k3.w_dot.y + k4.w_dot.y) * dt_6;
    state->omega.z += (k1.w_dot.z + 2.0f * k2.w_dot.z + 2.0f * k3.w_dot.z + k4.w_dot.z) * dt_6;

    for (int i = 0; i < 4; i++) {
        state->rpms[i] += (k1.rpm_dot[i] + 2.0f * k2.rpm_dot[i] + 2.0f * k3.rpm_dot[i] + k4.rpm_dot[i]) * dt_6;
    }

    quat_normalize(&state->quat);
}

void move_drone(Drone* drone, float* actions) {
    // clamp actions
    clamp4(actions, -1.0f, 1.0f);

    // Domain randomized dt
    float dt = DT * rndf(1.0f - DT_RNG, 1.0 + DT_RNG);

    // update drone state
    drone->prev_pos = drone->state.pos;
    rk4_step(&drone->state, &drone->params, actions, dt);

    // clamp and normalise for observations
    clamp3(&drone->state.vel, -drone->params.max_vel, drone->params.max_vel);
    clamp3(&drone->state.omega, -drone->params.max_omega, drone->params.max_omega);
}

void reset_rings(Ring* ring_buffer, int num_rings, float ring_radius) {
    ring_buffer[0] = rndring(ring_radius);
    
    // ensure rings are spaced at least 2*ring_radius apart
    for (int i = 1; i < num_rings; i++) {
        do {
            ring_buffer[i] = rndring(ring_radius);
        }  while (norm3(sub3(ring_buffer[i].pos, ring_buffer[i - 1].pos)) < 2.0f*ring_radius);
    }   
}

float check_ring(Drone* drone, Ring* ring) {
    // previous dot product negative if on the 'entry' side of the ring's plane
    float prev_dot = dot3(sub3(drone->prev_pos, ring->pos), ring->normal);

    // new dot product positive if on the 'exit' side of the ring's plane
    float new_dot = dot3(sub3(drone->state.pos, ring->pos), ring->normal);

    bool valid_dir = (prev_dot < 0.0f && new_dot > 0.0f);
    bool invalid_dir = (prev_dot > 0.0f && new_dot < 0.0f);

    // if we have crossed the plane of the ring
    if (valid_dir || invalid_dir) {
        // find intesection with ring's plane
        Vec3 dir = sub3(drone->state.pos, drone->prev_pos);
        float t = -prev_dot / dot3(ring->normal, dir); // possible nan

        Vec3 intersection = add3(drone->prev_pos, scalmul3(dir, t));
        float dist = norm3(sub3(intersection, ring->pos));

        // reward or terminate based on distance to ring center
        if (dist < (ring->radius - 0.5) && valid_dir) {
            return 1.0f;
        } else if (dist < ring->radius + 0.5) {
            return -1.0f;
        }
    }
    return 0.0f;
}



================================================
FILE: pufferlib/ocean/drone_swarm/binding.c
================================================
#include "drone_swarm.h"

#define Env DroneSwarm
#include "../env_binding.h"

static int my_init(Env *env, PyObject *args, PyObject *kwargs) {
    env->num_agents = unpack(kwargs, "num_agents");
    env->max_rings = unpack(kwargs, "max_rings");
    init(env);
    return 0;
}

static int my_log(PyObject *dict, Log *log) {
    assign_to_dict(dict, "perf", log->perf);
    assign_to_dict(dict, "score", log->score);
    assign_to_dict(dict, "rings_passed", log->rings_passed);
    assign_to_dict(dict, "collision_rate", log->collision_rate);
    assign_to_dict(dict, "oob", log->oob);
    assign_to_dict(dict, "episode_return", log->episode_return);
    assign_to_dict(dict, "episode_length", log->episode_length);
    assign_to_dict(dict, "n", log->n);
    return 0;
}



================================================
FILE: pufferlib/ocean/drone_swarm/drone_swarm.c
================================================
// Standalone C demo for DroneSwarm environment
// Compile using: ./scripts/build_ocean.sh drone [local|fast]
// Run with: ./drone

#include "drone_swarm.h"
#include "puffernet.h"
#include <time.h>

#ifdef __EMSCRIPTEN__
#include <emscripten.h>
#endif

double randn(double mean, double std) {
    static int has_spare = 0;
    static double spare;

    if (has_spare) {
        has_spare = 0;
        return mean + std * spare;
    }

    has_spare = 1;
    double u, v, s;
    do {
        u = 2.0 * rand() / RAND_MAX - 1.0;
        v = 2.0 * rand() / RAND_MAX - 1.0;
        s = u * u + v * v;
    } while (s >= 1.0 || s == 0.0);

    s = sqrt(-2.0 * log(s) / s);
    spare = v * s;
    return mean + std * (u * s);
}

typedef struct LinearContLSTM LinearContLSTM;
struct LinearContLSTM {
    int num_agents;
    float *obs;
    float *log_std;
    Linear *encoder;
    GELU *gelu1;
    LSTM *lstm;
    Linear *actor;
    Linear *value_fn;
    int num_actions;
};

LinearContLSTM *make_linearcontlstm(Weights *weights, int num_agents, int input_dim,
                                    int logit_sizes[], int num_actions) {
    LinearContLSTM *net = calloc(1, sizeof(LinearContLSTM));
    net->num_agents = num_agents;
    net->obs = calloc(num_agents * input_dim, sizeof(float));
    net->num_actions = logit_sizes[0];
    net->log_std = weights->data;
    weights->idx += net->num_actions;
    net->encoder = make_linear(weights, num_agents, input_dim, 128);
    net->gelu1 = make_gelu(num_agents, 128);
    int atn_sum = 0;
    for (int i = 0; i < num_actions; i++) {
        atn_sum += logit_sizes[i];
    }
    net->actor = make_linear(weights, num_agents, 128, atn_sum);
    net->value_fn = make_linear(weights, num_agents, 128, 1);
    net->lstm = make_lstm(weights, num_agents, 128, 128);
    return net;
}

void free_linearcontlstm(LinearContLSTM *net) {
    free(net->obs);
    free(net->encoder);
    free(net->gelu1);
    free(net->actor);
    free(net->value_fn);
    free(net->lstm);
    free(net);
}

void forward_linearcontlstm(LinearContLSTM *net, float *observations, float *actions) {
    linear(net->encoder, observations);
    gelu(net->gelu1, net->encoder->output);
    lstm(net->lstm, net->gelu1->output);
    linear(net->actor, net->lstm->state_h);
    linear(net->value_fn, net->lstm->state_h);
    for (int i = 0; i < net->num_actions; i++) {
        float std = expf(net->log_std[i]);
        float mean = net->actor->output[i];
        actions[i] = randn(mean, std);
    }
}

void generate_dummy_actions(DroneSwarm *env) {
    // Generate random floats in [-1, 1] range
    env->actions[0] = ((float)rand() / (float)RAND_MAX) * 2.0f - 1.0f;
    env->actions[1] = ((float)rand() / (float)RAND_MAX) * 2.0f - 1.0f;
    env->actions[2] = ((float)rand() / (float)RAND_MAX) * 2.0f - 1.0f;
    env->actions[3] = ((float)rand() / (float)RAND_MAX) * 2.0f - 1.0f;
}

#ifdef __EMSCRIPTEN__
typedef struct {
    DroneSwarm *env;
    LinearContLSTM *net;
    Weights *weights;
} WebRenderArgs;

void emscriptenStep(void *e) {
    WebRenderArgs *args = (WebRenderArgs *)e;
    DroneSwarm *env = args->env;
    LinearContLSTM *net = args->net;

    forward_linearcontlstm(net, env->observations, env->actions);
    c_step(env);
    c_render(env);
    return;
}

WebRenderArgs *web_args = NULL;
#endif

int main() {
    srand(time(NULL)); // Seed random number generator

    DroneSwarm *env = calloc(1, sizeof(DroneSwarm));
    env->num_agents = 64;
    env->max_rings = 10;
    env->task = TASK_ORBIT;
    init(env);

    size_t obs_size = 41;
    size_t act_size = 4;
    env->observations = (float *)calloc(env->num_agents * obs_size, sizeof(float));
    env->actions = (float *)calloc(env->num_agents * act_size, sizeof(float));
    env->rewards = (float *)calloc(env->num_agents, sizeof(float));
    env->terminals = (unsigned char *)calloc(env->num_agents, sizeof(float));

    //Weights *weights = load_weights("resources/drone/drone_weights.bin", 136073);
    //int logit_sizes[1] = {4};
    //LinearContLSTM *net = make_linearcontlstm(weights, env->num_agents, obs_size, logit_sizes, 1);

    if (!env->observations || !env->actions || !env->rewards) {
        fprintf(stderr, "ERROR: Failed to allocate memory for demo buffers.\n");
        free(env->observations);
        free(env->actions);
        free(env->rewards);
        free(env->terminals);
        free(env);
        return 0;
    }

    init(env);
    c_reset(env);

#ifdef __EMSCRIPTEN__
    WebRenderArgs *args = calloc(1, sizeof(WebRenderArgs));
    args->env = env;
    args->net = net;
    args->weights = weights;
    web_args = args;

    emscripten_set_main_loop_arg(emscriptenStep, args, 0, true);
#else
    c_render(env);

    while (!WindowShouldClose()) {
        //forward_linearcontlstm(net, env->observations, env->actions);
        c_step(env);
        c_render(env);
    }

    c_close(env);
    //free_linearcontlstm(net);
    free(env->observations);
    free(env->actions);
    free(env->rewards);
    free(env->terminals);
    free(env);
#endif

    return 0;
}



================================================
FILE: pufferlib/ocean/drone_swarm/drone_swarm.h
================================================
// Originally made by Sam Turner and Finlay Sanders, 2025.
// Included in pufferlib under the original project's MIT license.
// https://github.com/stmio/drone

#include <float.h>
#include <math.h>
#include <stdbool.h>
#include <stdio.h>
#include <stdlib.h>
#include <string.h>
#include <time.h>

#include "raylib.h"
#include "dronelib.h"

#define TASK_IDLE 0
#define TASK_HOVER 1
#define TASK_ORBIT 2
#define TASK_FOLLOW 3
#define TASK_CUBE 4
#define TASK_CONGO 5
#define TASK_FLAG 6
#define TASK_RACE 7
#define TASK_N 8

char* TASK_NAMES[TASK_N] = {
    "Idle", "Hover", "Orbit", "Follow",
    "Cube", "Congo", "FLAG", "Race"
};

#define R (Color){255, 0, 0, 255}
#define W (Color){255, 255, 255, 255}
#define B (Color){0, 0, 255, 255}
Color FLAG_COLORS[64] = {
    B, B, B, B, R, R, R, R,
    B, B, B, B, W, W, W, W,
    B, B, B, B, R, R, R, R,
    B, B, B, B, W, W, W, W,
    R, R, R, R, R, R, R, R,
    W, W, W, W, W, W, W, W,
    R, R, R, R, R, R, R, R,
    W, W, W, W, W, W, W, W
};
#undef R
#undef W
#undef B

typedef struct Client Client;
struct Client {
    Camera3D camera;
    float width;
    float height;

    float camera_distance;
    float camera_azimuth;
    float camera_elevation;
    bool is_dragging;
    Vector2 last_mouse_pos;

    // Trailing path buffer (for rendering only)
    Trail* trails;
};

typedef struct {
    float *observations;
    float *actions;
    float *rewards;
    unsigned char *terminals;

    Log log;
    int tick;
    int report_interval;

    int task;
    int num_agents;
    Drone* agents;

    int max_rings;
    Ring* ring_buffer;

    Client *client;
} DroneSwarm;

void init(DroneSwarm *env) {
    env->agents = calloc(env->num_agents, sizeof(Drone));
    env->ring_buffer = calloc(env->max_rings, sizeof(Ring));
    env->log = (Log){0};
    env->tick = 0;
}

void add_log(DroneSwarm *env, int idx, bool oob) {
    Drone *agent = &env->agents[idx];
    env->log.score += agent->score;
    env->log.episode_return += agent->episode_return;
    env->log.episode_length += agent->episode_length;
    env->log.collision_rate += agent->collisions / (float)agent->episode_length;
    env->log.perf += agent->score / (float)agent->episode_length;
    if (oob) {
        env->log.oob += 1.0f;
    }
    env->log.n += 1.0f;

    agent->episode_length = 0;
    agent->episode_return = 0.0f;
}

Drone* nearest_drone(DroneSwarm* env, Drone *agent) {
    float min_dist = 999999.0f;
    Drone *nearest = NULL;
    for (int i = 0; i < env->num_agents; i++) {
        Drone *other = &env->agents[i];
        if (other == agent) {
            continue;
        }
        float dx = agent->state.pos.x - other->state.pos.x;
        float dy = agent->state.pos.y - other->state.pos.y;
        float dz = agent->state.pos.z - other->state.pos.z;
        float dist = sqrtf(dx*dx + dy*dy + dz*dz);
        if (dist < min_dist) {
            min_dist = dist;
            nearest = other;
        }
    }
    if (nearest == NULL) {
        int x = 0;

    }
    return nearest;
}

void compute_observations(DroneSwarm *env) {
    int idx = 0;
    for (int i = 0; i < env->num_agents; i++) {
        Drone *agent = &env->agents[i];

        Quat q_inv = quat_inverse(agent->state.quat);
        Vec3 linear_vel_body = quat_rotate(q_inv, agent->state.vel);
        Vec3 drone_up_world = quat_rotate(agent->state.quat, (Vec3){0.0f, 0.0f, 1.0f});

        // TODO: Need abs observations now right?
        env->observations[idx++] = linear_vel_body.x / agent->params.max_vel;
        env->observations[idx++] = linear_vel_body.y / agent->params.max_vel;
        env->observations[idx++] = linear_vel_body.z / agent->params.max_vel;

        env->observations[idx++] = agent->state.omega.x / agent->params.max_omega;
        env->observations[idx++] = agent->state.omega.y / agent->params.max_omega;
        env->observations[idx++] = agent->state.omega.z / agent->params.max_omega;

        env->observations[idx++] = drone_up_world.x;
        env->observations[idx++] = drone_up_world.y;
        env->observations[idx++] = drone_up_world.z;

        env->observations[idx++] = agent->state.quat.w;
        env->observations[idx++] = agent->state.quat.x;
        env->observations[idx++] = agent->state.quat.y;
        env->observations[idx++] = agent->state.quat.z;

        env->observations[idx++] = agent->state.rpms[0] / agent->params.max_rpm;
        env->observations[idx++] = agent->state.rpms[1] / agent->params.max_rpm;
        env->observations[idx++] = agent->state.rpms[2] / agent->params.max_rpm;
        env->observations[idx++] = agent->state.rpms[3] / agent->params.max_rpm;

        env->observations[idx++] = agent->state.pos.x / GRID_X;
        env->observations[idx++] = agent->state.pos.y / GRID_Y;
        env->observations[idx++] = agent->state.pos.z / GRID_Z;

        env->observations[idx++] = agent->spawn_pos.x / GRID_X;
        env->observations[idx++] = agent->spawn_pos.y / GRID_Y;
        env->observations[idx++] = agent->spawn_pos.z / GRID_Z;

        float dx = agent->target_pos.x - agent->state.pos.x;
        float dy = agent->target_pos.y - agent->state.pos.y;
        float dz = agent->target_pos.z - agent->state.pos.z;
        env->observations[idx++] = clampf(dx, -1.0f, 1.0f);
        env->observations[idx++] = clampf(dy, -1.0f, 1.0f);
        env->observations[idx++] = clampf(dz, -1.0f, 1.0f);
        env->observations[idx++] = dx / GRID_X;
        env->observations[idx++] = dy / GRID_Y;
        env->observations[idx++] = dz / GRID_Z;

        env->observations[idx++] = agent->last_collision_reward;
        env->observations[idx++] = agent->last_target_reward;
        env->observations[idx++] = agent->last_abs_reward;

        // Multiagent obs
        Drone* nearest = nearest_drone(env, agent);
        if (env->num_agents > 1) {
            env->observations[idx++] = clampf(nearest->state.pos.x - agent->state.pos.x, -1.0f, 1.0f);
            env->observations[idx++] = clampf(nearest->state.pos.y - agent->state.pos.y, -1.0f, 1.0f);
            env->observations[idx++] = clampf(nearest->state.pos.z - agent->state.pos.z, -1.0f, 1.0f);
        } else {
            env->observations[idx++] = 0.0f;
            env->observations[idx++] = 0.0f;
            env->observations[idx++] = 0.0f;
        }

        // Ring obs
        if (env->task == TASK_RACE) {
            Ring ring = env->ring_buffer[agent->ring_idx];
            Vec3 to_ring = quat_rotate(q_inv, sub3(ring.pos, agent->state.pos));
            Vec3 ring_norm = quat_rotate(q_inv, ring.normal);
            env->observations[idx++] = to_ring.x / GRID_X;
            env->observations[idx++] = to_ring.y / GRID_Y;
            env->observations[idx++] = to_ring.z / GRID_Z;
            env->observations[idx++] = ring_norm.x;
            env->observations[idx++] = ring_norm.y;
            env->observations[idx++] = ring_norm.z;
        } else {
            env->observations[idx++] = 0.0f;
            env->observations[idx++] = 0.0f;
            env->observations[idx++] = 0.0f;
            env->observations[idx++] = 0.0f;
            env->observations[idx++] = 0.0f;
            env->observations[idx++] = 0.0f;
        }
    }
}

void move_target(DroneSwarm* env, Drone *agent) {
    agent->target_pos.x += agent->target_vel.x;
    agent->target_pos.y += agent->target_vel.y;
    agent->target_pos.z += agent->target_vel.z;
    if (agent->target_pos.x < -GRID_X || agent->target_pos.x > GRID_X) {
        agent->target_vel.x = -agent->target_vel.x;
    }
    if (agent->target_pos.y < -GRID_Y || agent->target_pos.y > GRID_Y) {
        agent->target_vel.y = -agent->target_vel.y;
    }
    if (agent->target_pos.z < -GRID_Z || agent->target_pos.z > GRID_Z) {
        agent->target_vel.z = -agent->target_vel.z;
    }
}

void set_target_idle(DroneSwarm* env, int idx) {
    Drone *agent = &env->agents[idx];
    agent->target_pos = (Vec3){rndf(-MARGIN_X, MARGIN_X), rndf(-MARGIN_Y, MARGIN_Y), rndf(-MARGIN_Z, MARGIN_Z)};
    agent->target_vel = (Vec3){rndf(-V_TARGET, V_TARGET), rndf(-V_TARGET, V_TARGET), rndf(-V_TARGET, V_TARGET)};
}

void set_target_hover(DroneSwarm* env, int idx) {
    Drone *agent = &env->agents[idx];
    agent->target_pos = agent->state.pos;
    agent->target_vel = (Vec3){0.0f, 0.0f, 0.0f};
}

void set_target_orbit(DroneSwarm* env, int idx) {
    // Fibbonacci sphere algorithm
    float R = 8.0f;
    float phi = PI * (sqrt(5.0f) - 1.0f);
    float y = 1.0f - 2*((float)idx / (float)env->num_agents);
    float radius = sqrtf(1.0f - y*y);

    float theta = phi * idx;

    float x = cos(theta) * radius;
    float z = sin(theta) * radius;

    Drone *agent = &env->agents[idx];
    agent->target_pos = (Vec3){R*x, R*z, R*y}; // convert to z up 
    agent->target_vel = (Vec3){0.0f, 0.0f, 0.0f};
}

void set_target_follow(DroneSwarm* env, int idx) {
    Drone* agent = &env->agents[idx];
    if (idx == 0) {
        set_target_idle(env, idx);
    } else {
        agent->target_pos = env->agents[0].target_pos;
        agent->target_vel = env->agents[0].target_vel;
    }
}

void set_target_cube(DroneSwarm* env, int idx) {
    Drone* agent = &env->agents[idx];
    float z = idx / 16;
    idx = idx % 16;
    float x = (float)(idx % 4);
    float y = (float)(idx / 4);
    agent->target_pos = (Vec3){4*x - 6, 4*y - 6, 4*z - 6};
    agent->target_vel = (Vec3){0.0f, 0.0f, 0.0f};
}

void set_target_congo(DroneSwarm* env, int idx) {
    if (idx == 0) {
        set_target_idle(env, idx);
        return;
    }
    Drone* follow = &env->agents[idx - 1];
    Drone* lead = &env->agents[idx];
    lead->target_pos = follow->target_pos;
    lead->target_vel = follow->target_vel;

    // TODO: Slow hack
    for (int i = 0; i < 40; i++) {
        move_target(env, lead);
    }
}

void set_target_flag(DroneSwarm* env, int idx) {
    Drone* agent = &env->agents[idx];
    float x = (float)(idx % 8);
    float y = (float)(idx / 8);
    x = 2.0f*x - 7;
    y = 5 - 1.5f*y;
    agent->target_pos = (Vec3){0.0f, x, y};
    agent->target_vel = (Vec3){0.0f, 0.0f, 0.0f};
}

void set_target_race(DroneSwarm* env, int idx) {
    Drone* agent = &env->agents[idx];
    agent->target_pos = env->ring_buffer[agent->ring_idx].pos;
    agent->target_vel = (Vec3){0.0f, 0.0f, 0.0f};
}

void set_target(DroneSwarm* env, int idx) {
    if (env->task == TASK_IDLE) {
        set_target_idle(env, idx);
    } else if (env->task == TASK_HOVER) {
        set_target_hover(env, idx);
    } else if (env->task == TASK_ORBIT) {
        set_target_orbit(env, idx);
    } else if (env->task == TASK_FOLLOW) {
        set_target_follow(env, idx);
    } else if (env->task == TASK_CUBE) {
        set_target_cube(env, idx);
    } else if (env->task == TASK_CONGO) {
        set_target_congo(env, idx);
    } else if (env->task == TASK_FLAG) {
        set_target_flag(env, idx);
    } else if (env->task == TASK_RACE) {
        set_target_race(env, idx);
    }
}

float compute_reward(DroneSwarm* env, Drone *agent, bool collision) {
    // Distance reward
    float dx = (agent->state.pos.x - agent->target_pos.x);
    float dy = (agent->state.pos.y - agent->target_pos.y);
    float dz = (agent->state.pos.z - agent->target_pos.z);
    float dist = sqrtf(dx*dx + dy*dy + dz*dz);
    float dist_reward = 1.0 - dist/MAX_DIST;
    //dist = clampf(dist, 0.0f, 1.0f);
    //float dist_reward = 1.0f - dist;

    // Density penalty
    float density_reward = 0.0f;
    if (collision && env->num_agents > 1) {
        Drone *nearest = nearest_drone(env, agent);
        dx = agent->state.pos.x - nearest->state.pos.x;
        dy = agent->state.pos.y - nearest->state.pos.y;
        dz = agent->state.pos.z - nearest->state.pos.z;
        float min_dist = sqrtf(dx*dx + dy*dy + dz*dz);
        if (min_dist < 1.0f) {
            density_reward = -1.0f;
            agent->collisions += 1.0f;
        }
    }

    float abs_reward = dist_reward + density_reward;

    // Prevent negative dist and density from making a positive reward
    if (dist_reward < 0.0f && density_reward < 0.0f) {
        abs_reward *= -1.0f;
    }

    float delta_reward = abs_reward - agent->last_abs_reward;

    agent->last_collision_reward = density_reward;
    agent->last_target_reward = dist_reward;
    agent->last_abs_reward = abs_reward;

    agent->episode_length++;
    agent->score += abs_reward;

    return delta_reward;
}

void reset_agent(DroneSwarm* env, Drone *agent, int idx) {
    agent->episode_return = 0.0f;
    agent->episode_length = 0;
    agent->collisions = 0.0f;
    agent->score = 0.0f;
    agent->ring_idx = 0;

    //float size = 0.2f;
    //init_drone(agent, size, 0.0f);
    float size = rndf(0.1f, 0.4);
    init_drone(agent, size, 0.1f);

    agent->state.pos = (Vec3){
        rndf(-MARGIN_X, MARGIN_X),
        rndf(-MARGIN_Y, MARGIN_Y),
        rndf(-MARGIN_Z, MARGIN_Z)
    };
    agent->prev_pos = agent->state.pos;
    agent->spawn_pos = agent->state.pos;

    compute_reward(env, agent, env->task != TASK_RACE);
}

void c_reset(DroneSwarm *env) {
    env->tick = 0;
    //env->task = rand() % (TASK_N - 1);
    
    if (rand() % 4) {
        env->task = TASK_RACE;
    } else {
        env->task = rand() % (TASK_N - 1);
    }
    
    //env->task = TASK_RACE;
    //env->task = TASK_HOVER;
    //env->task = TASK_FLAG;

    for (int i = 0; i < env->num_agents; i++) {
        Drone *agent = &env->agents[i];
        reset_agent(env, agent, i);
        set_target(env, i);
    }

    for (int i = 0; i < env->max_rings; i++) {
        Ring *ring = &env->ring_buffer[i];
        *ring = (Ring){0};
    }
    if (env->task == TASK_RACE) {
        float ring_radius = 2.0f;
        reset_rings(env->ring_buffer, env->max_rings, ring_radius);

        // start drone at least MARGIN away from the first ring
        for (int i = 0; i < env->num_agents; i++) {
            Drone *drone = &env->agents[i];
            do {
                drone->state.pos = (Vec3){
                    rndf(-MARGIN_X, MARGIN_X), 
                    rndf(-MARGIN_Y, MARGIN_Y), 
                    rndf(-MARGIN_Z, MARGIN_Z)
                };
            } while (norm3(sub3(drone->state.pos, env->ring_buffer[0].pos)) < 2.0f*ring_radius);
        }
    }
 
    compute_observations(env);
}

void c_step(DroneSwarm *env) {
    env->tick = (env->tick + 1) % HORIZON;
    for (int i = 0; i < env->num_agents; i++) {
        Drone *agent = &env->agents[i];
        env->rewards[i] = 0;
        env->terminals[i] = 0;

        float* atn = &env->actions[4*i];
        move_drone(agent, atn);

        // check out of bounds
        bool out_of_bounds = agent->state.pos.x < -GRID_X || agent->state.pos.x > GRID_X ||
                             agent->state.pos.y < -GRID_Y || agent->state.pos.y > GRID_Y ||
                             agent->state.pos.z < -GRID_Z || agent->state.pos.z > GRID_Z;

        move_target(env, agent);

        float reward = 0.0f;
        if (env->task == TASK_RACE) {
            Ring *ring = &env->ring_buffer[agent->ring_idx];
            reward = compute_reward(env, agent, true);
            float passed_ring = check_ring(agent, ring);
            if (passed_ring > 0) {
                agent->ring_idx = (agent->ring_idx + 1) % env->max_rings;
                env->log.rings_passed += 1.0f;
                set_target(env, i);
                compute_reward(env, agent, true);
            }
            reward += passed_ring;
        } else {
            // Delta reward
            reward = compute_reward(env, agent, true);
        }

        env->rewards[i] += reward;
        agent->episode_return += reward;

        if (out_of_bounds) {
            env->rewards[i] -= 1;
            env->terminals[i] = 1;
            add_log(env, i, true);
            reset_agent(env, agent, i);
        } else if (env->tick >= HORIZON - 1) {
            env->terminals[i] = 1;
            add_log(env, i, false);
        }
    }
    if (env->tick >= HORIZON - 1) {
        c_reset(env);
    }

    compute_observations(env);
}

void c_close_client(Client *client) {
    CloseWindow();
    free(client);
}

void c_close(DroneSwarm *env) {
    if (env->client != NULL) {
        c_close_client(env->client);
    }
}

static void update_camera_position(Client *c) {
    float r = c->camera_distance;
    float az = c->camera_azimuth;
    float el = c->camera_elevation;

    float x = r * cosf(el) * cosf(az);
    float y = r * cosf(el) * sinf(az);
    float z = r * sinf(el);

    c->camera.position = (Vector3){x, y, z};
    c->camera.target = (Vector3){0, 0, 0};
}

void handle_camera_controls(Client *client) {
    Vector2 mouse_pos = GetMousePosition();

    if (IsMouseButtonPressed(MOUSE_BUTTON_LEFT)) {
        client->is_dragging = true;
        client->last_mouse_pos = mouse_pos;
    }

    if (IsMouseButtonReleased(MOUSE_BUTTON_LEFT)) {
        client->is_dragging = false;
    }

    if (client->is_dragging && IsMouseButtonDown(MOUSE_BUTTON_LEFT)) {
        Vector2 mouse_delta = {mouse_pos.x - client->last_mouse_pos.x,
                               mouse_pos.y - client->last_mouse_pos.y};

        float sensitivity = 0.005f;

        client->camera_azimuth -= mouse_delta.x * sensitivity;

        client->camera_elevation += mouse_delta.y * sensitivity;
        client->camera_elevation =
            clampf(client->camera_elevation, -PI / 2.0f + 0.1f, PI / 2.0f - 0.1f);

        client->last_mouse_pos = mouse_pos;

        update_camera_position(client);
    }

    float wheel = GetMouseWheelMove();
    if (wheel != 0) {
        client->camera_distance -= wheel * 2.0f;
        client->camera_distance = clampf(client->camera_distance, 5.0f, 50.0f);
        update_camera_position(client);
    }
}

Client *make_client(DroneSwarm *env) {
    Client *client = (Client *)calloc(1, sizeof(Client));

    client->width = WIDTH;
    client->height = HEIGHT;

    SetConfigFlags(FLAG_MSAA_4X_HINT); // antialiasing
    InitWindow(WIDTH, HEIGHT, "PufferLib DroneSwarm");

#ifndef __EMSCRIPTEN__
    SetTargetFPS(60);
#endif

    if (!IsWindowReady()) {
        TraceLog(LOG_ERROR, "Window failed to initialize\n");
        free(client);
        return NULL;
    }

    client->camera_distance = 40.0f;
    client->camera_azimuth = 0.0f;
    client->camera_elevation = PI / 10.0f;
    client->is_dragging = false;
    client->last_mouse_pos = (Vector2){0.0f, 0.0f};

    client->camera.up = (Vector3){0.0f, 0.0f, 1.0f};
    client->camera.fovy = 45.0f;
    client->camera.projection = CAMERA_PERSPECTIVE;

    update_camera_position(client);

    // Initialize trail buffer
    client->trails = (Trail*)calloc(env->num_agents, sizeof(Trail));
    for (int i = 0; i < env->num_agents; i++) {
        Trail* trail = &client->trails[i];
        trail->index = 0;
        trail->count = 0;
        for (int j = 0; j < TRAIL_LENGTH; j++) {
            trail->pos[j] = env->agents[i].state.pos;
        }
    }

    return client;
}

const Color PUFF_RED = (Color){187, 0, 0, 255};
const Color PUFF_CYAN = (Color){0, 187, 187, 255};
const Color PUFF_WHITE = (Color){241, 241, 241, 241};
const Color PUFF_BACKGROUND = (Color){6, 24, 24, 255};

void DrawRing3D(Ring ring, float thickness, Color entryColor, Color exitColor) {
    float half_thick = thickness / 2.0f;

    Vector3 center_pos = {ring.pos.x, ring.pos.y, ring.pos.z};

    Vector3 entry_start_pos = {center_pos.x - half_thick * ring.normal.x,
                               center_pos.y - half_thick * ring.normal.y,
                               center_pos.z - half_thick * ring.normal.z};

    DrawCylinderWiresEx(entry_start_pos, center_pos, ring.radius, ring.radius, 32, entryColor);

    Vector3 exit_end_pos = {center_pos.x + half_thick * ring.normal.x,
                            center_pos.y + half_thick * ring.normal.y,
                            center_pos.z + half_thick * ring.normal.z};

    DrawCylinderWiresEx(center_pos, exit_end_pos, ring.radius, ring.radius, 32, exitColor);
}


void c_render(DroneSwarm *env) {
    if (env->client == NULL) {
        env->client = make_client(env);
        if (env->client == NULL) {
            TraceLog(LOG_ERROR, "Failed to initialize client for rendering\n");
            return;
        }
    }

    if (WindowShouldClose()) {
        c_close(env);
        exit(0);
    }

    if (IsKeyDown(KEY_ESCAPE)) {
        c_close(env);
        exit(0);
    }

    if (IsKeyPressed(KEY_SPACE)) {
        env->task = (env->task + 1) % TASK_N;
        for (int i = 0; i < env->num_agents; i++) {
            set_target(env, i);
        }
        if (env->task == TASK_RACE) {
            float ring_radius = 2.0f;
            reset_rings(env->ring_buffer, env->max_rings, ring_radius);
        }
    }

    handle_camera_controls(env->client);

    Client *client = env->client;

    for (int i = 0; i < env->num_agents; i++) {
        Drone *agent = &env->agents[i];
        Trail *trail = &client->trails[i];
        trail->pos[trail->index] = agent->state.pos;
        trail->index = (trail->index + 1) % TRAIL_LENGTH;
        if (trail->count < TRAIL_LENGTH) {
            trail->count++;
        }
        if (env->terminals[i]) {
            trail->index = 0;
            trail->count = 0;
        }
    }

    BeginDrawing();
    ClearBackground(PUFF_BACKGROUND);

    BeginMode3D(client->camera);

    // draws bounding cube
    DrawCubeWires((Vector3){0.0f, 0.0f, 0.0f}, GRID_X * 2.0f,
        GRID_Y * 2.0f, GRID_Z * 2.0f, WHITE);

    for (int i = 0; i < env->num_agents; i++) {
        Drone *agent = &env->agents[i];

        // draws drone body
        Color body_color = FLAG_COLORS[i];
        DrawSphere((Vector3){agent->state.pos.x, agent->state.pos.y, agent->state.pos.z}, 0.3f, body_color);

        // draws rotors according to thrust
        float T[4];
        for (int j = 0; j < 4; j++) {
            float rpm = (env->actions[4*i + j] + 1.0f) * 0.5f * agent->params.max_rpm;
            T[j] = agent->params.k_thrust * rpm * rpm;
        }

        const float rotor_radius = 0.15f;
        const float visual_arm_len = agent->params.arm_len * 4.0f;

        Vec3 rotor_offsets_body[4] = {{+visual_arm_len, 0.0f, 0.0f},
                                      {-visual_arm_len, 0.0f, 0.0f},
                                      {0.0f, +visual_arm_len, 0.0f},
                                      {0.0f, -visual_arm_len, 0.0f}};

        //Color base_colors[4] = {ORANGE, PURPLE, LIME, SKYBLUE};
        Color base_colors[4] = {body_color, body_color, body_color, body_color};

        for (int j = 0; j < 4; j++) {
            Vec3 world_off = quat_rotate(agent->state.quat, rotor_offsets_body[j]);

            Vector3 rotor_pos = {agent->state.pos.x + world_off.x, agent->state.pos.y + world_off.y,
                                 agent->state.pos.z + world_off.z};

            float rpm = (env->actions[4*i + j] + 1.0f) * 0.5f * agent->params.max_rpm;
            float intensity = 0.75f + 0.25f * (rpm / agent->params.max_rpm);

            Color rotor_color = (Color){(unsigned char)(base_colors[j].r * intensity),
                                        (unsigned char)(base_colors[j].g * intensity),
                                        (unsigned char)(base_colors[j].b * intensity), 255};

            DrawSphere(rotor_pos, rotor_radius, rotor_color);

            DrawCylinderEx((Vector3){agent->state.pos.x, agent->state.pos.y, agent->state.pos.z}, rotor_pos, 0.02f, 0.02f, 8,
                           BLACK);
        }

        // draws line with direction and magnitude of velocity / 10
        if (norm3(agent->state.vel) > 0.1f) {
            DrawLine3D((Vector3){agent->state.pos.x, agent->state.pos.y, agent->state.pos.z},
                       (Vector3){agent->state.pos.x + agent->state.vel.x * 0.1f, agent->state.pos.y + agent->state.vel.y * 0.1f,
                                 agent->state.pos.z + agent->state.vel.z * 0.1f},
                       MAGENTA);
        }

        // Draw trailing path
        Trail *trail = &client->trails[i];
        if (trail->count <= 2) {
            continue;
        }
        for (int j = 0; j < trail->count - 1; j++) {
            int idx0 = (trail->index - j - 1 + TRAIL_LENGTH) % TRAIL_LENGTH;
            int idx1 = (trail->index - j - 2 + TRAIL_LENGTH) % TRAIL_LENGTH;
            float alpha = (float)(TRAIL_LENGTH - j) / (float)trail->count * 0.8f; // fade out
            Color trail_color = ColorAlpha((Color){0, 187, 187, 255}, alpha);
            DrawLine3D((Vector3){trail->pos[idx0].x, trail->pos[idx0].y, trail->pos[idx0].z},
                       (Vector3){trail->pos[idx1].x, trail->pos[idx1].y, trail->pos[idx1].z},
                       trail_color);
        }

    }

    // Rings
    if (env->task == TASK_RACE) {
        float ring_thickness = 0.2f;
        for (int i = 0; i < env->max_rings; i++) {
            Ring ring = env->ring_buffer[i];
            DrawRing3D(ring, ring_thickness, GREEN, BLUE);
        }
    }

    if (IsKeyDown(KEY_TAB)) {
        for (int i = 0; i < env->num_agents; i++) {
            Drone *agent = &env->agents[i];
            Vec3 target_pos = agent->target_pos;
            DrawSphere((Vector3){target_pos.x, target_pos.y, target_pos.z}, 0.45f, (Color){0, 255, 255, 100});
        }
    }

    EndMode3D();

    DrawText("Left click + drag: Rotate camera", 10, 10, 16, PUFF_WHITE);
    DrawText("Mouse wheel: Zoom in/out", 10, 30, 16, PUFF_WHITE);
    DrawText(TextFormat("Task: %s", TASK_NAMES[env->task]), 10, 50, 16, PUFF_WHITE);

    EndDrawing();
}



================================================
FILE: pufferlib/ocean/drone_swarm/drone_swarm.py
================================================
import numpy as np
import gymnasium

import pufferlib
from pufferlib.ocean.drone_swarm import binding

class DroneSwarm(pufferlib.PufferEnv):
    def __init__(
        self,
        num_envs=16,
        num_drones=64,
        max_rings=5,
        render_mode=None,
        report_interval=1024,
        buf=None,
        seed=0,
    ):
        self.single_observation_space = gymnasium.spaces.Box(
            low=-1,
            high=1,
            shape=(41,),
            dtype=np.float32,
        )

        self.single_action_space = gymnasium.spaces.Box(
            low=-1, high=1, shape=(4,), dtype=np.float32
        )

        self.num_agents = num_envs*num_drones
        self.render_mode = render_mode
        self.report_interval = report_interval
        self.tick = 0

        super().__init__(buf)
        self.actions = self.actions.astype(np.float32)

        c_envs = []
        for i in range(num_envs):
            c_envs.append(binding.env_init(
                self.observations[i*num_drones:(i+1)*num_drones],
                self.actions[i*num_drones:(i+1)*num_drones],
                self.rewards[i*num_drones:(i+1)*num_drones],
                self.terminals[i*num_drones:(i+1)*num_drones],
                self.truncations[i*num_drones:(i+1)*num_drones],
                i,
                num_agents=num_drones,
                max_rings=max_rings,
            ))

        self.c_envs = binding.vectorize(*c_envs)

    def reset(self, seed=None):
        self.tick = 0
        binding.vec_reset(self.c_envs, seed)
        return self.observations, []

    def step(self, actions):
        self.actions[:] = actions

        self.tick += 1
        binding.vec_step(self.c_envs)

        info = []
        if self.tick % self.report_interval == 0:
            log_data = binding.vec_log(self.c_envs)
            if log_data:
                info.append(log_data)

        return (self.observations, self.rewards, self.terminals, self.truncations, info)

    def render(self):
        binding.vec_render(self.c_envs, 0)

    def close(self):
        binding.vec_close(self.c_envs)

def test_performance(timeout=10, atn_cache=1024):
    env = DroneSwarm(num_envs=1000)
    env.reset()
    tick = 0

    actions = [env.action_space.sample() for _ in range(atn_cache)]

    import time
    start = time.time()
    while time.time() - start < timeout:
        atn = actions[tick % atn_cache]
        env.step(atn)
        tick += 1

    print(f"SPS: {env.num_agents * tick / (time.time() - start)}")

if __name__ == "__main__":
    test_performance()



================================================
FILE: pufferlib/ocean/drone_swarm/dronelib.h
================================================
// Originally made by Sam Turner and Finlay Sanders, 2025.
// Included in pufferlib under the original project's MIT license.
// https://github.com/stmio/drone

#include <float.h>
#include <math.h>
#include <stdbool.h>
#include <stdio.h>
#include <stdlib.h>
#include <string.h>
#include <time.h>

#include "raylib.h"

// Visualisation properties
#define WIDTH 1080
#define HEIGHT 720
#define TRAIL_LENGTH 50
#define HORIZON 1024

// Physical constants for the drone
#define BASE_MASS 1.0f       // kg
#define BASE_IXX 0.01f       // kgm^2
#define BASE_IYY 0.01f       // kgm^2
#define BASE_IZZ 0.02f       // kgm^2
#define BASE_ARM_LEN 0.1f    // m
#define BASE_K_THRUST 3e-5f  // thrust coefficient
#define BASE_K_ANG_DAMP 0.2f // angular damping coefficient
#define BASE_K_DRAG 1e-6f    // drag (torque) coefficient
#define BASE_B_DRAG 0.1f     // linear drag coefficient
#define BASE_GRAVITY 9.81f   // m/s^2
#define BASE_MAX_RPM 750.0f  // rad/s
#define BASE_MAX_VEL 50.0f   // m/s
#define BASE_MAX_OMEGA 50.0f // rad/s
#define BASE_K_MOT 0.1f      // s (Motor lag constant)
#define BASE_J_MOT 1e-5f     // kgm^2 (Motor rotational inertia)

// Simulation properties
#define GRID_X 30.0f
#define GRID_Y 30.0f
#define GRID_Z 10.0f
#define MARGIN_X (GRID_X - 1)
#define MARGIN_Y (GRID_Y - 1)
#define MARGIN_Z (GRID_Z - 1)
#define V_TARGET 0.05f
#define DT 0.05f
#define DT_RNG 0.0f

// Corner to corner distance
#define MAX_DIST sqrtf((2*GRID_X)*(2*GRID_X) + (2*GRID_Y)*(2*GRID_Y) + (2*GRID_Z)*(2*GRID_Z))

typedef struct Log Log;
struct Log {
    float episode_return;
    float episode_length;
    float rings_passed;
    float collision_rate;
    float oob;
    float timeout;
    float score;
    float perf;
    float n;
};

typedef struct {
    float w, x, y, z;
} Quat;

typedef struct {
    float x, y, z;
} Vec3;

static inline float clampf(float v, float min, float max) {
    if (v < min)
        return min;
    if (v > max)
        return max;
    return v;
}

static inline float rndf(float a, float b) {
    return a + ((float)rand() / (float)RAND_MAX) * (b - a);
}

static inline Vec3 add3(Vec3 a, Vec3 b) { return (Vec3){a.x + b.x, a.y + b.y, a.z + b.z}; }

static inline Quat add_quat(Quat a, Quat b) { return (Quat){a.w + b.w, a.x + b.x, a.y + b.y, a.z + b.z}; }

static inline Vec3 sub3(Vec3 a, Vec3 b) { return (Vec3){a.x - b.x, a.y - b.y, a.z - b.z}; }

static inline Vec3 scalmul3(Vec3 a, float b) { return (Vec3){a.x * b, a.y * b, a.z * b}; }

static inline Quat scalmul_quat(Quat a, float b) { return (Quat){a.w * b, a.x * b, a.y * b, a.z * b}; }

static inline float dot3(Vec3 a, Vec3 b) { return a.x * b.x + a.y * b.y + a.z * b.z; }

static inline float norm3(Vec3 a) { return sqrtf(dot3(a, a)); }

static inline void clamp3(Vec3 *vec, float min, float max) {
    vec->x = clampf(vec->x, min, max);
    vec->y = clampf(vec->y, min, max);
    vec->z = clampf(vec->z, min, max);
}

static inline void clamp4(float a[4], float min, float max) {
    a[0] = clampf(a[0], min, max);
    a[1] = clampf(a[1], min, max);
    a[2] = clampf(a[2], min, max);
    a[3] = clampf(a[3], min, max);
}

static inline Quat quat_mul(Quat q1, Quat q2) {
    Quat out;
    out.w = q1.w * q2.w - q1.x * q2.x - q1.y * q2.y - q1.z * q2.z;
    out.x = q1.w * q2.x + q1.x * q2.w + q1.y * q2.z - q1.z * q2.y;
    out.y = q1.w * q2.y - q1.x * q2.z + q1.y * q2.w + q1.z * q2.x;
    out.z = q1.w * q2.z + q1.x * q2.y - q1.y * q2.x + q1.z * q2.w;
    return out;
}

static inline void quat_normalize(Quat *q) {
    float n = sqrtf(q->w * q->w + q->x * q->x + q->y * q->y + q->z * q->z);
    if (n > 0.0f) {
        q->w /= n;
        q->x /= n;
        q->y /= n;
        q->z /= n;
    }
}

static inline Vec3 quat_rotate(Quat q, Vec3 v) {
    Quat qv = {0.0f, v.x, v.y, v.z};
    Quat tmp = quat_mul(q, qv);
    Quat q_conj = {q.w, -q.x, -q.y, -q.z};
    Quat res = quat_mul(tmp, q_conj);
    return (Vec3){res.x, res.y, res.z};
}

static inline Quat quat_inverse(Quat q) { return (Quat){q.w, -q.x, -q.y, -q.z}; }

Quat rndquat() {
    float u1 = rndf(0.0f, 1.0f);
    float u2 = rndf(0.0f, 1.0f);
    float u3 = rndf(0.0f, 1.0f);

    float sqrt_1_minus_u1 = sqrtf(1.0f - u1);
    float sqrt_u1 = sqrtf(u1);

    float pi_2_u2 = 2.0f * M_PI * u2;
    float pi_2_u3 = 2.0f * M_PI * u3;

    Quat q;
    q.w = sqrt_1_minus_u1 * sinf(pi_2_u2);
    q.x = sqrt_1_minus_u1 * cosf(pi_2_u2);
    q.y = sqrt_u1 * sinf(pi_2_u3);
    q.z = sqrt_u1 * cosf(pi_2_u3);

    return q;
}

typedef struct {
    Vec3 pos;
    Quat orientation;
    Vec3 normal;
    float radius;
} Ring;

Ring rndring(float radius) {
    Ring ring;

    ring.pos.x = rndf(-GRID_X + 2*radius, GRID_X - 2*radius);
    ring.pos.y = rndf(-GRID_Y + 2*radius, GRID_Y - 2*radius);
    ring.pos.z = rndf(-GRID_Z + 2*radius, GRID_Z - 2*radius);

    ring.orientation = rndquat();

    Vec3 base_normal = {0.0f, 0.0f, 1.0f};
    ring.normal = quat_rotate(ring.orientation, base_normal);

    ring.radius = radius;

    return ring;
}

typedef struct {
    Vec3 pos[TRAIL_LENGTH];
    int index;
    int count;
} Trail;

typedef struct {
    Vec3 pos;      // global position (x, y, z)
    Vec3 vel;      // linear velocity (u, v, w)
    Quat quat;     // roll/pitch/yaw (phi/theta/psi) as a quaternion
    Vec3 omega;    // angular velocity (p, q, r)
    float rpms[4]; // motor RPMs
} State;

typedef struct {
    Vec3 vel;       // Derivative of position
    Vec3 v_dot;       // Derivative of velocity
    Quat q_dot;       // Derivative of quaternion
    Vec3 w_dot;       // Derivative of angular velocity
    float rpm_dot[4]; // Derivative of motor RPMs
} StateDerivative;

typedef struct {
    // Physical properties. Modeled as part of the drone
    // to make domain randomization easier.
    float mass; // kg
    float ixx; // kgm^2
    float iyy; // kgm^2
    float izz; // kgm^2
    float arm_len; // m
    float k_thrust; // thrust coefficient
    float k_ang_damp; // angular damping coefficient
    float k_drag; // drag (torque) coefficient
    float b_drag; // linear drag coefficient
    float gravity; // m/s^2
    float max_rpm; // rad/s
    float max_vel; // m/s
    float max_omega; // rad/s
    float k_mot; // s
    float j_mot; // kgm^2
} Params;

typedef struct {
    // core state and parameters
    State state;
    Params params;

    // helpers for ring/swarm logic
    Vec3 spawn_pos;
    Vec3 prev_pos;
    Vec3 target_pos;
    Vec3 target_vel;

    // logging utils
    float last_abs_reward;
    float last_target_reward;
    float last_collision_reward;
    float episode_return;
    float collisions;
    int episode_length;
    float score;
    int ring_idx;
} Drone;


void init_drone(Drone* drone, float size, float dr) {
    drone->params.arm_len = size / 2.0f;

    // m ~ x^3
    float mass_scale = powf(drone->params.arm_len, 3.0f) / powf(BASE_ARM_LEN, 3.0f);
    drone->params.mass = BASE_MASS * mass_scale * rndf(1.0f - dr, 1.0f + dr);

    // I ~ mx^2
    float base_Iscale = BASE_MASS * BASE_ARM_LEN * BASE_ARM_LEN;
    float I_scale = drone->params.mass * powf(drone->params.arm_len, 2.0f) / base_Iscale;
    drone->params.ixx = BASE_IXX * I_scale * rndf(1.0f - dr, 1.0f + dr);
    drone->params.iyy = BASE_IYY * I_scale * rndf(1.0f - dr, 1.0f + dr);
    drone->params.izz = BASE_IZZ * I_scale * rndf(1.0f - dr, 1.0f + dr);

    // k_thrust ~ m/l
    float k_thrust_scale = (drone->params.mass * drone->params.arm_len) / (BASE_MASS * BASE_ARM_LEN);
    drone->params.k_thrust = BASE_K_THRUST * k_thrust_scale * rndf(1.0f - dr, 1.0f + dr);

    // k_ang_damp ~ I
    float base_avg_inertia = (BASE_IXX + BASE_IYY + BASE_IZZ) / 3.0f;
    float avg_inertia = (drone->params.ixx + drone->params.iyy + drone->params.izz) / 3.0f;
    float avg_inertia_scale = avg_inertia / base_avg_inertia;
    drone->params.k_ang_damp = BASE_K_ANG_DAMP * avg_inertia_scale * rndf(1.0f - dr, 1.0f + dr);

    // drag ~ x^2
    float drag_scale = powf(drone->params.arm_len, 2.0f) / powf(BASE_ARM_LEN, 2.0f);
    drone->params.k_drag = BASE_K_DRAG * drag_scale * rndf(1.0f - dr, 1.0f + dr);
    drone->params.b_drag = BASE_B_DRAG * drag_scale * rndf(1.0f - dr, 1.0f + dr);

    // Small gravity randomization
    drone->params.gravity = BASE_GRAVITY * rndf(0.99f, 1.01f);

    // RPM ~ 1/x
    float rpm_scale = (BASE_ARM_LEN) / (drone->params.arm_len);
    drone->params.max_rpm = BASE_MAX_RPM * rpm_scale * rndf(1.0f - dr, 1.0f + dr);

    drone->params.max_vel = BASE_MAX_VEL;
    drone->params.max_omega = BASE_MAX_OMEGA;

    drone->params.k_mot = BASE_K_MOT * rndf(1.0f - dr, 1.0f + dr);
    drone->params.j_mot = BASE_J_MOT * I_scale * rndf(1.0f - dr, 1.0f + dr);
    
    for (int i = 0; i < 4; i++) {
        drone->state.rpms[i] = 0.0f;
    }
    drone->state.pos = (Vec3){0.0f, 0.0f, 0.0f};
    drone->prev_pos = drone->state.pos;
    drone->state.vel = (Vec3){0.0f, 0.0f, 0.0f};
    drone->state.omega = (Vec3){0.0f, 0.0f, 0.0f};
    drone->state.quat = (Quat){1.0f, 0.0f, 0.0f, 0.0f};
}

void compute_derivatives(State* state, Params* params, float* actions, StateDerivative* derivatives) {
    // first order rpm lag
    float target_rpms[4];
    for (int i = 0; i < 4; i++) {
        target_rpms[i] = (actions[i] + 1.0f) * 0.5f * params->max_rpm;
    }

    // rpm rates
    float rpm_dot[4];
    for (int i = 0; i < 4; i++) {
        rpm_dot[i] = (1.0f / params->k_mot) * (target_rpms[i] - state->rpms[i]);
    }

    // motor thrusts
    float T[4];
    for (int i = 0; i < 4; i++) {
        T[i] = params->k_thrust * powf(state->rpms[i], 2.0f);
    }

    // body frame net force
    Vec3 F_prop_body = {0.0f, 0.0f, T[0] + T[1] + T[2] + T[3]};

    // body frame force -> world frame force
    Vec3 F_prop = quat_rotate(state->quat, F_prop_body);

    // world frame linear drag
    Vec3 F_aero;
    F_aero.x = -params->b_drag * state->vel.x;
    F_aero.y = -params->b_drag * state->vel.y;
    F_aero.z = -params->b_drag * state->vel.z;

    // velocity rates, a = F/m
    Vec3 v_dot;
    v_dot.x = (F_prop.x + F_aero.x) / params->mass;
    v_dot.y = (F_prop.y + F_aero.y) / params->mass;
    v_dot.z = ((F_prop.z + F_aero.z) / params->mass) - params->gravity;

    // quaternion rates
    Quat omega_q = {0.0f, state->omega.x, state->omega.y, state->omega.z};
    Quat q_dot = quat_mul(state->quat, omega_q);
    q_dot.w *= 0.5f;
    q_dot.x *= 0.5f;
    q_dot.y *= 0.5f;
    q_dot.z *= 0.5f;

    // body frame torques
    Vec3 Tau_prop;
    Tau_prop.x = params->arm_len*(T[1] - T[3]);
    Tau_prop.y = params->arm_len*(T[2] - T[0]);
    Tau_prop.z = params->k_drag*(T[0] - T[1] + T[2] - T[3]);

    // torque from chaging motor speeds
    float Tau_mot_z = params->j_mot * (rpm_dot[0] - rpm_dot[1] + rpm_dot[2] - rpm_dot[3]);

    // torque from angular damping
    Vec3 Tau_aero;
    Tau_aero.x = -params->k_ang_damp * state->omega.x;
    Tau_aero.y = -params->k_ang_damp * state->omega.y;
    Tau_aero.z = -params->k_ang_damp * state->omega.z;
    
    // gyroscopic torque
    Vec3 Tau_iner;
    Tau_iner.x = (params->iyy - params->izz) * state->omega.y * state->omega.z;
    Tau_iner.y = (params->izz - params->ixx) * state->omega.z * state->omega.x;
    Tau_iner.z = (params->ixx - params->iyy) * state->omega.x * state->omega.y;

    // angular velocity rates
    Vec3 w_dot;
    w_dot.x = (Tau_prop.x + Tau_aero.x + Tau_iner.x) / params->ixx;
    w_dot.y = (Tau_prop.y + Tau_aero.y + Tau_iner.y) / params->iyy;
    w_dot.z = (Tau_prop.z + Tau_aero.z + Tau_iner.z + Tau_mot_z) / params->izz;

    derivatives->vel = state->vel;
    derivatives->v_dot = v_dot;
    derivatives->q_dot = q_dot;
    derivatives->w_dot = w_dot;
    for (int i = 0; i < 4; i++) {
        derivatives->rpm_dot[i] = rpm_dot[i];
    }
}

static void step(State* initial, StateDerivative* deriv, float dt, State* output) {
    output->pos = add3(initial->pos, scalmul3(deriv->vel, dt));
    output->vel = add3(initial->vel, scalmul3(deriv->v_dot, dt));
    output->quat = add_quat(initial->quat, scalmul_quat(deriv->q_dot, dt));
    output->omega = add3(initial->omega, scalmul3(deriv->w_dot, dt));
    for (int i = 0; i < 4; i++) {
        output->rpms[i] = initial->rpms[i] + deriv->rpm_dot[i] * dt;
    }
    quat_normalize(&output->quat);
}

void rk4_step(State* state, Params* params, float* actions, float dt) {
    StateDerivative k1, k2, k3, k4;
    State temp_state;

    compute_derivatives(state, params, actions, &k1);

    step(state, &k1, dt * 0.5f, &temp_state);
    compute_derivatives(&temp_state, params, actions, &k2);

    step(state, &k2, dt * 0.5f, &temp_state);
    compute_derivatives(&temp_state, params, actions, &k3);

    step(state, &k3, dt, &temp_state);
    compute_derivatives(&temp_state, params, actions, &k4);

    float dt_6 = dt / 6.0f;

    state->pos.x += (k1.vel.x + 2.0f * k2.vel.x + 2.0f * k3.vel.x + k4.vel.x) * dt_6;
    state->pos.y += (k1.vel.y + 2.0f * k2.vel.y + 2.0f * k3.vel.y + k4.vel.y) * dt_6;
    state->pos.z += (k1.vel.z + 2.0f * k2.vel.z + 2.0f * k3.vel.z + k4.vel.z) * dt_6;

    state->vel.x += (k1.v_dot.x + 2.0f * k2.v_dot.x + 2.0f * k3.v_dot.x + k4.v_dot.x) * dt_6;
    state->vel.y += (k1.v_dot.y + 2.0f * k2.v_dot.y + 2.0f * k3.v_dot.y + k4.v_dot.y) * dt_6;
    state->vel.z += (k1.v_dot.z + 2.0f * k2.v_dot.z + 2.0f * k3.v_dot.z + k4.v_dot.z) * dt_6;

    state->quat.w += (k1.q_dot.w + 2.0f * k2.q_dot.w + 2.0f * k3.q_dot.w + k4.q_dot.w) * dt_6;
    state->quat.x += (k1.q_dot.x + 2.0f * k2.q_dot.x + 2.0f * k3.q_dot.x + k4.q_dot.x) * dt_6;
    state->quat.y += (k1.q_dot.y + 2.0f * k2.q_dot.y + 2.0f * k3.q_dot.y + k4.q_dot.y) * dt_6;
    state->quat.z += (k1.q_dot.z + 2.0f * k2.q_dot.z + 2.0f * k3.q_dot.z + k4.q_dot.z) * dt_6;

    state->omega.x += (k1.w_dot.x + 2.0f * k2.w_dot.x + 2.0f * k3.w_dot.x + k4.w_dot.x) * dt_6;
    state->omega.y += (k1.w_dot.y + 2.0f * k2.w_dot.y + 2.0f * k3.w_dot.y + k4.w_dot.y) * dt_6;
    state->omega.z += (k1.w_dot.z + 2.0f * k2.w_dot.z + 2.0f * k3.w_dot.z + k4.w_dot.z) * dt_6;

    for (int i = 0; i < 4; i++) {
        state->rpms[i] += (k1.rpm_dot[i] + 2.0f * k2.rpm_dot[i] + 2.0f * k3.rpm_dot[i] + k4.rpm_dot[i]) * dt_6;
    }

    quat_normalize(&state->quat);
}

void move_drone(Drone* drone, float* actions) {
    // clamp actions
    clamp4(actions, -1.0f, 1.0f);

    // Domain randomized dt
    float dt = DT * rndf(1.0f - DT_RNG, 1.0 + DT_RNG);

    // update drone state
    drone->prev_pos = drone->state.pos;
    rk4_step(&drone->state, &drone->params, actions, dt);

    // clamp and normalise for observations
    clamp3(&drone->state.vel, -drone->params.max_vel, drone->params.max_vel);
    clamp3(&drone->state.omega, -drone->params.max_omega, drone->params.max_omega);
}

void reset_rings(Ring* ring_buffer, int num_rings, float ring_radius) {
    ring_buffer[0] = rndring(ring_radius);
    
    // ensure rings are spaced at least 2*ring_radius apart
    for (int i = 1; i < num_rings; i++) {
        do {
            ring_buffer[i] = rndring(ring_radius);
        }  while (norm3(sub3(ring_buffer[i].pos, ring_buffer[i - 1].pos)) < 2.0f*ring_radius);
    }   
}

float check_ring(Drone* drone, Ring* ring) {
    // previous dot product negative if on the 'entry' side of the ring's plane
    float prev_dot = dot3(sub3(drone->prev_pos, ring->pos), ring->normal);

    // new dot product positive if on the 'exit' side of the ring's plane
    float new_dot = dot3(sub3(drone->state.pos, ring->pos), ring->normal);

    bool valid_dir = (prev_dot < 0.0f && new_dot > 0.0f);
    bool invalid_dir = (prev_dot > 0.0f && new_dot < 0.0f);

    // if we have crossed the plane of the ring
    if (valid_dir || invalid_dir) {
        // find intesection with ring's plane
        Vec3 dir = sub3(drone->state.pos, drone->prev_pos);
        float t = -prev_dot / dot3(ring->normal, dir); // possible nan

        Vec3 intersection = add3(drone->prev_pos, scalmul3(dir, t));
        float dist = norm3(sub3(intersection, ring->pos));

        // reward or terminate based on distance to ring center
        if (dist < (ring->radius - 0.5) && valid_dir) {
            return 1.0f;
        } else if (dist < ring->radius + 0.5) {
            return -0.0f;
        }
    }
    return 0.0f;
}



================================================
FILE: pufferlib/ocean/enduro/binding.c
================================================
#include "enduro.h"
#include <Python.h>
#define Env Enduro
#include "../env_binding.h"

static int my_init(Env* env, PyObject* args, PyObject* kwargs) {
    env->width = unpack(kwargs, "width");
    env->height = unpack(kwargs, "height");
    env->car_width = unpack(kwargs, "car_width");
    env->car_height = unpack(kwargs, "car_height");
    env->max_enemies = unpack(kwargs, "max_enemies");
    env->continuous = unpack(kwargs, "continuous");

    PyObject* seed_val = PyDict_GetItemString(kwargs, "seed");
    if (seed_val) {
        env->seed = unpack(kwargs, "seed");
        // Initialize the RNG state with the seed
        env->rng_state = env->seed;
    }
    
    return 0;
}

static int my_log(PyObject* dict, Log* log) {
    assign_to_dict(dict, "perf", log->perf);
    assign_to_dict(dict, "score", log->score);
    assign_to_dict(dict, "episode_return", log->episode_return);
    assign_to_dict(dict, "episode_length", log->episode_length);
    assign_to_dict(dict, "reward", log->reward);
    assign_to_dict(dict, "step_rew_car_passed_no_crash", log->step_rew_car_passed_no_crash);
    assign_to_dict(dict, "crashed_penalty", log->crashed_penalty);
    assign_to_dict(dict, "passed_cars", log->passed_cars);
    assign_to_dict(dict, "passed_by_enemy", log->passed_by_enemy);
    assign_to_dict(dict, "cars_to_pass", log->cars_to_pass);
    assign_to_dict(dict, "days_completed", log->days_completed);
    assign_to_dict(dict, "days_failed", log->days_failed);
    assign_to_dict(dict, "collisions_player_vs_car", log->collisions_player_vs_car);
    assign_to_dict(dict, "collisions_player_vs_road", log->collisions_player_vs_road);
    return 0;
}



================================================
FILE: pufferlib/ocean/enduro/enduro.c
================================================
// puffer_enduro.c

#define MAX_ENEMIES 10

#include <stdio.h>
#include <stdlib.h>
#include <stddef.h>
#include <time.h>
#include "enduro.h"
#include "raylib.h"
#include "puffernet.h"

void get_input(Enduro* env) {
        if ((IsKeyDown(KEY_DOWN) && IsKeyDown(KEY_RIGHT)) || (IsKeyDown(KEY_S) && IsKeyDown(KEY_D))) {
            env->actions[0] = ACTION_DOWNRIGHT; // Decelerate and move right
        } else if ((IsKeyDown(KEY_DOWN) && IsKeyDown(KEY_LEFT)) || (IsKeyDown(KEY_S) && IsKeyDown(KEY_A))) {
            env->actions[0] = ACTION_DOWNLEFT; // Decelerate and move left
        } else if (IsKeyDown(KEY_SPACE) && (IsKeyDown(KEY_RIGHT) || IsKeyDown(KEY_D))) {
            env->actions[0] = ACTION_RIGHTFIRE; // Accelerate and move right
        } else if (IsKeyDown(KEY_SPACE) && (IsKeyDown(KEY_LEFT) || IsKeyDown(KEY_A))) {
            env->actions[0] = ACTION_LEFTFIRE; // Accelerate and move left   
        } else if (IsKeyDown(KEY_SPACE)) {
            env->actions[0] = ACTION_FIRE; // Accelerate
        } else if (IsKeyDown(KEY_DOWN) || IsKeyDown(KEY_S)) {
            env->actions[0] = ACTION_DOWN; // Decelerate
        } else if (IsKeyDown(KEY_LEFT) || IsKeyDown(KEY_A)) {
            env->actions[0] = ACTION_LEFT; // Move left
        } else if (IsKeyDown(KEY_RIGHT) || IsKeyDown(KEY_D)) {
            env->actions[0] = ACTION_RIGHT; // Move right
        } else {
            env->actions[0] = ACTION_NOOP; // No action
        }
}

int demo() {
    Weights* weights = load_weights("resources/enduro/enduro_weights.bin", 142218);
    int logit_sizes[1] = {9};
    LinearLSTM* net = make_linearlstm(weights, 1, 68, logit_sizes, 1);

    Enduro env = {
        .num_envs = 1,
        .max_enemies = MAX_ENEMIES,
        .obs_size = OBSERVATIONS_MAX_SIZE
    };

    allocate(&env);

    init(&env);
    c_reset(&env);
    c_render(&env);

    while (!WindowShouldClose()) {
        if (IsKeyDown(KEY_LEFT_SHIFT)) {
            get_input(&env);
        } else {
            forward_linearlstm(net, env.observations, env.actions);
        }

        c_step(&env);
        c_render(&env);
    }

    free_linearlstm(net);
    free(weights);
    free_allocated(&env);
    return 0;
}

void perftest(float test_time) {
    Enduro env = {
        .num_envs = 1,
        .max_enemies = MAX_ENEMIES,
        .obs_size = OBSERVATIONS_MAX_SIZE
    };

    allocate(&env);
    init(&env);
    c_reset(&env);

    int start = time(NULL);
    int i = 0;
    while (time(NULL) - start < test_time) {
        env.actions[0] = rand()%9;
        c_step(&env);
        i++;
    }

    int end = time(NULL);
    printf("SPS: %f\n", i / (float)(end - start));
    free_allocated(&env);
}

int main() {
   demo();
   //perftest(10.0f);
   return 0;
}



================================================
FILE: pufferlib/ocean/enduro/enduro.py
================================================
import random
import numpy as np
import gymnasium

import pufferlib
from pufferlib.ocean.enduro import binding

class Enduro(pufferlib.PufferEnv):
    def __init__(self, num_envs=1, render_mode=None,
                 width=152, height=210, car_width=16, car_height=11,
                 max_enemies=10, frameskip=1, continuous=False,
                 log_interval=128, buf=None, seed=None):
        
        self.single_observation_space = gymnasium.spaces.Box(
            low=0, high=1, shape=(8 + (5 * max_enemies) + 9 + 1,), dtype=np.float32
        )
        # Example: 9 discrete actions
        self.single_action_space = gymnasium.spaces.Discrete(9)
        
        self.render_mode = render_mode
        self.num_agents = num_envs
        self.continuous = continuous
        self.log_interval = log_interval
        self.human_action = None
        self.tick = 0
        if seed is None:
            self.seed = random.randint(1, 1000000)
        else:
            self.seed = 0
        super().__init__(buf)

        self.c_envs = binding.vec_init(
            self.observations, self.actions, self.rewards,
            self.terminals, self.truncations, num_envs, self.seed,
            width=width, height=height, car_width=car_width,
            car_height=car_height, max_enemies=max_enemies,
            frameskip=frameskip, continuous=continuous
        )

    def reset(self, seed=None):
        binding.vec_reset(self.c_envs, seed)
        self.tick = 0
        return self.observations, []

    def step(self, actions):
        self.actions[:] = actions
        
        self.tick += 1
        binding.vec_step(self.c_envs)
        
        info = []
        if self.tick % self.log_interval == 0:
            info.append(binding.vec_log(self.c_envs))
        
        return (self.observations, self.rewards,
            self.terminals, self.truncations, info)

    def render(self):
        binding.vec_render(self.c_envs, 0)

    def close(self):
        binding.vec_close(self.c_envs)

def test_performance(timeout=10, atn_cache=1024):
    env = Enduro(num_envs=2)
    env.reset(env.seed)
    tick = 0

    actions = np.random.randint(0, env.single_action_space.n, (atn_cache, env.num_agents))

    import time
    start = time.time()
    while time.time() - start < timeout:
        atn = actions[tick % atn_cache]
        env.step(atn)
        tick += 1

    print(f'SPS: {env.num_agents * tick / (time.time() - start)}')

if __name__ == '__main__':
    test_performance()


================================================
FILE: pufferlib/ocean/freeway/binding.c
================================================
#include "freeway.h"

#define Env Freeway
#include "../env_binding.h"

static int my_init(Env* env, PyObject* args, PyObject* kwargs) {
    env->frameskip = unpack(kwargs, "frameskip");
    env->width = unpack(kwargs, "width");
    env->height = unpack(kwargs, "height");
    env->player_width = unpack(kwargs, "player_width");
    env->player_height = unpack(kwargs, "player_height");
    env->car_width = unpack(kwargs, "car_width");
    env->car_height = unpack(kwargs, "car_height");
    env->lane_size = unpack(kwargs, "lane_size");
    env->level = unpack(kwargs, "level");
    env->difficulty = unpack(kwargs, "difficulty");
    env->use_dense_rewards = unpack(kwargs, "use_dense_rewards");
    env->env_randomization = unpack(kwargs, "env_randomization");
    env->enable_human_player = unpack(kwargs, "enable_human_player");
    init(env);
    return 0;
}

static int my_log(PyObject* dict, Log* log) {
    assign_to_dict(dict, "perf", log->perf);
    assign_to_dict(dict, "score", log->score);
    assign_to_dict(dict, "episode_return", log->episode_return);
    assign_to_dict(dict, "up_action_frac", log->up_action_frac);
    assign_to_dict(dict, "hits", log->hits);
    return 0;
}



================================================
FILE: pufferlib/ocean/freeway/freeway.c
================================================
#include <time.h>
#include "freeway.h"
#include "puffernet.h"
#include<unistd.h>

int main() {
    Weights* weights = load_weights("resources/freeway/freeway_weights.bin", 137092);
    int logit_sizes[1] = {3};
    LinearLSTM* net = make_linearlstm(weights, 1, 34, logit_sizes, 1);

    Freeway env = {
        .frameskip=4,
        .width=1216,
        .height=720,
        .player_width=64,
        .player_height=64,
        .car_width=64,
        .car_height=40,
        .lane_size=64,
        .difficulty=0,
        .level=4,
        .use_dense_rewards=1,
        .env_randomization=1,
        .enable_human_player=1,
    };
    allocate(&env);

    env.client = make_client(&env);

    c_reset(&env);
    while (!WindowShouldClose()) {
        forward_linearlstm(net, env.observations, env.actions);
        env.human_actions[0] = 0;
        if (IsKeyDown(KEY_UP)  || IsKeyDown(KEY_W)) env.human_actions[0] = 1;
        if (IsKeyDown(KEY_DOWN) || IsKeyDown(KEY_S)) env.human_actions[0] = 2;
        c_step(&env);
        c_render(&env);
        
    }
    free_allocated(&env);
    close_client(env.client);
}



================================================
FILE: pufferlib/ocean/freeway/freeway.h
================================================
#include <stdlib.h>
#include <math.h>
#include <assert.h>
#include <unistd.h>
#include <limits.h>
#include <string.h>
#include "raylib.h"
#include "freeway_levels.h"

#define min(a, b) (((a) < (b)) ? (a) : (b))
#define max(a, b) (((a) > (b)) ? (a) : (b))

#define NOOP 0
#define UP 1
#define DOWN 2

// Gameplay related
#define TICK_RATE 1.0f/60.0f
#define GAME_LENGTH 136.0f // Game length in seconds
#define RANDOMIZE_SPEED_FREQ 360 // How many ticks before randomize the speed of the enemies
#define TICKS_STUNT 40
#define PENALTY_HIT -0.01f // Penalty for hitting an enemy
// Rendering related
#define HALF_LINEWIDTH 2
#define DASH_SPACING 32
#define DASH_SIZE 32

// Based on https://ale.farama.org/environments/freeway/
typedef struct Log Log;
struct Log {
    float perf;
    float score;
    float episode_return;
    float episode_length;
    float up_action_frac;
    float hits;
    float n;
};

typedef struct FreewayPlayer FreewayPlayer;
struct FreewayPlayer {
    float player_x;
    float player_y;
    int best_lane_idx;// furthest lane achieved so far
    int ticks_stunts_left; 
    int score;
    int hits;
    int is_human;
};

typedef struct FreewayEnemy FreewayEnemy;
struct FreewayEnemy {
    float enemy_x;
    float enemy_y;
    float enemy_initial_x;
    float enemy_vx; // velocity in pixels per second
    int speed_randomization; // 0 for no randomization, 1 for randomization
    int initial_speed_idx; // index of the initial speed in the speed array
    int current_speed_idx; // index of the current speed in the speed array
    int is_enabled;
    int lane_idx; // lane index
    int type;
    int enemy_width;
    int enemy_height;
};

typedef struct Client Client;
typedef struct Freeway Freeway;
struct Freeway {
    Client* client;
    Log log;
    float* observations;
    int* actions;
    int* human_actions;
    float* rewards;
    unsigned char* terminals;

    FreewayPlayer ai_player; // Player-Related
    FreewayPlayer human_player; 
    int player_width;
    int player_height; 
    float ep_return;
    int up_count;

    FreewayEnemy* enemies; // Enemy-Related
    int car_width;
    int car_height;
    int truck_width;
    int truck_height;
    
    int difficulty; // Global
    int level; 
    int lane_size;
    float road_start;
    float road_end;
    int width;
    int height;
    int tick;
    float time_left;
    int frameskip;
    int use_dense_rewards;
    int env_randomization;
    int enable_human_player;
};

void load_level(Freeway* env, int level) {
    FreewayEnemy* enemy;
    for (int lane = 0; lane < NUM_LANES; lane++) {
        for (int i = 0; i < MAX_ENEMIES_PER_LANE; i++){
            enemy = &env->enemies[lane * MAX_ENEMIES_PER_LANE + i];
            enemy->is_enabled = (i < ENEMIES_PER_LANE[level][lane]);
            enemy->enemy_x = 0.0f;
            enemy->enemy_initial_x = ENEMIES_INITIAL_X[level][lane][i] * env->width;
            enemy->speed_randomization = SPEED_RANDOMIZATION[level];
            enemy->initial_speed_idx = ENEMIES_INITIAL_SPEED_IDX[level][lane];
            enemy->current_speed_idx = enemy->initial_speed_idx;
            enemy->lane_idx = lane;
            enemy->enemy_y = (env->road_start + (env->road_end - env->road_start) * lane / (float) NUM_LANES) - env->lane_size  / 2;
            enemy->type = ENEMIES_TYPES[level][lane];
            enemy->enemy_width = enemy->type == 0 ? env->car_width : env->truck_width;
            enemy->enemy_height = enemy->type == 0 ? env->car_height : env->truck_height;

            enemy->enemy_vx = enemy->lane_idx < NUM_LANES/2 ? SPEED_VALUES[enemy->current_speed_idx] * TICK_RATE * env->width: -SPEED_VALUES[enemy->current_speed_idx] * TICK_RATE * env->width;

        }
    }
}

void init(Freeway* env) {
    env->ai_player.player_x = env->width / 4;
    env->ai_player.player_y = env->height / 2;
    env->ai_player.best_lane_idx = 0;
    env->ai_player.ticks_stunts_left = 0;
    env->ai_player.score = 0;
    env->ai_player.is_human = 0;
    env->ai_player.hits = 0;

    env->human_player.player_x = 3 * env->width / 4;
    env->human_player.player_y = env->height / 2;
    env->human_player.best_lane_idx = 0;
    env->human_player.ticks_stunts_left = 0;
    env->human_player.score = 0;
    env->human_player.is_human = 1;
    env->human_player.hits = 0;
    

    env->truck_height = env->car_height;
    env->truck_width = 2*env->car_width;
    env->road_start = env->height / 2 + (NUM_LANES * env->lane_size) / 2;
    env->road_end = env->road_start - (NUM_LANES * env->lane_size);
    //enemies 
    env->enemies = (FreewayEnemy*)calloc(NUM_LANES*MAX_ENEMIES_PER_LANE, sizeof(FreewayEnemy));
    env->human_actions = (int*)calloc(1, sizeof(int));
    if ((env->level < 0) || (env->level >= NUM_LEVELS)) {
        env->level = rand() % NUM_LEVELS;
    }
    load_level(env, env->level);
}

void allocate(Freeway* env) {
    init(env);
    env->observations = (float*)calloc(4 + NUM_LANES*MAX_ENEMIES_PER_LANE, sizeof(float));
    env->actions = (int*)calloc(1, sizeof(int));
    env->rewards = (float*)calloc(1, sizeof(float));
    env->terminals = (unsigned char*)calloc(1, sizeof(unsigned char));
}

void c_close(Freeway* env) {
    free(env->human_actions);
    free(env->enemies);
}

void free_allocated(Freeway* env) {
    free(env->actions);
    free(env->observations);
    free(env->terminals);
    free(env->rewards);
    c_close(env);
}

void add_log(Freeway* env) {
    env->log.episode_length += env->tick;
    env->log.episode_return += env->ep_return;
    env->log.score += env->ai_player.score;
    env->log.perf += env->ai_player.score / ((float) HUMAN_HIGH_SCORE[env->level] * (GAME_LENGTH / 136.0f ));
    env->log.up_action_frac += env->up_count / (float) env->tick;
    env->log.hits += env->ai_player.hits;
    env->log.n += 1;
}

void compute_observations(Freeway* env) {
    env->observations[0] = env->ai_player.player_y / env->height;
    env->observations[1] = env->ai_player.best_lane_idx /(float) NUM_LANES;
    env->observations[2] = env->ai_player.score / (float) HUMAN_HIGH_SCORE[env->level];
    env->observations[3] = (env->ai_player.ticks_stunts_left  > 0);

    FreewayEnemy* enemy;
    for (int lane = 0; lane < NUM_LANES; lane++) {
        for (int i = 0; i < MAX_ENEMIES_PER_LANE; i++){
            enemy = &env->enemies[lane*MAX_ENEMIES_PER_LANE + i];
            if (enemy->is_enabled){
                env->observations[4 + lane * MAX_ENEMIES_PER_LANE + i] = enemy->enemy_x / env->width;
                env->observations[4 + lane * MAX_ENEMIES_PER_LANE + i] += (lane < NUM_LANES/2 ? enemy->enemy_height/(2 * env->width): -enemy->enemy_height/(2 * env->width));
            }
            else {
                env->observations[4 + lane * MAX_ENEMIES_PER_LANE + i] = 0.0f;
            }
        }
    }   
}

void spawn_enemies(Freeway* env) {
    float lane_offset_x;
    FreewayEnemy* enemy;
    for (int lane = 0; lane < NUM_LANES; lane++) {
        lane_offset_x =  env->width * (rand() / (float) RAND_MAX);
        for (int i = 0; i < MAX_ENEMIES_PER_LANE; i++){
            enemy = &env->enemies[lane * MAX_ENEMIES_PER_LANE + i];
            if (enemy->is_enabled){
                enemy->enemy_x = enemy->enemy_initial_x;
                if (lane>=NUM_LANES/2){
                    enemy->enemy_x = env->width - enemy->enemy_x;
                }
                if (env->env_randomization){
                    enemy->enemy_x += lane_offset_x;
                }
            }
        }
    }
}

void reset_player(Freeway* env, FreewayPlayer* player) {
    player->player_y = env->height - env->player_height / 2;
    player->ticks_stunts_left = 0;
}

bool check_collision(float player_min_x, float player_max_x,
                      float player_miny, float player_maxy,
                      float enemy_minx, float enemy_maxx,
                      float enemy_miny, float enemy_maxy) {
    return (player_min_x < enemy_maxx && player_max_x > enemy_minx &&
            player_miny < enemy_maxy && player_maxy > enemy_miny);
}

bool check_enemy_collisions(Freeway* env, FreewayPlayer* player){
    FreewayEnemy* enemy;
    for (int lane = 0; lane < NUM_LANES; lane++) {
        for (int i = 0; i < MAX_ENEMIES_PER_LANE; i++) {
            enemy = &env->enemies[lane*MAX_ENEMIES_PER_LANE + i];
            if (enemy->is_enabled) {
                float player_min_x = player->player_x - env->player_width / 2;
                float player_max_x = player->player_x + env->player_width / 2;
                float player_miny = player->player_y - env->player_height / 2;
                float player_maxy = player->player_y + env->player_height / 2;

                float enemy_minx = enemy->enemy_x - enemy->enemy_width / 2;
                float enemy_maxx = enemy->enemy_x + enemy->enemy_width / 2;
                float enemy_miny = enemy->enemy_y - enemy->enemy_height / 2;
                float enemy_maxy = enemy->enemy_y + enemy->enemy_height / 2;

                if (check_collision(player_min_x, player_max_x, player_miny, player_maxy,
                                    enemy_minx, enemy_maxx, enemy_miny, enemy_maxy)) {
                    return true;
                }
            }
        }
    }
    return false;
}

void reached_end(Freeway* env, FreewayPlayer* player){
    reset_player(env, player);
    player->best_lane_idx = 0;
    player->score += 1;
}

void clip_player_position(Freeway* env, FreewayPlayer* player){
    if (player->player_y <= env->player_height/2){
        player->player_y = fmaxf(env->player_height/2, player->player_y);
    } else {
        player->player_y = fminf(env->height - env->player_height/2, player->player_y);
    }
}

void clip_enemy_position(Freeway* env, FreewayEnemy* enemy){
    if (enemy->enemy_x > env->width + enemy->enemy_width / 2) {
        enemy->enemy_x -= env->width;
    }
    else if (enemy->enemy_x < -enemy->enemy_width / 2){
        enemy->enemy_x += env->width;
    }
}

void randomize_enemy_speed(Freeway* env) {
    FreewayEnemy* enemy;
    for (int lane = 0; lane < NUM_LANES; lane++) {
        int delta_speed = (rand() % 3) - 1; // Randomly increase or decrease speed
        for (int i = 0; i < MAX_ENEMIES_PER_LANE; i++) {
            if (enemy->speed_randomization) {
                enemy = &env->enemies[lane*MAX_ENEMIES_PER_LANE + i];
                enemy->current_speed_idx = min(max(enemy->initial_speed_idx-2, enemy->current_speed_idx), enemy->initial_speed_idx+2);
                enemy->current_speed_idx = min(max(0, enemy->current_speed_idx + delta_speed), 5);
                enemy->enemy_vx = enemy->lane_idx < NUM_LANES/2 ? SPEED_VALUES[enemy->current_speed_idx] * TICK_RATE * env->width: -SPEED_VALUES[enemy->current_speed_idx] * TICK_RATE * env->width;
            }
        }
    }
}

void move_enemies(Freeway* env) {
    FreewayEnemy* enemy;
    for (int lane = 0; lane < NUM_LANES; lane++) {
        for (int i = 0; i < MAX_ENEMIES_PER_LANE; i++) {
            enemy = &env->enemies[lane*MAX_ENEMIES_PER_LANE + i];
            if (enemy->is_enabled) {
                enemy->enemy_x += enemy->enemy_vx;
            }
            clip_enemy_position(env, enemy);
        }
    }
}

void step_player(Freeway* env, FreewayPlayer* player, int action) {
    float player_dy = 0.0;

    if (action == DOWN) {
        player_dy = -1.0;
    } 
    else if (action == UP) {
        player_dy = 1.0;
        env->up_count += 1;
    }

    if (player->ticks_stunts_left == 0){
        player->player_y -= player_dy * BASE_PLAYER_SPEED * env->height * TICK_RATE;
    }
    else {
        player->ticks_stunts_left -= 1;
        if (env->difficulty == 0){
            player->player_y += 1.5f * env->lane_size / (float) TICKS_STUNT;
        } 
    }
    clip_player_position(env, player);
    
    if (player->ticks_stunts_left == 0) {
        if (check_enemy_collisions(env, player) && player->ticks_stunts_left < TICKS_STUNT/4){
            player->hits+=1;
            player->ticks_stunts_left = TICKS_STUNT;
            if (env->use_dense_rewards){
                env->rewards[0] += PENALTY_HIT;
                env->ep_return += PENALTY_HIT;
            }
            if (env->difficulty == 1){
                reset_player(env, player);
            }  
        }
    }

    if (player->player_y <= env->road_start - (player->best_lane_idx+1) * env->lane_size){
        player->best_lane_idx += 1; 
        if (env->use_dense_rewards){
            env->rewards[0] += 1.0 / (float) NUM_LANES;
            env->ep_return += 1.0 / (float) NUM_LANES;
        }
        else{
            if (player->best_lane_idx == NUM_LANES){
                env->rewards[0] = 1.0;
                env->ep_return += 1.0;
            }
        }
    }

    if (player->best_lane_idx == NUM_LANES) {
        reached_end(env, player);
        env->rewards[0] += 1.0;
        env->ep_return += 1.0;
    }
}
void c_reset(Freeway* env) {
    env->ai_player.player_y = env->height / 2;
    env->ai_player.best_lane_idx = 0;
    env->ai_player.ticks_stunts_left = 0;
    env->ai_player.score = 0;
    env->ai_player.hits = 0;

    env->human_player.player_y = env->height / 2;
    env->human_player.best_lane_idx = 0;
    env->human_player.ticks_stunts_left = 0;
    env->human_player.score = 0;
    env->human_player.hits = 0;

    env->ep_return = 0.0;
    env->tick = 0;
    env->up_count = 0;
    env->time_left = GAME_LENGTH;
    reset_player(env, &env->ai_player);
    reset_player(env, &env->human_player);
    spawn_enemies(env);
    compute_observations(env);
}

void c_step(Freeway* env) {
    env->terminals[0] = 0;
    env->rewards[0] = 0.0;
    int ai_action = env->actions[0];
    int human_action = env->human_actions[0];
    env->time_left = GAME_LENGTH - env->tick*TICK_RATE;

    for (int i = 0; i < env->frameskip; i++) {
        env->tick += 1;
        step_player(env, &env->ai_player, ai_action);
        if (env->enable_human_player){
            step_player(env, &env->human_player, human_action);
        }
        move_enemies(env);
    }
    if (env->tick * TICK_RATE >= GAME_LENGTH) {
        env->terminals[0] = 1.0;
        add_log(env);
        c_reset(env);
    }
    if (env->tick % RANDOMIZE_SPEED_FREQ == 0) {
        randomize_enemy_speed(env);
    }
    compute_observations(env);
}



typedef struct Client Client;
struct Client {
    Texture2D chicken;
    Texture2D puffer;
    Texture2D car_body;
    Texture2D car_wheels;
    Texture2D truck_body;
    Texture2D truck_wheels;
};

static inline bool file_exists(const char* path) {
    return access(path, F_OK) != -1;
}

Client* make_client(Freeway* env) {
    Client* client = (Client*)calloc(1, sizeof(Client));
    
    InitWindow(env->width, env->height, "PufferLib Freeway");
    SetTargetFPS(60/env->frameskip);
    client->car_body = LoadTexture("resources/freeway/tex_car_body.png");
    client->car_wheels = LoadTexture("resources/freeway/tex_car_wheels.png");
    client->chicken = LoadTexture("resources/freeway/tex_chicken0.png");
    client->puffer = LoadTexture("resources/shared/puffers.png");
    client->truck_body = LoadTexture("resources/freeway/tex_truck_body.png");
    client->truck_wheels = LoadTexture("resources/freeway/tex_truck_wheels.png");
    return client;
}

void close_client(Client* client) {
    CloseWindow();
    free(client);
}

Color CAR_COLORS[10] = {
    (Color){ 139, 0, 0, 255 },      // Dark Red
    (Color){ 255, 140, 0, 255 },    // Dark Orange
    (Color){ 204, 204, 0, 255 },    // Dark Yellow
    (Color){ 0, 100, 0, 255 },      // Dark Green
    (Color){ 0, 0, 139, 255 },      // Dark Blue
    (Color){ 139, 0, 0, 255 },      // Dark Red
    (Color){ 255, 140, 0, 255 },    // Dark Orange
    (Color){ 204, 204, 0, 255 },    // Dark Yellow
    (Color){ 0, 100, 0, 255 },      // Dark Green
    (Color){ 0, 0, 139, 255 }       // Dark Blue
};
void c_render(Freeway* env) {
    if (env->client == NULL) {
        env->client = make_client(env);
    }

    Client* client = env->client;

    if (IsKeyDown(KEY_ESCAPE)) {
        exit(0);
    }
    if (IsKeyPressed(KEY_TAB)) {
        ToggleFullscreen();
    }

    BeginDrawing();
    ClearBackground((Color){170, 170, 170, 255});
    
    // Draw the road
    DrawRectangle(
        0, 
        env->road_end,
        env->width, env->road_start - env->road_end, (Color){150, 150, 150, 255}
    );
    DrawRectangle(
        0, 
        env->road_end- HALF_LINEWIDTH,
        env->width, 2*HALF_LINEWIDTH, (Color){0, 255}
    );
    DrawRectangle(
        0, 
        env->road_start - HALF_LINEWIDTH,
        env->width, 2*HALF_LINEWIDTH, (Color){0, 255}
    );

    for (int lane = 1; lane < NUM_LANES; lane++) {
        if (lane != NUM_LANES/2){
            for (int dash = 0; dash < env->width / (DASH_SPACING + DASH_SIZE) ; dash++){
                int dash_start = DASH_SPACING / 2 + (DASH_SPACING + DASH_SIZE) * dash;            
                DrawRectangle(
                    dash_start, 
                    env->road_start + (env->road_end - env->road_start) * lane/NUM_LANES - HALF_LINEWIDTH,
                    DASH_SIZE, 2*HALF_LINEWIDTH, (Color){235, 235, 235, 255}
                );
            }
        }
    }

    for (int dash = 0; dash < env->width / (DASH_SPACING + DASH_SIZE) ; dash++){
        int dash_start = DASH_SPACING / 2 + (DASH_SPACING + DASH_SIZE) * dash;            
        DrawRectangle(
            dash_start, 
            env->road_start + (env->road_end - env->road_start) / 2 - 3*HALF_LINEWIDTH,
            DASH_SIZE, 2*HALF_LINEWIDTH, (Color){235, 235, 100, 255}
        );
        DrawRectangle(
            dash_start, 
            env->road_start + (env->road_end - env->road_start) / 2 + HALF_LINEWIDTH,
            DASH_SIZE, 2*HALF_LINEWIDTH, (Color){235, 235, 100, 255}
        );
    }
    
    // Draw ai player
    DrawTexturePro(
        client->puffer,
        (Rectangle){
            0, 0, 128, 128,
        },
        (Rectangle){
            env->ai_player.player_x - env->player_width / 2,
            env->ai_player.player_y - env->player_height / 2,
            env->player_width,
            env->player_height,
        },
        (Vector2){0, 0},
        0,
        WHITE
    );

    DrawTexturePro(
        client->puffer,
        (Rectangle){
            128, 128, 128, 128,
        },
        (Rectangle){
            env->human_player.player_x - env->player_width / 2,
            env->human_player.player_y - env->player_height / 2,
            env->player_width,
            env->player_height,
        },
        (Vector2){0, 0},
        0,
        WHITE
    );

    // Draw enemies
    Rectangle src_rec;
    FreewayEnemy* enemy;
    for (int lane = 0; lane < NUM_LANES; lane++) {
        for (int i = 0; i < MAX_ENEMIES_PER_LANE; i++) {
            enemy = &env->enemies[lane*MAX_ENEMIES_PER_LANE + i];
            if (enemy->is_enabled) {
                Texture2D body = enemy->type == 0 ? client->car_body : client->truck_body;
                Texture2D wheels = enemy->type == 0 ? client->car_wheels : client->truck_wheels;
                if (lane < NUM_LANES/2) {
                    src_rec= enemy->type == 0 ? (Rectangle){16,0,16,10} : (Rectangle){32,10,32,10};
                }
                else {
                    src_rec = enemy->type == 0 ? (Rectangle){16 + 16, 0, -16, 10} : (Rectangle){32 + 32, 10, -32, 10};
                }
                DrawTexturePro(
                    body,
                    src_rec,
                    (Rectangle){
                        enemy->enemy_x - enemy->enemy_width / 2, 
                        enemy->enemy_y - enemy->enemy_height/ 2,
                        enemy->enemy_width, 
                        enemy->enemy_height,
                    },
                    (Vector2){0, 0},
                    0,
                    CAR_COLORS[lane]
                );
                DrawTexturePro(
                    wheels,
                    src_rec,
                    (Rectangle){
                        enemy->enemy_x - enemy->enemy_width / 2, 
                        enemy->enemy_y - enemy->enemy_height/ 2,
                        enemy->enemy_width, 
                        enemy->enemy_height,
                    },
                    (Vector2){0, 0},
                    0,
                    CAR_COLORS[lane]
                );
            }
        }
    }

    // Draw UI
    int rounded_time_left = round(env->time_left);
    DrawText(TextFormat("P1 Score: %i", env->ai_player.score), 10, 3, 40, (Color) {255, 160, 160, 255});
    DrawText(TextFormat("P2 Score: %i", env->human_player.score), round(0.77*env->width), 3, 40, (Color) {255, 160, 160, 255});
    DrawText(TextFormat("Time: %i", rounded_time_left), round(0.45*env->width) - 40, 3, 40, (Color) {255, 160, 160, 255});

    EndDrawing();

    //PlaySound(client->sound);
}



================================================
FILE: pufferlib/ocean/freeway/freeway.py
================================================
import numpy as np
import gymnasium

import pufferlib
from pufferlib.ocean.freeway import binding


class Freeway(pufferlib.PufferEnv):
    def __init__(
        self,
        num_envs=1,
        render_mode=None,
        frameskip=4,
        width=1216,
        height=720,
        player_width=64,
        player_height=64,
        car_width=64,
        car_height=40,
        lane_size=64,
        difficulty=0,
        level=0,
        use_dense_rewards=True,
        env_randomization=True,
        enable_human_player=False,
        log_interval=128,
        buf=None,
        seed=0,
    ):
        assert level < 8, "Level should be in {0, 1, 2, 3, 4, 5, 6, 7} or -1. Level -1 is a random mix of all 8 supported levels."
        self.single_observation_space = gymnasium.spaces.Box(
            low=0, high=1, shape=(34,), dtype=np.float32
        )
        self.render_mode = render_mode
        self.num_agents = num_envs
        self.log_interval = log_interval
        self.tick = 0

        self.single_action_space = gymnasium.spaces.Discrete(3)

        super().__init__(buf)

        self.c_envs = binding.vec_init(
            self.observations,
            self.actions,
            self.rewards,
            self.terminals,
            self.truncations,
            num_envs,
            seed,
            frameskip=frameskip,
            width=width,
            height=height,
            player_width=player_width,
            player_height=player_height,
            car_width=car_width,
            car_height=car_height,
            lane_size=lane_size,
            difficulty=difficulty,
            level = level,
            enable_human_player=enable_human_player,
            env_randomization=env_randomization,
            use_dense_rewards=use_dense_rewards,
        )

    def reset(self, seed=0):
        binding.vec_reset(self.c_envs, seed)
        self.tick = 0
        return self.observations, []

    def step(self, actions):
        self.actions[:] = actions
            
        self.tick += 1
        binding.vec_step(self.c_envs)

        info = []
        if self.tick % self.log_interval == 0:
            info.append(binding.vec_log(self.c_envs))

        return (self.observations, self.rewards,
            self.terminals, self.truncations, info)

    def render(self):
        binding.vec_render(self.c_envs, 0)

    def close(self):
        binding.vec_close(self.c_envs)

def test_performance(timeout=60, level = 0,atn_cache=1024):
    env = Freeway(num_envs=1024, level=level)
    env.reset()
    tick = 0

    actions = np.random.randint(0, 3, (atn_cache, env.num_agents))

    import time
    start = time.time()
    while time.time() - start < timeout:
        atn = actions[tick % atn_cache]
        env.step(atn)
        tick += 1
    env.close()
    print(f'SPS: %f', env.num_agents * tick / (time.time() - start))

def test_render(timeout=60, level = 0,atn_cache=1024):
    env = Freeway(num_envs=1, level=level)
    env.reset(seed=0)
    tick = 0

    actions = np.random.randint(0, 3, (atn_cache, env.num_agents))

    import time
    start = time.time()
    while time.time() - start < timeout:
        atn = actions[tick % atn_cache]
        obs, rew, term, trunc, i = env.step(atn)
        env.render()
        tick += 1
        if tick == 100:
            env.reset()
    env.close()


if __name__ == '__main__':
    # test_performance()
    for level in range(0,8):
        test_performance(level = level, timeout=5, atn_cache=1024)




================================================
FILE: pufferlib/ocean/freeway/freeway_levels.h
================================================
#define BASE_ROAD_SPEED 1.0f/13.0f // inverse number of seconds to go from the left to the right (slowest enemy)
#define MULT_ROAD_SPEED 1.35 // Factor of increase for the road speed (approximates the ratio between min speed and max speed of lvl1 (13/2.5)^(1/5))
#define BASE_PLAYER_SPEED 1.0f/3.5f // inverse number of seconds to go from the bottom to the top of the screen for the player
#define MAX_ENEMIES_PER_LANE 3
#define NUM_LEVELS 8
#define NUM_LANES 10

const float SPEED0 =  BASE_ROAD_SPEED;
const float SPEED1 =  MULT_ROAD_SPEED*BASE_ROAD_SPEED;
const float SPEED2 =  MULT_ROAD_SPEED*MULT_ROAD_SPEED*BASE_ROAD_SPEED;
const float SPEED3 =  MULT_ROAD_SPEED*MULT_ROAD_SPEED*MULT_ROAD_SPEED*BASE_ROAD_SPEED;
const float SPEED4 =  MULT_ROAD_SPEED*MULT_ROAD_SPEED*MULT_ROAD_SPEED*MULT_ROAD_SPEED*BASE_ROAD_SPEED;
const float SPEED5 =  MULT_ROAD_SPEED*MULT_ROAD_SPEED*MULT_ROAD_SPEED*MULT_ROAD_SPEED*MULT_ROAD_SPEED*BASE_ROAD_SPEED;

const float SPEED_VALUES[6] = {SPEED0, SPEED1, SPEED2, SPEED3, SPEED4, SPEED5};

const int HUMAN_HIGH_SCORE[] = {25, 20, 18, 20, 25, 18, 15, 18};

const int ENEMIES_PER_LANE[NUM_LEVELS][NUM_LANES] = {
    // Level 0
    {1, 1, 1, 1, 1, 1, 1, 1, 1, 1},
    // Level 1
    {1, 2, 2, 3, 2, 1, 2, 3, 2, 2},
    // Level 2
    {3, 3, 1, 3, 1, 1, 3, 1, 3, 1},
    // Level 3
    {1, 1, 1, 1, 1, 1, 1, 1, 1, 1},
    // Level 4
    {1, 1, 1, 1, 1, 1, 1, 1, 1, 1},
    // Level 5
    {1, 2, 2, 3, 2, 1, 2, 3, 2, 2},
    // Level 6
    {3, 3, 1, 3, 1, 1, 3, 1, 3, 1},
    // Level 7
    {1, 1, 1, 1, 1, 1, 1, 1, 1, 1},
};

const float ENEMIES_TYPES[NUM_LEVELS][NUM_LANES] = {
    // Level 0
    {0, 0, 0, 0, 0, 0, 0, 0, 0, 0},
    // Level 1
    {0, 0, 0, 0, 0, 1, 0, 0, 0, 0},
    // Level 2
    {0, 0, 0, 0, 1, 0, 0, 0, 0, 0},
    // Level 3
    {1, 1, 1, 1, 1, 1, 1, 1, 1, 1},
    // Level 4
    {0, 0, 0, 0, 0, 0, 0, 0, 0, 0},
    // Level 5
    {0, 0, 0, 0, 0, 1, 0, 0, 0, 0},
    // Level 6
    {0, 0, 0, 0, 1, 0, 0, 0, 0, 0},
    // Level 7
    {1, 1, 1, 1, 1, 1, 1, 1, 1, 1},
};

const int SPEED_RANDOMIZATION[NUM_LEVELS]= {0,0,0,0,1,1,1,1};


const float ENEMIES_INITIAL_X[NUM_LEVELS][NUM_LANES][MAX_ENEMIES_PER_LANE] = {
    // Level 0
    {
        {0.0, 0.0, 0.0}, // lane 0 to 9
        {0.0, 0.0, 0.0},
        {0.0, 0.0, 0.0},
        {0.0, 0.0, 0.0},
        {0.0, 0.0, 0.0},
        {0.0, 0.0, 0.0},
        {0.0, 0.0, 0.0},
        {0.0, 0.0, 0.0},
        {0.0, 0.0, 0.0},
        {0.0, 0.0, 0.0},
    },
    {
        // Level 1
        {0.0, 0.0, 0.0}, // lane 0 to 9
        {0.0, 0.1, 0.0},
        {0.0, 0.2, 0.0},
        {0.0, 0.1, 0.2},
        {0.0, 0.4, 0.0},
        {0.0, 0.0, 0.0},
        {0.0, 0.4, 0.0},
        {0.0, 0.1, 0.2},
        {0.0, 0.2, 0.0},
        {0.0, 0.1, 0.0},
    },
    {
        // Level 2
        {0.0, 0.2, 0.4}, // lane 0 to 9
        {0.0, 0.2, 0.4},
        {0.0, 0.0, 0.0},
        {0.0, 0.2, 0.4},
        {0.0, 0.0, 0.0},
        {0.0, 0.0, 0.0},
        {0.0, 0.2, 0.4},
        {0.0, 0.0, 0.0},
        {0.0, 0.2, 0.4},
        {0.0, 0.2, 0.4},
    },
    {
        // Level 3
        {0.0, 0.0, 0.0}, // lane 0 to 9
        {0.0, 0.0, 0.0},
        {0.0, 0.0, 0.0},
        {0.0, 0.0, 0.0},
        {0.0, 0.0, 0.0},
        {0.0, 0.0, 0.0},
        {0.0, 0.0, 0.0},
        {0.0, 0.0, 0.0},
        {0.0, 0.0, 0.0},
        {0.0, 0.0, 0.0},
    },
        // Level 4
    {
        {0.0, 0.0, 0.0}, // lane 0 to 9
        {0.0, 0.0, 0.0},
        {0.0, 0.0, 0.0},
        {0.0, 0.0, 0.0},
        {0.0, 0.0, 0.0},
        {0.0, 0.0, 0.0},
        {0.0, 0.0, 0.0},
        {0.0, 0.0, 0.0},
        {0.0, 0.0, 0.0},
        {0.0, 0.0, 0.0},
    },
    {
        // Level 5
        {0.0, 0.0, 0.0}, // lane 0 to 9
        {0.0, 0.1, 0.0},
        {0.0, 0.2, 0.0},
        {0.0, 0.1, 0.2},
        {0.0, 0.4, 0.0},
        {0.0, 0.0, 0.0},
        {0.0, 0.4, 0.0},
        {0.0, 0.1, 0.2},
        {0.0, 0.2, 0.0},
        {0.0, 0.1, 0.0},
    },
    {
        // Level 6
        {0.0, 0.2, 0.4}, // lane 0 to 9
        {0.0, 0.2, 0.4},
        {0.0, 0.0, 0.0},
        {0.0, 0.2, 0.4},
        {0.0, 0.0, 0.0},
        {0.0, 0.0, 0.0},
        {0.0, 0.2, 0.4},
        {0.0, 0.0, 0.0},
        {0.0, 0.2, 0.4},
        {0.0, 0.2, 0.4},
    },
    {
        // Level 7
        {0.0, 0.0, 0.0}, // lane 0 to 9
        {0.0, 0.0, 0.0},
        {0.0, 0.0, 0.0},
        {0.0, 0.0, 0.0},
        {0.0, 0.0, 0.0},
        {0.0, 0.0, 0.0},
        {0.0, 0.0, 0.0},
        {0.0, 0.0, 0.0},
        {0.0, 0.0, 0.0},
        {0.0, 0.0, 0.0},
    }
};

const float ENEMIES_INITIAL_SPEED_IDX[NUM_LEVELS][NUM_LANES] = {
    // Level 0
    {0,1,2,3,4,4,3,2,1,0},
    // Level 1
    {0,1,3,4,5,5,4,3,1,0},
    // Level 2
    {0,1,3,4,5,5,4,3,1,0},
    // Level 3
    {5,4,2,4,5,5,4,2,4,5},
    // Level 4
    {0,1,2,3,4,4,3,2,1,0},
    // Level 5
    {0,1,3,4,5,5,4,3,1,0},
    // Level 6
    {0,1,3,4,5,5,4,3,1,0},
    // Level 7
    {5,4,2,4,5,5,4,2,4,5},
};


================================================
FILE: pufferlib/ocean/g2048/2048.h
================================================
#include <stdlib.h>
#include <stdbool.h>
#include <stdio.h>
#include <time.h>
#include <math.h>
#include <string.h>
#include "raylib.h"

#define SIZE 4
#define EMPTY 0
#define UP 1
#define DOWN 2
#define LEFT 3
#define RIGHT 4

// Precomputed constants
#define REWARD_MULTIPLIER 0.09090909f
#define INVALID_MOVE_PENALTY -0.05f
#define GAME_OVER_PENALTY -1.0f

typedef struct {
    float perf;
    float score;
    float episode_return;
    float episode_length;
    float n;
} Log;

typedef struct {
    Log log;                        // Required
    unsigned char* observations;    // Cheaper in memory if encoded in uint_8
    int* actions;                   // Required
    float* rewards;                 // Required
    unsigned char* terminals;       // Required
    int score;
    int tick;
    unsigned char grid[SIZE][SIZE];
    float episode_reward;           // Accumulate episode reward
    
    // Cached values to avoid recomputation
    int empty_count;
    bool game_over_cached;
    bool grid_changed;
} Game;

// Precomputed color table for rendering optimization
const Color PUFF_BACKGROUND = (Color){6, 24, 24, 255};
const Color PUFF_WHITE = (Color){241, 241, 241, 241};
const Color PUFF_RED = (Color){187, 0, 0, 255};
const Color PUFF_CYAN = (Color){0, 187, 187, 255};

static Color tile_colors[12] = {
    {6, 24, 24, 255}, // Empty/background
    {187, 187, 187, 255}, // 2
    {170, 187, 187, 255}, // 4
    {150, 187, 187, 255}, // 8
    {130, 187, 187, 255},  // 16
    {110, 187, 187, 255},  // 32
    {90, 187, 187, 255},   // 64
    {70, 187, 187, 255}, // 128
    {50, 187, 187, 255},  // 256
    {30, 187, 187, 255},  // 512
    {10, 187, 187, 255},  // 1024
    {0, 187, 187, 255}   // 2048+
};

// --- Logging ---
void add_log(Game* game);

// --- Required functions for env_binding.h ---
void c_reset(Game* env);
void c_step(Game* env);
void c_render(Game* env);
void c_close(Game* env);

// Inline function for updating observations (avoid function call overhead)
static inline void update_observations(Game* game) {
    for (int i = 0; i < SIZE; i++) {
        for (int j = 0; j < SIZE; j++) {
            game->observations[i * SIZE + j] = game->grid[i][j];
        }
    }
}

// Cache empty cell count during grid operations
static inline void update_empty_count(Game* game) {
    int count = 0;
    for (int i = 0; i < SIZE; i++) {
        for (int j = 0; j < SIZE; j++) {
            if (game->grid[i][j] == EMPTY) count++;
        }
    }
    game->empty_count = count;
}

void add_log(Game* game) {
    game->log.score = (float)(1 << game->score);
    game->log.perf += ((float)game->score) * REWARD_MULTIPLIER;
    game->log.episode_length += game->tick;
    game->log.episode_return += game->episode_reward;
    game->log.n += 1;
}

void c_reset(Game* game) {
    for (int i = 0; i < SIZE; i++) {
        for (int j = 0; j < SIZE; j++) {
            game->grid[i][j] = EMPTY;
        }
    }

    game->score = 0;
    game->tick = 0;
    game->episode_reward = 0;
    game->empty_count = SIZE * SIZE;
    game->game_over_cached = false;
    game->grid_changed = true;
    
    if (game->terminals) game->terminals[0] = 0;
    
    // Add two random tiles at the start - optimized version
    for (int added = 0; added < 2; ) {
        int pos = rand() % (SIZE * SIZE);
        int i = pos / SIZE;
        int j = pos % SIZE;
        if (game->grid[i][j] == EMPTY) {
            game->grid[i][j] = (rand() % 10 == 0) ? 2 : 1;
            added++;
            game->empty_count--;
        }
    }
    
    update_observations(game);
}

void add_random_tile(Game* game) {
    if (game->empty_count == 0) return;
    
    // Use reservoir sampling for better performance
    int chosen_pos = -1;
    int count = 0;
    
    for (int pos = 0; pos < SIZE * SIZE; pos++) {
        int i = pos / SIZE;
        int j = pos % SIZE;
        if (game->grid[i][j] == EMPTY) {
            count++;
            if (rand() % count == 0) {
                chosen_pos = pos;
            }
        }
    }
    
    if (chosen_pos >= 0) {
        int i = chosen_pos / SIZE;
        int j = chosen_pos % SIZE;
        game->grid[i][j] = (rand() % 10 == 0) ? 2 : 1;
        game->empty_count--;
        game->grid_changed = true;
    }
    
    update_observations(game);
}

// Optimized slide and merge with fewer memory operations
static inline bool slide_and_merge(unsigned char* row, float* reward) {
    bool moved = false;
    int write_pos = 0;
    
    // Single pass: slide and identify merge candidates
    for (int read_pos = 0; read_pos < SIZE; read_pos++) {
        if (row[read_pos] != EMPTY) {
            if (write_pos != read_pos) {
                row[write_pos] = row[read_pos];
                row[read_pos] = EMPTY;
                moved = true;
            }
            write_pos++;
        }
    }
    
    // Merge pass
    for (int i = 0; i < SIZE - 1; i++) {
        if (row[i] != EMPTY && row[i] == row[i + 1]) {
            row[i]++;
            *reward += ((float)row[i]) * REWARD_MULTIPLIER;
            // Shift remaining elements left
            for (int j = i + 1; j < SIZE - 1; j++) {
                row[j] = row[j + 1];
            }
            row[SIZE - 1] = EMPTY;
            moved = true;
        }
    }
    
    return moved;
}

bool move(Game* game, int direction, float* reward) {
    bool moved = false;
    unsigned char temp[SIZE];
    
    if (direction == UP || direction == DOWN) {
        for (int col = 0; col < SIZE; col++) {
            // Extract column
            for (int i = 0; i < SIZE; i++) {
                int idx = (direction == UP) ? i : SIZE - 1 - i;
                temp[i] = game->grid[idx][col];
            }
            
            if (slide_and_merge(temp, reward)) {
                moved = true;
                // Write back column
                for (int i = 0; i < SIZE; i++) {
                    int idx = (direction == UP) ? i : SIZE - 1 - i;
                    game->grid[idx][col] = temp[i];
                }
            }
        }
    } else {
        for (int row = 0; row < SIZE; row++) {
            // Extract row
            for (int i = 0; i < SIZE; i++) {
                int idx = (direction == LEFT) ? i : SIZE - 1 - i;
                temp[i] = game->grid[row][idx];
            }
            
            if (slide_and_merge(temp, reward)) {
                moved = true;
                // Write back row
                for (int i = 0; i < SIZE; i++) {
                    int idx = (direction == LEFT) ? i : SIZE - 1 - i;
                    game->grid[row][idx] = temp[i];
                }
            }
        }
    }

    if (!moved) {
        *reward = INVALID_MOVE_PENALTY;
    } else {
        game->grid_changed = true;
        game->game_over_cached = false; // Invalidate cache
    }

    return moved;
}

bool is_game_over(Game* game) {
    // Use cached result if grid hasn't changed
    if (!game->grid_changed && game->game_over_cached) {
        return game->game_over_cached;
    }
    
    // Quick check: if there are empty cells, game is not over
    if (game->empty_count > 0) {
        game->game_over_cached = false;
        game->grid_changed = false;
        return false;
    }
    
    // Check for possible merges
    for (int i = 0; i < SIZE; i++) {
        for (int j = 0; j < SIZE; j++) {
            unsigned char current = game->grid[i][j];
            if (i < SIZE - 1 && current == game->grid[i + 1][j]) {
                game->game_over_cached = false;
                game->grid_changed = false;
                return false;
            }
            if (j < SIZE - 1 && current == game->grid[i][j + 1]) {
                game->game_over_cached = false;
                game->grid_changed = false;
                return false;
            }
        }
    }
    
    game->game_over_cached = true;
    game->grid_changed = false;
    return true;
}

// Optimized score calculation
static inline unsigned char calc_score(Game* game) {
    unsigned char max_tile = 0;
    // Unroll loop for better performance
    for (int i = 0; i < SIZE; i++) {
        for (int j = 0; j < SIZE; j++) {
            if (game->grid[i][j] > max_tile) {
                max_tile = game->grid[i][j];
            }
        }
    }
    return max_tile;
}

void c_step(Game* game) {
    float reward = 0.0f;
    bool did_move = move(game, game->actions[0] + 1, &reward);
    game->tick++;
    
    if (did_move) {
        add_random_tile(game);
        game->score = calc_score(game);
        update_empty_count(game); // Update after adding tile
    }
    
    bool game_over = is_game_over(game);
    game->terminals[0] = game_over ? 1 : 0;
    
    if (game_over) {
        reward = GAME_OVER_PENALTY;
    }
    
    game->rewards[0] = reward;
    game->episode_reward += reward;

    update_observations(game);

    if (game->terminals[0]) {
        add_log(game);
        c_reset(game);
    }
}

// Rendering optimizations
void c_render(Game* game) {
    static bool window_initialized = false;
    static char score_text[32];
    static const int px = 100;
    
    if (!window_initialized) {
        InitWindow(px * SIZE, px * SIZE + 50, "2048");
        SetTargetFPS(30); // Increased for smoother rendering
        window_initialized = true;
    }
    
    if (IsKeyDown(KEY_ESCAPE)) {
        CloseWindow();
        exit(0);
    }

    BeginDrawing();
    ClearBackground(PUFF_BACKGROUND);

    // Draw grid
    for (int i = 0; i < SIZE; i++) {
        for (int j = 0; j < SIZE; j++) {
            int val = game->grid[i][j];
            
            // Use precomputed colors
            Color color = (val == 0) ? tile_colors[0] : 
                         (val <= 11) ? tile_colors[val] : 
                         (Color){60, 60, 60, 255};
            
            DrawRectangle(j * px, i * px, px - 5, px - 5, color);
            
            if (val > 0) {
                int display_val = 1 << val; // Power of 2
                // Pre-format text to avoid repeated formatting
                snprintf(score_text, sizeof(score_text), "%d", display_val);
                if (display_val < 1000) {
                    DrawText(score_text, j * px + 30, i * px + 40, 32, PUFF_WHITE);
                } else {
                    DrawText(score_text, j * px + 20, i * px + 40, 32, PUFF_WHITE);
                }
            }
        }
    }
    
    // Draw score (format once per frame)
    snprintf(score_text, sizeof(score_text), "Score: %d", 1 << game->score);
    DrawText(score_text, 10, px * SIZE + 10, 24, PUFF_WHITE);
    
    EndDrawing();
}

void c_close(Game* game) {
    if (IsWindowReady()) {
        CloseWindow();
    }
}



================================================
FILE: pufferlib/ocean/g2048/binding.c
================================================
#include "2048.h"

#define Env Game
#include "../env_binding.h"

// 2048.h does not have a 'size' field, so my_init can just return 0
static int my_init(Env* env, PyObject* args, PyObject* kwargs) {
    // No custom initialization needed for 2048
    return 0;
}

static int my_log(PyObject* dict, Log* log) {
    assign_to_dict(dict, "perf", log->perf);
    assign_to_dict(dict, "score", log->score);
    assign_to_dict(dict, "episode_return", log->episode_return);
    assign_to_dict(dict, "episode_length", log->episode_length);
    return 0;
}


================================================
FILE: pufferlib/ocean/g2048/g2048.c
================================================
#include "2048.h"
#include "puffernet.h"

int main() {
    srand(time(NULL));
    Game env;
    unsigned char observations[SIZE * SIZE] = {0};
    unsigned char terminals[1] = {0};
    int actions[1] = {0};
    float rewards[1] = {0};

    env.observations = observations;
    env.terminals = terminals;
    env.actions = actions;
    env.rewards = rewards;

    Weights* weights = load_weights("resources/g2048/g2048_weights.bin", 134917);
    int logit_sizes[1] = {4};
    LinearLSTM* net = make_linearlstm(weights, 1, 16, logit_sizes, 1);
    c_reset(&env);
    c_render(&env);

    // Main game loop
    int frame = 0;
    while (!WindowShouldClose()) {
        c_render(&env);
        frame++;

        int action = 0;
        if (IsKeyDown(KEY_LEFT_SHIFT)) {
            if (IsKeyPressed(KEY_W) || IsKeyPressed(KEY_UP)) action = UP;
            else if (IsKeyPressed(KEY_S) || IsKeyPressed(KEY_DOWN)) action = DOWN;
            else if (IsKeyPressed(KEY_A) || IsKeyPressed(KEY_LEFT)) action = LEFT;
            else if (IsKeyPressed(KEY_D) || IsKeyPressed(KEY_RIGHT)) action = RIGHT;
            env.actions[0] = action - 1;
        } else if (frame % 10 != 0) {
            continue;
        } else {
            action = 1;
            for (int i = 0; i < 16; i++) {
                net->obs[i] = env.observations[i];
            }
            forward_linearlstm(net, net->obs, env.actions);
        }

        if (action != 0) {
            c_step(&env);
        }
    }

    free_linearlstm(net);
    c_close(&env);
    printf("Game Over! Final Max Tile: %d\n", env.score);
    return 0;
}



================================================
FILE: pufferlib/ocean/g2048/g2048.py
================================================
'''2048 Gymnasium-compatible environment using the C backend.'''

import gymnasium
import numpy as np

import pufferlib
from pufferlib.ocean.g2048 import binding

class G2048(pufferlib.PufferEnv):
    def __init__(self, num_envs=1, render_mode=None, log_interval=128, buf=None, seed=0):
        self.single_observation_space = gymnasium.spaces.Box(
            low=0, high=100, shape=(4,4), dtype=np.uint8
        )
        self.single_action_space = gymnasium.spaces.Discrete(4)
        self.render_mode = render_mode
        self.num_agents = num_envs
        self.log_interval = log_interval

        super().__init__(buf)
        self.c_envs = binding.vec_init(
            self.observations, self.actions, self.rewards,
            self.terminals, self.truncations, num_envs, seed
        )

    def reset(self, seed=0):
        binding.vec_reset(self.c_envs, seed)
        self.tick = 0
        return self.observations, []

    def step(self, actions):
        self.tick += 1

        self.actions[:] = actions
        binding.vec_step(self.c_envs)

        info = []
        if self.tick % self.log_interval == 0:
            info.append(binding.vec_log(self.c_envs))

        return (
            self.observations, self.rewards,
            self.terminals, self.truncations, info
        )

    def render(self):
        binding.vec_render(self.c_envs, 0)

    def close(self):
        binding.vec_close(self.c_envs)

if __name__ == '__main__':
    N = 128

    env = G2048(num_envs=N)
    env.reset()
    steps = 0

    CACHE = 1024
    actions = np.random.randint(0, 4, (CACHE, N))

    i = 0
    import time
    start = time.time()
    while time.time() - start < 10:
        env.step(actions[i % CACHE])
        steps += N
        i += 1

    print('2048 SPS:', int(steps / (time.time() - start)))



================================================
FILE: pufferlib/ocean/go/binding.c
================================================
#include "go.h"
#define Env CGo
#include "../env_binding.h"

static int my_init(Env* env, PyObject* args, PyObject* kwargs) {
    env->width = unpack(kwargs, "width");
    env->height = unpack(kwargs, "height");
    env->grid_size = unpack(kwargs, "grid_size");
    env->board_width = unpack(kwargs, "board_width");
    env->board_height = unpack(kwargs, "board_height");
    env->grid_square_size = unpack(kwargs, "grid_square_size");
    env->moves_made = unpack(kwargs, "moves_made");
    env->komi = unpack(kwargs, "komi");
    env->score = unpack(kwargs, "score");
    env->last_capture_position = unpack(kwargs, "last_capture_position");
    env->reward_move_pass = unpack(kwargs, "reward_move_pass");
    env->reward_move_invalid = unpack(kwargs, "reward_move_invalid");
    env->reward_move_valid = unpack(kwargs, "reward_move_valid");
    env->reward_player_capture = unpack(kwargs, "reward_player_capture");
    env->reward_opponent_capture = unpack(kwargs, "reward_opponent_capture");
    
    init(env);
    return 0;
}

static int my_log(PyObject* dict, Log* log) {
    assign_to_dict(dict, "perf", log->perf);
    assign_to_dict(dict, "score", log->score);
    assign_to_dict(dict, "episode_length", log->episode_length);
    assign_to_dict(dict, "episode_return", log->episode_return);
    assign_to_dict(dict, "n", log->n);
    return 0;
}



================================================
FILE: pufferlib/ocean/go/go.c
================================================
#include <time.h>
#include "go.h"
#include "puffernet.h"

typedef struct GoNet GoNet;
struct GoNet {
    int num_agents;
    float* obs_2d;
    float* obs_1d;
    Conv2D* conv1;
    ReLU* relu1;
    Conv2D* conv2;
    Linear* flat;
    CatDim1* cat;
    Linear* proj;
    ReLU* relu3;
    LSTM* lstm;
    Linear* actor;
    Linear* value_fn;
    Multidiscrete* multidiscrete;
};
GoNet* init_gonet(Weights* weights, int num_agents, int grid_size) {
    GoNet* net = calloc(1, sizeof(GoNet));
    int hidden_size = 128;
    int cnn_channels = 64;
    int conv1_output_size = grid_size - 2;
    int output_size = grid_size - 4;
    int cnn_flat_size = cnn_channels * output_size * output_size;

    net->num_agents = num_agents;
    net->obs_2d = calloc(num_agents * grid_size * grid_size * 2, sizeof(float)); // 2 channels for player and opponent
    net->obs_1d = calloc(num_agents * 2, sizeof(float)); // 2 additional features

    net->conv1 = make_conv2d(weights, num_agents, grid_size, grid_size, 2, cnn_channels, 3, 1);
    net->relu1 = make_relu(num_agents, cnn_channels * conv1_output_size * conv1_output_size);
    net->conv2 = make_conv2d(weights, num_agents, conv1_output_size, conv1_output_size, cnn_channels, cnn_channels, 3, 1);
    net->flat = make_linear(weights, num_agents, 2, 32);
    net->cat = make_cat_dim1(num_agents, cnn_flat_size, 32);
    net->proj = make_linear(weights, num_agents, cnn_flat_size + 32, hidden_size);
    net->relu3 = make_relu(num_agents, hidden_size);
    net->actor = make_linear(weights, num_agents, hidden_size, grid_size*grid_size + 1); // +1 for pass move
    net->value_fn = make_linear(weights, num_agents, hidden_size, 1);
    net->lstm = make_lstm(weights, num_agents, hidden_size, 128);
    int logit_sizes[6] = {grid_size*grid_size+1};
    net->multidiscrete = make_multidiscrete(num_agents, logit_sizes, 1);
    return net;
}

void free_gonet(GoNet* net) {
    free(net->obs_2d);
    free(net->obs_1d);
    free(net->conv1);
    free(net->relu1);
    free(net->conv2);
    free(net->flat);
    free(net->cat);
    free(net->relu3);
    free(net->proj);
    free(net->lstm);
    free(net->actor);
    free(net->value_fn);
    free(net);
}

void forward(GoNet* net, float* observations, int* actions, int grid_size) {
    int full_board = grid_size * grid_size;    
    // Clear previous observations
    memset(net->obs_2d, 0, net->num_agents * grid_size * grid_size * 2 * sizeof(float));
    memset(net->obs_1d, 0, net->num_agents * 2 * sizeof(float));
    
    // Reshape observations into 2D boards and additional features
    float (*obs_2d)[2][grid_size][grid_size] = (float (*)[2][grid_size][grid_size])net->obs_2d;
    float (*obs_1d)[2] = (float (*)[2])net->obs_1d;
    
    for (int b = 0; b < net->num_agents; b++) {
        int b_offset = b * (full_board * 2 + 2);  // offset for each batch
        
        // Process black stones board
        for (int i = 0; i < grid_size; i++) {
            for (int j = 0; j < grid_size; j++) {
                obs_2d[b][0][i][j] = observations[b_offset + i*grid_size + j];
            }
        }
        
        // Process white stones board
        for (int i = 0; i < grid_size; i++) {
            for (int j = 0; j < grid_size; j++) {
                obs_2d[b][1][i][j] = observations[b_offset + full_board + i*grid_size + j];
            }
        }
        
        // Process additional features
        obs_1d[b][0] = observations[b_offset + full_board * 2];
        obs_1d[b][1] = observations[b_offset + full_board * 2 + 1];
    }

    // Forward pass through the network
    conv2d(net->conv1, net->obs_2d);
    relu(net->relu1, net->conv1->output);
    conv2d(net->conv2, net->relu1->output);

    linear(net->flat, net->obs_1d);

    cat_dim1(net->cat, net->conv2->output, net->flat->output);
    linear(net->proj, net->cat->output);
    relu(net->relu3, net->proj->output);
    
    lstm(net->lstm, net->relu3->output);
    linear(net->actor, net->lstm->state_h);
    linear(net->value_fn, net->lstm->state_h);

    // Get action by taking argmax of actor output
    softmax_multidiscrete(net->multidiscrete, net->actor->output, actions);

}

void demo(int grid_size) {

    CGo env = {
        .width = 950,
        .height = 64*(grid_size+1),
        .grid_size = grid_size,
        .board_width = 64*(grid_size+1) + 400,
        .board_height = 64*(grid_size+1),
        .grid_square_size = 64,
        .moves_made = 0,
        .komi = 7.5,
        .reward_move_pass = -0.25,
        .reward_move_invalid = -0.1,
        .reward_move_valid = 0.1
    };

    Weights* weights = load_weights("resources/go/go_weights.bin", 254867);
    GoNet* net = init_gonet(weights, 1, grid_size);
    allocate(&env);
    c_reset(&env);
    c_render(&env);
 
    int tick = 0;
    while (!WindowShouldClose()) {
        // User can take control of the paddle
        if(tick % 12 == 0) {
            tick = 0;
            int human_action = env.actions[0];
            forward(net, env.observations, env.actions, grid_size);
            if (IsKeyDown(KEY_LEFT_SHIFT)) {
                env.actions[0] = human_action;
            }
            c_step(&env);
            if (IsKeyDown(KEY_LEFT_SHIFT)) {
                env.actions[0] = -1;
            }
        }
        tick++;
        if (IsKeyDown(KEY_LEFT_SHIFT)) {
            if (IsMouseButtonPressed(MOUSE_LEFT_BUTTON)) {
                Vector2 mousePos = GetMousePosition();
        
                // Calculate the offset for the board
                int boardOffsetX = env.grid_square_size;
                int boardOffsetY = env.grid_square_size;
                
                // Adjust mouse position relative to the board
                int relativeX = mousePos.x - boardOffsetX;
                int relativeY = mousePos.y - boardOffsetY;
                
                // Calculate cell indices for the corners
                int cellX = (relativeX + env.grid_square_size / 2) / env.grid_square_size;
                int cellY = (relativeY + env.grid_square_size / 2) / env.grid_square_size;
                
                // Ensure the click is within the game board
                if (cellX >= 0 && cellX <= env.grid_size && cellY >= 0 && cellY <= env.grid_size) {
                    // Calculate the point index (1-19) based on the click position
                    int pointIndex = cellY * (env.grid_size) + cellX + 1; 
                    env.actions[0] = (unsigned short)pointIndex;
                }
            // Check if pass button is clicked
                int passButtonX = env.width - 300;
                int passButtonY = 200;
                int passButtonWidth = 100;
                int passButtonHeight = 50;

                if (mousePos.x >= passButtonX && mousePos.x <= passButtonX + passButtonWidth &&
                    mousePos.y >= passButtonY && mousePos.y <= passButtonY + passButtonHeight) {
                    env.actions[0] = 0; // Send action 0 for pass
                }
            }
        }
        c_render(&env);
    }
    //close_client(client);
    free_allocated(&env);
}

void performance_test() {
    long test_time = 10;
    CGo env = {
        .width = 1000,
        .height = 800,
        .grid_size = 9,
        .board_width = 600,
        .board_height = 600,
        .grid_square_size = 600/9,
        .moves_made = 0,
        .komi = 7.5,
	.reward_move_pass = -0.25,
	.reward_move_invalid = -0.1,
	.reward_move_valid = 0.1
    };
    allocate(&env);
    c_reset(&env);

    long start = time(NULL);
    int i = 0;
    while (time(NULL) - start < test_time) {
        env.actions[0] = rand() % (env.grid_size)*(env.grid_size);
        c_step(&env);
        i++;
    }
    long end = time(NULL);
    printf("SPS: %ld\n", i / (end - start));
    free_allocated(&env);
}

int main() {
    demo(7);
    // performance_test();
    return 0;
}



================================================
FILE: pufferlib/ocean/go/go.h
================================================
#include <stdlib.h>
#include <stdio.h>
#include <math.h>
#include <assert.h>
#include <string.h>
#include "raylib.h"

#define NOOP 0
#define MOVE_MIN 1
#define TICK_RATE 1.0f/60.0f
#define NUM_DIRECTIONS 4
#define ENV_WIN -1
#define PLAYER_WIN 1
static const int DIRECTIONS[NUM_DIRECTIONS][2] = {{-1, 0}, {1, 0}, {0, -1}, {0, 1}};
//  LD_LIBRARY_PATH=raylib/lib ./go

typedef struct Log Log;
struct Log {
    float perf;
    float score;
    float episode_return;
    float episode_length;
    float n;
};

typedef struct Group Group;
struct Group {
    int parent;
    int rank;
    int size;
    int liberties;
};

int find(Group* groups, int x) {
    if (groups[x].parent != x)
        groups[x].parent = find(groups, groups[x].parent);
    return groups[x].parent;
}

void union_groups(Group* groups, int pos1, int pos2) {
    pos1 = find(groups, pos1);
    pos2 = find(groups, pos2);
    
    if (pos1 == pos2) return;
    
    if (groups[pos1].rank < groups[pos2].rank) {
        groups[pos1].parent = pos2;
        groups[pos2].size += groups[pos1].size;
        groups[pos2].liberties += groups[pos1].liberties;
    } else if (groups[pos1].rank > groups[pos2].rank) {
        groups[pos2].parent = pos1;
        groups[pos1].size += groups[pos2].size;
        groups[pos1].liberties += groups[pos2].liberties;
    } else {
        groups[pos2].parent = pos1;
        groups[pos1].rank++;
        groups[pos1].size += groups[pos2].size;
        groups[pos1].liberties += groups[pos2].liberties;
    }
}

typedef struct Client Client;
typedef struct CGo CGo;
struct CGo {
    Client* client;
    float* observations;
    int* actions;
    float* rewards;
    unsigned char* terminals;
    Log log;
    float score;
    int width;
    int height;
    int* board_x;
    int* board_y;
    int board_width;
    int board_height;
    int grid_square_size;
    int grid_size;
    int* board_states;
    int* previous_board_state;
    int last_capture_position;
    int* temp_board_states;
    int moves_made;
    int* capture_count;
    float komi;
    int* visited;
    Group* groups;
    Group* temp_groups;
    float reward_move_pass;
    float reward_move_invalid;
    float reward_move_valid;
    float reward_player_capture;
    float reward_opponent_capture;
    float tick;
};

void add_log(CGo* env) {
    env->log.episode_length += env->tick;
    
    // Calculate perf as a win rate (1.0 if win, 0.0 if loss)
    float win_value = 0.0;
    if (env->score > 0) {
        win_value = 1.0; // Win
    }
    else if (env->score < 0) {
        win_value = 0.0; // Loss
    }
    else {
        win_value = 0.0; // Tie
    }

    env->log.perf = (env->log.perf * env->log.n + win_value) / (env->log.n + 1.0);
    
    env->log.score += env->score;
    env->log.episode_return += env->rewards[0];
    env->log.n += 1.0;
}

void generate_board_positions(CGo* env) {
    for (int i = 0; i < (env->grid_size-1) * (env->grid_size-1); i++) {
        int row = i / (env->grid_size-1);
        int col = i % (env->grid_size-1);
        env->board_x[i] = col * (env->grid_square_size-1);
        env->board_y[i] = row * (env->grid_square_size-1);
    }
}

void init_groups(CGo* env) {
    for (int i = 0; i < (env->grid_size)*(env->grid_size); i++) {
        env->groups[i].parent = i;
        env->groups[i].rank = 0;
        env->groups[i].size = 1;
        env->groups[i].liberties = 0;
    }
}

void init(CGo* env) {
    int board_render_size = (env->grid_size-1)*(env->grid_size-1);
    int grid_size = env->grid_size*env->grid_size;
    env->board_x = (int*)calloc(board_render_size, sizeof(int));
    env->board_y = (int*)calloc(board_render_size, sizeof(int));
    env->board_states = (int*)calloc(grid_size, sizeof(int));
    env->visited = (int*)calloc(grid_size, sizeof(int));
    env->previous_board_state = (int*)calloc(grid_size, sizeof(int));
    env->temp_board_states = (int*)calloc(grid_size, sizeof(int));
    env->capture_count = (int*)calloc(2, sizeof(int));
    env->groups = (Group*)calloc(grid_size, sizeof(Group));
    env->temp_groups = (Group*)calloc(grid_size, sizeof(Group));
    generate_board_positions(env);
    init_groups(env);
}

void allocate(CGo* env) {
    init(env);
    env->observations = (float*)calloc((env->grid_size)*(env->grid_size)*2 + 2, sizeof(float));
    env->actions = (int*)calloc(1, sizeof(int));
    env->rewards = (float*)calloc(1, sizeof(float));
    env->terminals = (unsigned char*)calloc(1, sizeof(unsigned char));
}

void c_close(CGo* env) {
    free(env->board_x);
    free(env->board_y);
    free(env->board_states);
    free(env->visited);
    free(env->previous_board_state);
    free(env->temp_board_states);
    free(env->capture_count);
    free(env->temp_groups);
    free(env->groups);
}

void free_allocated(CGo* env) {
    free(env->actions);
    free(env->observations);
    free(env->terminals);
    free(env->rewards);
    c_close(env);
}

void compute_observations(CGo* env) {
    int observation_indx=0;
    for (int i = 0; i < (env->grid_size)*(env->grid_size); i++) {
	if(env->board_states[i] ==1 ){
		env->observations[observation_indx] = 1.0;
	}	
	else {
		env->observations[observation_indx] = 0.0;
	}
        observation_indx++;
    }
    for (int i = 0; i < (env->grid_size)*(env->grid_size); i++) {
        if(env->board_states[i] ==2 ){
            env->observations[observation_indx] = 1.0;
        }	
        else {
            env->observations[observation_indx] = 0.0;
        }
        observation_indx++;
    }
    env->observations[observation_indx] = env->capture_count[0];
    env->observations[observation_indx+1] = env->capture_count[1];

}

int is_valid_position(CGo* env, int x, int y) {
    return (x >= 0 && x < env->grid_size && y >= 0 && y < env->grid_size);
}

void reset_visited(CGo* env) {
    memset(env->visited, 0, sizeof(int) * (env->grid_size) * (env->grid_size));
}

void flood_fill(CGo* env, int x, int y, int* territory, int player) {
    if (!is_valid_position(env, x, y)) {
        return;
    }

    int pos = y * (env->grid_size) + x;
    if (env->visited[pos] || env->board_states[pos] != 0) {
        return;
    }
    env->visited[pos] = 1;
    territory[player]++;
    // Check adjacent positions
    for (int i = 0; i < 4; i++) {
        flood_fill(env, x + DIRECTIONS[i][0], y + DIRECTIONS[i][1], territory, player);
    }
}

void compute_score_tromp_taylor(CGo* env) {
    int player_score = 0;
    int opponent_score = 0;
    reset_visited(env);
    
    // Queue for BFS
    int queue_size = (env->grid_size) * (env->grid_size);
    int queue[queue_size];
    
    // First count stones
    for (int i = 0; i < queue_size; i++) {
        if (env->board_states[i] == 1) {
            player_score++;
        } else if (env->board_states[i] == 2) {
            opponent_score++;
        }
    }
    
    // Then process empty territories
    for (int start_pos = 0; start_pos < queue_size; start_pos++) {
        // Skip if not empty or already visited
        if (env->board_states[start_pos] != 0 || env->visited[start_pos]) {
            continue;
        }
        
        // Initialize BFS
        int front = 0, rear = 0;
        int territory_size = 0;
        int bordering_player = 0;  // 0=neutral, 1=player1, 2=player2, 3=mixed
        
        queue[rear++] = start_pos;
        env->visited[start_pos] = 1;
        
        // Process connected empty points
        while (front < rear) {
            int pos = queue[front++];
            territory_size++;
            int x = pos % env->grid_size;
            int y = pos / env->grid_size;
            
            // Check all adjacent positions
            for (int i = 0; i < 4; i++) {
                int nx = x + DIRECTIONS[i][0];
                int ny = y + DIRECTIONS[i][1];
                
                if (!is_valid_position(env, nx, ny)) {
                    continue;
                }
                
                int npos = ny * env->grid_size + nx;
                
                if (env->board_states[npos] == 0 && !env->visited[npos]) {
                    // Add unvisited empty points to queue
                    queue[rear++] = npos;
                    env->visited[npos] = 1;
                } else if (bordering_player == 0) {
                    bordering_player = env->board_states[npos];
                } else if (bordering_player != env->board_states[npos]) {
                    bordering_player = 3;  // Mixed territory
                }
            }
        }
        
        // Assign territory points
        if (bordering_player == 1) {
            player_score += territory_size;
        } else if (bordering_player == 2) {
            opponent_score += territory_size;
        }
        // Mixed territories (bordering_player == 3) are neutral and not counted
    }
    
    env->score = (float)player_score - (float)opponent_score - env->komi;
}

int find_in_group(int* group, int group_size, int value) {
    for (int i = 0; i < group_size; i++) {
        if (group[i] == value) {
            return 1;  // Found
        }
    }
    return 0;  // Not found
}


void capture_group(CGo* env, int* board, int root, int* affected_groups, int* affected_count) {
    // Reset visited array
    reset_visited(env);

    // Use a queue for BFS
    int queue_size = (env->grid_size) * (env->grid_size);
    int queue[queue_size];
    int front = 0, rear = 0;

    int captured_player = board[root];       // Player whose stones are being captured
    int capturing_player = 3 - captured_player;          // Player who captures

    queue[rear++] = root;
    env->visited[root] = 1;

    while (front != rear) {
        int pos = queue[front++];
        board[pos] = 0;  // Remove stone
        env->capture_count[capturing_player - 1]++;  // Update capturing player's count
	if(capturing_player-1 == 0){
		env->rewards[0] += env->reward_player_capture;
		env->log.episode_return += env->reward_player_capture;
	} else{
		env->rewards[0] += env->reward_opponent_capture;
		env->log.episode_return += env->reward_opponent_capture;
	}
        int x = pos % (env->grid_size);
        int y = pos / (env->grid_size);

        for (int i = 0; i < 4; i++) {
            int nx = x + DIRECTIONS[i][0];
            int ny = y + DIRECTIONS[i][1];
            int npos = ny * (env->grid_size) + nx;

            if (!is_valid_position(env, nx, ny)) {
                continue;
            }

            if (board[npos] == captured_player && !env->visited[npos]) {
                env->visited[npos] = 1;
                queue[rear++] = npos;
            }
            else if (board[npos] == capturing_player) {
                int adj_root = find(env->temp_groups, npos);
                if (find_in_group(affected_groups, *affected_count, adj_root)) {
                    continue;
                }
                affected_groups[(*affected_count)] = adj_root;
                (*affected_count)++;
            }
        }
    }
}


int count_liberties(CGo* env, int root, int* queue) {
    reset_visited(env);
    int liberties = 0;
    int front = 0;
    int rear = 0;
    
    queue[rear++] = root;
    env->visited[root] = 1;
    while (front < rear) {
        int pos = queue[front++];
        int x = pos % (env->grid_size);
        int y = pos / (env->grid_size);
        
        for (int i = 0; i < 4; i++) {
            int nx = x + DIRECTIONS[i][0];
            int ny = y + DIRECTIONS[i][1];
            if (!is_valid_position(env, nx, ny)) {
                continue;
            }
            
            int npos = ny * (env->grid_size) + nx;
            if (env->visited[npos]) {
                continue;
            }

            int temp_npos = env->temp_board_states[npos];
            if (temp_npos == 0) {
                liberties++;
                env->visited[npos] = 1;
            } else if (temp_npos == env->temp_board_states[root]) {
                queue[rear++] = npos;
                env->visited[npos] = 1;
            }
        }
    }
    return liberties;
}

int is_ko(CGo* env) {
    for (int i = 0; i < (env->grid_size) * (env->grid_size); i++) {
        if (env->temp_board_states[i] != env->previous_board_state[i]) {
            return 0;  // Not a ko
        }
    }
    return 1;  // Is a ko
}

int make_move(CGo* env, int pos, int player){
    int x = pos % (env->grid_size);
    int y = pos / (env->grid_size);
    // cannot place stone on occupied tile
    if (env->board_states[pos] != 0) {
        return 0 ;
    }
    // temp structures
    memcpy(env->temp_board_states, env->board_states, sizeof(int) * (env->grid_size) * (env->grid_size));
    memcpy(env->temp_groups, env->groups, sizeof(Group) * (env->grid_size) * (env->grid_size));
    // create new group
    env->temp_board_states[pos] = player;
    env->temp_groups[pos].parent = pos;
    env->temp_groups[pos].rank = 0;
    env->temp_groups[pos].size = 1;
    env->temp_groups[pos].liberties = 0;
    
    int max_affected_groups = (env->grid_size) * (env->grid_size);
    int affected_groups[max_affected_groups];
    int affected_count = 0;
    affected_groups[affected_count++] = pos;

    int queue[(env->grid_size) * (env->grid_size)];

    // Perform unions and track affected groups
    for (int i = 0; i < 4; i++) {
        int nx = x + DIRECTIONS[i][0];
        int ny = y + DIRECTIONS[i][1];
        int npos = ny * (env->grid_size) + nx;
        if (!is_valid_position(env, nx, ny)) {
            continue;
        }
        if (env->temp_board_states[npos] == player) {
            union_groups(env->temp_groups, pos, npos);
            affected_groups[affected_count++] = npos;
        } else if (env->temp_board_states[npos] == 3 - player) {
            affected_groups[affected_count++] = npos;
        }
    }

    // Recalculate liberties only for affected groups
    for (int i = 0; i < affected_count; i++) {
        int root = find(env->temp_groups, affected_groups[i]);
        env->temp_groups[root].liberties = count_liberties(env, root, queue);
    }

    // Check for captures
    bool captured = false;
    for (int i = 0; i < affected_count; i++) {
        int root = find(env->temp_groups, affected_groups[i]);
        if (env->temp_board_states[root] == 3 - player && env->temp_groups[root].liberties == 0) {
            capture_group(env, env->temp_board_states, root, affected_groups, &affected_count);
            captured = true;
        }
    }
    // If captures occurred, recalculate liberties again
    if (captured) {
        for (int i = 0; i < affected_count; i++) {
            int root = find(env->temp_groups, affected_groups[i]);
            env->temp_groups[root].liberties = count_liberties(env, root, queue);
        }
        // Check for ko rule violation
        if(is_ko(env)) {
            return 0;
        }
    }
    // self capture
    int root = find(env->temp_groups, pos);
    if (env->temp_groups[root].liberties == 0) {
        return 0;
    }
    memcpy(env->board_states, env->temp_board_states, sizeof(int) * (env->grid_size) * (env->grid_size));
    memcpy(env->groups, env->temp_groups, sizeof(Group) * (env->grid_size) * (env->grid_size));
    return 1;

}


void enemy_random_move(CGo* env){
    int num_positions = (env->grid_size)*(env->grid_size);
    int positions[num_positions];
    int count = 0;

    // Collect all empty positions
    for(int i = 0; i < num_positions; i++){
        if(env->board_states[i] == 0){
            positions[count++] = i;
        }
    }
    // Shuffle the positions
    for(int i = count - 1; i > 0; i--){
        int j = rand() % (i + 1);
        int temp = positions[i];
        positions[i] = positions[j];
        positions[j] = temp;
    }
    // Try to make a move in a random empty position
    for(int i = 0; i < count; i++){
        if(make_move(env, positions[i], 2)){
            return;
        }
    }
    // If no move is possible, pass or end the game
    env->terminals[0] = 1;
}

int find_group_liberty(CGo* env, int root){
    reset_visited(env);
    int queue[(env->grid_size)*(env->grid_size)];
    int front = 0, rear = 0;
    queue[rear++] = root;
    env->visited[root] = 1;

    while(front < rear){
        int pos = queue[front++];
        int x = pos % (env->grid_size);
        int y = pos / (env->grid_size);

        for(int i = 0; i < 4; i++){
            int nx = x + DIRECTIONS[i][0];
            int ny = y + DIRECTIONS[i][1];
            int npos = ny * (env->grid_size) + nx;
            if(!is_valid_position(env, nx, ny)){
                continue;
            }
            if(env->board_states[npos] == 0){
                return npos; // Found a liberty
            } else if(env->board_states[npos] == env->board_states[root] && !env->visited[npos]){
                env->visited[npos] = 1;
                queue[rear++] = npos;
            }
        }
    }
    return -1; // Should not happen if liberties > 0
}

void enemy_greedy_hard(CGo* env){
	// Attempt to capture opponent stones in atari
    int liberties[4][(env->grid_size) * (env->grid_size)];
    int liberty_counts[4] = {0};
    for(int i = 0; i < (env->grid_size)*(env->grid_size); i++){
        if(env->board_states[i]==0){
                continue;
        }
        if (env->board_states[i]==1){
            int root = find(env->groups, i);
            int group_liberties = env->groups[root].liberties;
            if (group_liberties >= 1 && group_liberties <= 4) {
                int liberty = find_group_liberty(env, root);
                liberties[group_liberties - 1][liberty_counts[group_liberties - 1]++] = liberty;
            }
        } else if (env->board_states[i]==2){
            int root = find(env->groups, i);
            int group_liberties = env->groups[root].liberties;
            if (group_liberties==1) {
                int liberty = find_group_liberty(env, root);
                liberties[group_liberties - 1][liberty_counts[group_liberties - 1]++] = liberty;
            }
        }
    }
    // make move to attack or defend
    for (int priority = 0; priority < 4; priority++) {
        for (int i = 0; i < liberty_counts[priority]; i++) {
            if (make_move(env, liberties[priority][i], 2)) {
                return;
            }
        }
    }
    // random move
    enemy_random_move(env);
}

void enemy_greedy_easy(CGo* env){
    // Attempt to capture opponent stones in atari
    for(int i = 0; i < (env->grid_size)*(env->grid_size); i++){
        if(env->board_states[i] != 1){
            continue;
        }
        int root = find(env->groups, i);
        if(env->groups[root].liberties == 1){
            int liberty = find_group_liberty(env, root);
            if(make_move(env, liberty, 2)){
                return; // Successful capture
            }
        }
    }
    // Protect own stones in atari
    for(int i = 0; i < (env->grid_size)*(env->grid_size); i++){
        if(env->board_states[i] != 2){
            continue;
        }
        // Enemy's own stones
        int root = find(env->groups, i);
        if(env->groups[root].liberties == 1){
            int liberty = find_group_liberty(env, root);
            if(make_move(env, liberty, 2)){
                return; // Successful defense
            }
        }
    }
    // Play a random legal move
    enemy_random_move(env);
}

void c_reset(CGo* env) {
    env->tick = 0;
    // We don't reset the log struct - leave it accumulating like in Pong
    env->terminals[0] = 0;
    env->score = 0;
    for (int i = 0; i < (env->grid_size)*(env->grid_size); i++) {
        env->board_states[i] = 0;
        env->temp_board_states[i] = 0;
        env->visited[i] = 0;
        env->previous_board_state[i] = 0;
        env->groups[i].parent = i;
        env->groups[i].rank = 0;
        env->groups[i].size = 0;
        env->groups[i].liberties = 0;
    }
    env->capture_count[0] = 0;
    env->capture_count[1] = 0;
    env->last_capture_position = -1;
    env->moves_made = 0;
    compute_observations(env);
}

void end_game(CGo* env){
    compute_score_tromp_taylor(env);
    if (env->score > 0) {
        env->rewards[0] = 1.0;
    }
    else if (env->score < 0) {
        env->rewards[0] = -1.0;
    }
    else {
        env->rewards[0] = 0.0;
    }
    add_log(env);
    c_reset(env);
}

void c_step(CGo* env) {
    env->tick += 1;
    env->rewards[0] = 0.0;
    int action = (int)env->actions[0];
    // useful for training , can prob be a hyper param. Recommend to increase with larger board size
    float max_moves = 3 * env->grid_size * env->grid_size;
    if (env->tick > max_moves) {
         env->terminals[0] = 1;
         end_game(env);
         compute_observations(env);
         return;
    }
    if(action == NOOP){
        env->rewards[0] = env->reward_move_pass;
        env->log.episode_return += env->reward_move_pass;
        enemy_greedy_hard(env);
        if (env->terminals[0] == 1) {
            end_game(env);
            return;
        }
        compute_observations(env);
        return;
    }
    if (action >= MOVE_MIN && action <= (env->grid_size)*(env->grid_size)) {
        memcpy(env->previous_board_state, env->board_states, sizeof(int) * (env->grid_size) * (env->grid_size));
        if(make_move(env, action-1, 1)) {
            env->moves_made++;
            env->rewards[0] = env->reward_move_valid;
            env->log.episode_return += env->reward_move_valid;
            enemy_greedy_hard(env);

        } else {
            env->rewards[0] = env->reward_move_invalid;
            env->log.episode_return += env->reward_move_invalid;
        }
        compute_observations(env);
    }

    if(env->rewards[0] > 1){
	    env->rewards[0] = 1;
    } 
    if(env->rewards[0] < -1){
	    env->rewards[0] = -1;
    }

    if (env->terminals[0] == 1) {
        end_game(env);
        return;
    }
    
    compute_observations(env);
}

const Color STONE_GRAY = (Color){80, 80, 80, 255};
const Color PUFF_RED = (Color){187, 0, 0, 255};
const Color PUFF_CYAN = (Color){0, 187, 187, 255};
const Color PUFF_WHITE = (Color){241, 241, 241, 241};
const Color PUFF_BACKGROUND = (Color){6, 24, 24, 255};
const Color PUFF_BACKGROUND2 = (Color){18, 72, 72, 255};

struct Client {
    float width;
    float height;
};

Client* make_client(int width, int height) {
    Client* client = (Client*)calloc(1, sizeof(Client));
    client->width = width;
    client->height = height;
    InitWindow(width, height, "PufferLib Ray Go");
    SetTargetFPS(60);
    return client;
}

void c_render(CGo* env) {
    if (env->client == NULL) {
        env->client = make_client(env->width, env->height);
    }

    if (IsKeyDown(KEY_ESCAPE)) {
        exit(0);
    }

    BeginDrawing();
    ClearBackground(PUFF_BACKGROUND);

    int board_size = (env->grid_size + 1) * env->grid_square_size;
    DrawRectangle(0, 0, board_size, board_size, PUFF_BACKGROUND);
    DrawRectangle(
        env->grid_square_size,
        env->grid_square_size,
        board_size - 2*env->grid_square_size,
        board_size - 2*env->grid_square_size,
        PUFF_BACKGROUND2
    );
    int start = env->grid_square_size;
    int end = board_size - env->grid_square_size;
    for (int i = 1; i <= env->grid_size; i++) {
        DrawLineEx(
            (Vector2){start, i*start},
            (Vector2){end, i*start},
            4, PUFF_BACKGROUND
        );
        DrawLineEx(
            (Vector2){i*start, start},
            (Vector2){i*start, end},
            4, PUFF_BACKGROUND
        );
    }

    for (int i = 0; i < (env->grid_size) * (env->grid_size); i++) {
        int position_state = env->board_states[i];
        int row = i / (env->grid_size);
        int col = i % (env->grid_size);
        int x = col * env->grid_square_size;
        int y = row * env->grid_square_size;
        // Calculate the circle position based on the grid
        int circle_x = x + env->grid_square_size;
        int circle_y = y + env->grid_square_size;
        // if player draw circle tile for black 
        int inner = (env->grid_square_size / 2) - 4;
        int outer = (env->grid_square_size / 2) - 2;
        if (position_state == 1) {
            DrawCircleGradient(circle_x, circle_y, outer, STONE_GRAY, BLACK);
        }
        // if enemy draw circle tile for white
        if (position_state == 2) {
            DrawCircleGradient(circle_x, circle_y, inner, WHITE, GRAY);
        }
    }
    // design a pass button
    int left = (env->grid_size + 1)*env->grid_square_size;
    int top = env->grid_square_size;
    DrawRectangle(left, top + 90, 100, 50, GRAY);
    DrawText("Pass", left + 25, top + 105, 20, PUFF_WHITE);

    // show capture count for both players
    DrawText(
        TextFormat("Player 1 Capture Count: %d", env->capture_count[0]),
        left, top, 20, PUFF_WHITE
    );
    DrawText(
        TextFormat("Player 2 Capture Count: %d", env->capture_count[1]),
        left, top + 40, 20, PUFF_WHITE
    );
    EndDrawing();
}
void close_client(Client* client) {
    CloseWindow();
    free(client);
}



================================================
FILE: pufferlib/ocean/go/go.py
================================================
'''High-perf Pong

Inspired from https://gist.github.com/Yttrmin/18ecc3d2d68b407b4be1
& https://jair.org/index.php/jair/article/view/10819/25823
& https://www.youtube.com/watch?v=PSQt5KGv7Vk
'''

import numpy as np
import gymnasium

import pufferlib
from pufferlib.ocean.go import binding

class Go(pufferlib.PufferEnv):
    def __init__(self, num_envs=1, render_mode=None, log_interval=1,
            width=950, height=800,
            grid_size=7,
            board_width=600, board_height=600,
            grid_square_size=600/9,
            moves_made=0,
            komi=7.5,
            score = 0.0,
            last_capture_position=-1,
            reward_move_pass = -0.25,
            reward_move_invalid = -0.1,
            reward_move_valid = 0.1,
            reward_player_capture = 0.25,
            reward_opponent_capture = -0.25,
            buf = None, seed=0):

        # env
        self.num_agents = num_envs
        self.render_mode = render_mode
        self.log_interval = log_interval
        self.tick = 0
        self.num_obs = (grid_size) * (grid_size)*2 + 2
        self.num_act = (grid_size) * (grid_size) + 1
        self.single_observation_space = gymnasium.spaces.Box(low=0, high=1,
            shape=(self.num_obs,), dtype=np.float32)
        self.single_action_space = gymnasium.spaces.Discrete(self.num_act)

        super().__init__(buf=buf)
        height = 64*(grid_size+1)
        self.c_envs = binding.vec_init(self.observations, self.actions, self.rewards,
            self.terminals, self.truncations, num_envs, seed, width=width, height=height, grid_size=grid_size,
            board_width=board_width, board_height=board_height, grid_square_size=grid_square_size,
            moves_made=moves_made, komi=komi, score=score, last_capture_position=last_capture_position,
            reward_move_pass=reward_move_pass, reward_move_invalid=reward_move_invalid,
            reward_move_valid=reward_move_valid, reward_player_capture=reward_player_capture,
            reward_opponent_capture=reward_opponent_capture)

    def reset(self, seed=None):
        binding.vec_reset(self.c_envs, seed)
        self.tick = 0
        return self.observations, []

    def step(self, actions):
        self.actions[:] = actions
        binding.vec_step(self.c_envs)
        self.tick += 1
        info = []
        if self.tick % self.log_interval == 0:
            info.append(binding.vec_log(self.c_envs))
            
        return (self.observations, self.rewards,
            self.terminals, self.truncations, info)

    def render(self):
        binding.vec_render(self.c_envs, 0)
        
    def close(self):
        binding.vec_close(self.c_envs)

def test_performance(timeout=10, atn_cache=1024):
    num_envs=1000
    env = Go(num_envs=num_envs)
    env.reset()
    tick = 0

    actions = np.random.randint(0, env.single_action_space.n, (atn_cache, num_envs))

    import time
    start = time.time()
    while time.time() - start < timeout:
        atn = actions[tick % atn_cache]
        env.step(atn)
        tick += 1

    sps = num_envs * tick / (time.time() - start)
    print(f'SPS: {sps:,}')
if __name__ == '__main__':
    test_performance()



================================================
FILE: pufferlib/ocean/grid/__init__.py
================================================
[Empty file]


================================================
FILE: pufferlib/ocean/grid/binding.c
================================================
#include "grid.h"

#define Env Grid 
#define MY_SHARED
#include "../env_binding.h"

static PyObject* my_shared(PyObject* self, PyObject* args, PyObject* kwargs) {
    int num_maps = unpack(kwargs, "num_maps");
    int max_size = unpack(kwargs, "max_size");
    int size = unpack(kwargs, "size");
    State* levels = calloc(num_maps, sizeof(State));

    if (max_size <= 5) {
        PyErr_SetString(PyExc_ValueError, "max_size must be >5");
        return NULL;
    }

    // Temporary env used to gen maps
    Grid env;
    env.max_size = max_size;
    init_grid(&env);

    srand(time(NULL));
    int start_seed = rand();
    for (int i = 0; i < num_maps; i++) {
        int sz = size;
        if (size == -1) {
            sz = 5 + (rand() % (max_size-5));
        }

        if (sz % 2 == 0) {
            sz -= 1;
        }

        float difficulty = (float)rand()/(float)(RAND_MAX);
        create_maze_level(&env, sz, sz, difficulty, start_seed + i);
        init_state(&levels[i], max_size, 1);
        get_state(&env, &levels[i]);
    }

    return PyLong_FromVoidPtr(levels);
}

static int my_init(Env* env, PyObject* args, PyObject* kwargs) {
    env->max_size = unpack(kwargs, "max_size");
    env->num_maps = unpack(kwargs, "num_maps");
    init_grid(env);

    PyObject* handle_obj = PyDict_GetItemString(kwargs, "state");
    if (!PyObject_TypeCheck(handle_obj, &PyLong_Type)) {
        PyErr_SetString(PyExc_TypeError, "state handle must be an integer");
        return 1;
    }

    State* levels = (State*)PyLong_AsVoidPtr(handle_obj);
    if (!levels) {
        PyErr_SetString(PyExc_ValueError, "Invalid state handle");
        return 1;
    }

    env->levels = levels;
    return 0;
}

static int my_log(PyObject* dict, Log* log) {
    assign_to_dict(dict, "perf", log->perf);
    assign_to_dict(dict, "score", log->score);
    assign_to_dict(dict, "episode_return", log->episode_return);
    assign_to_dict(dict, "episode_length", log->episode_length);
    return 0;
}



================================================
FILE: pufferlib/ocean/grid/c_grid.pyx
================================================
# distutils: define_macros=NPY_NO_DEPRECATED_API=NPY_1_7_API_VERSION
# cython: language_level=3
# cython: boundscheck=False
# cython: initializedcheck=False
# cython: wraparound=False
# cython: cdivision=True
# cython: nonecheck=False
# cython: profile=False

from libc.stdlib cimport rand

cdef:
    int EMPTY = 0
    int FOOD = 1
    int WALL = 2
    int AGENT_1 = 3
    int AGENT_2 = 4
    int AGENT_3 = 5
    int AGENT_4 = 6

    int PASS = 0
    int NORTH = 1
    int SOUTH = 2
    int EAST = 3
    int WEST = 4

cdef class Environment:
    cdef:
        int width
        int height
        int num_agents
        int horizon
        int vision_range
        float agent_speed
        bint discretize
        float food_reward
        int expected_lifespan
        int obs_size

        unsigned char[:, :] grid
        unsigned char[:, :, :] observations
        float[:] rewards
        float[:, :] agent_positions
        float[:, :] spawn_position_cands
        int[:] agent_colors

    def __init__(self, grid, agent_positions, spawn_position_cands, agent_colors,
            observations, rewards, int width, int height, int num_agents, int horizon,
            int vision_range, float agent_speed, bint discretize, float food_reward,
            int expected_lifespan):
        self.width = width 
        self.height = height
        self.num_agents = num_agents
        self.horizon = horizon
        self.vision_range = vision_range
        self.agent_speed = agent_speed
        self.discretize = discretize
        self.food_reward = food_reward
        self.expected_lifespan = expected_lifespan
        self.obs_size = 2*self.vision_range + 1

        self.grid = grid
        self.observations = observations
        self.rewards = rewards
        self.agent_positions = agent_positions
        self.spawn_position_cands = spawn_position_cands
        self.agent_colors = agent_colors

    cdef void compute_observations(self):
        cdef:
            float y
            float x
            int r
            int c
            int agent_idx

        for agent_idx in range(self.num_agents):
            y = self.agent_positions[agent_idx, 0]
            x = self.agent_positions[agent_idx, 1]
            r = int(y)
            c = int(x)
            self.observations[agent_idx, :] = self.grid[
                r-self.vision_range:r+self.vision_range+1,
                c-self.vision_range:c+self.vision_range+1
            ]

    cdef void spawn_food(self):
        cdef int r, c, tile
        while True:
            r = rand() % (self.height - 1)
            c = rand() % (self.width - 1)
            tile = self.grid[r, c]
            if tile == EMPTY:
                self.grid[r, c] = FOOD
                return

    cdef void spawn_agent(self, int agent_idx):
        cdef int old_r, old_c, r, c, tile

        # Delete agent from old position
        old_r = int(self.agent_positions[agent_idx, 0])
        old_c = int(self.agent_positions[agent_idx, 1])
        self.grid[old_r, old_c] = EMPTY

        r = rand() % (self.height - 1)
        c = rand() % (self.width - 1)
        tile = self.grid[r, c]
        if tile == EMPTY:
            # Spawn agent in new position
            self.grid[r, c] = self.agent_colors[agent_idx]
            self.agent_positions[agent_idx, 0] = r
            self.agent_positions[agent_idx, 1] = c
            return

    def reset(self, seed=0):
        # Add borders
        cdef int left = int(self.agent_speed * self.vision_range)
        cdef int right = self.width - int(self.agent_speed*self.vision_range) - 1
        cdef int bottom = self.height - int(self.agent_speed*self.vision_range) - 1
        self.grid[:left, :] = WALL
        self.grid[:, :left] = WALL
        self.grid[bottom:, :] = WALL
        self.grid[:, right:] = WALL

        # Agent spawning
        cdef:
            int spawn_idx
            float y
            float x
            int disc_y
            int disc_x
            int agent_idx = 0

        for spawn_idx in range(self.width*self.height):
            y = self.spawn_position_cands[spawn_idx, 0]
            x = self.spawn_position_cands[spawn_idx, 1]
            disc_y = int(y)
            disc_x = int(x)

            if self.grid[disc_y, disc_x] == EMPTY:
                self.grid[disc_y, disc_x] = self.agent_colors[agent_idx]
                self.agent_positions[agent_idx, 0] = y
                self.agent_positions[agent_idx, 1] = x
                agent_idx += 1
                if agent_idx == self.num_agents:
                    break

        self.compute_observations()

    def step(self, np_actions):
        cdef:
            float[:, :] actions_continuous
            unsigned int[:, :] actions_discrete
            int agent_idx
            float y
            float x
            float vel_y
            float vel_x
            int disc_y
            int disc_x
            int disc_dest_y
            int disc_dest_x

        if self.discretize:
            actions_discrete = np_actions
        else:
            actions_continuous = np_actions

        for agent_idx in range(self.num_agents):
            if self.discretize:
                # Convert [0, 1, 2] to [-1, 0, 1]
                vel_y = float(actions_discrete[agent_idx, 0]) - 1.0
                vel_x = float(actions_discrete[agent_idx, 1]) - 1.0
            else:
                vel_y = actions_continuous[agent_idx, 0]
                vel_x = actions_continuous[agent_idx, 1]

            y = self.agent_positions[agent_idx, 0]
            x = self.agent_positions[agent_idx, 1]
            dest_y = y + self.agent_speed * vel_y
            dest_x = x + self.agent_speed * vel_x

            # Discretize
            disc_y = int(y)
            disc_x = int(x)
            disc_dest_y = int(dest_y)
            disc_dest_x = int(dest_x)

            if self.grid[disc_dest_y, disc_dest_x] == FOOD:
                self.grid[disc_dest_y, disc_dest_x] = EMPTY
                self.rewards[agent_idx] = self.food_reward
                self.spawn_food()

            if self.grid[disc_dest_y, disc_dest_x] == 0:
                self.grid[disc_y, disc_x] = EMPTY
                self.grid[disc_dest_y, disc_dest_x] = self.agent_colors[agent_idx]

                # Continuous position update
                self.agent_positions[agent_idx, 0] = dest_y
                self.agent_positions[agent_idx, 1] = dest_x

            # Randomly respawn agents
            if rand() % self.expected_lifespan == 0:
                self.spawn_agent(agent_idx)

        self.compute_observations()



================================================
FILE: pufferlib/ocean/grid/cy_grid.pyx
================================================
# distutils: define_macros=NPY_NO_DEPRECATED_API=NPY_1_7_API_VERSION
# cython: language_level=3
# cython: boundscheck=False
# cython: initializedcheck=False
# cython: wraparound=False
# cython: cdivision=True
# cython: nonecheck=False
# cython: profile=True

from libc.stdlib cimport calloc, free, rand

cdef extern from "grid.h":
    int LOG_BUFFER_SIZE

    ctypedef struct Log:
        float episode_return;
        float episode_length;
        float score;

    ctypedef struct LogBuffer
    LogBuffer* allocate_logbuffer(int)
    void free_logbuffer(LogBuffer*)
    Log aggregate_and_clear(LogBuffer*)

    ctypedef struct Agent:
        float y;
        float x;
        float prev_y;
        float prev_x;
        float spawn_y;
        float spawn_x;
        int color;
        float direction;
        int held;

    ctypedef struct Grid:
        int width;
        int height;
        int num_agents;
        int horizon;
        int vision;
        float speed;
        int obs_size;
        int max_size;
        bint discretize;
        Log log;
        LogBuffer* log_buffer;
        Agent* agents;
        unsigned char* grid;
        int* counts;
        unsigned char* observations;
        float* actions;
        float* rewards;
        unsigned char* dones;

    ctypedef struct State:
        int width;
        int height;
        int num_agents;
        Agent* agents;
        unsigned char* grid;

    cdef:
        void create_maze_level(Grid* env, int width, int height, float difficulty, int seed)
        void load_locked_room_env(unsigned char* observations,
            unsigned int* actions, float* rewards, float* dones)
        void init_grid(Grid* env)
        void reset(Grid* env, int seed)
        void compute_observations(Grid* env)
        bint step(Grid* env)
        ctypedef struct Renderer
        Renderer* init_renderer(int cell_size, int width, int height)
        void render_global(Renderer*erenderer, Grid* env, float frac, float overlay)
        void clear_overlay(Renderer* renderer)
        void close_renderer(Renderer* renderer)
        void init_state(State* state, int max_size, int num_agents)
        void free_state(State* state)
        void get_state(Grid* env, State* state)
        void set_state(Grid* env, State* state)

import numpy as np
cimport numpy as cnp

cdef class CGrid:
    cdef:
        Grid* envs
        State* levels
        Renderer* client
        LogBuffer* logs
        int num_envs
        int num_maps
        int max_size

    def __init__(self, unsigned char[:, :] observations, float[:] actions,
            float[:] rewards, unsigned char[:] terminals, int num_envs, int num_maps,
            int size, int max_size):

        self.num_envs = num_envs
        self.num_maps = num_maps
        if size > max_size:
            max_size = size

        self.max_size = max_size

        self.client = NULL
        self.levels = <State*> calloc(num_maps, sizeof(State))
        self.envs = <Grid*> calloc(num_envs, sizeof(Grid))
        self.logs = allocate_logbuffer(LOG_BUFFER_SIZE)

        cdef int i
        for i in range(num_envs):
            self.envs[i] = Grid(
                observations = &observations[i, 0],
                actions = &actions[i],
                rewards = &rewards[i],
                dones = &terminals[i],
                log_buffer = self.logs,
                max_size =  max_size,
                num_agents = 1,
                vision = 5,
                speed = 1,
                discretize = True,
            )
            init_grid(&self.envs[i])

        cdef float difficulty
        cdef int sz
        for i in range(num_maps):

            # RNG or fixed size
            if size == -1:
                sz = np.random.randint(5, max_size)
            else:
                sz = size

            if sz % 2 == 0:
                sz -= 1

            difficulty = np.random.rand()
            create_maze_level(&self.envs[0], sz, sz, difficulty, i)
            init_state(&self.levels[i], max_size, 1)
            get_state(&self.envs[0], &self.levels[i])

    def reset(self):
        cdef int i, idx
        for i in range(self.num_envs):
            idx = rand() % self.num_maps
            reset(&self.envs[i], i)
            set_state(&self.envs[i], &self.levels[idx])
            compute_observations(&self.envs[i])

    def step(self):
        cdef:
            int i, idx
            bint done
        
        for i in range(self.num_envs):
            done = step(&self.envs[i])
            if done:
                idx = rand() % self.num_maps
                reset(&self.envs[i], i)
                set_state(&self.envs[i], &self.levels[idx])

                if i == 0 and self.client != NULL:
                    clear_overlay(self.client)

    def render(self, int cell_size=16, float overlay=0.0):
        if self.client == NULL:
            import os
            cwd = os.getcwd()
            os.chdir(os.path.abspath(os.path.join(os.path.dirname(__file__), "..", "..")))
            self.client = init_renderer(cell_size, self.max_size, self.max_size)
            os.chdir(cwd)

        render_global(self.client, &self.envs[0], 0, overlay)

    def log(self):
        cdef Log log = aggregate_and_clear(self.logs)
        return log

    def close(self):
        if self.client != NULL:
            close_renderer(self.client)
            self.client = NULL

        #free_envs(self.envs, self.num_envs)



================================================
FILE: pufferlib/ocean/grid/grid.c
================================================
#include "grid.h"

int main() {
    int max_size = 32;
    int width = 32;
    int height = 32;
    int num_agents = 1;
    int horizon = 128;
    float speed = 1;
    int vision = 5;
    bool discretize = true;

    int render_cell_size = 32;
    int seed = 0;

    Grid* env = allocate_grid(max_size, num_agents, horizon,
        vision, speed, discretize);

    //env->width = 32;
    //env->height = 32; env->agents[0].spawn_x = 16;
    //env->agents[0].spawn_y = 16;
    //env->agents[0].color = 6;
    //reset(env, seed);
    //load_locked_room_preset(env);
     
 
    State* levels = calloc(1, sizeof(State));

    create_maze_level(env, 31, 31, 0.85, seed);
    init_state(levels, max_size, num_agents);
    get_state(env, levels);
    env->num_maps = 1;
    env->levels = levels;
    //generate_locked_room(env);
    //State state;
    //init_state(&state, env->max_size, env->num_agents);
    //get_state(env, &state);

    /*
    width = height = 31;
    env->width=31;
    env->height=31;
    env->agents[0].spawn_x = 1;
    env->agents[0].spawn_y = 1;
    reset(env, seed);
    generate_growing_tree_maze(env->grid, env->width, env->height, max_size, 0.85, 0);
    env->grid[(env->height-2)*env->max_size + (env->width - 2)] = GOAL;
    */
 
    int tick = 0;
    c_render(env);
    while (!WindowShouldClose()) {
        // User can take control of the first agent
        env->actions[0] = ATN_FORWARD;
        Agent* agent = &env->agents[0];

        // TODO: Why are up and down flipped?
        if (IsKeyDown(KEY_LEFT_SHIFT)) {
            if (IsKeyDown(KEY_UP)    || IsKeyDown(KEY_W)){
                //env->actions[0] = ATN_FORWARD;
                agent->direction = 3.0*PI/2.0;
            } else if (IsKeyDown(KEY_DOWN)  || IsKeyDown(KEY_S)) {
                //env->actions[0] = ATN_BACK;
                agent->direction = PI/2.0;
            } else if (IsKeyDown(KEY_LEFT)  || IsKeyDown(KEY_A)) {
                //env->actions[0] = ATN_LEFT;
                agent->direction = PI;
            } else if (IsKeyDown(KEY_RIGHT) || IsKeyDown(KEY_D)) {
                //env->actions[0] = ATN_RIGHT;
                agent->direction = 0;
            } else {
                env->actions[0] = ATN_PASS;
            }
        } else {
            for (int i = 0; i < num_agents; i++) {
                env->actions[i] = rand() % 5;
            }
        }

        //env->actions[0] = actions[t];
        tick = (tick + 1)%12;
        bool done = false;
        if (tick % 1 == 0) {
            c_step(env);
            //printf("direction: %f\n", env->agents[0].direction);

        }
        c_render(env);
    }
    free_allocated_grid(env);
    return 0;
}




================================================
FILE: pufferlib/ocean/grid/grid.h
================================================
#include <stdlib.h>
#include <stdbool.h>
#include <string.h>
#include <stdio.h>
#include <assert.h>
#include <math.h>
#include "raylib.h"

#define TWO_PI 2.0*PI
#define MAX_SIZE 40

#define ATN_PASS 0
#define ATN_FORWARD 1
#define ATN_LEFT 2
#define ATN_RIGHT 3
#define ATN_BACK 4

#define DIR_WEST 0.0;
#define DIR_NORTH PI/2.0;
#define DIR_EAST PI;
#define DIR_SOUTH 3.0*PI/2.0;

#define EMPTY 0
#define WALL 1
#define LAVA 2
#define GOAL 3
#define REWARD 4
#define OBJECT 5
#define AGENT 6
#define KEY 14
#define DOOR_LOCKED 20
#define DOOR_OPEN 26

#define LOG_BUFFER_SIZE 4096

typedef struct Log Log;
struct Log {
    float perf;
    float score;
    float episode_return;
    float episode_length;
    float n;
};

// 8 unique agents
bool is_agent(int idx) {
    return idx >= AGENT && idx < AGENT + 8;
}
int rand_color() {
    return AGENT + rand()%8;
}

// 6 unique keys and doors
bool is_key(int idx) {
    return idx >= KEY && idx < KEY + 6;
}
bool is_locked_door(int idx) {
    return idx >= DOOR_LOCKED && idx < DOOR_LOCKED + 6;
}
bool is_open_door(int idx) {
    return idx >= DOOR_OPEN && idx <= DOOR_OPEN + 6;
}
bool is_correct_key(int key, int door) {
    return key == door - 6;
}

typedef struct Agent Agent;
struct Agent {
    float y;
    float x;
    float prev_y;
    float prev_x;
    float spawn_y;
    float spawn_x;
    int color;
    float direction;
    int held;
};

typedef struct Renderer Renderer;
typedef struct State State;
typedef struct Grid Grid;
struct Grid{
    Renderer* renderer;
    State* levels;
    int num_maps;
    int width;
    int height;
    int num_agents;
    int horizon;
    int vision;
    int tick;
    float speed;
    int obs_size;
    int max_size;
    bool discretize;
    Log log;
    Agent* agents;
    unsigned char* grid;
    int* counts;
    unsigned char* observations;
    float* actions;
    float* rewards;
    unsigned char* terminals;
};

void init_grid(Grid* env) {
    env->num_agents = 1;
    env->vision = 5;
    env->speed = 1;
    env->discretize = true;
    env->obs_size = 2*env->vision + 1;
    int env_mem= env->max_size * env->max_size;
    env->grid = calloc(env_mem, sizeof(unsigned char));
    env->counts = calloc(env_mem, sizeof(int));
    env->agents = calloc(env->num_agents, sizeof(Agent));
}

Grid* allocate_grid(int max_size, int num_agents, int horizon,
        int vision, float speed, bool discretize) {
    Grid* env = (Grid*)calloc(1, sizeof(Grid));
    env->max_size = max_size;
    env->num_agents = num_agents;
    env->horizon = horizon;
    env->vision = vision;
    env->speed = speed;
    env->discretize = discretize;
    int obs_size = 2*vision + 1;
    env->observations = calloc(
        num_agents*obs_size*obs_size, sizeof(unsigned char));
    env->actions = calloc(num_agents, sizeof(float));
    env->rewards = calloc(num_agents, sizeof(float));
    env->terminals = calloc(num_agents, sizeof(unsigned char));
    init_grid(env);
    return env;
}

void c_close(Grid* env) {
    free(env->grid);
    free(env->counts);
    free(env->agents);
    free(env);
}

void free_allocated_grid(Grid* env) {
    free(env->observations);
    free(env->actions);
    free(env->rewards);
    free(env->terminals);
    c_close(env);
}

bool in_bounds(Grid* env, int y, int c) {
    return (y >= 0 && y <= env->height
        && c >= 0 && c <= env->width);
}

int grid_offset(Grid* env, int y, int x) {
    return y*env->max_size + x;
}

void add_log(Grid* env, int idx) {
    env->log.perf += env->rewards[idx];
    env->log.score += env->rewards[idx];
    env->log.episode_return += env->rewards[idx];
    env->log.episode_length += env->tick;
    env->log.n += 1.0;
}
 
void compute_observations(Grid* env) {
    memset(env->observations, 0, env->obs_size*env->obs_size*env->num_agents);
    for (int agent_idx = 0; agent_idx < env->num_agents; agent_idx++) {
        Agent* agent = &env->agents[agent_idx];
        float y = agent->y;
        float x = agent->x;
        int start_r = y - env->vision;
        if (start_r < 0) {
            start_r = 0;
        }

        int start_c = x - env->vision;
        if (start_c < 0) {
            start_c = 0;
        }

        int end_r = y + env->vision;
        if (end_r >= env->max_size) {
            end_r = env->max_size - 1;
        }

        int end_c = x + env->vision;
        if (end_c >= env->max_size) {
            end_c = env->max_size - 1;
        }

        int obs_offset = agent_idx*env->obs_size*env->obs_size;
        for (int r = start_r; r <= end_r; r++) {
            for (int c = start_c; c <= end_c; c++) {
                int r_idx = r - y + env->vision;
                int c_idx = c - x + env->vision;
                int obs_adr = obs_offset + r_idx*env->obs_size + c_idx;
                int adr = grid_offset(env, r, c);
                env->observations[obs_adr] = env->grid[adr];
            }
        }
        /*
        int obs_adr = 0;
        for (int r = 0; r < env->obs_size; r++) {
            for (int c = 0; c < env->obs_size; c++) {
                printf("%d ", env->observations[obs_adr]);
                obs_adr++;
            }
            printf("\n");
        }
        */
    }
}

void make_border(Grid*env) {
    for (int r = 0; r < env->height; r++) {
        int adr = grid_offset(env, r, 0);
        env->grid[adr] = WALL;
        adr = grid_offset(env, r, env->width-1);
        env->grid[adr] = WALL;
    }
    for (int c = 0; c < env->width; c++) {
        int adr = grid_offset(env, 0, c);
        env->grid[adr] = WALL;
        adr = grid_offset(env, env->height-1, c);
        env->grid[adr] = WALL;
    }
}

void spawn_agent(Grid* env, int idx, int x, int y) {
    Agent* agent = &env->agents[idx];
    int spawn_y = y;
    int spawn_x = x;
    assert(in_bounds(env, spawn_y, spawn_x));
    int adr = grid_offset(env, spawn_y, spawn_x);
    assert(env->grid[adr] == EMPTY);
    agent->spawn_y = spawn_y;
    agent->spawn_x = spawn_x;
    agent->y = agent->spawn_y;
    agent->x = agent->spawn_x;
    agent->prev_y = agent->y;
    agent->prev_x = agent->x;
    env->grid[adr] = agent->color;
    agent->direction = 0;
    agent->held = -1;
    agent->color = AGENT;
}

struct State {
    int width;
    int height;
    int num_agents;
    Agent* agents;
    unsigned char* grid;
};

void init_state(State* state, int max_size, int num_agents) {
    state->agents = calloc(num_agents, sizeof(Agent));
    state->grid = calloc(max_size*max_size, sizeof(unsigned char));
}

void free_state(State* state) {
    free(state->agents);
    free(state->grid);
    free(state);
}

void get_state(Grid* env, State* state) {
    state->width = env->width;
    state->height = env->height;
    state->num_agents = env->num_agents;
    memcpy(state->agents, env->agents, env->num_agents*sizeof(Agent));
    memcpy(state->grid, env->grid, env->max_size*env->max_size);
}

void set_state(Grid* env, State* state) {
    env->width = state->width;
    env->height = state->height;
    env->horizon = 2*env->width*env->height;
    env->num_agents = state->num_agents;
    memcpy(env->agents, state->agents, env->num_agents*sizeof(Agent));
    memcpy(env->grid, state->grid, env->max_size*env->max_size);
}

void c_reset(Grid* env) {
    memset(env->grid, 0, env->max_size*env->max_size);
    memset(env->counts, 0, env->max_size*env->max_size*sizeof(int));
    env->tick = 0;
    int idx = rand() % env->num_maps;
    set_state(env, &env->levels[idx]);
    compute_observations(env);
}

int move_to(Grid* env, int agent_idx, float y, float x) {
    Agent* agent = &env->agents[agent_idx];
    if (!in_bounds(env, y, x)) {
        return 1;
    }

    int adr = grid_offset(env, round(y), round(x));
    int dest = env->grid[adr];
    if (dest == WALL) {
        return 1;
    } else if (dest == REWARD || dest == GOAL) {
        env->rewards[agent_idx] = 1.0;
        env->terminals[agent_idx] = 1;
        add_log(env, agent_idx);
    } else if (is_key(dest)) {
        if (agent->held != -1) {
            return 1;
        }
        agent->held = dest;
    } else if (is_locked_door(dest)) { if (!is_correct_key(agent->held, dest)) { return 1;
        }
        agent->held = -1;
        env->grid[adr] = EMPTY;
    }

    int start_y = round(agent->y);
    int start_x = round(agent->x);
    int start_adr = grid_offset(env, start_y, start_x);
    env->grid[start_adr] = EMPTY;

    env->grid[adr] = agent->color;
    agent->y = y;
    agent->x = x;
    return 0;
}
 
bool step_agent(Grid* env, int idx) {
    Agent* agent = &env->agents[idx];
    agent->prev_y = agent->y;
    agent->prev_x = agent->x;

    float atn = env->actions[idx];
    float direction = agent->direction;

    if (env->discretize) {
        int iatn = (int)atn;
        if (iatn == ATN_PASS) {
            return true;
        } else if (iatn == ATN_FORWARD) {
        } else if (iatn == ATN_LEFT) {
            direction -= PI/2.0;
        } else if (iatn == ATN_RIGHT) {
            direction += PI/2.0;
        } else if (iatn == ATN_BACK) {
            direction += PI;
        } else {
            printf("Invalid action: %f\n", atn);
            exit(1);
        }
        if (direction < 0) {
            direction += TWO_PI;
        } else if (direction >= TWO_PI) {
            direction -= TWO_PI;
        }
    } else {
        assert(atn >= -1.0);
        assert(atn <= 1.0);
        direction += PI*atn;
    }

    float x = agent->x;
    float y = agent->y;
    float dx = env->speed*cos(direction);
    float dy = env->speed*sin(direction);
    agent->direction = direction;
    if (env->discretize) {
        float dest_x = x + dx;
        float dest_y = y + dy;
        if (!in_bounds(env, dest_y, dest_x)) {
            return false;
        }
        int err = move_to(env, idx, dest_y, dest_x);
        if (err) {
            return false;
        }
    } else {
        for (int substep = 1; substep <= 4; substep++) {
            float dest_x = x + dx/(float)substep;
            float dest_y = y + dy/(float)substep;
            int err = move_to(env, idx, dest_y, dest_x);
            if (!err) {
                continue;
            } else if (substep == 1) {
                return false;
            } else {
                break;
            }
        }
    }

    int x_int = agent->x;
    int y_int = agent->y;
    int adr = grid_offset(env, y_int, x_int);
    env->counts[adr]++;
    //env->rewards[idx] += 0.01 / (float)env->counts[adr];
    //env->log.episode_return += 0.01 / (float)env->counts[adr];
    return true;
}

void c_step(Grid* env) {
    memset(env->terminals, 0, env->num_agents);
    memset(env->rewards, 0, env->num_agents*sizeof(float));
    env->tick++;

    for (int i = 0; i < env->num_agents; i++) {
        step_agent(env, i);
    }
    compute_observations(env);

    bool done = true;
    for (int i = 0; i < env->num_agents; i++) {
        if (!env->terminals[i]) {
            done = false;
            break;
        }
    }

    if (env->tick >= env->horizon) {
        done = true;
        add_log(env, 0);
    }

    if (done) {
        c_reset(env);
        int idx = rand() % env->num_maps;
        set_state(env, &env->levels[idx]);
        compute_observations(env);
    }
}

// Raylib client
Color COLORS[] = {
    (Color){6, 24, 24, 255},
    (Color){0, 0, 255, 255},
    (Color){0, 128, 255, 255},
    (Color){128, 128, 128, 255},
    (Color){255, 0, 0, 255},
    (Color){255, 255, 255, 255},
    (Color){255, 85, 85, 255},
    (Color){170, 170, 170, 255},
    (Color){0, 255, 255, 255},
    (Color){255, 255, 0, 255},
};

Rectangle UV_COORDS[7] = {
    (Rectangle){0, 0, 0, 0},
    (Rectangle){512, 0, 128, 128},
    (Rectangle){0, 0, 0, 0},
    (Rectangle){0, 0, 128, 128},
    (Rectangle){128, 0, 128, 128},
    (Rectangle){256, 0, 128, 128},
    (Rectangle){384, 0, 128, 128},
};

struct Renderer {
    int cell_size;
    int width;
    int height;
    Texture2D puffer;
    float* overlay;
};

Renderer* init_renderer(int cell_size, int width, int height) {
    Renderer* renderer = (Renderer*)calloc(1, sizeof(Renderer));
    renderer->cell_size = cell_size;
    renderer->width = width;
    renderer->height = height;

    renderer->overlay = (float*)calloc(width*height, sizeof(float));

    InitWindow(width*cell_size, height*cell_size, "PufferLib Grid");
    SetTargetFPS(60);

    renderer->puffer = LoadTexture("resources/shared/puffers_128.png");
    return renderer;
}

void clear_overlay(Renderer* renderer) {
    memset(renderer->overlay, 0, renderer->width*renderer->height*sizeof(float));
}

void close_renderer(Renderer* renderer) {
    CloseWindow();
    free(renderer->overlay);
    free(renderer);
}

void c_render(Grid* env) {
    // TODO: fractional rendering
    float frac = 0.0;
    float overlay = 0.0;
    if (env->renderer == NULL) {
        env->renderer = init_renderer(16, env->max_size, env->max_size);
    }
    Renderer* renderer = env->renderer;
 
    if (IsKeyDown(KEY_ESCAPE)) {
        exit(0);
    }

    Agent* agent = &env->agents[0];
    int r = agent->y;
    int c = agent->x;
    int adr = grid_offset(env, r, c);
    //renderer->overlay[adr] = overlay;
    //renderer->overlay[adr] -= 0.1;
    //renderer->overlay[adr] = -1 + 1.0/(float)env->counts[adr];

    BeginDrawing();
    ClearBackground((Color){6, 24, 24, 255});

    int ts = renderer->cell_size;
    for (int r = 0; r < env->height; r++) {
        for (int c = 0; c < env->width; c++){
            adr = grid_offset(env, r, c);
            int tile = env->grid[adr];
            if (tile == EMPTY) {
                continue;
                overlay = renderer->overlay[adr];
                if (overlay == 0) {
                    continue;
                }
                Color color;
                if (overlay < 0) {
                    overlay = -fmaxf(-1.0, overlay);
                    color = (Color){255.0*overlay, 0, 0, 255};
                } else {
                    overlay = fminf(1.0, overlay);
                    color = (Color){0, 255.0*overlay, 0, 255};
                }
                DrawRectangle(c*ts, r*ts, ts, ts, color);
            }

            Color color;
            if (tile == WALL) {
                color = (Color){128, 128, 128, 255};
            } else if (tile == GOAL) {
                color = GREEN;
            } else if (is_locked_door(tile)) {
                int weight = 40*(tile - DOOR_LOCKED);
                color = (Color){weight, 0, 0, 255};
            } else if (is_open_door(tile)) {
                int weight = 40*(tile - DOOR_OPEN);
                color = (Color){0, weight, 0, 255};
            } else if (is_key(tile)) {
                int weight = 40*(tile - KEY);
                color = (Color){0, 0, weight, 255};
            } else {
                continue;
            }
 
            DrawRectangle(c*ts, r*ts, ts, ts, color);
       }
    }

    for (int i = 0; i < env->num_agents; i++) {
        agent = &env->agents[0];
        float y = agent->y + (frac - 1)*(agent->y - agent->prev_y);
        float x = agent->x + (frac - 1)*(agent->x - agent->prev_x);
        int u = 0;
        int v = 0;
        Rectangle source_rect = (Rectangle){u, v, 128, 128};
        Rectangle dest_rect = (Rectangle){x*ts, y*ts, ts, ts};
        DrawTexturePro(renderer->puffer, source_rect, dest_rect,
            (Vector2){0, 0}, 0, WHITE);
    }
 
    EndDrawing();
}

void generate_locked_room(Grid* env) {
    assert(env->max_size >= 19);
    env->width = 19;
    env->height = 19;
    env->num_agents = 1;
    env->horizon = 1000;
    env->speed = 1;
    env->vision = 3;
    env->discretize = true;

    Agent* agent = &env->agents[0];
    agent->x = 9;
    agent->y = 9;
    agent->prev_x = 9;
    agent->prev_y = 9;
    agent->spawn_y = 9;
    agent->spawn_x = 9;
    agent->color = 6;
    agent->held = -1;

    make_border(env);

    for (int r = 0; r < env->height; r++) {
        int adr = grid_offset(env, r, 7);
        env->grid[adr] = WALL;
        adr = grid_offset(env, r, 11);
        env->grid[adr] = WALL;
    }
    for (int c = 0; c < 7; c++) {
        int adr = grid_offset(env, 6, c);
        env->grid[adr] = WALL;
        adr = grid_offset(env, 12, c);
        env->grid[adr] = WALL;
    }
    for (int c = 11; c < env->width; c++) {
        int adr = grid_offset(env, 6, c);
        env->grid[adr] = WALL;
        adr = grid_offset(env, 12, c);
        env->grid[adr] = WALL;
    }
    int adr = grid_offset(env, 3, 7);
    env->grid[adr] = DOOR_OPEN;
    adr = grid_offset(env, 9, 7);
    env->grid[adr] = DOOR_OPEN + 1;
    adr = grid_offset(env, 15, 7);
    env->grid[adr] = DOOR_OPEN + 2;
    adr = grid_offset(env, 3, 11);
    env->grid[adr] = DOOR_OPEN + 3;
    adr = grid_offset(env, 9, 11);
    env->grid[adr] = DOOR_OPEN + 4;
    adr = grid_offset(env, 15, 11);
    env->grid[adr] = DOOR_LOCKED + 5;

    adr = grid_offset(env, 4, 15);
    env->grid[adr] = KEY + 5;

    adr = grid_offset(env, 16, 17);
    env->grid[adr] = GOAL;
}

void generate_growing_tree_maze(unsigned char* grid,
        int width, int height, int max_size, float difficulty, int seed) {
    srand(seed);
    int dx[4] = {-1, 0, 1, 0};
    int dy[4] = {0, 1, 0, -1};
    int dirs[4] = {0, 1, 2, 3};
    int cells[2*width*height];
    int num_cells = 1;

    bool visited[width*height];
    memset(visited, false, width*height);

    memset(grid, WALL, max_size*height);
    for (int r = 0; r < height; r++) {
        for (int c = 0; c < width; c++) {
            int adr = r*max_size + c;
            if (r % 2 == 1 && c % 2 == 1) {
                grid[adr] = EMPTY;
            }
        }
    }

    int x_init = rand() % (width - 1);
    int y_init = rand() % (height - 1);

    if (x_init % 2 == 0) {
        x_init++;
    }
    if (y_init % 2 == 0) {
        y_init++;
    }

    int adr = y_init*height + x_init;
    visited[adr] = true;
    cells[0] = x_init;
    cells[1] = y_init;

    //int cell = 32;
    //InitWindow(width*cell, height*cell, "PufferLib Ray Grid");
    //SetTargetFPS(60);

    while (num_cells > 0) {
        if (rand() % 1000 > 1000*difficulty) {
            int i = rand() % num_cells;
            int tmp_x = cells[2*num_cells - 2];
            int tmp_y = cells[2*num_cells - 1];
            cells[2*num_cells - 2] = cells[2*i];
            cells[2*num_cells - 1] = cells[2*i + 1];
            cells[2*i] = tmp_x;
            cells[2*i + 1] = tmp_y;
 
        }

        int x = cells[2*num_cells - 2];
        int y = cells[2*num_cells - 1];
 
        int nx, ny;

        // In-place direction shuffle
        for (int i = 0; i < 4; i++) {
            int ii = i + rand() % (4 - i);
            int tmp = dirs[i];
            dirs[i] = dirs[ii];
            dirs[ii] = tmp;
        }

        bool made_path = false;
        for (int dir_i = 0; dir_i < 4; dir_i++) {
            int dir = dirs[dir_i];
            nx = x + 2*dx[dir];
            ny = y + 2*dy[dir];
           
            if (nx <= 0 || nx >= width-1 || ny <= 0 || ny >= height-1) {
                continue;
            }

            int visit_adr = ny*width + nx;
            if (visited[visit_adr]) {
                continue;
            }

            visited[visit_adr] = true;
            cells[2*num_cells] = nx;
            cells[2*num_cells + 1] = ny;

            nx = x + dx[dir];
            ny = y + dy[dir];

            int adr = ny*max_size + nx;
            grid[adr] = EMPTY;
            num_cells++;

            made_path = true;

            /*
            if (IsKeyPressed(KEY_ESCAPE)) {
                exit(0);
            }
            BeginDrawing();
            ClearBackground((Color){6, 24, 24, 255});
            Color color = (Color){128, 128, 128, 255};
            for (int r = 0; r < height; r++) {
                for (int c = 0; c < width; c++){
                    int adr = r*max_size + c;
                    int tile = grid[adr];
                    if (tile == WALL) {
                        DrawRectangle(c*cell, r*cell, cell, cell, color);
                    }
               }
            }
            EndDrawing();
            */

            break;
        }
        if (!made_path) {
            num_cells--;
        }
    }
}

void create_maze_level(Grid* env, int width, int height, float difficulty, int seed) {
    env->width = width;
    env->height = height;
    generate_growing_tree_maze(env->grid, width, height, env->max_size, difficulty, seed);
    make_border(env);
    spawn_agent(env, 0, 1, 1);
    int goal_adr = grid_offset(env, env->height - 2, env->width - 2);
    env->grid[goal_adr] = GOAL;
}



================================================
FILE: pufferlib/ocean/grid/grid.py
================================================
import numpy as np
import os

import gymnasium

import pufferlib
from pufferlib.ocean.grid import binding

class Grid(pufferlib.PufferEnv):
    def __init__(self, render_mode='raylib', vision_range=5,
            num_envs=4096, num_maps=1000, map_size=-1, max_size=9,
            report_interval=128, buf=None, seed=0):
        assert map_size <= max_size
        self.obs_size = 2*vision_range + 1
        self.single_observation_space = gymnasium.spaces.Box(low=0, high=255,
            shape=(self.obs_size*self.obs_size,), dtype=np.uint8)
        self.single_action_space = gymnasium.spaces.Discrete(5)
        self.render_mode = render_mode
        self.num_agents = num_envs
        self.report_interval = report_interval
        super().__init__(buf=buf)
        self.float_actions = np.zeros_like(self.actions).astype(np.float32)
        self.c_state = binding.shared(num_maps=num_maps, max_size=max_size, size=map_size)
        self.c_envs = binding.vec_init(self.observations, self.float_actions,
            self.rewards, self.terminals, self.truncations, num_envs, seed,
            state=self.c_state, max_size=max_size, num_maps=num_maps)
        pass

    def reset(self, seed=None):
        self.tick = 0
        binding.vec_reset(self.c_envs, seed)
        return self.observations, []

    def step(self, actions):
        self.float_actions[:] = actions
        binding.vec_step(self.c_envs)

        info = []
        if self.tick % self.report_interval == 0:
            info.append(binding.vec_log(self.c_envs))

        self.tick += 1
        return (self.observations, self.rewards,
            self.terminals, self.truncations, info)

    def render(self, overlay=0):
        binding.vec_render(self.c_envs, overlay)

    def close(self):
        pass
        #binding.vec_close(self.c_envs)

def test_performance(timeout=10, atn_cache=1024):
    env = CGrid(num_envs=1000)
    env.reset()
    tick = 0

    actions = np.random.randint(0, 2, (atn_cache, env.num_envs))

    import time
    start = time.time()
    while time.time() - start < timeout:
        atn = actions[tick % atn_cache]
        env.step(atn)
        tick += 1

    print(f'SPS: %f', env.num_envs * tick / (time.time() - start))



================================================
FILE: pufferlib/ocean/impulse_wars/README.md
================================================
# Impulse Wars

To build, you need to have the following:
- cmake
- make
- ninja
- raylib required deps installed: https://github.com/raysan5/raylib/wiki/Working-on-GNU-Linux

Run `make && cp python-module-release/binding.*.so .` to build the python module in release mode.
`puffer_impulse_wars` env should now be trainable.

When watching evaluations, you need to set all instances of `is_training = False` and `render = True` in the config file.



================================================
FILE: pufferlib/ocean/impulse_wars/benchmark.c
================================================
#include "env.h"

void randActions(iwEnv *e) {
    // e->lastRandState = e->randState;
    uint8_t actionOffset = 0;
    for (uint8_t i = 0; i < e->numDrones; i++) {
        e->actions[actionOffset + 0] = randFloat(&e->randState, -1.0f, 1.0f);
        e->actions[actionOffset + 1] = randFloat(&e->randState, -1.0f, 1.0f);
        e->actions[actionOffset + 2] = randFloat(&e->randState, -1.0f, 1.0f);
        e->actions[actionOffset + 3] = randFloat(&e->randState, -1.0f, 1.0f);
        e->actions[actionOffset + 4] = randFloat(&e->randState, -1.0f, 1.0f);
        e->actions[actionOffset + 5] = randFloat(&e->randState, -1.0f, 1.0f);
        e->actions[actionOffset + 6] = randFloat(&e->randState, -1.0f, 1.0f);

        actionOffset += CONTINUOUS_ACTION_SIZE;
    }
}

void perfTest(const uint32_t numSteps) {
    const uint8_t NUM_DRONES = 2;

    iwEnv *e = fastCalloc(1, sizeof(iwEnv));

    posix_memalign((void **)&e->observations, sizeof(void *), alignedSize(NUM_DRONES * obsBytes(NUM_DRONES), sizeof(float)));
    e->rewards = fastCalloc(NUM_DRONES, sizeof(float));
    e->actions = fastCalloc(NUM_DRONES * CONTINUOUS_ACTION_SIZE, sizeof(float));
    e->masks = fastCalloc(NUM_DRONES, sizeof(uint8_t));
    e->terminals = fastCalloc(NUM_DRONES, sizeof(uint8_t));
    e->truncations = fastCalloc(NUM_DRONES, sizeof(uint8_t));

    // rayClient *client = createRayClient();
    // e->client = client;

    uint64_t seed = time(NULL);
    printf("seed: %lu\n", seed);
    initEnv(e, NUM_DRONES, 0, -1, seed, false, false, true, false);
    initMaps(e);

    // randActions(e);
    setupEnv(e);
    stepEnv(e);

    uint32_t steps = 0;
    while (steps != numSteps) {
        // randActions(e);
        stepEnv(e);
        steps++;
    }

    destroyEnv(e);
    destroyMaps();
    free(e->observations);
    fastFree(e->actions);
    fastFree(e->rewards);
    fastFree(e->masks);
    fastFree(e->terminals);
    fastFree(e->truncations);
    fastFree(e);
}

int main(void) {
    perfTest(2500000);
    return 0;
}



================================================
FILE: pufferlib/ocean/impulse_wars/binding.c
================================================
#include <Python.h>

#include "env.h"

static PyObject *get_consts(PyObject *self, PyObject *args);

#define Env iwEnv
#define MY_SHARED
#define MY_METHODS {"get_consts", get_consts, METH_VARARGS, "Get constants"}

#include "../env_binding.h"

#define setDictVal(dict, key, val)                                            \
    if (PyDict_SetItemString(dict, key, PyLong_FromLong(val)) < 0) {          \
        PyErr_SetString(PyExc_RuntimeError, "Failed to set " key " in dict"); \
        return NULL;                                                          \
    }

static PyObject *get_consts(PyObject *self, PyObject *args) {
    PyObject *dronesArg = PyTuple_GetItem(args, 0);
    if (!PyObject_TypeCheck(dronesArg, &PyLong_Type)) {
        PyErr_SetString(PyExc_TypeError, "num_drones must be an integer");
        return NULL;
    }
    const uint8_t numDrones = (uint8_t)PyLong_AsLong(dronesArg);

    PyObject *dict = PyDict_New();
    if (PyErr_Occurred()) {
        return NULL;
    }

    const uint16_t droneObsOffset = ENEMY_DRONE_OBS_OFFSET + ((numDrones - 1) * ENEMY_DRONE_OBS_SIZE);

    setDictVal(dict, "obsBytes", obsBytes(numDrones));
    setDictVal(dict, "mapObsSize", MAP_OBS_SIZE);
    setDictVal(dict, "discreteObsSize", discreteObsSize(numDrones));
    setDictVal(dict, "continuousObsSize", continuousObsSize(numDrones));
    setDictVal(dict, "continuousObsBytes", continuousObsSize(numDrones) * sizeof(float));
    setDictVal(dict, "wallTypes", NUM_WALL_TYPES);
    setDictVal(dict, "weaponTypes", NUM_WEAPONS + 1);
    setDictVal(dict, "mapObsRows", MAP_OBS_ROWS);
    setDictVal(dict, "mapObsColumns", MAP_OBS_COLUMNS);
    setDictVal(dict, "continuousObsOffset", alignedSize(MAP_OBS_SIZE, sizeof(float)));
    setDictVal(dict, "numNearWallObs", NUM_NEAR_WALL_OBS);
    setDictVal(dict, "nearWallTypesObsOffset", NEAR_WALL_TYPES_OBS_OFFSET);
    setDictVal(dict, "nearWallPosObsSize", NEAR_WALL_POS_OBS_SIZE);
    setDictVal(dict, "nearWallObsSize", NEAR_WALL_OBS_SIZE);
    setDictVal(dict, "nearWallPosObsOffset", NEAR_WALL_POS_OBS_OFFSET);
    setDictVal(dict, "numFloatingWallObs", NUM_FLOATING_WALL_OBS);
    setDictVal(dict, "floatingWallTypesObsOffset", FLOATING_WALL_TYPES_OBS_OFFSET);
    setDictVal(dict, "floatingWallInfoObsSize", FLOATING_WALL_INFO_OBS_SIZE);
    setDictVal(dict, "floatingWallObsSize", FLOATING_WALL_OBS_SIZE);
    setDictVal(dict, "floatingWallInfoObsOffset", FLOATING_WALL_INFO_OBS_OFFSET);
    setDictVal(dict, "numWeaponPickupObs", NUM_WEAPON_PICKUP_OBS);
    setDictVal(dict, "weaponPickupTypesObsOffset", WEAPON_PICKUP_WEAPONS_OBS_OFFSET);
    setDictVal(dict, "weaponPickupPosObsSize", WEAPON_PICKUP_POS_OBS_SIZE);
    setDictVal(dict, "weaponPickupObsSize", WEAPON_PICKUP_OBS_SIZE);
    setDictVal(dict, "weaponPickupPosObsOffset", WEAPON_PICKUP_POS_OBS_OFFSET);
    setDictVal(dict, "numProjectileObs", NUM_PROJECTILE_OBS);
    setDictVal(dict, "projectileDroneObsOffset", PROJECTILE_DRONE_OBS_OFFSET);
    setDictVal(dict, "projectileTypesObsOffset", PROJECTILE_WEAPONS_OBS_OFFSET);
    setDictVal(dict, "projectileInfoObsSize", PROJECTILE_INFO_OBS_SIZE);
    setDictVal(dict, "projectileObsSize", PROJECTILE_OBS_SIZE);
    setDictVal(dict, "projectileInfoObsOffset", PROJECTILE_INFO_OBS_OFFSET);
    setDictVal(dict, "enemyDroneWeaponsObsOffset", ENEMY_DRONE_WEAPONS_OBS_OFFSET);
    setDictVal(dict, "enemyDroneObsOffset", ENEMY_DRONE_OBS_OFFSET);
    setDictVal(dict, "enemyDroneObsSize", ENEMY_DRONE_OBS_SIZE);
    setDictVal(dict, "droneObsOffset", droneObsOffset);
    setDictVal(dict, "droneObsSize", DRONE_OBS_SIZE);
    setDictVal(dict, "miscObsSize", MISC_OBS_SIZE);
    setDictVal(dict, "miscObsOffset", droneObsOffset + DRONE_OBS_SIZE);

    setDictVal(dict, "maxDrones", MAX_DRONES);
    setDictVal(dict, "contActionsSize", CONTINUOUS_ACTION_SIZE);

    return dict;
}

static PyObject *my_shared(PyObject *self, PyObject *args, PyObject *kwargs) {
    VecEnv *ve = unpack_vecenv(args);
    initMaps(ve->envs[0]);

    for (uint16_t i = 0; i < ve->num_envs; i++) {
        iwEnv *e = (iwEnv *)ve->envs[i];
        setupEnv(e);
    }

    return Py_None;
}

static int my_init(iwEnv *e, PyObject *args, PyObject *kwargs) {
    initEnv(
        e,
        (uint8_t)unpack(kwargs, "num_drones"),
        (uint8_t)unpack(kwargs, "num_agents"),
        (int8_t)unpack(kwargs, "map_idx"),
        (uint64_t)unpack(kwargs, "seed"),
        (bool)unpack(kwargs, "enable_teams"),
        (bool)unpack(kwargs, "sitting_duck"),
        (bool)unpack(kwargs, "is_training"),
        (bool)unpack(kwargs, "continuous")
    );
    return 0;
}

#define _LOG_BUF_SIZE 128

char *droneLog(char *buf, const uint8_t droneIdx, const char *name) {
    snprintf(buf, _LOG_BUF_SIZE, "drone_%d_%s", droneIdx, name);
    return buf;
}

char *weaponLog(char *buf, const uint8_t droneIdx, const uint8_t weaponIdx, const char *name) {
    snprintf(buf, _LOG_BUF_SIZE, "drone_%d_%s_%s", droneIdx, weaponNames[weaponIdx], name);
    return buf;
}

static int my_log(PyObject *dict, Log *log) {
    assign_to_dict(dict, "episode_length", log->length);
    assign_to_dict(dict, "ties", log->ties);

    assign_to_dict(dict, "perf", log->stats[0].wins);
    assign_to_dict(dict, "score", log->stats[0].wins);

    char buf[_LOG_BUF_SIZE] = {0};
    for (uint8_t i = 0; i < MAX_DRONES; i++) {
        assign_to_dict(dict, droneLog(buf, i, "returns"), log->stats[i].returns);
        assign_to_dict(dict, droneLog(buf, i, "distance_traveled"), log->stats[i].distanceTraveled);
        assign_to_dict(dict, droneLog(buf, i, "abs_distance_traveled"), log->stats[i].absDistanceTraveled);
        assign_to_dict(dict, droneLog(buf, i, "brake_time"), log->stats[i].brakeTime);
        assign_to_dict(dict, droneLog(buf, i, "total_bursts"), log->stats[i].totalBursts);
        assign_to_dict(dict, droneLog(buf, i, "bursts_hit"), log->stats[i].burstsHit);
        assign_to_dict(dict, droneLog(buf, i, "energy_emptied"), log->stats[i].energyEmptied);
        assign_to_dict(dict, droneLog(buf, i, "wins"), log->stats[i].wins);

        // useful for debugging weapon balance, but really slows down
        // sweeps due to adding a ton of extra logging data
        //
        // for (uint8_t j = 0; j < _NUM_WEAPONS; j++) {
        //     assign_to_dict(dict, weaponLog(buf, i, j, "shots_fired"), log->stats[i].shotsFired[j]);
        //     assign_to_dict(dict, weaponLog(buf, i, j, "shots_hit"), log->stats[i].shotsHit[j]);
        //     assign_to_dict(dict, weaponLog(buf, i, j, "shots_taken"), log->stats[i].shotsTaken[j]);
        //     assign_to_dict(dict, weaponLog(buf, i, j, "own_shots_taken"), log->stats[i].ownShotsTaken[j]);
        //     assign_to_dict(dict, weaponLog(buf, i, j, "picked_up"), log->stats[i].weaponsPickedUp[j]);
        //     assign_to_dict(dict, weaponLog(buf, i, j, "shot_distances"), log->stats[i].shotDistances[j]);
        // }

        assign_to_dict(dict, droneLog(buf, i, "total_shots_fired"), log->stats[i].totalShotsFired);
        assign_to_dict(dict, droneLog(buf, i, "total_shots_hit"), log->stats[i].totalShotsHit);
        assign_to_dict(dict, droneLog(buf, i, "total_shots_taken"), log->stats[i].totalShotsTaken);
        assign_to_dict(dict, droneLog(buf, i, "total_own_shots_taken"), log->stats[i].totalOwnShotsTaken);
        assign_to_dict(dict, droneLog(buf, i, "total_picked_up"), log->stats[i].totalWeaponsPickedUp);
        assign_to_dict(dict, droneLog(buf, i, "total_shot_distances"), log->stats[i].totalShotDistances);
    }

    return 0;
}



================================================
FILE: pufferlib/ocean/impulse_wars/CMakeLists.txt
================================================
# 3.22 was released on Nov 2021, should be widely available
cmake_minimum_required(VERSION 3.22)
include(FetchContent)

project(
	impulse-wars
	DESCRIPTION "Impulse Wars"
	LANGUAGES C
)

message(INFO " C Compiler: ${CMAKE_C_COMPILER} ${CMAKE_C_COMPILER_VERSION} ${CMAKE_C_COMPILER_ID}")

# use ccache if available to speed up subsequent builds
find_program(CCACHE_FOUND "ccache")
if(CCACHE_FOUND)
	set(CMAKE_C_COMPILER_LAUNCHER "ccache")
endif()

# enable some C23 features, the c2x standard is a WIP standard supported
# by gcc since 9 (May 2019) and clang since 9 (Sep 2019)
set(CMAKE_C_FLAGS_INIT " -std=c2x")

# force position independent code everywhere to prevent some rare
# linker errors depending on what compiler is used
add_compile_options("-fPIC")

if(CMAKE_BUILD_TYPE MATCHES Debug)
	# leak detection doesn't work correctly when the code is called by
	# Python, so disable it
	if(DEFINED BUILD_PYTHON_MODULE)
		add_compile_options("-fno-omit-frame-pointer" "-fsanitize=address,undefined,bounds,pointer-overflow")
		add_link_options("-shared-libasan" "-fno-omit-frame-pointer" "-fsanitize=address,undefined,bounds,pointer-overflow")
	else()
		add_compile_options("-fno-omit-frame-pointer" "-fsanitize=address,undefined,bounds,pointer-overflow,leak")
		add_link_options("-fno-omit-frame-pointer" "-fsanitize=address,undefined,bounds,pointer-overflow,leak")
	endif()

	# mold is an extremely fast linker, use it if available
	# only use mold in debug mode, link time optimization currently doesn't
	# work with mold and provides large speedups
	find_program(MOLD_FOUND "mold")
	if(MOLD_FOUND)
		add_link_options("-fuse-ld=mold")
	endif()
else()
	add_compile_options("-flto" "-fno-math-errno")
	if (NOT DEFINED EMSCRIPTEN)
		# emscripten doesn't support -march=native, it doesn't make sense
		# for WASM anyway
		add_compile_options("-march=native")
	else()
		# tell emscripten to generate an HTML file that can be used to
		# test the WASM, and ensure necessary code is transformed to be
		# async friendly; it allows the game to be run much more smoothly
		set(CMAKE_EXECUTABLE_SUFFIX ".html")
		add_link_options("-sASYNCIFY")
	endif()
	# ensure the linker used is from the same compiler toolchain, or else
	# link time optimization will probably fail; if we're using
	# emscripten it will use it's own linker
	if(CMAKE_C_COMPILER_ID MATCHES "Clang" AND NOT DEFINED EMSCRIPTEN)
		add_link_options("-fuse-ld=lld")
	endif()

	# add_compile_options("-pg")
	# add_link_options("-pg")
endif()

set_property(GLOBAL PROPERTY USE_FOLDERS ON)
set(FETCHCONTENT_QUIET FALSE)

# fetch and configure dependencies
FetchContent_Declare(
	raylib
	URL https://github.com/raysan5/raylib/archive/c1ab645ca298a2801097931d1079b10ff7eb9df8.zip # 5.5
)
set(BUILD_SHARED_LIBS OFF CACHE BOOL "Statically link raylib" FORCE)
set(WITH_PIC "Compile static library as position-independent code" ON)
set(CUSTOMIZE_BUILD ON CACHE BOOL "Customize raylib build settings" FORCE)
set(USE_AUDIO OFF CACHE BOOL "Don't build unused audio module" FORCE)
FetchContent_MakeAvailable(raylib)

# if box2d is fetched first installing built python module will fail
# for reasons unbeknownst to mere mortals
# maybe due to install prefix schenanigans?
FetchContent_Declare(
	box2d
	URL https://github.com/capnspacehook/box2d/archive/df25d747be0ab2fd9425eece022d2ec897c2028d.zip
)
set(BOX2D_ENABLE_SIMD ON CACHE BOOL "Enable SIMD math (faster)" FORCE)
set(BOX2D_AVX2 ON CACHE BOOL "Enable AVX2 (faster)" FORCE)
add_compile_definitions(B2_MAX_WORLDS=65534)
FetchContent_MakeAvailable(box2d)
# this is set to off by box2d to enable cross platform determinism, but
# I don't care about that and want the small speedup instead
target_compile_options(box2d PRIVATE "-ffp-contract=fast")

function(configure_target target_name)
	target_include_directories(
		${target_name} PRIVATE
		"${CMAKE_CURRENT_SOURCE_DIR}"
		"${CMAKE_CURRENT_SOURCE_DIR}/include"
	)

	# Mark box2d as a system include directory to suppress warnings from it
	target_include_directories(${target_name} SYSTEM PRIVATE "${box2d_SOURCE_DIR}/src")

	target_link_libraries(${target_name} PRIVATE raylib box2d)

	target_compile_options(${target_name} PRIVATE
		"-Werror" "-Wall" "-Wextra" "-Wpedantic"
		"-Wno-implicit-fallthrough" "-Wno-variadic-macros" "-Wno-strict-prototypes" "-Wno-gnu-statement-expression"
	)
endfunction()

if(DEFINED BUILD_PYTHON_MODULE)
	find_package(
		Python
		COMPONENTS Interpreter Development.Module NumPy
		REQUIRED
	)

	python_add_library(binding MODULE binding.c WITH_SOABI)

	target_include_directories(binding PRIVATE
    	${Python_NumPy_INCLUDE_DIRS}
	)

	configure_target(binding)

	install(TARGETS binding DESTINATION .)
elseif(DEFINED BUILD_DEMO)
	add_executable(demo "${CMAKE_CURRENT_SOURCE_DIR}/impulse_wars.c")
	configure_target(demo)
elseif(DEFINED BUILD_BENCHMARK)
	add_executable(benchmark "${CMAKE_CURRENT_SOURCE_DIR}/benchmark.c")
	configure_target(benchmark)
endif()



================================================
FILE: pufferlib/ocean/impulse_wars/env.h
================================================
#ifndef IMPULSE_WARS_ENV_H
#define IMPULSE_WARS_ENV_H

#ifdef __EMSCRIPTEN__
#include <emscripten.h>

double lastFrameTime = 0.0;
double accumulator = 0.0;
#endif

#include "game.h"
#include "map.h"
#include "render.h"
#include "scripted_agent.h"
#include "settings.h"
#include "types.h"

const uint8_t THREE_BIT_MASK = 0x7;
const uint8_t FOUR_BIT_MASK = 0xf;

// pufferlib compatibility
#define c_step stepEnv
#define c_reset resetEnv
#define c_render setupRayClient
#define c_close destroyEnv

// returns a cell index that is closest to pos that isn't cellIdx
uint16_t findNearestCell(const iwEnv *e, const b2Vec2 pos, const uint16_t cellIdx) {
    uint16_t closestCell = cellIdx;
    float minDistance = FLT_MAX;
    const uint8_t cellCol = cellIdx / e->map->columns;
    const uint8_t cellRow = cellIdx % e->map->columns;
    for (uint8_t i = 0; i < 8; i++) {
        const int8_t newCellCol = cellCol + cellOffsets[i][0];
        if (newCellCol < 0 || newCellCol >= e->map->columns) {
            continue;
        }
        const int8_t newCellRow = cellRow + cellOffsets[i][1];
        if (newCellRow < 0 || newCellRow >= e->map->rows) {
            continue;
        }
        const int16_t newCellIdx = cellIndex(e, newCellCol, newCellRow);
        const mapCell *cell = safe_array_get_at(e->cells, newCellIdx);
        if (minDistance != min(minDistance, b2DistanceSquared(pos, cell->pos))) {
            closestCell = newCellIdx;
        }
    }

    return closestCell;
}

// normalize a drone's ammo count, setting infinite ammo as no ammo
static inline float scaleAmmo(const iwEnv *e, const droneEntity *drone) {
    int8_t maxAmmo = weaponAmmo(e->defaultWeapon->type, drone->weaponInfo->type);
    float scaledAmmo = 0;
    if (drone->ammo != INFINITE) {
        scaledAmmo = scaleValue(drone->ammo, maxAmmo, true);
    }
    return scaledAmmo;
}

// fills a small 2D grid centered around the agent with discretized
// walls, floating walls, weapon pickups, and drone positions
void computeMapObs(iwEnv *e, const uint8_t agentIdx, const uint16_t obsStartOffset) {
    droneEntity *drone = safe_array_get_at(e->drones, agentIdx);
    const uint8_t droneCellCol = drone->mapCellIdx % e->map->columns;
    const uint8_t droneCellRow = drone->mapCellIdx / e->map->columns;

    const int8_t obsStartCol = droneCellCol - (MAP_OBS_COLUMNS / 2);
    const int8_t startCol = max(obsStartCol, 0);
    const int8_t obsStartRow = droneCellRow - (MAP_OBS_ROWS / 2);
    const int8_t startRow = max(obsStartRow, 0);

    const int8_t obsEndCol = droneCellCol + (MAP_OBS_COLUMNS / 2);
    const int8_t endCol = min(obsEndCol, e->map->columns - 1);
    const int8_t endRow = min(droneCellRow + (MAP_OBS_ROWS / 2), e->map->rows - 1);

    const int8_t obsColOffset = startCol - obsStartCol;
    const int8_t obsRowOffset = startRow - obsStartRow;
    uint16_t startOffset = obsStartOffset;
    if (obsColOffset == 0 && obsRowOffset != 0) {
        startOffset += obsRowOffset * MAP_OBS_COLUMNS;
    } else if (obsColOffset != 0 && obsRowOffset == 0) {
        startOffset += obsColOffset;
    } else if (obsColOffset != 0 && obsRowOffset != 0) {
        startOffset += obsColOffset + (obsRowOffset * MAP_OBS_COLUMNS);
    }
    uint16_t offset = startOffset;

    // compute map layout, and discretized positions of weapon pickups
    if (!e->suddenDeathWallsPlaced) {
        // copy precomputed map layout if sudden death walls haven't been placed
        const int8_t numCols = endCol - startCol + 1;
        for (int8_t row = startRow; row <= endRow; row++) {
            const int16_t cellIdx = cellIndex(e, startCol, row);
            memcpy(e->observations + offset, e->map->packedLayout + cellIdx, numCols * sizeof(uint8_t));
            offset += MAP_OBS_COLUMNS;
        }

        // compute discretized location of weapon pickups on grid
        for (size_t i = 0; i < cc_array_size(e->pickups); i++) {
            const weaponPickupEntity *pickup = safe_array_get_at(e->pickups, i);
            const uint8_t cellCol = pickup->mapCellIdx % e->map->columns;
            if (cellCol < startCol || cellCol > endCol) {
                continue;
            }
            const uint8_t cellRow = pickup->mapCellIdx / e->map->columns;
            if (cellRow < startRow || cellRow > endRow) {
                continue;
            }

            offset = startOffset + ((cellCol - startCol) + ((cellRow - startRow) * MAP_OBS_COLUMNS));
            ASSERTF(offset <= startOffset + MAP_OBS_SIZE, "offset: %d", offset);
            e->observations[offset] |= 1 << 3;
        }
    } else {
        // sudden death walls have been placed so compute may layout manually
        const int8_t colPadding = obsColOffset + (obsEndCol - endCol);
        for (int8_t row = startRow; row <= endRow; row++) {
            for (int8_t col = startCol; col <= endCol; col++) {
                const int16_t cellIdx = cellIndex(e, col, row);
                const mapCell *cell = safe_array_get_at(e->cells, cellIdx);
                if (cell->ent == NULL) {
                    offset++;
                    continue;
                }

                if (entityTypeIsWall(cell->ent->type)) {
                    e->observations[offset] = ((cell->ent->type + 1) & TWO_BIT_MASK) << 5;
                } else if (cell->ent->type == WEAPON_PICKUP_ENTITY) {
                    e->observations[offset] |= 1 << 3;
                }

                offset++;
            }
            offset += colPadding;
        }
        ASSERTF(offset <= startOffset + MAP_OBS_SIZE, "offset %u startOffset %u", offset, startOffset);
    }

    // compute discretized locations of floating walls on grid
    for (size_t i = 0; i < cc_array_size(e->floatingWalls); i++) {
        const wallEntity *wall = safe_array_get_at(e->floatingWalls, i);
        const uint8_t cellCol = wall->mapCellIdx % e->map->columns;
        if (cellCol < startCol || cellCol > endCol) {
            continue;
        }
        const uint8_t cellRow = wall->mapCellIdx / e->map->columns;
        if (cellRow < startRow || cellRow > endRow) {
            continue;
        }

        offset = startOffset + ((cellCol - startCol) + ((cellRow - startRow) * MAP_OBS_COLUMNS));
        ASSERTF(offset <= startOffset + MAP_OBS_SIZE, "offset: %d", offset);
        e->observations[offset] = ((wall->type + 1) & TWO_BIT_MASK) << 5;
        e->observations[offset] |= 1 << 4;
    }

    // compute discretized location and index of drones on grid
    uint8_t newDroneIdx = 1;
    uint16_t droneCells[e->numDrones];
    memset(droneCells, 0x0, sizeof(droneCells));
    for (uint8_t i = 0; i < cc_array_size(e->drones); i++) {
        if (i == agentIdx) {
            continue;
        }

        // ensure drones do not share cells in the observation
        droneEntity *otherDrone = safe_array_get_at(e->drones, i);
        if (i != 0) {
            for (uint8_t j = 0; j < i; j++) {
                if (droneCells[j] == otherDrone->mapCellIdx) {
                    otherDrone->mapCellIdx = findNearestCell(e, otherDrone->pos, otherDrone->mapCellIdx);
                    break;
                }
            }
        }
        const uint8_t cellCol = otherDrone->mapCellIdx % e->map->columns;
        if (cellCol < startCol || cellCol > endCol) {
            continue;
        }
        const uint8_t cellRow = otherDrone->mapCellIdx / e->map->columns;
        if (cellRow < startRow || cellRow > endRow) {
            continue;
        }
        droneCells[i] = otherDrone->mapCellIdx;

        offset = startOffset + ((cellCol - startCol) + ((cellRow - startRow) * MAP_OBS_COLUMNS));
        ASSERTF(offset <= startOffset + MAP_OBS_SIZE, "offset: %d", offset);
        e->observations[offset] |= (newDroneIdx++ & THREE_BIT_MASK);
    }
}

// computes observations for N nearest walls, floating walls, and weapon pickups
void computeNearObs(iwEnv *e, const droneEntity *drone, const uint16_t discreteObsStart, float *continuousObs) {
    nearEntity nearWalls[NUM_NEAR_WALL_OBS];
    findNearWalls(e, drone, nearWalls, NUM_NEAR_WALL_OBS);

    uint16_t offset;

    // compute type and position of N nearest walls
    for (uint8_t i = 0; i < NUM_NEAR_WALL_OBS; i++) {
        const wallEntity *wall = nearWalls[i].entity;

        offset = discreteObsStart + NEAR_WALL_TYPES_OBS_OFFSET + i;
        ASSERTF(offset <= discreteObsStart + FLOATING_WALL_TYPES_OBS_OFFSET, "offset: %d", offset);
        e->observations[offset] = wall->type;

        // DEBUG_LOGF("wall %d cell %d", i, wall->mapCellIdx);

        offset = NEAR_WALL_POS_OBS_OFFSET + (i * NEAR_WALL_POS_OBS_SIZE);
        ASSERTF(offset <= FLOATING_WALL_INFO_OBS_OFFSET, "offset: %d", offset);
        const b2Vec2 wallRelPos = b2Sub(wall->pos, drone->pos);

        continuousObs[offset++] = scaleValue(wallRelPos.x, MAX_X_POS, false);
        continuousObs[offset] = scaleValue(wallRelPos.y, MAX_Y_POS, false);
    }

    if (cc_array_size(e->floatingWalls) != 0) {
        // find N nearest floating walls
        nearEntity nearFloatingWalls[MAX_FLOATING_WALLS] = {0};
        for (uint8_t i = 0; i < cc_array_size(e->floatingWalls); i++) {
            wallEntity *wall = safe_array_get_at(e->floatingWalls, i);
            const nearEntity nearEnt = {
                .entity = wall,
                .distanceSquared = b2DistanceSquared(wall->pos, drone->pos),
            };
            nearFloatingWalls[i] = nearEnt;
        }
        insertionSort(nearFloatingWalls, cc_array_size(e->floatingWalls));

        // compute type, position, angle and velocity of N nearest floating walls
        for (uint8_t i = 0; i < cc_array_size(e->floatingWalls); i++) {
            if (i == NUM_FLOATING_WALL_OBS) {
                break;
            }
            const wallEntity *wall = nearFloatingWalls[i].entity;

            const b2Transform wallTransform = b2Body_GetTransform(wall->bodyID);
            const b2Vec2 wallRelPos = b2Sub(wallTransform.p, drone->pos);
            const float angle = b2Rot_GetAngle(wallTransform.q);

            offset = discreteObsStart + FLOATING_WALL_TYPES_OBS_OFFSET + i;
            ASSERTF(offset <= discreteObsStart + PROJECTILE_DRONE_OBS_OFFSET, "offset: %d", offset);
            e->observations[offset] = wall->type + 1;

            // DEBUG_LOGF("floating wall %d cell %d", i, wall->mapCellIdx);

            offset = FLOATING_WALL_INFO_OBS_OFFSET + (i * FLOATING_WALL_INFO_OBS_SIZE);
            ASSERTF(offset <= WEAPON_PICKUP_POS_OBS_OFFSET, "offset: %d", offset);
            continuousObs[offset++] = scaleValue(wallRelPos.x, MAX_X_POS, false);
            continuousObs[offset++] = scaleValue(wallRelPos.y, MAX_Y_POS, false);
            continuousObs[offset++] = scaleValue(angle, MAX_ANGLE, false);
            continuousObs[offset++] = scaleValue(wall->velocity.x, MAX_SPEED, false);
            continuousObs[offset] = scaleValue(wall->velocity.y, MAX_SPEED, false);
        }
    }

    if (cc_array_size(e->pickups) != 0) {
        // find N nearest weapon pickups
        nearEntity nearPickups[MAX_WEAPON_PICKUPS] = {0};
        for (uint8_t i = 0; i < cc_array_size(e->pickups); i++) {
            weaponPickupEntity *pickup = safe_array_get_at(e->pickups, i);
            const nearEntity nearEnt = {
                .entity = pickup,
                .distanceSquared = b2DistanceSquared(pickup->pos, drone->pos),
            };
            nearPickups[i] = nearEnt;
        }
        insertionSort(nearPickups, cc_array_size(e->pickups));

        // compute type and location of N nearest weapon pickups
        for (uint8_t i = 0; i < cc_array_size(e->pickups); i++) {
            if (i == NUM_WEAPON_PICKUP_OBS) {
                break;
            }
            const weaponPickupEntity *pickup = nearPickups[i].entity;

            offset = discreteObsStart + WEAPON_PICKUP_WEAPONS_OBS_OFFSET + i;
            ASSERTF(offset <= discreteObsStart + ENEMY_DRONE_WEAPONS_OBS_OFFSET, "offset: %d", offset);
            e->observations[offset] = pickup->weapon + 1;

            // DEBUG_LOGF("pickup %d cell %d", i, pickup->mapCellIdx);

            offset = WEAPON_PICKUP_POS_OBS_OFFSET + (i * WEAPON_PICKUP_POS_OBS_SIZE);
            ASSERTF(offset <= PROJECTILE_INFO_OBS_OFFSET, "offset: %d", offset);
            const b2Vec2 pickupRelPos = b2Sub(pickup->pos, drone->pos);
            continuousObs[offset++] = scaleValue(pickupRelPos.x, MAX_X_POS, false);
            continuousObs[offset] = scaleValue(pickupRelPos.y, MAX_Y_POS, false);
        }
    }
}

void computeObs(iwEnv *e) {
    for (uint8_t agentIdx = 0; agentIdx < e->numAgents; agentIdx++) {
        droneEntity *agentDrone = safe_array_get_at(e->drones, agentIdx);
        // if the drone is dead, only compute observations if it died
        // this step and it isn't out of bounds
        if (agentDrone->livesLeft == 0 && (!agentDrone->diedThisStep || agentDrone->mapCellIdx == -1)) {
            continue;
        }

        // compute discrete map observations
        const uint16_t discreteObsStart = e->obsBytes * agentIdx;
        memset(e->observations + discreteObsStart, 0x0, e->obsBytes);
        computeMapObs(e, agentIdx, discreteObsStart);

        // compute continuous observations
        uint16_t discreteObsOffset;
        uint16_t continuousObsOffset;
        const uint16_t continuousObsStart = discreteObsStart + e->discreteObsBytes;
        float *continuousObs = (float *)(e->observations + continuousObsStart);

        computeNearObs(e, agentDrone, discreteObsStart, continuousObs);

        // sort projectiles by distance to the current agent
        const b2Vec2 agentPos = agentDrone->pos;
        const size_t numProjectiles = cc_array_size(e->projectiles);
        if (numProjectiles > 0) {
            projectileEntity *sortedProjectiles[numProjectiles];
            memcpy(sortedProjectiles, e->projectiles->buffer, numProjectiles * sizeof(projectileEntity *));

            for (int16_t i = 1; i < (int64_t)numProjectiles; i++) {
                projectileEntity *key = sortedProjectiles[i];
                const float keyDistance = b2DistanceSquared(agentPos, key->pos);
                int16_t j = i - 1;

                while (j >= 0 && b2DistanceSquared(agentPos, sortedProjectiles[j]->pos) > keyDistance) {
                    sortedProjectiles[j + 1] = sortedProjectiles[j];
                    j = j - 1;
                }

                sortedProjectiles[j + 1] = key;
            }

            // compute type and location of N projectiles
            for (size_t i = 0; i < numProjectiles; i++) {
                if (i == NUM_PROJECTILE_OBS) {
                    break;
                }
                const projectileEntity *projectile = sortedProjectiles[i];

                discreteObsOffset = discreteObsStart + PROJECTILE_DRONE_OBS_OFFSET + i;
                ASSERTF(discreteObsOffset <= discreteObsStart + PROJECTILE_WEAPONS_OBS_OFFSET, "offset: %d", discreteObsOffset);
                e->observations[discreteObsOffset] = projectile->droneIdx + 1;

                discreteObsOffset = discreteObsStart + PROJECTILE_WEAPONS_OBS_OFFSET + i;
                ASSERTF(discreteObsOffset <= discreteObsStart + WEAPON_PICKUP_WEAPONS_OBS_OFFSET, "offset: %d", discreteObsOffset);
                e->observations[discreteObsOffset] = projectile->weaponInfo->type + 1;

                continuousObsOffset = PROJECTILE_INFO_OBS_OFFSET + (i * PROJECTILE_INFO_OBS_SIZE);
                ASSERTF(continuousObsOffset <= ENEMY_DRONE_OBS_OFFSET, "offset: %d", continuousObsOffset);
                const b2Vec2 projectileRelPos = b2Sub(projectile->pos, agentDrone->pos);
                continuousObs[continuousObsOffset++] = scaleValue(projectileRelPos.x, MAX_X_POS, false);
                continuousObs[continuousObsOffset++] = scaleValue(projectileRelPos.y, MAX_Y_POS, false);
                continuousObs[continuousObsOffset++] = scaleValue(projectile->velocity.x, MAX_SPEED, false);
                continuousObs[continuousObsOffset] = scaleValue(projectile->velocity.y, MAX_SPEED, false);
            }
        }

        // compute enemy drone observations
        bool hitShot = false;
        bool tookShot = false;
        uint8_t processedDrones = 0;
        for (uint8_t i = 0; i < e->numDrones; i++) {
            if (i == agentIdx) {
                continue;
            }

            if (agentDrone->stepInfo.shotHit[i]) {
                hitShot = true;
            }
            if (agentDrone->stepInfo.shotTaken[i]) {
                tookShot = true;
            }

            droneEntity *enemyDrone = safe_array_get_at(e->drones, i);
            if (enemyDrone->livesLeft == 0) {
                processedDrones++;
                continue;
            }

            const b2Vec2 enemyDroneRelPos = b2Sub(enemyDrone->pos, agentDrone->pos);
            const float enemyDroneDistance = b2Distance(enemyDrone->pos, agentDrone->pos);
            const b2Vec2 enemyDroneAccel = b2Sub(enemyDrone->velocity, enemyDrone->lastVelocity);
            const b2Vec2 enemyDroneRelNormPos = b2Normalize(b2Sub(enemyDrone->pos, agentDrone->pos));
            const float enemyDroneAimAngle = atan2f(enemyDrone->lastAim.y, enemyDrone->lastAim.x);
            float enemyDroneBraking = 0.0f;
            if (enemyDrone->braking) {
                enemyDroneBraking = 1.0f;
            }

            discreteObsOffset = discreteObsStart + ENEMY_DRONE_WEAPONS_OBS_OFFSET + processedDrones;
            e->observations[discreteObsOffset] = enemyDrone->weaponInfo->type + 1;

            continuousObsOffset = ENEMY_DRONE_OBS_OFFSET + (e->numDrones - 1) + (processedDrones * ENEMY_DRONE_OBS_SIZE);
            continuousObs[continuousObsOffset++] = enemyDrone->team == agentDrone->team;
            continuousObs[continuousObsOffset++] = scaleValue(enemyDroneRelPos.x, MAX_X_POS, false);
            continuousObs[continuousObsOffset++] = scaleValue(enemyDroneRelPos.y, MAX_Y_POS, false);
            continuousObs[continuousObsOffset++] = scaleValue(enemyDroneDistance, MAX_DISTANCE, true);
            continuousObs[continuousObsOffset++] = scaleValue(enemyDrone->velocity.x, MAX_SPEED, false);
            continuousObs[continuousObsOffset++] = scaleValue(enemyDrone->velocity.y, MAX_SPEED, false);
            continuousObs[continuousObsOffset++] = scaleValue(enemyDroneAccel.x, MAX_ACCEL, false);
            continuousObs[continuousObsOffset++] = scaleValue(enemyDroneAccel.y, MAX_ACCEL, false);
            continuousObs[continuousObsOffset++] = scaleValue(enemyDroneRelNormPos.x, 1.0f, false);
            continuousObs[continuousObsOffset++] = scaleValue(enemyDroneRelNormPos.y, 1.0f, false);
            continuousObs[continuousObsOffset++] = scaleValue(enemyDrone->lastAim.x, 1.0f, false);
            continuousObs[continuousObsOffset++] = scaleValue(enemyDrone->lastAim.y, 1.0f, false);
            continuousObs[continuousObsOffset++] = scaleValue(enemyDroneAimAngle, PI, false);
            continuousObs[continuousObsOffset++] = scaleAmmo(e, enemyDrone);
            continuousObs[continuousObsOffset++] = scaleValue(enemyDrone->weaponCooldown, enemyDrone->weaponInfo->coolDown, true);
            continuousObs[continuousObsOffset++] = scaleValue(enemyDrone->weaponCharge, enemyDrone->weaponInfo->charge, true);
            continuousObs[continuousObsOffset++] = scaleValue(enemyDrone->energyLeft, DRONE_ENERGY_MAX, true);
            continuousObs[continuousObsOffset++] = (float)enemyDrone->energyFullyDepleted;
            continuousObs[continuousObsOffset++] = enemyDroneBraking;
            continuousObs[continuousObsOffset++] = scaleValue(enemyDrone->burstCooldown, DRONE_BURST_COOLDOWN, true);
            continuousObs[continuousObsOffset++] = (float)enemyDrone->chargingBurst;
            continuousObs[continuousObsOffset++] = scaleValue(enemyDrone->burstCharge, DRONE_ENERGY_MAX, true);
            continuousObs[continuousObsOffset++] = scaleValue(enemyDrone->livesLeft, DRONE_LIVES, true);
            continuousObs[continuousObsOffset++] = !enemyDrone->dead;

            processedDrones++;
            ASSERTF(continuousObsOffset == ENEMY_DRONE_OBS_OFFSET + (e->numDrones - 1) + (processedDrones * ENEMY_DRONE_OBS_SIZE), "offset: %d", continuousObsOffset);
        }

        // compute active drone observations
        continuousObsOffset = ENEMY_DRONE_OBS_OFFSET + ((e->numDrones - 1) * ENEMY_DRONE_OBS_SIZE);
        const b2Vec2 agentDroneAccel = b2Sub(agentDrone->velocity, agentDrone->lastVelocity);
        float agentDroneBraking = 0.0f;
        if (agentDrone->braking) {
            agentDroneBraking = 1.0f;
        }

        discreteObsOffset = discreteObsStart + ENEMY_DRONE_WEAPONS_OBS_OFFSET + e->numDrones - 1;
        e->observations[discreteObsOffset] = agentDrone->weaponInfo->type + 1;

        continuousObs[continuousObsOffset++] = scaleValue(agentDrone->pos.x, MAX_X_POS, false);
        continuousObs[continuousObsOffset++] = scaleValue(agentDrone->pos.y, MAX_Y_POS, false);
        continuousObs[continuousObsOffset++] = scaleValue(agentDrone->velocity.x, MAX_SPEED, false);
        continuousObs[continuousObsOffset++] = scaleValue(agentDrone->velocity.y, MAX_SPEED, false);
        continuousObs[continuousObsOffset++] = scaleValue(agentDroneAccel.x, MAX_ACCEL, false);
        continuousObs[continuousObsOffset++] = scaleValue(agentDroneAccel.y, MAX_ACCEL, false);
        continuousObs[continuousObsOffset++] = scaleValue(agentDrone->lastAim.x, 1.0f, false);
        continuousObs[continuousObsOffset++] = scaleValue(agentDrone->lastAim.y, 1.0f, false);
        continuousObs[continuousObsOffset++] = scaleAmmo(e, agentDrone);
        continuousObs[continuousObsOffset++] = scaleValue(agentDrone->weaponCooldown, agentDrone->weaponInfo->coolDown, true);
        continuousObs[continuousObsOffset++] = scaleValue(agentDrone->weaponCharge, agentDrone->weaponInfo->charge, true);
        continuousObs[continuousObsOffset++] = scaleValue(agentDrone->energyLeft, DRONE_ENERGY_MAX, true);
        continuousObs[continuousObsOffset++] = (float)agentDrone->energyFullyDepleted;
        continuousObs[continuousObsOffset++] = agentDroneBraking;
        continuousObs[continuousObsOffset++] = scaleValue(agentDrone->burstCooldown, DRONE_BURST_COOLDOWN, true);
        continuousObs[continuousObsOffset++] = (float)agentDrone->chargingBurst;
        continuousObs[continuousObsOffset++] = scaleValue(agentDrone->burstCharge, DRONE_ENERGY_MAX, true);
        continuousObs[continuousObsOffset++] = hitShot;
        continuousObs[continuousObsOffset++] = tookShot;
        continuousObs[continuousObsOffset++] = agentDrone->stepInfo.ownShotTaken;
        continuousObs[continuousObsOffset++] = scaleValue(agentDrone->livesLeft, DRONE_LIVES, true);
        continuousObs[continuousObsOffset++] = !agentDrone->dead;

        ASSERTF(continuousObsOffset == ENEMY_DRONE_OBS_OFFSET + ((e->numDrones - 1) * ENEMY_DRONE_OBS_SIZE) + DRONE_OBS_SIZE, "offset: %d", continuousObsOffset);
        continuousObs[continuousObsOffset] = scaleValue(e->stepsLeft, e->totalSteps, true);
    }
}

void setupEnv(iwEnv *e) {
    e->needsReset = false;

    e->stepsLeft = e->totalSteps;
    e->suddenDeathSteps = e->totalSuddenDeathSteps;
    e->suddenDeathWallCounter = 0;

    e->lastSpawnQuad = -1;

    int8_t mapIdx = e->pinnedMapIdx;
    if (e->pinnedMapIdx == -1) {
        uint8_t firstMap = 0;
        // don't evaluate on the boring empty map
        if (!e->isTraining) {
            firstMap = 1;
        }
        mapIdx = randInt(&e->randState, firstMap, NUM_MAPS - 1);
    }
    DEBUG_LOGF("setting up map %d", mapIdx);
    setupMap(e, mapIdx);

    DEBUG_LOG("creating drones");
    for (uint8_t i = 0; i < e->numDrones; i++) {
        createDrone(e, i);
    }

    DEBUG_LOG("placing floating walls");
    placeRandFloatingWalls(e, mapIdx);

    DEBUG_LOG("creating weapon pickups");
    // start spawning pickups in a random quadrant
    e->lastSpawnQuad = randInt(&e->randState, 0, 3);
    for (uint8_t i = 0; i < maps[mapIdx]->weaponPickups; i++) {
        createWeaponPickup(e);
    }

    if (e->client != NULL) {
        setupEnvCamera(e);
        renderEnv(e, true, false, -1, -1);
    }

    computeObs(e);
}

// sets the timing related variables for the environment depending on
// the frame rate
void setEnvFrameRate(iwEnv *e) {
    float frameRate = TRAINING_FRAME_RATE;
    e->box2dSubSteps = TRAINING_BOX2D_SUBSTEPS;
    // set a higher frame rate and physics substeps when evaluating
    // to make it more enjoyable to play
    if (!e->isTraining) {
        frameRate = EVAL_FRAME_RATE;
        e->box2dSubSteps = EVAL_BOX2D_SUBSTEPS;
    }

    e->frameRate = frameRate;
    e->deltaTime = 1.0f / (float)frameRate;
    e->frameSkip = frameRate / TRAINING_ACTIONS_PER_SECOND;

    e->totalSteps = ROUND_STEPS * frameRate;
    e->totalSuddenDeathSteps = SUDDEN_DEATH_STEPS * frameRate;
}

iwEnv *initEnv(iwEnv *e, uint8_t numDrones, uint8_t numAgents, int8_t mapIdx, uint64_t seed, bool enableTeams, bool sittingDuck, bool isTraining, bool continuousActions) {
    DEBUG_LOGF("seed: %lu", seed);

    e->numDrones = numDrones;
    e->numAgents = numAgents;
    e->teamsEnabled = enableTeams;
    e->numTeams = numDrones;
    if (e->teamsEnabled) {
        e->numTeams = 2;
    }
    e->sittingDuck = sittingDuck;
    e->isTraining = isTraining;

    e->obsBytes = obsBytes(e->numDrones);
    e->discreteObsBytes = alignedSize(discreteObsSize(e->numDrones) * sizeof(uint8_t), sizeof(float));

    e->continuousActions = continuousActions;

    // TODO: remove when puffer bindings add truncations
    e->truncations = fastCalloc(numDrones, sizeof(uint8_t));

    setEnvFrameRate(e);
    e->randState = seed;
    e->needsReset = false;

    b2WorldDef worldDef = b2DefaultWorldDef();
    worldDef.gravity = (b2Vec2){.x = 0.0f, .y = 0.0f};
    e->worldID = b2CreateWorld(&worldDef);
    e->pinnedMapIdx = mapIdx;
    e->mapIdx = -1;

    e->idPool = b2CreateIdPool();
    create_array(&e->entities, 128);

    create_array(&e->cells, 512);
    create_array(&e->walls, 128);
    create_array(&e->floatingWalls, MAX_FLOATING_WALLS);
    create_array(&e->drones, e->numDrones);
    create_array(&e->pickups, MAX_WEAPON_PICKUPS);
    create_array(&e->projectiles, 64);
    create_array(&e->explosions, 8);
    create_array(&e->explodingProjectiles, 8);
    create_array(&e->dronePieces, 16);

    e->mapPathing = fastCalloc(NUM_MAPS, sizeof(pathingInfo));
    for (uint8_t i = 0; i < NUM_MAPS; i++) {
        const mapEntry *map = maps[i];
        pathingInfo *info = &e->mapPathing[i];
        info->paths = fastMalloc(map->rows * map->columns * map->rows * map->columns * sizeof(uint8_t));
        memset(info->paths, UINT8_MAX, map->rows * map->columns * map->rows * map->columns * sizeof(uint8_t));
        info->pathBuffer = fastCalloc(3 * 8 * map->rows * map->columns, sizeof(int8_t));
    }

    e->humanInput = false;
    e->humanDroneInput = 0;
    e->connectedControllers = 0;

    return e;
}

void clearEnv(iwEnv *e) {
    // rewards get cleared in stepEnv every step
    // memset(e->masks, 1, e->numAgents * sizeof(uint8_t));
    memset(e->terminals, 0x0, e->numAgents * sizeof(uint8_t));
    memset(e->truncations, 0x0, e->numAgents * sizeof(uint8_t));

    e->episodeLength = 0;
    memset(e->stats, 0x0, sizeof(e->stats));

    for (uint8_t i = 0; i < e->numDrones; i++) {
        droneEntity *drone = safe_array_get_at(e->drones, i);
        destroyDrone(e, drone);
    }

    for (size_t i = 0; i < cc_array_size(e->floatingWalls); i++) {
        wallEntity *wall = safe_array_get_at(e->floatingWalls, i);
        destroyWall(e, wall, false);
    }

    for (size_t i = 0; i < cc_array_size(e->pickups); i++) {
        weaponPickupEntity *pickup = safe_array_get_at(e->pickups, i);
        destroyWeaponPickup(e, pickup);
    }

    for (size_t i = 0; i < cc_array_size(e->projectiles); i++) {
        projectileEntity *p = safe_array_get_at(e->projectiles, i);
        destroyProjectile(e, p, false, false);
    }

    for (size_t i = 0; i < cc_array_size(e->explosions); i++) {
        explosionInfo *explosion = safe_array_get_at(e->explosions, i);
        fastFree(explosion);
    }

    for (size_t i = 0; i < cc_array_size(e->dronePieces); i++) {
        dronePieceEntity *piece = safe_array_get_at(e->dronePieces, i);
        destroyDronePiece(e, piece);
    }

    cc_array_remove_all(e->drones);
    cc_array_remove_all(e->floatingWalls);
    cc_array_remove_all(e->pickups);
    cc_array_remove_all(e->projectiles);
    cc_array_remove_all(e->explodingProjectiles);
    cc_array_remove_all(e->explosions);
    cc_array_remove_all(e->dronePieces);
}

void destroyEnv(iwEnv *e) {
    clearEnv(e);

    for (uint8_t i = 0; i < NUM_MAPS; i++) {
        pathingInfo *info = &e->mapPathing[i];
        fastFree(info->paths);
        fastFree(info->pathBuffer);
    }
    fastFree(e->mapPathing);

    for (size_t i = 0; i < cc_array_size(e->walls); i++) {
        wallEntity *wall = safe_array_get_at(e->walls, i);
        destroyWall(e, wall, false);
    }

    for (size_t i = 0; i < cc_array_size(e->cells); i++) {
        mapCell *cell = safe_array_get_at(e->cells, i);
        fastFree(cell);
    }

    for (size_t i = 0; i < cc_array_size(e->entities); i++) {
        entity *ent = safe_array_get_at(e->entities, i);
        fastFree(ent->id);
        fastFree(ent);
    }
    b2DestroyIdPool(&e->idPool);

    cc_array_destroy(e->entities);
    cc_array_destroy(e->cells);
    cc_array_destroy(e->walls);
    cc_array_destroy(e->drones);
    cc_array_destroy(e->floatingWalls);
    cc_array_destroy(e->pickups);
    cc_array_destroy(e->projectiles);
    cc_array_destroy(e->explosions);
    cc_array_destroy(e->explodingProjectiles);
    cc_array_destroy(e->dronePieces);

    b2DestroyWorld(e->worldID);
}

void resetEnv(iwEnv *e) {
    clearEnv(e);
    setupEnv(e);
}

float computeShotReward(const droneEntity *drone, const weaponInformation *weaponInfo) {
    const float weaponForce = weaponInfo->fireMagnitude * weaponInfo->invMass;
    const float scaledForce = (weaponForce * (weaponForce * SHOT_HIT_REWARD_COEF)) + 0.25f;
    return scaledForce + computeHitStrength(drone);
}

float computeExplosionReward(const droneEntity *drone) {
    return computeHitStrength(drone) * EXPLOSION_HIT_REWARD_COEF;
}

float computeReward(iwEnv *e, droneEntity *drone) {
    float reward = 0.0f;

    if (drone->energyFullyDepleted && drone->energyRefillWait == DRONE_ENERGY_REFILL_EMPTY_WAIT) {
        reward += ENERGY_EMPTY_PUNISHMENT;
    }

    // only reward picking up a weapon if the standard weapon was
    // previously held; every weapon is better than the standard
    // weapon, but other weapons are situational better so don't
    // reward switching a non-standard weapon
    if (drone->stepInfo.pickedUpWeapon && drone->stepInfo.prevWeapon == STANDARD_WEAPON) {
        reward += WEAPON_PICKUP_REWARD;
    }

    for (uint8_t i = 0; i < e->numDrones; i++) {
        if (i == drone->idx) {
            continue;
        }
        droneEntity *enemyDrone = safe_array_get_at(e->drones, i);
        const bool onTeam = drone->team == enemyDrone->team;

        if (drone->stepInfo.shotHit[i] != 0 && !onTeam) {
            // subtract 1 from the weapon type because 1 is added so we
            // can use 0 as no shot was hit
            const weaponInformation *weaponInfo = weaponInfos[drone->stepInfo.shotHit[i] - 1];
            reward += computeShotReward(enemyDrone, weaponInfo);
        }
        if (drone->stepInfo.explosionHit[i] && !onTeam) {
            reward += computeExplosionReward(enemyDrone);
        }

        if (e->numAgents == e->numDrones) {
            if (drone->stepInfo.shotTaken[i] != 0 && !onTeam) {
                const weaponInformation *weaponInfo = weaponInfos[drone->stepInfo.shotTaken[i] - 1];
                reward -= computeShotReward(drone, weaponInfo) * 0.5f;
            }
            if (drone->stepInfo.explosionTaken[i] && !onTeam) {
                reward -= computeExplosionReward(drone) * 0.5f;
            }
        }

        if (enemyDrone->dead && enemyDrone->diedThisStep) {
            if (!onTeam) {
                reward += ENEMY_DEATH_REWARD;
                if (drone->killed[i]) {
                    reward += ENEMY_KILL_REWARD;
                }
            } else {
                reward += TEAMMATE_DEATH_PUNISHMENT;
                if (drone->killed[i]) {
                    reward += TEAMMATE_KILL_PUNISHMENT;
                }
            }
            continue;
        }

        const b2Vec2 enemyDirection = b2Normalize(b2Sub(enemyDrone->pos, drone->pos));
        const float velocityToEnemy = b2Dot(drone->lastVelocity, enemyDirection);
        const float enemyDistance = b2Distance(enemyDrone->pos, drone->pos);
        // stop rewarding approaching an enemy if they're very close
        // to avoid constant clashing; always reward approaching when
        // the current weapon is the shotgun, it greatly benefits from
        // being close to enemies
        if (velocityToEnemy > 0.1f && (drone->weaponInfo->type == SHOTGUN_WEAPON || enemyDistance > DISTANCE_CUTOFF)) {
            reward += APPROACH_REWARD;
        }
    }

    return reward;
}

const float REWARD_EPS = 1.0e-6f;

void computeRewards(iwEnv *e, const bool roundOver, const int8_t winner, const int8_t winningTeam) {
    if (roundOver && winner != -1 && winner < e->numAgents) {
        e->rewards[winner] += WIN_REWARD;
    }

    for (uint8_t i = 0; i < e->numDrones; i++) {
        float reward = 0.0f;
        droneEntity *drone = safe_array_get_at(e->drones, i);
        if (!drone->dead) {
            reward = computeReward(e, drone);
            if (roundOver && winningTeam == drone->team) {
                reward += WIN_REWARD;
            }
        } else if (drone->diedThisStep) {
            reward = DEATH_PUNISHMENT;
            if (drone->killedBy == drone->idx) {
                reward += SELF_KILL_PUNISHMENT;
            }
        }
        if (i < e->numAgents) {
            e->rewards[i] += reward;
        }
        e->stats[i].returns += reward;
    }
}

static inline bool isActionNoop(const b2Vec2 action) {
    return b2Length(action) < ACTION_NOOP_MAGNITUDE;
}

agentActions _computeActions(iwEnv *e, droneEntity *drone, const agentActions *manualActions) {
    agentActions actions = {0};

    const uint8_t offset = drone->idx * CONTINUOUS_ACTION_SIZE;
    if (manualActions == NULL) {
        actions.move = (b2Vec2){.x = e->actions[offset + 0], .y = e->actions[offset + 1]};
        actions.aim = (b2Vec2){.x = e->actions[offset + 2], .y = e->actions[offset + 3]};
        if (e->continuousActions) {
            actions.move.x = tanhf(actions.move.x);
            actions.move.y = tanhf(actions.move.y);
            actions.aim.x = tanhf(actions.aim.x);
            actions.aim.y = tanhf(actions.aim.y);
        }
        actions.chargingWeapon = e->actions[offset + 4] > 0.0f;
        actions.shoot = actions.chargingWeapon;
        if (!actions.chargingWeapon && drone->chargingWeapon) {
            actions.shoot = true;
        }
        actions.brake = e->actions[offset + 5] > 0.0f;
        actions.chargingBurst = e->actions[offset + 6] > 0.0f;
    } else {
        actions.move = manualActions->move;
        actions.aim = manualActions->aim;
        actions.chargingWeapon = manualActions->chargingWeapon;
        actions.shoot = manualActions->shoot;
        actions.brake = manualActions->brake;
        actions.chargingBurst = manualActions->chargingBurst;
        actions.discardWeapon = manualActions->discardWeapon;
    }

    // cap movement magnitude to 1.0
    if (b2Length(actions.move) > 1.0f) {
        actions.move = b2Normalize(actions.move);
    } else if (isActionNoop(actions.move)) {
        actions.move = b2Vec2_zero;
    }

    if (isActionNoop(actions.aim)) {
        actions.aim = b2Vec2_zero;
    } else {
        actions.aim = b2Normalize(actions.aim);
    }

    return actions;
}

agentActions computeActions(iwEnv *e, droneEntity *drone, const agentActions *manualActions) {
    const agentActions actions = _computeActions(e, drone, manualActions);
    drone->lastMove = actions.move;
    if (!b2VecEqual(actions.aim, b2Vec2_zero)) {
        drone->lastAim = actions.aim;
    }
    return actions;
}

void updateConnectedControllers(iwEnv *e) {
    for (uint8_t i = 0; i < e->numDrones; i++) {
        if (IsGamepadAvailable(i)) {
            e->connectedControllers++;
        }
    }
}

void updateHumanInputToggle(iwEnv *e) {
    if (IsKeyPressed(KEY_LEFT_CONTROL)) {
        e->humanInput = !e->humanInput;
        if (!e->humanInput) {
            e->connectedControllers = 0;
        }
    }
    if (e->humanInput && e->connectedControllers == 0) {
        updateConnectedControllers(e);
    }
    if (e->connectedControllers > 1) {
        e->humanDroneInput = e->numDrones - e->connectedControllers;
        return;
    }

    if (IsKeyPressed(KEY_ONE) || IsKeyPressed(KEY_KP_1)) {
        e->humanDroneInput = 0;
    }
    if (IsKeyPressed(KEY_TWO) || IsKeyPressed(KEY_KP_2)) {
        e->humanDroneInput = 1;
    }
    if (e->numDrones >= 3 && (IsKeyPressed(KEY_THREE) || IsKeyPressed(KEY_KP_3))) {
        e->humanDroneInput = 2;
    }
    if (e->numDrones >= 4 && (IsKeyPressed(KEY_FOUR) || IsKeyPressed(KEY_KP_4))) {
        e->humanDroneInput = 3;
    }
}

agentActions getPlayerInputs(iwEnv *e, droneEntity *drone, uint8_t gamepadIdx) {
    if (IsKeyPressed(KEY_R)) {
        e->needsReset = true;
    }

    agentActions actions = {0};

    bool controllerConnected = false;
    if (IsGamepadAvailable(gamepadIdx)) {
        controllerConnected = true;
    }
    if (controllerConnected) {
        float lStickX = GetGamepadAxisMovement(gamepadIdx, GAMEPAD_AXIS_LEFT_X);
        float lStickY = GetGamepadAxisMovement(gamepadIdx, GAMEPAD_AXIS_LEFT_Y);
        float rStickX = GetGamepadAxisMovement(gamepadIdx, GAMEPAD_AXIS_RIGHT_X);
        float rStickY = GetGamepadAxisMovement(gamepadIdx, GAMEPAD_AXIS_RIGHT_Y);

        if (IsGamepadButtonDown(gamepadIdx, GAMEPAD_BUTTON_RIGHT_TRIGGER_2)) {
            actions.chargingWeapon = true;
            actions.shoot = true;
        } else if (drone->chargingWeapon && IsGamepadButtonUp(gamepadIdx, GAMEPAD_BUTTON_RIGHT_TRIGGER_2)) {
            actions.shoot = true;
        }

        if (IsGamepadButtonDown(gamepadIdx, GAMEPAD_BUTTON_LEFT_TRIGGER_2)) {
            actions.brake = true;
        }

        if (IsGamepadButtonDown(gamepadIdx, GAMEPAD_BUTTON_RIGHT_TRIGGER_1) || IsGamepadButtonDown(gamepadIdx, GAMEPAD_BUTTON_RIGHT_FACE_DOWN)) {
            actions.chargingBurst = true;
        }

        if (IsGamepadButtonPressed(gamepadIdx, GAMEPAD_BUTTON_RIGHT_FACE_LEFT)) {
            actions.discardWeapon = true;
        }

        actions.move = (b2Vec2){.x = lStickX, .y = lStickY};
        actions.aim = (b2Vec2){.x = rStickX, .y = rStickY};
        return computeActions(e, drone, &actions);
    }
    if (!controllerConnected && drone->idx != e->humanDroneInput) {
        return actions;
    }

    b2Vec2 move = b2Vec2_zero;
    if (IsKeyDown(KEY_W)) {
        move.y += -1.0f;
    }
    if (IsKeyDown(KEY_S)) {
        move.y += 1.0f;
    }
    if (IsKeyDown(KEY_A)) {
        move.x += -1.0f;
    }
    if (IsKeyDown(KEY_D)) {
        move.x += 1.0f;
    }
    actions.move = b2Normalize(move);

    Vector2 mousePos = (Vector2){.x = (float)GetMouseX(), .y = (float)GetMouseY()};
    actions.aim = b2Normalize(b2Sub(rayVecToB2Vec(e, mousePos), drone->pos));

    if (IsMouseButtonDown(MOUSE_BUTTON_LEFT)) {
        actions.chargingWeapon = true;
        actions.shoot = true;
    } else if (drone->chargingWeapon && IsMouseButtonUp(MOUSE_BUTTON_LEFT)) {
        actions.shoot = true;
    }
    if (IsKeyDown(KEY_SPACE)) {
        actions.brake = true;
    }
    if (IsMouseButtonDown(MOUSE_BUTTON_RIGHT)) {
        actions.chargingBurst = true;
    }

    return computeActions(e, drone, &actions);
}

bool droneControlledByHuman(const iwEnv *e, uint8_t i) {
    if (!e->humanInput) {
        return false;
    }
    return (e->connectedControllers > 1 && i >= e->humanDroneInput) || (e->connectedControllers <= 1 && i == e->humanDroneInput);
}

void addLog(iwEnv *e, Log *log) {
    e->log.length += log->length;
    e->log.ties += log->ties;

    for (uint8_t j = 0; j < e->numDrones; j++) {
        e->log.stats[j].returns += log->stats[j].returns;
        e->log.stats[j].wins += log->stats[j].wins;

        e->log.stats[j].distanceTraveled += log->stats[j].distanceTraveled;
        e->log.stats[j].absDistanceTraveled += log->stats[j].absDistanceTraveled;
        e->log.stats[j].brakeTime += log->stats[j].brakeTime;
        e->log.stats[j].totalBursts += log->stats[j].totalBursts;
        e->log.stats[j].burstsHit += log->stats[j].burstsHit;
        e->log.stats[j].energyEmptied += log->stats[j].energyEmptied;

        for (uint8_t k = 0; k < NUM_WEAPONS; k++) {
            e->log.stats[j].shotsFired[k] += log->stats[j].shotsFired[k];
            e->log.stats[j].shotsHit[k] += log->stats[j].shotsHit[k];
            e->log.stats[j].shotsTaken[k] += log->stats[j].shotsTaken[k];
            e->log.stats[j].ownShotsTaken[k] += log->stats[j].ownShotsTaken[k];
            e->log.stats[j].weaponsPickedUp[k] += log->stats[j].weaponsPickedUp[k];
            e->log.stats[j].shotDistances[k] += log->stats[j].shotDistances[k];
        }

        e->log.stats[j].totalShotsFired += log->stats[j].totalShotsFired;
        e->log.stats[j].totalShotsHit += log->stats[j].totalShotsHit;
        e->log.stats[j].totalShotsTaken += log->stats[j].totalShotsTaken;
        e->log.stats[j].totalOwnShotsTaken += log->stats[j].totalOwnShotsTaken;
        e->log.stats[j].totalWeaponsPickedUp += log->stats[j].totalWeaponsPickedUp;
        e->log.stats[j].totalShotDistances += log->stats[j].totalShotDistances;
    }

    e->log.n += 1.0f;
}

void stepEnv(iwEnv *e) {
    if (e->needsReset) {
        DEBUG_LOG("Resetting environment");
        resetEnv(e);

#ifdef __EMSCRIPTEN__
        lastFrameTime = emscripten_get_now();
        accumulator = 0.0;
#endif
    }

    agentActions stepActions[e->numDrones];
    memset(stepActions, 0x0, e->numDrones * sizeof(agentActions));

    // preprocess agent actions for the next frameSkip steps
    for (uint8_t i = 0; i < e->numDrones; i++) {
        droneEntity *drone = safe_array_get_at(e->drones, i);
        if (drone->dead || droneControlledByHuman(e, i)) {
            continue;
        }

        if (i < e->numAgents) {
            stepActions[i] = computeActions(e, drone, NULL);
        } else {
            const agentActions scriptedActions = scriptedAgentActions(e, drone);
            stepActions[i] = computeActions(e, drone, &scriptedActions);
        }
    }

    // reset reward buffer
    memset(e->rewards, 0x0, e->numAgents * sizeof(float));

    for (int i = 0; i < e->frameSkip; i++) {
#ifdef __EMSCRIPTEN__
        // running at a fixed frame rate doesn't seem to work well in
        // the browser, so we need to adjust to handle a variable frame
        // rate; see https://www.gafferongames.com/post/fix_your_timestep/
        const double curTime = emscripten_get_now();
        const double deltaTime = (curTime - lastFrameTime) / 1000.0;
        lastFrameTime = curTime;

        accumulator += deltaTime;
        while (accumulator >= e->deltaTime) {
            if (e->needsReset) {
                break;
            }
#endif
            e->episodeLength++;

            // handle actions
            if (e->client != NULL) {
                updateHumanInputToggle(e);
            }

            for (uint8_t i = 0; i < e->numDrones; i++) {
                droneEntity *drone = safe_array_get_at(e->drones, i);
                memset(&drone->stepInfo, 0x0, sizeof(droneStepInfo));
                if (drone->dead) {
                    drone->diedThisStep = false;
                }
                memset(&drone->killed, 0x0, sizeof(drone->killed));
            }

            for (uint8_t i = 0; i < e->numDrones; i++) {
                droneEntity *drone = safe_array_get_at(e->drones, i);
                if (drone->dead) {
                    continue;
                }

                agentActions actions;
                // take inputs from humans every frame
                if (droneControlledByHuman(e, i)) {
                    actions = getPlayerInputs(e, drone, i - e->humanDroneInput);
                } else {
                    actions = stepActions[i];
                }

                if (actions.discardWeapon) {
                    droneDiscardWeapon(e, drone);
                }
                if (actions.shoot) {
                    droneShoot(e, drone, actions.aim, actions.chargingWeapon);
                }
                if (actions.chargingBurst) {
                    droneChargeBurst(e, drone);
                } else if (drone->chargingBurst) {
                    droneBurst(e, drone);
                }
                if (!b2VecEqual(actions.move, b2Vec2_zero)) {
                    droneMove(e, drone, actions.move);
                }
                droneBrake(e, drone, actions.brake);

                // update shield velocity if its active
                if (drone->shield != NULL) {
                    b2Body_SetLinearVelocity(drone->shield->bodyID, b2Body_GetLinearVelocity(drone->bodyID));
                }
            }

            b2World_Step(e->worldID, e->deltaTime, e->box2dSubSteps);

            // update dynamic body positions and velocities
            handleBodyMoveEvents(e);

            // handle collisions
            handleContactEvents(e);
            handleSensorEvents(e);

            // handle sudden death
            e->stepsLeft = max(e->stepsLeft - 1, 0);
            if ((!e->isTraining || e->numDrones == e->numAgents) && e->stepsLeft == 0) {
                e->suddenDeathSteps = max(e->suddenDeathSteps - 1, 0);
                if (e->suddenDeathSteps == 0) {
                    DEBUG_LOG("placing sudden death walls");
                    handleSuddenDeath(e);
                    e->suddenDeathSteps = e->totalSuddenDeathSteps;
                }
            }

            projectilesStep(e);

            int8_t lastAlive = -1;
            int8_t lastAliveTeam = -1;
            bool allAliveOnSameTeam = false;
            bool roundOver = false;
            uint8_t deadDrones = 0;
            for (uint8_t i = 0; i < e->numDrones; i++) {
                droneEntity *drone = safe_array_get_at(e->drones, i);
                if (drone->livesLeft != 0) {
                    if (!droneStep(e, drone)) {
                        // couldn't find a respawn position, end the round
                        deadDrones++;
                        roundOver = true;
                    }
                    lastAlive = i;

                    if (e->teamsEnabled) {
                        if (lastAliveTeam == -1) {
                            lastAliveTeam = drone->team;
                            allAliveOnSameTeam = true;
                        } else if (drone->team != lastAliveTeam) {
                            allAliveOnSameTeam = false;
                        }
                    }
                } else {
                    deadDrones++;
                    if (i < e->numAgents) {
                        if (drone->diedThisStep) {
                            e->terminals[i] = 1;
                        }
                        // else {
                        //     e->masks[i] = 0;
                        // }
                    }
                }
            }

            weaponPickupsStep(e);

            if (!roundOver) {
                roundOver = deadDrones >= e->numDrones - 1;
            }
            if (e->teamsEnabled && allAliveOnSameTeam) {
                roundOver = true;
                lastAlive = -1;
            }
            // if the enemy drone(s) are scripted don't enable sudden death
            // so that the agent has to work for victories
            if (e->isTraining && e->numDrones != e->numAgents && e->stepsLeft == 0) {
                roundOver = true;
                lastAliveTeam = -1;
            }
            if (roundOver && deadDrones < e->numDrones - 1) {
                lastAlive = -1;
            }
            computeRewards(e, roundOver, lastAlive, lastAliveTeam);

            if (e->client != NULL) {
                renderEnv(e, false, roundOver, lastAlive, lastAliveTeam);
            }

            if (roundOver) {
                if (e->numDrones != e->numAgents && e->stepsLeft == 0) {
                    DEBUG_LOG("truncating episode");
                    memset(e->truncations, 1, e->numAgents * sizeof(uint8_t));
                } else {
                    DEBUG_LOG("terminating episode");
                    memset(e->terminals, 1, e->numAgents * sizeof(uint8_t));
                }

                Log log = {0};
                log.length = e->episodeLength;
                if (lastAlive != -1) {
                    e->stats[lastAlive].wins = 1.0f;
                } else if (!e->teamsEnabled || (e->teamsEnabled && lastAliveTeam == -1)) {
                    log.ties = 1.0f;
                }

                for (uint8_t i = 0; i < e->numDrones; i++) {
                    const droneEntity *drone = safe_array_get_at(e->drones, i);
                    if (!drone->dead && e->teamsEnabled && drone->team == lastAliveTeam) {
                        e->stats[i].wins = 1.0f;
                    }
                    // set absolute distance traveled of agent drones
                    e->stats[i].absDistanceTraveled = b2Distance(drone->initalPos, drone->pos);
                }

                memcpy(log.stats, e->stats, sizeof(e->stats));
                addLog(e, &log);

                e->needsReset = true;
                break;
            }
#ifdef __EMSCRIPTEN__
            accumulator -= e->deltaTime;
        }

        if (e->needsReset) {
            break;
        }
#endif
    }

#ifndef NDEBUG
    bool gotReward = false;
    for (uint8_t i = 0; i < e->numDrones; i++) {
        if (e->rewards[i] > REWARD_EPS || e->rewards[i] < -REWARD_EPS) {
            gotReward = true;
            break;
        }
    }
    if (gotReward) {
        DEBUG_RAW_LOG("!!! rewards: [");
        for (uint8_t i = 0; i < e->numDrones; i++) {
            const float reward = e->rewards[i];
            DEBUG_RAW_LOGF("%f", reward);
            if (i < e->numDrones - 1) {
                DEBUG_RAW_LOG(", ");
            }
        }
        DEBUG_RAW_LOGF("] step %d\n", e->totalSteps - e->stepsLeft);
    }
#endif

    computeObs(e);
}

#endif



================================================
FILE: pufferlib/ocean/impulse_wars/helpers.h
================================================
#ifndef IMPULSE_WARS_HELPERS_H
#define IMPULSE_WARS_HELPERS_H

#include <signal.h>
#include <stdio.h>
#include <stdlib.h>
#include <time.h>

#include "box2d/box2d.h"

#include "include/cc_array.h"

#ifndef NDEBUG
#define ON_ERROR __builtin_trap()
#define _DEBUG_GET_TIMEINFO() \
    time_t _t = time(NULL);   \
    struct tm *_timeinfo;     \
    _timeinfo = localtime(&_t)
#define DEBUG_RAW_LOG(msg) \
    do {                   \
        printf(msg);       \
        fflush(stdout);    \
    } while (0)
#define DEBUG_RAW_LOGF(fmt, args...) \
    do {                             \
        printf(fmt, args);           \
        fflush(stdout);              \
    } while (0)
#define DEBUG_LOGF(fmt, args...)                                                                                             \
    do {                                                                                                                     \
        _DEBUG_GET_TIMEINFO();                                                                                               \
        printf(fmt " %d:%d:%d %s:%d\n", args, _timeinfo->tm_hour, _timeinfo->tm_min, _timeinfo->tm_sec, __FILE__, __LINE__); \
        fflush(stdout);                                                                                                      \
    } while (0)
#define DEBUG_LOG(msg)                                                                                                 \
    do {                                                                                                               \
        _DEBUG_GET_TIMEINFO();                                                                                         \
        printf(msg " %d:%d:%d %s:%d\n", _timeinfo->tm_hour, _timeinfo->tm_min, _timeinfo->tm_sec, __FILE__, __LINE__); \
        fflush(stdout);                                                                                                \
    } while (0)

#define ASSERT(condition)                                                                  \
    do {                                                                                   \
        if (!(condition)) {                                                                \
            printf("\nASSERTION FAILED: %s at %s:%d\n\n", #condition, __FILE__, __LINE__); \
            fflush(stdout);                                                                \
            ON_ERROR;                                                                      \
        }                                                                                  \
    } while (0)
#define ASSERTF(condition, fmt, args...)                                                                   \
    do {                                                                                                   \
        if (!(condition)) {                                                                                \
            printf("\nASSERTION FAILED: %s; " fmt "; at %s:%d\n\n", #condition, args, __FILE__, __LINE__); \
            fflush(stdout);                                                                                \
            ON_ERROR;                                                                                      \
        }                                                                                                  \
    } while (0)
#else
#define ON_ERROR abort()
#define DEBUG_RAW_LOG(msg)
#define DEBUG_RAW_LOGF(fmt, args...)
#define DEBUG_LOGF(fmt, args...)
#define DEBUG_LOG(msg)
#define ASSERT(condition)
#define ASSERTF(condition, fmt, args...)
#endif

#define ERRORF(fmt, args...)                                             \
    fprintf(stderr, "FATAL: " fmt " %s:%d\n", args, __FILE__, __LINE__); \
    fflush(stderr);                                                      \
    ON_ERROR
#define ERROR(msg)                                                 \
    fprintf(stderr, "FATAL: " msg " %s:%d\n", __FILE__, __LINE__); \
    fflush(stderr);                                                \
    ON_ERROR

// ignore compiler warnings about unused variables for variables that are
// only used in debug builds
#define MAYBE_UNUSED(x) (void)x

#define SQUARED(x) ((x) * (x))

#ifndef PI
#define PI 3.14159265358979323846f
#endif

#ifndef RAD2DEG
#define RAD2DEG (180.0f / PI)
#endif

#define MASS(density, radius) ((density) * PI * (radius) * (radius))
#define INV_MASS(mass) (1.0f / (mass))
// given a mass and radius, calculate the density such that the density
// and radius will result in the same mass as what's given
#define MATCHING_DENSITY(mass, radius) ((mass) / (PI * (radius) * (radius)))

#define ASSERT_VEC_BOUNDED(vec)                  \
    ASSERTF(vec.x <= 1.0f, "vec.x: %f", vec.x);  \
    ASSERTF(vec.x >= -1.0f, "vec.x: %f", vec.x); \
    ASSERTF(vec.y <= 1.0f, "vec.y: %f", vec.y);  \
    ASSERTF(vec.y >= -1.0f, "vec.y: %f", vec.y)

#define ASSERT_VEC_NORMALIZED(vec)                                                                           \
    ASSERT_VEC_BOUNDED(vec);                                                                                 \
    do {                                                                                                     \
        const b2Vec2 norm = b2Normalize(vec);                                                                \
        MAYBE_UNUSED(norm);                                                                                  \
        ASSERTF(fabs(vec.x - norm.x) < 0.000001f, "vec: %f, %f norm: %f, %f", vec.x, vec.y, norm.x, norm.y); \
        ASSERTF(fabs(vec.y - norm.y) < 0.000001f, "vec: %f, %f norm: %f, %f", vec.x, vec.y, norm.x, norm.y); \
    } while (0)

// use malloc when debugging so the address sanitizer can find issues with
// heap memory, use dlmalloc in release mode for performance; emscripten
// uses dlmalloc by default so no need to change anything here; dlmalloc
// sometimes won't compile on macOS so just use malloc and friends
#if !defined(NDEBUG) || defined(__EMSCRIPTEN__) || defined(__APPLE__)
#define fastMalloc(size) malloc(size)
#define fastMallocFn malloc
#define fastCalloc(nmemb, size) calloc(nmemb, size)
#define fastCallocFn calloc
#define fastFree(ptr) free(ptr)
#define fastFreeFn free
#else
#include "include/dlmalloc.h"
#define fastMalloc(size) dlmalloc(size)
#define fastMallocFn dlmalloc
#define fastCalloc(nmemb, size) dlcalloc(nmemb, size)
#define fastCallocFn dlcalloc
#define fastFree(ptr) dlfree(ptr)
#define fastFreeFn dlfree
#endif

static inline void create_array(CC_Array **array, size_t initialCap) {
    CC_ArrayConf conf;
    cc_array_conf_init(&conf);
    conf.capacity = initialCap;
    conf.mem_alloc = fastMallocFn;
    conf.mem_calloc = fastCallocFn;
    conf.mem_free = fastFreeFn;

    cc_array_new_conf(&conf, array);
}

// automatically checks that the index is valid and returns the value
// so callers can use it as a constant expression
static inline void *safe_array_get_at(const CC_Array *const array, size_t index) {
    void *val;
    const enum cc_stat res = cc_array_get_at(array, index, &val);
    ASSERT(res == CC_OK);
    MAYBE_UNUSED(res);
    return val;
}

static inline bool b2VecEqual(const b2Vec2 v1, const b2Vec2 v2) {
    return v1.x == v2.x && v1.y == v2.y;
}

// from https://lemire.me/blog/2019/03/19/the-fastest-conventional-random-number-generator-that-can-pass-big-crush/
// see also https://github.com/lemire/testingRNG
uint64_t wyhash64(uint64_t *state) {
    *state += 0x60bee2bee120fc15;
    __uint128_t tmp;
    tmp = (__uint128_t)(*state) * 0xa3b195354a39b70d;
    uint64_t m1 = (tmp >> 64) ^ tmp;
    tmp = (__uint128_t)m1 * 0x1b03738712fad5c9;
    uint64_t m2 = (tmp >> 64) ^ tmp;
    return m2;
}

static inline float randFloat(uint64_t *state, const float min, const float max) {
    float n = wyhash64(state) / (float)UINT64_MAX;
    return min + n * (max - min);
}

static inline int randInt(uint64_t *state, const int min, const int max) {
    return min + wyhash64(state) % (max - min + 1);
}

static inline float logBasef(const float v, const float b) {
    return log2f(v) / log2(b);
}

#define min(a, b)               \
    ({                          \
        __typeof__(a) _a = (a); \
        __typeof__(b) _b = (b); \
        _a < _b ? _a : _b;      \
    })

#define max(a, b)               \
    ({                          \
        __typeof__(a) _a = (a); \
        __typeof__(b) _b = (b); \
        _a > _b ? _a : _b;      \
    })

// clamps between 0 and 1
static inline float clamp(float f) {
    return min(max(f, 0.0f), 1.0f);
}

// normalize value to be between 0 and max, or -max and max;
// minIsZero determines if the min value is 0 or -max
static inline float scaleValue(const float v, const float max, const bool minIsZero) {
    ASSERTF(v <= max, "v: %f, max: %f", v, max);
    ASSERTF(!minIsZero || v >= 0, "v: %f", v);
    ASSERTF(minIsZero || v >= -max, "v: %f, -max: %f", v, -max);

    float scaled = v / max;
    if (minIsZero) {
        return max(min(scaled, max), 0.0f);
    } else {
        return max(min(scaled, max), -1.0f);
    }
}

static inline uint8_t oneHotEncode(float *obs, const uint16_t offset, const uint8_t val, const uint8_t max) {
    ASSERTF(val < max, "val: %d, max: %d", val, max);
    memset(obs + offset, 0x0, max * sizeof(float));
    obs[offset + val] = 1;
    return max;
}

static inline uint16_t alignedSize(const uint16_t size, const uint8_t align) {
    return (size + align - 1) & ~(align - 1);
}

#define BITNSLOTS(nb) ((nb + sizeof(uint8_t) - 1) / sizeof(uint8_t))

static inline uint16_t bitMask(const uint16_t n) {
    return 1 << (n % sizeof(uint8_t));
}

static inline uint16_t bitSlot(const uint16_t n) {
    return n / sizeof(uint8_t);
}

static inline void bitSet(uint8_t *b, const uint16_t n) {
    b[bitSlot(n)] |= bitMask(n);
}

static inline bool bitTest(const uint8_t *b, const uint16_t n) {
    return b[bitSlot(n)] & bitMask(n);
}

#endif



================================================
FILE: pufferlib/ocean/impulse_wars/impulse_wars.c
================================================
#include "env.h"
#include "render.h"

#ifdef __EMSCRIPTEN__
void emscriptenStep(void *e) {
    stepEnv((iwEnv *)e);
    return;
}
#endif

int main(void) {
    const int NUM_DRONES = 2;

    iwEnv *e = fastCalloc(1, sizeof(iwEnv));

    posix_memalign((void **)&e->observations, sizeof(void *), alignedSize(NUM_DRONES * obsBytes(NUM_DRONES), sizeof(float)));
    e->rewards = fastCalloc(NUM_DRONES, sizeof(float));
    e->actions = fastCalloc(NUM_DRONES * CONTINUOUS_ACTION_SIZE, sizeof(float));
    e->masks = fastCalloc(NUM_DRONES, sizeof(uint8_t));
    e->terminals = fastCalloc(NUM_DRONES, sizeof(uint8_t));
    e->truncations = fastCalloc(NUM_DRONES, sizeof(uint8_t));

    rayClient *client = createRayClient();
    e->client = client;

    initEnv(e, NUM_DRONES, 0, -1, time(NULL), false, false, false, false);
    initMaps(e);
    setupEnv(e);
    // e->humanInput = true;

#ifdef __EMSCRIPTEN__
    lastFrameTime = emscripten_get_now();
    emscripten_set_main_loop_arg(emscriptenStep, e, 0, true);
#else
    while (!WindowShouldClose()) {
        stepEnv(e);
    }

    destroyEnv(e);
    destroyMaps();
    free(e->observations);
    fastFree(e->actions);
    fastFree(e->rewards);
    fastFree(e->masks);
    fastFree(e->terminals);
    fastFree(e->truncations);
    fastFree(e);
    destroyRayClient(client);
#endif
    return 0;
}



================================================
FILE: pufferlib/ocean/impulse_wars/impulse_wars.py
================================================
from types import SimpleNamespace

import gymnasium
import numpy as np

import pufferlib
from pufferlib.ocean.impulse_wars import binding


discMoveToContMove = np.array([
    [1.0, 0.707107, 0.0, -0.707107, -1.0, -0.707107, 0.0, 0.707107, 0.0],
    [0.0, 0.707107, 1.0, 0.707107, 0.0, -0.707107, -1.0, -0.707107, 0.0],
], dtype=np.float32)
discAimToContAim = np.array([
    [1.0, 0.92388, 0.707107, 0.382683, 0.0, -0.382683, -0.707107, -0.92388, -1.0, -0.92388, -0.707107, -0.382683, 0.0, 0.382683, 0.707107, 0.92388, 0.0],
    [0.0, 0.382683, 0.707107, 0.92388, 1.0, 0.92388, 0.707107, 0.382683, 0.0, -0.382683, -0.707107, -0.92388, -1.0, -0.92388, -0.707107, -0.382683, 0.0],
], dtype=np.float32)


class ImpulseWars(pufferlib.PufferEnv):
    def __init__(
        self,
        num_envs: int = 1,
        num_drones: int = 2,
        num_agents: int = 1,
        enable_teams: bool = False,
        sitting_duck: bool = False,
        continuous: bool = False,
        is_training: bool = True,
        human_control: bool = False,
        seed: int = 0,
        render: bool = False,
        report_interval: int = 64,
        buf = None,
    ):
        self.obsInfo = SimpleNamespace(**binding.get_consts(num_drones))

        if num_envs <= 0:
            raise ValueError("num_envs must be greater than 0")
        if num_drones > self.obsInfo.maxDrones or num_drones <= 0:
            raise ValueError(f"num_drones must greater than 0 and less than or equal to {self.obsInfo.maxDrones}")
        if num_agents > num_drones or num_agents <= 0:
            raise ValueError("num_agents must greater than 0 and less than or equal to num_drones")
        if enable_teams and (num_drones % 2 != 0 or num_drones <= 2):
            raise ValueError("enable_teams is only supported for even numbers of drones greater than 2")

        self.numDrones = num_drones
        self.continuous = continuous

        self.num_agents = num_agents * num_envs
        self.tick = 0

        # map observations are bit packed to save space, and scalar
        # observations need to be floats
        self.single_observation_space = gymnasium.spaces.Box(
            low=0, high=255, shape=(self.obsInfo.obsBytes,), dtype=np.uint8
        )

        if self.continuous:
            # action space is actually bounded by (-1, 1) but pufferlib
            # will check that actions are within the bounds of the action
            # space before actions get to the env, and we ensure the actions
            # are bounded there; so set bounds to (-inf, inf) here so
            # action bounds checks pass
            self.single_action_space = gymnasium.spaces.Box(
                low=float("-inf"), high=float("inf"), shape=(self.obsInfo.contActionsSize,), dtype=np.float32
            )
        else:
            self.single_action_space = gymnasium.spaces.MultiDiscrete(
                [
                    9,  # move, noop + 8 directions
                    17,  # aim, noop + 16 directions
                    2,  # shoot or not
                    2,  # brake or not
                    2,  # burst
                ]
            )

        self.report_interval = report_interval
        self.render_mode = "human" if render else None

        super().__init__(buf)
        if not self.continuous:
            self.actions = np.zeros((self.num_agents, self.obsInfo.contActionsSize), dtype=np.float32)

        self.c_envs = binding.vec_init(
            self.observations,
            self.actions,
            self.rewards,
            self.terminals,
            self.truncations,
            num_envs,
            seed,
            num_drones=num_drones,
            num_agents=num_agents,
            map_idx=-1,
            enable_teams=enable_teams,
            sitting_duck=sitting_duck,
            is_training=is_training,
            continuous=continuous,
        )

        binding.shared(self.c_envs)

    def reset(self, seed=None):
        self.tick = 0
        if seed is None:
            binding.vec_reset(self.c_envs, 0)
        else:
            binding.vec_reset(self.c_envs, seed)
        return self.observations, []

    def step(self, actions):
        if self.continuous:
            self.actions[:] = actions
        else:
            contMove = discMoveToContMove[:, actions[:, 0]].T
            contAim =  discAimToContAim[:, actions[:, 1]].T
            contRest = actions[:, 2:].astype(np.float32)
            self.actions[:] = np.concatenate([contMove, contAim, contRest], axis=1)

        self.tick += 1    
        binding.vec_step(self.c_envs)

        infos = []
        if self.tick % self.report_interval == 0:
            infos.append(binding.vec_log(self.c_envs))

        return self.observations, self.rewards, self.terminals, self.truncations, infos

    def render(self):
        binding.vec_render(self.c_envs, 0)

    def close(self):
        binding.vec_close(self.c_envs)


def testPerf(timeout, actionCache, numEnvs):
    env = ImpulseWars(numEnvs)

    import time

    np.random.seed(int(time.time()))
    actions = np.random.uniform(-1, 1, (actionCache, env.num_agents, 7))

    tick = 0
    start = time.time()
    while time.time() - start < timeout:
        action = actions[tick % actionCache]
        env.step(action)
        tick += 1

    sps = numEnvs * (tick / (time.time() - start))
    print(f"SPS: {sps:,}")
    print(f"Steps: {numEnvs * tick}")

    env.close()


if __name__ == "__main__":
    testPerf(timeout=5, actionCache=1024, numEnvs=1)



================================================
FILE: pufferlib/ocean/impulse_wars/Makefile
================================================
RELEASE_PYTHON_MODULE_DIR := python-module-release
DEBUG_PYTHON_MODULE_DIR := python-module-debug
DEBUG_DIR := debug-demo
RELEASE_DIR := release-demo
RELEASE_WEB_DIR := release-demo-web
BENCHMARK_DIR := benchmark

DEBUG_BUILD_TYPE := Debug
RELEASE_BUILD_TYPE := Release

# install build dependencies if this is a fresh build, Python won't
# install build dependencies when --no-build-isolation is passed
# build with no isolation so that builds can be cached and/or incremental

# build Python module in release mode
.PHONY: python-module-release
python-module-release:
	@test -d $(RELEASE_PYTHON_MODULE_DIR) || pip install scikit-build-core autopxd2 cython
	@pip install --no-build-isolation --config-settings=editable.rebuild=true -Cbuild-dir=$(RELEASE_PYTHON_MODULE_DIR) -v .

# build Python module in debug mode
.PHONY: python-module-debug
python-module-debug:
	@test -d $(DEBUG_PYTHON_MODULE_DIR) || pip install scikit-build-core autopxd2 cython
	@pip install --no-build-isolation --config-settings=editable.rebuild=true --config-settings=cmake.build-type="Debug" -Cbuild-dir=$(DEBUG_PYTHON_MODULE_DIR) -v .	

# build C demo in debug mode
.PHONY: debug-demo
debug-demo:
	@mkdir -p $(DEBUG_DIR)
	@cd $(DEBUG_DIR) && \
	cmake -GNinja -DCMAKE_BUILD_TYPE=$(DEBUG_BUILD_TYPE) -DBUILD_DEMO=true -DCMAKE_C_COMPILER=clang-20 .. && \
	cmake --build .

# build C demo in release mode
.PHONY: release-demo
release-demo:
	@mkdir -p $(RELEASE_DIR)
	@cd $(RELEASE_DIR) && \
	cmake -GNinja -DCMAKE_BUILD_TYPE=$(RELEASE_BUILD_TYPE) -DBUILD_DEMO=true -DCMAKE_C_COMPILER=clang-20 .. && \
	cmake --build .

# build C demo in release mode for web
.PHONY: release-demo-web
release-demo-web:
	@mkdir -p $(RELEASE_WEB_DIR)
	@cd $(RELEASE_WEB_DIR) && \
	emcmake cmake -GNinja -DCMAKE_BUILD_TYPE=$(RELEASE_BUILD_TYPE) -DPLATFORM=Web -DBUILD_DEMO=true .. && \
	cmake --build .

# build C benchmark
.PHONY: benchmark
benchmark:
	@mkdir -p $(BENCHMARK_DIR)
	@cd $(BENCHMARK_DIR) && \
	cmake -GNinja -DCMAKE_BUILD_TYPE=$(RELEASE_BUILD_TYPE) -DBUILD_BENCHMARK=true -DCMAKE_C_COMPILER=clang-20 .. && \
	cmake --build .

.PHONY: clean
clean:
	@rm -rf build $(RELEASE_PYTHON_MODULE_DIR) $(DEBUG_PYTHON_MODULE_DIR) $(DEBUG_DIR) $(RELEASE_DIR) $(RELEASE_WEB_DIR) $(BENCHMARK_DIR)



================================================
FILE: pufferlib/ocean/impulse_wars/map.h
================================================
#ifndef IMPULSE_WARS_MAP_H
#define IMPULSE_WARS_MAP_H

#include <errno.h>
#include <string.h>

#include "env.h"
#include "settings.h"

// clang-format off

const char boringLayout[] = {
    'D','D','D','D','D','D','D','D','D','D','D','D','D','D','D','D','D','D','D','D','D',
    'D','O','O','O','O','O','O','O','O','O','O','O','O','O','O','O','O','O','O','O','D',
    'D','O','O','O','O','O','O','O','O','O','O','O','O','O','O','O','O','O','O','O','D',
    'D','O','O','O','O','O','O','O','O','O','O','O','O','O','O','O','O','O','O','O','D',
    'D','O','O','O','O','O','O','O','O','O','O','O','O','O','O','O','O','O','O','O','D',
    'D','O','O','O','O','O','O','O','O','O','O','O','O','O','O','O','O','O','O','O','D',
    'D','O','O','O','O','O','O','O','O','O','O','O','O','O','O','O','O','O','O','O','D',
    'D','O','O','O','O','O','O','O','O','O','O','O','O','O','O','O','O','O','O','O','D',
    'D','O','O','O','O','O','O','O','O','O','O','O','O','O','O','O','O','O','O','O','D',
    'D','O','O','O','O','O','O','O','O','O','O','O','O','O','O','O','O','O','O','O','D',
    'D','O','O','O','O','O','O','O','O','O','O','O','O','O','O','O','O','O','O','O','D',
    'D','O','O','O','O','O','O','O','O','O','O','O','O','O','O','O','O','O','O','O','D',
    'D','O','O','O','O','O','O','O','O','O','O','O','O','O','O','O','O','O','O','O','D',
    'D','O','O','O','O','O','O','O','O','O','O','O','O','O','O','O','O','O','O','O','D',
    'D','O','O','O','O','O','O','O','O','O','O','O','O','O','O','O','O','O','O','O','D',
    'D','O','O','O','O','O','O','O','O','O','O','O','O','O','O','O','O','O','O','O','D',
    'D','O','O','O','O','O','O','O','O','O','O','O','O','O','O','O','O','O','O','O','D',
    'D','O','O','O','O','O','O','O','O','O','O','O','O','O','O','O','O','O','O','O','D',
    'D','O','O','O','O','O','O','O','O','O','O','O','O','O','O','O','O','O','O','O','D',
    'D','O','O','O','O','O','O','O','O','O','O','O','O','O','O','O','O','O','O','O','D',
    'D','D','D','D','D','D','D','D','D','D','D','D','D','D','D','D','D','D','D','D','D',
};

mapEntry boringMap = {
    .layout = boringLayout,
    .columns = 21,
    .rows = 21,
    .randFloatingStandardWalls = 0,
    .randFloatingBouncyWalls = 0,
    .randFloatingDeathWalls = 0,
    .hasSetFloatingWalls = false,
    .weaponPickups = 8,
    .defaultWeapon = STANDARD_WEAPON,
    .maxSuddenDeathWalls = 5,
};

const char prototypeArenaLayout[] = {
    'D','D','D','D','D','D','D','D','D','D','D','D','D','D','D','D','D','D','D','D',
    'D','O','O','O','O','O','O','O','d','O','O','O','O','O','O','O','O','O','O','D',
    'D','O','w','O','O','O','O','O','O','O','O','O','O','O','O','O','O','d','O','D',
    'D','O','O','O','O','O','O','O','O','O','O','O','O','O','O','O','O','O','O','D',
    'D','O','O','O','O','O','O','O','O','O','O','O','O','O','O','O','O','O','O','D',
    'D','O','O','O','O','O','O','O','O','O','O','O','O','O','w','O','O','O','O','D',
    'D','O','O','O','O','O','O','O','O','O','O','O','O','O','O','O','O','O','O','D',
    'D','O','O','O','O','O','O','O','O','W','W','O','O','O','O','O','O','O','O','D',
    'D','O','O','O','O','O','O','D','D','W','W','D','O','O','O','O','O','O','O','D',
    'D','O','O','O','O','d','O','D','D','D','D','D','O','O','O','O','O','O','O','D',
    'D','O','w','O','O','O','O','D','D','D','D','D','O','O','O','O','w','O','O','D',
    'D','O','O','O','O','O','O','D','D','D','D','D','O','O','O','O','O','O','O','D',
    'D','O','O','O','O','O','O','O','W','W','O','O','O','d','O','O','O','O','O','D',
    'D','O','O','O','O','O','O','O','O','O','O','O','O','O','O','O','O','O','O','D',
    'D','O','O','O','O','O','O','O','O','O','O','O','O','O','O','O','O','O','O','D',
    'D','O','O','O','O','O','O','O','O','O','O','O','O','O','O','O','O','O','O','D',
    'D','O','O','O','O','O','O','O','O','O','O','O','O','O','O','O','w','O','O','D',
    'D','O','O','O','w','O','O','O','O','O','O','O','O','O','O','O','O','O','O','D',
    'D','O','d','O','O','O','O','O','O','O','O','O','O','O','O','O','O','O','d','D',
    'D','D','D','D','D','D','D','D','D','D','D','D','D','D','D','D','D','D','D','D',
};

mapEntry prototypeArenaMap = {
    .layout = prototypeArenaLayout,
    .columns = 20,
    .rows = 20,
    .randFloatingStandardWalls = 0,
    .randFloatingBouncyWalls = 0,
    .randFloatingDeathWalls = 0,
    .hasSetFloatingWalls = true,
    .weaponPickups = 6,
    .defaultWeapon = STANDARD_WEAPON,
    .maxSuddenDeathWalls = 4,
};

const char snipersLayout[] = {
    'B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B',
    'B','D','D','O','O','O','O','O','O','O','O','O','O','O','O','O','O','O','D','D','B',
    'B','D','D','O','O','O','O','O','O','O','O','O','O','O','O','O','O','O','D','D','B',
    'B','O','O','O','O','O','O','O','O','O','O','O','O','O','O','O','O','O','O','O','B',
    'B','O','O','O','O','O','O','O','O','O','O','O','O','O','O','O','O','O','O','O','B',
    'B','O','O','O','O','O','O','O','O','O','O','O','O','O','O','O','O','O','O','O','B',
    'B','O','O','O','O','O','O','D','D','B','O','B','D','D','O','O','O','O','O','O','B',
    'B','O','O','O','O','O','D','D','D','B','O','B','D','D','D','O','O','O','O','O','B',
    'B','O','O','O','O','O','D','D','D','B','O','B','D','D','D','O','O','O','O','O','B',
    'B','O','O','O','O','O','B','B','B','B','O','B','B','B','B','O','O','O','O','O','B',
    'B','O','O','O','O','O','O','O','O','O','O','O','O','O','O','O','O','O','O','O','B',
    'B','O','O','O','O','O','B','B','B','B','O','B','B','B','B','O','O','O','O','O','B',
    'B','O','O','O','O','O','D','D','D','B','O','B','D','D','D','O','O','O','O','O','B',
    'B','O','O','O','O','O','D','D','D','B','O','B','D','D','D','O','O','O','O','O','B',
    'B','O','O','O','O','O','O','D','D','B','O','B','D','D','O','O','O','O','O','O','B',
    'B','O','O','O','O','O','O','O','O','O','O','O','O','O','O','O','O','O','O','O','B',
    'B','O','O','O','O','O','O','O','O','O','O','O','O','O','O','O','O','O','O','O','B',
    'B','O','O','O','O','O','O','O','O','O','O','O','O','O','O','O','O','O','O','O','B',
    'B','D','D','O','O','O','O','O','O','O','O','O','O','O','O','O','O','O','D','D','B',
    'B','D','D','O','O','O','O','O','O','O','O','O','O','O','O','O','O','O','D','D','B',
    'B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B',
};

mapEntry snipersMap = {
    .layout = snipersLayout,
    .columns = 21,
    .rows = 21,
    .randFloatingStandardWalls = 0,
    .randFloatingBouncyWalls = 0,
    .randFloatingDeathWalls = 0,
    .hasSetFloatingWalls = false,
    .weaponPickups = 6,
    .defaultWeapon = SNIPER_WEAPON,
    .maxSuddenDeathWalls = 4,
};

const char roomsLayout[] = {
    'D','D','D','D','D','D','D','D','D','D','D','D','D','D','D','D','D','D','D','D','D',
    'D','O','O','O','O','O','O','O','O','O','D','O','O','O','O','O','O','O','O','O','D',
    'D','O','O','O','O','O','O','O','O','O','D','O','O','O','O','O','O','O','O','O','D',
    'D','O','O','O','O','O','O','O','O','O','W','O','O','O','O','O','O','O','O','O','D',
    'D','O','O','O','O','O','O','O','O','O','O','O','O','O','O','O','O','O','O','O','D',
    'D','O','O','O','O','O','O','O','O','O','O','O','O','O','O','O','O','O','O','O','D',
    'D','O','O','O','O','O','O','O','O','O','O','O','O','O','O','O','O','O','O','O','D',
    'D','O','O','O','O','O','O','O','O','O','W','O','O','O','O','O','O','O','O','O','D',
    'D','O','O','O','O','O','O','O','O','O','D','O','O','O','O','O','O','O','O','O','D',
    'D','O','O','O','O','O','O','O','O','O','D','O','O','O','O','O','O','O','O','O','D',
    'D','D','D','W','O','O','O','W','D','D','D','D','D','W','O','O','O','W','D','D','D',
    'D','O','O','O','O','O','O','O','O','O','D','O','O','O','O','O','O','O','O','O','D',
    'D','O','O','O','O','O','O','O','O','O','D','O','O','O','O','O','O','O','O','O','D',
    'D','O','O','O','O','O','O','O','O','O','W','O','O','O','O','O','O','O','O','O','D',
    'D','O','O','O','O','O','O','O','O','O','O','O','O','O','O','O','O','O','O','O','D',
    'D','O','O','O','O','O','O','O','O','O','O','O','O','O','O','O','O','O','O','O','D',
    'D','O','O','O','O','O','O','O','O','O','O','O','O','O','O','O','O','O','O','O','D',
    'D','O','O','O','O','O','O','O','O','O','W','O','O','O','O','O','O','O','O','O','D',
    'D','O','O','O','O','O','O','O','O','O','D','O','O','O','O','O','O','O','O','O','D',
    'D','O','O','O','O','O','O','O','O','O','D','O','O','O','O','O','O','O','O','O','D',
    'D','D','D','D','D','D','D','D','D','D','D','D','D','D','D','D','D','D','D','D','D',
};

mapEntry roomsMap = {
    .layout = roomsLayout,
    .columns = 21,
    .rows = 21,
    .randFloatingStandardWalls = 3,
    .randFloatingBouncyWalls = 0,
    .randFloatingDeathWalls = 3,
    .hasSetFloatingWalls = false,
    .weaponPickups = 10,
    .defaultWeapon = STANDARD_WEAPON,
    .maxSuddenDeathWalls = 5,
};

const char xArenaLayout[] = {
    'D','D','D','D','D','D','D','D','D','D','D','D','D','D','D','D','D','D','D','D','D','D','D',
    'D','O','O','O','O','O','O','O','O','O','O','O','O','O','O','O','O','O','O','O','O','O','D',
    'D','O','O','O','O','O','O','O','O','O','d','O','O','O','O','O','O','O','O','O','d','O','D',
    'D','O','w','O','O','O','O','O','O','O','O','O','O','O','O','O','O','O','O','O','O','O','D',
    'D','O','O','d','O','O','O','O','O','O','O','O','O','O','O','O','O','O','O','O','O','O','D',
    'D','O','O','O','O','O','O','O','O','O','W','W','O','O','O','O','O','O','O','O','O','O','D',
    'D','O','O','O','O','O','O','O','O','D','W','W','D','D','O','O','O','O','O','O','O','O','D',
    'D','O','O','O','O','O','O','O','O','O','D','D','D','O','O','O','O','O','O','O','O','O','D',
    'D','O','O','O','O','O','O','O','O','O','O','D','O','O','O','O','O','O','O','O','O','O','D',
    'D','O','O','O','O','O','D','O','O','O','O','O','O','O','O','O','D','O','O','O','O','O','D',
    'D','O','O','O','O','O','D','D','O','O','w','O','O','O','O','D','W','W','O','O','O','O','D',
    'D','O','O','O','O','W','W','D','D','O','O','O','O','O','D','D','W','W','O','O','w','O','D',
    'D','O','w','O','O','W','W','D','O','O','O','O','d','O','O','D','D','O','O','O','O','O','D',
    'D','O','O','O','O','O','D','O','O','O','O','O','O','O','O','O','D','O','O','O','O','O','D',
    'D','O','O','O','O','O','O','O','O','O','O','D','O','O','O','O','O','O','O','O','O','O','D',
    'D','O','O','O','O','O','O','O','O','O','D','D','D','O','O','O','O','O','O','O','O','O','D',
    'D','O','O','O','O','O','O','O','O','D','D','W','W','D','O','O','O','O','O','O','O','O','D',
    'D','O','O','O','O','O','O','O','O','O','O','W','W','O','O','O','O','O','O','O','O','O','D',
    'D','O','O','O','O','O','O','O','O','O','O','O','O','O','O','O','O','w','O','O','O','O','D',
    'D','O','O','O','O','O','O','O','O','O','O','O','O','O','O','O','O','O','O','O','O','O','D',
    'D','O','O','O','w','O','O','O','O','O','O','O','O','O','O','O','O','O','O','O','O','d','D',
    'D','O','d','O','O','O','O','O','O','O','O','O','O','O','O','O','O','O','O','O','O','O','D',
    'D','D','D','D','D','D','D','D','D','D','D','D','D','D','D','D','D','D','D','D','D','D','D',
};

mapEntry xArena = {
    .layout = xArenaLayout,
    .columns = 23,
    .rows = 23,
    .randFloatingStandardWalls = 0,
    .randFloatingBouncyWalls = 0,
    .randFloatingDeathWalls = 0,
    .hasSetFloatingWalls = true,
    .weaponPickups = 8,
    .defaultWeapon = STANDARD_WEAPON,
    .maxSuddenDeathWalls = 6,
};

const char crossBounceLayout[] = {
    'D','D','D','D','D','D','D','D','D','D','D','D','D','D','D','D','D','D','D','D','D','D','D','D',
    'D','B','B','B','B','O','O','O','O','B','D','D','D','D','B','O','O','O','O','B','B','B','B','D',
    'D','B','O','O','O','O','O','O','O','O','O','O','O','O','O','O','O','O','O','O','O','O','B','D',
    'D','B','O','O','O','O','O','O','O','O','O','O','O','O','O','O','O','O','O','O','O','O','B','D',
    'D','B','O','O','B','B','O','O','O','O','O','w','d','O','O','O','O','O','B','B','O','O','B','D',
    'D','O','O','O','B','D','D','O','O','O','O','O','O','O','O','O','O','D','D','B','O','O','O','D',
    'D','O','O','O','O','D','O','O','O','O','O','O','O','O','O','O','O','O','D','O','O','O','O','D',
    'D','O','O','O','O','O','O','O','O','O','O','O','O','O','O','O','O','O','O','O','O','O','O','D',
    'D','O','O','O','O','O','O','O','O','D','B','O','O','B','D','O','O','O','O','O','O','O','O','D',
    'D','B','O','O','O','O','O','O','D','D','B','O','O','B','D','D','O','O','O','O','O','O','B','D',
    'D','D','O','O','O','O','O','O','B','B','B','O','O','B','B','B','O','O','O','O','O','O','D','D',
    'D','D','O','O','d','O','O','O','O','O','O','O','O','O','O','O','O','O','O','w','O','O','D','D',
    'D','D','O','O','w','O','O','O','O','O','O','O','O','O','O','O','O','O','O','d','O','O','D','D',
    'D','D','O','O','O','O','O','O','B','B','B','O','O','B','B','B','O','O','O','O','O','O','D','D',
    'D','B','O','O','O','O','O','O','D','D','B','O','O','B','D','D','O','O','O','O','O','O','B','D',
    'D','O','O','O','O','O','O','O','O','D','B','O','O','B','D','O','O','O','O','O','O','O','O','D',
    'D','O','O','O','O','O','O','O','O','O','O','O','O','O','O','O','O','O','O','O','O','O','O','D',
    'D','O','O','O','O','D','O','O','O','O','O','O','O','O','O','O','O','O','D','O','O','O','O','D',
    'D','O','O','O','B','D','D','O','O','O','O','O','O','O','O','O','O','D','D','B','O','O','O','D',
    'D','B','O','O','B','B','O','O','O','O','O','d','w','O','O','O','O','O','B','B','O','O','B','D',
    'D','B','O','O','O','O','O','O','O','O','O','O','O','O','O','O','O','O','O','O','O','O','B','D',
    'D','B','O','O','O','O','O','O','O','O','O','O','O','O','O','O','O','O','O','O','O','O','B','D',
    'D','B','B','B','B','O','O','O','O','B','D','D','D','D','B','O','O','O','O','B','B','B','B','D',
    'D','D','D','D','D','D','D','D','D','D','D','D','D','D','D','D','D','D','D','D','D','D','D','D',
};

mapEntry crossBounce = {
    .layout = crossBounceLayout,
    .columns = 24,
    .rows = 24,
    .randFloatingStandardWalls = 0,
    .randFloatingBouncyWalls = 0,
    .randFloatingDeathWalls = 0,
    .hasSetFloatingWalls = true,
    .weaponPickups = 8,
    .defaultWeapon = STANDARD_WEAPON,// TODO: make this exploding weapon
    .maxSuddenDeathWalls = 6,
};

const char asteriskArenaLayout[]= {
    'D','D','D','D','D','D','D','D','D','D','D','D','D','D','D','D','D','D','D','D','D','D','D',
    'D','O','O','O','O','O','O','O','O','O','O','O','O','O','O','O','O','O','O','O','O','O','D',
    'D','O','O','O','O','O','O','O','O','O','O','O','O','O','O','O','O','O','O','O','O','O','D',
    'D','O','O','O','O','O','O','O','O','O','O','O','O','O','O','O','O','O','O','O','O','O','D',
    'D','O','O','O','O','O','O','O','O','O','O','O','O','O','O','O','O','O','O','O','O','O','D',
    'D','O','O','O','O','O','O','O','D','W','O','O','O','W','D','O','O','O','O','O','O','O','D',
    'D','O','O','O','O','O','O','O','O','D','O','O','O','D','O','O','O','O','O','O','O','O','D',
    'D','O','O','O','O','O','O','O','O','O','O','O','O','O','O','O','O','O','O','O','O','O','D',
    'D','O','O','O','O','D','D','O','O','O','O','O','O','O','O','O','D','D','O','O','O','O','D',
    'D','O','O','O','O','W','W','D','O','O','O','O','O','O','O','D','W','W','O','O','O','O','D',
    'D','O','O','O','O','O','O','O','O','O','O','O','O','O','O','O','O','O','O','O','O','O','D',
    'D','O','O','O','O','O','O','O','O','O','O','D','O','O','O','O','O','O','O','O','O','O','D',
    'D','O','O','O','O','O','O','O','O','O','O','O','O','O','O','O','O','O','O','O','O','O','D',
    'D','O','O','O','O','W','W','D','O','O','O','O','O','O','O','D','W','W','O','O','O','O','D',
    'D','O','O','O','O','D','D','O','O','O','O','O','O','O','O','O','D','D','O','O','O','O','D',
    'D','O','O','O','O','O','O','O','O','O','O','O','O','O','O','O','O','O','O','O','O','O','D',
    'D','O','O','O','O','O','O','O','O','D','O','O','O','D','O','O','O','O','O','O','O','O','D',
    'D','O','O','O','O','O','O','O','D','W','O','O','O','W','D','O','O','O','O','O','O','O','D',
    'D','O','O','O','O','O','O','O','O','O','O','O','O','O','O','O','O','O','O','O','O','O','D',
    'D','O','O','O','O','O','O','O','O','O','O','O','O','O','O','O','O','O','O','O','O','O','D',
    'D','O','O','O','O','O','O','O','O','O','O','O','O','O','O','O','O','O','O','O','O','O','D',
    'D','O','O','O','O','O','O','O','O','O','O','O','O','O','O','O','O','O','O','O','O','O','D',
    'D','D','D','D','D','D','D','D','D','D','D','D','D','D','D','D','D','D','D','D','D','D','D',
};

mapEntry asteriskArena = {
    .layout = asteriskArenaLayout,
    .columns = 23,
    .rows = 23,
    .randFloatingStandardWalls = 0,
    .randFloatingBouncyWalls = 0,
    .randFloatingDeathWalls = 0,
    .hasSetFloatingWalls = false,
    .weaponPickups = 8,
    .defaultWeapon = STANDARD_WEAPON,
    .maxSuddenDeathWalls = 7,
};

const char foamPitLayout[] = {
    'B','B','B','W','W','W','D','D','D','B','B','D','D','D','W','W','W','B','B','B',
    'B','O','O','O','O','O','O','O','D','B','B','D','O','O','O','O','O','O','O','B',
    'B','O','O','O','O','O','O','O','O','B','B','O','O','O','O','O','O','O','O','B',
    'W','O','O','d','O','O','O','O','O','O','O','O','O','O','O','O','d','O','O','W',
    'W','O','O','O','O','O','O','O','O','O','O','O','O','O','O','O','O','O','O','W',
    'W','O','O','O','O','O','O','O','O','O','O','O','O','O','O','O','O','O','O','W',
    'D','O','O','O','O','O','O','O','O','O','O','O','O','O','O','O','O','O','O','D',
    'D','O','O','O','O','O','O','d','O','O','O','O','d','O','O','O','O','O','O','D',
    'D','D','O','O','O','O','O','O','O','O','O','O','O','O','O','O','O','O','D','D',
    'B','B','B','O','O','O','O','O','O','d','d','O','O','O','O','O','O','B','B','B',
    'B','B','B','O','O','O','O','O','O','d','d','O','O','O','O','O','O','B','B','B',
    'D','D','O','O','O','O','O','O','O','O','O','O','O','O','O','O','O','O','D','D',
    'D','O','O','O','O','O','O','d','O','O','O','O','d','O','O','O','O','O','O','D',
    'D','O','O','O','O','O','O','O','O','O','O','O','O','O','O','O','O','O','O','D',
    'W','O','O','O','O','O','O','O','O','O','O','O','O','O','O','O','O','O','O','W',
    'W','O','O','O','O','O','O','O','O','O','O','O','O','O','O','O','O','O','O','W',
    'W','O','O','d','O','O','O','O','O','O','O','O','O','O','O','O','d','O','O','W',
    'B','O','O','O','O','O','O','O','O','B','B','O','O','O','O','O','O','O','O','B',
    'B','O','O','O','O','O','O','O','D','B','B','D','O','O','O','O','O','O','O','B',
    'B','B','B','W','W','W','D','D','D','B','B','D','D','D','W','W','W','B','B','B',
};

mapEntry foamPitMap = {
    .layout = foamPitLayout,
    .columns = 20,
    .rows = 20,
    .randFloatingStandardWalls = 0,
    .randFloatingBouncyWalls = 0,
    .randFloatingDeathWalls = 0,
    .hasSetFloatingWalls = true,
    .weaponPickups = 6,
    .defaultWeapon = STANDARD_WEAPON,
    .maxSuddenDeathWalls = 5,
};

const char siegeLayout[] = {
    'B','B','B','W','W','W','W','W','D','D','D','D','D','D','D','D','D','W','W','W','W','W','B','B','B',
    'B','O','O','O','O','O','O','O','O','O','O','O','O','O','O','O','O','O','O','O','O','O','O','O','B',
    'B','O','d','O','O','O','O','O','O','O','O','O','O','O','O','O','O','O','O','O','O','O','d','O','B',
    'W','O','O','O','O','O','O','O','O','O','O','O','O','O','O','O','O','O','O','O','O','O','O','O','W',
    'W','W','O','O','O','W','D','D','W','W','O','O','O','O','O','W','W','D','D','W','O','O','O','W','W',
    'W','O','O','O','O','O','D','O','O','O','O','O','O','O','O','O','O','O','D','O','O','O','O','O','W',
    'W','O','O','O','b','O','W','O','b','O','O','O','O','O','O','O','b','O','W','O','b','O','O','O','W',
    'W','O','b','O','O','O','W','O','O','O','O','O','O','O','O','O','O','O','W','O','O','O','b','O','W',
    'W','O','O','O','O','O','W','O','O','O','O','B','B','B','O','O','O','O','W','O','O','O','O','O','W',
    'W','W','O','O','O','W','W','O','O','O','O','O','O','O','O','O','O','O','W','W','O','O','O','W','W',
    'D','O','O','O','O','O','O','O','O','O','O','O','O','O','O','O','O','O','O','O','O','O','O','O','D',
    'D','O','O','O','O','O','O','O','O','O','O','O','b','O','O','O','O','O','O','O','O','O','O','O','D',
    'D','O','O','O','O','O','O','O','O','O','O','O','O','O','O','O','O','O','O','O','O','O','O','O','D',
    'D','O','O','O','O','O','O','O','O','O','O','O','O','O','O','O','O','O','O','O','O','O','O','O','D',
    'W','W','O','O','O','W','W','O','O','O','O','O','O','O','O','O','O','O','W','W','O','O','O','W','W',
    'W','O','O','O','O','O','W','O','O','O','O','B','B','B','O','O','O','O','W','O','O','O','O','O','W',
    'W','O','b','O','O','O','W','O','O','O','O','O','O','O','O','O','O','O','W','O','O','O','b','O','W',
    'W','O','O','O','b','O','W','O','b','O','O','O','O','O','O','O','b','O','W','O','b','O','O','O','W',
    'W','O','O','O','O','O','D','O','O','O','O','O','O','O','O','O','O','O','D','O','O','O','O','O','W',
    'W','W','O','O','O','W','D','D','W','W','O','O','O','O','O','W','W','D','D','W','O','O','O','W','W',
    'W','O','O','O','O','O','O','O','O','O','O','O','O','O','O','O','O','O','O','O','O','O','O','O','W',
    'B','O','d','O','O','O','O','O','O','O','O','O','O','O','O','O','O','O','O','O','O','O','d','O','B',
    'B','O','O','O','O','O','O','O','O','O','O','O','O','O','O','O','O','O','O','O','O','O','O','O','B',
    'B','B','B','W','W','W','W','W','D','D','D','D','D','D','D','D','D','W','W','W','W','W','B','B','B',
};

mapEntry siegeMap = {
    .layout = siegeLayout,
    .columns = 25,
    .rows = 24,
    .randFloatingStandardWalls = 0,
    .randFloatingBouncyWalls = 0,
    .randFloatingDeathWalls = 0,
    .hasSetFloatingWalls = true,
    .weaponPickups = 5,
    .defaultWeapon = STANDARD_WEAPON,
    .maxSuddenDeathWalls = 8,
};

// clang-format on

mapEntry *maps[] = {
    &boringMap,
    &prototypeArenaMap,
    &snipersMap,
    &roomsMap,
    &xArena,
    &crossBounce,
    &asteriskArena,
    &foamPitMap,
    &siegeMap,
};

void resetMap(iwEnv *e) {
    // if sudden death walls were placed, remove them
    if (e->suddenDeathWallsPlaced) {
        e->suddenDeathWallsPlaced = false;
        DEBUG_LOG("removing sudden death walls");
        // remove walls from the end of the array, sudden death walls
        // are added last
        for (int16_t i = cc_array_size(e->walls) - 1; i >= 0; i--) {
            wallEntity *wall = safe_array_get_at(e->walls, i);
            if (!wall->isSuddenDeath) {
                // if we reached the first non sudden death wall, we're done
                break;
            }
            cc_array_remove_last(e->walls, NULL);
            destroyWall(e, wall, true);
        }
    }

    // place floating walls with a set position if there are any
    const mapEntry *map = maps[e->mapIdx];
    if (!map->hasSetFloatingWalls) {
        return;
    }
    DEBUG_LOG("placing set floating walls");

    const uint8_t columns = map->columns;
    const uint8_t rows = map->rows;
    const char *layout = map->layout;
    uint16_t cellIdx = 0;

    for (int row = 0; row < rows; row++) {
        for (int col = 0; col < columns; col++) {
            char cellType = layout[col + (row * columns)];

            enum entityType wallType;
            switch (cellType) {
            case 'w':
                wallType = STANDARD_WALL_ENTITY;
                break;
            case 'b':
                wallType = BOUNCY_WALL_ENTITY;
                break;
            case 'd':
                wallType = DEATH_WALL_ENTITY;
                break;
            default:
                cellIdx++;
                continue;
            }

            const mapCell *cell = safe_array_get_at(e->cells, cellIdx);
            createWall(e, cell->pos, FLOATING_WALL_THICKNESS, FLOATING_WALL_THICKNESS, cellIdx, wallType, true);
            cellIdx++;
        }
    }
}

void setupMap(iwEnv *e, const uint8_t mapIdx) {
    // reset the map if we're switching to the same map
    if (e->mapIdx == mapIdx) {
        resetMap(e);
        return;
    }

    // clear the old map
    for (size_t i = 0; i < cc_array_size(e->walls); i++) {
        wallEntity *wall = safe_array_get_at(e->walls, i);
        destroyWall(e, wall, false);
    }

    for (size_t i = 0; i < cc_array_size(e->cells); i++) {
        mapCell *cell = safe_array_get_at(e->cells, i);
        fastFree(cell);
    }

    cc_array_remove_all(e->walls);
    cc_array_remove_all(e->cells);
    e->suddenDeathWallsPlaced = false;

    const uint8_t columns = maps[mapIdx]->columns;
    const uint8_t rows = maps[mapIdx]->rows;
    const char *layout = maps[mapIdx]->layout;

    e->mapIdx = mapIdx;
    e->map = maps[mapIdx];
    e->defaultWeapon = weaponInfos[maps[mapIdx]->defaultWeapon];
    if (e->isTraining && randFloat(&e->randState, 0.0f, 1.0f) < 0.25f) {
        e->defaultWeapon = weaponInfos[randInt(&e->randState, 0, NUM_WEAPONS - 1)];
    }

    uint16_t cellIdx = 0;
    for (int row = 0; row < rows; row++) {
        for (int col = 0; col < columns; col++) {
            char cellType = layout[col + (row * columns)];
            enum entityType wallType;
            const float x = (col - ((columns - 1) * 0.5f)) * WALL_THICKNESS;
            const float y = (row - (rows - 1) * 0.5f) * WALL_THICKNESS;

            b2Vec2 pos = {.x = x, .y = y};
            mapCell *cell = fastCalloc(1, sizeof(mapCell));
            cell->ent = NULL;
            cell->pos = pos;
            cc_array_add(e->cells, cell);

            bool floating = false;
            float thickness = WALL_THICKNESS;
            switch (cellType) {
            case 'O':
                cellIdx++;
                continue;
            case 'w':
                thickness = FLOATING_WALL_THICKNESS;
                floating = true;
            case 'W':
                wallType = STANDARD_WALL_ENTITY;
                break;
            case 'b':
                thickness = FLOATING_WALL_THICKNESS;
                floating = true;
            case 'B':
                wallType = BOUNCY_WALL_ENTITY;
                break;
            case 'd':
                thickness = FLOATING_WALL_THICKNESS;
                floating = true;
            case 'D':
                wallType = DEATH_WALL_ENTITY;
                break;
            default:
                ERRORF("unknown map layout cell %c", cellType);
            }

            entity *ent = createWall(e, pos, thickness, thickness, cellIdx, wallType, floating);
            if (!floating) {
                cell->ent = ent;
            }
            cellIdx++;
        }
    }
}

void computeMapBoundsAndQuadrants(iwEnv *e, mapEntry *map) {
    mapBounds bounds = {.min = {.x = FLT_MAX, .y = FLT_MAX}, .max = {.x = FLT_MIN, .y = FLT_MIN}};
    for (size_t i = 0; i < cc_array_size(e->walls); i++) {
        const wallEntity *wall = safe_array_get_at(e->walls, i);
        bounds.min.x = min(wall->pos.x - wall->extent.x + WALL_THICKNESS, bounds.min.x);
        bounds.min.y = min(wall->pos.y - wall->extent.y + WALL_THICKNESS, bounds.min.y);
        bounds.max.x = max(wall->pos.x + wall->extent.x - WALL_THICKNESS, bounds.max.x);
        bounds.max.y = max(wall->pos.y + wall->extent.y - WALL_THICKNESS, bounds.max.y);
    }
    map->bounds = bounds;
    map->spawnQuads[0] = (mapBounds){
        .min = (b2Vec2){
            .x = map->bounds.min.x + WALL_THICKNESS,
            .y = map->bounds.min.y + WALL_THICKNESS,
        },
        .max = (b2Vec2){
            .x = 0.0f,
            .y = 0.0f,
        }
    };
    map->spawnQuads[1] = (mapBounds){
        .min = (b2Vec2){
            .x = 0.0f,
            .y = map->bounds.min.y + WALL_THICKNESS,
        },
        .max = (b2Vec2){
            .x = map->bounds.max.x - WALL_THICKNESS,
            .y = 0.0f,
        }
    };
    map->spawnQuads[2] = (mapBounds){
        .min = (b2Vec2){
            .x = map->bounds.min.x + WALL_THICKNESS,
            .y = 0.0f,
        },
        .max = (b2Vec2){
            .x = 0.0f,
            .y = map->bounds.max.y - WALL_THICKNESS,
        }
    };
    map->spawnQuads[3] = (mapBounds){
        .min = (b2Vec2){
            .x = 0.0f,
            .y = 0.0f,
        },
        .max = (b2Vec2){
            .x = map->bounds.max.x - WALL_THICKNESS,
            .y = map->bounds.max.y - WALL_THICKNESS,
        }
    };
}

bool posValidDroneSpawnPoint(const iwEnv *e, const b2Vec2 pos) {
    const b2QueryFilter filter = {
        .categoryBits = DRONE_SHAPE,
        .maskBits = WALL_SHAPE | FLOATING_WALL_SHAPE,
    };
    droneEntity dummyDrone = {.pos = pos};
    const entity ent = {.type = DRONE_ENTITY, .entity = &dummyDrone};
    const enum entityType deathWallType = DEATH_WALL_ENTITY;

    if (isOverlappingCircleInLineOfSight(e, &ent, pos, DRONE_DEATH_WALL_SPAWN_DISTANCE, filter, &deathWallType)) {
        return false;
    }
    if (isOverlappingAABB(e, pos, DRONE_WALL_SPAWN_DISTANCE, filter)) {
        return false;
    }

    return true;
}

void initMaps(iwEnv *e) {
    for (uint8_t i = 0; i < NUM_MAPS; i++) {
        setupMap(e, i);
        mapEntry *map = maps[i];

        computeMapBoundsAndQuadrants(e, map);

        bool *droneSpawns = fastCalloc(map->columns * map->rows, sizeof(bool));
        uint8_t *packedLayout = fastCalloc(map->columns * map->rows, sizeof(uint8_t));
        nearEntity *nearestWalls = fastCalloc(MAX_NEAREST_WALLS * map->columns * map->rows, sizeof(nearEntity));

        for (uint16_t i = 0; i < cc_array_size(e->cells); i++) {
            const mapCell *cell = safe_array_get_at(e->cells, i);

            // precompute packed map layout
            if (cell->ent != NULL) {
                packedLayout[i] = ((cell->ent->type + 1) & TWO_BIT_MASK) << 5;
                continue;
            } else {
                // precompute valid cells for drones to spawn
                droneSpawns[i] = posValidDroneSpawnPoint(e, cell->pos);
            }

            // find nearest walls for each empty cell
            uint16_t wallIdx = 0;
            nearEntity walls[map->columns * map->rows];
            memset(walls, 0x0, map->columns * map->rows * sizeof(nearEntity));
            for (uint16_t j = 0; j < cc_array_size(e->cells); j++) {
                const mapCell *c = safe_array_get_at(e->cells, j);
                if (c->ent == NULL) {
                    continue;
                }

                walls[wallIdx].idx = wallIdx;
                walls[wallIdx].distanceSquared = b2DistanceSquared(cell->pos, c->pos);
                wallIdx++;
            }
            insertionSort(walls, wallIdx);

            const uint32_t startIdx = i * MAX_NEAREST_WALLS;
            memcpy(nearestWalls + startIdx, walls, MAX_NEAREST_WALLS * sizeof(nearEntity));
        }
        map->droneSpawns = droneSpawns;
        map->packedLayout = packedLayout;
        map->nearestWalls = nearestWalls;

        // clear floating walls from the map
        for (uint8_t i = 0; i < cc_array_size(e->floatingWalls); i++) {
            wallEntity *wall = safe_array_get_at(e->floatingWalls, i);
            destroyWall(e, wall, false);
        }
        cc_array_remove_all(e->floatingWalls);
    }

    e->mapIdx = -1;
}

void destroyMaps() {
    for (uint8_t i = 0; i < NUM_MAPS; i++) {
        mapEntry *map = maps[i];
        fastFree(map->droneSpawns);
        fastFree(map->packedLayout);
        fastFree(map->nearestWalls);
    }
}

void placeRandFloatingWall(iwEnv *e, const enum entityType wallType) {
    b2Vec2 pos;
    if (!findOpenPos(e, FLOATING_WALL_SHAPE, &pos, -1)) {
        ERROR("failed to find open position for floating wall");
    }
    int16_t cellIdx = entityPosToCellIdx(e, pos);
    createWall(e, pos, FLOATING_WALL_THICKNESS, FLOATING_WALL_THICKNESS, cellIdx, wallType, true);
}

void placeRandFloatingWalls(iwEnv *e, const int mapIdx) {
    for (int i = 0; i < maps[mapIdx]->randFloatingStandardWalls; i++) {
        placeRandFloatingWall(e, STANDARD_WALL_ENTITY);
    }
    for (int i = 0; i < maps[mapIdx]->randFloatingBouncyWalls; i++) {
        placeRandFloatingWall(e, BOUNCY_WALL_ENTITY);
    }
    for (int i = 0; i < maps[mapIdx]->randFloatingDeathWalls; i++) {
        placeRandFloatingWall(e, DEATH_WALL_ENTITY);
    }
}

#endif



================================================
FILE: pufferlib/ocean/impulse_wars/pyproject.toml
================================================
[build-system]
requires = ["scikit-build-core>=0.10", "autopxd2>=2.5.0", "cython>=3.0.11"]
build-backend = "scikit_build_core.build"

[project]
name = "binding"
version = "1.0.0"
requires-python = ">=3.11"

[tool.scikit-build]
minimum-version = "build-system.requires"
cmake.build-type = "Release"
build.verbose = true
logging.level = "INFO"

[tool.scikit-build.cmake.define]
BUILD_PYTHON_MODULE = true
CMAKE_C_COMPILER = "clang-20"

[tool.ruff]
line-length = 110

[tool.ruff.lint]
# skip "Module level import not at top of file"
ignore = ["E402"]



================================================
FILE: pufferlib/ocean/impulse_wars/scripted_agent.h
================================================
#ifndef IMPULSE_WARS_SCRIPTED_BOT_H
#define IMPULSE_WARS_SCRIPTED_BOT_H

#include "game.h"
#include "types.h"

const uint8_t NUM_NEAR_WALLS = 3;
const uint8_t NUM_NEAR_PICKUPS = 1;

const float WALL_CHECK_DISTANCE_SQUARED = SQUARED(6.0f);
const float WALL_AVOID_DISTANCE = 4.0f;
const float WALL_DANGER_DISTANCE = 3.0f;
const float WALL_BRAKE_DISTANCE = 20.0f;
const float WALL_BRAKE_SPEED = 12.5f;
const float WALL_BRAKE_TIME = 0.5f;
const float BURST_MIN_RADIUS_SQUARED = SQUARED(DRONE_BURST_RADIUS_MIN);
const float MOVE_SPEED_SQUARED = SQUARED(5.0f);

typedef struct castCircleCtx {
    bool hit;
    b2ShapeId shapeID;
} castCircleCtx;

float castCircleCallback(b2ShapeId shapeId, b2Vec2 point, b2Vec2 normal, float fraction, void *context) {
    // these parameters are required by the callback signature
    MAYBE_UNUSED(point);
    MAYBE_UNUSED(normal);
    if (!b2Shape_IsValid(shapeId) || (fraction == 0.0f && b2VecEqual(normal, b2Vec2_zero))) {
        // skip this shape if it isn't valid or this is an initial overlap
        return -1.0f;
    }

    castCircleCtx *ctx = context;
    ctx->hit = true;
    ctx->shapeID = shapeId;

    return 0.0f;
}

static inline uint32_t pathOffset(const iwEnv *e, uint16_t srcCellIdx, uint16_t destCellIdx) {
    const uint8_t srcCol = srcCellIdx % e->map->columns;
    const uint8_t srcRow = srcCellIdx / e->map->columns;
    const uint8_t destCol = destCellIdx % e->map->columns;
    const uint8_t destRow = destCellIdx / e->map->columns;
    return (destRow * e->map->rows * e->map->columns * e->map->rows) + (destCol * e->map->rows * e->map->columns) + (srcRow * e->map->columns) + srcCol;
}

void pathfindBFS(const iwEnv *e, uint8_t *flatPaths, uint16_t destCellIdx) {
    uint8_t (*paths)[e->map->columns] = (uint8_t (*)[e->map->columns])flatPaths;
    int8_t (*buffer)[3] = (int8_t (*)[3])e->mapPathing[e->mapIdx].pathBuffer;

    uint16_t start = 0;
    uint16_t end = 1;

    const mapCell *cell = safe_array_get_at(e->cells, destCellIdx);
    if (cell->ent != NULL && entityTypeIsWall(cell->ent->type)) {
        return;
    }
    const int8_t destCol = destCellIdx % e->map->columns;
    const int8_t destRow = destCellIdx / e->map->columns;

    buffer[start][0] = 8;
    buffer[start][1] = destCol;
    buffer[start][2] = destRow;
    while (start < end) {
        const int8_t direction = buffer[start][0];
        const int8_t startCol = buffer[start][1];
        const int8_t startRow = buffer[start][2];
        start++;

        if (startCol < 0 || startCol >= e->map->columns || startRow < 0 || startRow >= e->map->rows || paths[startRow][startCol] != UINT8_MAX) {
            continue;
        }
        int16_t cellIdx = cellIndex(e, startCol, startRow);
        const mapCell *cell = safe_array_get_at(e->cells, cellIdx);
        if (cell->ent != NULL && entityTypeIsWall(cell->ent->type)) {
            paths[startRow][startCol] = 8;
            continue;
        }

        paths[startRow][startCol] = direction;

        buffer[end][0] = 6; // up
        buffer[end][1] = startCol;
        buffer[end][2] = startRow + 1;
        end++;

        buffer[end][0] = 2; // down
        buffer[end][1] = startCol;
        buffer[end][2] = startRow - 1;
        end++;

        buffer[end][0] = 0; // right
        buffer[end][1] = startCol - 1;
        buffer[end][2] = startRow;
        end++;

        buffer[end][0] = 4; // left
        buffer[end][1] = startCol + 1;
        buffer[end][2] = startRow;
        end++;

        buffer[end][0] = 5; // up left
        buffer[end][1] = startCol + 1;
        buffer[end][2] = startRow + 1;
        end++;

        buffer[end][0] = 3; // down left
        buffer[end][1] = startCol + 1;
        buffer[end][2] = startRow - 1;
        end++;

        buffer[end][0] = 1; // down right
        buffer[end][1] = startCol - 1;
        buffer[end][2] = startRow - 1;
        end++;

        buffer[end][0] = 7; // up right
        buffer[end][1] = startCol - 1;
        buffer[end][2] = startRow + 1;
        end++;
    }
}

float distanceWithDamping(const iwEnv *e, const droneEntity *drone, const b2Vec2 direction, const float linearDamping, const float steps) {
    float speed = drone->weaponInfo->recoilMagnitude * DRONE_INV_MASS;
    if (!b2VecEqual(drone->velocity, b2Vec2_zero)) {
        speed = b2Length(b2MulAdd(drone->velocity, speed, direction));
    }

    const float damping = 1.0f + linearDamping * e->deltaTime;
    return speed * (damping / linearDamping) * (1.0f - powf(1.0f / damping, steps));
}

bool safeToFire(iwEnv *e, const droneEntity *drone, const b2Vec2 direction) {
    float shotWait;
    if (drone->ammo > 1) {
        shotWait = ((drone->weaponInfo->coolDown + drone->weaponInfo->charge) / e->deltaTime) * 1.5f;
    } else {
        shotWait = ((e->defaultWeapon->coolDown + e->defaultWeapon->charge) / e->deltaTime) * 1.5f;
    }
    const b2Vec2 invDirection = b2MulSV(-1.0f, direction);
    const float recoilDistance = distanceWithDamping(e, drone, invDirection, DRONE_LINEAR_DAMPING, shotWait);

    // e->debugPoint = b2MulAdd(drone->pos, recoilDistance, invDirection);

    const b2Vec2 pos = drone->pos;
    const b2Vec2 rayEnd = b2MulAdd(pos, recoilDistance, invDirection);
    const b2Vec2 translation = b2Sub(rayEnd, pos);
    const b2ShapeProxy cirProxy = b2MakeProxy(&pos, 1, DRONE_RADIUS);
    const b2QueryFilter filter = {.categoryBits = DRONE_SHAPE, .maskBits = WALL_SHAPE | FLOATING_WALL_SHAPE | DRONE_SHAPE};

    castCircleCtx ctx = {0};
    b2World_CastShape(e->worldID, &cirProxy, translation, filter, castCircleCallback, &ctx);
    if (!ctx.hit) {
        return true;
    } else {
        const entity *ent = b2Shape_GetUserData(ctx.shapeID);
        if (ent->type == STANDARD_WALL_ENTITY || ent->type == BOUNCY_WALL_ENTITY || ent->type == DRONE_ENTITY) {
            return true;
        }
    }

    return false;
}

bool weaponSafeForMovement(const droneEntity *drone) {
    switch (drone->weaponInfo->type) {
    case IMPLODER_WEAPON:
    case MINE_LAUNCHER_WEAPON:
        return false;
    default:
        return true;
    }
}

void scriptedAgentShoot(const droneEntity *drone, agentActions *actions) {
    actions->shoot = true;
    if (drone->chargingWeapon && drone->weaponCharge == drone->weaponInfo->charge) {
        actions->chargingWeapon = false;
    }
}

void moveTo(iwEnv *e, const droneEntity *drone, agentActions *actions, const b2Vec2 dstPos) {
    ASSERT(drone->mapCellIdx != -1);
    int16_t dstIdx = entityPosToCellIdx(e, dstPos);
    if (dstIdx == -1) {
        return;
    }

    uint32_t pathIdx = pathOffset(e, drone->mapCellIdx, dstIdx);
    uint8_t *paths = e->mapPathing[e->mapIdx].paths;
    uint8_t direction = paths[pathIdx];
    if (direction == UINT8_MAX) {
        uint32_t bfsIdx = pathOffset(e, 0, dstIdx);
        pathfindBFS(e, &paths[bfsIdx], dstIdx);
        direction = paths[pathIdx];
    }
    if (direction >= 8) {
        return;
    }
    actions->move.x += discMoveToContMoveMap[0][direction];
    actions->move.y += discMoveToContMoveMap[1][direction];
    actions->move = b2Normalize(actions->move);

    const b2Vec2 invDirection = b2MulSV(-1.0f, actions->move);
    if (!weaponSafeForMovement(drone) || !safeToFire(e, drone, invDirection)) {
        return;
    }
    actions->aim = invDirection;
    scriptedAgentShoot(drone, actions);
}

float weaponIdealRangeSquared(const droneEntity *drone) {
    switch (drone->weaponInfo->type) {
    case STANDARD_WEAPON:
        return SQUARED(20.0f);
    case MACHINEGUN_WEAPON:
        return SQUARED(30.0f);
    case SHOTGUN_WEAPON:
        return SQUARED(20.0f);
    case IMPLODER_WEAPON:
        return SQUARED(30.0f);
    case FLAK_CANNON_WEAPON:
        return SQUARED(FLAK_CANNON_SAFE_DISTANCE + 5.0f);
    case SNIPER_WEAPON:
    case ACCELERATOR_WEAPON:
    case MINE_LAUNCHER_WEAPON:
    case BLACK_HOLE_WEAPON:
    case NUKE_WEAPON:
        return FLT_MAX;

    default:
        ERRORF("unknown weapon type %d", drone->weaponInfo->type);
    }
}

bool shouldShootAtEnemy(iwEnv *e, const droneEntity *drone, const droneEntity *enemyDrone, const b2Vec2 enemyDroneDirection) {
    if (!safeToFire(e, drone, enemyDroneDirection)) {
        return false;
    }

    // cast a circle that's the size of a projectile of the current weapon
    const float enemyDroneDistance = b2Distance(enemyDrone->pos, drone->pos);
    const b2Vec2 castEnd = b2MulAdd(drone->pos, enemyDroneDistance, enemyDroneDirection);
    const b2Vec2 translation = b2Sub(castEnd, drone->pos);
    const b2ShapeProxy cirProxy = b2MakeProxy(&drone->pos, 1, drone->weaponInfo->radius);
    const b2QueryFilter filter = {.categoryBits = PROJECTILE_SHAPE, .maskBits = WALL_SHAPE | FLOATING_WALL_SHAPE | DRONE_SHAPE};

    castCircleCtx ctx = {0};
    b2World_CastShape(e->worldID, &cirProxy, translation, filter, castCircleCallback, &ctx);
    if (!ctx.hit) {
        return false;
    }
    ASSERT(b2Shape_IsValid(ctx.shapeID));
    const entity *ent = b2Shape_GetUserData(ctx.shapeID);
    if (ent == NULL || ent->type != DRONE_ENTITY) {
        return false;
    }

    return true;
}

b2Vec2 predictiveAim(const droneEntity *drone, const droneEntity *enemyDrone, const float distanceSquared) {
    const float timeToImpact = sqrtf(distanceSquared) / drone->weaponInfo->initialSpeed;
    const b2Vec2 predictedPos = b2MulAdd(enemyDrone->pos, timeToImpact, enemyDrone->velocity);
    return b2Normalize(b2Sub(predictedPos, drone->pos));
}

void handleWallProximity(iwEnv *e, const droneEntity *drone, const wallEntity *wall, const float distance, agentActions *actions) {
    if (distance > WALL_BRAKE_DISTANCE) {
        return;
    }

    const b2Vec2 wallDirection = b2Normalize(b2Sub(wall->pos, drone->pos));
    const float speedToWall = b2Dot(drone->velocity, wallDirection);
    if (speedToWall > WALL_BRAKE_SPEED) {
        float damping = DRONE_LINEAR_DAMPING;
        if (drone->braking) {
            damping *= DRONE_BRAKE_DAMPING_COEF;
        }
        const float travelDistance = distanceWithDamping(e, drone, wallDirection, damping, WALL_BRAKE_TIME / e->deltaTime);
        if (travelDistance >= distance) {
            actions->brake = true;
        }
    }
    if (actions->brake || distance <= WALL_AVOID_DISTANCE) {
        const b2Vec2 invWallDirection = b2MulSV(-1.0f, wallDirection);
        actions->move = b2MulAdd(actions->move, distance, invWallDirection);
    }

    if (distance > WALL_DANGER_DISTANCE) {
        return;
    }

    // shoot to move away faster from a death wall if we're too close and it's safe
    if (weaponSafeForMovement(drone) && safeToFire(e, drone, wallDirection)) {
        actions->aim = wallDirection;
        scriptedAgentShoot(drone, actions);
    }
}

void scriptedAgentBurst(const droneEntity *drone, agentActions *actions) {
    if (drone->chargingBurst) {
        return;
    } else {
        actions->chargingBurst = true;
    }
}

agentActions scriptedAgentActions(iwEnv *e, droneEntity *drone) {
    agentActions actions = {0};
    if (e->sittingDuck) {
        return actions;
    }

    // keep the weapon charged and ready if it needs it
    if (drone->weaponInfo->charge != 0.0f) {
        actions.chargingWeapon = true;
        actions.shoot = true;
    }

    // find the nearest death wall or floating wall
    nearEntity nearWalls[MAX_NEAREST_WALLS] = {0};
    findNearWalls(e, drone, nearWalls, NUM_NEAR_WALLS);

    // find the distance between the closest points on the drone and the nearest wall
    for (uint8_t i = 0; i < NUM_NEAR_WALLS; i++) {
        const wallEntity *wall = nearWalls[i].entity;
        if (wall->type != DEATH_WALL_ENTITY) {
            continue;
        }

        const b2DistanceOutput output = closestPoint(drone->ent, wall->ent);
        handleWallProximity(e, drone, wall, output.distance, &actions);
    }

    for (uint8_t i = 0; i < cc_array_size(e->floatingWalls); i++) {
        wallEntity *floatingWall = safe_array_get_at(e->floatingWalls, i);
        if (floatingWall->type != DEATH_WALL_ENTITY) {
            continue;
        }
        if (b2DistanceSquared(floatingWall->pos, drone->pos) > WALL_CHECK_DISTANCE_SQUARED) {
            continue;
        }

        const b2DistanceOutput output = closestPoint(drone->ent, floatingWall->ent);
        handleWallProximity(e, drone, floatingWall, output.distance, &actions);
    }

    // get a weapon if the standard weapon is active
    if (drone->weaponInfo->type == STANDARD_WEAPON && cc_array_size(e->pickups) != 0) {
        nearEntity nearPickups[MAX_WEAPON_PICKUPS] = {0};
        uint8_t numActivePickups = 0;
        for (uint8_t i = 0; i < cc_array_size(e->pickups); i++) {
            weaponPickupEntity *pickup = safe_array_get_at(e->pickups, i);
            if (pickup->floatingWallsTouching > 0) {
                continue;
            }
            const nearEntity nearEnt = {
                .entity = pickup,
                .distanceSquared = b2DistanceSquared(pickup->pos, drone->pos),
            };
            nearPickups[numActivePickups++] = nearEnt;
        }
        if (numActivePickups > 0) {
            insertionSort(nearPickups, numActivePickups);
            const weaponPickupEntity *pickup = nearPickups[0].entity;
            moveTo(e, drone, &actions, pickup->pos);
            return actions;
        }
    }

    // find closest enemy drone
    droneEntity *enemyDrone = NULL;
    float closestDistanceSquared = FLT_MAX;
    for (uint8_t i = 0; i < cc_array_size(e->drones); i++) {
        if (i == drone->idx) {
            continue;
        }
        droneEntity *otherDrone = safe_array_get_at(e->drones, i);
        if (otherDrone->dead || otherDrone->team == drone->team) {
            continue;
        }
        const float distanceSquared = b2DistanceSquared(otherDrone->pos, drone->pos);
        if (distanceSquared < closestDistanceSquared) {
            closestDistanceSquared = distanceSquared;
            enemyDrone = otherDrone;
        }
    }
    if (enemyDrone == NULL) {
        return actions;
    }

    // if we're close enough to a wall to need to shoot at it, don't
    // worry about enemies
    if (!b2VecEqual(actions.aim, b2Vec2_zero)) {
        return actions;
    }

    // burst if the enemy drone is very close
    if (closestDistanceSquared <= BURST_MIN_RADIUS_SQUARED) {
        scriptedAgentBurst(drone, &actions);
    }
    // move into ideal range for the current weapon
    if (closestDistanceSquared > weaponIdealRangeSquared(drone)) {
        moveTo(e, drone, &actions, enemyDrone->pos);
        return actions;
    }

    // shoot at enemy drone if it's in line of sight and safe, otherwise move towards it
    const b2Vec2 enemyDroneDirection = b2Normalize(b2Sub(enemyDrone->pos, drone->pos));
    if (shouldShootAtEnemy(e, drone, enemyDrone, enemyDroneDirection)) {
        if (drone->weaponCooldown == 0.0f && drone->weaponCharge >= drone->weaponInfo->charge - e->deltaTime) {
            actions.move.x += enemyDroneDirection.x;
            actions.move.y += enemyDroneDirection.y;
            actions.move = b2Normalize(actions.move);
        }
        actions.aim = predictiveAim(drone, enemyDrone, closestDistanceSquared);
        scriptedAgentShoot(drone, &actions);
    } else {
        moveTo(e, drone, &actions, enemyDrone->pos);
    }

    // fight recoil if we're not otherwise moving
    if (b2VecEqual(actions.move, b2Vec2_zero)) {
        const float speedSquared = b2LengthSquared(drone->velocity);
        if (speedSquared > MOVE_SPEED_SQUARED) {
            actions.move = b2MulSV(-1.0f, b2Normalize(drone->velocity));
        }
    }

    return actions;
}

#endif



================================================
FILE: pufferlib/ocean/impulse_wars/settings.h
================================================
#ifndef IMPULSE_WARS_SETTINGS_H
#define IMPULSE_WARS_SETTINGS_H

#include "helpers.h"
#include "types.h"

#define INFINITE -1
const uint8_t TWO_BIT_MASK = 0x3;

// general settings
const uint8_t TRAINING_ACTIONS_PER_SECOND = 10;
const uint8_t TRAINING_FRAME_RATE = 30;
const uint8_t TRAINING_BOX2D_SUBSTEPS = 2;

const uint8_t EVAL_FRAME_RATE = 120;
const uint8_t EVAL_BOX2D_SUBSTEPS = 4;

const uint8_t NUM_MAPS = 9;
#define _MAX_MAP_COLUMNS 25
#define _MAX_MAP_ROWS 25
#define MAX_CELLS _MAX_MAP_COLUMNS *_MAX_MAP_ROWS + 1
#define MAX_FLOATING_WALLS 18
#define MAX_WEAPON_PICKUPS 12

#define MAX_NEAREST_WALLS 8

const uint8_t DRONE_LIVES = 1;
const float DRONE_RESPAWN_WAIT = 2.0f;
const uint8_t ROUND_STEPS = 90;
const uint8_t SUDDEN_DEATH_STEPS = 5;

const uint8_t MAX_DRONES = _MAX_DRONES;

const uint16_t LOG_BUFFER_SIZE = 1024;

// reward settings
const float WIN_REWARD = 1.5f;
const float SELF_KILL_PUNISHMENT = -2.0f;
const float ENEMY_DEATH_REWARD = 1.0f;
const float ENEMY_KILL_REWARD = 1.0f;
const float TEAMMATE_DEATH_PUNISHMENT = -0.5f;
const float TEAMMATE_KILL_PUNISHMENT = -1.0f;
const float DEATH_PUNISHMENT = 0.0f;
const float ENERGY_EMPTY_PUNISHMENT = -0.75f;
const float WEAPON_PICKUP_REWARD = 0.5f;
const float SHOT_HIT_REWARD_COEF = 0.000013333f;
const float EXPLOSION_HIT_REWARD_COEF = 5.0f;
const float APPROACH_REWARD = 0.0f;

// approach reward doesn't apply within the cutoff to avoid constant clashing
const uint8_t DISTANCE_CUTOFF = 15.0f;

// observation constants

// map layout observations
const uint8_t MAP_OBS_ROWS = 11;
const uint8_t MAP_OBS_COLUMNS = 11;
const uint16_t MAP_OBS_SIZE = MAP_OBS_ROWS * MAP_OBS_COLUMNS;

// discrete observations
#define _NUM_NEAR_WALL_OBS 4
const uint8_t NUM_NEAR_WALL_OBS = _NUM_NEAR_WALL_OBS;
const uint16_t NEAR_WALL_TYPES_OBS_OFFSET = MAP_OBS_SIZE;

#define _NUM_FLOATING_WALL_OBS 4
const uint8_t NUM_FLOATING_WALL_OBS = _NUM_FLOATING_WALL_OBS;
const uint16_t FLOATING_WALL_TYPES_OBS_OFFSET = NEAR_WALL_TYPES_OBS_OFFSET + NUM_NEAR_WALL_OBS;

const uint8_t NUM_PROJECTILE_OBS = 30;
const uint16_t PROJECTILE_DRONE_OBS_OFFSET = FLOATING_WALL_TYPES_OBS_OFFSET + NUM_FLOATING_WALL_OBS;
const uint16_t PROJECTILE_WEAPONS_OBS_OFFSET = PROJECTILE_DRONE_OBS_OFFSET + NUM_PROJECTILE_OBS;

#define _NUM_WEAPON_PICKUP_OBS 3
const uint8_t NUM_WEAPON_PICKUP_OBS = _NUM_WEAPON_PICKUP_OBS;
const uint16_t WEAPON_PICKUP_WEAPONS_OBS_OFFSET = PROJECTILE_WEAPONS_OBS_OFFSET + NUM_PROJECTILE_OBS;

const uint16_t ENEMY_DRONE_WEAPONS_OBS_OFFSET = WEAPON_PICKUP_WEAPONS_OBS_OFFSET + NUM_WEAPON_PICKUP_OBS;

// continuous observations
const uint8_t NEAR_WALL_POS_OBS_SIZE = 2;
const uint8_t NEAR_WALL_OBS_SIZE = NUM_NEAR_WALL_OBS * NEAR_WALL_POS_OBS_SIZE;
const uint16_t NEAR_WALL_POS_OBS_OFFSET = 0;

const uint8_t FLOATING_WALL_INFO_OBS_SIZE = 5;
const uint8_t FLOATING_WALL_OBS_SIZE = NUM_FLOATING_WALL_OBS * FLOATING_WALL_INFO_OBS_SIZE;
const uint16_t FLOATING_WALL_INFO_OBS_OFFSET = NEAR_WALL_POS_OBS_OFFSET + NEAR_WALL_OBS_SIZE;

const uint8_t WEAPON_PICKUP_POS_OBS_SIZE = 2;
const uint8_t WEAPON_PICKUP_OBS_SIZE = NUM_WEAPON_PICKUP_OBS * WEAPON_PICKUP_POS_OBS_SIZE;
const uint16_t WEAPON_PICKUP_POS_OBS_OFFSET = FLOATING_WALL_INFO_OBS_OFFSET + FLOATING_WALL_OBS_SIZE;

const uint8_t PROJECTILE_INFO_OBS_SIZE = 4;
const uint8_t PROJECTILE_OBS_SIZE = NUM_PROJECTILE_OBS * PROJECTILE_INFO_OBS_SIZE;
const uint16_t PROJECTILE_INFO_OBS_OFFSET = WEAPON_PICKUP_POS_OBS_OFFSET + WEAPON_PICKUP_OBS_SIZE;

const uint16_t ENEMY_DRONE_OBS_OFFSET = PROJECTILE_INFO_OBS_OFFSET + PROJECTILE_OBS_SIZE;
const uint8_t ENEMY_DRONE_OBS_SIZE = 24;

const uint8_t DRONE_OBS_SIZE = 22;

const uint8_t MISC_OBS_SIZE = 1;

const uint16_t _DISCRETE_OBS_SIZE = MAP_OBS_SIZE + NUM_NEAR_WALL_OBS + NUM_FLOATING_WALL_OBS + (NUM_PROJECTILE_OBS * 2) + NUM_WEAPON_PICKUP_OBS + 1;
const uint16_t _CONTINUOUS_OBS_SIZE = NEAR_WALL_OBS_SIZE + FLOATING_WALL_OBS_SIZE + WEAPON_PICKUP_OBS_SIZE + PROJECTILE_OBS_SIZE + DRONE_OBS_SIZE + MISC_OBS_SIZE;

uint16_t discreteObsSize(uint8_t numDrones) {
    return _DISCRETE_OBS_SIZE + ((numDrones - 1));
}

uint16_t continuousObsSize(uint8_t numDrones) {
    return _CONTINUOUS_OBS_SIZE + ((numDrones - 1) * ENEMY_DRONE_OBS_SIZE);
}

uint16_t obsBytes(uint8_t numDrones) {
    return alignedSize((discreteObsSize(numDrones) * sizeof(uint8_t)) + (continuousObsSize(numDrones) * sizeof(float)), sizeof(float));
}

const float MAX_X_POS = 150.0f;
const float MAX_Y_POS = 150.0f;
const float MAX_DISTANCE = 200.0f;
const float MAX_SPEED = 500.0f;
const float MAX_ACCEL = 1000.0f;
const float MAX_ANGLE = PI;

// action constants
const uint8_t CONTINUOUS_ACTION_SIZE = 7;
const uint8_t DISCRETE_ACTION_SIZE = 5;
const float ACTION_NOOP_MAGNITUDE = 0.1f;

const float discMoveToContMoveMap[2][8] = {
    {1.0f, 0.707107f, 0.0f, -0.707107f, -1.0f, -0.707107f, 0.0f, 0.707107f},
    {0.0f, 0.707107f, 1.0f, 0.707107f, 0.0f, -0.707107f, -1.0f, -0.707107f},
};
const float discAimToContAimMap[2][16] = {
    {1.0f, 0.92388f, 0.707107f, 0.382683f, 0.0f, -0.382683f, -0.707107f, -0.92388f, -1.0f, -0.92388f, -0.707107f, -0.382683f, 0.0f, 0.382683f, 0.707107f, 0.92388f},
    {0.0f, 0.382683f, 0.707107f, 0.92388f, 1.0f, 0.92388f, 0.707107f, 0.382683f, 0.0f, -0.382683f, -0.707107f, -0.92388f, -1.0f, -0.92388f, -0.707107f, -0.382683f},
};

const float MIN_SPAWN_DISTANCE = 6.0f;
const float MIN_SD_SPAWN_DISTANCE = 3.0f;

// wall settings
const float WALL_THICKNESS = 4.0f;
const float FLOATING_WALL_THICKNESS = 3.0f;
const float FLOATING_WALL_DAMPING = 0.75f;
const float STANDARD_WALL_RESTITUTION = 0.01f;
const float STANDARD_WALL_FRICTION = 0.3f;
const float BOUNCY_WALL_RESTITUTION = 1.0f;
const float WALL_DENSITY = 4.0f;

// weapon pickup settings
const float PICKUP_THICKNESS = 3.0f;
const float PICKUP_SPAWN_DISTANCE_SQUARED = SQUARED(10.0f);
const float PICKUP_RESPAWN_WAIT = 3.0f;
const float SUDDEN_DEATH_PICKUP_RESPAWN_WAIT = 2.0f;

// drone settings
const float DRONE_WALL_SPAWN_DISTANCE = 2.0f;
const float DRONE_DEATH_WALL_SPAWN_DISTANCE = 7.5f;
const float DRONE_DRONE_SPAWN_DISTANCE_SQUARED = SQUARED(10.0f);

#define DRONE_RADIUS 1.0f
#define DRONE_DENSITY 1.25f
const float DRONE_RESTITUTION = 0.3f;
const float DRONE_FRICTION = 0.1f;
#define DRONE_INV_MASS INV_MASS(MASS(DRONE_DENSITY, DRONE_RADIUS))
const float DRONE_MOVE_MAGNITUDE = 35.0f;
const float DRONE_LINEAR_DAMPING = 1.0f;
const float DRONE_MOVE_AIM_COEF = 0.1f;

const float DRONE_ENERGY_MAX = 1.0f;
const float DRONE_BRAKE_DAMPING_COEF = 2.5f;
const float DRONE_BRAKE_DRAIN_RATE = 0.5f;
const float DRONE_ENERGY_REFILL_WAIT = 1.0f;
const float DRONE_ENERGY_REFILL_EMPTY_WAIT = 3.0f;
const float DRONE_ENERGY_REFILL_RATE = 0.03f;
const float DRONE_ENERGY_RESPAWN_REFILL = 0.5f;

const float DRONE_BURST_BASE_COST = 0.1f;
const float DRONE_BURST_CHARGE_RATE = 0.6f;
const float DRONE_BURST_RADIUS_BASE = 5.0f;
const float DRONE_BURST_RADIUS_MIN = 3.5f;
const float DRONE_BURST_IMPACT_BASE = 140.0f;
const float DRONE_BURST_IMPACT_MIN = 35.0f;
const float DRONE_BURST_COOLDOWN = 0.5f;

const float DRONE_SHIELD_RADIUS = DRONE_RADIUS * 1.5f;
const float DRONE_SHIELD_HEALTH = 100.0f;
const float DRONE_SHIELD_START_DURATION = 1.5f;
const float DRONE_SHIELD_RESPAWN_DURATION = 3.0f;
const float DRONE_SHIELD_EXPLOSION_REDUCTION = 0.5f;
const float DRONE_SHIELD_HEALTH_IMPULSE_COEF = 0.5f;
const float DRONE_SHIELD_HEALTH_EXPLOSION_COEF = 0.8;
const float DRONE_SHIELD_BREAK_ENERGY_COST = -0.33f;
const float DRONE_SHIELD_BREAK_ENERGY_REFILL = 0.25f;

#define PROJECTILE_ENERGY_REFILL_COEF 0.001f
const float EXPLOSION_ENERGY_REFILL_COEF = 1.75f;
const float WEAPON_DISCARD_COST = 0.2f;

const uint8_t DRONE_PIECE_COUNT = 5;
const float DRONE_PIECE_MIN_DISTANCE = 1.5f;
const float DRONE_PIECE_MAX_DISTANCE = 3.0f;
const float DRONE_PIECE_LINEAR_DAMPING = 1.0f;
const float DRONE_PIECE_ANGULAR_DAMPING = 1.0f;
const float DRONE_PIECE_MIN_SPEED = 5.0f;
const float DRONE_PIECE_MAX_SPEED = 10.0f;

// TODO: increase impulse of imploder of entity it directly hits?
// weapon projectile settings
#define STANDARD_AMMO INFINITE
#define STANDARD_PROJECTILES 1
#define STANDARD_RECOIL_MAGNITUDE 20.0f
#define STANDARD_FIRE_MAGNITUDE 17.0f
#define STANDARD_DAMPING 0.0f
#define STANDARD_CHARGE 0.0f
#define STANDARD_COOL_DOWN 0.37f
#define STANDARD_MAX_DISTANCE 80.0f
#define STANDARD_RADIUS 0.2
#define STANDARD_DENSITY 3.25f
#define STANDARD_MASS MASS(STANDARD_DENSITY, STANDARD_RADIUS)
#define STANDARD_INV_MASS INV_MASS(STANDARD_MASS)
#define STANDARD_BOUNCE 2
#define STANDARD_SPAWN_WEIGHT 0

#define MACHINEGUN_AMMO 50
#define MACHINEGUN_PROJECTILES 1
#define MACHINEGUN_RECOIL_MAGNITUDE 12.8f
#define MACHINEGUN_FIRE_MAGNITUDE 30.0f
#define MACHINEGUN_DAMPING 0.1f
#define MACHINEGUN_CHARGE 0.0f
#define MACHINEGUN_COOL_DOWN 0.07f
#define MACHINEGUN_MAX_DISTANCE 225.0f
#define MACHINEGUN_RADIUS 0.15f
#define MACHINEGUN_DENSITY 3.2f
#define MACHINEGUN_MASS MASS(MACHINEGUN_DENSITY, MACHINEGUN_RADIUS)
#define MACHINEGUN_INV_MASS INV_MASS(MACHINEGUN_MASS)
#define MACHINEGUN_BOUNCE 1
#define MACHINEGUN_ENERGY_REFILL_COEF 0.2f
#define MACHINEGUN_SPAWN_WEIGHT 3.0f

#define SNIPER_AMMO 3
#define SNIPER_PROJECTILES 1
#define SNIPER_RECOIL_MAGNITUDE 96.0f
#define SNIPER_FIRE_MAGNITUDE 300.0f
#define SNIPER_DAMPING 0.05f
#define SNIPER_CHARGE 1.0f
#define SNIPER_COOL_DOWN 1.5f
#define SNIPER_MAX_DISTANCE INFINITE
#define SNIPER_RADIUS 0.5f
#define SNIPER_DENSITY 2.0f
#define SNIPER_MASS MASS(SNIPER_DENSITY, SNIPER_RADIUS)
#define SNIPER_INV_MASS INV_MASS(SNIPER_MASS)
#define SNIPER_BOUNCE 0
#define SNIPER_ENERGY_REFILL_COEF 1.2f
#define SNIPER_SPAWN_WEIGHT 3.0f

#define SHOTGUN_AMMO 8
#define SHOTGUN_PROJECTILES 8
#define SHOTGUN_RECOIL_MAGNITUDE 100.0f
#define SHOTGUN_FIRE_MAGNITUDE 25.0f
#define SHOTGUN_DAMPING 0.3f
#define SHOTGUN_CHARGE 0.0f
#define SHOTGUN_COOL_DOWN 0.8f
#define SHOTGUN_MAX_DISTANCE 100.0f
#define SHOTGUN_RADIUS 0.15f
#define SHOTGUN_DENSITY 2.5f
#define SHOTGUN_MASS MASS(SHOTGUN_DENSITY, SHOTGUN_RADIUS)
#define SHOTGUN_INV_MASS INV_MASS(SHOTGUN_MASS)
#define SHOTGUN_BOUNCE 1
#define SHOTGUN_ENERGY_REFILL_COEF 0.5f
#define SHOTGUN_SPAWN_WEIGHT 3.0f

#define IMPLODER_AMMO 1
#define IMPLODER_PROJECTILES 1
#define IMPLODER_RECOIL_MAGNITUDE 65.0f
#define IMPLODER_FIRE_MAGNITUDE 60.0f
#define IMPLODER_DAMPING 0.0f
#define IMPLODER_CHARGE 2.0f
#define IMPLODER_COOL_DOWN 1.5f
#define IMPLODER_MAX_DISTANCE INFINITE
#define IMPLODER_RADIUS 0.8f
#define IMPLODER_DENSITY 1.0f
#define IMPLODER_MASS MASS(IMPLODER_DENSITY, IMPLODER_RADIUS)
#define IMPLODER_INV_MASS INV_MASS(IMPLODER_MASS)
#define IMPLODER_BOUNCE 0
#define IMPLODER_SPAWN_WEIGHT 1.0f

#define ACCELERATOR_AMMO 1
#define ACCELERATOR_PROJECTILES 1
#define ACCELERATOR_RECOIL_MAGNITUDE 100.0f
#define ACCELERATOR_FIRE_MAGNITUDE 35.0f
#define ACCELERATOR_DAMPING 0.0f
#define ACCELERATOR_CHARGE 0.0f
#define ACCELERATOR_COOL_DOWN 2.0f
#define ACCELERATOR_MAX_DISTANCE INFINITE
#define ACCELERATOR_RADIUS 0.5f
#define ACCELERATOR_DENSITY 2.0f
#define ACCELERATOR_MASS MASS(ACCELERATOR_DENSITY, ACCELERATOR_RADIUS)
#define ACCELERATOR_INV_MASS INV_MASS(ACCELERATOR_MASS)
#define ACCELERATOR_BOUNCE 100
#define ACCELERATOR_BOUNCE_SPEED_COEF 1.07f
#define ACCELERATOR_MAX_SPEED 500.f
#define ACCELERATOR_SPAWN_WEIGHT 1.0f

#define FLAK_CANNON_AMMO 12
#define FLAK_CANNON_PROJECTILES 1
#define FLAK_CANNON_RECOIL_MAGNITUDE 30.0f
#define FLAK_CANNON_FIRE_MAGNITUDE 14.0f
#define FLAK_CANNON_DAMPING 0.15f
#define FLAK_CANNON_CHARGE 0.0f
#define FLAK_CANNON_COOL_DOWN 0.4f
#define FLAK_CANNON_MAX_DISTANCE 100.0f
#define FLAK_CANNON_RADIUS 0.3f
#define FLAK_CANNON_DENSITY 1.0f
#define FLAK_CANNON_MASS MASS(FLAK_CANNON_DENSITY, FLAK_CANNON_RADIUS)
#define FLAK_CANNON_INV_MASS INV_MASS(FLAK_CANNON_MASS)
#define FLAK_CANNON_BOUNCE INFINITE
#define FLAK_CANNON_SAFE_DISTANCE 25.0f
#define FLAK_CANNON_PROXIMITY_RADIUS 2.0f
#define FLAK_CANNON_SPAWN_WEIGHT 2.0f

#define MINE_LAUNCHER_AMMO 3
#define MINE_LAUNCHER_PROJECTILES 1
#define MINE_LAUNCHER_RECOIL_MAGNITUDE 20.0f
#define MINE_LAUNCHER_FIRE_MAGNITUDE 25.0f
#define MINE_LAUNCHER_DAMPING 0.2f
#define MINE_LAUNCHER_CHARGE 0.0f
#define MINE_LAUNCHER_COOL_DOWN 0.6f
#define MINE_LAUNCHER_MAX_DISTANCE INFINITE
#define MINE_LAUNCHER_RADIUS 0.5f
#define MINE_LAUNCHER_DENSITY 0.5f
#define MINE_LAUNCHER_MASS MASS(MINE_LAUNCHER_DENSITY, MINE_LAUNCHER_RADIUS)
#define MINE_LAUNCHER_INV_MASS INV_MASS(MINE_LAUNCHER_MASS)
#define MINE_LAUNCHER_BOUNCE INFINITE // this is to avoid mines sometimes exploding when hitting walls
#define MINE_LAUNCHER_SPAWN_WEIGHT 2.0f
#define MINE_LAUNCHER_PROXIMITY_RADIUS 7.5f

#define BLACK_HOLE_AMMO 3
#define BLACK_HOLE_PROJECTILES 1
#define BLACK_HOLE_RECOIL_MAGNITUDE 75.0f
#define BLACK_HOLE_FIRE_MAGNITUDE 125.0f
#define BLACK_HOLE_DAMPING 0.0f
#define BLACK_HOLE_CHARGE 0.75f
#define BLACK_HOLE_COOL_DOWN 1.0f
#define BLACK_HOLE_MAX_DISTANCE INFINITE
#define BLACK_HOLE_RADIUS 0.5f
#define BLACK_HOLE_DENSITY 10.0f
#define BLACK_HOLE_MASS MASS(BLACK_HOLE_DENSITY, BLACK_HOLE_RADIUS)
#define BLACK_HOLE_INV_MASS INV_MASS(BLACK_HOLE_MASS)
#define BLACK_HOLE_BOUNCE 0
#define BLACK_HOLE_SPAWN_WEIGHT 2.0f
#define BLACK_HOLE_PROXIMITY_RADIUS 10.0f
#define BLACK_HOLE_PARENT_IGNORE_DISTANCE BLACK_HOLE_PROXIMITY_RADIUS * 1.5f
#define BLACK_HOLE_PULL_MAGNITUDE -300.0f

#define NUKE_AMMO 1
#define NUKE_PROJECTILES 1
#define NUKE_RECOIL_MAGNITUDE 150.0f
#define NUKE_FIRE_MAGNITUDE 85.0f
#define NUKE_DAMPING 0.0f
#define NUKE_CHARGE 5.0f
#define NUKE_COOL_DOWN 3.0f
#define NUKE_MAX_DISTANCE INFINITE
#define NUKE_RADIUS 0.8f
#define NUKE_DENSITY 3.0f
#define NUKE_MASS MASS(NUKE_DENSITY, NUKE_RADIUS)
#define NUKE_INV_MASS INV_MASS(NUKE_MASS)
#define NUKE_BOUNCE 0
#define NUKE_SPAWN_WEIGHT 0.5f

const weaponInformation standard = {
    .type = STANDARD_WEAPON,
    .isPhysicsBullet = true,
    .canSleep = false,
    .numProjectiles = STANDARD_PROJECTILES,
    .fireMagnitude = STANDARD_FIRE_MAGNITUDE,
    .recoilMagnitude = STANDARD_RECOIL_MAGNITUDE,
    .damping = STANDARD_DAMPING,
    .charge = STANDARD_CHARGE,
    .coolDown = STANDARD_COOL_DOWN,
    .maxDistance = STANDARD_MAX_DISTANCE,
    .radius = STANDARD_RADIUS,
    .density = STANDARD_DENSITY,
    .mass = STANDARD_MASS,
    .invMass = STANDARD_INV_MASS,
    .initialSpeed = STANDARD_FIRE_MAGNITUDE * STANDARD_INV_MASS,
    .maxBounces = STANDARD_BOUNCE + 1,
    .explosive = false,
    .destroyedOnDroneHit = false,
    .explodesOnDroneHit = false,
    .hasSensor = false,
    .energyRefillCoef = PROJECTILE_ENERGY_REFILL_COEF,
    .spawnWeight = STANDARD_SPAWN_WEIGHT,
};

const weaponInformation machineGun = {
    .type = MACHINEGUN_WEAPON,
    .isPhysicsBullet = true,
    .canSleep = false,
    .numProjectiles = MACHINEGUN_PROJECTILES,
    .fireMagnitude = MACHINEGUN_FIRE_MAGNITUDE,
    .recoilMagnitude = MACHINEGUN_RECOIL_MAGNITUDE,
    .damping = MACHINEGUN_DAMPING,
    .charge = MACHINEGUN_CHARGE,
    .coolDown = MACHINEGUN_COOL_DOWN,
    .maxDistance = MACHINEGUN_MAX_DISTANCE,
    .radius = MACHINEGUN_RADIUS,
    .density = MACHINEGUN_DENSITY,
    .mass = MACHINEGUN_MASS,
    .invMass = MACHINEGUN_INV_MASS,
    .initialSpeed = MACHINEGUN_FIRE_MAGNITUDE * MACHINEGUN_INV_MASS,
    .maxBounces = MACHINEGUN_BOUNCE + 1,
    .explosive = false,
    .destroyedOnDroneHit = false,
    .explodesOnDroneHit = false,
    .hasSensor = false,
    .energyRefillCoef = PROJECTILE_ENERGY_REFILL_COEF * MACHINEGUN_ENERGY_REFILL_COEF,
    .spawnWeight = MACHINEGUN_SPAWN_WEIGHT,
};

const weaponInformation sniper = {
    .type = SNIPER_WEAPON,
    .isPhysicsBullet = true,
    .canSleep = false,
    .numProjectiles = SNIPER_PROJECTILES,
    .fireMagnitude = SNIPER_FIRE_MAGNITUDE,
    .recoilMagnitude = SNIPER_RECOIL_MAGNITUDE,
    .damping = SNIPER_DAMPING,
    .charge = SNIPER_CHARGE,
    .coolDown = SNIPER_COOL_DOWN,
    .maxDistance = SNIPER_MAX_DISTANCE,
    .radius = SNIPER_RADIUS,
    .density = SNIPER_DENSITY,
    .mass = SNIPER_MASS,
    .invMass = SNIPER_INV_MASS,
    .initialSpeed = SNIPER_FIRE_MAGNITUDE * SNIPER_INV_MASS,
    .maxBounces = SNIPER_BOUNCE + 1,
    .explosive = false,
    .destroyedOnDroneHit = true,
    .explodesOnDroneHit = false,
    .hasSensor = false,
    .energyRefillCoef = PROJECTILE_ENERGY_REFILL_COEF * SNIPER_ENERGY_REFILL_COEF,
    .spawnWeight = SNIPER_SPAWN_WEIGHT,
};

const weaponInformation shotgun = {
    .type = SHOTGUN_WEAPON,
    .isPhysicsBullet = true,
    .canSleep = false,
    .numProjectiles = SHOTGUN_PROJECTILES,
    .fireMagnitude = SHOTGUN_FIRE_MAGNITUDE,
    .recoilMagnitude = SHOTGUN_RECOIL_MAGNITUDE,
    .damping = SHOTGUN_DAMPING,
    .charge = SHOTGUN_CHARGE,
    .coolDown = SHOTGUN_COOL_DOWN,
    .maxDistance = SHOTGUN_MAX_DISTANCE,
    .radius = SHOTGUN_RADIUS,
    .density = SHOTGUN_DENSITY,
    .mass = SHOTGUN_MASS,
    .invMass = SHOTGUN_INV_MASS,
    .initialSpeed = SHOTGUN_FIRE_MAGNITUDE * SHOTGUN_INV_MASS,
    .maxBounces = SHOTGUN_BOUNCE + 1,
    .explosive = false,
    .destroyedOnDroneHit = false,
    .explodesOnDroneHit = false,
    .hasSensor = false,
    .energyRefillCoef = PROJECTILE_ENERGY_REFILL_COEF * SHOTGUN_ENERGY_REFILL_COEF,
    .spawnWeight = SHOTGUN_SPAWN_WEIGHT,
};

const weaponInformation imploder = {
    .type = IMPLODER_WEAPON,
    .isPhysicsBullet = false,
    .canSleep = false,
    .numProjectiles = IMPLODER_PROJECTILES,
    .fireMagnitude = IMPLODER_FIRE_MAGNITUDE,
    .recoilMagnitude = IMPLODER_RECOIL_MAGNITUDE,
    .damping = IMPLODER_DAMPING,
    .charge = IMPLODER_CHARGE,
    .coolDown = IMPLODER_COOL_DOWN,
    .maxDistance = IMPLODER_MAX_DISTANCE,
    .radius = IMPLODER_RADIUS,
    .density = IMPLODER_DENSITY,
    .mass = IMPLODER_MASS,
    .invMass = IMPLODER_INV_MASS,
    .initialSpeed = IMPLODER_FIRE_MAGNITUDE * IMPLODER_INV_MASS,
    .maxBounces = IMPLODER_BOUNCE + 1,
    .explosive = true,
    .destroyedOnDroneHit = true,
    .explodesOnDroneHit = true,
    .hasSensor = false,
    .energyRefillCoef = PROJECTILE_ENERGY_REFILL_COEF,
    .spawnWeight = IMPLODER_SPAWN_WEIGHT,
};

const weaponInformation accelerator = {
    .type = ACCELERATOR_WEAPON,
    .isPhysicsBullet = true,
    .canSleep = false,
    .numProjectiles = ACCELERATOR_PROJECTILES,
    .fireMagnitude = ACCELERATOR_FIRE_MAGNITUDE,
    .recoilMagnitude = ACCELERATOR_RECOIL_MAGNITUDE,
    .damping = ACCELERATOR_DAMPING,
    .charge = ACCELERATOR_CHARGE,
    .coolDown = ACCELERATOR_COOL_DOWN,
    .maxDistance = ACCELERATOR_MAX_DISTANCE,
    .radius = ACCELERATOR_RADIUS,
    .density = ACCELERATOR_DENSITY,
    .mass = ACCELERATOR_MASS,
    .invMass = ACCELERATOR_INV_MASS,
    .initialSpeed = ACCELERATOR_FIRE_MAGNITUDE * ACCELERATOR_INV_MASS,
    .maxBounces = ACCELERATOR_BOUNCE + 1,
    .explosive = false,
    .destroyedOnDroneHit = true,
    .explodesOnDroneHit = false,
    .hasSensor = false,
    .energyRefillCoef = PROJECTILE_ENERGY_REFILL_COEF,
    .spawnWeight = ACCELERATOR_SPAWN_WEIGHT,
};

const weaponInformation flakCannon = {
    .type = FLAK_CANNON_WEAPON,
    .isPhysicsBullet = true,
    .canSleep = false,
    .numProjectiles = FLAK_CANNON_PROJECTILES,
    .fireMagnitude = FLAK_CANNON_FIRE_MAGNITUDE,
    .recoilMagnitude = FLAK_CANNON_RECOIL_MAGNITUDE,
    .damping = FLAK_CANNON_DAMPING,
    .charge = FLAK_CANNON_CHARGE,
    .coolDown = FLAK_CANNON_COOL_DOWN,
    .maxDistance = FLAK_CANNON_MAX_DISTANCE,
    .radius = FLAK_CANNON_RADIUS,
    .density = FLAK_CANNON_DENSITY,
    .mass = FLAK_CANNON_MASS,
    .invMass = FLAK_CANNON_INV_MASS,
    .initialSpeed = FLAK_CANNON_FIRE_MAGNITUDE * FLAK_CANNON_INV_MASS,
    .maxBounces = FLAK_CANNON_BOUNCE + 1,
    .explosive = true,
    .destroyedOnDroneHit = false,
    .explodesOnDroneHit = false,
    .hasSensor = true,
    .energyRefillCoef = PROJECTILE_ENERGY_REFILL_COEF,
    .spawnWeight = FLAK_CANNON_SPAWN_WEIGHT,
};

const weaponInformation mineLauncher = {
    .type = MINE_LAUNCHER_WEAPON,
    .isPhysicsBullet = false,
    .canSleep = true,
    .numProjectiles = MINE_LAUNCHER_PROJECTILES,
    .fireMagnitude = MINE_LAUNCHER_FIRE_MAGNITUDE,
    .recoilMagnitude = MINE_LAUNCHER_RECOIL_MAGNITUDE,
    .damping = MINE_LAUNCHER_DAMPING,
    .charge = MINE_LAUNCHER_CHARGE,
    .coolDown = MINE_LAUNCHER_COOL_DOWN,
    .maxDistance = MINE_LAUNCHER_MAX_DISTANCE,
    .radius = MINE_LAUNCHER_RADIUS,
    .density = MINE_LAUNCHER_DENSITY,
    .mass = MINE_LAUNCHER_MASS,
    .invMass = MINE_LAUNCHER_INV_MASS,
    .initialSpeed = MINE_LAUNCHER_FIRE_MAGNITUDE * MINE_LAUNCHER_INV_MASS,
    .maxBounces = MINE_LAUNCHER_BOUNCE + 1,
    .explosive = true,
    .destroyedOnDroneHit = true,
    .explodesOnDroneHit = false,
    .hasSensor = true,
    .energyRefillCoef = PROJECTILE_ENERGY_REFILL_COEF,
    .spawnWeight = MINE_LAUNCHER_SPAWN_WEIGHT,
};

const weaponInformation blackHole = {
    .type = BLACK_HOLE_WEAPON,
    .isPhysicsBullet = false,
    .canSleep = false,
    .numProjectiles = BLACK_HOLE_PROJECTILES,
    .fireMagnitude = BLACK_HOLE_FIRE_MAGNITUDE,
    .recoilMagnitude = BLACK_HOLE_RECOIL_MAGNITUDE,
    .damping = BLACK_HOLE_DAMPING,
    .charge = BLACK_HOLE_CHARGE,
    .coolDown = BLACK_HOLE_COOL_DOWN,
    .maxDistance = BLACK_HOLE_MAX_DISTANCE,
    .radius = BLACK_HOLE_RADIUS,
    .density = BLACK_HOLE_DENSITY,
    .mass = BLACK_HOLE_MASS,
    .invMass = BLACK_HOLE_INV_MASS,
    .initialSpeed = BLACK_HOLE_FIRE_MAGNITUDE * BLACK_HOLE_INV_MASS,
    .maxBounces = BLACK_HOLE_BOUNCE + 1,
    .explosive = false,
    .destroyedOnDroneHit = false,
    .explodesOnDroneHit = false,
    .hasSensor = true,
    .energyRefillCoef = 0.0f,
    .spawnWeight = BLACK_HOLE_SPAWN_WEIGHT,
};

const weaponInformation nuke = {
    .type = NUKE_WEAPON,
    .isPhysicsBullet = false,
    .canSleep = false,
    .numProjectiles = NUKE_PROJECTILES,
    .fireMagnitude = NUKE_FIRE_MAGNITUDE,
    .recoilMagnitude = NUKE_RECOIL_MAGNITUDE,
    .damping = NUKE_DAMPING,
    .charge = NUKE_CHARGE,
    .coolDown = NUKE_COOL_DOWN,
    .maxDistance = NUKE_MAX_DISTANCE,
    .radius = NUKE_RADIUS,
    .density = NUKE_DENSITY,
    .mass = NUKE_MASS,
    .invMass = NUKE_INV_MASS,
    .initialSpeed = NUKE_FIRE_MAGNITUDE * NUKE_INV_MASS,
    .maxBounces = NUKE_BOUNCE + 1,
    .explosive = true,
    .destroyedOnDroneHit = true,
    .explodesOnDroneHit = true,
    .hasSensor = false,
    .energyRefillCoef = PROJECTILE_ENERGY_REFILL_COEF,
    .spawnWeight = NUKE_SPAWN_WEIGHT,
};

weaponInformation *weaponInfos[] = {
    (weaponInformation *)&standard,
    (weaponInformation *)&machineGun,
    (weaponInformation *)&sniper,
    (weaponInformation *)&shotgun,
    (weaponInformation *)&imploder,
    (weaponInformation *)&accelerator,
    (weaponInformation *)&flakCannon,
    (weaponInformation *)&mineLauncher,
    (weaponInformation *)&blackHole,
    (weaponInformation *)&nuke,
};

const char *weaponNames[] = {
    "standard",
    "machine_gun",
    "sniper",
    "shotgun",
    "imploder",
    "accelerator",
    "flak_cannon",
    "mine_launcher",
    "black_hole",
    "tactical_nuke",
};

// max ammo of weapon
int8_t weaponAmmo(const enum weaponType defaultWep, const enum weaponType type) {
    if (type == defaultWep) {
        return INFINITE;
    }
    switch (type) {
    case STANDARD_WEAPON:
        return STANDARD_AMMO;
    case MACHINEGUN_WEAPON:
        return MACHINEGUN_AMMO;
    case SNIPER_WEAPON:
        return SNIPER_AMMO;
    case SHOTGUN_WEAPON:
        return SHOTGUN_AMMO;
    case IMPLODER_WEAPON:
        return IMPLODER_AMMO;
    case ACCELERATOR_WEAPON:
        return ACCELERATOR_AMMO;
    case FLAK_CANNON_WEAPON:
        return FLAK_CANNON_AMMO;
    case MINE_LAUNCHER_WEAPON:
        return MINE_LAUNCHER_AMMO;
    case BLACK_HOLE_WEAPON:
        return BLACK_HOLE_AMMO;
    case NUKE_WEAPON:
        return NUKE_AMMO;
    default:
        ERRORF("unknown weapon type %d", type);
    }
}

b2ShapeId weaponSensor(const b2BodyId bodyID, const enum weaponType type) {
    b2ShapeDef sensorShapeDef = b2DefaultShapeDef();
    sensorShapeDef.density = 0.0f;
    sensorShapeDef.isSensor = true;
    sensorShapeDef.enableSensorEvents = true;
    b2Circle sensorCircle = {.center = b2Vec2_zero};

    switch (type) {
    case FLAK_CANNON_WEAPON:
        sensorShapeDef.filter.categoryBits = PROJECTILE_SHAPE;
        sensorShapeDef.filter.maskBits = DRONE_SHAPE;
        sensorCircle.radius = FLAK_CANNON_PROXIMITY_RADIUS;
        break;
    case MINE_LAUNCHER_WEAPON:
        sensorShapeDef.filter.categoryBits = PROJECTILE_SHAPE;
        sensorShapeDef.filter.maskBits = DRONE_SHAPE;
        sensorCircle.radius = MINE_LAUNCHER_PROXIMITY_RADIUS;
        break;
    case BLACK_HOLE_WEAPON:
        sensorShapeDef.filter.categoryBits = PROJECTILE_SHAPE;
        sensorShapeDef.filter.maskBits = FLOATING_WALL_SHAPE | PROJECTILE_SHAPE | DRONE_SHAPE;
        sensorCircle.radius = BLACK_HOLE_PROXIMITY_RADIUS;
        break;
    default:
        ERRORF("unknown weapon type with sensor %d", type);
    }

    return b2CreateCircleShape(bodyID, &sensorShapeDef, &sensorCircle);
}

// amount of force to apply to projectile
float weaponFire(uint64_t *seed, const enum weaponType type) {
    switch (type) {
    case STANDARD_WEAPON:
        return STANDARD_FIRE_MAGNITUDE;
    case MACHINEGUN_WEAPON:
        return MACHINEGUN_FIRE_MAGNITUDE;
    case SNIPER_WEAPON:
        return SNIPER_FIRE_MAGNITUDE;
    case SHOTGUN_WEAPON: {
        const int maxOffset = 3;
        const int fireOffset = randInt(seed, -maxOffset, maxOffset);
        return SHOTGUN_FIRE_MAGNITUDE + fireOffset;
    }
    case IMPLODER_WEAPON:
        return IMPLODER_FIRE_MAGNITUDE;
    case ACCELERATOR_WEAPON:
        return ACCELERATOR_FIRE_MAGNITUDE;
    case FLAK_CANNON_WEAPON:
        return FLAK_CANNON_FIRE_MAGNITUDE;
    case MINE_LAUNCHER_WEAPON:
        return MINE_LAUNCHER_FIRE_MAGNITUDE;
    case BLACK_HOLE_WEAPON:
        return BLACK_HOLE_FIRE_MAGNITUDE;
    case NUKE_WEAPON:
        return NUKE_FIRE_MAGNITUDE;
    default:
        ERRORF("unknown weapon type %d", type);
        return 0;
    }
}

b2Vec2 weaponAdjustAim(uint64_t *seed, const enum weaponType type, const uint16_t heat, const b2Vec2 normAim) {
    switch (type) {
    case MACHINEGUN_WEAPON: {
        const float swayCoef = logBasef((heat / 5.0f) + 1, 180);
        const float maxSway = 0.1f;
        const float swayX = randFloat(seed, maxSway * -swayCoef, maxSway * swayCoef);
        const float swayY = randFloat(seed, maxSway * -swayCoef, maxSway * swayCoef);
        b2Vec2 machinegunAim = {.x = normAim.x + swayX, .y = normAim.y + swayY};
        return b2Normalize(machinegunAim);
    }
    case SHOTGUN_WEAPON: {
        const float maxOffset = 0.11f;
        const float offsetX = randFloat(seed, -maxOffset, maxOffset);
        const float offsetY = randFloat(seed, -maxOffset, maxOffset);
        b2Vec2 shotgunAim = {.x = normAim.x + offsetX, .y = normAim.y + offsetY};
        return b2Normalize(shotgunAim);
    }
    default:
        return normAim;
    }
}

// sets explosion parameters and returns true if an explosion should be created
// when a projectile is destroyed
void weaponExplosion(const enum weaponType type, b2ExplosionDef *explosionDef) {
    switch (type) {
    case IMPLODER_WEAPON:
        explosionDef->radius = 15.0f;
        explosionDef->impulsePerLength = -200.0f;
        return;
    case FLAK_CANNON_WEAPON:
        explosionDef->radius = 7.5f;
        // will typically be closer to 45 with projectile velocity
        // factored in
        explosionDef->impulsePerLength = 10.0f;
        return;
    case MINE_LAUNCHER_WEAPON:
        explosionDef->radius = 15.0f;
        explosionDef->impulsePerLength = 200.0f;
        return;
    case NUKE_WEAPON:
        explosionDef->radius = 35.0f;
        explosionDef->impulsePerLength = 300.0f;
        return;
    default:
        ERRORF("unknown weapon type %d for projectile explosion", type);
    }
}

// insertion sort should be faster than quicksort for small arrays
void insertionSort(nearEntity arr[], uint8_t size) {
    for (int16_t i = 1; i < size; i++) {
        nearEntity key = arr[i];
        int16_t j = i - 1;

        while (j >= 0 && arr[j].distanceSquared > key.distanceSquared) {
            arr[j + 1] = arr[j];
            j = j - 1;
        }

        arr[j + 1] = key;
    }
}

#endif



================================================
FILE: pufferlib/ocean/impulse_wars/types.h
================================================
#ifndef IMPULSE_WARS_TYPES_H
#define IMPULSE_WARS_TYPES_H

#include "box2d/box2d.h"
#include "id_pool.h"
#include "raylib.h"
#include "rlights.h"

#include "include/cc_array.h"

#include "settings.h"

#define _MAX_DRONES 4

const uint8_t NUM_WALL_TYPES = 3;

#define MAX_TRAIL_POINTS 20
#define MAX_DRONE_TRAIL_POINTS 20
#define MAX_PROJECTLE_TRAIL_POINTS 10

enum entityType {
    STANDARD_WALL_ENTITY,
    BOUNCY_WALL_ENTITY,
    DEATH_WALL_ENTITY,
    WEAPON_PICKUP_ENTITY,
    PROJECTILE_ENTITY,
    DRONE_ENTITY,
    SHIELD_ENTITY,
    DRONE_PIECE_ENTITY,
};

// the category bit that will be set on each entity's shape; this is
// used to control what entities can collide with each other
enum shapeCategory {
    WALL_SHAPE = 1,
    FLOATING_WALL_SHAPE = 2,
    PROJECTILE_SHAPE = 4,
    WEAPON_PICKUP_SHAPE = 8,
    DRONE_SHAPE = 16,
    SHIELD_SHAPE = 32,
    DRONE_PIECE_SHAPE = 64,
};

typedef struct entityID {
    int32_t id;
    uint16_t generation;
} entityID;

// general purpose entity object
typedef struct entity {
    entityID *id;
    uint32_t generation;
    enum entityType type;
    void *entity;
} entity;

#define _NUM_WEAPONS 10
const uint8_t NUM_WEAPONS = _NUM_WEAPONS;

enum weaponType {
    STANDARD_WEAPON,
    MACHINEGUN_WEAPON,
    SNIPER_WEAPON,
    SHOTGUN_WEAPON,
    IMPLODER_WEAPON,
    ACCELERATOR_WEAPON,
    FLAK_CANNON_WEAPON,
    MINE_LAUNCHER_WEAPON,
    BLACK_HOLE_WEAPON,
    NUKE_WEAPON,
};

typedef struct mapBounds {
    b2Vec2 min;
    b2Vec2 max;
} mapBounds;

// used for N near entities observations
typedef struct nearEntity {
    uint16_t idx;
    void *entity;
    float distanceSquared;
} nearEntity;

typedef struct mapEntry {
    const char *layout;
    const uint8_t columns;
    const uint8_t rows;
    const uint8_t randFloatingStandardWalls;
    const uint8_t randFloatingBouncyWalls;
    const uint8_t randFloatingDeathWalls;
    // are there any floating walls that have consistent starting positions
    const bool hasSetFloatingWalls;
    const uint16_t weaponPickups;
    const enum weaponType defaultWeapon;
    const uint8_t maxSuddenDeathWalls;

    mapBounds bounds;
    mapBounds spawnQuads[4];
    bool *droneSpawns;
    uint8_t *packedLayout;
    nearEntity *nearestWalls;
} mapEntry;

// a cell in the map; ent will be NULL if the cell is empty
typedef struct mapCell {
    entity *ent;
    b2Vec2 pos;
} mapCell;

typedef struct wallEntity {
    b2BodyId bodyID;
    b2ShapeId shapeID;
    b2Vec2 pos;
    b2Rot rot;
    b2Vec2 velocity;
    b2Vec2 extent;
    int16_t mapCellIdx;
    bool isFloating;
    enum entityType type;
    bool isSuddenDeath;

    entity *ent;
} wallEntity;

typedef struct weaponInformation {
    const enum weaponType type;
    // should the body be treated as a bullet by box2d; if so CCD
    // (continuous collision detection) will be enabled to prevent
    // tunneling through static bodies which is expensive so it's
    // only enabled for fast moving projectiles
    const bool isPhysicsBullet;
    // can the projectile ever be stationary? if so, it should be
    // allowed to sleep to save on physics updates
    const bool canSleep;
    const uint8_t numProjectiles;
    const float fireMagnitude;
    const float recoilMagnitude;
    const float damping;
    const float charge;
    const float coolDown;
    const float maxDistance;
    const float radius;
    const float density;
    const float mass;
    const float invMass;
    const float initialSpeed;
    const uint8_t maxBounces;
    const bool explosive;
    const bool destroyedOnDroneHit;
    const bool explodesOnDroneHit;
    const bool hasSensor;
    const float energyRefillCoef;
    const float spawnWeight;
} weaponInformation;

typedef struct weaponPickupEntity {
    b2BodyId bodyID;
    b2ShapeId shapeID;
    enum weaponType weapon;
    float respawnWait;
    // how many floating walls are touching this pickup
    uint8_t floatingWallsTouching;
    b2Vec2 pos;
    int16_t mapCellIdx;

    entity *ent;
    bool bodyDestroyed;
} weaponPickupEntity;

typedef struct droneEntity droneEntity;

typedef struct trailPoints {
    Vector2 points[MAX_TRAIL_POINTS];
    uint8_t length;
} trailPoints;

typedef struct projectileEntity {
    uint8_t droneIdx;

    b2BodyId bodyID;
    b2ShapeId shapeID;
    // used for proximity explosive projectiles
    b2ShapeId sensorID;
    weaponInformation *weaponInfo;
    b2Vec2 pos;
    int16_t mapCellIdx;
    b2Vec2 lastPos;
    b2Vec2 velocity;
    b2Vec2 lastVelocity;
    float speed;
    float lastSpeed;
    float distance;
    uint8_t bounces;
    uint8_t contacts;
    bool setMine;
    uint8_t numDronesBehindWalls;
    uint8_t dronesBehindWalls[_MAX_DRONES];
    CC_Array *entsInBlackHole;
    bool needsToBeDestroyed;

    entity *ent;

    // for rendering
    trailPoints trailPoints;
} projectileEntity;

// used to keep track of what happened each step for reward purposes
typedef struct droneStepInfo {
    bool firedShot;
    bool pickedUpWeapon;
    enum weaponType prevWeapon;
    uint8_t shotHit[_MAX_DRONES];
    bool explosionHit[_MAX_DRONES];
    uint8_t shotTaken[_MAX_DRONES];
    bool explosionTaken[_MAX_DRONES];
    bool ownShotTaken;
} droneStepInfo;

typedef struct shieldEntity {
    droneEntity *drone;

    b2BodyId bodyID;
    b2ShapeId shapeID;
    b2ShapeId bufferShapeID;
    b2Vec2 pos;
    float health;
    float duration;

    entity *ent;
} shieldEntity;

typedef struct dronePieceEntity {
    uint8_t droneIdx;

    b2BodyId bodyID;
    b2ShapeId shapeID;
    b2Vec2 pos;
    b2Rot rot;
    b2Vec2 vertices[3];
    bool isShieldPiece;

    entity *ent;

    uint16_t lifetime;
} dronePieceEntity;

typedef struct physicsStepInfo {
    uint8_t srcIdx;
    b2Vec2 impulse;
    b2Vec2 force;
    bool brakeToggled;
    uint16_t step;
} physicsStepInfo;

typedef struct droneEntity {
    b2BodyId bodyID;
    b2ShapeId shapeID;
    weaponInformation *weaponInfo;
    int8_t ammo;
    float weaponCooldown;
    uint16_t heat;
    bool chargingWeapon;
    float weaponCharge;
    float energyLeft;
    bool braking;
    bool chargingBurst;
    float burstCharge;
    float burstCooldown;
    bool energyFullyDepleted;
    bool energyFullyDepletedThisStep;
    float energyRefillWait;
    bool shotThisStep;
    bool diedThisStep;

    uint8_t idx;
    uint8_t team;
    b2Vec2 initalPos;
    b2Vec2 pos;
    int16_t mapCellIdx;
    b2Vec2 lastPos;
    b2Vec2 lastMove;
    b2Vec2 lastAim;
    b2Vec2 velocity;
    b2Vec2 lastVelocity;
    droneStepInfo stepInfo;
    float respawnWait;
    uint8_t livesLeft;
    bool dead;

    CC_Array *physicsTracking;
    int8_t killedBy;
    bool killed[_MAX_DRONES];

    shieldEntity *shield;
    entity *ent;

    // for rendering
    trailPoints trailPoints;
    CC_Array *brakeTrailPoints;
    uint16_t respawnGuideLifetime;
} droneEntity;

// stats for the whole episode
typedef struct droneStats {
    float returns;
    float distanceTraveled;
    float absDistanceTraveled;
    float brakeTime;
    float totalBursts;
    float burstsHit;
    float energyEmptied;
    float wins;

    float shotsFired[_NUM_WEAPONS];
    float shotsHit[_NUM_WEAPONS];
    float shotsTaken[_NUM_WEAPONS];
    float ownShotsTaken[_NUM_WEAPONS];
    float weaponsPickedUp[_NUM_WEAPONS];
    float shotDistances[_NUM_WEAPONS];

    float totalShotsFired;
    float totalShotsHit;
    float totalShotsTaken;
    float totalOwnShotsTaken;
    float totalWeaponsPickedUp;
    float totalShotDistances;
} droneStats;

typedef struct Log {
    float length;
    float ties;
    droneStats stats[_MAX_DRONES];

    float n;
} Log;

typedef struct gameCamera {
    Camera3D camera3D;
    Camera2D camera2D;
    Vector2 targetPos;
    float maxZoom;
    bool orthographic;
} gameCamera;

typedef struct rayClient {
    float scale;
    uint16_t width;
    uint16_t height;
    uint16_t halfWidth;
    uint16_t halfHeight;

    gameCamera *camera;

    Shader blurShader;
    int32_t blurShaderDirLoc;
    Shader bloomShader;
    int32_t bloomIntensityLoc;
    int32_t bloomTexColorLoc;
    int32_t bloomTexBloomBlurLoc;
    Shader gridShader;
    int32_t gridShaderPosLoc[4];
    int32_t gridShaderColorLoc[4];
    Texture2D wallTexture;
    RenderTexture2D blurSrcTexture;
    RenderTexture2D blurDstTexture;
    RenderTexture2D projRawTex;
    RenderTexture2D projBloomTex;
    RenderTexture2D droneRawTex;
    RenderTexture2D droneBloomTex;
} rayClient;

typedef struct brakeTrailPoint {
    b2Vec2 pos;
    uint16_t lifetime;
    bool isEnd;
} brakeTrailPoint;

typedef struct explosionInfo {
    b2ExplosionDef def;
    bool isBurst;
    uint8_t droneIdx;
    uint16_t renderSteps;
} explosionInfo;

typedef struct agentActions {
    b2Vec2 move;
    b2Vec2 aim;
    bool chargingWeapon;
    bool shoot;
    bool brake;
    bool chargingBurst;
    bool discardWeapon;
} agentActions;

typedef struct pathingInfo {
    uint8_t *paths;
    int8_t *pathBuffer;
} pathingInfo;

typedef struct iwEnv {
    uint8_t numDrones;
    uint8_t numAgents;
    uint8_t numTeams;
    bool teamsEnabled;
    bool sittingDuck;
    bool isTraining;

    uint16_t obsBytes;
    uint16_t discreteObsBytes;
    bool continuousActions;

    uint8_t *observations;
    float *rewards;
    float *actions;
    uint8_t *masks;
    uint8_t *terminals;
    uint8_t *truncations;

    uint8_t frameRate;
    float deltaTime;
    uint8_t frameSkip;
    uint8_t box2dSubSteps;
    uint64_t randState;
    bool needsReset;

    uint16_t episodeLength;
    Log log;
    droneStats stats[_MAX_DRONES];

    b2WorldId worldID;
    int8_t pinnedMapIdx;
    int8_t mapIdx;
    mapEntry *map;
    int8_t lastSpawnQuad;
    uint8_t spawnedWeaponPickups[_NUM_WEAPONS];
    weaponInformation *defaultWeapon;
    b2IdPool idPool;
    CC_Array *entities;
    CC_Array *cells;
    CC_Array *walls;
    CC_Array *floatingWalls;
    CC_Array *drones;
    CC_Array *pickups;
    CC_Array *projectiles;
    CC_Array *explodingProjectiles;
    CC_Array *dronePieces;

    pathingInfo *mapPathing;

    uint16_t totalSteps;
    uint16_t totalSuddenDeathSteps;
    // steps left until sudden death
    uint16_t stepsLeft;
    // steps left until the next set of sudden death walls are spawned
    uint16_t suddenDeathSteps;
    // the amount of sudden death walls that have been spawned
    uint8_t suddenDeathWallCounter;
    bool suddenDeathWallsPlaced;

    bool humanInput;
    uint8_t humanDroneInput;
    uint8_t connectedControllers;

    // used for rendering
    rayClient *client;
    float renderScale;
    CC_Array *explosions;
    b2Vec2 debugPoint;
} iwEnv;

#endif



================================================
FILE: pufferlib/ocean/impulse_wars/.clang-format
================================================
---
Language:        Cpp
AccessModifierOffset: -2
AlignAfterOpenBracket: BlockIndent
AlignArrayOfStructures: None
AlignConsecutiveAssignments:
  Enabled:         false
  AcrossEmptyLines: false
  AcrossComments:  false
  AlignCompound:   false
  AlignFunctionPointers: false
  PadOperators:    true
AlignConsecutiveBitFields:
  Enabled:         false
  AcrossEmptyLines: false
  AcrossComments:  false
  AlignCompound:   false
  AlignFunctionPointers: false
  PadOperators:    false
AlignConsecutiveDeclarations:
  Enabled:         false
  AcrossEmptyLines: false
  AcrossComments:  false
  AlignCompound:   false
  AlignFunctionPointers: false
  PadOperators:    false
AlignConsecutiveMacros:
  Enabled:         false
  AcrossEmptyLines: false
  AcrossComments:  false
  AlignCompound:   false
  AlignFunctionPointers: false
  PadOperators:    false
AlignConsecutiveShortCaseStatements:
  Enabled:         false
  AcrossEmptyLines: false
  AcrossComments:  false
  AlignCaseColons: false
AlignEscapedNewlines: Right
AlignOperands:   Align
AlignTrailingComments:
  Kind:            Always
  OverEmptyLines:  0
AllowAllArgumentsOnNextLine: true
AllowAllParametersOfDeclarationOnNextLine: true
AllowBreakBeforeNoexceptSpecifier: Never
AllowShortBlocksOnASingleLine: Never
AllowShortCaseLabelsOnASingleLine: false
AllowShortCompoundRequirementOnASingleLine: true
AllowShortEnumsOnASingleLine: true
AllowShortFunctionsOnASingleLine: All
AllowShortIfStatementsOnASingleLine: Never
AllowShortLambdasOnASingleLine: All
AllowShortLoopsOnASingleLine: false
AlwaysBreakAfterDefinitionReturnType: None
AlwaysBreakAfterReturnType: None
AlwaysBreakBeforeMultilineStrings: false
AlwaysBreakTemplateDeclarations: MultiLine
AttributeMacros:
  - __capability
BinPackArguments: false
BinPackParameters: false
BitFieldColonSpacing: Both
BraceWrapping:
  AfterCaseLabel:  false
  AfterClass:      false
  AfterControlStatement: Never
  AfterEnum:       false
  AfterExternBlock: false
  AfterFunction:   false
  AfterNamespace:  false
  AfterObjCDeclaration: false
  AfterStruct:     false
  AfterUnion:      false
  BeforeCatch:     false
  BeforeElse:      false
  BeforeLambdaBody: false
  BeforeWhile:     false
  IndentBraces:    false
  SplitEmptyFunction: true
  SplitEmptyRecord: true
  SplitEmptyNamespace: true
BreakAdjacentStringLiterals: true
BreakAfterAttributes: Leave
BreakAfterJavaFieldAnnotations: false
BreakArrays:     true
BreakBeforeBinaryOperators: None
BreakBeforeClosingBracket: Always
BreakBeforeConceptDeclarations: Always
BreakBeforeBraces: Custom
BreakBeforeInlineASMColon: OnlyMultiline
BreakBeforeTernaryOperators: true
BreakConstructorInitializers: BeforeColon
BreakInheritanceList: BeforeColon
BreakStringLiterals: true
ColumnLimit:     0
CommentPragmas:  '^ IWYU pragma:'
CompactNamespaces: false
ConstructorInitializerIndentWidth: 4
ContinuationIndentWidth: 4
Cpp11BracedListStyle: true
DerivePointerAlignment: false
DisableFormat:   false
EmptyLineAfterAccessModifier: Never
EmptyLineBeforeAccessModifier: LogicalBlock
ExperimentalAutoDetectBinPacking: false
FixNamespaceComments: false
IfMacros:
  - KJ_IF_MAYBE
IncludeBlocks:   Preserve
IncludeIsMainRegex: '(Test)?$'
IncludeIsMainSourceRegex: ''
IndentAccessModifiers: false
IndentCaseBlocks: false
IndentCaseLabels: false
IndentExternBlock: AfterExternBlock
IndentGotoLabels: true
IndentPPDirectives: None
IndentRequiresClause: true
IndentWidth:     4
IndentWrappedFunctionNames: false
InsertBraces:    true
InsertNewlineAtEOF: false
InsertTrailingCommas: Wrapped
IntegerLiteralSeparator:
  Binary:          0
  BinaryMinDigits: 0
  Decimal:         0
  DecimalMinDigits: 0
  Hex:             0
  HexMinDigits:    0
JavaScriptQuotes: Leave
JavaScriptWrapImports: true
KeepEmptyLinesAtTheStartOfBlocks: true
KeepEmptyLinesAtEOF: false
LambdaBodyIndentation: Signature
LineEnding:      DeriveLF
MacroBlockBegin: ''
MacroBlockEnd:   ''
MaxEmptyLinesToKeep: 1
NamespaceIndentation: None
ObjCBinPackProtocolList: Auto
ObjCBlockIndentWidth: 2
ObjCBreakBeforeNestedBlockParam: true
ObjCSpaceAfterProperty: false
ObjCSpaceBeforeProtocolList: true
PackConstructorInitializers: Never
PenaltyBreakAssignment: 2
PenaltyBreakBeforeFirstCallParameter: 19
PenaltyBreakComment: 300
PenaltyBreakFirstLessLess: 120
PenaltyBreakOpenParenthesis: 0
PenaltyBreakScopeResolution: 500
PenaltyBreakString: 1000
PenaltyBreakTemplateDeclaration: 10
PenaltyExcessCharacter: 1000000
PenaltyIndentedWhitespace: 0
PenaltyReturnTypeOnItsOwnLine: 60
PointerAlignment: Right
PPIndentWidth:   -1
QualifierAlignment: Leave
ReferenceAlignment: Pointer
ReflowComments:  true
RemoveBracesLLVM: false
RemoveParentheses: Leave
RemoveSemicolon: false
RequiresClausePosition: OwnLine
RequiresExpressionIndentation: OuterScope
SeparateDefinitionBlocks: Leave
ShortNamespaceLines: 1
SkipMacroDefinitionBody: false
SortIncludes:    CaseSensitive
SortJavaStaticImport: Before
SortUsingDeclarations: LexicographicNumeric
SpaceAfterCStyleCast: false
SpaceAfterLogicalNot: false
SpaceAfterTemplateKeyword: true
SpaceAroundPointerQualifiers: Default
SpaceBeforeAssignmentOperators: true
SpaceBeforeCaseColon: false
SpaceBeforeCpp11BracedList: false
SpaceBeforeCtorInitializerColon: true
SpaceBeforeInheritanceColon: true
SpaceBeforeJsonColon: false
SpaceBeforeParens: ControlStatements
SpaceBeforeParensOptions:
  AfterControlStatements: true
  AfterForeachMacros: true
  AfterFunctionDefinitionName: false
  AfterFunctionDeclarationName: false
  AfterIfMacros:   true
  AfterOverloadedOperator: false
  AfterPlacementOperator: true
  AfterRequiresInClause: false
  AfterRequiresInExpression: false
  BeforeNonEmptyParentheses: false
SpaceBeforeRangeBasedForLoopColon: true
SpaceBeforeSquareBrackets: false
SpaceInEmptyBlock: false
SpacesBeforeTrailingComments: 1
SpacesInAngles:  Never
SpacesInContainerLiterals: true
SpacesInLineCommentPrefix:
  Minimum:         1
  Maximum:         -1
SpacesInParens:  Never
SpacesInParensOptions:
  InCStyleCasts:   false
  InConditionalStatements: false
  InEmptyParentheses: false
  Other:           false
SpacesInSquareBrackets: false
Standard:        Latest
StatementAttributeLikeMacros:
  - Q_EMIT
StatementMacros:
  - Q_UNUSED
  - QT_REQUIRE_VERSION
TabWidth:        4
UseTab:          Never
VerilogBreakBetweenInstancePorts: true
WhitespaceSensitiveMacros:
  - BOOST_PP_STRINGIZE
  - CF_SWIFT_NAME
  - NS_SWIFT_NAME
  - PP_STRINGIZE
  - STRINGIZE
...




================================================
FILE: pufferlib/ocean/impulse_wars/include/cc_array.h
================================================
/*
 * Collections-C
 * Copyright (C) 2013-2015 Srđan Panić <srdja.panic@gmail.com>
 *
 * This file is part of Collections-C.
 *
 * Collections-C is free software: you can redistribute it and/or modify
 * it under the terms of the GNU Lesser General Public License as published by
 * the Free Software Foundation, either version 3 of the License, or
 * (at your option) any later version.
 *
 * Collections-C is distributed in the hope that it will be useful,
 * but WITHOUT ANY WARRANTY; without even the implied warranty of
 * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
 * GNU Lesser General Public License for more details.
 *
 * You should have received a copy of the GNU Lesser General Public License
 * along with Collections-C.  If not, see <http://www.gnu.org/licenses/>.
 */

#ifndef CC_ARRAY_H
#define CC_ARRAY_H

#ifdef __cplusplus
extern "C" {
#endif

#include "cc_common.h"

/**
 * A dynamic array that expands automatically as elements are
 * added. The array supports amortized constant time insertion
 * and removal of elements at the end of the array, as well as
 * constant time access.
 */
typedef struct cc_array_s CC_Array;

/**
 * Array configuration structure. Used to initialize a new Array
 * with specific values.
 */
typedef struct cc_array_conf_s {
    /**
     * The initial capacity of the array */
    size_t capacity;

    /**
     * The rate at which the buffer expands (capacity * exp_factor). */
    float exp_factor;

    /**
     * Memory allocators used to allocate the Array structure and the
     * underlying data buffers. */
    void *(*mem_alloc)(size_t size);
    void *(*mem_calloc)(size_t blocks, size_t size);
    void (*mem_free)(void *block);
} CC_ArrayConf;

/**
 * Array iterator structure. Used to iterate over the elements of
 * the array in an ascending order. The iterator also supports
 * operations for safely adding and removing elements during
 * iteration.
 */
typedef struct cc_array_iter_s {
    /**
     * The array associated with this iterator */
    CC_Array *ar;

    /**
     * The current position of the iterator.*/
    size_t index;

    /**
     * Set to true if the last returned element was removed. */
    bool last_removed;
} CC_ArrayIter;

/**
 * Array zip iterator structure. Used to iterate over the elements of two
 * arrays in lockstep in an ascending order until one of the Arrays is
 * exhausted. The iterator also supports operations for safely adding
 * and removing elements during iteration.
 */
typedef struct array_zip_iter_s {
    CC_Array *ar1;
    CC_Array *ar2;
    size_t index;
    bool last_removed;
} CC_ArrayZipIter;

enum cc_stat cc_array_new(CC_Array **out);
enum cc_stat cc_array_new_conf(CC_ArrayConf const *const conf, CC_Array **out);
void cc_array_conf_init(CC_ArrayConf *conf);
size_t cc_array_struct_size();

void cc_array_destroy(CC_Array *ar);
void cc_array_destroy_cb(CC_Array *ar, void (*cb)(void *));

enum cc_stat cc_array_add(CC_Array *ar, void *element);
enum cc_stat cc_array_add_at(CC_Array *ar, void *element, size_t index);
enum cc_stat cc_array_replace_at(CC_Array *ar, void *element, size_t index, void **out);
enum cc_stat cc_array_swap_at(CC_Array *ar, size_t index1, size_t index2);

enum cc_stat cc_array_remove(CC_Array *ar, void *element, void **out);
enum cc_stat cc_array_remove_fast(CC_Array *ar, void *element, void **out);
enum cc_stat cc_array_remove_at(CC_Array *ar, size_t index, void **out);
enum cc_stat cc_array_remove_fast_at(CC_Array *ar, size_t index, void **out);
enum cc_stat cc_array_remove_last(CC_Array *ar, void **out);
void cc_array_remove_all(CC_Array *ar);
void cc_array_remove_all_free(CC_Array *ar);

enum cc_stat cc_array_get_at(const CC_Array *ar, size_t index, void **out);
enum cc_stat cc_array_get_last(const CC_Array *ar, void **out);

enum cc_stat cc_array_subarray(CC_Array *ar, size_t from, size_t to, CC_Array **out);
enum cc_stat cc_array_copy_shallow(CC_Array *ar, CC_Array **out);
enum cc_stat cc_array_copy_deep(CC_Array *ar, void *(*cp)(void *), CC_Array **out);

void cc_array_reverse(CC_Array *ar);
enum cc_stat cc_array_trim_capacity(CC_Array *ar);

size_t cc_array_contains(const CC_Array *ar, void *element);
size_t cc_array_contains_value(const CC_Array *ar, void *element, int (*cmp)(const void *, const void *));
size_t cc_array_size(const CC_Array *ar);
size_t cc_array_capacity(const CC_Array *ar);

enum cc_stat cc_array_index_of(const CC_Array *ar, void *element, size_t *index);
void cc_array_sort(CC_Array *ar, int (*cmp)(const void *, const void *));

void cc_array_map(CC_Array *ar, void (*fn)(void *));
void cc_array_reduce(CC_Array *ar, void (*fn)(void *, void *, void *), void *result);

enum cc_stat cc_array_filter_mut(CC_Array *ar, bool (*predicate)(const void *));
enum cc_stat cc_array_filter(CC_Array *ar, bool (*predicate)(const void *), CC_Array **out);

void cc_array_iter_init(CC_ArrayIter *iter, CC_Array *ar);
enum cc_stat cc_array_iter_next(CC_ArrayIter *iter, void **out);
enum cc_stat cc_array_iter_remove(CC_ArrayIter *iter, void **out);
enum cc_stat cc_array_iter_remove_fast(CC_ArrayIter *iter, void **out);
enum cc_stat cc_array_iter_add(CC_ArrayIter *iter, void *element);
enum cc_stat cc_array_iter_replace(CC_ArrayIter *iter, void *element, void **out);
size_t cc_array_iter_index(CC_ArrayIter *iter);

void cc_array_zip_iter_init(CC_ArrayZipIter *iter, CC_Array *a1, CC_Array *a2);
enum cc_stat cc_array_zip_iter_next(CC_ArrayZipIter *iter, void **out1, void **out2);
enum cc_stat cc_array_zip_iter_add(CC_ArrayZipIter *iter, void *e1, void *e2);
enum cc_stat cc_array_zip_iter_remove(CC_ArrayZipIter *iter, void **out1, void **out2);
enum cc_stat cc_array_zip_iter_replace(CC_ArrayZipIter *iter, void *e1, void *e2, void **out1, void **out2);
size_t cc_array_zip_iter_index(CC_ArrayZipIter *iter);

const void *const *cc_array_get_buffer(CC_Array *ar);

#define CC_ARRAY_FOREACH(val, array, body)                                               \
    {                                                                                    \
        CC_ArrayIter cc_array_iter_53d46d2a04458e7b;                                     \
        cc_array_iter_init(&cc_array_iter_53d46d2a04458e7b, array);                      \
        void *val;                                                                       \
        while (cc_array_iter_next(&cc_array_iter_53d46d2a04458e7b, &val) != CC_ITER_END) \
            body                                                                         \
    }

#define CC_ARRAY_FOREACH_ZIP(val1, val2, array1, array2, body)                                            \
    {                                                                                                     \
        CC_ArrayZipIter cc_array_zip_iter_ea08d3e52f25883b3;                                              \
        cc_array_zip_iter_init(&cc_array_zip_iter_ea08d3e52f25883b3, array1, array2);                     \
        void *val1;                                                                                       \
        void *val2;                                                                                       \
        while (cc_array_zip_iter_next(&cc_array_zip_iter_ea08d3e52f25883b3, &val1, &val2) != CC_ITER_END) \
            body                                                                                          \
    }

#define DEFAULT_CAPACITY 8
#define DEFAULT_EXPANSION_FACTOR 2

struct cc_array_s {
    size_t size;
    size_t capacity;
    float exp_factor;
    void **buffer;

    void *(*mem_alloc)(size_t size);
    void *(*mem_calloc)(size_t blocks, size_t size);
    void (*mem_free)(void *block);
};

static enum cc_stat expand_array_capacity(CC_Array *ar);

/**
 * Creates a new empty array and returns a status code.
 *
 * @param[out] out pointer to where the newly created CC_Array is to be stored
 *
 * @return CC_OK if the creation was successful, or CC_ERR_ALLOC if the
 * memory allocation for the new CC_Array structure failed.
 */
enum cc_stat cc_array_new(CC_Array **out) {
    CC_ArrayConf c;
    cc_array_conf_init(&c);
    return cc_array_new_conf(&c, out);
}

/**
 * Creates a new empty CC_Array based on the specified CC_ArrayConf struct and
 * returns a status code.
 *
 * The CC_Array is allocated using the allocators specified in the CC_ArrayConf
 * struct. The allocation may fail if underlying allocator fails. It may also
 * fail if the values of exp_factor and capacity in the CC_ArrayConf do not meet
 * the following condition: <code>exp_factor < (CC_MAX_ELEMENTS / capacity)</code>.
 *
 * @param[in] conf array configuration structure
 * @param[out] out pointer to where the newly created CC_Array is to be stored
 *
 * @return CC_OK if the creation was successful, CC_ERR_INVALID_CAPACITY if
 * the above mentioned condition is not met, or CC_ERR_ALLOC if the memory
 * allocation for the new CC_Array structure failed.
 */
enum cc_stat cc_array_new_conf(CC_ArrayConf const *const conf, CC_Array **out) {
    float ex;

    /* The expansion factor must be greater than one for the
     * array to grow */
    if (conf->exp_factor <= 1) {
        ex = DEFAULT_EXPANSION_FACTOR;
    } else {
        ex = conf->exp_factor;
    }

    /* Needed to avoid an integer overflow on the first resize and
     * to easily check for any future overflows. */
    if (!conf->capacity || ex >= CC_MAX_ELEMENTS / conf->capacity) {
        return CC_ERR_INVALID_CAPACITY;
    }

    CC_Array *ar = (CC_Array *)conf->mem_calloc(1, sizeof(CC_Array));

    if (!ar) {
        return CC_ERR_ALLOC;
    }

    void **buff = (void **)conf->mem_alloc(conf->capacity * sizeof(void *));

    if (!buff) {
        conf->mem_free(ar);
        return CC_ERR_ALLOC;
    }

    ar->buffer = buff;
    ar->exp_factor = ex;
    ar->capacity = conf->capacity;
    ar->mem_alloc = conf->mem_alloc;
    ar->mem_calloc = conf->mem_calloc;
    ar->mem_free = conf->mem_free;

    *out = ar;
    return CC_OK;
}

/**
 * Initializes the fields of the CC_ArrayConf struct to default values.
 *
 * @param[in, out] conf CC_ArrayConf structure that is being initialized
 */
void cc_array_conf_init(CC_ArrayConf *conf) {
    conf->exp_factor = DEFAULT_EXPANSION_FACTOR;
    conf->capacity = DEFAULT_CAPACITY;
    conf->mem_alloc = malloc;
    conf->mem_calloc = calloc;
    conf->mem_free = free;
}

/**
 * Destroys the CC_Array structure, but leaves the data it used to hold intact.
 *
 * @param[in] ar the array that is to be destroyed
 */
void cc_array_destroy(CC_Array *ar) {
    ar->mem_free(ar->buffer);
    ar->mem_free(ar);
}

/**
 * Destroys the CC_Array structure along with all the data it holds.
 *
 * @note
 * This function should not be called on a array that has some of its elements
 * allocated on the stack.
 *
 * @param[in] ar the array that is being destroyed
 */
void cc_array_destroy_cb(CC_Array *ar, void (*cb)(void *)) {
    size_t i;
    for (i = 0; i < ar->size; i++) {
        cb(ar->buffer[i]);
    }

    cc_array_destroy(ar);
}

/**
 * Adds a new element to the CC_Array. The element is appended to the array making
 * it the last element (the one with the highest index) of the CC_Array.
 *
 * @param[in] ar the array to which the element is being added
 * @param[in] element the element that is being added
 *
 * @return CC_OK if the element was successfully added, CC_ERR_ALLOC if the
 * memory allocation for the new element failed, or CC_ERR_MAX_CAPACITY if the
 * array is already at maximum capacity.
 */
enum cc_stat cc_array_add(CC_Array *ar, void *element) {
    if (ar->size >= ar->capacity) {
        enum cc_stat status = expand_array_capacity(ar);
        if (status != CC_OK) {
            return status;
        }
    }

    ar->buffer[ar->size] = element;
    ar->size++;

    return CC_OK;
}

/**
 * Adds a new element to the array at a specified position by shifting all
 * subsequent elements by one. The specified index must be within the bounds
 * of the array. This function may also fail if the memory allocation for
 * the new element was unsuccessful.
 *
 * @param[in] ar the array to which the element is being added
 * @param[in] element the element that is being added
 * @param[in] index the position in the array at which the element is being
 *            added
 *
 * @return CC_OK if the element was successfully added, CC_ERR_OUT_OF_RANGE if
 * the specified index was not in range, CC_ERR_ALLOC if the memory
 * allocation for the new element failed, or CC_ERR_MAX_CAPACITY if the
 * array is already at maximum capacity.
 */
enum cc_stat cc_array_add_at(CC_Array *ar, void *element, size_t index) {
    if (index == ar->size) {
        return cc_array_add(ar, element);
    }

    if ((ar->size == 0 && index != 0) || index > (ar->size - 1)) {
        return CC_ERR_OUT_OF_RANGE;
    }

    if (ar->size >= ar->capacity) {
        enum cc_stat status = expand_array_capacity(ar);
        if (status != CC_OK) {
            return status;
        }
    }

    size_t shift = (ar->size - index) * sizeof(void *);

    memmove(&(ar->buffer[index + 1]),
            &(ar->buffer[index]),
            shift);

    ar->buffer[index] = element;
    ar->size++;

    return CC_OK;
}

/**
 * Replaces an array element at the specified index and optionally sets the out
 * parameter to the value of the replaced element. The specified index must be
 * within the bounds of the CC_Array.
 *
 * @param[in]  ar      array whose element is being replaced
 * @param[in]  element replacement element
 * @param[in]  index   index at which the replacement element should be inserted
 * @param[out] out     pointer to where the replaced element is stored, or NULL if
 *                     it is to be ignored
 *
 * @return CC_OK if the element was successfully replaced, or CC_ERR_OUT_OF_RANGE
 *         if the index was out of range.
 */
enum cc_stat cc_array_replace_at(CC_Array *ar, void *element, size_t index, void **out) {
    if (index >= ar->size) {
        return CC_ERR_OUT_OF_RANGE;
    }

    if (out) {
        *out = ar->buffer[index];
    }

    ar->buffer[index] = element;

    return CC_OK;
}

enum cc_stat cc_array_swap_at(CC_Array *ar, size_t index1, size_t index2) {
    void *tmp;

    if (index1 >= ar->size || index2 >= ar->size) {
        return CC_ERR_OUT_OF_RANGE;
    }

    tmp = ar->buffer[index1];

    ar->buffer[index1] = ar->buffer[index2];
    ar->buffer[index2] = tmp;
    return CC_OK;
}

/**
 * Removes the specified element from the CC_Array if such element exists and
 * optionally sets the out parameter to the value of the removed element.
 *
 * @param[in] ar array from which the element is being removed
 * @param[in] element element being removed
 * @param[out] out pointer to where the removed value is stored, or NULL
 *                 if it is to be ignored
 *
 * @return CC_OK if the element was successfully removed, or
 * CC_ERR_VALUE_NOT_FOUND if the element was not found.
 */
enum cc_stat cc_array_remove(CC_Array *ar, void *element, void **out) {
    size_t index;
    enum cc_stat status = cc_array_index_of(ar, element, &index);

    if (status == CC_ERR_OUT_OF_RANGE) {
        return CC_ERR_VALUE_NOT_FOUND;
    }

    if (index != ar->size - 1) {
        size_t block_size = (ar->size - 1 - index) * sizeof(void *);

        memmove(&(ar->buffer[index]),
                &(ar->buffer[index + 1]),
                block_size);
    }
    ar->size--;

    if (out) {
        *out = element;
    }

    return CC_OK;
}

/**
 * Removes a CC_Array element without preserving order and optionally sets the
 * out parameter to the value of the removed element. The last element of the
 * array is moved to the index of the element being removed, and the last
 * element is removed.
 *
 * @param[in] ar the array whose last element is being removed
 * @param[out] out pointer to where the removed value is stored, or NULL if it is
 *                 to be ignored
 *
 * @return CC_OK if the element was successfully removed, or CC_ERR_OUT_OF_RANGE
 * if the CC_Array is already empty.
 */
enum cc_stat cc_array_remove_fast(CC_Array *ar, void *element, void **out) {
    size_t index = 0;
    const enum cc_stat status = cc_array_index_of(ar, element, &index);
    if (status != CC_OK) {
        return status;
    }

    if (out) {
        *out = ar->buffer[index];
    }

    ar->buffer[index] = ar->buffer[ar->size - 1];
    ar->size--;

    return CC_OK;
}

/**
 * Removes an CC_Array element from the specified index and optionally sets the
 * out parameter to the value of the removed element. The index must be within
 * the bounds of the array.
 *
 * @param[in] ar the array from which the element is being removed
 * @param[in] index the index of the element being removed.
 * @param[out] out  pointer to where the removed value is stored,
 *                  or NULL if it is to be ignored
 *
 * @return CC_OK if the element was successfully removed, or CC_ERR_OUT_OF_RANGE
 * if the index was out of range.
 */
enum cc_stat cc_array_remove_at(CC_Array *ar, size_t index, void **out) {
    if (index >= ar->size) {
        return CC_ERR_OUT_OF_RANGE;
    }

    if (out) {
        *out = ar->buffer[index];
    }

    if (index != ar->size - 1) {
        size_t block_size = (ar->size - 1 - index) * sizeof(void *);

        memmove(&(ar->buffer[index]),
                &(ar->buffer[index + 1]),
                block_size);
    }
    ar->size--;

    return CC_OK;
}

/**
 * Removes a CC_Array element from the specified index and optionally sets the
 * out parameter to the value of the removed element without preserving ordering.
 * The last element of the array is moved to the index of the element being removed,
 * and the last element is removed. The index must be within the bounds of the array.
 *
 * @param[in] ar the array from which the element is being removed
 * @param[in] index the index of the element being removed.
 * @param[out] out  pointer to where the removed value is stored,
 *                  or NULL if it is to be ignored
 *
 * @return CC_OK if the element was successfully removed, or CC_ERR_OUT_OF_RANGE
 * if the index was out of range.
 */
enum cc_stat cc_array_remove_fast_at(CC_Array *ar, size_t index, void **out) {
    if (index >= ar->size) {
        return CC_ERR_OUT_OF_RANGE;
    }

    if (out) {
        *out = ar->buffer[index];
    }

    ar->buffer[index] = ar->buffer[ar->size - 1];
    ar->size--;

    return CC_OK;
}

/**
 * Removes an CC_Array element from the end of the array and optionally sets the
 * out parameter to the value of the removed element.
 *
 * @param[in] ar the array whose last element is being removed
 * @param[out] out pointer to where the removed value is stored, or NULL if it is
 *                 to be ignored
 *
 * @return CC_OK if the element was successfully removed, or CC_ERR_OUT_OF_RANGE
 * if the CC_Array is already empty.
 */
enum cc_stat cc_array_remove_last(CC_Array *ar, void **out) {
    return cc_array_remove_at(ar, ar->size - 1, out);
}

/**
 * Removes all elements from the specified array. This function does not shrink
 * the array capacity.
 *
 * @param[in] ar array from which all elements are to be removed
 */
void cc_array_remove_all(CC_Array *ar) {
    ar->size = 0;
}

/**
 * Removes and frees all elements from the specified array. This function does
 * not shrink the array capacity.
 *
 * @param[in] ar array from which all elements are to be removed
 */
void cc_array_remove_all_free(CC_Array *ar) {
    size_t i;
    for (i = 0; i < ar->size; i++) {
        free(ar->buffer[i]);
    }

    cc_array_remove_all(ar);
}

/**
 * Gets an CC_Array element from the specified index and sets the out parameter to
 * its value. The specified index must be within the bounds of the array.
 *
 * @param[in] ar the array from which the element is being retrieved
 * @param[in] index the index of the array element
 * @param[out] out pointer to where the element is stored
 *
 * @return CC_OK if the element was found, or CC_ERR_OUT_OF_RANGE if the index
 * was out of range.
 */
enum cc_stat cc_array_get_at(const CC_Array *ar, size_t index, void **out) {
    if (index >= ar->size) {
        return CC_ERR_OUT_OF_RANGE;
    }

    *out = ar->buffer[index];
    return CC_OK;
}

/**
 * Gets the last element of the array or the element at the highest index
 * and sets the out parameter to its value.
 *
 * @param[in] ar the array whose last element is being returned
 * @param[out] out pointer to where the element is stored
 *
 * @return CC_OK if the element was found, or CC_ERR_VALUE_NOT_FOUND if the
 * CC_Array is empty.
 */
enum cc_stat cc_array_get_last(const CC_Array *ar, void **out) {
    if (ar->size == 0) {
        return CC_ERR_VALUE_NOT_FOUND;
    }

    return cc_array_get_at(ar, ar->size - 1, out);
}

/**
 * Returns the underlying array buffer.
 *
 * @note Any direct modification of the buffer may invalidate the CC_Array.
 *
 * @param[in] ar array whose underlying buffer is being returned
 *
 * @return array's internal buffer.
 */
const void *const *cc_array_get_buffer(CC_Array *ar) {
    return (const void *const *)ar->buffer;
}

/**
 * Gets the index of the specified element. The returned index is the index
 * of the first occurrence of the element starting from the beginning of the
 * CC_Array.
 *
 * @param[in] ar array being searched
 * @param[in] element the element whose index is being looked up
 * @param[out] index  pointer to where the index is stored
 *
 * @return CC_OK if the index was found, or CC_OUT_OF_RANGE if not.
 */
enum cc_stat cc_array_index_of(const CC_Array *ar, void *element, size_t *index) {
    size_t i;
    for (i = 0; i < ar->size; i++) {
        if (ar->buffer[i] == element) {
            *index = i;
            return CC_OK;
        }
    }
    return CC_ERR_OUT_OF_RANGE;
}

/**
 * Creates a subarray of the specified CC_Array, ranging from <code>b</code>
 * index (inclusive) to <code>e</code> index (inclusive). The range indices
 * must be within the bounds of the CC_Array, while the <code>e</code> index
 * must be greater or equal to the <code>b</code> index.
 *
 * @note The new CC_Array is allocated using the original CC_Array's allocators
 *       and it also inherits the configuration of the original CC_Array.
 *
 * @param[in] ar array from which the subarray is being created
 * @param[in] b the beginning index (inclusive) of the subarray that must be
 *              within the bounds of the array and must not exceed the
 *              the end index
 * @param[in] e the end index (inclusive) of the subarray that must be within
 *              the bounds of the array and must be greater or equal to the
 *              beginning index
 * @param[out] out pointer to where the new sublist is stored
 *
 * @return CC_OK if the subarray was successfully created, CC_ERR_INVALID_RANGE
 * if the specified index range is invalid, or CC_ERR_ALLOC if the memory allocation
 * for the new subarray failed.
 */
enum cc_stat cc_array_subarray(CC_Array *ar, size_t b, size_t e, CC_Array **out) {
    if (b > e || e >= ar->size) {
        return CC_ERR_INVALID_RANGE;
    }

    CC_Array *sub_ar = (CC_Array *)ar->mem_calloc(1, sizeof(CC_Array));

    if (!sub_ar) {
        return CC_ERR_ALLOC;
    }

    /* Try to allocate the buffer */
    if (!(sub_ar->buffer = (void **)ar->mem_alloc(ar->capacity * sizeof(void *)))) {
        ar->mem_free(sub_ar);
        return CC_ERR_ALLOC;
    }

    sub_ar->mem_alloc = ar->mem_alloc;
    sub_ar->mem_calloc = ar->mem_calloc;
    sub_ar->mem_free = ar->mem_free;
    sub_ar->size = e - b + 1;
    sub_ar->capacity = sub_ar->size;

    memcpy(sub_ar->buffer,
           &(ar->buffer[b]),
           sub_ar->size * sizeof(void *));

    *out = sub_ar;
    return CC_OK;
}

/**
 * Creates a shallow copy of the specified CC_Array. A shallow copy is a copy of
 * the CC_Array structure, but not the elements it holds.
 *
 * @note The new CC_Array is allocated using the original CC_Array's allocators
 *       and it also inherits the configuration of the original array.
 *
 * @param[in] ar the array to be copied
 * @param[out] out pointer to where the newly created copy is stored
 *
 * @return CC_OK if the copy was successfully created, or CC_ERR_ALLOC if the
 * memory allocation for the copy failed.
 */
enum cc_stat cc_array_copy_shallow(CC_Array *ar, CC_Array **out) {
    CC_Array *copy = (CC_Array *)ar->mem_alloc(sizeof(CC_Array));

    if (!copy) {
        return CC_ERR_ALLOC;
    }

    if (!(copy->buffer = (void **)ar->mem_calloc(ar->capacity, sizeof(void *)))) {
        ar->mem_free(copy);
        return CC_ERR_ALLOC;
    }
    copy->exp_factor = ar->exp_factor;
    copy->size = ar->size;
    copy->capacity = ar->capacity;
    copy->mem_alloc = ar->mem_alloc;
    copy->mem_calloc = ar->mem_calloc;
    copy->mem_free = ar->mem_free;

    memcpy(copy->buffer,
           ar->buffer,
           copy->size * sizeof(void *));

    *out = copy;
    return CC_OK;
}

/**
 * Creates a deep copy of the specified CC_Array. A deep copy is a copy of
 * both the CC_Array structure and the data it holds.
 *
 * @note The new CC_Array is allocated using the original CC_Array's allocators
 *       and it also inherits the configuration of the original CC_Array.
 *
 * @param[in] ar   array to be copied
 * @param[in] cp   the copy function that should return a pointer to the copy of
 *                 the data
 * @param[out] out pointer to where the newly created copy is stored
 *
 * @return CC_OK if the copy was successfully created, or CC_ERR_ALLOC if the
 * memory allocation for the copy failed.
 */
enum cc_stat cc_array_copy_deep(CC_Array *ar, void *(*cp)(void *), CC_Array **out) {
    CC_Array *copy = (CC_Array *)ar->mem_alloc(sizeof(CC_Array));

    if (!copy) {
        return CC_ERR_ALLOC;
    }

    if (!(copy->buffer = (void **)ar->mem_calloc(ar->capacity, sizeof(void *)))) {
        ar->mem_free(copy);
        return CC_ERR_ALLOC;
    }

    copy->exp_factor = ar->exp_factor;
    copy->size = ar->size;
    copy->capacity = ar->capacity;
    copy->mem_alloc = ar->mem_alloc;
    copy->mem_calloc = ar->mem_calloc;
    copy->mem_free = ar->mem_free;

    size_t i;
    for (i = 0; i < copy->size; i++) {
        copy->buffer[i] = cp(ar->buffer[i]);
    }

    *out = copy;

    return CC_OK;
}

/**
 * Filters the CC_Array by modifying it. It removes all elements that don't
 * return true on pred(element).
 *
 * @param[in] ar   array that is to be filtered
 * @param[in] pred predicate function which returns true if the element should
 *                 be kept in the CC_Array
 *
 * @return CC_OK if the CC_Array was filtered successfully, or CC_ERR_OUT_OF_RANGE
 * if the CC_Array is empty.
 */
enum cc_stat cc_array_filter_mut(CC_Array *ar, bool (*pred)(const void *)) {
    if (ar->size == 0) {
        return CC_ERR_OUT_OF_RANGE;
    }

    size_t rm = 0;
    size_t keep = 0;

    /* Look for clusters of non matching elements before moving
     * in order to minimize the number of memmoves */
    for (size_t i = ar->size - 1; i != ((size_t)-1); i--) {
        if (!pred(ar->buffer[i])) {
            rm++;
            continue;
        }
        if (rm > 0) {
            if (keep > 0) {
                size_t block_size = keep * sizeof(void *);
                memmove(&(ar->buffer[i + 1]),
                        &(ar->buffer[i + 1 + rm]),
                        block_size);
            }
            ar->size -= rm;
            rm = 0;
        }
        keep++;
    }
    /* Remove any remaining elements*/
    if (rm > 0) {
        size_t block_size = keep * sizeof(void *);
        memmove(&(ar->buffer[0]),
                &(ar->buffer[rm]),
                block_size);

        ar->size -= rm;
    }
    return CC_OK;
}

/**
 * Filters the CC_Array by creating a new CC_Array that contains all elements from the
 * original CC_Array that return true on pred(element) without modifying the original
 * CC_Array.
 *
 * @param[in] ar   array that is to be filtered
 * @param[in] pred predicate function which returns true if the element should
 *                 be kept in the filtered array
 * @param[out] out pointer to where the new filtered CC_Array is to be stored
 *
 * @return CC_OK if the CC_Array was filtered successfully, CC_ERR_OUT_OF_RANGE
 * if the CC_Array is empty, or CC_ERR_ALLOC if the memory allocation for the
 * new CC_Array failed.
 */
enum cc_stat cc_array_filter(CC_Array *ar, bool (*pred)(const void *), CC_Array **out) {
    if (ar->size == 0) {
        return CC_ERR_OUT_OF_RANGE;
    }

    CC_Array *filtered = (CC_Array *)ar->mem_alloc(sizeof(CC_Array));

    if (!filtered) {
        return CC_ERR_ALLOC;
    }

    if (!(filtered->buffer = (void **)ar->mem_calloc(ar->capacity, sizeof(void *)))) {
        ar->mem_free(filtered);
        return CC_ERR_ALLOC;
    }

    filtered->exp_factor = ar->exp_factor;
    filtered->size = 0;
    filtered->capacity = ar->capacity;
    filtered->mem_alloc = ar->mem_alloc;
    filtered->mem_calloc = ar->mem_calloc;
    filtered->mem_free = ar->mem_free;

    size_t f = 0;
    for (size_t i = 0; i < ar->size; i++) {
        if (pred(ar->buffer[i])) {
            filtered->buffer[f++] = ar->buffer[i];
            filtered->size++;
        }
    }
    *out = filtered;

    return CC_OK;
}

/**
 * Reverses the order of elements in the specified array.
 *
 * @param[in] ar array that is being reversed
 */
void cc_array_reverse(CC_Array *ar) {
    if (ar->size == 0) {
        return;
    }

    size_t i;
    size_t j;
    for (i = 0, j = ar->size - 1; i < ar->size / 2; i++, j--) {
        void *tmp = ar->buffer[i];
        ar->buffer[i] = ar->buffer[j];
        ar->buffer[j] = tmp;
    }
}

/**
 * Trims the array's capacity, in other words, it shrinks the capacity to match
 * the number of elements in the CC_Array, however the capacity will never shrink
 * below 1.
 *
 * @param[in] ar array whose capacity is being trimmed
 *
 * @return CC_OK if the capacity was trimmed successfully, or CC_ERR_ALLOC if
 * the reallocation failed.
 */
enum cc_stat cc_array_trim_capacity(CC_Array *ar) {
    if (ar->size == ar->capacity) {
        return CC_OK;
    }

    void **new_buff = (void **)ar->mem_calloc(ar->size, sizeof(void *));

    if (!new_buff) {
        return CC_ERR_ALLOC;
    }

    size_t size = ar->size < 1 ? 1 : ar->size;

    memcpy(new_buff, ar->buffer, size * sizeof(void *));
    ar->mem_free(ar->buffer);

    ar->buffer = new_buff;
    ar->capacity = ar->size;

    return CC_OK;
}

/**
 * Returns the number of occurrences of the element within the specified CC_Array.
 *
 * @param[in] ar array that is being searched
 * @param[in] element the element that is being searched for
 *
 * @return the number of occurrences of the element.
 */
size_t cc_array_contains(const CC_Array *ar, void *element) {
    size_t o = 0;
    size_t i;
    for (i = 0; i < ar->size; i++) {
        if (ar->buffer[i] == element) {
            o++;
        }
    }
    return o;
}

/**
 * Returns the number of occurrences of the value pointed to by <code>e</code>
 * within the specified CC_Array.
 *
 * @param[in] ar array that is being searched
 * @param[in] element the element that is being searched for
 * @param[in] cmp comparator function which returns 0 if the values passed to it are equal
 *
 * @return the number of occurrences of the value.
 */
size_t cc_array_contains_value(const CC_Array *ar, void *element, int (*cmp)(const void *, const void *)) {
    size_t o = 0;
    size_t i;
    for (i = 0; i < ar->size; i++) {
        if (cmp(element, ar->buffer[i]) == 0) {
            o++;
        }
    }
    return o;
}

/**
 * Returns the size of the specified CC_Array. The size of the array is the
 * number of elements contained within the CC_Array.
 *
 * @param[in] ar array whose size is being returned
 *
 * @return the the number of element within the CC_Array.
 */
size_t cc_array_size(const CC_Array *ar) {
    return ar->size;
}

/**
 * Returns the capacity of the specified CC_Array. The capacity of the CC_Array is
 * the maximum number of elements an CC_Array can hold before it has to be resized.
 *
 * @param[in] ar array whose capacity is being returned
 *
 * @return the capacity of the CC_Array.
 */
size_t cc_array_capacity(const CC_Array *ar) {
    return ar->capacity;
}

/**
 * Sorts the specified array.
 *
 * @note
 * Pointers passed to the comparator function will be pointers to the array
 * elements that are of type (void*) ie. void**. So an extra step of
 * dereferencing will be required before the data can be used for comparison:
 * eg. <code>my_type e = *(*((my_type**) ptr));</code>.
 *
 * @code
 * enum cc_stat mycmp(const void *e1, const void *e2) {
 *     MyType el1 = *(*((enum cc_stat**) e1));
 *     MyType el2 = *(*((enum cc_stat**) e2));
 *
 *     if (el1 < el2) return -1;
 *     if (el1 > el2) return 1;
 *     return 0;
 * }
 *
 * ...
 *
 * cc_array_sort(array, mycmp);
 * @endcode
 *
 * @param[in] ar  array to be sorted
 * @param[in] cmp the comparator function that must be of type <code>
 *                enum cc_stat cmp(const void e1*, const void e2*)</code> that
 *                returns < 0 if the first element goes before the second,
 *                0 if the elements are equal and > 0 if the second goes
 *                before the first
 */
void cc_array_sort(CC_Array *ar, int (*cmp)(const void *, const void *)) {
    qsort(ar->buffer, ar->size, sizeof(void *), cmp);
}

/**
 * Expands the CC_Array capacity. This might fail if the the new buffer
 * cannot be allocated. In case the expansion would overflow the index
 * range, a maximum capacity buffer is allocated instead. If the capacity
 * is already at the maximum capacity, no new buffer is allocated.
 *
 * @param[in] ar array whose capacity is being expanded
 *
 * @return CC_OK if the buffer was expanded successfully, CC_ERR_ALLOC if
 * the memory allocation for the new buffer failed, or CC_ERR_MAX_CAPACITY
 * if the array is already at maximum capacity.
 */
static enum cc_stat expand_array_capacity(CC_Array *ar) {
    if (ar->capacity == CC_MAX_ELEMENTS) {
        return CC_ERR_MAX_CAPACITY;
    }

    size_t new_capacity = (size_t)(ar->capacity * ar->exp_factor);

    /* As long as the capacity is greater that the expansion factor
     * at the point of overflow, this is check is valid. */
    if (new_capacity <= ar->capacity) {
        ar->capacity = CC_MAX_ELEMENTS;
    } else {
        ar->capacity = new_capacity;
    }

    void **new_buff = (void **)ar->mem_alloc(ar->capacity * sizeof(void *));

    if (!new_buff) {
        return CC_ERR_ALLOC;
    }

    memcpy(new_buff, ar->buffer, ar->size * sizeof(void *));

    ar->mem_free(ar->buffer);
    ar->buffer = new_buff;

    return CC_OK;
}

/**
 * Applies the function fn to each element of the CC_Array.
 *
 * @param[in] ar array on which this operation is performed
 * @param[in] fn operation function that is to be invoked on each CC_Array
 *               element
 */
void cc_array_map(CC_Array *ar, void (*fn)(void *e)) {
    size_t i;
    for (i = 0; i < ar->size; i++) {
        fn(ar->buffer[i]);
    }
}

/**
 * A fold/reduce function that collects all of the elements in the array
 * together. For example, if we have an array of [a,b,c...] the end result
 * will be (...((a+b)+c)+...).
 *
 * @param[in] ar the array on which this operation is performed
 * @param[in] fn the operation function that is to be invoked on each array
 *               element
 * @param[in] result the pointer which will collect the end result
 */
void cc_array_reduce(CC_Array *ar, void (*fn)(void *, void *, void *), void *result) {
    if (ar->size == 1) {
        fn(ar->buffer[0], NULL, result);
        return;
    }
    if (ar->size > 1) {
        fn(ar->buffer[0], ar->buffer[1], result);
    }

    for (size_t i = 2; i < ar->size; i++) {
        fn(result, ar->buffer[i], result);
    }
}

/**
 * Initializes the iterator.
 *
 * @param[in] iter the iterator that is being initialized
 * @param[in] ar the array to iterate over
 */
void cc_array_iter_init(CC_ArrayIter *iter, CC_Array *ar) {
    iter->ar = ar;
    iter->index = 0;
    iter->last_removed = false;
}

/**
 * Advances the iterator and sets the out parameter to the value of the
 * next element in the sequence.
 *
 * @param[in] iter the iterator that is being advanced
 * @param[out] out pointer to where the next element is set
 *
 * @return CC_OK if the iterator was advanced, or CC_ITER_END if the
 * end of the CC_Array has been reached.
 */
enum cc_stat cc_array_iter_next(CC_ArrayIter *iter, void **out) {
    if (iter->index >= iter->ar->size) {
        return CC_ITER_END;
    }

    *out = iter->ar->buffer[iter->index];

    iter->index++;
    iter->last_removed = false;

    return CC_OK;
}

/**
 * Removes the last returned element by <code>cc_array_iter_next()</code>
 * function without invalidating the iterator and optionally sets the out
 * parameter to the value of the removed element.
 *
 * @note This function should only ever be called after a call to <code>
 * cc_array_iter_next()</code>.

 * @param[in] iter the iterator on which this operation is being performed
 * @param[out] out pointer to where the removed element is stored, or NULL
 *                 if it is to be ignored
 *
 * @return CC_OK if the element was successfully removed, or
 * CC_ERR_VALUE_NOT_FOUND.
 */
enum cc_stat cc_array_iter_remove(CC_ArrayIter *iter, void **out) {
    enum cc_stat status = CC_ERR_VALUE_NOT_FOUND;

    if (!iter->last_removed) {
        status = cc_array_remove_at(iter->ar, iter->index - 1, out);
        if (status != CC_OK) {
            return status;
        }

        iter->last_removed = true;
        if (iter->index > 0) {
            iter->index--;
        }
    }
    return status;
}

/**
 * Removes the last returned element by <code>cc_array_iter_next()</code>
 * function without invalidating the iterator and optionally sets the out
 * parameter to the value of the removed element. The order of the array
 * is not preserved, the last element of the array is moved to the index
 * of the last returned element and the last element is removed.
 *
 * @note This function should only ever be called after a call to <code>
 * cc_array_iter_next()</code>.

 * @param[in] iter the iterator on which this operation is being performed
 * @param[out] out pointer to where the removed element is stored, or NULL
 *                 if it is to be ignored
 *
 * @return CC_OK if the element was successfully removed, or
 * CC_ERR_VALUE_NOT_FOUND.
 */
enum cc_stat cc_array_iter_remove_fast(CC_ArrayIter *iter, void **out) {
    enum cc_stat status = CC_ERR_VALUE_NOT_FOUND;

    if (!iter->last_removed) {
        status = cc_array_remove_fast_at(iter->ar, iter->index - 1, out);
        if (status != CC_OK) {
            return status;
        }

        iter->last_removed = true;
        if (iter->index > 0) {
            iter->index--;
        }
    }
    return status;
}

/**
 * Adds a new element to the CC_Array after the last returned element by
 * <code>cc_array_iter_next()</code> function without invalidating the
 * iterator.
 *
 * @note This function should only ever be called after a call to <code>
 * cc_array_iter_next()</code>.
 *
 * @param[in] iter the iterator on which this operation is being performed
 * @param[in] element the element being added
 *
 * @return CC_OK if the element was successfully added, CC_ERR_ALLOC if the
 * memory allocation for the new element failed, or CC_ERR_MAX_CAPACITY if
 * the array is already at maximum capacity.
 */
enum cc_stat cc_array_iter_add(CC_ArrayIter *iter, void *element) {
    return cc_array_add_at(iter->ar, element, iter->index++);
}

/**
 * Replaces the last returned element by <code>cc_array_iter_next()</code>
 * with the specified element and optionally sets the out parameter to
 * the value of the replaced element.
 *
 * @note This function should only ever be called after a call to <code>
 * cc_array_iter_next()</code>.
 *
 * @param[in] iter the iterator on which this operation is being performed
 * @param[in] element the replacement element
 * @param[out] out pointer to where the replaced element is stored, or NULL
 *                if it is to be ignored
 *
 * @return CC_OK if the element was replaced successfully, or
 * CC_ERR_OUT_OF_RANGE.
 */
enum cc_stat cc_array_iter_replace(CC_ArrayIter *iter, void *element, void **out) {
    return cc_array_replace_at(iter->ar, element, iter->index - 1, out);
}

/**
 * Returns the index of the last returned element by <code>cc_array_iter_next()
 * </code>.
 *
 * @note
 * This function should not be called before a call to <code>cc_array_iter_next()
 * </code>.
 *
 * @param[in] iter the iterator on which this operation is being performed
 *
 * @return the index.
 */
size_t cc_array_iter_index(CC_ArrayIter *iter) {
    return iter->index - 1;
}

/**
 * Initializes the zip iterator.
 *
 * @param[in] iter iterator that is being initialized
 * @param[in] ar1  first array
 * @param[in] ar2  second array
 */
void cc_array_zip_iter_init(CC_ArrayZipIter *iter, CC_Array *ar1, CC_Array *ar2) {
    iter->ar1 = ar1;
    iter->ar2 = ar2;
    iter->index = 0;
    iter->last_removed = false;
}

/**
 * Outputs the next element pair in the sequence and advances the iterator.
 *
 * @param[in]  iter iterator that is being advanced
 * @param[out] out1 output of the first array element
 * @param[out] out2 output of the second array element
 *
 * @return CC_OK if a next element pair is returned, or CC_ITER_END if the end of one
 * of the arrays has been reached.
 */
enum cc_stat cc_array_zip_iter_next(CC_ArrayZipIter *iter, void **out1, void **out2) {
    if (iter->index >= iter->ar1->size || iter->index >= iter->ar2->size) {
        return CC_ITER_END;
    }

    *out1 = iter->ar1->buffer[iter->index];
    *out2 = iter->ar2->buffer[iter->index];

    iter->index++;
    iter->last_removed = false;

    return CC_OK;
}

/**
 * Removes and outputs the last returned element pair by <code>cc_array_zip_iter_next()
 * </code> without invalidating the iterator.
 *
 * @param[in]  iter iterator on which this operation is being performed
 * @param[out] out1 output of the removed element from the first array
 * @param[out] out2 output of the removed element from the second array
 *
 * @return CC_OK if the element was successfully removed, CC_ERR_OUT_OF_RANGE if the
 * state of the iterator is invalid, or CC_ERR_VALUE_NOT_FOUND if the element was
 * already removed.
 */
enum cc_stat cc_array_zip_iter_remove(CC_ArrayZipIter *iter, void **out1, void **out2) {
    if ((iter->index - 1) >= iter->ar1->size || (iter->index - 1) >= iter->ar2->size) {
        return CC_ERR_OUT_OF_RANGE;
    }

    if (!iter->last_removed) {
        cc_array_remove_at(iter->ar1, iter->index - 1, out1);
        cc_array_remove_at(iter->ar2, iter->index - 1, out2);
        iter->last_removed = true;
        return CC_OK;
    }
    return CC_ERR_VALUE_NOT_FOUND;
}

/**
 * Adds a new element pair to the arrays after the last returned element pair by
 * <code>cc_array_zip_iter_next()</code> and immediately before an element pair
 * that would be returned by a subsequent call to <code>cc_array_zip_iter_next()</code>
 * without invalidating the iterator.
 *
 * @param[in] iter iterator on which this operation is being performed
 * @param[in] e1   element added to the first array
 * @param[in] e2   element added to the second array
 *
 * @return CC_OK if the element pair was successfully added to the arrays, or
 * CC_ERR_ALLOC if the memory allocation for the new elements failed.
 */
enum cc_stat cc_array_zip_iter_add(CC_ArrayZipIter *iter, void *e1, void *e2) {
    size_t index = iter->index++;
    CC_Array *ar1 = iter->ar1;
    CC_Array *ar2 = iter->ar2;

    /* Make sure both array buffers have room */
    if ((ar1->size == ar1->capacity && (expand_array_capacity(ar1) != CC_OK)) ||
        (ar2->size == ar2->capacity && (expand_array_capacity(ar2) != CC_OK))) {
        return CC_ERR_ALLOC;
    }

    cc_array_add_at(ar1, e1, index);
    cc_array_add_at(ar2, e2, index);

    return CC_OK;
}

/**
 * Replaces the last returned element pair by <code>cc_array_zip_iter_next()</code>
 * with the specified replacement element pair.
 *
 * @param[in] iter  iterator on which this operation is being performed
 * @param[in]  e1   first array's replacement element
 * @param[in]  e2   second array's replacement element
 * @param[out] out1 output of the replaced element from the first array
 * @param[out] out2 output of the replaced element from the second array
 *
 * @return CC_OK if the element was successfully replaced, or CC_ERR_OUT_OF_RANGE.
 */
enum cc_stat cc_array_zip_iter_replace(CC_ArrayZipIter *iter, void *e1, void *e2, void **out1, void **out2) {
    if ((iter->index - 1) >= iter->ar1->size || (iter->index - 1) >= iter->ar2->size) {
        return CC_ERR_OUT_OF_RANGE;
    }

    cc_array_replace_at(iter->ar1, e1, iter->index - 1, out1);
    cc_array_replace_at(iter->ar2, e2, iter->index - 1, out2);

    return CC_OK;
}

/**
 * Returns the index of the last returned element pair by <code>cc_array_zip_iter_next()</code>.
 *
 * @param[in] iter iterator on which this operation is being performed
 *
 * @return current iterator index.
 */
size_t cc_array_zip_iter_index(CC_ArrayZipIter *iter) {
    return iter->index - 1;
}

size_t cc_array_struct_size() {
    return sizeof(CC_Array);
}

#ifdef __cplusplus
}
#endif

#endif



================================================
FILE: pufferlib/ocean/impulse_wars/include/cc_common.h
================================================
/*
 * Collections-C
 * Copyright (C) 2013-2014 Srđan Panić <srdja.panic@gmail.com>
 *
 * This file is part of Collections-C.
 *
 * Collections-C is free software: you can redistribute it and/or modify
 * it under the terms of the GNU Lesser General Public License as published by
 * the Free Software Foundation, either version 3 of the License, or
 * (at your option) any later version.
 *
 * Collections-C is distributed in the hope that it will be useful,
 * but WITHOUT ANY WARRANTY; without even the implied warranty of
 * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
 * GNU Lesser General Public License for more details.
 *
 * You should have received a copy of the GNU Lesser General Public License
 * along with Collections-C.  If not, see <http://www.gnu.org/licenses/>.
 */

#ifndef CC_COMMON_H
#define CC_COMMON_H

#ifdef __cplusplus
extern "C" {
#endif

#include <stdbool.h>
#include <stdint.h>
#include <stdlib.h>
#include <string.h>

#ifdef ARCH_64
#define MAX_POW_TWO (((size_t)1) << 63)
#else
#define MAX_POW_TWO (((size_t)1) << 31)
#endif /* ARCH_64 */

enum cc_stat {
    CC_OK = 0,

    CC_ERR_ALLOC = 1,
    CC_ERR_INVALID_CAPACITY = 2,
    CC_ERR_INVALID_RANGE = 3,
    CC_ERR_MAX_CAPACITY = 4,
    CC_ERR_KEY_NOT_FOUND = 6,
    CC_ERR_VALUE_NOT_FOUND = 7,
    CC_ERR_OUT_OF_RANGE = 8,

    CC_ITER_END = 9,
};

#define CC_MAX_ELEMENTS ((size_t) - 2)

#if defined(_MSC_VER)

#define INLINE __inline
#define FORCE_INLINE __forceinline

#else

#define INLINE inline
#define FORCE_INLINE inline __attribute__((always_inline))

#endif /* _MSC_VER */

int cc_common_cmp_str(const void *key1, const void *key2);

#define CC_CMP_STRING cc_common_cmp_str

#ifdef __cplusplus
}
#endif

#endif



================================================
FILE: pufferlib/ocean/impulse_wars/include/rlights.h
================================================
/**********************************************************************************************
*
*   raylib.lights - Some useful functions to deal with lights data
*
*   CONFIGURATION:
*
*   #define RLIGHTS_IMPLEMENTATION
*       Generates the implementation of the library into the included file.
*       If not defined, the library is in header only mode and can be included in other headers 
*       or source files without problems. But only ONE file should hold the implementation.
*
*   LICENSE: zlib/libpng
*
*   Copyright (c) 2017-2024 Victor Fisac (@victorfisac) and Ramon Santamaria (@raysan5)
*
*   This software is provided "as-is", without any express or implied warranty. In no event
*   will the authors be held liable for any damages arising from the use of this software.
*
*   Permission is granted to anyone to use this software for any purpose, including commercial
*   applications, and to alter it and redistribute it freely, subject to the following restrictions:
*
*     1. The origin of this software must not be misrepresented; you must not claim that you
*     wrote the original software. If you use this software in a product, an acknowledgment
*     in the product documentation would be appreciated but is not required.
*
*     2. Altered source versions must be plainly marked as such, and must not be misrepresented
*     as being the original software.
*
*     3. This notice may not be removed or altered from any source distribution.
*
**********************************************************************************************/

#ifndef RLIGHTS_H
#define RLIGHTS_H

//----------------------------------------------------------------------------------
// Defines and Macros
//----------------------------------------------------------------------------------
#define MAX_LIGHTS  64         // Max dynamic lights supported by shader

//----------------------------------------------------------------------------------
// Types and Structures Definition
//----------------------------------------------------------------------------------

// Light data
typedef struct {   
    int type;
    bool enabled;
    Vector3 position;
    Vector3 target;
    Color color;
    float attenuation;
    
    // Shader locations
    int enabledLoc;
    int typeLoc;
    int positionLoc;
    int targetLoc;
    int colorLoc;
    int attenuationLoc;
} Light;

// Light type
typedef enum {
    LIGHT_DIRECTIONAL = 0,
    LIGHT_POINT
} LightType;

#ifdef __cplusplus
extern "C" {            // Prevents name mangling of functions
#endif

//----------------------------------------------------------------------------------
// Module Functions Declaration
//----------------------------------------------------------------------------------
Light CreateLight(int type, Vector3 position, Vector3 target, Color color, Shader shader);   // Create a light and get shader locations
void UpdateLightValues(Shader shader, Light light);         // Send light properties to shader

#ifdef __cplusplus
}
#endif

#endif // RLIGHTS_H


/***********************************************************************************
*
*   RLIGHTS IMPLEMENTATION
*
************************************************************************************/

#if defined(RLIGHTS_IMPLEMENTATION)

#include "raylib.h"

//----------------------------------------------------------------------------------
// Defines and Macros
//----------------------------------------------------------------------------------
// ...

//----------------------------------------------------------------------------------
// Types and Structures Definition
//----------------------------------------------------------------------------------
// ...

//----------------------------------------------------------------------------------
// Global Variables Definition
//----------------------------------------------------------------------------------
static int lightsCount = 0;    // Current amount of created lights

//----------------------------------------------------------------------------------
// Module specific Functions Declaration
//----------------------------------------------------------------------------------
// ...

//----------------------------------------------------------------------------------
// Module Functions Definition
//----------------------------------------------------------------------------------

// Create a light and get shader locations
Light CreateLight(int type, Vector3 position, Vector3 target, Color color, Shader shader)
{
    Light light = { 0 };

    if (lightsCount < MAX_LIGHTS)
    {
        light.enabled = true;
        light.type = type;
        light.position = position;
        light.target = target;
        light.color = color;

        // NOTE: Lighting shader naming must be the provided ones
        light.enabledLoc = GetShaderLocation(shader, TextFormat("lights[%i].enabled", lightsCount));
        light.typeLoc = GetShaderLocation(shader, TextFormat("lights[%i].type", lightsCount));
        light.positionLoc = GetShaderLocation(shader, TextFormat("lights[%i].position", lightsCount));
        light.targetLoc = GetShaderLocation(shader, TextFormat("lights[%i].target", lightsCount));
        light.colorLoc = GetShaderLocation(shader, TextFormat("lights[%i].color", lightsCount));

        UpdateLightValues(shader, light);
        
        lightsCount++;
    }

    return light;
}

// Send light properties to shader
// NOTE: Light shader locations should be available 
void UpdateLightValues(Shader shader, Light light)
{
    // Send to shader light enabled state and type
    SetShaderValue(shader, light.enabledLoc, &light.enabled, SHADER_UNIFORM_INT);
    SetShaderValue(shader, light.typeLoc, &light.type, SHADER_UNIFORM_INT);

    // Send to shader light position values
    float position[3] = { light.position.x, light.position.y, light.position.z };
    SetShaderValue(shader, light.positionLoc, position, SHADER_UNIFORM_VEC3);

    // Send to shader light target position values
    float target[3] = { light.target.x, light.target.y, light.target.z };
    SetShaderValue(shader, light.targetLoc, target, SHADER_UNIFORM_VEC3);

    // Send to shader light color values
    float color[4] = { (float)light.color.r/(float)255, (float)light.color.g/(float)255, 
                       (float)light.color.b/(float)255, (float)light.color.a/(float)255 };
    SetShaderValue(shader, light.colorLoc, color, SHADER_UNIFORM_VEC4);
}

#endif // RLIGHTS_IMPLEMENTATION



================================================
FILE: pufferlib/ocean/matsci/binding.c
================================================
#include "matsci.h"

#define Env Matsci 
#include "../env_binding.h"

static int my_init(Env* env, PyObject* args, PyObject* kwargs) {
    env->num_agents = unpack(kwargs, "num_agents");
    init(env);
    return 0;
}

static int my_log(PyObject* dict, Log* log) {
    assign_to_dict(dict, "score", log->score);
    return 0;
}



================================================
FILE: pufferlib/ocean/matsci/matsci.c
================================================
#include "matsci.h"

int main() {
    int num_agents = 16;
    Matsci env = {.num_agents=num_agents};
    env.observations = (float*)calloc(3*num_agents, sizeof(float));
    env.actions = (float*)calloc(3*num_agents, sizeof(float));
    env.rewards = (float*)calloc(num_agents, sizeof(float));
    env.terminals = (unsigned char*)calloc(num_agents, sizeof(unsigned char));
    init(&env);

    c_reset(&env);
    c_render(&env);
    while (!WindowShouldClose()) {
	for (int i=0; i<3*num_agents; i++) {
            env.actions[i] = rndf(-1.0f, 1.0f);
	}
        c_step(&env);
        c_render(&env);
    }
    free(env.observations);
    free(env.actions);
    free(env.rewards);
    free(env.terminals);
    c_close(&env);
}




================================================
FILE: pufferlib/ocean/matsci/matsci.h
================================================
#include <stdlib.h>
#include <string.h>
#include <stdio.h>
#include <math.h>
#include <lammps/library.h>
#include "raylib.h"

#define WIDTH 1080
#define HEIGHT 720

const Color PUFF_RED = (Color){187, 0, 0, 255};
const Color PUFF_CYAN = (Color){0, 187, 187, 255};
const Color PUFF_WHITE = (Color){241, 241, 241, 241};
const Color PUFF_BACKGROUND = (Color){6, 24, 24, 255};

typedef struct {
    float x, y, z;
} Vec3;

static inline float clampf(float v, float min, float max) {
    if (v < min)
        return min;
    if (v > max)
        return max;
    return v;
}

static inline float rndf(float a, float b) {
    return a + ((float)rand() / (float)RAND_MAX) * (b - a);
}

static inline Vec3 add3(Vec3 a, Vec3 b) { return (Vec3){a.x + b.x, a.y + b.y, a.z + b.z}; }

static inline Vec3 sub3(Vec3 a, Vec3 b) { return (Vec3){a.x - b.x, a.y - b.y, a.z - b.z}; }

static inline Vec3 scalmul3(Vec3 a, float b) { return (Vec3){a.x * b, a.y * b, a.z * b}; }

static inline float dot3(Vec3 a, Vec3 b) { return a.x * b.x + a.y * b.y + a.z * b.z; }

static inline float norm3(Vec3 a) { return sqrtf(dot3(a, a)); }

static inline void clamp3(Vec3 *vec, float min, float max) {
    vec->x = clampf(vec->x, min, max);
    vec->y = clampf(vec->y, min, max);
    vec->z = clampf(vec->z, min, max);
}


// Only use floats!
typedef struct {
    float score;
    float n; // Required as the last field 
} Log;

typedef struct {
    Camera3D camera;
    float width;
    float height;

    float camera_distance;
    float camera_azimuth;
    float camera_elevation;
    bool is_dragging;
    Vector2 last_mouse_pos;
} Client;

typedef struct {
    Log log;                     // Required field
    float* observations;         // Required field. Ensure type matches in .py and .c
    float* actions;              // Required field. Ensure type matches in .py and .c
    float* rewards;              // Required field
    unsigned char* terminals;    // Required field
    int num_agents;
    Vec3 goal;
    int tick;
    Client* client;
    void* handle;
} Matsci;

void init(Matsci* env) {
  void *handle;
  const char *lmpargv[] = { "liblammps", "-log", "none", "-screen", "none"};
  int lmpargc = sizeof(lmpargv)/sizeof(const char *);

  /* create LAMMPS instance */
  handle = lammps_open_no_mpi(lmpargc, (char **)lmpargv, NULL);
  if (handle == NULL) {
    printf("LAMMPS initialization failed\n");
    lammps_mpi_finalize();
  }

  // Setup the basic simulation parameters via string commands
  lammps_command(handle, "units lj");
  lammps_command(handle, "dimension 3");
  lammps_command(handle, "boundary p p p");
  lammps_command(handle, "atom_style atomic");
  lammps_command(handle, "pair_style zero 1.0 nocoeff");  // Dummy pair style for no interactions
  lammps_command(handle, "region box block -10 10 -10 10 -10 10");
  lammps_command(handle, "create_box 1 box");
  lammps_command(handle, "mass 1 1.0");
  lammps_command(handle, "pair_coeff 1 1 1.0 1.0");

  lammps_command(handle, "region randbox block -10 10 -10 10 -10 10");
  char cmd[256];
  int seed = 123;
  snprintf(cmd, sizeof(cmd), "create_atoms 1 random %d %d randbox overlap 0.8", env->num_agents, seed);
  lammps_command(handle, cmd);

  // Setup for running simulations (timestep and integrator)
  lammps_command(handle, "timestep 0.5");
  lammps_command(handle, "fix 1 all nve");

  // Initialize
  lammps_command(handle, "run 0");
  env->handle = handle;
}

void compute_observations(Matsci* env) {
    double** x = (double **) lammps_extract_atom(env->handle, "x");
    for (int i=0; i<env->num_agents; i++) {
        env->observations[3*i] = x[i][0] - env->goal.x;
        env->observations[3*i + 1] = x[i][1] - env->goal.y;
        env->observations[3*i + 2] = x[i][2] - env->goal.z;
    }
}

void reset_atom(Matsci* env, double** x, int i) {
    x[i][0] = rndf(-10.0f, 10.0f);
    x[i][1] = rndf(-10.0f, 10.0f);
    x[i][2] = rndf(-10.0f, 10.0f);
}

void c_reset(Matsci* env) {
    void* handle = env->handle;
    double** x = (double **) lammps_extract_atom(handle, "x");
    for (int i=0; i<env->num_agents; i++) {
	reset_atom(env, x, i);
    }
    env->goal.x = rndf(-10.0f, 10.0f);
    env->goal.y = rndf(-10.0f, 10.0f);
    env->goal.z = rndf(-10.0f, 10.0f);
    env->tick = 0;
}

void c_step(Matsci* env) {
    void* handle = env->handle;
    env->tick++;

    if (env->tick >= 1024) {
        c_reset(env);
        for (int i=0; i<env->num_agents; i++) {
            env->rewards[i] = -1;
            env->terminals[i] = 1;
            env->log.n += 1;
	}
	return;
    }

    double **v = (double **) lammps_extract_atom(handle, "v");
    for (int i=0; i<env->num_agents; i++) {
	env->rewards[i] = 0;
	env->terminals[i] = 0;

        v[i][0] = env->actions[3*i];
        v[i][1] = env->actions[3*i + 1];
        v[i][2] = env->actions[3*i + 2];
    }

    lammps_command(handle, "run 1");

    double** x = (double **) lammps_extract_atom(handle, "x");
    for (int i=0; i<env->num_agents; i++) {
        Vec3 pos = (Vec3){x[i][0], x[i][1], x[i][2]};
        float dist = norm3(sub3(pos, env->goal));

        if (dist > 20.0f) {
            reset_atom(env, x, i);
            env->rewards[i] = -1;
            env->terminals[i] = 1;
            env->log.n += 1;
	}

        if (dist < 1.0f) {
           reset_atom(env, x, i);
           env->rewards[i] = 1;
           env->terminals[i] = 1;
           env->log.score += 1;
           env->log.n += 1;
	}
    }

    compute_observations(env);
}

static void update_camera_position(Client *c) {
    float r = c->camera_distance;
    float az = c->camera_azimuth;
    float el = c->camera_elevation;

    float x = r * cosf(el) * cosf(az);
    float y = r * cosf(el) * sinf(az);
    float z = r * sinf(el);

    c->camera.position = (Vector3){x, y, z};
    c->camera.target = (Vector3){0, 0, 0};
}

void handle_camera_controls(Client *client) {
    Vector2 mouse_pos = GetMousePosition();

    if (IsMouseButtonPressed(MOUSE_BUTTON_LEFT)) {
        client->is_dragging = true;
        client->last_mouse_pos = mouse_pos;
    }

    if (IsMouseButtonReleased(MOUSE_BUTTON_LEFT)) {
        client->is_dragging = false;
    }

    if (client->is_dragging && IsMouseButtonDown(MOUSE_BUTTON_LEFT)) {
        Vector2 mouse_delta = {mouse_pos.x - client->last_mouse_pos.x,
                               mouse_pos.y - client->last_mouse_pos.y};

        float sensitivity = 0.005f;

        client->camera_azimuth -= mouse_delta.x * sensitivity;

        client->camera_elevation += mouse_delta.y * sensitivity;
        client->camera_elevation =
            clampf(client->camera_elevation, -PI / 2.0f + 0.1f, PI / 2.0f - 0.1f);

        client->last_mouse_pos = mouse_pos;

        update_camera_position(client);
    }

    float wheel = GetMouseWheelMove();
    if (wheel != 0) {
        client->camera_distance -= wheel * 2.0f;
        client->camera_distance = clampf(client->camera_distance, 5.0f, 50.0f);
        update_camera_position(client);
    }
}

void c_close(Matsci* env) {
    /*
    if (IsWindowReady()) {
        CloseWindow();
    }
    */
}

void c_render(Matsci* env) {
    if (!IsWindowReady()) {
        Client *client = (Client *)calloc(1, sizeof(Client));

        client->width = WIDTH;
        client->height = HEIGHT;

        SetConfigFlags(FLAG_MSAA_4X_HINT); // antialiasing
        InitWindow(WIDTH, HEIGHT, "PufferLib DroneSwarm");

        #ifndef __EMSCRIPTEN__
            SetTargetFPS(60);
        #endif

        client->camera_distance = 40.0f;
        client->camera_azimuth = 0.0f;
        client->camera_elevation = PI / 10.0f;
        client->is_dragging = false;
        client->last_mouse_pos = (Vector2){0.0f, 0.0f};

        client->camera.up = (Vector3){0.0f, 0.0f, 1.0f};
        client->camera.fovy = 45.0f;
        client->camera.projection = CAMERA_PERSPECTIVE;
	env->client = client;

        update_camera_position(client);
    }

    if (WindowShouldClose()) {
        c_close(env);
        exit(0);
    }

    if (IsKeyDown(KEY_ESCAPE)) {

        c_close(env);
        exit(0);
    }

    handle_camera_controls(env->client);

    Client *client = env->client;

    BeginDrawing();
    ClearBackground(PUFF_BACKGROUND);
    BeginMode3D(client->camera);
    DrawCubeWires((Vector3){0.0f, 0.0f, 0.0f}, 20.0f, 20.0f, 20.0f, WHITE);

    double** x = (double **) lammps_extract_atom(env->handle, "x");
    for (int i=0; i<env->num_agents; i++) {
        DrawSphere((Vector3){x[i][0], x[i][1], x[i][2]}, 0.1f, PUFF_CYAN);
    }

    DrawSphere((Vector3){env->goal.x, env->goal.y, env->goal.z}, 0.1f, PUFF_RED);
    EndMode3D();

    DrawText("Left click + drag: Rotate camera", 10, 10, 16, PUFF_WHITE);
    DrawText("Mouse wheel: Zoom in/out", 10, 30, 16, PUFF_WHITE);

    EndDrawing();
}





================================================
FILE: pufferlib/ocean/matsci/matsci.py
================================================
'''A minimal matsci for your own envs.'''

import gymnasium
import numpy as np

import pufferlib
from pufferlib.ocean.matsci import binding

class Matsci(pufferlib.PufferEnv):
    def __init__(self, num_envs=1, num_atoms=2, render_mode=None, log_interval=128, buf=None, seed=0):
        self.single_observation_space = gymnasium.spaces.Box(low=0, high=1,
            shape=(3,), dtype=np.float32)
        self.single_action_space = gymnasium.spaces.Box(
            low=-1, high=1, shape=(3,), dtype=np.float32
        )
        self.render_mode = render_mode
        self.num_agents = num_envs*num_atoms

        super().__init__(buf)
        c_envs = []
        for i in range(num_envs):
            c_envs.append(binding.env_init(
                self.observations[i*num_atoms:(i+1)*num_atoms],
                self.actions[i*num_atoms:(i+1)*num_atoms],
                self.rewards[i*num_atoms:(i+1)*num_atoms],
                self.terminals[i*num_atoms:(i+1)*num_atoms],
                self.truncations[i*num_atoms:(i+1)*num_atoms],
                i,
                num_agents=num_atoms,
            ))

        self.c_envs = binding.vectorize(*c_envs)
 
    def reset(self, seed=0):
        binding.vec_reset(self.c_envs, seed)
        return self.observations, []

    def step(self, actions):
        self.actions[:] = actions
        binding.vec_step(self.c_envs)
        info = [binding.vec_log(self.c_envs)]
        return (self.observations, self.rewards,
            self.terminals, self.truncations, info)

    def render(self):
        binding.vec_render(self.c_envs, 0)

    def close(self):
        binding.vec_close(self.c_envs)

if __name__ == '__main__':
    N = 4096
    env = Matsci(num_envs=N)
    env.reset()
    steps = 0

    CACHE = 1024
    actions = np.random.randint(0, 5, (CACHE, N))

    import time
    start = time.time()
    while time.time() - start < 10:
        env.step(actions[steps % CACHE])
        steps += 1

    print('Squared SPS:', int(env.num_agents*steps / (time.time() - start)))



================================================
FILE: pufferlib/ocean/memory/binding.c
================================================
#include "memory.h"

#define Env Memory 
#include "../env_binding.h"

static int my_init(Env* env, PyObject* args, PyObject* kwargs) {
    env->length = unpack(kwargs, "length");
    return 0;
}

static int my_log(PyObject* dict, Log* log) {
    assign_to_dict(dict, "score", log->score);
    return 0;
}



================================================
FILE: pufferlib/ocean/memory/memory.c
================================================
#include "memory.h"

int main() {
    Memory env = {.length = 16};
    env.observations = (float*)calloc(1, sizeof(unsigned char));
    env.actions = (int*)calloc(1, sizeof(int));
    env.rewards = (float*)calloc(1, sizeof(float));
    env.terminals = (unsigned char*)calloc(1, sizeof(unsigned char));

    c_reset(&env);
    c_render(&env);
    while (!WindowShouldClose()) {
        if (IsKeyDown(KEY_LEFT_SHIFT)) {
            if (IsKeyDown(KEY_A) || IsKeyDown(KEY_LEFT)) {
                env.actions[0] = 0;
            } else if (IsKeyDown(KEY_D) || IsKeyDown(KEY_RIGHT)) {
                env.actions[0] = 1;
            } else {
                env.actions[0] = -1;
            }
        } else {
            env.actions[0] = rand() % 2;
        }
        c_step(&env);
        c_render(&env);
    }
    free(env.observations);
    free(env.actions);
    free(env.rewards);
    free(env.terminals);
    c_close(&env);
}




================================================
FILE: pufferlib/ocean/memory/memory.h
================================================
#include <stdlib.h>
#include <string.h>
#include "raylib.h"

const Color PUFF_RED = (Color){187, 0, 0, 255};
const Color PUFF_CYAN = (Color){0, 187, 187, 255};
const Color PUFF_WHITE = (Color){241, 241, 241, 241};
const Color PUFF_BACKGROUND = (Color){6, 24, 24, 255};

// Only use floats!
typedef struct {
    float score;
    float n; // Required as the last field 
} Log;

typedef struct {
    Log log;                     // Required field
    float* observations;         // Required field. Ensure type matches in .py and .c
    int* actions;                // Required field. Ensure type matches in .py and .c
    float* rewards;              // Required field
    unsigned char* terminals;    // Required field
    int length;
    int goal;
    int tick;
} Memory;

void c_reset(Memory* env) {
    env->goal = (rand()%2 == 0) ? -1 : 1;
    env->observations[0] = env->goal;
    env->tick = 0;
}

void c_step(Memory* env) {
    env->rewards[0] = 0;
    env->terminals[0] = 0;
    env->observations[0] = 0;
    env->tick++;

    if (env->tick < env->length) {
	return;
    }

    float val = 0.0f;
    if (env->actions[0] == 0 && env->goal == -1) {
	val = 1.0f;
    }
    if (env->actions[0] == 1 && env->goal == 1) {
	val = 1.0f;
    }

    c_reset(env);
    env->rewards[0] = val;
    env->terminals[0] = 1;
    env->log.score += val;
    env->log.n += 1;
}

void c_render(Memory* env) {
    if (!IsWindowReady()) {
        InitWindow(960, 480, "PufferLib Memory");
        SetTargetFPS(5);
    }

    if (IsKeyDown(KEY_ESCAPE)) {
        exit(0);
    }

    BeginDrawing();
    DrawRectangle(0, 0, 480, 480, (env->goal == -1 ? PUFF_CYAN : PUFF_RED));
    DrawRectangle(480, 0, 480, 480, (env->rewards[0] == 0 ? PUFF_RED: GREEN));
    DrawText(TextFormat("Tick %.0d. Simon says...", env->tick), 20, 20, 20, PUFF_WHITE);

    ClearBackground(PUFF_BACKGROUND);
    EndDrawing();
}

void c_close(Memory* env) {
    if (IsWindowReady()) {
        CloseWindow();
    }
}



================================================
FILE: pufferlib/ocean/memory/memory.py
================================================
'''A minimal test env for memory (note: requires credit assignment too because RL)'''

import gymnasium
import numpy as np

import pufferlib
from pufferlib.ocean.memory import binding

class Memory(pufferlib.PufferEnv):
    def __init__(self, num_envs=1, render_mode=None, log_interval=128, length=4, buf=None, seed=0):
        self.single_observation_space = gymnasium.spaces.Box(low=0, high=1,
            shape=(1,), dtype=np.float32)
        self.single_action_space = gymnasium.spaces.Discrete(2)
        self.render_mode = render_mode
        self.num_agents = num_envs

        super().__init__(buf)
        self.c_envs = binding.vec_init(self.observations, self.actions, self.rewards,
            self.terminals, self.truncations, num_envs, seed, length=length)
 
    def reset(self, seed=0):
        binding.vec_reset(self.c_envs, seed)
        return self.observations, []

    def step(self, actions):
        self.actions[:] = actions
        binding.vec_step(self.c_envs)
        info = [binding.vec_log(self.c_envs)]
        return (self.observations, self.rewards,
            self.terminals, self.truncations, info)

    def render(self):
        binding.vec_render(self.c_envs, 0)

    def close(self):
        binding.vec_close(self.c_envs)

if __name__ == '__main__':
    N = 4096
    env = Memory(num_envs=N)
    env.reset()
    steps = 0

    CACHE = 1024
    actions = np.random.randint(0, 5, (CACHE, N))

    import time
    start = time.time()
    while time.time() - start < 10:
        env.step(actions[steps % CACHE])
        steps += 1

    print('Squared SPS:', int(env.num_agents*steps / (time.time() - start)))



================================================
FILE: pufferlib/ocean/moba/__init__.py
================================================
[Empty file]


================================================
FILE: pufferlib/ocean/moba/binding.c
================================================
#include "moba.h"

#define Env MOBA
#define MY_SHARED
#include "../env_binding.h"

static PyObject* my_shared(PyObject* self, PyObject* args, PyObject* kwargs) {
    unsigned char* game_map_npy = read_file("resources/moba/game_map.npy");
    int* ai_path_buffer = calloc(3*8*128*128, sizeof(int));
    unsigned char* ai_paths = calloc(128*128*128*128, sizeof(unsigned char));
    for (int i = 0; i < 128*128*128*128; i++) {
        ai_paths[i] = 255;
    }

    PyObject* ai_path_buffer_handle = PyLong_FromVoidPtr(ai_path_buffer);
    PyObject* ai_paths_handle = PyLong_FromVoidPtr(ai_paths);
    PyObject* game_map_handle = PyLong_FromVoidPtr(game_map_npy);
    PyObject* state = PyDict_New();
    PyDict_SetItemString(state, "ai_path_buffer", ai_path_buffer_handle);
    PyDict_SetItemString(state, "ai_paths", ai_paths_handle);
    PyDict_SetItemString(state, "game_map", game_map_handle);
    return PyLong_FromVoidPtr(state);
}

static int my_init(Env* env, PyObject* args, PyObject* kwargs) {
    env->vision_range = unpack(kwargs, "vision_range");
    env->agent_speed = unpack(kwargs, "agent_speed");
    env->discretize = unpack(kwargs, "discretize");
    env->reward_death = unpack(kwargs, "reward_death");
    env->reward_xp = unpack(kwargs, "reward_xp");
    env->reward_distance = unpack(kwargs, "reward_distance");
    env->reward_tower = unpack(kwargs, "reward_tower");
    env->script_opponents = unpack(kwargs, "script_opponents");

    PyObject* handle_obj = PyDict_GetItemString(kwargs, "state");
    if (handle_obj == NULL) {
        PyErr_SetString(PyExc_KeyError, "Key 'state' not found in kwargs");
        return 1;
    }

    // Check if handle_obj is a PyLong
    if (!PyLong_Check(handle_obj)) {
        PyErr_SetString(PyExc_TypeError, "state handle must be an integer");
        return 1;
    }

    // Convert PyLong to PyObject* (state dictionary)
    PyObject* state_dict = (PyObject*)PyLong_AsVoidPtr(handle_obj);
    if (state_dict == NULL) {
        PyErr_SetString(PyExc_ValueError, "Invalid state dictionary pointer");
        return 1;
    }

    // Verify it’s a dictionary
    if (!PyDict_Check(state_dict)) {
        PyErr_SetString(PyExc_TypeError, "State pointer does not point to a dictionary");
        return 1;
    }

    // Basic validation: check reference count
    if (state_dict->ob_refcnt <= 0) {
        PyErr_SetString(PyExc_RuntimeError, "State dictionary has invalid reference count");
        return 1;
    }

    // Extract ai_path_buffer
    PyObject* ai_path_buffer_obj = PyDict_GetItemString(state_dict, "ai_path_buffer");
    if (ai_path_buffer_obj == NULL) {
        PyErr_SetString(PyExc_KeyError, "Key 'ai_path_buffer' not found in state");
        return 1;
    }
    if (!PyLong_Check(ai_path_buffer_obj)) {
        PyErr_SetString(PyExc_TypeError, "ai_path_buffer must be an integer");
        return 1;
    }
    env->ai_path_buffer = (int*)PyLong_AsVoidPtr(ai_path_buffer_obj);
    if (env->ai_path_buffer == NULL) {
        PyErr_SetString(PyExc_ValueError, "Invalid ai_path_buffer pointer");
        return 1;
    }

    // Extract ai_paths
    PyObject* ai_paths_obj = PyDict_GetItemString(state_dict, "ai_paths");
    if (ai_paths_obj == NULL) {
        PyErr_SetString(PyExc_KeyError, "Key 'ai_paths' not found in state");
        return 1;
    }
    if (!PyLong_Check(ai_paths_obj)) {
        PyErr_SetString(PyExc_TypeError, "ai_paths must be an integer");
        return 1;
    }
    env->ai_paths = (unsigned char*)PyLong_AsVoidPtr(ai_paths_obj);
    if (env->ai_paths == NULL) {
        PyErr_SetString(PyExc_ValueError, "Invalid ai_paths pointer");
        return 1;
    }

    // Extract game_map
    PyObject* game_map_obj = PyDict_GetItemString(state_dict, "game_map");
    if (game_map_obj == NULL) {
        PyErr_SetString(PyExc_KeyError, "Key 'game_map' not found in state");
        return 1;
    }
    if (!PyLong_Check(game_map_obj)) {
        PyErr_SetString(PyExc_TypeError, "game_map must be an integer");
        return 1;
    }
    unsigned char* game_map = (unsigned char*)PyLong_AsVoidPtr(game_map_obj);
    if (game_map == NULL) {
        PyErr_SetString(PyExc_ValueError, "Invalid game_map pointer");
        return 1;
    }

    init_moba(env, game_map);
    return 0;
}

static int my_log(PyObject* dict, Log* log) {
    assign_to_dict(dict, "perf", log->perf);
    assign_to_dict(dict, "score", log->score);
    assign_to_dict(dict, "episode_return", log->episode_return);
    assign_to_dict(dict, "episode_length", log->episode_length);
    assign_to_dict(dict, "radiant_victory", log->radiant_victory);
    assign_to_dict(dict, "dire_victory", log->dire_victory);
    assign_to_dict(dict, "radiant_level", log->radiant_level);
    assign_to_dict(dict, "dire_level", log->dire_level);
    assign_to_dict(dict, "radiant_towers_alive", log->radiant_towers_alive);
    assign_to_dict(dict, "dire_towers_alive", log->dire_towers_alive);

    assign_to_dict(dict, "radiant_support_episode_return", log->radiant_support_episode_return);
    assign_to_dict(dict, "radiant_support_reward_death", log->radiant_support_reward_death);
    assign_to_dict(dict, "radiant_support_reward_xp", log->radiant_support_reward_xp);
    assign_to_dict(dict, "radiant_support_reward_distance", log->radiant_support_reward_distance);
    assign_to_dict(dict, "radiant_support_reward_tower", log->radiant_support_reward_tower);
    assign_to_dict(dict, "radiant_support_level", log->radiant_support_level);
    assign_to_dict(dict, "radiant_support_kills", log->radiant_support_kills);
    assign_to_dict(dict, "radiant_support_deaths", log->radiant_support_deaths);
    assign_to_dict(dict, "radiant_support_damage_dealt", log->radiant_support_damage_dealt);
    assign_to_dict(dict, "radiant_support_damage_received", log->radiant_support_damage_received);
    assign_to_dict(dict, "radiant_support_healing_dealt", log->radiant_support_healing_dealt);
    assign_to_dict(dict, "radiant_support_healing_received", log->radiant_support_healing_received);
    assign_to_dict(dict, "radiant_support_creeps_killed", log->radiant_support_creeps_killed);
    assign_to_dict(dict, "radiant_support_neutrals_killed", log->radiant_support_neutrals_killed);
    assign_to_dict(dict, "radiant_support_towers_killed", log->radiant_support_towers_killed);
    assign_to_dict(dict, "radiant_support_usage_auto", log->radiant_support_usage_auto);
    assign_to_dict(dict, "radiant_support_usage_q", log->radiant_support_usage_q);
    assign_to_dict(dict, "radiant_support_usage_w", log->radiant_support_usage_w);
    assign_to_dict(dict, "radiant_support_usage_e", log->radiant_support_usage_e);
    return 0;
}



================================================
FILE: pufferlib/ocean/moba/cy_moba.pyx
================================================
from libc.stdlib cimport calloc, free
import numpy as np

cdef extern from "moba.h":
    int EMPTY
    int WALL
    int TOWER
    int RADIANT_CREEP
    int DIRE_CREEP
    int NEUTRAL
    int RADIANT_SUPPORT
    int RADIANT_ASSASSIN
    int RADIANT_BURST
    int RADIANT_TANK
    int RADIANT_CARRY
    int DIRE_SUPPORT
    int DIRE_ASSASSIN
    int DIRE_BURST
    int DIRE_TANK
    int DIRE_CARRY

    int TOWER_VISION
    int CREEP_VISION
    int NEUTRAL_VISION

    int ENTITY_PLAYER
    int ENTITY_CREEP
    int ENTITY_NEUTRAL
    int ENTITY_TOWER

    int XP_RANGE
    int MAX_USES

    int LOG_BUFFER_SIZE

    ctypedef struct PlayerLog:
        float episode_return
        float reward_death
        float reward_xp
        float reward_distance
        float reward_tower
        float level
        float kills
        float deaths
        float damage_dealt
        float damage_received
        float healing_dealt
        float healing_received
        float creeps_killed
        float neutrals_killed
        float towers_killed
        float usage_auto
        float usage_q
        float usage_w
        float usage_e

    ctypedef struct Log:
        float perf
        float score
        float episode_return
        float episode_length
        float reward_death
        float reward_xp
        float reward_distance
        float reward_tower
     
        float radiant_victory
        float radiant_level
        float radiant_towers_alive

        float dire_victory
        float dire_level
        float dire_towers_alive
       
        PlayerLog radiant_support
        PlayerLog radiant_assassin
        PlayerLog radiant_burst
        PlayerLog radiant_tank
        PlayerLog radiant_carry

        PlayerLog dire_support
        PlayerLog dire_assassin
        PlayerLog dire_burst
        PlayerLog dire_tank
        PlayerLog dire_carry

    ctypedef struct LogBuffer
    LogBuffer* allocate_logbuffer(int)
    void free_logbuffer(LogBuffer*)
    Log aggregate_and_clear(LogBuffer*)

    ctypedef int (*skill)(MOBA*, Entity*, Entity*) noexcept

    ctypedef struct Map:
        unsigned char* grid;
        int* pids
        int width
        int height

    ctypedef struct CachedRNG:
        float* rng
        int rng_n
        int rng_idx

    ctypedef struct Reward:
        float death;
        float xp;
        float distance;
        float tower;

    ctypedef struct Entity:
        int pid;
        int entity_type;
        int hero_type;
        int grid_id;
        int team;
        float health;
        float max_health;
        float mana;
        float max_mana;
        float y;
        float x;
        float spawn_y;
        float spawn_x;
        float damage;
        int lane;
        int waypoint;
        float move_speed;
        float move_modifier;
        int stun_timer;
        int move_timer;
        int q_timer;
        int w_timer;
        int e_timer;
        int basic_attack_timer;
        int basic_attack_cd;
        int is_hit;
        int level;
        int xp;
        int xp_on_kill;
        float reward;
        int tier;
        float base_health;
        float base_mana;
        float base_damage;
        int hp_gain_per_level;
        int mana_gain_per_level;
        int damage_gain_per_level;
        float last_x;
        float last_y;
        int target_pid;
        int attack_aoe;

    ctypedef struct MOBA:
        int vision_range;
        float agent_speed;
        unsigned char discretize;
        unsigned char script_opponents;
        int obs_size;
        int creep_idx;
        int tick;

        Map* map;
        unsigned char* ai_paths;
        int* ai_path_buffer;
        unsigned char* observations;
        int* actions;
        float* rewards;
        unsigned char* terminals;
        unsigned char* truncations;
        Entity* entities;
        Reward* reward_components;
        LogBuffer* log_buffer;
        PlayerLog log[10];

        float reward_death;
        float reward_xp;
        float reward_distance;
        float reward_tower;
        
        int total_towers_taken;
        int total_levels_gained;
        int radiant_victories;
        int dire_victories;

        # MAX_ENTITIES x MAX_SCANNED_TARGETS
        Entity* scanned_targets[256][121];
        skill skills[10][3];

        CachedRNG *rng;

    ctypedef struct GameRenderer
    GameRenderer* init_game_renderer(int cell_size, int width, int height)
    int render_game(GameRenderer* renderer, MOBA* env, int frame)
    void close_game_renderer(GameRenderer* renderer)

    ctypedef struct Reward
    void init_moba(MOBA* env, unsigned char* game_map_npy)
    void free_moba(MOBA* env)
 
    unsigned char* read_file(char* filename)

    void c_reset(MOBA* env)
    void c_step(MOBA* env)
    void randomize_tower_hp(MOBA* env)

cpdef entity_dtype():
    '''Make a dummy entity to get the dtype'''
    cdef Entity entity
    return np.asarray(<Entity[:1]>&entity).dtype

cpdef reward_dtype():
    '''Make a dummy reward to get the dtype'''
    cdef Reward reward
    return np.asarray(<Reward[:1]>&reward).dtype

cdef class CyMOBA:
    cdef MOBA* envs
    cdef GameRenderer* client
    cdef int num_envs
    cdef LogBuffer* logs

    cdef int* ai_path_buffer
    cdef unsigned char* ai_paths

    def __init__(self, unsigned char[:, :] observations, int[:, :] actions,
            float[:] rewards, unsigned char[:] terminals, int num_envs,  int vision_range,
            float agent_speed, bint discretize, float reward_death, float reward_xp,
            float reward_distance, float reward_tower, bint script_opponents):

        self.num_envs = num_envs
        self.client = NULL
        self.envs = <MOBA*> calloc(num_envs, sizeof(MOBA))
        self.logs = allocate_logbuffer(LOG_BUFFER_SIZE)

        import os
        cwd = os.getcwd()
        os.chdir(os.path.abspath(os.path.join(os.path.dirname(__file__), "..", "..")))
        cdef unsigned char* game_map_npy = read_file("resources/moba/game_map.npy");
        os.chdir(cwd)

        self.ai_path_buffer = <int*> calloc(3*8*128*128, sizeof(int))
        self.ai_paths = <unsigned char*> calloc(128*128*128*128, sizeof(unsigned char))
        cdef int i
        for i in range(128*128*128*128):
            self.ai_paths[i] = 255

        cdef int inc = 5 if script_opponents else 10
        for i in range(num_envs):
            self.envs[i] = MOBA(
                observations=&observations[inc*i, 0],
                actions=&actions[inc*i, 0],
                rewards=&rewards[inc*i],
                terminals=&terminals[inc*i],
                ai_paths = self.ai_paths,
                ai_path_buffer = self.ai_path_buffer,
                log_buffer=self.logs,
                vision_range=vision_range,
                agent_speed=agent_speed,
                discretize=discretize,
                reward_death=reward_death,
                reward_xp=reward_xp,
                reward_distance=reward_distance,
                reward_tower=reward_tower,
                script_opponents=script_opponents,
            )
            init_moba(&self.envs[i], game_map_npy)

    def reset(self):
        cdef int i
        for i in range(self.num_envs):
            c_reset(&self.envs[i])

    def step(self):
        cdef int i
        for i in range(self.num_envs):
            c_step(&self.envs[i])

    def render(self, int tick):
        if self.client == NULL:
            import os
            cwd = os.getcwd()
            os.chdir(os.path.abspath(os.path.join(os.path.dirname(__file__), "..", "..")))
            self.client = init_game_renderer(32, 41, 23)
            os.chdir(cwd)

        render_game(self.client, &self.envs[0], tick)

    def close(self):
        if self.client != NULL:
            close_game_renderer(self.client)
            self.client = NULL

        # TODO: free
        #free_moba(self.envs)

    def log(self):
        cdef Log log = aggregate_and_clear(self.logs)
        return log



================================================
FILE: pufferlib/ocean/moba/moba.c
================================================
#include "moba.h"
#include "puffernet.h"

typedef struct MOBANet MOBANet;
struct MOBANet {
    int num_agents;
    float* obs_2d;
    float* obs_1d;
    Conv2D* conv1;
    ReLU* relu1;
    Conv2D* conv2;
    Linear* flat;
    CatDim1* cat;
    ReLU* relu2;
    Linear* proj;
    ReLU* relu3;
    LSTM* lstm;
    Linear* actor;
    Linear* value_fn;
    Multidiscrete* multidiscrete;
};

MOBANet* init_mobanet(Weights* weights, int num_agents) {
    MOBANet* net = calloc(1, sizeof(MOBANet));
    int hidden = 128;
    net->num_agents = num_agents;
    net->obs_2d = calloc(num_agents*11*11*19, sizeof(float));
    net->obs_1d = calloc(num_agents*26, sizeof(float));
    net->conv1 = make_conv2d(weights, num_agents, 11, 11, 19, hidden, 5, 3);
    net->relu1 = make_relu(num_agents, hidden*3*3);
    net->conv2 = make_conv2d(weights, num_agents, 3, 3, hidden, hidden, 3, 1);
    net->flat = make_linear(weights, num_agents, 26, hidden);
    net->cat = make_cat_dim1(num_agents, hidden, hidden);
    net->relu2 = make_relu(num_agents, 2*hidden);
    net->proj = make_linear(weights, num_agents, 2*hidden, 128);
    net->relu3 = make_relu(num_agents, 128);
    net->actor = make_linear(weights, num_agents, 128, 23);
    net->value_fn = make_linear(weights, num_agents, 128, 1);
    net->lstm = make_lstm(weights, num_agents, 128, 128);
    int logit_sizes[6] = {7, 7, 3, 2, 2, 2};
    net->multidiscrete = make_multidiscrete(num_agents, logit_sizes, 6);
    return net;
}

void free_mobanet(MOBANet* net) {
    free(net->obs_2d);
    free(net->obs_1d);
    free(net->conv1);
    free(net->relu1);
    free(net->conv2);
    free(net->flat);
    free(net->cat);
    free(net->relu2);
    free(net->proj);
    free(net->relu3);
    free(net->actor);
    free(net->value_fn);
    free(net->lstm);
    free(net->multidiscrete);
    free(net);
}

void forward(MOBANet* net, unsigned char* observations, int* actions) {
    memset(net->obs_2d, 0, net->num_agents*11*11*19*sizeof(float));
    float (*obs_2d)[19][11][11] = (float (*)[19][11][11])net->obs_2d;
    float (*obs_1d)[26] = (float (*)[26])net->obs_1d;
    for (int b = 0; b < net->num_agents; b++) {
        int b_offset = b*(11*11*4 + 26);
        for (int i = 0; i < 11; i++) {
            for (int j = 0; j < 11; j++) {
                int elem_offset = 4*(i*11 + j);
                int one_hot_idx = observations[b_offset + elem_offset];
                obs_2d[b][one_hot_idx][i][j] = 1;
                obs_2d[b][16][i][j] = observations[b_offset + elem_offset+1] / 255.0f;
                obs_2d[b][17][i][j] = observations[b_offset + elem_offset+2] / 255.0f;
                obs_2d[b][18][i][j] = observations[b_offset + elem_offset+3] / 255.0f;
            }
        }
        for (int i = 0; i < 26; i++) {
            obs_1d[b][i] = observations[b_offset + 11*11*4 + i] / 255.0f;
        }
    }

    conv2d(net->conv1, net->obs_2d);
    relu(net->relu1, net->conv1->output);
    conv2d(net->conv2, net->relu1->output);

    linear(net->flat, net->obs_1d);

    cat_dim1(net->cat, net->conv2->output, net->flat->output);
    relu(net->relu2, net->cat->output);
    linear(net->proj, net->relu2->output);
    relu(net->relu3, net->proj->output);
    
    lstm(net->lstm, net->relu3->output);

    linear(net->actor, net->lstm->state_h);
    linear(net->value_fn, net->lstm->state_h);

    softmax_multidiscrete(net->multidiscrete, net->actor->output, actions);
    for (int i = 0; i < net->num_agents; i++) {
        actions[i*6] = 100*(actions[i*6] - 3);
        actions[i*6 + 1] = 100*(actions[i*6 + 1] - 3);
    }
}

void demo() {
    Weights* weights = load_weights("resources/moba/moba_weights.bin", 380056);
    bool script_opponents = true;

    int num_agents = script_opponents ? 5 : 10;
    MOBANet* net = init_mobanet(weights, num_agents);

    MOBA env = {
        .vision_range = 5,
        .agent_speed = 1.0,
        .discretize = true,
        .reward_death = -1.0,
        .reward_xp = 0.006,
        .reward_distance = 0.05,
        .reward_tower = 3.0,
        .script_opponents = script_opponents,
    };
    allocate_moba(&env);
    c_reset(&env);
    c_render(&env);
    int frame = 1;
    while (!WindowShouldClose()) {
        if (frame % 12 == 0) {
            c_step(&env);
            forward(net, env.observations, env.actions);
        }
        c_render(&env);
        frame = (frame + 1) % 12;
    }
    free_mobanet(net);
    free(weights);
    free_allocated_moba(&env);
    //close_game_renderer(renderer);
}

void test_performance(float test_time) {
    bool script_opponents = true;
    int num_agents = script_opponents ? 5 : 10;

    MOBA env = {
        .vision_range = 5,
        .agent_speed = 1.0,
        .discretize = true,
        .reward_death = -1.0,
        .reward_xp = 0.006,
        .reward_distance = 0.05,
        .reward_tower = 3.0,
        .script_opponents = script_opponents,
    };
    allocate_moba(&env);

    c_reset(&env);
    int start = time(NULL);
    int i = 0;
    while (time(NULL) - start < test_time) {
        for (int j = 0; j < num_agents; j++) {
            env.actions[6*j] = rand()%600 - 300;
            env.actions[6*j + 1] = rand()%600 - 300;
            env.actions[6*j + 2] = rand()%3;
            env.actions[6*j + 3] = rand()%2;
            env.actions[6*j + 4] = rand()%2;
            env.actions[6*j + 5] = rand()%2;
        }
        c_step(&env);
        i++;
    }
    int end = time(NULL);
    printf("SPS: %f\n", (float)num_agents*i / (end - start));
}

void test_bugs(float test_time) {
    Weights* weights = load_weights("resources/moba/moba_weights.bin", 380056);
    MOBANet* net = init_mobanet(weights, 10);

    MOBA env = {
        .vision_range = 5,
        .agent_speed = 1.0,
        .discretize = true,
        .reward_death = -1.0,
        .reward_xp = 0.006,
        .reward_distance = 0.05,
        .reward_tower = 3.0,
        .script_opponents = true,
    };
    allocate_moba(&env);

    c_reset(&env);
    int start = time(NULL);
    int i = 0;
    while (time(NULL) - start < test_time) {
        c_step(&env);
        forward(net, env.observations, env.actions);
        i++;
    }
    int end = time(NULL);
    printf("SPS: %f\n", 10.0f*i / (end - start));
    printf("Frames: %i\n", i);
    free_mobanet(net);
    free(weights);
    free_allocated_moba(&env);
}


int main() {
    //test_bugs(2.0f);
    demo();
    //test_performance(30.0f);
    return 0;
}



================================================
FILE: pufferlib/ocean/moba/moba.py
================================================
from pdb import set_trace as T
import numpy as np
import os

import pettingzoo
import gymnasium

import pufferlib
from pufferlib.ocean.moba import binding

MAP_OBS_N = 11*11*4
PLAYER_OBS_N = 26

class Moba(pufferlib.PufferEnv):
    def __init__(self, num_envs=4, vision_range=5, agent_speed=1.0,
            discretize=True, reward_death=-1.0, reward_xp=0.006,
            reward_distance=0.05, reward_tower=3.0, report_interval=32,
            script_opponents=True, render_mode='human', buf=None, seed=0):

        self.report_interval = report_interval
        self.render_mode = render_mode
        self.num_agents = 5*num_envs if script_opponents else 10*num_envs

        self.single_observation_space = gymnasium.spaces.Box(low=0, high=255,
            shape=(MAP_OBS_N + PLAYER_OBS_N,), dtype=np.uint8)
        self.single_action_space = gymnasium.spaces.MultiDiscrete([7, 7, 3, 2, 2, 2])

        super().__init__(buf=buf)

        c_envs = []
        players = 5 if script_opponents else 10
        self.c_state = binding.shared()
        for i in range(num_envs):
            env_id = binding.env_init(
                self.observations[i*players:(i+1)*players],
                self.actions[i*players:(i+1)*players],
                self.rewards[i*players:(i+1)*players],
                self.terminals[i*players:(i+1)*players],
                self.truncations[i*players:(i+1)*players],
                i + seed*num_envs,
                vision_range=vision_range,
                agent_speed=agent_speed,
                discretize=discretize,
                reward_death=reward_death,
                reward_xp=reward_xp,
                reward_distance=reward_distance,
                reward_tower=reward_tower,
                script_opponents=script_opponents,
                state=self.c_state,
            )
            c_envs.append(env_id)

        self.c_envs = binding.vectorize(*c_envs)
 
    def reset(self, seed=0):
        binding.vec_reset(self.c_envs, seed)
        self.tick = 0
        return self.observations, []

    def step(self, actions):
        self.actions[:] = actions
        self.actions[:, 0] = 100*(self.actions[:, 0] - 3)
        self.actions[:, 1] = 100*(self.actions[:, 1] - 3)
        binding.vec_step(self.c_envs)

        infos = []
        self.tick += 1
        if self.tick % self.report_interval == 0:
            log = binding.vec_log(self.c_envs)
            if log:
                infos.append(log)

        return (self.observations, self.rewards,
            self.terminals, self.truncations, infos)

    def render(self):
        for frame in range(12):
            binding.vec_render(self.c_envs, 0)

    def close(self):
        binding.vec_close(self.c_envs)


def test_performance(timeout=20, atn_cache=1024, num_envs=400):
    tick = 0

    import time
    start = time.time()
    while time.time() - start < timeout:
        atns = actions[tick % atn_cache]
        env.step(atns)
        tick += 1

    print(f'SPS: %f', 10*num_envs*tick / (time.time() - start))

if __name__ == '__main__':
    # Run with c profile
    from cProfile import run
    num_envs = 400
    env = Moba(num_envs=num_envs, report_interval=10000000)
    env.reset()
    actions = np.random.randint(0, env.single_action_space.nvec, (1024, 10*num_envs, 6))
    test_performance(20, 1024, num_envs)
    exit(0)

    run('test_performance(20)', 'stats.profile')
    import pstats
    from pstats import SortKey
    p = pstats.Stats('stats.profile')
    p.sort_stats(SortKey.TIME).print_stats(25)
    exit(0)

    #test_performance(10)



================================================
FILE: pufferlib/ocean/nmmo3/binding.c
================================================
#include "nmmo3.h"

#define Env MMO
#include "../env_binding.h"

static int my_init(Env* env, PyObject* args, PyObject* kwargs) {
    env->width = unpack(kwargs, "width");
    env->height = unpack(kwargs, "height");
    env->num_players = unpack(kwargs, "num_players");
    env->num_enemies = unpack(kwargs, "num_enemies");
    env->num_resources = unpack(kwargs, "num_resources");
    env->num_weapons = unpack(kwargs, "num_weapons");
    env->num_gems = unpack(kwargs, "num_gems");
    env->tiers = unpack(kwargs, "tiers");
    env->levels = unpack(kwargs, "levels");
    env->teleportitis_prob = unpack(kwargs, "teleportitis_prob");
    env->enemy_respawn_ticks = unpack(kwargs, "enemy_respawn_ticks");
    env->item_respawn_ticks = unpack(kwargs, "item_respawn_ticks");
    env->x_window = unpack(kwargs, "x_window");
    env->y_window = unpack(kwargs, "y_window");
    env->reward_combat_level = unpack(kwargs, "reward_combat_level");
    env->reward_prof_level = unpack(kwargs, "reward_prof_level");
    env->reward_item_level = unpack(kwargs, "reward_item_level");
    env->reward_market = unpack(kwargs, "reward_market");
    env->reward_death = unpack(kwargs, "reward_death");
    init(env);
    return 0;
}

static int my_log(PyObject* dict, Log* log) {
    assign_to_dict(dict, "perf", log->perf);
    assign_to_dict(dict, "score", log->score);
    assign_to_dict(dict, "episode_return", log->episode_return);
    assign_to_dict(dict, "episode_length", log->episode_length);
    assign_to_dict(dict, "return_comb_lvl", log->return_comb_lvl);
    assign_to_dict(dict, "return_prof_lvl", log->return_prof_lvl);
    assign_to_dict(dict, "return_item_atk_lvl", log->return_item_atk_lvl);
    assign_to_dict(dict, "return_item_def_lvl", log->return_item_def_lvl);
    assign_to_dict(dict, "return_market_buy", log->return_market_buy);
    assign_to_dict(dict, "return_market_sell", log->return_market_sell);
    assign_to_dict(dict, "return_death", log->return_death);
    assign_to_dict(dict, "min_comb_prof", log->min_comb_prof);
    assign_to_dict(dict, "purchases", log->purchases);
    assign_to_dict(dict, "sales", log->sales);
    assign_to_dict(dict, "equip_attack", log->equip_attack);
    assign_to_dict(dict, "equip_defense", log->equip_defense);
    assign_to_dict(dict, "r", log->r);
    assign_to_dict(dict, "c", log->c);
    return 0;
}



================================================
FILE: pufferlib/ocean/nmmo3/cy_nmmo3.pyx
================================================
# distutils: define_macros=NPY_NO_DEPRECATED_API=NPY_1_7_API_VERSION
# cython: language_level=3
# cython: boundscheck=True
# cython: initializedcheck=True
# cython: wraparound=True
# cython: cdivision=False
# cython: nonecheck=True
# cython: profile=True

from libc.stdlib cimport calloc, free
cimport numpy as cnp
import numpy as np

cdef extern from "nmmo3.h":
    int LOG_BUFFER_SIZE

    ctypedef struct Log:
        float episode_return;
        float episode_length;
        float return_comb_lvl;
        float return_prof_lvl;
        float return_item_atk_lvl;
        float return_item_def_lvl;
        float return_market_buy;
        float return_market_sell;
        float return_death;
        float min_comb_prof;
        float purchases;
        float sales;
        float equip_attack;
        float equip_defense;
        float r;
        float c;

    ctypedef struct LogBuffer
    LogBuffer* allocate_logbuffer(int)
    void free_logbuffer(LogBuffer*)
    Log aggregate_and_clear(LogBuffer*)

    int ATN_NOOP

    ctypedef struct Entity:
        int type
        int comb_lvl
        int element
        int dir
        int anim
        int hp
        int hp_max
        int prof_lvl
        int ui_mode
        int market_tier
        int sell_idx
        int gold
        int in_combat
        int equipment[5]
        int inventory[12]
        int is_equipped[12]
        int wander_range
        int ranged
        int goal
        int equipment_attack
        int equipment_defense
        int r
        int c
        int spawn_r
        int spawn_c
        int min_comb_prof[500]
        int min_comb_prof_idx
        int time_alive;
        int purchases;
        int sales;

    ctypedef struct Reward:
        float total
        float death;
        float pioneer;
        float comb_lvl;
        float prof_lvl;
        float item_atk_lvl;
        float item_def_lvl;
        float item_tool_lvl;
        float market_buy;
        float market_sell;

    ctypedef struct ItemMarket:
        int offer_idx
        int max_offers

    ctypedef struct RespawnBuffer:
        int* buffer
        int ticks
        int size

    ctypedef struct MMO:
        int width
        int height
        int num_players
        int num_enemies
        int num_resources
        int num_weapons
        int num_gems
        char* terrain
        unsigned char* rendered
        Entity* players
        Entity* enemies
        short* pids
        unsigned char* items
        Reward* rewards
        unsigned char* counts
        unsigned char* obs
        int* actions
        int tick
        int tiers
        int levels
        float teleportitis_prob
        int x_window
        int y_window
        int obs_size
        int enemy_respawn_ticks
        int item_respawn_ticks
        ItemMarket* market
        int market_buys
        int market_sells
        RespawnBuffer* resource_respawn_buffer
        RespawnBuffer* enemy_respawn_buffer
        Log* logs
        LogBuffer* log_buffer
        float reward_combat_level
        float reward_prof_level
        float reward_item_level
        float reward_market
        float reward_death

    ctypedef struct Client
    Client* make_client(MMO* env)
    #void close_client(Client* client)
    int tick(Client* client, MMO* env, float delta)

    void init_mmo(MMO* env)
    void c_reset(MMO* env, int seed)
    void c_step(MMO* env)

cpdef entity_dtype():
    '''Make a dummy entity to get the dtype'''
    cdef Entity entity
    return np.asarray(<Entity[:1]>&entity).dtype

cpdef reward_dtype():
    '''Make a dummy reward to get the dtype'''
    cdef Reward reward
    return np.asarray(<Reward[:1]>&reward).dtype

cdef class Environment:
    cdef:
        MMO* envs
        Client* client
        LogBuffer* logs
        int num_envs

    def __init__(self, unsigned char[:, :] observations, int[:, :] players,
            int[:, :] enemies, float[:, :] rewards, int[:] actions,
            list width, list height, int num_envs, list num_players,
            list num_enemies, list num_resources, list num_weapons, list num_gems,
            list tiers, list levels, list teleportitis_prob, list enemy_respawn_ticks,
            list item_respawn_ticks, float reward_combat_level, float reward_prof_level,
            float reward_item_level, float reward_market, float reward_death,
            int x_window=7, int y_window=5):

        cdef:
            int total_players = 0
            int total_enemies = 0
            int n_players = 0
            int n_enemies = 0

        self.num_envs = num_envs
        self.envs = <MMO*> calloc(num_envs, sizeof(MMO))
        self.logs = allocate_logbuffer(LOG_BUFFER_SIZE)
        for i in range(num_envs):
            obs_i = observations[total_players:total_players+n_players]
            rewards_i = rewards[total_players:total_players+n_players]
            players_i = players[total_players:total_players+n_players]
            enemies_i = enemies[total_enemies:total_enemies+n_enemies]
            #counts_i = counts[total_players:total_players+n_players]
            #terrain_i = terrain[total_players:total_players+n_players]
            #rendered_i = rendered[total_players:total_players+n_players]
            actions_i = actions[total_players:total_players+n_players]

            self.envs[i] = MMO(
                obs=&observations[total_players, 0],
                rewards=<Reward*> &rewards[total_players, 0],
                players=<Entity*> &players[total_players, 0],
                enemies=<Entity*> &enemies[total_enemies, 0],
                actions=&actions[total_players],
                width=width[i],
                height=height[i],
                num_players=num_players[i],
                num_enemies=num_enemies[i],
                num_resources=num_resources[i],
                num_weapons=num_weapons[i],
                num_gems=num_gems[i],
                tiers=tiers[i],
                levels=levels[i],
                teleportitis_prob=teleportitis_prob[i],
                enemy_respawn_ticks=enemy_respawn_ticks[i],
                item_respawn_ticks=item_respawn_ticks[i],
                x_window=x_window,
                y_window=y_window,
                log_buffer=self.logs,
                reward_combat_level=reward_combat_level,
                reward_prof_level=reward_prof_level,
                reward_item_level=reward_item_level,
                reward_market=reward_market,
                reward_death=reward_death,
            )
            n_players = num_players[i]
            n_enemies = num_enemies[i]

            init_mmo(&self.envs[i])
            total_players += n_players
            total_enemies += n_enemies

        self.client = NULL

    def reset(self):
        cdef int i
        for i in range(self.num_envs):
            # TODO: Seed
            c_reset(&self.envs[i], i+1)
            # Do I need to reset terrain here?

    def step(self):
        cdef int i
        for i in range(self.num_envs):
            c_step(&self.envs[i])

    def pids(self):
        ary = np.zeros((512, 512), dtype=np.intc)
        cdef int i, j
        for i in range(512):
            for j in range(512):
                ary[i, j] = self.envs[0].pids[512*i + j]
        return ary

    def render(self):
        if self.client == NULL:
            import os
            cwd = os.getcwd()
            os.chdir(os.path.abspath(os.path.join(os.path.dirname(__file__), "..", "..")))
            self.client = make_client(&self.envs[0])
            os.chdir(cwd)

        cdef int i, atn
        cdef int action = ATN_NOOP;
        for i in range(36):
            atn = tick(self.client, &self.envs[0], i/36.0)
            if atn != ATN_NOOP:
                action = atn

        self.envs[0].actions[0] = action

    # TODO
    def close(self):
        if self.client != NULL:
            #close_game_renderer(self.renderer)
            self.client = NULL

    def log(self):
        cdef Log log = aggregate_and_clear(self.logs)
        return log



================================================
FILE: pufferlib/ocean/nmmo3/make_sprite_sheets.py
================================================
'''
This script is used to generate scaled and combined sprite sheets for nmmo3
You will need the to put the following folders into the same directory. They
can be purchased from ManaSeed on itch.io

20.04c - Summer Forest 4.2
20.05c - Spring Forest 4.1
20.06a - Autumn Forest 4.1
20.07a - Winter Forest 4.0a
20.01a - Character Base 2.5c
20.01c - Peasant Farmer Pants & Hat 2.1 (comp. v01)
20.01c - Peasant Farmer Pants & Hat 2.2 (optional combat animations)
20.08b - Bow Combat 3.2
21.07b - Sword & Shield Combat 2.3
21.10a - Forester Pointed Hat & Tunic 2.1a (comp. v01)
21.10a - Forester Pointed Hat & Tunic 2.2 (optional, combat animations)
'''

from itertools import product
from PIL import Image
import pyray as ray
import numpy as np
import random
import sys
import os
import cv2


SHEET_SIZE = 2048
N_GENERATE = 10

ELEMENTS = (
    ('neutral', 1, ray.Color(255, 255, 255, 255)),
    ('fire', 5, ray.Color(255, 128, 128, 255)),
    ('water', 9, ray.Color(128, 128, 255, 255)),
    ('earth', 11, ray.Color(128, 255, 128, 255)),
    ('air', 3, ray.Color(255, 255, 128, 255)),      
)

BASE = list(range(8))
HAIR = list(range(14))
CLOTHES = list(range(1, 6))
SWORD = list(range(1, 6))
BOW = list(range(1, 6))
QUIVER = list(range(1, 9))

# Hair colors, indices into files
'''
HAIR = {
    ELEM_NEUTRAL: 1,
    ELEM_FIRE: 5,
    ELEM_WATER: 9,
    ELEM_EARTH: 11,
    ELEM_AIR: 3
}
'''


# Character base
character = 'char_a_p1_0bas_humn_v{i:02}.png'
demon = 'char_a_p1_0bas_demn_v{i:02}.png'
goblin = 'char_a_p1_0bas_gbln_v{i:02}.png'
hair_dap = 'char_a_p1_4har_dap1_v{i:02}.png'
hair_bob = 'char_a_p1_4har_bob1_v{i:02}.png'

# Combat animations
sword_character = 'char_a_pONE3_0bas_humn_v{i:02}.png'
sword_weapon = 'char_a_pONE3_6tla_sw01_v{i:02}.png'
sword_hair_bob = 'char_a_pONE3_4har_bob1_v{i:02}.png'
sword_hair_dap = 'char_a_pONE3_4har_dap1_v{i:02}.png'
bow_character = 'char_a_pBOW3_0bas_humn_v{i:02}.png'
bow_hair_dap = 'char_a_pBOW3_4har_dap1_v{i:02}.png'
bow_hair_bob = 'char_a_pBOW3_4har_bob1_v{i:02}.png'
bow_weapon = 'char_a_pBOW3_6tla_bo01_v{i:02}.png'
bow_quiver = 'char_a_pBOW3_7tlb_qv01_v{i:02}.png'
arrow = 'aro_comn_v{i:02}.png'

# Peasant character alternative
peasant_clothes = 'char_a_p1_1out_pfpn_v{i:02}.png'
sword_peasant_clothes = 'char_a_pONE3_1out_pfpn_v{i:02}.png'
bow_peasant_clothes = 'char_a_pBOW3_1out_pfpn_v{i:02}.png'

# Forester character alternative
forester_hat = 'char_a_p1_5hat_pnty_v{i:02}.png'
forester_clothes = 'char_a_p1_1out_fstr_v{i:02}.png'
sword_forester_hat = 'char_a_pONE3_5hat_pnty_v{i:02}.png'
sword_forester_clothes = 'char_a_pONE3_1out_fstr_v{i:02}.png'
bow_forester_hat = 'char_a_pBOW3_5hat_pnty_v{i:02}.png'
bow_forester_clothes = 'char_a_pBOW3_1out_fstr_v{i:02}.png'

sword_mask = np.array((
    (1, 1, 1, 1, 1, 1, 1, 1),
    (1, 1, 1, 1, 1, 1, 1, 1),
    (1, 0, 1, 1, 1, 1, 1, 1),
    (1, 0, 1, 1, 1, 1, 1, 1),
    (0, 0, 1, 1, 1, 1, 1, 1),
    (1, 1, 1, 1, 1, 1, 1, 1),
    (0, 0, 1, 1, 0, 0, 0, 0),
    (0, 0, 1, 1, 0, 0, 0, 0),
))

bow_mask = np.array((
    (0, 0, 0, 0, 0, 0, 0, 0), 
    (1, 1, 1, 1, 1, 1, 1, 1),
    (1, 0, 0, 0, 0, 0, 0, 0),
    (1, 0, 0, 0, 0, 0, 0, 0),
    (0, 0, 0, 0, 0, 0, 0, 0),
    (1, 1, 1, 1, 1, 1, 1, 1),
    (1, 0, 0, 0, 0, 0, 0, 0),
    (1, 0, 0, 0, 0, 0, 0, 0),
))

quiver_mask = np.array((
    (1, 1, 1, 1, 1, 1, 1, 1),
    (0, 0, 0, 0, 0, 0, 0, 0),
    (1, 1, 1, 1, 1, 1, 1, 1),
    (1, 1, 1, 1, 1, 1, 1, 1),
    (1, 1, 1, 1, 1, 1, 1, 1),
    (0, 0, 0, 0, 0, 0, 0, 0),
    (1, 1, 1, 1, 1, 1, 1, 1),
    (1, 1, 1, 1, 1, 1, 1, 1),
))

def draw_tex(path, f_name, i, x, y, tint=None):
    if tint is None:
        tint = ray.WHITE

    path = os.path.join(path, f_name).format(i=i)
    texture = ray.load_texture(path)
    source_rect = ray.Rectangle(0, 0, texture.width, -texture.height)
    dest_rect = ray.Rectangle(x, y, texture.width, texture.height)
    ray.draw_texture_pro(texture, source_rect, dest_rect, (0, 0), 0, tint)

def draw_masked_tex(path, f_name, i, x, y, mask, tint=None):
    if tint is None:
        tint = ray.WHITE

    path = os.path.join(path, f_name).format(i=i)
    texture = ray.load_texture(path)
    Y, X = mask.shape
    for r, row in enumerate(mask):
        for c, m in enumerate(row):
            if m == 0:
                continue

            src_x = c * 128
            src_y = r * 128
            source_rect = ray.Rectangle(src_x, src_y, 128, -128)

            dst_x = x + src_x
            dst_y = y + (Y-r-1)*128
            dest_rect = ray.Rectangle(dst_x, dst_y, 128, 128)

            ray.draw_texture_pro(texture, source_rect, dest_rect, (0, 0), 0, tint)

def draw_arrow(tex, src_x, src_y, dst_x, dst_y, offset_x, offset_y, rot):
    source_rect = ray.Rectangle(src_x*32, src_y*32, 32, -32)
    dest_rect = ray.Rectangle(dst_x*128 + offset_x, SHEET_SIZE-(dst_y+1)*128+ offset_y, 32, 32)
    ray.draw_texture_pro(tex, source_rect, dest_rect, (0, 0), rot, ray.WHITE)

def draw_sheet(src, hair_i, tint, seed=None):
    if seed is not None:
        random.seed(seed)

    base_i = random.choice(BASE)
    if hair_i is None:
        hair_i = random.choice(HAIR)
    clothes_i = random.choice(CLOTHES)
    sword_i = random.choice(SWORD)
    bow_i = random.choice(BOW)
    quiver_i = random.choice(QUIVER)

    hair_variant = random.randint(0, 1)
    hair = [hair_dap, hair_bob][hair_variant]
    sword_hair = [sword_hair_dap, sword_hair_bob][hair_variant]
    bow_hair = [bow_hair_dap, bow_hair_bob][hair_variant]

    clothes_variant = random.randint(0, 1)
    clothes = [peasant_clothes, forester_clothes][clothes_variant]
    sword_clothes = [sword_peasant_clothes, sword_forester_clothes][clothes_variant]
    bow_clothes = [bow_peasant_clothes, bow_forester_clothes][clothes_variant]

    x = 0
    y = 1024
    draw_tex(src, character, base_i, x, y)
    draw_tex(src, hair, hair_i, x, y)
    draw_tex(src, clothes, clothes_i, x, y)

    x = 0
    y = 0
    draw_masked_tex(src, sword_weapon, sword_i, x, y, sword_mask, tint=tint)
    draw_tex(src, sword_character, base_i, x, y)
    draw_tex(src, sword_hair, hair_i, x, y)
    draw_tex(src, sword_clothes, clothes_i, x, y)
    draw_masked_tex(src, sword_weapon, sword_i, x, y, 1-sword_mask, tint=tint)

    x = 1024
    y = 1024
    draw_masked_tex(src, bow_weapon, bow_i, x, y, bow_mask, tint=tint)
    draw_masked_tex(src, bow_quiver, quiver_i, x, y, quiver_mask, tint=tint)
    draw_tex(src, bow_character, base_i, x, y)
    draw_tex(src, bow_hair, hair_i, x, y)
    draw_tex(src, bow_clothes, clothes_i, x, y)
    draw_masked_tex(src, bow_weapon, bow_i, x, y, 1-bow_mask, tint=tint)
    draw_masked_tex(src, bow_quiver, quiver_i, x, y, 1-quiver_mask, tint=tint)

    arrow_path = os.path.join(src, arrow).format(i=quiver_i)
    arrow_tex = ray.load_texture(arrow_path)

    ### Arrows are manually aligned
    # Left facing arrows
    draw_arrow(arrow_tex, 4, 1, 9, 3, 24, 40, 0)
    draw_arrow(arrow_tex, 4, 1, 10, 3, 24, 40, 0)
    draw_arrow(arrow_tex, 3, 1, 11, 3, 24, 52, 0)
    draw_arrow(arrow_tex, 1, 1, 12, 3, 38, 64, 0)

    # Right facing arrows
    draw_arrow(arrow_tex, 4, 1, 9, 2, 64+42, 48, 120)
    draw_arrow(arrow_tex, 4, 1, 10, 2, 64+42, 48, 120)
    draw_arrow(arrow_tex, 3, 1, 11, 2, 64+32, 82, 180)
    draw_arrow(arrow_tex, 1, 1, 12, 2, 56, 98, 180+80)


def scale_image(image_array, scale_factor):
    if scale_factor < 1:
        # Scale down with exact interpolation
        scaled_image_array = image_array[::int(1/scale_factor), ::int(1/scale_factor)]
    elif scale_factor > 1:
        # Scale up (duplicate pixels)
        scaled_image_array = np.repeat(
            np.repeat(
                image_array, scale_factor, axis=0
            ), scale_factor, axis=1
        )
    else:
        # No scaling
        scaled_image_array = image_array

    return scaled_image_array

def copy_and_scale_files(source_directory, target_directory, scale_factor):
    for root, dirs, files in os.walk(source_directory):
        relative_path = os.path.relpath(root, source_directory)
        target_path = os.path.join(target_directory)
        os.makedirs(target_path, exist_ok=True)
        
        for file in files:
            src_file_path = os.path.join(root, file)
            target_file_path = os.path.join(target_directory, file)
            
            path = src_file_path.lower()
            if path.endswith('.ttf'):
                os.copy(src_file_path, target_file_path)
                continue

            if not src_file_path.lower().endswith(('.png', '.jpg', '.jpeg')):
                continue

            image = Image.open(src_file_path)
            image_array = np.array(image)
            scaled_image_array = scale_image(image_array, scale_factor)
            scaled_image = Image.fromarray(scaled_image_array)
            scaled_image.save(target_file_path)

if len(sys.argv) != 4:
    print("Usage: script.py source_directory target_directory scale_factor")
    sys.exit(1)

source_directory = sys.argv[1]
target_directory = sys.argv[2]
scale_factor = float(sys.argv[3])

if not os.path.exists(source_directory):
    print("Source directory does not exist.")
    sys.exit(1)

valid_scales = [0.125, 0.25, 0.5, 1, 2, 4]
if scale_factor not in valid_scales:
    print(f'Scale factor must be one of {valid_scales}')

intermediate_directory = os.path.join(target_directory, 'temp')
if not os.path.exists(intermediate_directory):
    os.makedirs(intermediate_directory)
    copy_and_scale_files(source_directory, intermediate_directory, scale_factor)

ray.init_window(SHEET_SIZE, SHEET_SIZE, "NMMO3")
ray.set_target_fps(60)

output_image = ray.load_render_texture(SHEET_SIZE, SHEET_SIZE)

i = 0
while not ray.window_should_close() and i < N_GENERATE:
    ray.set_window_title(f'Generating sheet {i+1}/{N_GENERATE}')

    for elem in ELEMENTS:
        elem_name, hair_i, tint = elem
        ray.begin_drawing()
        ray.begin_texture_mode(output_image)
        ray.clear_background(ray.BLANK)
        draw_sheet(intermediate_directory, hair_i, tint, seed=i)
        ray.end_texture_mode()

        image = ray.load_image_from_texture(output_image.texture)
        f_path = os.path.join(target_directory, f'{elem_name}_{i}.png')
        ray.export_image(image, f_path)

        ray.clear_background(ray.GRAY)
        ray.draw_texture(output_image.texture, 0, 0, ray.WHITE)
        ray.end_drawing()

    i += 1

coords = (0, 1)
spring = cv2.imread(intermediate_directory + '/spring forest.png')
summer = cv2.imread(intermediate_directory + '/summer forest.png')
autumn = cv2.imread(intermediate_directory + '/autumn forest (bare).png')
winter = cv2.imread(intermediate_directory + '/winter forest (clean).png')

spring = scale_image(spring, 2)
summer = scale_image(summer, 2)
autumn = scale_image(autumn, 2)
winter = scale_image(winter, 2)

SEASONS = [spring, summer, autumn, winter]

spring_sparkle = cv2.imread(intermediate_directory + '/spring water sparkles B.png')
summer_sparkle = cv2.imread(intermediate_directory + '/summer water sparkles B 16x16.png')
autumn_sparkle = cv2.imread(intermediate_directory + '/autumn water sparkles B 16x16.png')
winter_sparkle = cv2.imread(intermediate_directory + '/winter water sparkles B 16x16.png')

spring_sparkle = scale_image(spring_sparkle, 2)
summer_sparkle = scale_image(summer_sparkle, 2)
autumn_sparkle = scale_image(autumn_sparkle, 2)
winter_sparkle = scale_image(winter_sparkle, 2)

SPARKLES = [spring_sparkle, summer_sparkle, autumn_sparkle, winter_sparkle]

GRASS_OFFSET = (0, 0)
DIRT_OFFSET = (5, 0)
STONE_OFFSET = (9, 0)
WATER_OFFSET = (29, 16)

# Not compatible with water
GRASS_1 = (0, 1)
GRASS_2 = (0, 2)
GRASS_3 = (0, 3)
GRASS_4 = (0, 4)
GRASS_5 = (0, 5)

DIRT_1 = (8, 0)
DIRT_2 = (8, 1)
DIRT_3 = (8, 2)
DIRT_4 = (8, 3)
DIRT_5 = (8, 4)

STONE_1 = (12, 0)
STONE_2 = (12, 1)
STONE_3 = (12, 2)
STONE_4 = (12, 3)
STONE_5 = (12, 4)

WATER_1 = (27, 14)
WATER_2 = (28, 13)
WATER_3 = (28, 14)
WATER_4 = (29, 13)
WATER_5 = (29, 14)

GRASS_N = [GRASS_1, GRASS_2, GRASS_3, GRASS_4, GRASS_5]
DIRT_N = [DIRT_1, DIRT_2, DIRT_3, DIRT_4, DIRT_5]
STONE_N = [STONE_1, STONE_2, STONE_3, STONE_4, STONE_5]
WATER_N = [WATER_1, WATER_2, WATER_3, WATER_4, WATER_5]

ALL_MATERIALS = [DIRT_N, STONE_N, WATER_N]
ALL_OFFSETS = [DIRT_OFFSET, STONE_OFFSET, WATER_OFFSET]

# These values are just sentinels
# They will be mapped to GRASS/DIRT/STONE/WATER
FULL = (-1, 0)
EMPTY = (0, -1)

TL_CORNER = (0, 0)
T_FLAT = (1, 0)
TR_CORNER = (2, 0)
L_FLAT = (0, 1)
CENTER = (1, 1)
R_FLAT = (2, 1)
BL_CORNER = (0, 2)
B_FLAT = (1, 2)
BR_CORNER = (2, 2)
TL_DIAG = (0, 3)
TR_DIAG = (1, 3)
BL_DIAG = (0, 4)
BR_DIAG = (1, 4)
TRR_DIAG = (2, 3)
BRR_DIAG = (2, 4)

OFFSETS = [TL_CORNER, T_FLAT, TR_CORNER, L_FLAT, CENTER, R_FLAT, BL_CORNER,
    B_FLAT, BR_CORNER, TL_DIAG, TR_DIAG, BL_DIAG, BR_DIAG, TRR_DIAG, BRR_DIAG]

TILE_SIZE = int(32 * scale_factor)
SHEET_SIZE = 64
SHEET_PX = TILE_SIZE * SHEET_SIZE
merged_sheet = np.zeros((SHEET_PX, SHEET_PX, 3), dtype=np.uint8)

def gen_lerps():
    valid_combinations = []
    for combo in product(range(10), repeat=4):
        if sum(combo) == 9 and any(weight > 0 for weight in combo):
            valid_combinations.append(combo)

    return valid_combinations

def gen_lerps():
    valid_combinations = []
    for total_sum in range(1, 10):  # Loop through all possible sums from 1 to 9
        for combo in product(range(10), repeat=4):
            if sum(combo) == total_sum:
                valid_combinations.append(combo)
    return valid_combinations

def slice(r, c):
    return np.s_[
        r*TILE_SIZE:(r+1)*TILE_SIZE,
        c*TILE_SIZE:(c+1)*TILE_SIZE
    ]

idx = 0
for sheet in SEASONS:
    for offset, material in zip(ALL_OFFSETS, ALL_MATERIALS):
        src_dx, src_dy = offset

        # Write full tile textures. These are irregularly
        # arranged in the source sheet and require manual offsets.
        for src_x, src_y in material:
            dst_r, dst_c = divmod(idx, SHEET_SIZE)
            idx += 1

            src_pos = slice(src_y, src_x)
            tile_tex = sheet[src_pos]

            dst_pos = slice(dst_r, dst_c)
            merged_sheet[dst_pos] = tile_tex

        # Write partial tile textures. These have fixed offsets
        for dx, dy in OFFSETS:
            dst_r, dst_c = divmod(idx, SHEET_SIZE)
            idx += 1
            
            src_pos = slice(dy+src_dy, dx+src_dx)
            tile_tex = sheet[src_pos]

            dst_pos = slice(dst_r, dst_c)
            merged_sheet[dst_pos] = tile_tex

for x, y in WATER_N:
    # 3 animations
    for anim_y in range(3):
        for season, sparkle in zip(SEASONS, SPARKLES):
            src_pos = slice(y, x)
            tile_tex = season[src_pos]

            # 4 frame animation
            for anim_x in range(4):
                dst_r, dst_c = divmod(idx, SHEET_SIZE)
                idx += 1

                src_pos = slice(anim_y, anim_x)
                sparkle_tex = sparkle[src_pos]

                dst_pos = slice(dst_r, dst_c)
                merged_sheet[dst_pos] = tile_tex
                mask = np.where(sparkle_tex != 0)
                merged_sheet[dst_pos][mask] = sparkle_tex[mask]


for src in range(1, 5):
    tex_src = slice(src, 0)
    tiles = [spring[tex_src], summer[tex_src], autumn[tex_src], winter[tex_src]]
    for combo in gen_lerps():
        tex = np.zeros((TILE_SIZE, TILE_SIZE, 3))
        total_weight = sum(combo)
        for i, weight in enumerate(combo):
            tex += weight/total_weight * tiles[i]

        tex = tex.astype(np.uint8)

        dst_r, dst_c = divmod(idx, SHEET_SIZE)
        idx += 1

        dst_pos = slice(dst_r, dst_c)
        merged_sheet[dst_pos] = tex

    print(idx)

# save image
cv2.imwrite('merged_sheet.png', merged_sheet)
cv2.imshow('merged_sheet', merged_sheet)
cv2.waitKey(0)



================================================
FILE: pufferlib/ocean/nmmo3/nmmo3.c
================================================
#include <stdio.h>
#include <stdlib.h>
#include <string.h>
#include "puffernet.h"
#include "nmmo3.h"

// Only rens a few agents in the C
// version, and reduces for web.
// You can run the full 1024 on GPU
// with PyTorch.
#if defined(PLATFORM_WEB)
    #define NUM_AGENTS 4
#else
    #define NUM_AGENTS 16
#endif


typedef struct MMONet MMONet;
struct MMONet {
    int num_agents;
    float* ob_map;
    int* ob_player_discrete;
    float* ob_player_continuous;
    float* ob_reward;
    Conv2D* map_conv1;
    ReLU* map_relu;
    Conv2D* map_conv2;
    Embedding* player_embed;
    float* proj_buffer;
    Linear* proj;
    ReLU* proj_relu;
    LayerNorm* layer_norm;
    LSTM* lstm;
    Linear* actor;
    Linear* value_fn;
    Multidiscrete* multidiscrete;
};

MMONet* init_mmonet(Weights* weights, int num_agents) {
    MMONet* net = calloc(1, sizeof(MMONet));
    int hidden = 512;
    net->num_agents = num_agents;
    net->ob_map = calloc(num_agents*11*15*59, sizeof(float));
    net->ob_player_discrete = calloc(num_agents*47, sizeof(int));
    net->ob_player_continuous = calloc(num_agents*47, sizeof(float));
    net->ob_reward = calloc(num_agents*10, sizeof(float));
    net->map_conv1 = make_conv2d(weights, num_agents, 15, 11, 59, 128, 5, 3);
    net->map_relu = make_relu(num_agents, 128*3*4);
    net->map_conv2 = make_conv2d(weights, num_agents, 4, 3, 128, 128, 3, 1);
    net->player_embed = make_embedding(weights, num_agents*47, 128, 32);
    net->proj_buffer = calloc(num_agents*1817, sizeof(float));
    net->proj = make_linear(weights, num_agents, 1817, hidden);
    net->proj_relu = make_relu(num_agents, hidden);
    net->layer_norm = make_layernorm(weights, num_agents, hidden);
    net->actor = make_linear(weights, num_agents, hidden, 26);
    net->value_fn = make_linear(weights, num_agents, hidden, 1);
    net->lstm = make_lstm(weights, num_agents, hidden, hidden);
    int logit_sizes[1] = {26};
    net->multidiscrete = make_multidiscrete(num_agents, logit_sizes, 1);
    return net;
}

void free_mmonet(MMONet* net) {
    free(net->ob_map);
    free(net->ob_player_discrete);
    free(net->ob_player_continuous);
    free(net->ob_reward);
    free(net->map_conv1);
    free(net->map_relu);
    free(net->map_conv2);
    free(net->player_embed);
    free(net->proj_buffer);
    free(net->proj);
    free(net->proj_relu);
    free(net->layer_norm);
    free(net->actor);
    free(net->value_fn);
    free(net->lstm);
    free(net->multidiscrete);
    free(net);
}

void forward(MMONet* net, unsigned char* observations, int* actions) {
    memset(net->ob_map, 0, net->num_agents*11*15*59*sizeof(float));

    // DUMMY INPUT FOR TESTING
    //for (int i = 0; i < 11*15*10 + 47 + 10; i++) {
    //    observations[i] = i % 4;
    //}

    // CNN subnetwork
    int factors[10] = {4, 4, 17, 5, 3, 5, 5, 5, 7, 4};
    float (*ob_map)[59][11][15] = (float (*)[59][11][15])net->ob_map;
    for (int b = 0; b < net->num_agents; b++) {
        int b_offset = b*(11*15*10 + 47 + 10);
        for (int i = 0; i < 11; i++) {
            for (int j = 0; j < 15; j++) {
                int f_offset = 0;
                for (int f = 0; f < 10; f++) {
                    int obs_idx = f_offset + observations[b_offset + i*15*10 + j*10 + f];
                    ob_map[b][obs_idx][i][j] = 1;
                    f_offset += factors[f];
                }
            }
        }
    }
    conv2d(net->map_conv1, net->ob_map);
    relu(net->map_relu, net->map_conv1->output);
    conv2d(net->map_conv2, net->map_relu->output);

    // Player embedding subnetwork
    for (int b = 0; b < net->num_agents; b++) {
        for (int i = 0; i < 47; i++) {
            unsigned char ob = observations[b*(11*15*10 + 47 + 10) + 11*15*10 + i];
            net->ob_player_discrete[b*47 + i] = ob;
            net->ob_player_continuous[b*47 + i] = ob;
        }
    }
    embedding(net->player_embed, net->ob_player_discrete);

    // Rewards
    for (int b = 0; b < net->num_agents; b++) {
        for (int i = 0; i < 10; i++) {
            net->ob_reward[b*10 + i] = observations[b*(11*15*10 + 47 + 10) + 11*15*10 + 47 + i];
        }
    }

    for (int b = 0; b < net->num_agents; b++) {
        int b_offset = b*1817;
        for (int i = 0; i < 256; i++) {
            net->proj_buffer[b_offset + i] = net->map_conv2->output[b*256 + i];
        }

        b_offset += 256;
        for (int i = 0; i < 47*32; i++) {
            net->proj_buffer[b_offset + i] = net->player_embed->output[b*47*32 + i];
        }

        b_offset += 47*32;
        for (int i = 0; i < 47; i++) {
            net->proj_buffer[b_offset + i] = net->ob_player_continuous[b*47 + i];
        }

        b_offset += 47;
        for (int i = 0; i < 10; i++) {
            net->proj_buffer[b_offset + i] = net->ob_reward[b*10 + i];
        }
    }

    linear(net->proj, net->proj_buffer);
    relu(net->proj_relu, net->proj->output);

    lstm(net->lstm, net->proj_relu->output);
    layernorm(net->layer_norm, net->lstm->state_h);

    linear(net->actor, net->layer_norm->output);
    linear(net->value_fn, net->layer_norm->output);

    softmax_multidiscrete(net->multidiscrete, net->actor->output, actions);
}

void demo(int num_players) {
    srand(time(NULL));
    Weights* weights = load_weights("resources/nmmo3/nmmo3_weights.bin", 3387547);
    MMONet* net = init_mmonet(weights, num_players);

    MMO env = {
        .client = NULL,
        .width = 512,
        .height = 512,
        .num_players = num_players,
        .num_enemies = 2048,
        .num_resources = 2048,
        .num_weapons = 1024,
        .num_gems = 512,
        .tiers = 5,
        .levels = 40,
        .teleportitis_prob = 0.0,
        .enemy_respawn_ticks = 2,
        .item_respawn_ticks = 100,
        .x_window = 7,
        .y_window = 5,
    };
    allocate_mmo(&env);

    c_reset(&env);
    c_render(&env);

    int human_action = ATN_NOOP;
    bool human_mode = false;
    int i = 1;
    while (!WindowShouldClose()) {
        if (IsKeyPressed(KEY_LEFT_CONTROL)) {
            human_mode = !human_mode;
        }
        if (i % 36 == 0) {
            forward(net, env.observations, env.actions);
            if (human_mode) {
                env.actions[0] = human_action;
            }

            c_step(&env);
            human_action = ATN_NOOP;
        }
        int atn = c_render(&env);
        if (atn != ATN_NOOP) {
            human_action = atn;
        }
        i = (i + 1) % 36;
    }

    free_mmonet(net);
    free(weights);
    free_allocated_mmo(&env);
    //close_client(client);
}

void test_mmonet_performance(int num_players, int timeout) {
    Weights* weights = load_weights("nmmo3_weights.bin", 1101403);
    MMONet* net = init_mmonet(weights, num_players);

    MMO env = {
        .width = 512,
        .height = 512,
        .num_players = num_players,
        .num_enemies = 128,
        .num_resources = 32,
        .num_weapons = 32,
        .num_gems = 32,
        .tiers = 5,
        .levels = 7,
        .teleportitis_prob = 0.001,
        .enemy_respawn_ticks = 10,
        .item_respawn_ticks = 200,
        .x_window = 7,
        .y_window = 5,
    };
    allocate_mmo(&env);
    c_reset(&env);

    int start = time(NULL);
    int num_steps = 0;
    while (time(NULL) - start < timeout) {
        forward(net, env.observations, env.actions);
        c_step(&env);
        num_steps++;
    }

    int end = time(NULL);
    float sps = num_players * num_steps / (end - start);
    printf("Test Environment Performance FPS: %f\n", sps);
    free_allocated_mmo(&env);
    free_mmonet(net);
    free(weights);
}

void copy_cast(float* input, unsigned char* output, int width, int height) {
    for (int r = 0; r < height; r++) {
        for (int c = 0; c < width; c++) {
            int adr = r*width + c;
            output[adr] = 255*input[adr];
        }
    }
}

void raylib_grid(unsigned char* grid, int width, int height, int tile_size) {
    InitWindow(width*tile_size, height*tile_size, "Raylib Grid");
    SetTargetFPS(1);

    while (!WindowShouldClose()) {
        BeginDrawing();
        ClearBackground(BLACK);
        for (int r = 0; r < height; r++) {
            for (int c = 0; c < width; c++) {
                int adr = r*width + c;
                unsigned char val = grid[adr];

                Color color = (Color){val, val, val, 255};

                int x = c*tile_size;
                int y = r*tile_size;
                DrawRectangle(x, y, tile_size, tile_size, color);
            } }
        EndDrawing();
    }
    CloseWindow();
}

void raylib_grid_colored(unsigned char* grid, int width, int height, int tile_size) {
    InitWindow(width*tile_size, height*tile_size, "Raylib Grid");
    SetTargetFPS(1);

    while (!WindowShouldClose()) {
        BeginDrawing();
        ClearBackground(BLACK);
        for (int r = 0; r < height; r++) {
            for (int c = 0; c < width; c++) {
                int adr = 3*(r*width + c);
                unsigned char red = grid[adr];
                unsigned char green = grid[adr+1];
                unsigned char blue = grid[adr+2];

                Color color = (Color){red, green, blue, 255};

                int x = c*tile_size;
                int y = r*tile_size;
                DrawRectangle(x, y, tile_size, tile_size, color);
            }
        }
        EndDrawing();
    }
    CloseWindow();
}

void test_perlin_noise(int width, int height,
        float base_frequency, int octaves, int seed) {
    float terrain[width*height];
    perlin_noise((float*)terrain, width, height, base_frequency, octaves, seed, seed);

    unsigned char map[width*height];
    copy_cast((float*)terrain, (unsigned char*)map, width, height);
    raylib_grid((unsigned char*)map, width, height, 1024.0/width);
}

void test_flood_fill(int width, int height, int colors) {
    unsigned char unfilled[width][height];
    memset(unfilled, 0, width*height);

    // Draw some squares
    for (int i = 0; i < 32; i++) {
        int w = rand() % width/4;
        int h = rand() % height/4;
        int start_r = rand() % (3*height/4);
        int start_c = rand() % (3*width/4);
        int end_r = start_r + h;
        int end_c = start_c + w;
        for (int r = start_r; r < end_r; r++) {
            unfilled[r][start_c] = 1;
            unfilled[r][end_c] = 1;
        }
        for (int c = start_c; c < end_c; c++) {
            unfilled[start_r][c] = 1;
            unfilled[end_r][c] = 1;
        }
    }

    char filled[width*height];
    flood_fill((unsigned char*)unfilled, (char*)filled,
        width, height, colors, width*height);

    // Cast and colorize
    unsigned char output[width*height];
    for (int r = 0; r < height; r++) {
        for (int c = 0; c < width; c++) {
            int adr = r*width + c;
            int val = filled[adr];
            if (val == 0) {
                output[adr] = 0;
            }
            output[adr] = 128 + (128/colors)*val;
        }
    }

    raylib_grid((unsigned char*)output, width, height, 1024.0/width);
}

void test_cellular_automata(int width, int height, int colors, int max_fill) {
    char grid[width][height];
    for (int r = 0; r < height; r++) {
        for (int c = 0; c < width; c++) {
            grid[r][c] = -1;
        }
    }

    // Fill some squares
    for (int i = 0; i < 32; i++) {
        int w = rand() % width/4;
        int h = rand() % height/4;
        int start_r = rand() % (3*height/4);
        int start_c = rand() % (3*width/4);
        int end_r = start_r + h;
        int end_c = start_c + w;
        int color = rand() % colors;
        for (int r = start_r; r < end_r; r++) {
            for (int c = start_c; c < end_c; c++) {
                grid[r][c] = color;
            }
        }
    }

    cellular_automata((char*)grid, width, height, colors, max_fill);

    // Colorize
    unsigned char output[width*height];
    for (int r = 0; r < height; r++) {
        for (int c = 0; c < width; c++) {
            int val = grid[r][c];
            int adr = r*width + c;
            if (val == 0) {
                output[adr] = 0;
            }
            output[adr] = (255/colors)*val;
        }
    }

    raylib_grid((unsigned char*)output, width, height, 1024.0/width);
}

void test_generate_terrain(int width, int height, int x_border, int y_border) {
    char terrain[width][height];
    unsigned char rendered[width][height][3];
    generate_terrain((char*)terrain, (unsigned char*)rendered, width, height, x_border, y_border);


    // Colorize
    /*
    unsigned char output[width*height];
    for (int r = 0; r < height; r++) {
        for (int c = 0; c < width; c++) {
            int val = terrain[r][c];
            int adr = r*width + c;
            if (val == 0) {
                output[adr] = 0;
            }
            output[adr] = (255/4)*val;
        }
    }
    */

    raylib_grid_colored((unsigned char*)rendered, width, height, 1024.0/width);
}

void test_performance(int num_players, int timeout) {
    MMO env = {
        .width = 512,
        .height = 512,
        .num_players = num_players,
        .num_enemies = 128,
        .num_resources = 32,
        .num_weapons = 32,
        .num_gems = 32,
        .tiers = 5,
        .levels = 7,
        .teleportitis_prob = 0.001,
        .enemy_respawn_ticks = 10,
        .item_respawn_ticks = 200,
        .x_window = 7,
        .y_window = 5,
    };
    allocate_mmo(&env);
    c_reset(&env);

    int start = time(NULL);
    int num_steps = 0;
    while (time(NULL) - start < timeout) {
        for (int i = 0; i < num_players; i++) {
            env.actions[i] = rand() % 23;
        }
        c_step(&env);
        num_steps++;
    }

    int end = time(NULL);
    float sps = num_players * num_steps / (end - start);
    printf("Test Environment SPS: %f\n", sps);
    free_allocated_mmo(&env);
}

int main() {

    /*
    int width = 512;
    int height = 512;
    float base_frequency = 1.0/64.0;
    int octaves = 2;
    int seed = 0;
    test_perlin_noise(width, height, base_frequency, octaves, seed);
    test_flood_fill(width, height, 4);
    test_cellular_automata(width, height, 4, 4000);
    test_generate_terrain(width, height, 8, 8);
    */
    //test_performance(64, 10);
    demo(NUM_AGENTS);
    //test_mmonet_performance(1024, 10);
}



================================================
FILE: pufferlib/ocean/nmmo3/nmmo3.py
================================================
from pdb import set_trace as T
import numpy as np
from types import SimpleNamespace
import gymnasium
import pettingzoo
import time

from pufferlib.ocean.nmmo3 import binding
#import binding

import pufferlib

class NMMO3(pufferlib.PufferEnv):
    def __init__(self, width=8*[512], height=8*[512], num_envs=4,
            num_players=1024, num_enemies=2048, num_resources=2048,
            num_weapons=1024, num_gems=512, tiers=5, levels=40,
            teleportitis_prob=0.001, enemy_respawn_ticks=2,
            item_respawn_ticks=100, x_window=7, y_window=5,
            reward_combat_level=1.0, reward_prof_level=1.0,
            reward_item_level=0.5, reward_market=0.01,
            reward_death=-1.0, log_interval=128, buf=None, seed=0):

        self.log_interval = log_interval

        if len(width) > num_envs:
            width = width[:num_envs]
        if len(height) > num_envs:
            height = height[:num_envs]

        if not isinstance(width, list):
            width = num_envs * [width]
        if not isinstance(height, list):
            height = num_envs * [height]
        if not isinstance(num_players, list):
            num_players = num_envs * [num_players]
        if not isinstance(num_enemies, list):
            num_enemies = num_envs * [num_enemies]
        if not isinstance(num_resources, list):
            num_resources = num_envs * [num_resources]
        if not isinstance(num_weapons, list):
            num_weapons = num_envs * [num_weapons]
        if not isinstance(num_gems, list):
            num_gems = num_envs * [num_gems]
        if not isinstance(tiers, list):
            tiers = num_envs * [tiers]
        if not isinstance(levels, list):
            levels = num_envs * [levels]
        if not isinstance(teleportitis_prob, list):
            teleportitis_prob = num_envs * [teleportitis_prob]
        if not isinstance(enemy_respawn_ticks, list):
            enemy_respawn_ticks = num_envs * [enemy_respawn_ticks]
        if not isinstance(item_respawn_ticks, list):
            item_respawn_ticks = num_envs * [item_respawn_ticks]

        assert isinstance(width, list)
        assert isinstance(height, list)
        assert isinstance(num_players, list)
        assert isinstance(num_enemies, list)
        assert isinstance(num_resources, list)
        assert isinstance(num_weapons, list)
        assert isinstance(num_gems, list)
        assert isinstance(tiers, list)
        assert isinstance(levels, list)
        assert isinstance(teleportitis_prob, list)
        assert isinstance(enemy_respawn_ticks, list)
        assert isinstance(item_respawn_ticks, list)
        assert isinstance(x_window, int)
        assert isinstance(y_window, int)

        assert len(width) == num_envs
        assert len(height) == num_envs
        assert len(num_players) == num_envs
        assert len(num_enemies) == num_envs
        assert len(num_resources) == num_envs
        assert len(num_weapons) == num_envs
        assert len(num_gems) == num_envs
        assert len(tiers) == num_envs
        assert len(levels) == num_envs
        assert len(teleportitis_prob) == num_envs
        assert len(enemy_respawn_ticks) == num_envs
        assert len(item_respawn_ticks) == num_envs

        total_players = 0
        total_enemies = 0
        for idx in range(num_envs):
            assert isinstance(width[idx], int)
            assert isinstance(height[idx], int)

            if num_players[idx] is None:
                num_players[idx] = width[idx] * height[idx] // 2048
            if num_enemies[idx] is None:
                num_enemies[idx] = width[idx] * height[idx] // 512
            if num_resources[idx] is None:
                num_resources[idx] = width[idx] * height[idx] // 1024
            if num_weapons[idx] is None:
                num_weapons[idx] = width[idx] * height[idx] // 2048
            if num_gems[idx] is None:
                num_gems[idx] = width[idx] * height[idx] // 4096
            if tiers[idx] is None:
                if height[idx] <= 128:
                    tiers[idx] = 1
                elif height[idx] <= 256:
                    tiers[idx] = 2
                elif height[idx] <= 512:
                    tiers[idx] = 3
                elif height[idx] <= 1024:
                    tiers[idx] = 4
                else:
                    tiers[idx] = 5
            if levels[idx] is None:
                if height[idx] <= 128:
                    levels[idx] = 7
                elif height[idx] <= 256:
                    levels[idx] = 15
                elif height[idx] <= 512:
                    levels[idx] = 31
                elif height[idx] <= 1024:
                    levels[idx] = 63
                else:
                    levels[idx] = 99

            total_players += num_players[idx]
            total_enemies += num_enemies[idx]

        self.players_flat = np.zeros((total_players, 51+501+3), dtype=np.intc)
        self.enemies_flat = np.zeros((total_enemies, 51+501+3), dtype=np.intc)
        self.rewards_flat = np.zeros((total_players, 10), dtype=np.float32)
        #map_obs = np.zeros((total_players, 11*15 + 47 + 10), dtype=np.intc)
        #counts = np.zeros((num_envs, height, width), dtype=np.uint8)
        #terrain = np.zeros((num_envs, height, width), dtype=np.uint8)
        #rendered = np.zeros((num_envs, height, width, 3), dtype=np.uint8)
        actions = np.zeros((total_players), dtype=np.intc)
        self.actions = actions

        self.num_agents = total_players
        self.num_players = total_players
        self.num_enemies = total_enemies

        self.comb_goal_mask = np.array([1, 0, 1, 0, 1, 1, 0, 1, 1, 1])
        self.prof_goal_mask = np.array([0, 0, 0, 1, 0, 0, 1, 1, 1, 1])
        self.tick = 0

        self.single_observation_space = gymnasium.spaces.Box(low=0,
            high=255, shape=(11*15*10+47+10,), dtype=np.uint8)
        self.single_action_space = gymnasium.spaces.Discrete(26)
        self.render_mode = 'human'

        super().__init__(buf)
        player_count = 0
        enemy_count = 0
        c_envs = []
        for i in range(num_envs):
            players = num_players[i]
            enemies = num_enemies[i]
            env_id = binding.env_init(
                self.observations[player_count:player_count+players],
                self.actions[player_count:player_count+players],
                self.rewards[player_count:player_count+players],
                self.terminals[player_count:player_count+players],
                self.truncations[player_count:player_count+players],
                i + seed*num_envs,
                width=width[i],
                height=height[i],
                num_players=num_players[i],
                num_enemies=num_enemies[i],
                num_resources=num_resources[i],
                num_weapons=num_weapons[i],
                num_gems=num_gems[i],
                tiers=tiers[i],
                levels=levels[i],
                teleportitis_prob=teleportitis_prob[i],
                enemy_respawn_ticks=enemy_respawn_ticks[i],
                item_respawn_ticks=item_respawn_ticks[i],
                x_window=x_window,
                y_window=y_window,
                reward_combat_level=reward_combat_level,
                reward_prof_level=reward_prof_level,
                reward_item_level=reward_item_level,
                reward_market=reward_market,
                reward_death=reward_death,
            )
            c_envs.append(env_id)
            player_count += players
            enemy_count += enemies

        self.c_envs = binding.vectorize(*c_envs)

    def reset(self, seed=0):
        self.rewards.fill(0)
        self.is_reset = True
        binding.vec_reset(self.c_envs, seed)
        return self.observations, []

    def step(self, actions):
        if not hasattr(self, 'is_reset'):
            raise Exception('Must call reset before step')
        self.rewards.fill(0)
        self.actions[:] = actions[:]
        binding.vec_step(self.c_envs)

        info = []
        if self.tick % self.log_interval == 0:
            info.append(binding.vec_log(self.c_envs))

        self.tick += 1
        return self.observations, self.rewards, self.terminals, self.truncations, info

    def render(self):
        for i in range(36):
            binding.vec_render(self.c_envs, 0)

    def close(self):
        binding.vec_close(self.c_envs)

def test_performance(cls, timeout=10, atn_cache=1024):
    env = cls(num_envs=1)
    env.reset()
    tick = 0

    actions = np.random.randint(0, 2, (atn_cache, env.num_agents))

    import time
    start = time.time()
    while time.time() - start < timeout:
        atn = actions[tick % atn_cache]
        env.step(atn)
        tick += 1

    print(f'{env.__class__.__name__}: SPS: {env.num_agents * tick / (time.time() - start)}')

if __name__ == '__main__':
    test_performance(NMMO3)



================================================
FILE: pufferlib/ocean/nmmo3/simplex.h
================================================
// Original work (noise library) Copyright (c) 2008 Casey Duncan
// Modified work (vec_noise library) Copyright (c) 2017 Zev Benjamin
// Single-file C port (this file) Copyright (c) 2024 Joseph Suarez
// Distributed under the MIT license. This is a simple copy-paste job.
// I did this because the original code mixed Python bindings into the
// C source, so there wasn't a clean way to use it as a C standalone.

#include <math.h>
const float GRAD3[][3] = {
    {1,1,0},{-1,1,0},{1,-1,0},{-1,-1,0},
    {1,0,1},{-1,0,1},{1,0,-1},{-1,0,-1},
    {0,1,1},{0,-1,1},{0,1,-1},{0,-1,-1},
    {1,0,-1},{-1,0,-1},{0,-1,1},{0,1,1}};

const float GRAD4[][4] = {
    {0,1,1,1}, {0,1,1,-1}, {0,1,-1,1}, {0,1,-1,-1},
    {0,-1,1,1}, {0,-1,1,-1}, {0,-1,-1,1}, {0,-1,-1,-1},
    {1,0,1,1}, {1,0,1,-1}, {1,0,-1,1}, {1,0,-1,-1},
    {-1,0,1,1}, {-1,0,1,-1}, {-1,0,-1,1}, {-1,0,-1,-1},
    {1,1,0,1}, {1,1,0,-1}, {1,-1,0,1}, {1,-1,0,-1},
    {-1,1,0,1}, {-1,1,0,-1}, {-1,-1,0,1}, {-1,-1,0,-1},
    {1,1,1,0}, {1,1,-1,0}, {1,-1,1,0}, {1,-1,-1,0},
    {-1,1,1,0}, {-1,1,-1,0}, {-1,-1,1,0}, {-1,-1,-1,0}};

// At the possible cost of unaligned access, we use char instead of
// int here to try to ensure that this table fits in L1 cache
const unsigned char PERM[] = {
  151, 160, 137, 91, 90, 15, 131, 13, 201, 95, 96, 53, 194, 233, 7, 225, 140,
  36, 103, 30, 69, 142, 8, 99, 37, 240, 21, 10, 23, 190, 6, 148, 247, 120,
  234, 75, 0, 26, 197, 62, 94, 252, 219, 203, 117, 35, 11, 32, 57, 177, 33,
  88, 237, 149, 56, 87, 174, 20, 125, 136, 171, 168, 68, 175, 74, 165, 71,
  134, 139, 48, 27, 166, 77, 146, 158, 231, 83, 111, 229, 122, 60, 211, 133,
  230, 220, 105, 92, 41, 55, 46, 245, 40, 244, 102, 143, 54, 65, 25, 63, 161,
  1, 216, 80, 73, 209, 76, 132, 187, 208, 89, 18, 169, 200, 196, 135, 130,
  116, 188, 159, 86, 164, 100, 109, 198, 173, 186, 3, 64, 52, 217, 226, 250,
  124, 123, 5, 202, 38, 147, 118, 126, 255, 82, 85, 212, 207, 206, 59, 227,
  47, 16, 58, 17, 182, 189, 28, 42, 223, 183, 170, 213, 119, 248, 152, 2, 44,
  154, 163, 70, 221, 153, 101, 155, 167, 43, 172, 9, 129, 22, 39, 253, 19, 98,
  108, 110, 79, 113, 224, 232, 178, 185, 112, 104, 218, 246, 97, 228, 251, 34,
  242, 193, 238, 210, 144, 12, 191, 179, 162, 241, 81, 51, 145, 235, 249, 14,
  239, 107, 49, 192, 214, 31, 181, 199, 106, 157, 184, 84, 204, 176, 115, 121,
  50, 45, 127, 4, 150, 254, 138, 236, 205, 93, 222, 114, 67, 29, 24, 72, 243,
  141, 128, 195, 78, 66, 215, 61, 156, 180, 151, 160, 137, 91, 90, 15, 131,
  13, 201, 95, 96, 53, 194, 233, 7, 225, 140, 36, 103, 30, 69, 142, 8, 99, 37,
  240, 21, 10, 23, 190, 6, 148, 247, 120, 234, 75, 0, 26, 197, 62, 94, 252,
  219, 203, 117, 35, 11, 32, 57, 177, 33, 88, 237, 149, 56, 87, 174, 20, 125,
  136, 171, 168, 68, 175, 74, 165, 71, 134, 139, 48, 27, 166, 77, 146, 158,
  231, 83, 111, 229, 122, 60, 211, 133, 230, 220, 105, 92, 41, 55, 46, 245,
  40, 244, 102, 143, 54, 65, 25, 63, 161, 1, 216, 80, 73, 209, 76, 132, 187,
  208, 89, 18, 169, 200, 196, 135, 130, 116, 188, 159, 86, 164, 100, 109, 198,
  173, 186, 3, 64, 52, 217, 226, 250, 124, 123, 5, 202, 38, 147, 118, 126,
  255, 82, 85, 212, 207, 206, 59, 227, 47, 16, 58, 17, 182, 189, 28, 42, 223,
  183, 170, 213, 119, 248, 152, 2, 44, 154, 163, 70, 221, 153, 101, 155, 167,
  43, 172, 9, 129, 22, 39, 253, 19, 98, 108, 110, 79, 113, 224, 232, 178, 185,
  112, 104, 218, 246, 97, 228, 251, 34, 242, 193, 238, 210, 144, 12, 191, 179,
  162, 241, 81, 51, 145, 235, 249, 14, 239, 107, 49, 192, 214, 31, 181, 199,
  106, 157, 184, 84, 204, 176, 115, 121, 50, 45, 127, 4, 150, 254, 138, 236,
  205, 93, 222, 114, 67, 29, 24, 72, 243, 141, 128, 195, 78, 66, 215, 61, 156,
  180};

const unsigned char SIMPLEX[][4] = {
    {0,1,2,3},{0,1,3,2},{0,0,0,0},{0,2,3,1},{0,0,0,0},{0,0,0,0},{0,0,0,0},
    {1,2,3,0},{0,2,1,3},{0,0,0,0},{0,3,1,2},{0,3,2,1},{0,0,0,0},{0,0,0,0},
    {0,0,0,0},{1,3,2,0},{0,0,0,0},{0,0,0,0},{0,0,0,0},{0,0,0,0},{0,0,0,0},
    {0,0,0,0},{0,0,0,0},{0,0,0,0},{1,2,0,3},{0,0,0,0},{1,3,0,2},{0,0,0,0},
    {0,0,0,0},{0,0,0,0},{2,3,0,1},{2,3,1,0},{1,0,2,3},{1,0,3,2},{0,0,0,0},
    {0,0,0,0},{0,0,0,0},{2,0,3,1},{0,0,0,0},{2,1,3,0},{0,0,0,0},{0,0,0,0},
    {0,0,0,0},{0,0,0,0},{0,0,0,0},{0,0,0,0},{0,0,0,0},{0,0,0,0},{2,0,1,3},
    {0,0,0,0},{0,0,0,0},{0,0,0,0},{3,0,1,2},{3,0,2,1},{0,0,0,0},{3,1,2,0},
    {2,1,0,3},{0,0,0,0},{0,0,0,0},{0,0,0,0},{3,1,0,2},{0,0,0,0},{3,2,0,1},
    {3,2,1,0}};

#define fastfloor(n) (int)(n) - (((n) < 0.0f) & ((n) != (int)(n)))

// Fast sine/cosine functions from
// http://devmaster.net/forums/topic/4648-fast-and-accurate-sinecosine/page__st__80
// Note the input to these functions is not radians
// instead x = [0, 2] for r = [0, 2*PI]

inline float fast_sin(float x)
{
    // Convert the input value to a range of -1 to 1
    // x = x * (1.0f / PI);

    // Wrap around
    volatile float z = (x + 25165824.0f);
    x = x - (z - 25165824.0f);

    #if LOW_SINE_PRECISION
        return 4.0f * (x - x * fabsf(x));
    #else
    {
        float y = x - x * fabsf(x);
        const float Q = 3.1f;
        const float P = 3.6f;
        return y * (Q + P * fabsf(y));
    }
    #endif
}

inline float fast_cos(float x)
{
    return fast_sin(x + 0.5f);
}

// 2D simplex skew factors
#define F2 0.3660254037844386f  // 0.5 * (sqrt(3.0) - 1.0)
#define G2 0.21132486540518713f // (3.0 - sqrt(3.0)) / 6.0

float
noise2(float x, float y)
{
    int i1, j1, II, JJ, c;
    float s = (x + y) * F2;
    float i = floorf(x + s);
    float j = floorf(y + s);
    float t = (i + j) * G2;

    float xx[3], yy[3], f[3];
    float noise[3] = {0.0f, 0.0f, 0.0f};
    int g[3];

    xx[0] = x - (i - t);
    yy[0] = y - (j - t);

    i1 = xx[0] > yy[0];
    j1 = xx[0] <= yy[0];

    xx[2] = xx[0] + G2 * 2.0f - 1.0f;
    yy[2] = yy[0] + G2 * 2.0f - 1.0f;
    xx[1] = xx[0] - i1 + G2;
    yy[1] = yy[0] - j1 + G2;

    II = (int) i & 255;
    JJ = (int) j & 255;
    g[0] = PERM[II + PERM[JJ]] % 12;
    g[1] = PERM[II + i1 + PERM[JJ + j1]] % 12;
    g[2] = PERM[II + 1 + PERM[JJ + 1]] % 12;

    for (c = 0; c <= 2; c++)
        f[c] = 0.5f - xx[c]*xx[c] - yy[c]*yy[c];

    for (c = 0; c <= 2; c++)
        if (f[c] > 0)
            noise[c] = f[c]*f[c]*f[c]*f[c] * (GRAD3[g[c]][0]*xx[c] + GRAD3[g[c]][1]*yy[c]);

    return (noise[0] + noise[1] + noise[2]) * 70.0f;
}



================================================
FILE: pufferlib/ocean/nmmo3/tile_atlas.h
================================================
int lerps[10000] = {0, 0, 4, 14, 34, 69, 125, 209, 329, 494, 1, 5, 15, 35, 70, 126, 210, 330, 495, 0, 6, 16, 36, 71, 127, 211, 331, 496, 0, 0, 17, 37, 72, 128, 212, 332, 497, 0, 0, 0, 38, 73, 129, 213, 333, 498, 0, 0, 0, 0, 74, 130, 214, 334, 499, 0, 0, 0, 0, 0, 131, 215, 335, 500, 0, 0, 0, 0, 0, 0, 216, 336, 501, 0, 0, 0, 0, 0, 0, 0, 337, 502, 0, 0, 0, 0, 0, 0, 0, 0, 503, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 7, 18, 39, 75, 132, 217, 338, 504, 0, 8, 19, 40, 76, 133, 218, 339, 505, 0, 0, 20, 41, 77, 134, 219, 340, 506, 0, 0, 0, 42, 78, 135, 220, 341, 507, 0, 0, 0, 0, 79, 136, 221, 342, 508, 0, 0, 0, 0, 0, 137, 222, 343, 509, 0, 0, 0, 0, 0, 0, 223, 344, 510, 0, 0, 0, 0, 0, 0, 0, 345, 511, 0, 0, 0, 0, 0, 0, 0, 0, 512, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 9, 21, 43, 80, 138, 224, 346, 513, 0, 0, 22, 44, 81, 139, 225, 347, 514, 0, 0, 0, 45, 82, 140, 226, 348, 515, 0, 0, 0, 0, 83, 141, 227, 349, 516, 0, 0, 0, 0, 0, 142, 228, 350, 517, 0, 0, 0, 0, 0, 0, 229, 351, 518, 0, 0, 0, 0, 0, 0, 0, 352, 519, 0, 0, 0, 0, 0, 0, 0, 0, 520, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 23, 46, 84, 143, 230, 353, 521, 0, 0, 0, 47, 85, 144, 231, 354, 522, 0, 0, 0, 0, 86, 145, 232, 355, 523, 0, 0, 0, 0, 0, 146, 233, 356, 524, 0, 0, 0, 0, 0, 0, 234, 357, 525, 0, 0, 0, 0, 0, 0, 0, 358, 526, 0, 0, 0, 0, 0, 0, 0, 0, 527, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 48, 87, 147, 235, 359, 528, 0, 0, 0, 0, 88, 148, 236, 360, 529, 0, 0, 0, 0, 0, 149, 237, 361, 530, 0, 0, 0, 0, 0, 0, 238, 362, 531, 0, 0, 0, 0, 0, 0, 0, 363, 532, 0, 0, 0, 0, 0, 0, 0, 0, 533, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 89, 150, 239, 364, 534, 0, 0, 0, 0, 0, 151, 240, 365, 535, 0, 0, 0, 0, 0, 0, 241, 366, 536, 0, 0, 0, 0, 0, 0, 0, 367, 537, 0, 0, 0, 0, 0, 0, 0, 0, 538, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 152, 242, 368, 539, 0, 0, 0, 0, 0, 0, 243, 369, 540, 0, 0, 0, 0, 0, 0, 0, 370, 541, 0, 0, 0, 0, 0, 0, 0, 0, 542, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 244, 371, 543, 0, 0, 0, 0, 0, 0, 0, 372, 544, 0, 0, 0, 0, 0, 0, 0, 0, 545, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 373, 546, 0, 0, 0, 0, 0, 0, 0, 0, 547, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 548, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 10, 24, 49, 90, 153, 245, 374, 549, 0, 11, 25, 50, 91, 154, 246, 375, 550, 0, 0, 26, 51, 92, 155, 247, 376, 551, 0, 0, 0, 52, 93, 156, 248, 377, 552, 0, 0, 0, 0, 94, 157, 249, 378, 553, 0, 0, 0, 0, 0, 158, 250, 379, 554, 0, 0, 0, 0, 0, 0, 251, 380, 555, 0, 0, 0, 0, 0, 0, 0, 381, 556, 0, 0, 0, 0, 0, 0, 0, 0, 557, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 12, 27, 53, 95, 159, 252, 382, 558, 0, 0, 28, 54, 96, 160, 253, 383, 559, 0, 0, 0, 55, 97, 161, 254, 384, 560, 0, 0, 0, 0, 98, 162, 255, 385, 561, 0, 0, 0, 0, 0, 163, 256, 386, 562, 0, 0, 0, 0, 0, 0, 257, 387, 563, 0, 0, 0, 0, 0, 0, 0, 388, 564, 0, 0, 0, 0, 0, 0, 0, 0, 565, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 29, 56, 99, 164, 258, 389, 566, 0, 0, 0, 57, 100, 165, 259, 390, 567, 0, 0, 0, 0, 101, 166, 260, 391, 568, 0, 0, 0, 0, 0, 167, 261, 392, 569, 0, 0, 0, 0, 0, 0, 262, 393, 570, 0, 0, 0, 0, 0, 0, 0, 394, 571, 0, 0, 0, 0, 0, 0, 0, 0, 572, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 58, 102, 168, 263, 395, 573, 0, 0, 0, 0, 103, 169, 264, 396, 574, 0, 0, 0, 0, 0, 170, 265, 397, 575, 0, 0, 0, 0, 0, 0, 266, 398, 576, 0, 0, 0, 0, 0, 0, 0, 399, 577, 0, 0, 0, 0, 0, 0, 0, 0, 578, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 104, 171, 267, 400, 579, 0, 0, 0, 0, 0, 172, 268, 401, 580, 0, 0, 0, 0, 0, 0, 269, 402, 581, 0, 0, 0, 0, 0, 0, 0, 403, 582, 0, 0, 0, 0, 0, 0, 0, 0, 583, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 173, 270, 404, 584, 0, 0, 0, 0, 0, 0, 271, 405, 585, 0, 0, 0, 0, 0, 0, 0, 406, 586, 0, 0, 0, 0, 0, 0, 0, 0, 587, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 272, 407, 588, 0, 0, 0, 0, 0, 0, 0, 408, 589, 0, 0, 0, 0, 0, 0, 0, 0, 590, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 409, 591, 0, 0, 0, 0, 0, 0, 0, 0, 592, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 593, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 13, 30, 59, 105, 174, 273, 410, 594, 0, 0, 31, 60, 106, 175, 274, 411, 595, 0, 0, 0, 61, 107, 176, 275, 412, 596, 0, 0, 0, 0, 108, 177, 276, 413, 597, 0, 0, 0, 0, 0, 178, 277, 414, 598, 0, 0, 0, 0, 0, 0, 278, 415, 599, 0, 0, 0, 0, 0, 0, 0, 416, 600, 0, 0, 0, 0, 0, 0, 0, 0, 601, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 32, 62, 109, 179, 279, 417, 602, 0, 0, 0, 63, 110, 180, 280, 418, 603, 0, 0, 0, 0, 111, 181, 281, 419, 604, 0, 0, 0, 0, 0, 182, 282, 420, 605, 0, 0, 0, 0, 0, 0, 283, 421, 606, 0, 0, 0, 0, 0, 0, 0, 422, 607, 0, 0, 0, 0, 0, 0, 0, 0, 608, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 64, 112, 183, 284, 423, 609, 0, 0, 0, 0, 113, 184, 285, 424, 610, 0, 0, 0, 0, 0, 185, 286, 425, 611, 0, 0, 0, 0, 0, 0, 287, 426, 612, 0, 0, 0, 0, 0, 0, 0, 427, 613, 0, 0, 0, 0, 0, 0, 0, 0, 614, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 114, 186, 288, 428, 615, 0, 0, 0, 0, 0, 187, 289, 429, 616, 0, 0, 0, 0, 0, 0, 290, 430, 617, 0, 0, 0, 0, 0, 0, 0, 431, 618, 0, 0, 0, 0, 0, 0, 0, 0, 619, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 188, 291, 432, 620, 0, 0, 0, 0, 0, 0, 292, 433, 621, 0, 0, 0, 0, 0, 0, 0, 434, 622, 0, 0, 0, 0, 0, 0, 0, 0, 623, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 293, 435, 624, 0, 0, 0, 0, 0, 0, 0, 436, 625, 0, 0, 0, 0, 0, 0, 0, 0, 626, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 437, 627, 0, 0, 0, 0, 0, 0, 0, 0, 628, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 629, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 33, 65, 115, 189, 294, 438, 630, 0, 0, 0, 66, 116, 190, 295, 439, 631, 0, 0, 0, 0, 117, 191, 296, 440, 632, 0, 0, 0, 0, 0, 192, 297, 441, 633, 0, 0, 0, 0, 0, 0, 298, 442, 634, 0, 0, 0, 0, 0, 0, 0, 443, 635, 0, 0, 0, 0, 0, 0, 0, 0, 636, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 67, 118, 193, 299, 444, 637, 0, 0, 0, 0, 119, 194, 300, 445, 638, 0, 0, 0, 0, 0, 195, 301, 446, 639, 0, 0, 0, 0, 0, 0, 302, 447, 640, 0, 0, 0, 0, 0, 0, 0, 448, 641, 0, 0, 0, 0, 0, 0, 0, 0, 642, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 120, 196, 303, 449, 643, 0, 0, 0, 0, 0, 197, 304, 450, 644, 0, 0, 0, 0, 0, 0, 305, 451, 645, 0, 0, 0, 0, 0, 0, 0, 452, 646, 0, 0, 0, 0, 0, 0, 0, 0, 647, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 198, 306, 453, 648, 0, 0, 0, 0, 0, 0, 307, 454, 649, 0, 0, 0, 0, 0, 0, 0, 455, 650, 0, 0, 0, 0, 0, 0, 0, 0, 651, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 308, 456, 652, 0, 0, 0, 0, 0, 0, 0, 457, 653, 0, 0, 0, 0, 0, 0, 0, 0, 654, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 458, 655, 0, 0, 0, 0, 0, 0, 0, 0, 656, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 657, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 68, 121, 199, 309, 459, 658, 0, 0, 0, 0, 122, 200, 310, 460, 659, 0, 0, 0, 0, 0, 201, 311, 461, 660, 0, 0, 0, 0, 0, 0, 312, 462, 661, 0, 0, 0, 0, 0, 0, 0, 463, 662, 0, 0, 0, 0, 0, 0, 0, 0, 663, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 123, 202, 313, 464, 664, 0, 0, 0, 0, 0, 203, 314, 465, 665, 0, 0, 0, 0, 0, 0, 315, 466, 666, 0, 0, 0, 0, 0, 0, 0, 467, 667, 0, 0, 0, 0, 0, 0, 0, 0, 668, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 204, 316, 468, 669, 0, 0, 0, 0, 0, 0, 317, 469, 670, 0, 0, 0, 0, 0, 0, 0, 470, 671, 0, 0, 0, 0, 0, 0, 0, 0, 672, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 318, 471, 673, 0, 0, 0, 0, 0, 0, 0, 472, 674, 0, 0, 0, 0, 0, 0, 0, 0, 675, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 473, 676, 0, 0, 0, 0, 0, 0, 0, 0, 677, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 678, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 124, 205, 319, 474, 679, 0, 0, 0, 0, 0, 206, 320, 475, 680, 0, 0, 0, 0, 0, 0, 321, 476, 681, 0, 0, 0, 0, 0, 0, 0, 477, 682, 0, 0, 0, 0, 0, 0, 0, 0, 683, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 207, 322, 478, 684, 0, 0, 0, 0, 0, 0, 323, 479, 685, 0, 0, 0, 0, 0, 0, 0, 480, 686, 0, 0, 0, 0, 0, 0, 0, 0, 687, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 324, 481, 688, 0, 0, 0, 0, 0, 0, 0, 482, 689, 0, 0, 0, 0, 0, 0, 0, 0, 690, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 483, 691, 0, 0, 0, 0, 0, 0, 0, 0, 692, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 693, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 208, 325, 484, 694, 0, 0, 0, 0, 0, 0, 326, 485, 695, 0, 0, 0, 0, 0, 0, 0, 486, 696, 0, 0, 0, 0, 0, 0, 0, 0, 697, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 327, 487, 698, 0, 0, 0, 0, 0, 0, 0, 488, 699, 0, 0, 0, 0, 0, 0, 0, 0, 700, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 489, 701, 0, 0, 0, 0, 0, 0, 0, 0, 702, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 703, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 328, 490, 704, 0, 0, 0, 0, 0, 0, 0, 491, 705, 0, 0, 0, 0, 0, 0, 0, 0, 706, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 492, 707, 0, 0, 0, 0, 0, 0, 0, 0, 708, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 709, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 493, 710, 0, 0, 0, 0, 0, 0, 0, 0, 711, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 712, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 713, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0};

char tile_atlas[256] = {-2, 9, 7, 7, 10, 7, 7, 7, 5, 5, 8, 8, 8, 8, 8, 8, 3, 6, 6, 6, 3, 6, 6, 6, -1, -1, -1, -1, -1, -1, -1, -1, 11, 5, 8, 8, 14, 8, 8, 8, 5, 5, 8, 8, 8, 8, 8, 8, 0, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 1, 2, -1, -1, 0, -1, -1, -1, 2, 2, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 1, 2, -1, -1, 0, -1, -1, -1, 2, 2, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 12, 13, 6, 6, 3, 6, 6, 6, 2, 2, -1, -1, -1, -1, -1, -1, 3, 6, 6, 6, 3, 6, 6, 6, -1, -1, -1, -1, -1, -1, -1, -1, 1, 2, -1, -1, 0, -1, -1, -1, 2, 2, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 1, 2, -1, -1, 0, -1, -1, -1, 2, 2, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 1, 2, -1, -1, 0, -1, -1, -1, 2, 2, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1};


================================================
FILE: pufferlib/ocean/onestateworld/README.md
================================================
The One-State World is a simple research testbed designed for studying exploration strategies.

- Structure:

    - The environment has only one state.
    - At each step, the agent can choose between two actions:
        - Action 0: Produces a reward with low expectation and low variance. Concretely, rewards are sampled from a Gaussian distribution with mean 0.1 and variance 0 (i.e., always 0.1).
        - Action 1: Produces a reward with higher expectation but also higher variance. Rewards are sampled from a Gaussian distribution with a larger mean, but with substantial variance.
- Objective: 
    - The environment is meant to challenge exploration algorithms by forcing them to balance between:
        - Exploiting the “safe” but low-payoff option (Action 0).
        - Exploring the “risky” but potentially more rewarding option (Action 1).

The core goal is to evaluate how well different exploration methods can identify and manage stochastic, high-variance rewards in a minimal setting.


================================================
FILE: pufferlib/ocean/onestateworld/binding.c
================================================
#include "onestateworld.h"

#define Env World
#include "../env_binding.h"

static int my_init(Env* env, PyObject* args, PyObject* kwargs) {
    env->mean_left = unpack(kwargs, "mean_left");
    env->mean_right = unpack(kwargs, "mean_right");
    env->var_right = unpack(kwargs, "var_right");
    return 0;
}

static int my_log(PyObject* dict, Log* log) {
    assign_to_dict(dict, "perf", log->perf);
    assign_to_dict(dict, "score", log->score);
    assign_to_dict(dict, "episode_return", log->episode_return);
    assign_to_dict(dict, "episode_length", log->episode_length);
    return 0;
}



================================================
FILE: pufferlib/ocean/onestateworld/onestateworld.c
================================================
#include "onestateworld.h"

int main() {
    World env = {
        .mean_left = 0.1f,
        .mean_right = 0.5f,
        .var_right = 10.0f
    };
    allocate_World(&env);

    c_reset(&env);
    c_render(&env);
    while (!WindowShouldClose()) {
        if (IsKeyDown(KEY_LEFT_SHIFT)) {
            env.actions[0] = 0;
            if (IsKeyDown(KEY_LEFT)  || IsKeyDown(KEY_A)) env.actions[0] = LEFT;
            if (IsKeyDown(KEY_RIGHT) || IsKeyDown(KEY_D)) env.actions[0] = RIGHT;
        } else {
            env.actions[0] = rand() % 2;
        }
        c_step(&env);
        c_render(&env);
    }
    free_allocated(&env);
}




================================================
FILE: pufferlib/ocean/onestateworld/onestateworld.h
================================================
#include <stdlib.h>
#include <string.h>
#include <stdio.h>
#include <math.h>
#include <time.h>
#include "raylib.h"

// Marsaglia polar method from https://en.wikipedia.org/wiki/Marsaglia_polar_method
double gaussian_sample(double mean, double variance) {
    static int hasSpare = 0;
    static double spare;
    
    if (hasSpare) {
        hasSpare = 0;
        return mean + sqrt(variance) * spare;
    }

    hasSpare = 1;
    double u, v, s;
    do {
        u = (rand() / ((double) RAND_MAX)) * 2.0 - 1.0;
        v = (rand() / ((double) RAND_MAX)) * 2.0 - 1.0;
        s = u * u + v * v;
    } while (s >= 1 || s == 0);

    s = sqrt(-2.0 * log(s) / s);
    spare = v * s;
    return mean + sqrt(variance) * (u * s);
}

const unsigned char LEFT = 0;
const unsigned char RIGHT = 1;

typedef struct {
    float perf; // Recommended 0-1 normalized single real number perf metric
    float score; // Recommended unnormalized single real number perf metric
    float episode_return; // Recommended metric: sum of agent rewards over episode
    float episode_length; // Recommended metric: number of steps of agent episode
    // Any extra fields you add here may be exported to Python in binding.c
    float n; // Required as the last field 
} Log;

// Required that you have some struct for your env
// Recommended that you name it the same as the env file
typedef struct {
    Log log; // Required field. Env binding code uses this to aggregate logs
    unsigned char* observations; // Required. You can use any obs type, but make sure it matches in Python!
    int* actions; // Required. int* for discrete/multidiscrete, float* for box
    float* rewards; // Required
    unsigned char* terminals; // Required. We don't yet have truncations as standard yet
    int tick;

    float var_right;
    float mean_right;
    float mean_left;

    Texture2D puffer; 

} World;

World* allocate_World(World *env) {
    env->observations = calloc(1, sizeof(unsigned char));
    env->actions = calloc(1, sizeof(float));
    env->rewards = calloc(1, sizeof(float));
    env->terminals = calloc(1, sizeof(unsigned char));
    return env;
}

void free_allocated(World* env) {
    free(env->observations);
    free(env->actions);
    free(env->rewards);
    free(env->terminals);
    free(env);
}

void add_log(World* env) {
    env->log.perf += env->rewards[0];
    env->log.score += env->rewards[0];
    env->log.episode_length += env->tick;
    env->log.episode_return += env->rewards[0];
    env->log.n++;
}

// Required function
void c_reset(World* env) {
    env->observations[0] = 0;
    env->tick = 0;
    srand(time(NULL)); 
}

// Required function
void c_step(World* env) {
    env->tick += 1;

    // Clear previous buffers
    env->terminals[0] = 0;
    env->rewards[0] = 0;

    int action = env->actions[0];

    // Tanh here because of pufferlib clamping 
    if (action == LEFT) {
        env->rewards[0] = tanh(gaussian_sample(env->mean_left, 0));
    } else {
        env->rewards[0] = tanh(gaussian_sample(env->mean_right, env->var_right));
    }

    if (env->tick >= 1000) {
        env->terminals[0] = 1;
        add_log(env);
        c_reset(env);
    }
}

// Required function. Should handle creating the client on first call
void c_render(World* env) {
    int px = 64;

    if (!IsWindowReady()) {
        InitWindow(px*5, px*5, "PufferLib OneStateWorld");
        SetTargetFPS(1);
        env->puffer = LoadTexture("resources/shared/puffers_128.png");
    }

    // Standard across our envs so exiting is always the same
    if (IsKeyDown(KEY_ESCAPE)) {
        exit(0);
    }

    BeginDrawing();
    ClearBackground((Color){6, 24, 24, 255});

    // Draw the puffer for fun 
    Color color = (Color){255, 255, 255, 255};
    Rectangle source_rect = (Rectangle){0, 0, 128, 128};
    Rectangle dest_rect = (Rectangle){2*px, 2*px, px, px};        
    DrawTexturePro(env->puffer, source_rect, dest_rect,
                (Vector2){0, 0}, 0, color);

    // Print the action taken either on left or right, with the reward received
    if (env->actions[0] == LEFT) {
            char score_text[32];
            snprintf(score_text, sizeof(score_text), "R: %.4f", env->rewards[0]);
            DrawText(score_text, 0, 2.5*px, 28, (Color){255, 255, 255, 255});
    } else {
            char score_text[32];
            snprintf(score_text, sizeof(score_text), "R: %.4f", env->rewards[0]);
            DrawText(score_text, 3*px, 2.5*px, 28, (Color){255, 255, 255, 255});
    }

    EndDrawing();
}

// Required function. Should clean up anything you allocated
// Do not free env->observations, actions, rewards, terminals
void c_close(World* env) {
    if (IsWindowReady()) {
        CloseWindow();
    }
}



================================================
FILE: pufferlib/ocean/onestateworld/onestateworld.py
================================================
'''A simple sample environment. Use this as a template for your own envs.'''

import gymnasium
import numpy as np

import pufferlib
from pufferlib.ocean.onestateworld import binding

class World(pufferlib.PufferEnv):
    def __init__(self, num_envs=1, render_mode=None, log_interval=128, 
                 mean_left=0.1, mean_right=1, var_right=5, buf=None, seed=0):
        self.single_observation_space = gymnasium.spaces.Box(low=0, high=0,
            shape=(1,), dtype=np.uint8)
        self.single_action_space = gymnasium.spaces.Discrete(2)
        self.render_mode = render_mode
        self.num_agents = num_envs
        self.log_interval = log_interval

        super().__init__(buf)
        self.c_envs = binding.vec_init(self.observations, self.actions, self.rewards,
            self.terminals, self.truncations, num_envs, seed, 
            mean_left=mean_left, mean_right=mean_right, var_right=var_right
        )
 
    def reset(self, seed=0):
        binding.vec_reset(self.c_envs, seed)
        self.tick = 0
        return self.observations, []

    def step(self, actions):
        self.tick += 1

        self.actions[:] = actions
        binding.vec_step(self.c_envs)

        info = []
        if self.tick % self.log_interval == 0:
            info.append(binding.vec_log(self.c_envs))

        return (self.observations, self.rewards,
            self.terminals, self.truncations, info)

    def render(self):
        binding.vec_render(self.c_envs, 0)

    def close(self):
        binding.vec_close(self.c_envs)

if __name__ == '__main__':
    size = 10

    env = World(size=size)
    env.reset()
    steps = 0

    CACHE = 1024
    actions = np.random.randint(0, 2, (CACHE,))

    i = 0
    import time
    start = time.time()
    while time.time() - start < 10:
        env.step(actions[i % CACHE])
        steps += 1
        i += 1

    print('SPS:', int(steps / (time.time() - start)))



================================================
FILE: pufferlib/ocean/pacman/binding.c
================================================
#include "pacman.h"

#define Env PacmanEnv
#include "../env_binding.h"

static int my_init(Env* env, PyObject* args, PyObject* kwargs) {
    env->randomize_starting_position = unpack(kwargs, "randomize_starting_position");
    env->min_start_timeout = unpack(kwargs, "min_start_timeout");
    env->max_start_timeout = unpack(kwargs, "max_start_timeout");
    env->frightened_time = unpack(kwargs, "frightened_time");
    env->max_mode_changes = unpack(kwargs, "max_mode_changes");
    env->scatter_mode_length = unpack(kwargs, "scatter_mode_length");
    env->chase_mode_length = unpack(kwargs, "chase_mode_length");
    init(env);
    return 0;
}

static int my_log(PyObject* dict, Log* log) {
    assign_to_dict(dict, "score", log->score);
    assign_to_dict(dict, "episode_return", log->episode_return);
    assign_to_dict(dict, "episode_length", log->episode_length);
    return 0;
}



================================================
FILE: pufferlib/ocean/pacman/helpers.h
================================================
#include <stdlib.h>
#include <stdbool.h>

typedef struct Position {
    int x;
    int y;
} Position;


#define DOWN (0)
#define UP (1)
#define RIGHT (2)
#define LEFT (3)

static inline int reverse_direction(int direction) {
  return direction ^ 1;
}

static inline bool pos_equal(Position a, Position b) {
    return a.x == b.x && a.y == b.y;
}

static inline int pos_distance_squared(Position pos, Position target) {
    int dx = pos.x - target.x;
    int dy = pos.y - target.y;
    return dx * dx + dy * dy;
}

static inline Position pos_move(Position pos, int direction, int distance) {
    switch (direction) {
    case UP:
      pos.y -= distance;
      break;
    case DOWN:
      pos.y += distance;
      break;
    case LEFT:
      pos.x -= distance;
      break;
    case RIGHT:
      pos.x += distance;
      break;
    }

    return pos;
}

static inline int rand_range(int min, int max) {
  if (min == max) {
    return min;
  }

  return min + (rand() % (max - min));
}



================================================
FILE: pufferlib/ocean/pacman/pacman.c
================================================
#include <time.h>
#include "pacman.h"
#include "puffernet.h"

void demo() {
    // printf("OBSERVATIONS_COUNT: %d\n", OBSERVATIONS_COUNT);
    Weights* weights = load_weights("resources/pacman/pacman_weights.bin", 170117);
    int logit_sizes[1] = {4};
    LinearLSTM* net = make_linearlstm(weights, 1, OBSERVATIONS_COUNT, logit_sizes, 1);

    PacmanEnv env = {
        .randomize_starting_position = false,
        .min_start_timeout = 0, // randomized ghost delay range
        .max_start_timeout = 49,
        .frightened_time = 35,   // ghost frighten time
        .max_mode_changes = 6,
        .scatter_mode_length = 700,
        .chase_mode_length = 70,
    };
    allocate(&env);
    c_reset(&env);
 
    Client* client = make_client(&env);
    bool human_control = false;

    while (!WindowShouldClose()) {
        if (IsKeyDown(KEY_LEFT_SHIFT)) {
            if (IsKeyDown(KEY_DOWN)  || IsKeyDown(KEY_S)) env.actions[0] = DOWN;
            if (IsKeyDown(KEY_UP)    || IsKeyDown(KEY_W)) env.actions[0] = UP;
            if (IsKeyDown(KEY_LEFT)  || IsKeyDown(KEY_A)) env.actions[0] = LEFT;
            if (IsKeyDown(KEY_RIGHT) || IsKeyDown(KEY_D)) env.actions[0] = RIGHT;
            human_control = true;
        } else {
            human_control = false;
        }

        if (!human_control) {
            forward_linearlstm(net, env.observations, env.actions);
        }

        c_step(&env);
        if (env.terminals[0]) {
            c_reset(&env);
        }

        for (int i = 0; i < FRAMES; i++) {
            c_render(&env);
        }
    }
    free_linearlstm(net);
    free(weights);
    free_allocated(&env);
    close_client(client);
}

void performance_test() {
    long test_time = 10;
    PacmanEnv env = {};
    allocate(&env);
    c_reset(&env);

    long start = time(NULL);
    int i = 0;
    while (time(NULL) - start < test_time) {
        env.actions[0] = rand() % 4;
        c_step(&env);
        i++;
    }
    long end = time(NULL);
    printf("SPS: %ld\n", i / (end - start));
    free_allocated(&env);
}

int main() {
    //performance_test();
    demo();
    return 0;
}



================================================
FILE: pufferlib/ocean/pacman/pacman.h
================================================
#include "helpers.h"
#include "raylib.h"
#include <assert.h>
#include <math.h>
#include <stdbool.h>
#include <stdlib.h>
#include <string.h>
#include <time.h>

#define PX_PADDING_TOP 40 // 40px padding on top of the window

#define LOG_BUFFER_SIZE 1024

#define MAX_STEPS 10000 // max steps before truncation

#define GHOST_OBSERVATIONS_COUNT 9
#define PLAYER_OBSERVATIONS_COUNT 11
#define NUM_GHOSTS 4
#define FRAMES 7  // Fixed number of frames per step for interpolation

#define NUM_DOTS 240
#define NUM_POWERUPS 4
#define OBSERVATIONS_COUNT                                                                         \
    (PLAYER_OBSERVATIONS_COUNT + GHOST_OBSERVATIONS_COUNT * NUM_GHOSTS + NUM_DOTS + NUM_POWERUPS)

#define PINKY_TARGET_LEAD 4
#define INKY_TARGET_LEAD 2
#define CLYDE_TARGET_RADIUS 8

typedef struct Log Log;
struct Log {
        float episode_return;
        float episode_length;
        float score;
        float n;
};

typedef enum Tile {
    WALL_TILE = '#',
    DOT_TILE = '.',
    POWER_TILE = 'x',
    PLAYER_TILE = 'p',
    EMPTY_TILE = ' ',

    INKY_TILE = '1',
    BLINKY_TILE = '2',
    PINKY_TILE = '3',
    CLYDE_TILE = '4',
} Tile;

#define MAP_HEIGHT 31
#define MAP_WIDTH 28

static const char original_map[MAP_HEIGHT][MAP_WIDTH] = {
    "############################",
    "#............##............#",
    "#.####.#####.##.#####.####.#",
    "#x####.#####.##.#####.####x#",
    "#.####.#####.##.#####.####.#",
    "#..........................#",
    "#.####.##.########.##.####.#",
    "#.####.##.########.##.####.#",
    "#......##....##....##......#",
    "######.##### ## #####.######",
    "######.##### ## #####.######",
    "######.##   1234   ##.######",
    "######.## ######## ##.######",
    "######.## ######## ##.######",
    "      .   ########   .      ",
    "######.## ######## ##.######",
    "######.## ######## ##.######",
    "######.##          ##.######",
    "######.## ######## ##.######",
    "######.## ######## ##.######",
    "#............##............#",
    "#.####.#####.##.#####.####.#",
    "#.####.#####.##.#####.####.#",
    "#x..##.......p .......##..x#",
    "###.##.##.########.##.##.###",
    "###.##.##.########.##.##.###",
    "#......##....##....##......#",
    "#.##########.##.##########.#",
    "#.##########.##.##########.#",
    "#..........................#",
    "############################",
};

static const Position GHOST_CORNERS[NUM_GHOSTS] = {
    {3, -3},                     // PINKY
    {MAP_WIDTH - 4, -3},         // BLINKY
    {MAP_WIDTH - 1, MAP_HEIGHT}, // INKY
    {0, MAP_HEIGHT}              // CLYDE
};

static const char GHOST_TILES[NUM_GHOSTS] = {PINKY_TILE, BLINKY_TILE, INKY_TILE, CLYDE_TILE};

typedef struct Ghost {
        Position spawn_pos;
        Position pos;
        Position last_pos;    // use for interpolation
        Position target;
        int direction;
        int start_timeout;    // randomized delay before moving
        bool frightened;      // whether the ghost is frightened
        bool return_to_spawn; // whether to return to spawn position
        bool half_move;
} Ghost;

typedef struct Client Client;
typedef struct PacmanEnv {
        Client *client;
        bool randomize_starting_position; // randomize player starting position
        int min_start_timeout;            // randomized ghost delay range
        int max_start_timeout;
        int frightened_time; // ghost frighten time
        int max_mode_changes;
        int scatter_mode_length;
        int chase_mode_length;

        float *observations;
        int *actions;
        float *rewards;
        char *terminals;
        Log log;

        int step_count;
        int score;

        Tile *game_map;
        float *pickup_obs;
        float **pickup_obs_map;

        int remaining_pickups;

        Position *possible_spawn_pos;

        Position player_spawn_pos; // for when it's not randomized
        Position player_pos;
        Position last_player_pos;
        int player_direction;

        bool reverse_directions;
        bool scatter_mode;
        int mode_time_left;
        int mode_changes;

        int frightened_time_left;

        bool player_caught;

        Ghost ghosts[NUM_GHOSTS];
} PacmanEnv;

void add_log(PacmanEnv *env) {
    env->log.score += env->score;
    env->log.episode_return += env->score;
    env->log.episode_length = env->step_count;
    env->log.n++;
}

static inline Position pos_move_wrapped(Position pos, int direction, int distance) {
    pos = pos_move(pos, direction, distance);
    if (pos.x < 0)
        pos.x = MAP_WIDTH - 1;
    else if (pos.x >= MAP_WIDTH)
        pos.x = 0;
    return pos;
}

static inline Tile *tile_at(PacmanEnv *env, Position pos) {
    return &env->game_map[pos.y * MAP_WIDTH + pos.x];
}

static inline bool can_move_in_direction(PacmanEnv *env, Position pos, int direction) {
    return *tile_at(env, pos_move_wrapped(pos, direction, 1)) != WALL_TILE;
}

void init(PacmanEnv *env) {
    int dot_count = 0;
    Position pos;
    Tile source_tile;
    Tile *target_tile;

    env->game_map = (Tile *)calloc(MAP_WIDTH * MAP_HEIGHT, sizeof(Tile));
    env->possible_spawn_pos = (Position *)calloc(NUM_DOTS, sizeof(Position));

    env->pickup_obs = (float *)calloc(NUM_DOTS + NUM_POWERUPS, sizeof(float));
    env->pickup_obs_map = (float **)calloc(MAP_WIDTH * MAP_HEIGHT, sizeof(float *));

    int obs_map_n = 0;

    // one time map setup
    for (int y = 0; y < MAP_HEIGHT; y++) {
        for (int x = 0; x < MAP_WIDTH; x++) {
            source_tile = original_map[y][x];

            pos = (Position){x, y};
            target_tile = tile_at(env, pos);
            *target_tile = source_tile;

            float **p = &env->pickup_obs_map[y * MAP_WIDTH + x];
            *p = NULL;

            switch (source_tile) {
            case DOT_TILE:
                env->possible_spawn_pos[dot_count] = pos;
                dot_count++;
            case POWER_TILE:
                *p = &env->pickup_obs[obs_map_n++];
                break;
            case PLAYER_TILE:
                env->player_spawn_pos = pos;
                break;
            default:
                break;
            }

            for (int i = 0; i < NUM_GHOSTS; i++) {
                if (source_tile == GHOST_TILES[i]) {
                    env->ghosts[i].spawn_pos = pos;
                }
            }
        }
    }
}

void allocate(PacmanEnv *env) {
    init(env);
    env->observations = (float *)calloc(OBSERVATIONS_COUNT, sizeof(float));
    env->actions = (int *)calloc(1, sizeof(int));
    env->rewards = (float *)calloc(1, sizeof(float));
    env->terminals = (char *)calloc(1, sizeof(char));
}

void c_close(PacmanEnv *env) {
    free(env->game_map);
    free(env->possible_spawn_pos);

    free(env->pickup_obs);
    free(env->pickup_obs_map);
}

void free_allocated(PacmanEnv *env) {
    free(env->actions);
    free(env->observations);
    free(env->terminals);
    free(env->rewards);
    c_close(env);
}

#define INV_MAP_WIDTH (1.0f / MAP_WIDTH)
#define INV_MAP_HEIGHT (1.0f / MAP_HEIGHT)

void compute_observations(PacmanEnv *env) {
    float *obs = env->observations;
    // player observations
    obs[0] = env->player_pos.x * INV_MAP_WIDTH;
    obs[1] = env->player_pos.y * INV_MAP_HEIGHT;
    obs[2] = env->player_direction == UP;
    obs[3] = env->player_direction == DOWN;
    obs[4] = env->player_direction == LEFT;
    obs[5] = env->player_direction == RIGHT;
    obs[6] = can_move_in_direction(env, env->player_pos, UP);
    obs[7] = can_move_in_direction(env, env->player_pos, DOWN);
    obs[8] = can_move_in_direction(env, env->player_pos, LEFT);
    obs[9] = can_move_in_direction(env, env->player_pos, RIGHT);
    obs[10] = env->frightened_time_left / (float)env->frightened_time;

    // ghost obs
    for (int i = 0; i < NUM_GHOSTS; i++) {
        Ghost *ghost = &env->ghosts[i];
        int p = PLAYER_OBSERVATIONS_COUNT + (i * GHOST_OBSERVATIONS_COUNT);

        obs[p] = ghost->pos.x * INV_MAP_WIDTH;
        obs[p + 1] = ghost->pos.y * INV_MAP_HEIGHT;
        obs[p + 2] = ghost->direction == UP;
        obs[p + 3] = ghost->direction == DOWN;
        obs[p + 4] = ghost->direction == LEFT;
        obs[p + 5] = ghost->direction == RIGHT;
        obs[p + 6] = !ghost->frightened && !ghost->return_to_spawn;
        obs[p + 7] = ghost->frightened;
        obs[p + 8] = ghost->return_to_spawn;
    }

    memcpy(obs + PLAYER_OBSERVATIONS_COUNT + (NUM_GHOSTS * GHOST_OBSERVATIONS_COUNT),
           env->pickup_obs, sizeof(float) * (NUM_DOTS + NUM_POWERUPS));
}

void update_interpolation(PacmanEnv *env) {
    env->last_player_pos = env->player_pos;

    for (int i = 0; i < NUM_GHOSTS; i++) {
        Ghost *ghost = &env->ghosts[i];
        if (!ghost->frightened || !ghost->half_move) {
            ghost->last_pos = ghost->pos;
        }
    }
}

static inline void reset_round(PacmanEnv *env) {
    env->scatter_mode = false;
    env->mode_time_left = 0;
    env->mode_changes = 0;
    env->frightened_time_left = 0;

    env->step_count = 0;
    env->remaining_pickups = NUM_DOTS + NUM_POWERUPS;

    for (int i = 0; i < NUM_DOTS + NUM_POWERUPS; i++) {
        env->pickup_obs[i] = 1.0f;
    }

    for (int i = 0; i < NUM_GHOSTS; i++) {
        Ghost *ghost = &env->ghosts[i];
        ghost->pos = ghost->spawn_pos;
        ghost->direction = UP;
        ghost->start_timeout = rand_range(env->min_start_timeout, env->max_start_timeout);
        ghost->frightened = false;
        ghost->return_to_spawn = false;
        ghost->half_move = false;
    }

    for (int y = 0; y < MAP_HEIGHT; y++) {
        for (int x = 0; x < MAP_WIDTH; x++) {
            env->game_map[y * MAP_WIDTH + x] = (Tile)original_map[y][x];
        }
    }

    if (env->randomize_starting_position) {
        int player_randomizer = rand() % NUM_DOTS;
        env->player_pos = env->possible_spawn_pos[player_randomizer];
    } else {
        env->player_pos = env->player_spawn_pos;
    }
    env->player_direction = RIGHT;
}

void c_reset(PacmanEnv *env) {
    env->score = 0;
    reset_round(env);
    compute_observations(env);
    update_interpolation(env);
}

static inline void set_frightened(PacmanEnv *env) {
    env->frightened_time_left = env->frightened_time;
    env->reverse_directions = true;

    for (int i = 0; i < NUM_GHOSTS; i++) {
        env->ghosts[i].frightened = !env->ghosts[i].return_to_spawn;
    }
}

static inline void unset_pickup_obs(PacmanEnv *env, Position pos) {
    float *obs = env->pickup_obs_map[pos.y * MAP_WIDTH + pos.x];
    if (obs != NULL) {
        *obs = 0.0f;
    }
}

static inline void player_move(PacmanEnv *env, int action) {
    Position new_pos = pos_move_wrapped(env->player_pos, action, 1);
    Tile *new_tile = tile_at(env, new_pos);

    // if the player action is into a wall, move in the current direction
    if (*new_tile == WALL_TILE) {
        new_pos = pos_move_wrapped(env->player_pos, env->player_direction, 1);
        new_tile = tile_at(env, new_pos);
    } else {
        env->player_direction = action;
    }

    if (*new_tile != WALL_TILE) {
        env->player_pos = new_pos;

        if (*new_tile == POWER_TILE) {
            set_frightened(env);
        }
        if (*new_tile == DOT_TILE || *new_tile == POWER_TILE) {
            env->score += 1.0f;
            env->rewards[0] += 1.0f;

            env->remaining_pickups--;
            *new_tile = EMPTY_TILE;

            unset_pickup_obs(env, new_pos);
        }
    }
}

static inline int ghost_movement_options(PacmanEnv *env, Ghost *ghost, int *directions) {
    int n = 0;
    int rev = reverse_direction(ghost->direction);

    for (int i = 0; i < 4; i++) {
        if (i != rev && can_move_in_direction(env, ghost->pos, i)) {
            directions[n++] = i;
        }
    }
    return n;
}

static inline int min_index(int *array, int length) {
    int min_index = 0;
    for (int i = 1; i < length; i++) {
        if (array[i] < array[min_index]) {
            min_index = i;
        }
    }
    return min_index;
}

static inline int ghost_direction(PacmanEnv *env, Ghost *ghost) {
    int directions[4];
    int distances[4];
    if (env->reverse_directions && !ghost->return_to_spawn) {
        return reverse_direction(ghost->direction);
    }

    int option_count = ghost_movement_options(env, ghost, directions);

    if (option_count == 1) {
        return directions[0];
    }

    if (ghost->frightened) {
        int random_index = rand() % option_count;
        return directions[random_index];
    }

    for (int i = 0; i < option_count; i++) {
        distances[i] = pos_distance_squared(pos_move(ghost->pos, directions[i], 1), ghost->target);
    }

    return directions[min_index(distances, option_count)];
}

#define PINKY 0
#define BLINKY 1
#define INKY 2
#define CLYDE 3

static inline void set_chase_targets(PacmanEnv *env) {
    env->ghosts[PINKY].target = pos_move(env->player_pos, env->player_direction, PINKY_TARGET_LEAD);
    env->ghosts[BLINKY].target = env->player_pos;

    Position inky_intermediate = pos_move(env->player_pos, env->player_direction, INKY_TARGET_LEAD);
    env->ghosts[INKY].target.x = 2 * inky_intermediate.x - env->ghosts[1].pos.x;
    env->ghosts[INKY].target.y = 2 * inky_intermediate.y - env->ghosts[1].pos.y;

    int clyde_distance = pos_distance_squared(env->player_pos, env->ghosts[3].pos);
    if (clyde_distance > CLYDE_TARGET_RADIUS * CLYDE_TARGET_RADIUS) {
        env->ghosts[CLYDE].target = env->player_pos;
    } else {
        env->ghosts[CLYDE].target = GHOST_CORNERS[3];
    }
}

static inline bool check_collision(Position a, Position old_a, Position b, Position old_b) {
    return (a.x >= b.x - 1 && a.x <= b.x + 1 && a.y == b.y) ||
           (a.y >= b.y - 1 && a.y <= b.y + 1 && a.x == b.x);
}

static inline void ghost_move(PacmanEnv *env, Ghost *ghost, Position old_player_pos) {
    Position old_ghost_position = ghost->pos;
    --ghost->start_timeout;

    if (ghost->frightened && ghost->half_move) {
        ghost->half_move = false;
    } else {
        ghost->half_move = true;

        if (ghost->return_to_spawn) {
            ghost->target = ghost->spawn_pos;
        }

        ghost->direction = ghost_direction(env, ghost);

        if (ghost->start_timeout < 0) {
            Position new_pos = pos_move_wrapped(ghost->pos, ghost->direction, 1);
            if (*tile_at(env, new_pos) != WALL_TILE) {
                ghost->pos = new_pos;
            }
        }
    }

    if (ghost->return_to_spawn) {
        if (pos_equal(ghost->pos, ghost->spawn_pos)) {
            ghost->return_to_spawn = false;
        }
    } else if (check_collision(ghost->pos, old_ghost_position, env->player_pos, old_player_pos)) {
        if (ghost->frightened) {
            ghost->frightened = false;
            ghost->half_move = false;
            ghost->return_to_spawn = true;

            env->rewards[0] += 1.0f;
        } else {
            env->player_caught = true;
        }
    }
}

static inline void check_mode_change(PacmanEnv *env) {
    if (env->mode_changes > env->max_mode_changes) {
        return;
    }

    if (--env->mode_time_left <= 0) {
        env->scatter_mode = !env->scatter_mode;
        env->reverse_directions = true;
        env->mode_changes++;

        if (env->scatter_mode) {
            env->mode_time_left = env->scatter_mode_length;
        } else {
            env->mode_time_left = env->chase_mode_length;
        }
    }
}

void c_step(PacmanEnv *env) {
    update_interpolation(env);

    Position old_player_pos = env->player_pos;
    int action = env->actions[0];

    env->step_count += 1;
    env->terminals[0] = 0;
    env->rewards[0] = 0.0f;

    env->reverse_directions = false;
    env->player_caught = false;

    if (env->frightened_time_left > 0) {
        env->frightened_time_left--;
    } else {
        for (int i = 0; i < NUM_GHOSTS; i++) {
            env->ghosts[i].frightened = false;
            env->ghosts[i].half_move = false;
        }
    }

    check_mode_change(env);
    if (env->scatter_mode) {
        for (int i = 0; i < NUM_GHOSTS; i++) {
            env->ghosts[i].target = GHOST_CORNERS[i];
        }
    } else {
        set_chase_targets(env);
    }

    player_move(env, action);

    for (int i = 0; i < NUM_GHOSTS; i++) {
        ghost_move(env, &env->ghosts[i], old_player_pos);
    }

    compute_observations(env);

    if (env->player_caught || env->step_count >= MAX_STEPS || env->remaining_pickups <= 0) {
        add_log(env);

        env->terminals[0] = 1;
        c_reset(env);
    }
}

typedef struct DirectionSprites {
        Texture2D up;
        Texture2D down;
        Texture2D left;
        Texture2D right;
} DirectionSprites;

typedef struct Client Client;
struct Client {
        int tile_size;
        int frame;

        Texture2D tileset;
        Texture2D pacman;
        Texture2D frightened;
        DirectionSprites ghost_sprites[4];
        DirectionSprites eyes;
};

Vector2 lerp_position(Position a, Position b, float progress) {
    if (abs(a.x - b.x) > 1) {
        b.x = a.x;
    }

    float a_x = (float)a.x;
    float a_y = (float)a.y;
    float b_x = (float)b.x;
    float b_y = (float)b.y;

    return (Vector2){a_x + (b_x - a_x) * progress, a_y + (b_y - a_y) * progress};
}

void draw_tiled(Client *client, Texture2D texture, Vector2 position, float rotation, bool flip_x,
                float source_width, float source_height) {
    Rectangle source = (Rectangle){0, 0, flip_x ? -source_width : source_width, source_height};

    DrawTexturePro(texture, source,
                   (Rectangle){(position.x + 0.50f) * client->tile_size,
                               (position.y + 0.50f) * client->tile_size + PX_PADDING_TOP,
                               client->tile_size * 1.5f, client->tile_size * 1.5f},
                   (Vector2){client->tile_size * 0.75f, client->tile_size * 0.75f}, rotation,
                   WHITE);
}

void draw_entity(Client *client, Texture2D texture, Position previous_pos, Position pos,
                 float progress, float rotation, bool flip_x, float source_width,
                 float source_height) {
    Vector2 position;

    if (pos.x == 0 && previous_pos.x == MAP_WIDTH - 1) {
        position = lerp_position((Position){-1, previous_pos.y}, pos, progress);
        draw_tiled(client, texture, position, rotation, flip_x, source_width, source_height);

        position.x += (float)MAP_WIDTH;
        draw_tiled(client, texture, position, rotation, flip_x, source_width, source_height);
    } else if (previous_pos.x == 0 && pos.x == MAP_WIDTH - 1) {
        position = lerp_position((Position){MAP_WIDTH, previous_pos.y}, pos, progress);
        draw_tiled(client, texture, position, rotation, flip_x, source_width, source_height);

        position.x -= (float)MAP_WIDTH;
        draw_tiled(client, texture, position, rotation, flip_x, source_width, source_height);
    } else {
        position = lerp_position(previous_pos, pos, progress);
        draw_tiled(client, texture, position, rotation, flip_x, source_width, source_height);
    }
}


Client *make_client(PacmanEnv *env) {
    Client *client = (Client *)calloc(1, sizeof(Client));
    env->client = client;
    client->tile_size = 20;

    update_interpolation(env);

    srand(time(NULL));

    InitWindow(client->tile_size * MAP_WIDTH, client->tile_size * MAP_HEIGHT + PX_PADDING_TOP,
               "PufferLib Pacman");
    SetTargetFPS(60);

    client->tileset = LoadTexture("resources/pacman/tileset.png");
    client->pacman = LoadTexture("resources/shared/puffers_128.png");
    client->frightened = LoadTexture("resources/pacman/scared.png");

    client->ghost_sprites[0].up = LoadTexture("resources/pacman/pinky_up.png");
    client->ghost_sprites[0].down = LoadTexture("resources/pacman/pinky_down.png");
    client->ghost_sprites[0].left = LoadTexture("resources/pacman/pinky_left.png");
    client->ghost_sprites[0].right = LoadTexture("resources/pacman/pinky_right.png");

    client->ghost_sprites[1].up = LoadTexture("resources/pacman/blinky_up.png");
    client->ghost_sprites[1].down = LoadTexture("resources/pacman/blinky_down.png");
    client->ghost_sprites[1].left = LoadTexture("resources/pacman/blinky_left.png");
    client->ghost_sprites[1].right = LoadTexture("resources/pacman/blinky_right.png");

    client->ghost_sprites[2].up = LoadTexture("resources/pacman/inky_up.png");
    client->ghost_sprites[2].down = LoadTexture("resources/pacman/inky_down.png");
    client->ghost_sprites[2].left = LoadTexture("resources/pacman/inky_left.png");
    client->ghost_sprites[2].right = LoadTexture("resources/pacman/inky_right.png");

    client->ghost_sprites[3].up = LoadTexture("resources/pacman/clyde_up.png");
    client->ghost_sprites[3].down = LoadTexture("resources/pacman/clyde_down.png");
    client->ghost_sprites[3].left = LoadTexture("resources/pacman/clyde_left.png");
    client->ghost_sprites[3].right = LoadTexture("resources/pacman/clyde_right.png");

    client->eyes.up = LoadTexture("resources/pacman/eyes_up.png");
    client->eyes.down = LoadTexture("resources/pacman/eyes_down.png");
    client->eyes.left = LoadTexture("resources/pacman/eyes_left.png");
    client->eyes.right = LoadTexture("resources/pacman/eyes_right.png");

    return client;
}

const Color PUFF_RED = (Color){187, 0, 0, 255};
const Color PUFF_CYAN = (Color){0, 187, 187, 255};
const Color PUFF_WHITE = (Color){241, 241, 241, 241};
const Color PUFF_BACKGROUND = (Color){6, 24, 24, 255};

void render_ghost(Client *client, Ghost *ghost, DirectionSprites *sprites, float progress) {
    Texture2D texture;

    if (ghost->frightened) {
        texture = client->frightened;

        progress *= 0.5f;
        if (!ghost->half_move) {
            progress += 0.5f;
        }
    } else {
        if (ghost->return_to_spawn) {
            sprites = &client->eyes;
        }

        switch (ghost->direction) {
        case UP:
            texture = sprites->up;
            break;
        case DOWN:
            texture = sprites->down;
            break;
        case LEFT:
            texture = sprites->left;
            break;
        case RIGHT:
            texture = sprites->right;
            break;
        }
    }

    draw_entity(client, texture, ghost->last_pos, ghost->pos, progress, 0.0f, false, 16, 16);
}

void render_player(Client *client, PacmanEnv *env, float progress) {
    float rotation = 0.0f;

    if (env->player_direction == UP) {
        rotation = 270.0f;
    } else if (env->player_direction == DOWN) {
        rotation = 90.0f;
    }

    draw_entity(client, client->pacman, env->last_player_pos, env->player_pos, progress,
                rotation, env->player_direction == LEFT, 128, 128);
}

bool is_wall(PacmanEnv *env, int x, int y) {
    Position pos = {x, y};
    if (pos.x < 0 || pos.x >= MAP_WIDTH || pos.y < 0 || pos.y >= MAP_HEIGHT) {
        return true;
    }

    return *tile_at(env, pos) == WALL_TILE;
}

#define TILE_N (1 << 0)
#define TILE_NE (1 << 1)
#define TILE_E (1 << 2)
#define TILE_SE (1 << 3)
#define TILE_S (1 << 4)
#define TILE_SW (1 << 5)
#define TILE_W (1 << 6)
#define TILE_NW (1 << 7)

int get_tile_index(PacmanEnv *env, Position pos) {
    int tile_bits = 0;

    if (is_wall(env, pos.x, pos.y - 1))
        tile_bits |= TILE_N;
    if (is_wall(env, pos.x + 1, pos.y - 1))
        tile_bits |= TILE_NE;
    if (is_wall(env, pos.x + 1, pos.y))
        tile_bits |= TILE_E;
    if (is_wall(env, pos.x + 1, pos.y + 1))
        tile_bits |= TILE_SE;
    if (is_wall(env, pos.x, pos.y + 1))
        tile_bits |= TILE_S;
    if (is_wall(env, pos.x - 1, pos.y + 1))
        tile_bits |= TILE_SW;
    if (is_wall(env, pos.x - 1, pos.y))
        tile_bits |= TILE_W;
    if (is_wall(env, pos.x - 1, pos.y - 1))
        tile_bits |= TILE_NW;

    switch (tile_bits) {
    case TILE_NW | TILE_N | TILE_NE | TILE_W | TILE_E | TILE_SE:
    case TILE_NW | TILE_N | TILE_NE | TILE_W | TILE_E | TILE_SW:
    case TILE_NW | TILE_N | TILE_NE | TILE_W | TILE_E:
        return 13;

    case TILE_NE | TILE_E | TILE_SE | TILE_N | TILE_S | TILE_NW:
    case TILE_NE | TILE_E | TILE_SE | TILE_N | TILE_S | TILE_SW:
    case TILE_NE | TILE_E | TILE_SE | TILE_N | TILE_S:
        return 6;

    case TILE_SE | TILE_S | TILE_SW | TILE_E | TILE_W | TILE_NW:
    case TILE_SE | TILE_S | TILE_SW | TILE_E | TILE_W | TILE_NE:
    case TILE_SE | TILE_S | TILE_SW | TILE_E | TILE_W:
        return 1;

    case TILE_SW | TILE_W | TILE_NW | TILE_S | TILE_N | TILE_NE:
    case TILE_SW | TILE_W | TILE_NW | TILE_S | TILE_N | TILE_SE:
    case TILE_SW | TILE_W | TILE_NW | TILE_S | TILE_N:
        return 8;
    case TILE_S | TILE_E | TILE_SE:
        return 0;
    case TILE_S | TILE_W | TILE_SW:
        return 2;
    case TILE_N | TILE_E | TILE_NE:
        return 12;
    case TILE_N | TILE_W | TILE_NW:
        return 14;

    case 255 & ~TILE_NW:
        return 25;
    case 255 & ~TILE_NE:
        return 26;
    case 255 & ~TILE_SW:
        return 31;
    case 255 & ~TILE_SE:
        return 32;
    }

    return 7;
}

void render_tile(Client *client, PacmanEnv *env, Position pos) {
    int tile_index = get_tile_index(env, pos);
    int tile_x = tile_index % 6;
    int tile_y = tile_index / 6;
    Rectangle source = (Rectangle){tile_x * 9, tile_y * 9, 8, 8};

    DrawTexturePro(client->tileset, source,
                   (Rectangle){pos.x * client->tile_size,
                               pos.y * client->tile_size + PX_PADDING_TOP, client->tile_size,
                               client->tile_size},
                   (Vector2){0, 0}, 0.0f, WHITE);
}

void render_map(Client *client, PacmanEnv *env) {
    for (int y = 0; y < MAP_HEIGHT; y++) {
        for (int x = 0; x < MAP_WIDTH; x++) {
            char tile = env->game_map[y * MAP_WIDTH + x];
            if (tile == WALL_TILE) {
                render_tile(client, env, (Position){x, y});
            } else if (tile == DOT_TILE) {
                float width = client->tile_size / 4.0f;
                float height = client->tile_size / 4.0f;
                DrawRectangle(x * client->tile_size + client->tile_size / 2.0f - width / 2.0f,
                              y * client->tile_size + client->tile_size / 2.0f - height / 2.0f +
                                  PX_PADDING_TOP,
                              width, height, PUFF_WHITE);
            } else if (tile == POWER_TILE) {
                DrawCircle(x * client->tile_size + client->tile_size / 2.0f,
                           y * client->tile_size + client->tile_size / 2.0f + PX_PADDING_TOP,
                           client->tile_size / 3.0f, PUFF_RED);
            }
        }
    }
}

void handle_input(PacmanEnv *env) {
    if (IsKeyDown(KEY_ESCAPE)) {
        exit(0);
    }
    if (IsKeyPressed(KEY_TAB)) {
        ToggleFullscreen();
    }
}

void c_render(PacmanEnv *env) {
    if (env->client == NULL) {
        env->client = make_client(env);
    }
    Client *client = env->client;

    float progress = client->frame / (float)FRAMES;
    client->frame = (client->frame + 1) % (FRAMES);

    handle_input(env);

    BeginDrawing();
    ClearBackground(PUFF_BACKGROUND);

    render_map(client, env);

    render_player(client, env, progress);
    for (int i = 0; i < 4; i++) {
        render_ghost(client, &env->ghosts[i], &client->ghost_sprites[i], progress);
    }

    DrawText(TextFormat("Score: %i", env->score), 10, 10, 20, WHITE);
    DrawText(TextFormat("Mode: %s", env->scatter_mode ? "Scatter" : "Chase"), 150, 10, 20, WHITE);

    EndDrawing();
}

void unload_direction_sprites(DirectionSprites *sprites) {
    UnloadTexture(sprites->up);
    UnloadTexture(sprites->down);
    UnloadTexture(sprites->left);
    UnloadTexture(sprites->right);
}

void close_client(Client *client) {
    CloseWindow();

    UnloadTexture(client->tileset);
    UnloadTexture(client->pacman);
    UnloadTexture(client->frightened);
    for (int i = 0; i < 4; i++) {
        unload_direction_sprites(&client->ghost_sprites[i]);
    }
    unload_direction_sprites(&client->eyes);
    free(client);
}



================================================
FILE: pufferlib/ocean/pacman/pacman.py
================================================
import pufferlib

import numpy as np
import gymnasium

import pufferlib
from pufferlib.ocean.pacman import binding


class Pacman(pufferlib.PufferEnv):
    def __init__(self, num_envs=1, render_mode=None,
            randomize_starting_position = 0,
            min_start_timeout = 0,
            max_start_timeout = 49,
            frightened_time = 35,
            max_mode_changes = 6,
            scatter_mode_length = 70,
            chase_mode_length = 140,
            log_interval=128,
            buf=None, seed=0):
        
        ghost_observations_count = 9
        player_observations_count = 11
        num_ghosts = 4

        num_dots = 244
        observations_count = (player_observations_count + ghost_observations_count * num_ghosts + num_dots)

        self.single_observation_space = gymnasium.spaces.Box(
            low=0,
            high=1,
            shape=(observations_count,),
            dtype=np.float32
        )
        self.single_action_space = gymnasium.spaces.Discrete(4)
        
        self.render_mode = render_mode
        self.num_agents = num_envs
        self.log_interval = log_interval
        self.human_action = None
        self.tick = 0

        super().__init__(buf)

        self.c_envs = binding.vec_init(self.observations, self.actions, self.rewards,
            self.terminals, self.truncations, num_envs, seed,
            randomize_starting_position = randomize_starting_position,
            min_start_timeout = min_start_timeout,
            max_start_timeout = max_start_timeout,
            frightened_time = frightened_time,
            max_mode_changes = max_mode_changes,
            scatter_mode_length = scatter_mode_length,
            chase_mode_length = chase_mode_length,
        )

    def reset(self, seed=None):
        binding.vec_reset(self.c_envs, seed)
        self.tick = 0
        return self.observations, []

    def step(self, actions):
        self.actions[:] = actions
 
        self.tick += 1
        binding.vec_step(self.c_envs)

        info = []
        if self.tick % self.log_interval == 0:
            info.append(binding.vec_log(self.c_envs))

        return (self.observations, self.rewards,
            self.terminals, self.truncations, info)

    def render(self):
        for _ in range(7):
            binding.vec_render(self.c_envs, 0)

    def close(self):
        binding.vec_close(self.c_envs)



================================================
FILE: pufferlib/ocean/pong/binding.c
================================================
#include "pong.h"

#define Env Pong
#include "../env_binding.h"

static int my_init(Env* env, PyObject* args, PyObject* kwargs) {
    env->width = unpack(kwargs, "width");
    env->height = unpack(kwargs, "height");
    env->paddle_width = unpack(kwargs, "paddle_width");
    env->paddle_height = unpack(kwargs, "paddle_height");
    env->ball_width = unpack(kwargs, "ball_width");
    env->ball_height = unpack(kwargs, "ball_height");
    env->paddle_speed = unpack(kwargs, "paddle_speed");
    env->ball_initial_speed_x = unpack(kwargs, "ball_initial_speed_x");
    env->ball_initial_speed_y = unpack(kwargs, "ball_initial_speed_y");
    env->ball_max_speed_y = unpack(kwargs, "ball_max_speed_y");
    env->ball_speed_y_increment = unpack(kwargs, "ball_speed_y_increment");
    env->max_score = unpack(kwargs, "max_score");
    env->frameskip = unpack(kwargs, "frameskip");
    env->continuous = unpack(kwargs, "continuous");
    init(env);
    return 0;
}

static int my_log(PyObject* dict, Log* log) {
    assign_to_dict(dict, "perf", log->perf);
    assign_to_dict(dict, "score", log->score);
    assign_to_dict(dict, "episode_return", log->episode_return);
    assign_to_dict(dict, "episode_length", log->episode_length);
    return 0;
}



================================================
FILE: pufferlib/ocean/pong/cy_pong.pyx
================================================
cimport numpy as cnp
from libc.stdlib cimport calloc, free
import os

cdef extern from "pong.h":
    ctypedef char* LOG_KEYS[];
    ctypedef struct Client:
        pass

    ctypedef struct Pong:
        Client* client;
        float log[4];
        float* observations;
        float* actions;
        float* rewards;
        unsigned char* terminals;
        float paddle_yl;
        float paddle_yr;
        float ball_x;
        float ball_y;
        float ball_vx;
        float ball_vy;
        unsigned int score_l;
        unsigned int score_r;
        float width;
        float height;
        float paddle_width;
        float paddle_height;
        float ball_width;
        float ball_height;
        float paddle_speed;
        float ball_initial_speed_x;
        float ball_initial_speed_y;
        float ball_max_speed_y;
        float ball_speed_y_increment;
        unsigned int max_score;
        float min_paddle_y;
        float max_paddle_y;
        float paddle_dir;
        int tick;
        int n_bounces;
        int win;
        int frameskip;
        int continuous;

    void init(Pong* env)
    void c_reset(Pong* env)
    void c_step(Pong* env)

    void c_render(Pong* env)

cdef class CyPong:
    cdef:
        Pong* envs
        int num_envs
        float width
        float height
        float paddle_width
        float paddle_height
        float ball_width
        float ball_height

    def __init__(self, float[:, :] observations, float[:] actions,
            float[:] rewards, unsigned char[:] terminals, int num_envs,
            float width, float height, float paddle_width, float paddle_height,
            float ball_width, float ball_height, float paddle_speed,
            float ball_initial_speed_x, float ball_initial_speed_y,
            float ball_max_speed_y, float ball_speed_y_increment,
            unsigned int max_score, int frameskip, int continuous):

        self.num_envs = num_envs
        self.envs = <Pong*> calloc(num_envs, sizeof(Pong))

        cdef int i
        for i in range(num_envs):
            self.envs[i] = Pong(
                observations = &observations[i, 0],
                actions = &actions[i],
                rewards = &rewards[i],
                terminals = &terminals[i],
                width=width,
                height=height,
                paddle_width=paddle_width,
                paddle_height=paddle_height,
                ball_width=ball_width,
                ball_height=ball_height,
                paddle_speed=paddle_speed,
                ball_initial_speed_x=ball_initial_speed_x,
                ball_initial_speed_y=ball_initial_speed_y,
                ball_max_speed_y=ball_max_speed_y,
                ball_speed_y_increment=ball_speed_y_increment,
                max_score=max_score,
                frameskip=frameskip,
                continuous=continuous,
            )
            init(&self.envs[i])

    def reset(self):
        cdef int i
        for i in range(self.num_envs):
            c_reset(&self.envs[i])

    def step(self):
        cdef int i
    
        for i in range(self.num_envs):
            c_step(&self.envs[i])

    def render(self):
        cdef Pong* env = &self.envs[0]
        c_render(env)

    def close(self):
        free(self.envs)



================================================
FILE: pufferlib/ocean/pong/pong.c
================================================
#include <time.h>
#include "pong.h"
#include "puffernet.h"

void demo() {
    Weights* weights = load_weights("resources/pong/pong_weights.bin", 133764);

    int logit_sizes[1] = {3};
    LinearLSTM* net = make_linearlstm(weights, 1, 8, logit_sizes, 1);

    Pong env = {
        .width = 500,
        .height = 640,
        .paddle_width = 20,
        .paddle_height = 70,
        .ball_width = 32,
        .ball_height = 32,
        .paddle_speed = 8,
        .ball_initial_speed_x = 10,
        .ball_initial_speed_y = 1,
        .ball_speed_y_increment = 3,
        .ball_max_speed_y = 13,
        .max_score = 21,
        .frameskip = 1,
        .continuous = 0,
    };
    allocate(&env);
    c_reset(&env);
    c_render(&env);
    SetTargetFPS(60);
    int frame = 0;
    while (!WindowShouldClose()) {
        // User can take control of the paddle
        if (IsKeyDown(KEY_LEFT_SHIFT)) {
            if(env.continuous) {
                float move = GetMouseWheelMove();
                float clamped_wheel = fmaxf(-1.0f, fminf(1.0f, move));
                env.actions[0] = clamped_wheel;
                printf("Mouse wheel move: %f\n", env.actions[0]);
            } else {
                env.actions[0] = 0.0;
                if (IsKeyDown(KEY_UP)    || IsKeyDown(KEY_W)) env.actions[0] = 1.0;
                if (IsKeyDown(KEY_DOWN)  || IsKeyDown(KEY_S)) env.actions[0] = 2.0;
            }
        } else if (frame % 8 == 0) {
            // Apply frameskip outside the env for smoother rendering
            int* actions = (int*)env.actions;
            forward_linearlstm(net, env.observations, actions);
            env.actions[0] = actions[0];
        }

        frame = (frame + 1) % 8;
        c_step(&env);
        c_render(&env);
    }
    free_linearlstm(net);
    free(weights);
    free_allocated(&env);
    close_client(env.client);
}

void test_performance(int timeout) {
    Pong env = {
        .width = 500,
        .height = 640,
        .paddle_width = 20,
        .paddle_height = 70,
        .ball_width = 32,
        .ball_height = 32,
        .paddle_speed = 8,
        .ball_initial_speed_x = 10,
        .ball_initial_speed_y = 1,
        .ball_speed_y_increment = 3,
        .ball_max_speed_y = 13,
        .max_score = 21,
        .frameskip = 1,
        .continuous = 0,
    };
    allocate(&env);
    c_reset(&env);

    int start = time(NULL);
    int num_steps = 0;
    while (time(NULL) - start < timeout) {
        env.actions[0] = rand() % 3;
        c_step(&env);
        num_steps++;
    }

    int end = time(NULL);
    float sps = num_steps / (end - start);
    printf("Test Environment SPS: %f\n", sps);
    free_allocated(&env);
}

int main() {
    demo();
    //test_performance(10);
}



================================================
FILE: pufferlib/ocean/pong/pong.h
================================================
#include <stdlib.h>
#include <stdbool.h>
#include <math.h>
#include "raylib.h"

typedef struct Log Log;
struct Log {
    float perf;
    float score;
    float episode_return;
    float episode_length;
    float n;
};

typedef struct Client Client;
typedef struct Pong Pong;
struct Pong {
    Client* client;
    Log log;
    float* observations;
    float* actions;
    float* rewards;
    unsigned char* terminals;
    float paddle_yl;
    float paddle_yr;
    float ball_x;
    float ball_y;
    float ball_vx;
    float ball_vy;
    unsigned int score_l;
    unsigned int score_r;
    float width;
    float height;
    float paddle_width;
    float paddle_height;
    float ball_width;
    float ball_height;
    float paddle_speed;
    float ball_initial_speed_x;
    float ball_initial_speed_y;
    float ball_max_speed_y;
    float ball_speed_y_increment;
    unsigned int max_score;
    float min_paddle_y;
    float max_paddle_y;
    float paddle_dir;
    int tick;
    int n_bounces;
    int win;
    int frameskip;
    int continuous;
};

void init(Pong* env) {
    // logging
    env->tick = 0;
    env->n_bounces = 0;
    env->win = 0;

    // precompute
    env->min_paddle_y = -env->paddle_height / 2;
    env->max_paddle_y = env->height - env->paddle_height/2;
    
    env->paddle_dir = 0;
}

void allocate(Pong* env) {
    init(env);
    env->observations = (float*)calloc(8, sizeof(float));
    env->actions = (float*)calloc(1, sizeof(float));
    env->rewards = (float*)calloc(1, sizeof(float));
    env->terminals = (unsigned char*)calloc(1, sizeof(unsigned char));
}

void free_allocated(Pong* env) {
    free(env->observations);
    free(env->actions);
    free(env->rewards);
    free(env->terminals);
}

void c_close(Pong* env) {
}

void add_log(Pong* env) {
    float score = (float)env->score_r - (float)env->score_l;
    env->log.episode_length += env->tick;
    env->log.episode_return += score;
    env->log.score += score;
    env->log.perf += (float)(env->score_r) / ((float)env->score_l + (float)env->score_r);
    env->log.n += 1;
}

void compute_observations(Pong* env) {
    env->observations[0] = (env->paddle_yl - env->min_paddle_y) / (env->max_paddle_y - env->min_paddle_y);
    env->observations[1] = (env->paddle_yr - env->min_paddle_y) / (env->max_paddle_y - env->min_paddle_y);
    env->observations[2] = env->ball_x / env->width;
    env->observations[3] = env->ball_y / env->height;
    env->observations[4] = (env->ball_vx + env->ball_initial_speed_x) / (2 * env->ball_initial_speed_x);
    env->observations[5] = (env->ball_vy + env->ball_max_speed_y) / (2 * env->ball_max_speed_y);
    env->observations[6] = env->score_l / env->max_score;
    env->observations[7] = env->score_r / env->max_score;
}

void reset_round(Pong* env) {
    env->paddle_yl = env->height / 2 - env->paddle_height / 2;
    env->paddle_yr = env->height / 2 - env->paddle_height / 2;
    env->ball_x = env->width / 5;
    env->ball_y = env->height / 2 - env->ball_height / 2;
    env->ball_vx = env->ball_initial_speed_x;
    env->ball_vy = (rand() % 2 - 1) * env->ball_initial_speed_y;
    env->tick = 0;
    env->n_bounces = 0;
}

void c_reset(Pong* env) {
    reset_round(env);
    env->score_l = 0;
    env->score_r = 0;
    compute_observations(env);
}

void c_step(Pong* env) {
    env->tick += 1;
    env->rewards[0] = 0;
    env->terminals[0] = 0;
    // move ego paddle
    if (env->continuous) {
        env->paddle_dir = env->actions[0];
    } else {
        float act = env->actions[0];
        env->paddle_dir = 0;
        if (act == 0.0) { // still
            env->paddle_dir = 0;
        } else if (act == 1.0) { // up
            env->paddle_dir = 1;
        } else if (act == 2.0) { // down
            env->paddle_dir = -1;
        }
    }

    for (int i = 0; i < env->frameskip; i++) {
        env->paddle_yr += env->paddle_speed * env->paddle_dir;
        
        // move opponent paddle
        float opp_paddle_delta = env->ball_y - (env->paddle_yl + env->paddle_height / 2);
        opp_paddle_delta = fminf(fmaxf(opp_paddle_delta, -env->paddle_speed), env->paddle_speed);
        env->paddle_yl += opp_paddle_delta;

        // clip paddles
        env->paddle_yr = fminf(fmaxf(
            env->paddle_yr, env->min_paddle_y), env->max_paddle_y);
        env->paddle_yl = fminf(fmaxf(
            env->paddle_yl, env->min_paddle_y), env->max_paddle_y);

        // move ball
        env->ball_x += env->ball_vx;
        env->ball_y += env->ball_vy;

        // handle collision with top & bottom walls
        if (env->ball_y < 0 || env->ball_y + env->ball_height > env->height) {
            env->ball_vy = -env->ball_vy;
        }

        // handle collision on left
        if (env->ball_x < 0) {
            if (env->ball_y + env->ball_height > env->paddle_yl && \
                env->ball_y < env->paddle_yl + env->paddle_height) {
                // collision with paddle
                env->ball_vx = -env->ball_vx;
                env->n_bounces += 1;
            } else {
                // collision with wall: WIN
                env->win = 1;
                env->score_r += 1;
                env->rewards[0] = 1; // agent wins
                if (env->score_r == env->max_score) {
                    env->terminals[0] = 1;
                    add_log(env);
                    c_reset(env);
                    return;
                } else {
                    reset_round(env);
                    return;
                }
            }
        }

        // handle collision on right (TODO duplicated code)
        if (env->ball_x + env->ball_width > env->width) {
            if (env->ball_y + env->ball_height > env->paddle_yr && \
                env->ball_y < env->paddle_yr + env->paddle_height) {
                // collision with paddle
                env->ball_vx = -env->ball_vx;
                env->n_bounces += 1;
		env->rewards[0] = 0.1; // agent bounced the ball
                // ball speed change
                env->ball_vy += env->ball_speed_y_increment * env->paddle_dir;
                env->ball_vy = fminf(fmaxf(env->ball_vy, -env->ball_max_speed_y), env->ball_max_speed_y);
                if (fabsf(env->ball_vy) < 0.01) { // we dont want a horizontal ball
                    env->ball_vy = env->ball_speed_y_increment;
                }
            } else {
                // collision with wall: LOSE
                env->win = 0;
                env->score_l += 1;
                env->rewards[0] = -1.0;
                if (env->score_l == env->max_score) {
                    env->terminals[0] = 1;
                    add_log(env);
                    c_reset(env);
                    return;
                } else {
                    reset_round(env);
                    return;
                }
            }

            // clip ball
            env->ball_x = fminf(fmaxf(env->ball_x, 0), env->width - env->ball_width);
            env->ball_y = fminf(fmaxf(env->ball_y, 0), env->height - env->ball_height);
        }
        compute_observations(env);
    }
}

typedef struct Client Client;
struct Client {
    float width;
    float height;
    float paddle_width;
    float paddle_height;
    float ball_width;
    float ball_height;
    float x_pad;
    Color paddle_left_color;
    Color paddle_right_color;
    Color ball_color;
    Texture2D ball;
};

Client* make_client(Pong* env) {
    Client* client = (Client*)calloc(1, sizeof(Client));
    client->width = env->width;
    client->height = env->height;
    client->paddle_width = env->paddle_width;
    client->paddle_height = env->paddle_height;
    client->ball_width = env->ball_width;
    client->ball_height = env->ball_height;
    client->x_pad = 3*client->paddle_width;
    client->paddle_left_color = (Color){255, 0, 0, 255};
    client->paddle_right_color = (Color){0, 255, 255, 255};
    client->ball_color = (Color){255, 255, 255, 255};

    InitWindow(env->width + 2*client->x_pad, env->height, "PufferLib Pong");
    SetTargetFPS(60 / env->frameskip);

    client->ball = LoadTexture("resources/shared/puffers_128.png");
    return client;
}

void close_client(Client* client) {
    CloseWindow();
    free(client);
}

void c_render(Pong* env) {
    if (env->client == NULL) {
        env->client = make_client(env);
    }
    Client* client = env->client;

    if (IsKeyDown(KEY_ESCAPE)) {
        exit(0);
    }

    BeginDrawing();
    ClearBackground((Color){6, 24, 24, 255});

    // Draw left paddle
    DrawRectangle(
        client->x_pad - client->paddle_width,
        client->height - env->paddle_yl - client->paddle_height,
        client->paddle_width,
        client->paddle_height,
        client->paddle_left_color
    );

    // Draw right paddle
    DrawRectangle(
        client->width + client->x_pad,
        client->height - env->paddle_yr - client->paddle_height,
        client->paddle_width,
        client->paddle_height,
        client->paddle_right_color
    );

    // Draw ball
    DrawTexturePro(
        client->ball,
        (Rectangle){
            (env->ball_vx > 0) ? 0 : 128,
            0, 128, 128,
        },
        (Rectangle){
            client->x_pad + env->ball_x,
            client->height - env->ball_y - client->ball_height,
            client->ball_width,
            client->ball_height
        },
        (Vector2){0, 0},
        0,
        WHITE
    );

    //DrawFPS(10, 10);

    // Draw scores
    DrawText(
        TextFormat("%i", env->score_l),
        client->width / 2 + client->x_pad - 50 - MeasureText(TextFormat("%i", env->score_l), 30) / 2,
        10, 30, (Color){0, 187, 187, 255}
    );
    DrawText(
        TextFormat("%i", env->score_r),
        client->width / 2 + client->x_pad + 50 - MeasureText(TextFormat("%i", env->score_r), 30) / 2,
        10, 30, (Color){0, 187, 187, 255}
    );

    EndDrawing();
}



================================================
FILE: pufferlib/ocean/pong/pong.py
================================================
'''High-perf Pong

Inspired from https://gist.github.com/Yttrmin/18ecc3d2d68b407b4be1
& https://jair.org/index.php/jair/article/view/10819/25823
& https://www.youtube.com/watch?v=PSQt5KGv7Vk
'''

import numpy as np
import gymnasium

import pufferlib
from pufferlib.ocean.pong import binding
#import binding

class Pong(pufferlib.PufferEnv):
    def __init__(self, num_envs=1, render_mode=None,
            width=500, height=640, paddle_width=20, paddle_height=70,
            ball_width=32, ball_height=32, paddle_speed=8,
            ball_initial_speed_x=10, ball_initial_speed_y=1,
            ball_speed_y_increment=3, ball_max_speed_y=13,
            max_score=21, frameskip=1, continuous=False, log_interval=128,
            buf=None, seed=0):
        self.single_observation_space = gymnasium.spaces.Box(
            low=0, high=1, shape=(8,), dtype=np.float32,
        )
        if continuous:
            self.single_action_space = gymnasium.spaces.Box(
                low=-1, high=1,  dtype=np.float32,
            )
        else:
            self.single_action_space = gymnasium.spaces.Discrete(3)
        
        self.render_mode = render_mode
        self.num_agents = num_envs
        self.continuous = continuous
        self.log_interval = log_interval
        self.human_action = None
        self.tick = 0

        super().__init__(buf)
        if continuous:
            self.actions = self.actions.flatten()
        else:
            self.actions = self.actions.astype(np.float32)

        self.c_envs = binding.vec_init(self.observations, self.actions, self.rewards,
            self.terminals, self.truncations, num_envs, seed, width=width, height=height,
            paddle_width=paddle_width, paddle_height=paddle_height,
            ball_width=ball_width, ball_height=ball_height,
            paddle_speed=paddle_speed, ball_initial_speed_x=ball_initial_speed_x,
            ball_initial_speed_y=ball_initial_speed_y,
            ball_max_speed_y=ball_max_speed_y, ball_speed_y_increment=ball_speed_y_increment,
            max_score=max_score, frameskip=frameskip, continuous=continuous
        )

    def reset(self, seed=0):
        binding.vec_reset(self.c_envs, seed)
        self.tick = 0
        return self.observations, []

    def step(self, actions):
        if  self.continuous:
            self.actions[:] = np.clip(actions.flatten(), -1.0, 1.0)
        else: 
            self.actions[:] = actions
 
        self.tick += 1
        binding.vec_step(self.c_envs)

        info = []
        if self.tick % self.log_interval == 0:
            info.append(binding.vec_log(self.c_envs))

        return (self.observations, self.rewards,
            self.terminals, self.truncations, info)

    def render(self):
        binding.vec_render(self.c_envs, 0)

    def close(self):
        binding.vec_close(self.c_envs)

#from cy_pong import CyPong
class CythonPong(pufferlib.PufferEnv):
    def __init__(self, num_envs=1, render_mode=None,
            width=500, height=640, paddle_width=20, paddle_height=70,
            ball_width=32, ball_height=32, paddle_speed=8,
            ball_initial_speed_x=10, ball_initial_speed_y=1,
            ball_speed_y_increment=3, ball_max_speed_y=13,
            max_score=21, frameskip=1, continuous=False, report_interval=128, buf=None):
        self.single_observation_space = gymnasium.spaces.Box(
            low=0, high=1, shape=(8,), dtype=np.float32,
        )
        if continuous:
            self.single_action_space = gymnasium.spaces.Box(
                low=-1, high=1,  dtype=np.float32,
            )
        else:
            self.single_action_space = gymnasium.spaces.Discrete(3)
        
        self.render_mode = render_mode
        self.num_agents = num_envs
        self.continuous = continuous
        self.report_interval = report_interval
        self.human_action = None
        self.tick = 0

        super().__init__(buf)
        if continuous:
            self.actions = self.actions.flatten()
        else:
            self.actions = self.actions.astype(np.float32)
        self.c_envs = CyPong(self.observations, self.actions, self.rewards,
            self.terminals, num_envs, width, height,
            paddle_width, paddle_height, ball_width, ball_height,
            paddle_speed, ball_initial_speed_x, ball_initial_speed_y,
            ball_max_speed_y, ball_speed_y_increment, max_score, frameskip, continuous)
 
    def reset(self, seed=None):
        self.tick = 0
        self.c_envs.reset()
        return self.observations, []

    def step(self, actions):
        if  self.continuous:
            self.actions[:] = np.clip(actions.flatten(), -1.0, 1.0)
        else: 
            self.actions[:] = actions
        
        self.c_envs.step()
        info = []
        self.tick += 1
        return (self.observations, self.rewards,
            self.terminals, self.truncations, info)

    def render(self):
        self.c_envs.render()

    def close(self):
        self.c_envs.close()

def test_performance(cls, timeout=10, atn_cache=1024):
    env = cls(num_envs=1000)
    env.reset()
    tick = 0

    actions = np.random.randint(0, 2, (atn_cache, env.num_agents))

    import time
    start = time.time()
    while time.time() - start < timeout:
        atn = actions[tick % atn_cache]
        env.step(atn)
        tick += 1

    print(f'{env.__class__.__name__}: SPS: {env.num_agents * tick / (time.time() - start)}')

if __name__ == '__main__':
    test_performance(Pong)



================================================
FILE: pufferlib/ocean/pysquared/pysquared.py
================================================
'''Pure python version of Squared, a simple single-agent sample environment.
   Use this as a template for your own envs.
'''

# We only use Gymnasium for their spaces API for compatibility with other libraries.
import gymnasium
import numpy as np

import pufferlib

NOOP = 0
DOWN = 1
UP = 2
LEFT = 3
RIGHT = 4

EMPTY = 0
AGENT = 1
TARGET = 2

# Inherit from PufferEnv
class PySquared(pufferlib.PufferEnv):
    # Required keyword arguments: render_mode, buf, seed
    def __init__(self, render_mode='ansi', size=11, buf=None, seed=0):
        # Required attributes below
        self.single_observation_space = gymnasium.spaces.Box(low=0, high=1,
            shape=(size*size,), dtype=np.uint8)
        self.single_action_space = gymnasium.spaces.Discrete(5)
        self.render_mode = render_mode
        self.num_agents = 1

        # Call super after initializing attributes
        super().__init__(buf)

        # Add anything else you want
        self.size = size

    # All methods below are required with the signatures shown
    def reset(self, seed=0):
        self.observations[0, :] = EMPTY
        self.observations[0, self.size*self.size//2] = AGENT
        self.r = self.size//2
        self.c = self.size//2
        self.tick = 0
        while True:
            target_r, target_c = np.random.randint(0, self.size, 2)
            if target_r != self.r or target_c != self.c:
                self.observations[0, target_r*self.size + target_c] = TARGET
                break

        # Observations are read from self. Don't create extra copies
        return self.observations, []

    def step(self, actions):
        atn = actions[0]

        # Note that terminals, rewards, etc. are updated in-place
        self.terminals[0] = False
        self.rewards[0] = 0

        self.observations[0, self.r*self.size + self.c] = EMPTY

        if atn == DOWN:
            self.r += 1
        elif atn == RIGHT:
            self.c += 1
        elif atn == UP:
            self.r -= 1
        elif atn == LEFT:
            self.c -= 1

        # Info is a list of dictionaries
        info = []
        pos = self.r*self.size + self.c
        if (self.tick > 3*self.size
                or self.r < 0
                or self.c < 0
                or self.r >= self.size
                or self.c >= self.size):
            self.terminals[0] = True
            self.rewards[0] = -1.0
            info = [{'reward': -1.0}]
            self.reset()
        elif self.observations[0, pos] == TARGET:
            self.terminals[0] = True
            self.rewards[0] = 1.0
            info = [{'reward': 1.0}]
            self.reset()
        else:
            self.observations[0, pos] = AGENT
            self.tick += 1

        # Return the in-place versions. Don't copy!
        return self.observations, self.rewards, self.terminals, self.truncations, info

    def render(self):
        # Quick ascii rendering. If you want a Python-based renderer,
        # we highly recommend Raylib over PyGame etc. If you use the
        # C-style Python API, it will be very easy to port to C native later.
        chars = []
        grid = self.observations.reshape(self.size, self.size)
        for row in grid:
            for val in row:
                if val == AGENT:
                    color = 94
                elif val == TARGET:
                    color = 91
                else:
                    color = 90
                chars.append(f'\033[{color}m██\033[0m')
            chars.append('\n')
        return ''.join(chars)

    def close(self):
        pass

if __name__ == '__main__':
    env = PySquared()
    env.reset()
    steps = 0

    CACHE = 1024
    actions = np.random.randint(0, 5, (CACHE, 1))

    import time
    start = time.time()
    while time.time() - start < 10:
        env.step(actions[steps % CACHE])
        steps += 1

    print('PySquared SPS:', int(steps / (time.time() - start)))



================================================
FILE: pufferlib/ocean/robocode/build_local.sh
================================================
clang -Wall -Wuninitialized -Wmisleading-indentation -fsanitize=address,undefined,bounds,pointer-overflow,leak -ferror-limit=3 -g -o robocode robocode.c -I./raylib-5.0_linux_amd64/include/ -L./raylib-5.0_linux_amd64/lib/ -lraylib -lGL -lm -lpthread -ldl -lrt -lX11 -DPLATFORM_DESKTOP





================================================
FILE: pufferlib/ocean/robocode/robocode.c
================================================
#include "robocode.h"

int main() {
    Env env = {0};
    env.num_agents = 2;
    env.width = 768;
    env.height = 576;
    allocate_env(&env);
    reset(&env);

    Client* client = make_client(&env);

    while (!WindowShouldClose()) {
        for (int i = 0; i < NUM_ACTIONS; i++) {
            env.actions[i] = 0;
        }

        env.actions[0] = 16.0f;
        float x = env.robots[0].x;
        float y = env.robots[0].y;
        float op_x = env.robots[1].x;
        float op_y = env.robots[1].y;
        float gun_heading = env.robots[0].gun_heading;
        float angle_to_op = 180*atan2(op_y - y, op_x - x)/M_PI;
        float gun_delta = angle_to_op - gun_heading;
        if (gun_delta < -180) gun_delta += 360;
        env.actions[2] = (gun_delta > 0) ? 1.0f : -1.0f;
        if (gun_delta < 5 && gun_delta > -5) env.actions[4] = 1.0;

        env.actions[5] = 16.0f;
        x = env.robots[1].x;
        y = env.robots[1].y;
        op_x = env.robots[0].x;
        op_y = env.robots[0].y;
        gun_heading = env.robots[1].gun_heading;
        angle_to_op = 180*atan2(op_y - y, op_x - x)/M_PI;
        gun_delta = angle_to_op - gun_heading;
        if (gun_delta < -180) gun_delta += 360;
        env.actions[7] = (gun_delta > 0) ? 1.0f : -1.0f;
        if (gun_delta < 5 && gun_delta > -5) env.actions[9] = 1.0;


        //if (IsKeyPressed(KEY_ESCAPE)) break;
        if (IsKeyDown(KEY_W)) env.actions[0] = 16.0f;
        if (IsKeyDown(KEY_S)) env.actions[0] = -16.0f;
        if (IsKeyDown(KEY_A)) env.actions[1] = -2.0f;
        if (IsKeyDown(KEY_D)) env.actions[1] = 2.0f;
        if (IsKeyDown(KEY_Q)) env.actions[2] = -1.0f;
        if (IsKeyDown(KEY_E)) env.actions[2] = 1.0f;
        if (IsKeyDown(KEY_SPACE)) env.actions[4] = 1.0f;

        step(&env);
        render(client, &env);
    }
    CloseWindow();
    return 0;
}



================================================
FILE: pufferlib/ocean/robocode/robocode.h
================================================
#include <stdlib.h>
#include <stdbool.h>
#include <stdio.h>
#include <assert.h>
#include <math.h>
#include "raylib.h"

#define NUM_ACTIONS 5
#define NUM_BULLETS 16

float cos_deg(float deg) {
    return cos(deg * 3.14159265358979323846 / 180.0);
}

float sin_deg(float deg) {
    return sin(deg * 3.14159265358979323846 / 180.0);
}

typedef struct Bullet Bullet;
struct Bullet {
    float x;
    float y;
    float heading;
    float firepower;
    bool live;
};

typedef struct Robot Robot;
struct Robot {
    float x;
    float y;
    float v;
    float heading;
    float gun_heading;
    float radar_heading_prev;
    float radar_heading;
    float gun_heat;
    int energy;
    int bullet_idx;
};

typedef struct Env Env;
struct Env {
    int num_agents;
    int width;
    int height;
    Robot* robots;
    Bullet* bullets;
    float* actions;
};

void allocate_env(Env* env) {
    env->robots = (Robot*)calloc(env->num_agents, sizeof(Robot));
    env->bullets = (Bullet*)calloc(NUM_BULLETS*env->num_agents, sizeof(Bullet));
    env->actions = (float*)calloc(NUM_ACTIONS*env->num_agents, sizeof(float));
}

void free_env(Env* env) {
    free(env->robots);
    free(env->actions);
}

void move(Env* env, Robot* robot, float distance) {
    float dx = cos_deg(robot->heading);
    float dy = sin_deg(robot->heading);
    //float accel = 1.0;//2.0*distance / (robot->v * robot->v);
    float accel = distance;

    if (accel > 1.0) {
        accel = 1.0;
    } else if (accel < -2.0) {
        accel = -2.0;
    }

    robot->v += accel;
    if (robot->v > 8.0) {
        robot->v = 8.0;
    } else if (robot->v < -8.0) {
        robot->v = -8.0;
    }

    float new_x = robot->x + dx * robot->v;
    float new_y = robot->y + dy * robot->v;

    // Collision check
    for (int j = 0; j < env->num_agents; j++) {
        Robot* target = &env->robots[j];
        if (target == robot) {
            continue;
        }
        float dx = target->x - new_x;
        float dy = target->y - new_y;
        float dist = sqrt(dx*dx + dy*dy);
        if (dist > 32.0f) {
            continue;
        }

        target->energy -= 0.6;
        robot->energy -= 0.6;
        return;
    }
    
    robot->x = new_x;
    robot->y = new_y;

}

float turn(Env* env, Robot* robot, float degrees) {
    float abs_v = fabs(robot->v);
    float d_angle = 10 - 0.75*abs_v;
    if (degrees > d_angle) {
        degrees = d_angle;
    } else if (degrees < -d_angle) {
        degrees = -d_angle;
    }

    robot->heading += degrees;
    if (robot->heading > 360) {
        robot->heading -= 360;
    } else if (robot->heading < 0) {
        robot->heading += 360;
    }
    return degrees;
}

void fire(Env* env, Robot* robot, float firepower) {
    if (robot->gun_heat > 0) {
        return;
    }
    if (robot->energy < firepower) {
        return;
    }
    robot->energy -= firepower;

    Bullet* bullet = &env->bullets[robot->bullet_idx];
    robot->bullet_idx = (robot->bullet_idx + 1) % NUM_BULLETS;
    robot->gun_heat += 1.0f + firepower/5.0f;

    bullet->x = robot->x + 64*cos_deg(robot->gun_heading);
    bullet->y = robot->y + 64*sin_deg(robot->gun_heading);
    bullet->heading = robot->gun_heading;
    bullet->firepower = firepower;
    bullet->live = true;
}

void reset(Env* env) {
    int idx = 0;
    float x, y;
    while (idx < env->num_agents) {
        Robot* robot = &env->robots[idx];
        x = 16 + rand() % (env->width-32);
        y = 16 + rand() % (env->height-32);
        bool collided = false;
        for (int j = 0; j < idx; j++) {
            Robot* other = &env->robots[j];
            float dx = x - other->x;
            float dy = y - other->y;
            float dist = sqrt(dx*dx + dy*dy);
            if (dist < 32.0f) {
                collided = true;
                break;
            }
        }
        if (!collided) {
            robot->x = x;
            robot->y = y;
            robot->v = 0;
            robot->heading = 0;
            robot->energy = 100;
            robot->gun_heat = 3;
            idx += 1;
        }
    }
}

void step(Env* env) {
    // Update bullets
    for (int agent = 0; agent < env->num_agents; agent++) {
        Robot* robot = &env->robots[agent];
        if (robot->energy <= 0) {
            reset(env);
            return;
        }

        for (int blt = 0; blt < NUM_BULLETS; blt++) {
            Bullet* bullet = &env->bullets[agent*NUM_BULLETS + blt];
            if (!bullet->live) {
                continue;
            }

            float v = 20.0f - 3.0f*bullet->firepower;
            bullet->x += v*cos_deg(bullet->heading);
            bullet->y += v*sin_deg(bullet->heading);

            // Bounds check
            if (bullet->x < 0 || bullet->x > env->width
                    || bullet->y < 0 || bullet->y > env->height) {
                bullet->live = false;
                continue;
            }

            // Collision check
            for (int j = 0; j < env->num_agents; j++) {
                Robot* target = &env->robots[j];
                float dx = target->x - bullet->x;
                float dy = target->y - bullet->y;
                float dist = sqrt(dx*dx + dy*dy);
                if (dist > 32.0f) {
                    continue;
                }

                float damage = 4*bullet->firepower;
                if (bullet->firepower > 1.0f) {
                    damage += 2*(bullet->firepower - 1.0f);
                }

                target->energy -= damage;
                robot->energy += 3*bullet->firepower;
                bullet->live = false;
            }
        }
    }

    for (int i = 0; i < env->num_agents; i++) {
        Robot* robot = &env->robots[i];
        int atn_offset = i*NUM_ACTIONS;

        // Cool down gun
        if (robot->gun_heat > 0) {
            robot->gun_heat -= 0.1f;
        }

        // Move
        int move_atn = env->actions[atn_offset];
        move(env, robot, move_atn);

        // Turn
        int turn_atn = env->actions[atn_offset + 1];
        float turn_degrees = turn(env, robot, turn_atn);

        // Gun 
        float gun_degrees = env->actions[atn_offset + 2] + turn_degrees;
        robot->gun_heading += gun_degrees;
        if (robot->gun_heading > 360) {
            robot->gun_heading -= 360;
        } else if (robot->gun_heading < 0) {
            robot->gun_heading += 360;
        }

        // Radar
        float radar_degrees = env->actions[atn_offset + 3] + gun_degrees;
        robot->radar_heading_prev = robot->radar_heading;
        robot->radar_heading += radar_degrees;
        if (robot->radar_heading > 360) {
            robot->radar_heading -= 360;
        } else if (robot->radar_heading < 0) {
            robot->radar_heading += 360;
        }

        // Fire
        float firepower = env->actions[atn_offset + 4];
        if (firepower > 0) {
            fire(env, robot, firepower);
        }

        // Clip position
        if (robot->x < 16) {
            robot->x = 16;
        } else if (robot->x > env->width - 16) {
            robot->x = env->width - 16;
        }
        if (robot->y < 16) {
            robot->y = 16;
        } else if (robot->y > env->height - 16) {
            robot->y = env->height - 16;
        }
    }
}

typedef struct Client Client;
struct Client {
    Texture2D atlas;
};

Client* make_client(Env* env) {
    InitWindow(768, 576, "PufferLib Ray Robocode");
    SetTargetFPS(60);
    Client* client = (Client*)calloc(1, sizeof(Client));
    client->atlas = LoadTexture("resources/robocode/robocode.png");
    return client;
}

void close_client(Client* client) {
    UnloadTexture(client->atlas);
    CloseWindow();
}

void render(Client* client, Env* env) {
    BeginDrawing();
    ClearBackground((Color){6, 24, 24, 255});

    for (int x = 0; x < env->width; x+=64) {
        for (int y = 0; y < env->height; y+=64) {
            int src_x = 64 * ((x*33409+ y*30971) % 5);
            Rectangle src_rect = (Rectangle){src_x, 0, 64, 64};
            Vector2 dest_pos = (Vector2){x, y};
            DrawTextureRec(client->atlas, src_rect, dest_pos, WHITE);
        }
    }

    for (int i = 0; i < env->num_agents; i++) {
        int atn_offset = i*NUM_ACTIONS;
        int turn_atn = env->actions[atn_offset + 1];
        int gun_atn = env->actions[atn_offset + 2] + turn_atn;
        int radar_atn = env->actions[atn_offset + 3] + gun_atn;

        Robot robot = env->robots[i];
        Vector2 robot_pos = (Vector2){robot.x, robot.y};

        // Radar
        float radar_left = (radar_atn > 0) ? robot.radar_heading: robot.radar_heading_prev;
        float radar_right = (radar_atn > 0) ? robot.radar_heading_prev : robot.radar_heading;
        Vector2 radar_left_pos = (Vector2){
            robot.x + 1200*cos_deg(radar_left),
            robot.y + 1200*sin_deg(radar_left)
        };
        Vector2 radar_right_pos = (Vector2){
            robot.x + 1200*cos_deg(radar_right),
            robot.y + 1200*sin_deg(radar_right)
        };
        DrawTriangle(robot_pos, radar_left_pos, radar_right_pos, (Color){0, 255, 0, 128});

        // Gun 
        Vector2 gun_pos = (Vector2){
            robot.x + 64*cos_deg(robot.gun_heading),
            robot.y + 64*sin_deg(robot.gun_heading)
        };
        //DrawLineEx(robot_pos, gun_pos, 4, WHITE);

        // Robot
        //DrawCircle(robot.x, robot.y, 32, RED);
        //DrawCircle(robot.x, robot.y, 16, WHITE);
        float theta = robot.heading;
        float dx = cos_deg(theta);
        float dy = sin_deg(theta);
        int src_y = 64 + 64*(i%2);
        Rectangle body_rect = (Rectangle){0, src_y, 64, 64};
        Rectangle radar_rect = (Rectangle){64, src_y, 64, 64};
        Rectangle gun_rect = (Rectangle){128, src_y, 64, 64};
        Rectangle dest_rect = (Rectangle){robot.x, robot.y, 64, 64};
        Vector2 origin = (Vector2){32, 32};
        DrawTexturePro(client->atlas, body_rect, dest_rect, origin, robot.heading+90, WHITE);
        DrawTexturePro(client->atlas, radar_rect, dest_rect, origin, robot.radar_heading+90, WHITE);
        DrawTexturePro(client->atlas, gun_rect, dest_rect, origin, robot.gun_heading+90, WHITE);

        DrawText(TextFormat("%i", robot.energy), robot.x-16, robot.y-48, 12, WHITE);
    }

    for (int i = 0; i < env->num_agents*NUM_BULLETS; i++) {
        Bullet bullet = env->bullets[i];
        if (!bullet.live) {
            continue;
        }
        Vector2 bullet_pos = (Vector2){bullet.x, bullet.y};
        DrawCircleV(bullet_pos, 4, WHITE);
    }

    EndDrawing();
}



================================================
FILE: pufferlib/ocean/rocket_lander/cy_rocket_lander.pyx
================================================
cimport numpy as cnp
from libc.stdlib cimport calloc, free
import os

cdef extern from "rocket_lander.h":
    int LOG_BUFFER_SIZE

    ctypedef struct b2WorldId:
        unsigned short index1
        unsigned short revision
    ctypedef struct b2BodyId:
        int index1
        unsigned short revision
        unsigned char world0
    ctypedef struct b2Vec2:
        float x
        float y

    ctypedef struct Log:
        float episode_return;
        float episode_length;
        float score;

    ctypedef struct LogBuffer
    LogBuffer* allocate_logbuffer(int)
    void free_logbuffer(LogBuffer*)
    Log aggregate_and_clear(LogBuffer*)

    ctypedef struct Entity:
        b2BodyId bodyId;
        b2Vec2 extent;

    ctypedef struct Lander:
        float* observations;
        float* actions;
        float* reward;
        unsigned char* terminal;
        unsigned char* truncation;
        LogBuffer* log_buffer;
        Log log;
        int tick;
        b2WorldId world_id;
        b2BodyId barge_id;
        b2BodyId lander_id;
        Entity barge;
        Entity lander;

    ctypedef struct Client

    void init_lander(Lander* env)
    void reset(Lander* env)
    void step(Lander* env)
    void free_lander(Lander* env)

    Client* make_client(Lander* env)
    void render(Client* client, Lander* env)
    void close_client(Client* client)

cdef class CyRocketLander:
    cdef:
        Lander* envs
        Client* client
        LogBuffer* logs
        int num_envs

    def __init__(self, cnp.ndarray observations, cnp.ndarray actions,
            cnp.ndarray rewards, cnp.ndarray terminals,
            cnp.ndarray truncations, int num_envs):

        self.num_envs = num_envs
        self.client = NULL
        self.envs = <Lander*> calloc(num_envs, sizeof(Lander))
        self.logs = allocate_logbuffer(LOG_BUFFER_SIZE)

        cdef:
            cnp.ndarray observations_i
            cnp.ndarray actions_i
            cnp.ndarray rewards_i
            cnp.ndarray terminals_i
            cnp.ndarray truncations_i

        cdef int i
        for i in range(num_envs):
            observations_i = observations[i:i+1]
            actions_i = actions[i:i+1]
            rewards_i = rewards[i:i+1]
            terminals_i = terminals[i:i+1]
            truncations_i = truncations[i:i+1]
            self.envs[i] = Lander(
                observations = <float*> observations_i.data,
                actions = <float*> actions_i.data,
                reward = <float*> rewards_i.data,
                terminal = <unsigned char*> terminals_i.data,
                truncation = <unsigned char*> truncations_i.data,
                log_buffer=self.logs,
            )
            init_lander(&self.envs[i])

    def reset(self):
        cdef int i
        for i in range(self.num_envs):
            reset(&self.envs[i])

    def step(self):
        cdef int i
        for i in range(self.num_envs):
            step(&self.envs[i])

    def render(self):
        cdef Lander* env = &self.envs[0]
        if self.client == NULL:
            self.client = make_client(env)

        render(self.client, env)

    def close(self):
        if self.client != NULL:
            close_client(self.client)
            self.client = NULL

        cdef int i
        for i in range(self.num_envs):
            free_lander(&self.envs[i])
        free(self.envs)
        free(self.logs)

    def log(self):
        cdef Log log = aggregate_and_clear(self.logs)
        return log



================================================
FILE: pufferlib/ocean/rocket_lander/rocket_lander.c
================================================
#include "rocket_lander.h"

int main(void) {
    demo();
    return 0;
}



    /*
    Entity legs[2] = {0};
    for (int i = 0; i < 2; i++) {
        float leg_i = (i == 0) ? -1 : 1;
        b2Vec2 leg_extent = (b2Vec2){LEG_W / SCALE, LEG_H / SCALE};

        b2BodyDef leg = b2DefaultBodyDef();
        leg.type = b2_dynamicBody;
        leg.position = (b2Vec2){-leg_i * LEG_AWAY, INITIAL_Y - LANDER_HEIGHT/2 - leg_extent.y/2};
        //leg.position = (b2Vec2){0, 0};
        leg.rotation = b2MakeRot(leg_i * 1.05);
        b2BodyId leg_id = b2CreateBody(world_id, &leg);

        b2Polygon leg_box = b2MakeBox(leg_extent.x, leg_extent.y);
        b2ShapeDef leg_shape = b2DefaultShapeDef();
        b2CreatePolygonShape(leg_id, &leg_shape, &leg_box);

        float joint_x = leg_i*LANDER_WIDTH/2;
        float joint_y = INITIAL_Y - LANDER_HEIGHT/2 - leg_extent.y/2;
        b2Vec2 joint_p = (b2Vec2){joint_x, joint_y};

        b2RevoluteJointDef joint = b2DefaultRevoluteJointDef();
        joint.bodyIdA = lander_id;
        joint.bodyIdB = leg_id;
        joint.localAnchorA = b2Body_GetLocalPoint(lander_id, joint_p);
        joint.localAnchorB = b2Body_GetLocalPoint(leg_id, joint_p);
        joint.localAnchorB = (b2Vec2){i * 0.5, LEG_DOWN};
        joint.enableMotor = true;
        joint.enableLimit = true;
        joint.maxMotorTorque = LEG_SPRING_TORQUE;
        joint.motorSpeed = 0.3*i;

        if (i == 0) {
            joint.lowerAngle = 40 * DEGTORAD;
            joint.upperAngle = 45 * DEGTORAD;
        } else {
            joint.lowerAngle = -45 * DEGTORAD;
            joint.upperAngle = -40 * DEGTORAD;
        }

        b2JointId joint_id = b2CreateRevoluteJoint(world_id, &joint);

        legs[i] = (Entity){
            .extent = leg_extent,
            .bodyId = leg_id,
        };
    }
    */





================================================
FILE: pufferlib/ocean/rocket_lander/rocket_lander.h
================================================
#include <stdio.h>
#include <stdlib.h>
#include <assert.h>
#include <math.h>

#include "raylib.h"
#include "box2d/box2d.h"

// This shows how to use Box2D v3 with raylib.
// It also show how to use Box2D with pixel units.
//
#define LOG_BUFFER_SIZE 1024

typedef struct Log Log;
struct Log {
    float episode_return;
    float episode_length;
    float score;
};

typedef struct LogBuffer LogBuffer;
struct LogBuffer {
    Log* logs;
    int length;
    int idx;
};

LogBuffer* allocate_logbuffer(int size) {
    LogBuffer* logs = (LogBuffer*)calloc(1, sizeof(LogBuffer));
    logs->logs = (Log*)calloc(size, sizeof(Log));
    logs->length = size;
    logs->idx = 0;
    return logs;
}

void free_logbuffer(LogBuffer* buffer) {
    free(buffer->logs);
    free(buffer);
}

void add_log(LogBuffer* logs, Log* log) {
    if (logs->idx == logs->length) {
        return;
    }
    logs->logs[logs->idx] = *log;
    logs->idx += 1;
    //printf("Log: %f, %f, %f\n", log->episode_return, log->episode_length, log->score);
}

Log aggregate_and_clear(LogBuffer* logs) {
    Log log = {0};
    if (logs->idx == 0) {
        return log;
    }
    for (int i = 0; i < logs->idx; i++) {
        log.episode_return += logs->logs[i].episode_return;
        log.episode_length += logs->logs[i].episode_length;
        log.score += logs->logs[i].score;
    }
    log.episode_return /= logs->idx;
    log.episode_length /= logs->idx;
    log.score /= logs->idx;
    logs->idx = 0;
    return log;
}
 
typedef struct Entity
{
	b2BodyId bodyId;
	b2Vec2 extent;
} Entity;

#define GROUND_COUNT 14
#define BOX_COUNT 10

const float SCALE = 30;
const float VIEWPORT_W = 1000;
const float VIEWPORT_H = 800;
const float GRAVITY = 9.8f;
const float W = VIEWPORT_W / SCALE;
const float H = VIEWPORT_H / SCALE;

const float BARGE_FRICTION = 2;
const float BARGE_HEIGHT = 10;
const float BARGE_WIDTH = 100;

const float LANDER_WIDTH = 20;
const float LANDER_HEIGHT = 227;

const float LEG_AWAY = 20;
const float LEG_DOWN = 0.3;
const float LEG_W = 30;
const float LEG_H = 10*LANDER_HEIGHT / 8;
const float LEG_SPRING_TORQUE = LANDER_HEIGHT / 2;

const float INITIAL_Y = 500;


const float PX_PER_METER = 1.0f;
const float DEGTORAD = PI / 180.0f;

const float THRUST_SCALE = 1000000;
const float SIDE_THRUST_SCALE = 1000000;

void DrawEntity(const Entity* entity, Color color)
{
	// The boxes were created centered on the bodies, but raylib draws textures starting at the top left corner.
	// b2Body_GetWorldPoint gets the top left corner of the box accounting for rotation.
	b2Vec2 p = b2Body_GetWorldPoint(entity->bodyId, (b2Vec2){0, 0});
    float width = 2*entity->extent.x;
    float height = 2*entity->extent.y;

    b2Rot rotation = b2Body_GetRotation(entity->bodyId);
    float radians = b2Rot_GetAngle(rotation);
    float degrees = radians / DEGTORAD;

	//b2Rot rotation = b2Body_GetRotation(entity->bodyId);
	//float radians = b2Rot_GetAngle(rotation);
    //printf("\t: x: %f, y: %f, w: %f, h: %f, deg: %f\n", p.x, p.y, width, height, degrees);

    Rectangle rec = (Rectangle){
        PX_PER_METER*p.x,
        -PX_PER_METER*p.y,
        PX_PER_METER*width,
        PX_PER_METER*height,
    };
    DrawRectanglePro(rec, (Vector2){rec.width/2, rec.height/2}, -degrees, color);
	//DrawTextureEx(entity->texture, ps, RAD2DEG * radians, 1.0f, WHITE);

	// I used these circles to ensure the coordinates are correct
	//DrawCircleV(ps, 5.0f, BLACK);
	//p = b2Body_GetWorldPoint(entity->bodyId, (b2Vec2){0.0f, 0.0f});
	//ps = (Vector2){ p.x, p.y };
	//DrawCircleV(ps, 5.0f, BLUE);
	//p = b2Body_GetWorldPoint(entity->bodyId, (b2Vec2){ entity->extent.x, entity->extent.y });
	//ps = (Vector2){ p.x, p.y };
	//DrawCircleV(ps, 5.0f, RED);
}


typedef struct Lander Lander;
struct Lander {
    float* observations;
    float* actions;
    float* reward;
    unsigned char* terminal;
    unsigned char* truncation;
    LogBuffer* log_buffer;
    Log log;
    int tick;
    b2WorldId world_id;
    b2BodyId barge_id;
    b2BodyId lander_id;
    Entity barge;
    Entity lander;
};

void init_lander(Lander* env) {
	b2SetLengthUnitsPerMeter(PX_PER_METER);

	// 128 pixels per meter is a appropriate for this scene. The boxes are 128 pixels wide.
	b2WorldDef worldDef = b2DefaultWorldDef();

	// Realistic gravity is achieved by multiplying gravity by the length unit.
	worldDef.gravity.y = -9.8f * PX_PER_METER;
	b2WorldId world_id = b2CreateWorld(&worldDef);
    env->world_id = world_id;

    b2BodyDef barge_body = b2DefaultBodyDef();
    barge_body.position = (b2Vec2){0, 0};
    barge_body.type = b2_staticBody;
    b2BodyId barge_id = b2CreateBody(world_id, &barge_body);
    env->barge_id = barge_id;

    b2Vec2 barge_extent = (b2Vec2){BARGE_WIDTH/2, BARGE_HEIGHT/2};
    b2Polygon barge_box = b2MakeBox(barge_extent.x, barge_extent.y);
    b2ShapeDef barge_shape = b2DefaultShapeDef();
    b2CreatePolygonShape(barge_id, &barge_shape, &barge_box);
    Entity barge = {
        .extent = barge_extent,
        .bodyId = barge_id,
    };
    env->barge = barge;

    b2BodyDef lander_body = b2DefaultBodyDef();
    lander_body.position = (b2Vec2){0, INITIAL_Y};
    lander_body.type = b2_dynamicBody;
    b2BodyId lander_id = b2CreateBody(world_id, &lander_body);
    env->lander_id = lander_id;

    b2Vec2 lander_extent = (b2Vec2){LANDER_WIDTH / 2, LANDER_HEIGHT / 2};
    b2Polygon lander_box = b2MakeBox(lander_extent.x, lander_extent.y);
    b2ShapeDef lander_shape = b2DefaultShapeDef();
    b2CreatePolygonShape(lander_id, &lander_shape, &lander_box);
    Entity lander = {
        .extent = lander_extent,
        .bodyId = lander_id,
    };
    env->lander = lander;
}

void allocate_lander(Lander* env) {
    env->observations = (float*)calloc(6, sizeof(float));
    env->actions = (float*)calloc(3, sizeof(float));
    env->reward = (float*)calloc(1, sizeof(float));
    env->terminal = (unsigned char*)calloc(1, sizeof(unsigned char));
    env->truncation = (unsigned char*)calloc(1, sizeof(unsigned char));
    env->log_buffer = allocate_logbuffer(LOG_BUFFER_SIZE);
    init_lander(env);
}

void compute_observations_and_reward(Lander* env, float prev_x, float prev_y) {
    b2Transform transform = b2Body_GetTransform(env->lander_id);
    b2Vec2 pos = transform.p;
    float rot = b2Rot_GetAngle(transform.q);
    b2Vec2 vel = b2Body_GetLinearVelocity(env->lander_id);
    float ang_vel = b2Body_GetAngularVelocity(env->lander_id);

    float reward_x = (fabsf(prev_x) - fabsf(pos.x))/ 1000;
    float reward_y = (fabsf(prev_y) - fabsf(pos.y))/ 1000;
    //float reward_rot = -fabsf(rot)/ PI / 10;
    //printf("Reward: %f, %f, %f\n", reward_x, reward_y, reward_rot);
    float reward = reward_x + reward_y;// + reward_rot;
    
    reward = 0;
    if (env->actions[0] == 0) {
        reward = 1;
    }

    env->reward[0] = reward;
    env->log.episode_return += reward;

    env->observations[0] = pos.x / 500;
    env->observations[1] = pos.y / 1200;
    env->observations[2] = vel.x / 200;
    env->observations[3] = vel.y / 200;
    env->observations[4] = rot / PI / 10;
    env->observations[5] = ang_vel / PI;
}

void reset(Lander* env) {
    env->log = (Log){0};
    env->tick = 0;

    b2Body_SetTransform(
        env->lander_id,
        (b2Vec2){0, INITIAL_Y},
        b2MakeRot(0)
    );
    b2Body_SetLinearVelocity(env->lander_id, (b2Vec2){0, 0});
    b2Body_SetAngularVelocity(env->lander_id, 0.0f); 

    b2Transform transform = b2Body_GetTransform(env->lander_id);
    b2Vec2 pos = transform.p;
    env->reward[0] = 0;
    compute_observations_and_reward(env, pos.x, pos.y);
    env->reward[0] = 0;
}

void step(Lander* env) {
    env->reward[0] = 0;
    b2Transform transform = b2Body_GetTransform(env->lander_id);
    b2Vec2 pos = transform.p;
    printf("Pos x: %f, y: %f\n", pos.x, pos.y);

    b2Vec2 p_thrust = b2Body_GetWorldPoint(env->lander_id,
        (b2Vec2){0, -LANDER_HEIGHT/2});
    b2Vec2 p_left = b2Body_GetWorldPoint(env->lander_id,
        (b2Vec2){-LANDER_WIDTH/2, LANDER_HEIGHT/2});
    b2Vec2 p_right= b2Body_GetWorldPoint(env->lander_id,
        (b2Vec2){LANDER_WIDTH/2, LANDER_HEIGHT/2});

    b2Vec2 force = (b2Vec2){0, 0};
    b2Rot rotation = b2Body_GetRotation(env->lander_id);
    float radians = b2Rot_GetAngle(rotation);


    // Main thruster
    float atn_thrust = THRUST_SCALE * env->actions[0];
    float rad_thrust = radians + 0.02*(float)rand()/RAND_MAX;
    force = (b2Vec2){
        atn_thrust*sin(rad_thrust),
        atn_thrust*cos(rad_thrust)
    };
    b2Body_ApplyForce(env->lander_id, force, p_thrust, true);

    // Top left thruster
    float atn_left = SIDE_THRUST_SCALE * env->actions[1];
    float rad_left = -radians + PI/2 + 0.02*(float)rand()/RAND_MAX;
    force = (b2Vec2){
        atn_left*sin(rad_left),
        atn_left*cos(rad_left)
    };
    b2Body_ApplyForce(env->lander_id, force, p_left, true);

    // Top right thruster
    float atn_right = SIDE_THRUST_SCALE * env->actions[2];
    float rad_right = -radians - PI/2 + 0.02*(float)rand()/RAND_MAX;
    force = (b2Vec2){
        atn_right*sin(rad_right),
        atn_right*cos(rad_right)
    };
    b2Body_ApplyForce(env->lander_id, force, p_right, true);

    b2World_Step(env->world_id, 60.0f, 4);


    transform = b2Body_GetTransform(env->lander_id);
    bool do_reset = false;
    if (transform.p.x < -500 || transform.p.x > 500 || transform.p.y > 1200) {
        do_reset = true;
        //env->reward[0] -= 1.0;
        //env->log.episode_return -= 1.0;
    }
    if (transform.p.y < 120) {
        do_reset = true;
        printf("Transform y: %f\n", transform.p.y);
    }
    if (env->tick > 1000) {
        do_reset = true;
        printf("Tick: %i\n", env->tick);
    }
    if (do_reset) {
        printf("Reset\n");
        env->log.episode_length = env->tick;
        add_log(env->log_buffer, &env->log);
        reset(env);
    }
    env->tick += 1;

    compute_observations_and_reward(env, pos.x, pos.y);
    //printf("Reward: %f\n", env->reward[0]);
    //env->reward[0] = -(atn_thrust + atn_left + atn_right) / 10000000;
}

void free_lander(Lander* env) {
    free_logbuffer(env->log_buffer);
    b2DestroyWorld(env->world_id);
}

typedef struct Client Client;
struct Client {
    Camera2D camera;
};

Client* make_client(Lander* env) {
    Client* client = (Client*)calloc(1, sizeof(Client));

	int width = 1920, height = 1080;
	InitWindow(width, height, "box2d-raylib");
	SetTargetFPS(60);

    client->camera = (Camera2D){
        .target = (Vector2){0, 0},
        .offset = (Vector2){width/2, 9*height/10},
        .rotation = 0.0f,
        .zoom = 1.0f,
    };
    return client;
}

void close_client(Client* client) {
    CloseWindow();
    free(client);
}

void render(Client* client, Lander* env) {
    if (IsKeyPressed(KEY_ESCAPE)) {
        exit(0);
    }
    BeginDrawing();
    ClearBackground(DARKGRAY);
    BeginMode2D(client->camera);

    env->actions[0] = 0;
    env->actions[1] = 0;
    env->actions[2] = 0;
    if (IsKeyDown(KEY_W)) {
        env->actions[0] = 1;
    }
    if (IsKeyDown(KEY_Q)) {
        env->actions[1] = 1;
    }
    if (IsKeyDown(KEY_E)) {
        env->actions[2] = 1;
    }


    /*
    b2Rot rotation = b2Body_GetRotation(lander_id);
    float radians = b2Rot_GetAngle(rotation);
    float mag = 1000000;

    if (IsKeyDown(KEY_W)) {
        float rad_thrust = radians + 0.02*(float)rand()/RAND_MAX;
        b2Vec2 force = (b2Vec2){mag*sin(rad_thrust), mag*cos(rad_thrust)};
        b2Body_ApplyForce(lander_id, force, p_thrust, true);
        DrawCircle(p_thrust.x, -p_thrust.y, 20, RED);
    }
    if (IsKeyDown(KEY_Q)) {
        float rad_left = -radians + PI/2 + 0.02*(float)rand()/RAND_MAX;
        if (rad_left > PI) {
            rad_left -= 2*PI;
        }
        b2Vec2 force = (b2Vec2){mag*sin(rad_left), mag*cos(rad_left)};
        b2Body_ApplyForce(lander_id, force, p_left, true);
        DrawCircle(p_left.x, -p_left.y, 20, RED);
    }
    if (IsKeyDown(KEY_E)) {
        float rad_right = -radians - PI/2 + 0.02*(float)rand()/RAND_MAX;
        b2Vec2 force = (b2Vec2){mag*sin(rad_right), mag*cos(rad_right)};
        b2Body_ApplyForce(lander_id, force, p_right, true);
        DrawCircle(p_right.x, -p_right.y, 20, RED);
    }


    if (IsMouseButtonDown(MOUSE_LEFT_BUTTON)) {
        Vector2 mousePos = GetScreenToWorld2D(GetMousePosition(), camera);
        float x = mousePos.x;
        float y = -mousePos.y;
        b2Vec2 origin = (b2Vec2){x, y};
        b2Vec2 p = b2Body_GetWorldPoint(lander_id,
            (b2Vec2){0, -LANDER_HEIGHT/2});
        float mag = 1000;
        b2Vec2 force = (b2Vec2){
            mag * (p.x - origin.x),
            mag * (p.y - origin.y),
        };
        b2Body_ApplyForce(lander_id, force, p, true);
        DrawLine(mousePos.x, mousePos.y, p.x, -p.y, RED);
    }

    b2Transform transform = b2Body_GetTransform(lander_id);
    printf("y: %f\n", transform.p.y);
    if (transform.p.y < 120) {
        reset(&env);
    }
    */

    //DrawRectangle(0, 0, 100, 100, RED);
    DrawEntity(&env->barge, RED);
    DrawEntity(&env->lander, BLUE);
    //DrawEntity(&legs[0], GREEN);
    //DrawEntity(&legs[1], GREEN);
    EndMode2D();
    EndDrawing();
}

void demo() {
    Lander env = {0};
    allocate_lander(&env);
    Client* client = make_client(&env);

    while (!WindowShouldClose()) {
        step(&env);
        render(client, &env);
    }
}

void test_render() {
    InitWindow(1920, 1080, "box2d-raylib");
    SetTargetFPS(60);

    while (!WindowShouldClose()) {
        BeginDrawing();
        ClearBackground(DARKGRAY);

        Rectangle rec = (Rectangle){500, 500, 200, 200};
        Vector2 origin = (Vector2){rec.width/2, rec.height/2};
        DrawRectanglePro(rec, origin, 45, RED);

        DrawCircle(500, 500, 30, BLUE);

        EndDrawing();
    }

}





================================================
FILE: pufferlib/ocean/rocket_lander/rocket_lander.py
================================================
'''High-perf Pong

Inspired from https://gist.github.com/Yttrmin/18ecc3d2d68b407b4be1
& https://jair.org/index.php/jair/article/view/10819/25823
& https://www.youtube.com/watch?v=PSQt5KGv7Vk
'''

import numpy as np
import gymnasium

import pufferlib
from pufferlib.ocean.rocket_lander.cy_rocket_lander import CyRocketLander

class RocketLander(pufferlib.PufferEnv):
    def __init__(self, num_envs=1, render_mode=None, report_interval=32, buf=None):
        self.single_observation_space = gymnasium.spaces.Box(low=0, high=1,
            shape=(6,), dtype=np.float32)
        self.single_action_space = gymnasium.spaces.Discrete(4)
        self.render_mode = render_mode
        self.num_agents = num_envs
        self.report_interval = report_interval

        super().__init__(buf)
        self.float_actions = np.zeros((num_envs, 3), dtype=np.float32)
        self.c_envs = CyRocketLander(self.observations, self.float_actions, self.rewards,
            self.terminals, self.truncations, num_envs)
 
    def reset(self, seed=None):
        self.tick = 0
        self.c_envs.reset()
        return self.observations, []

    def step(self, actions):
        self.float_actions[:, :] = 0
        self.float_actions[:, 0] = actions == 1
        self.float_actions[:, 1] = actions == 2
        self.float_actions[:, 2] = actions == 3
        self.c_envs.step()

        info = []
        if self.tick % self.report_interval == 0:
            log = self.c_envs.log()
            if log['episode_length'] > 0:
                info.append(log)

        self.tick += 1
        return (self.observations, self.rewards,
            self.terminals, self.truncations, info)

    def render(self):
        self.c_envs.render()

    def close(self):
        self.c_envs.close()

def test_performance(timeout=10, atn_cache=1024):
    env = RocketLander(num_envs=1000)
    env.reset()
    tick = 0

    actions = np.random.randint(0, 2, (atn_cache, env.num_envs))

    import time
    start = time.time()
    while time.time() - start < timeout:
        atn = actions[tick % atn_cache]
        env.step(atn)
        tick += 1

    print(f'SPS: %f', env.num_envs * tick / (time.time() - start))

if __name__ == '__main__':
    test_performance()



================================================
FILE: pufferlib/ocean/rware/binding.c
================================================
#include "rware.h"

#define Env CRware
#include "../env_binding.h"

static int my_init(Env* env, PyObject* args, PyObject* kwargs) {
    env->width = unpack(kwargs, "width");
    env->height = unpack(kwargs, "height");
    env->map_choice = unpack(kwargs, "map_choice");
    env->num_agents = unpack(kwargs, "num_agents");
    env->num_requested_shelves = unpack(kwargs, "num_requested_shelves");
    env->grid_square_size = unpack(kwargs, "grid_square_size");
    env->human_agent_idx = unpack(kwargs, "human_agent_idx");
    init(env);
    return 0;
}

static int my_log(PyObject* dict, Log* log) {
    assign_to_dict(dict, "perf", log->perf);
    assign_to_dict(dict, "score", log->score);
    assign_to_dict(dict, "episode_return", log->episode_return);
    assign_to_dict(dict, "episode_length", log->episode_length);
    return 0;
}



================================================
FILE: pufferlib/ocean/rware/rware.c
================================================
#include <time.h>
#include <unistd.h>
#include "rware.h"
#include "puffernet.h"
#define MAP_TINY_WIDTH 640
#define MAP_TINY_HEIGHT 704
#define MAP_SMALL_WIDTH 1280
#define MAP_SMALL_HEIGHT 640
#define MAP_MEDIUM_WIDTH 1280
#define MAP_MEDIUM_HEIGHT 640

void demo(int map_choice) {
    int width;
    int height;
    if (map_choice == 1) {
        width = MAP_TINY_WIDTH;
        height = MAP_TINY_HEIGHT;
    } else if (map_choice == 2) {
        width = MAP_SMALL_WIDTH;
        height = MAP_SMALL_HEIGHT;
    } else {
        width = MAP_MEDIUM_WIDTH;
        height = MAP_MEDIUM_HEIGHT;
    }
    CRware env = {
        .width = width,
        .height = height,
        .map_choice = map_choice,
        .num_agents = 8,
        .num_requested_shelves = 8,
        .grid_square_size = 64,
        .human_agent_idx = 0,
	.reward_type = 2
    };
    Weights* weights = load_weights("resources/rware/rware_weights.bin", 136454);
    int logit_sizes[1] = {5};
    LinearLSTM* net = make_linearlstm(weights, env.num_agents, 27, logit_sizes, 1);

    allocate(&env);
    c_reset(&env);
    c_render(&env);

    int tick = 1;
    while (!WindowShouldClose()) {
        if (tick % 12 == 0) {
            tick = 0;

            int human_action = env.actions[env.human_agent_idx];
            for (int i = 0; i < env.num_agents * 27; i++) {
                net->obs[i] = (float)env.observations[i];
            }
            forward_linearlstm(net, net->obs, env.actions);

            if (IsKeyDown(KEY_LEFT_SHIFT)) {
                env.actions[env.human_agent_idx] = human_action;
            }

            c_step(&env);

            if (IsKeyDown(KEY_LEFT_SHIFT)) {
                env.actions[env.human_agent_idx] = NOOP;
            }
        }
        tick++;

        if (IsKeyDown(KEY_LEFT_SHIFT)) {
            // Handle keyboard input only for selected agent
            if (IsKeyPressed(KEY_UP) || IsKeyPressed(KEY_W)) {
                env.actions[env.human_agent_idx] = FORWARD;
            }
            if (IsKeyPressed(KEY_LEFT) || IsKeyPressed(KEY_A)) {
                env.actions[env.human_agent_idx] = LEFT;
            }
            if (IsKeyPressed(KEY_RIGHT) || IsKeyPressed(KEY_D)) {
                env.actions[env.human_agent_idx] = RIGHT;
            }
            if (IsKeyPressed(KEY_SPACE) || IsKeyPressed(KEY_ENTER)) {
                env.actions[env.human_agent_idx] = TOGGLE_LOAD;
            }
            // Add agent switching with TAB key
            if (IsKeyPressed(KEY_TAB)) {
                env.human_agent_idx = (env.human_agent_idx + 1) % env.num_agents;
            }
        }

        c_render(&env);
    }
    //close_client(client);
    free_allocated(&env);
}

void performance_test() {
    long test_time = 10;
    CRware env = {
        .width = 1280,
        .height = 704,
        .map_choice = 2,
        .num_agents = 4,
        .num_requested_shelves = 4,
	.reward_type = 2
    };
    allocate(&env);
    c_reset(&env);

    long start = time(NULL);
    int i = 0;
    while (time(NULL) - start < test_time) {
        env.actions[0] = rand() % 5;
        c_step(&env);
        i++;
    }
    long end = time(NULL);
    printf("SPS: %ld\n", i / (end - start));
    free_allocated(&env);
}

int main() {
    demo(2);
    //performance_test();
    return 0;
}



================================================
FILE: pufferlib/ocean/rware/rware.h
================================================
#include <stdlib.h>
#include <stdio.h>
#include <math.h>
#include <assert.h>
#include <string.h>
#include <time.h>
#include "raylib.h"

#define NOOP 0
#define FORWARD 1
#define LEFT 2
#define RIGHT 3
#define TOGGLE_LOAD 4
#define TICK_RATE 1.0f/60.0f
#define NUM_DIRECTIONS 4

// warehouse states
#define EMPTY 0
#define SHELF 1
#define REQUESTED_SHELF 2
#define GOAL 3

// agent states
#define UNLOADED 0
#define HOLDING_REQUESTED_SHELF 1
#define HOLDING_EMPTY_SHELF 2

// observation types
#define SELF_OBS 3
#define VISION_OBS 24

// Facing directions
#define FACING_RIGHT 0
#define FACING_DOWN 1
#define FACING_LEFT 2
#define FACING_UP 3

// Reward Type
#define INDIVIDUAL 1
#define GLOBAL 2
static const int DIRECTIONS[NUM_DIRECTIONS] = {0, 1, 2, 3};
static const int DIRECTION_VECTORS[NUM_DIRECTIONS][2] = {{1, 0}, {0, 1}, {-1, 0}, {0, -1}};
static const int SURROUNDING_VECTORS[8][2] = {{0,-1}, {1,-1}, {1,0}, {1,1}, {0,1}, {-1,1}, {-1,0}, {-1,-1}};
static const int tiny_map[110] = {
    0,0,0,0,0,0,0,0,0,0,  
    0,1,1,0,0,0,0,1,1,0,  
    0,1,1,0,0,0,0,1,1,0, 
    0,1,1,0,0,0,0,1,1,0,  
    0,1,1,0,0,0,0,1,1,0,  
    0,1,1,0,0,0,0,1,1,0,  
    0,1,1,0,0,0,0,1,1,0,  
    0,1,1,0,0,0,0,1,1,0, 
    0,1,1,0,0,0,0,1,1,0,  
    0,0,0,0,0,0,0,0,0,0,  
    0,0,0,0,3,3,0,0,0,0   
};
static const int tiny_shelf_locations[32] = {
    11, 12, 17, 18,  // Row 1
    21, 22, 27, 28,  // Row 2
    31, 32, 37, 38,  // Row 3
    41, 42, 47, 48,  // Row 4
    51, 52, 57, 58,  // Row 5
    61, 62, 67, 68,  // Row 6
    71, 72, 77, 78,  // Row 7
    81, 82, 87, 88   // Row 8
};

static const int small_map[200] = {
    0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,
    0,0,1,1,1,1,1,1,1,1,0,1,1,1,1,1,1,1,1,0,
    0,0,1,1,1,1,1,1,1,1,0,1,1,1,1,1,1,1,1,0,
    0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,
    3,0,0,0,0,0,0,0,0,0,0,1,1,1,1,1,1,1,1,0,
    3,0,0,0,0,0,0,0,0,0,0,1,1,1,1,1,1,1,1,0,
    0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,
    0,0,1,1,1,1,1,1,1,1,0,1,1,1,1,1,1,1,1,0,
    0,0,1,1,1,1,1,1,1,1,0,1,1,1,1,1,1,1,1,0,
    0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,
};

static const int small_shelf_locations[80] = {
    22,23,24,25,26,27,28,29,31,32,33,34,35,36,37,38,  // Row 1
    42,43,44,45,46,47,48,49,51,52,53,54,55,56,57,58,  // Row 2
    91,92,93,94,95,96,97,98,  // Row 4 (right side only)
    111,112,113,114,115,116,117,118,  // Row 5 (right side only)
    142,143,144,145,146,147,148,149,151,152,153,154,155,156,157,158,  // Row 7 (both sides)
    162,163,164,165,166,167,168,169,171,172,173,174,175,176,177,178   // Row 8 (both sides)
};

static const int medium_map[320] = {
    0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,
    0,0,1,1,1,1,1,1,1,1,0,1,1,1,1,1,1,1,1,0,
    0,0,1,1,1,1,1,1,1,1,0,1,1,1,1,1,1,1,1,0,
    0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,
    0,0,1,1,1,1,1,1,1,1,0,1,1,1,1,1,1,1,1,0,
    0,0,1,1,1,1,1,1,1,1,0,1,1,1,1,1,1,1,1,0,
    0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,
    3,0,0,0,0,0,0,0,0,0,0,1,1,1,1,1,1,1,1,0,
    3,0,0,0,0,0,0,0,0,0,0,1,1,1,1,1,1,1,1,0,
    0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,
    0,0,1,1,1,1,1,1,1,1,0,1,1,1,1,1,1,1,1,0,
    0,0,1,1,1,1,1,1,1,1,0,1,1,1,1,1,1,1,1,0,
    0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,
    0,0,1,1,1,1,1,1,1,1,0,1,1,1,1,1,1,1,1,0,
    0,0,1,1,1,1,1,1,1,1,0,1,1,1,1,1,1,1,1,0,
    0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,
};

static const int medium_shelf_locations[144]={
    22,23,24,25,26,27,28,29,31,32,33,34,35,36,37,38,  // Row 1
    42,43,44,45,46,47,48,49,51,52,53,54,55,56,57,58,  // Row 2
    82,83,84,85,86,87,88,89,91,92,93,94,95,96,97,98,  // Row 4
    102,103,104,105,106,107,108,109,111,112,113,114,115,116,117,118,  // Row 5
    151,152,153,154,155,156,157,158,  // Row 7 (right side only)
    171,172,173,174,175,176,177,178,  // Row 8 (right side only)
    202,203,204,205,206,207,208,209,211,212,213,214,215,216,217,218,  // Row 10
    222,223,224,225,226,227,228,229,231,232,233,234,235,236,237,238,  // Row 11
    262,263,264,265,266,267,268,269,271,272,273,274,275,276,277,278,  // Row 13
    282,283,284,285,286,287,288,289,291,292,293,294,295,296,297,298   // Row 14
};

static const int map_sizes[3] = {110, 200, 320};
static const int map_rows[3] = {11, 10, 16};
static const int map_cols[3] = {10, 20, 20};
static const int* maps[3] = {tiny_map, small_map, medium_map};

static inline int max(int a, int b) {
    return (a > b) ? a : b;
}
	
typedef struct Client Client;
typedef struct CRware CRware;
typedef struct Log Log;

struct Log {
    float perf;
    float score;
    float episode_return;
    float episode_length;
    float n;
};

typedef struct MovementGraph MovementGraph;
struct MovementGraph {
    int* target_positions;
    int* cycle_ids;
    int* weights;
    int num_cycles;
};

struct CRware {
    Client* client;
    float* observations;
    int* actions;
    float* rewards;
    unsigned char* terminals;
    Log* agent_logs;
    Log log;
    float* scores;
    int reward_type;
    int width;
    int height;
    int map_choice;
    int* warehouse_states;
    int num_agents;
    int num_requested_shelves;
    int* agent_locations;
    int* old_agent_locations;
    int* agent_directions;
    int* agent_states;
    int human_agent_idx;
    int grid_square_size;
    int* original_shelve_locations;
    MovementGraph* movement_graph;
};

void add_log(CRware* env, Log* agent_log) {
    env->log.episode_return += agent_log->episode_return;
    env->log.episode_length += agent_log->episode_length;
    env->log.score += agent_log->score;   
    env->log.perf += fmaxf(0.0, agent_log->score - 0.01*agent_log->episode_length);
    env->log.n += 1;
}

int find_agent_at_position(CRware* env, int position) {
    for (int i = 0; i < env->num_agents; i++) {
        if (env->agent_locations[i] == position) {
            return i;
        }
    }
    return -1;
}

void place_agent(CRware* env, int agent_idx) {
    // map size fixed at top
    int map_size = map_sizes[env->map_choice - 1];
    
    int found_valid_position = 0;
    while (!found_valid_position) {
        int random_pos = rand() % map_size;
        
        // Skip if position is not empty
        if (env->warehouse_states[random_pos] != EMPTY) {
            continue;
        }

        // Skip if another agent is already here
        int agent_at_position = find_agent_at_position(env, random_pos);
        if (agent_at_position != -1) {
            continue;
        }

        // Position is valid, place the agent
        env->old_agent_locations[agent_idx] = random_pos;
        env->agent_locations[agent_idx] = random_pos;
        env->agent_directions[agent_idx] = rand() % 4;
        env->agent_states[agent_idx] = 0;
        found_valid_position = 1;
    }
}

int request_new_shelf(CRware* env) {
    int total_shelves;
    const int* shelf_locations;
    if (env->map_choice == 1) {
        total_shelves = 32;
        shelf_locations = tiny_shelf_locations;
    } else if (env->map_choice == 2) {
        total_shelves = 80;
        shelf_locations = small_shelf_locations;
    } else {
        total_shelves = 144;
        shelf_locations = medium_shelf_locations;
    }
    int random_index = rand() % total_shelves;
    int shelf_location = shelf_locations[random_index];
    if (env->warehouse_states[shelf_location] == SHELF ) {
        env->warehouse_states[shelf_location] = REQUESTED_SHELF;
        return 1;
    }
    return 0;
}

void generate_map(CRware* env,const int* map) {
    // seed new random
    srand(time(NULL));
    int map_size = map_sizes[env->map_choice - 1];
    memcpy(env->warehouse_states, map, map_size * sizeof(int));

    int requested_shelves_count = 0;
    while (requested_shelves_count < env->num_requested_shelves) {
        requested_shelves_count += request_new_shelf(env);
    }
    // set agents random locations
    for (int i = 0; i < env->num_agents; i++) {
        place_agent(env, i);
    }
}

MovementGraph* init_movement_graph(CRware* env) {
    MovementGraph* graph = (MovementGraph*)calloc(1, sizeof(MovementGraph));
    graph->target_positions = (int*)calloc(env->num_agents, sizeof(int));
    graph->cycle_ids = (int*)calloc(env->num_agents, sizeof(int));
    graph->weights = (int*)calloc(env->num_agents, sizeof(int));
    graph->num_cycles = 0;

    // Initialize arrays
    for (int i = 0; i < env->num_agents; i++) {
        graph->target_positions[i] = -1;   // No target
        graph->cycle_ids[i] = -1;          // Not in cycle
    }
    return graph;
}

void init(CRware* env) {
    int map_size = map_sizes[env->map_choice - 1];
    env->warehouse_states = (int*)calloc(map_size, sizeof(int));
    env->agent_locations = (int*)calloc(env->num_agents, sizeof(int));
    env->old_agent_locations = (int*)calloc(env->num_agents, sizeof(int));
    env->agent_directions = (int*)calloc(env->num_agents, sizeof(int));
    env->agent_states = (int*)calloc(env->num_agents, sizeof(int));
    env->scores = (float*)calloc(env->num_agents, sizeof(float));
    env->movement_graph = init_movement_graph(env);
    env->agent_logs = (Log*)calloc(env->num_agents, sizeof(Log));
    if (env->map_choice == 1) {
        generate_map(env, tiny_map);
    }
    else if (env->map_choice == 2) {
        generate_map(env, small_map);
    }
    else {
        generate_map(env, medium_map);
    }
}

void allocate(CRware* env) {
    init(env);
    env->observations = (float*)calloc(env->num_agents*(SELF_OBS+VISION_OBS), sizeof(float));
    env->actions = (int*)calloc(env->num_agents, sizeof(int));
    env->rewards = (float*)calloc(env->num_agents, sizeof(float));
    env->terminals = (unsigned char*)calloc(env->num_agents, sizeof(unsigned char));
}

void c_close(CRware* env) {
    free(env->warehouse_states);
    free(env->agent_locations);
    free(env->agent_directions);
    free(env->agent_states);
    free(env->movement_graph->target_positions);
    free(env->movement_graph->cycle_ids);
    free(env->movement_graph->weights);
    free(env->movement_graph);
    free(env->agent_logs);
    free(env->scores);
}

void free_allocated(CRware* env) {
    free(env->actions);
    free(env->observations);
    free(env->terminals);
    free(env->rewards);
    c_close(env);
}

void compute_observations(CRware* env) {
    int surround_indices[8];
    int cols = map_cols[env->map_choice - 1];
    int rows = map_rows[env->map_choice - 1];
    float (*observations)[SELF_OBS+VISION_OBS] = (float(*)[SELF_OBS+VISION_OBS])env->observations;
    for (int i = 0; i < env->num_agents; i++) {
        // Agent location, direction, state
        float* obs = &observations[i][0];
        int agent_location = env->agent_locations[i];
        int current_x = agent_location % cols;
        int current_y = agent_location / cols;
        // Self observations
        obs[0] = env->agent_locations[i] / (float)(rows*cols);
        obs[1] = (env->agent_directions[i] + 1) / 4.0;
        obs[2] = env->agent_states[i];
	// Vision observations
        for (int j = 0; j < 8; j++) {
            int new_x = current_x + SURROUNDING_VECTORS[j][0];
            int new_y = current_y + SURROUNDING_VECTORS[j][1];
            surround_indices[j] = new_x + new_y * cols;
            // other robots location and rotation if on that spot
            for (int k = 0; k < env->num_agents; k++) {
                if(i==k){
                    continue;
                }
                if(env->agent_locations[k] == surround_indices[j]){
                    obs[3 + j*3] = 1;
                    obs[4 + j*3] = (env->agent_directions[k] + 1) / 4.0;
                    break;
                } else {
                    obs[3 + j*3] = 0;
                    obs[4 + j*3] = 0;
                    break;
                }
            }
            // boundary check
            if (new_x < 0 || new_x >= cols || new_y < 0 || new_y >= rows) {
                obs[5 + j*3] = 0;
            } else {
                obs[5 + j*3] = (env->warehouse_states[surround_indices[j]] + 1) / 4.0;
            }
        }
    }
}

void c_reset(CRware* env) {
     
	env->terminals[0] = 0;
    // set agents in center
    env->human_agent_idx = 0;
    if (env->map_choice == 1) {
        generate_map(env, tiny_map);
    } else if (env->map_choice == 2) {
        generate_map(env, small_map);
    } else {
        generate_map(env, medium_map);
    }
    for(int x = 0;x<env->num_agents; x++){
	    env->scores[x] = 0.0;
    }
    compute_observations(env);
    
}

int get_direction(CRware* env, int action, int agent_idx) {
    // For reference: 
    // 0 = right (initial), 1 = down, 2 = left, 3 = up
    int current_direction = env->agent_directions[agent_idx];
    if (action == FORWARD) {
        return current_direction;
    }
    else if (action == LEFT) {
        // Rotate counter-clockwise
        return (current_direction + 3) % NUM_DIRECTIONS;
    }
    else if (action == RIGHT) {
        // Rotate clockwise
        return (current_direction + 1) % NUM_DIRECTIONS;
    }
    return current_direction;
}

int get_new_position(CRware* env, int agent_idx) {
    int cols = map_cols[env->map_choice - 1];
    int rows = map_rows[env->map_choice - 1];
    int current_position_x = env->agent_locations[agent_idx] % cols;
    int current_position_y = env->agent_locations[agent_idx] / cols;
    int current_direction = env->agent_directions[agent_idx];
    int new_position_x = current_position_x + DIRECTION_VECTORS[current_direction][0];
    int new_position_y = current_position_y + DIRECTION_VECTORS[current_direction][1];
    int new_position = new_position_x + new_position_y * cols;
    // check boundary
    if (new_position_x < 0 || new_position_x >= cols || new_position_y < 0 || new_position_y >= rows) {
        return -1;
    }
    // check if holding shelf and next position is a shelf
    int agent_state = env->agent_states[agent_idx];
    int warehouse_state = env->warehouse_states[new_position];
    if ((agent_state == HOLDING_EMPTY_SHELF || agent_state == HOLDING_REQUESTED_SHELF) && 
    (warehouse_state == SHELF || warehouse_state == REQUESTED_SHELF)) {
        return -1;
    }
    // check if agent is trying to move into a goal with empty shelf
    if (agent_state == HOLDING_EMPTY_SHELF && warehouse_state == GOAL) {
        return -1;
    }
    return new_position;
}

int detect_cycle_for_agent(CRware* env, int start_agent) {
    MovementGraph* graph = env->movement_graph;
    
    // If already processed or no target, skip
    if (graph->cycle_ids[start_agent] != -1 || 
        graph->target_positions[start_agent] == -1) {
        return -1;
    }

    // Initialize tortoise and hare
    int tortoise = find_agent_at_position(env, graph->target_positions[start_agent]);
    if (tortoise == -1) return -1;
    int hare = tortoise;

    // Move hare ahead by one step
    hare = find_agent_at_position(env, graph->target_positions[hare]);
    if (hare == -1) return -1;

    // Loop to detect cycle
    while (tortoise != hare) {
        tortoise = find_agent_at_position(env, graph->target_positions[tortoise]);
        if (tortoise == -1) return -1;

        hare = find_agent_at_position(env, graph->target_positions[hare]);
        if (hare == -1) return -1;
        hare = find_agent_at_position(env, graph->target_positions[hare]);
        if (hare == -1) return -1;
    }

    // Find the start of the cycle
    tortoise = start_agent;
    while (tortoise != hare) {
        tortoise = find_agent_at_position(env, graph->target_positions[tortoise]);
        hare = find_agent_at_position(env, graph->target_positions[hare]);
    }

    int cycle_start = tortoise;

    // Mark all agents in the cycle
    int cycle_id = graph->num_cycles++;
    int current = cycle_start;
    do {
        graph->cycle_ids[current] = cycle_id;
        current = find_agent_at_position(env, graph->target_positions[current]);
    } while (current != cycle_start);

    return cycle_id;
}

void detect_cycles(CRware* env) {
    for (int i = 0; i < env->num_agents; i++) {
        if(env->movement_graph->cycle_ids[i] == -1) {
            detect_cycle_for_agent(env, i);
        }
    }
}

void calculate_weights(CRware* env) {
    MovementGraph* graph = env->movement_graph;
    
    // First pass: identify leaf nodes (agents not targeted by others)
    for (int i = 0; i < env->num_agents; i++) {
        if (graph->cycle_ids[i] != -1) continue;  // Skip agents in cycles
        
        bool is_leaf = true;
        for (int j = 0; j < env->num_agents; j++) {
            if (graph->target_positions[j] != env->agent_locations[i]) continue;
            is_leaf = false;
            break;
        }
        
        if (is_leaf) {
            graph->weights[i] = 1;  // Leaf node
        }
    }

    bool changed = true;
    while(changed) {
        changed = false;
        for (int i = 0; i < env->num_agents; i++) {
            if (graph->cycle_ids[i] != -1) continue;
            
            // Find agents targeting this agent's position
            int max_child_weight = 0;
            for (int j = 0; j < env->num_agents; j++) {
                if (graph->target_positions[j] != env->agent_locations[i]) continue;
                max_child_weight = max(max_child_weight, graph->weights[j]);
            }
            
            if (max_child_weight == 0 || graph->weights[i] == max_child_weight + 1) {
                continue;
            }
            graph->weights[i] = max_child_weight + 1;
            changed = true;
        }
    }
}

void update_movement_graph(CRware* env, int agent_idx) {
    MovementGraph* graph = env->movement_graph;
    int new_position = get_new_position(env, agent_idx);
    if (new_position == -1) {
        return;
    }
    graph->target_positions[agent_idx] = new_position;

    // reset cycle and weights
    for (int i = 0; i < env->num_agents; i++) {
        graph->cycle_ids[i] = -1;
        graph->weights[i] = 0;
    }
    graph->num_cycles = 0;

    // detect cycles with Floyd algorithm
    detect_cycles(env);

    // calculate weights for tree
    calculate_weights(env);
}

void move_agent(CRware* env, int agent_idx) {
    
    int new_position = get_new_position(env, agent_idx);
    // check boundary
    if (new_position == -1) {
        return;
    }
    // if reach goal
    int agent_state = env->agent_states[agent_idx];
    int agent_location = env->agent_locations[agent_idx];
    int new_position_state = env->warehouse_states[new_position];
    int current_position_state = env->warehouse_states[agent_location];
    if (new_position_state == GOAL && agent_state== HOLDING_REQUESTED_SHELF) {
        if (current_position_state != GOAL) {
            env->warehouse_states[agent_location] = 0;
        }
        env->agent_locations[agent_idx] = new_position;
        return;
    }
    // if agent is holding requested shelf
    if (agent_state == HOLDING_REQUESTED_SHELF) {
        if (current_position_state != GOAL) {
            env->warehouse_states[agent_location] = 0;
        }
        env->warehouse_states[new_position] = REQUESTED_SHELF;
    }
    // if agent is holding empty shelf
    if (agent_state == HOLDING_EMPTY_SHELF) {
        if (current_position_state != GOAL){
            env->warehouse_states[agent_location] = 0;
        }
        env->warehouse_states[new_position] = SHELF;
    }
    env->agent_locations[agent_idx] = new_position;
    env->movement_graph->target_positions[agent_idx] = -1;
}

void pickup_shelf(CRware* env, int agent_idx) {
    // pickup shelf
    const int* map = maps[env->map_choice - 1];
    int agent_location = env->agent_locations[agent_idx];
    int agent_state = env->agent_states[agent_idx];
    int current_position_state = env->warehouse_states[agent_location];
    int original_map_state = map[agent_location];
    if ((current_position_state == REQUESTED_SHELF) && (agent_state==UNLOADED)) {
        env->rewards[agent_idx] = 0.5;
	env->agent_logs[agent_idx].episode_return += 0.5;
	env->agent_states[agent_idx]=HOLDING_REQUESTED_SHELF;
    }
    // return empty shelf
    else if (agent_state == HOLDING_EMPTY_SHELF && current_position_state == original_map_state 
    && original_map_state != GOAL) {
        env->agent_states[agent_idx]=UNLOADED;
        env->warehouse_states[agent_location] = original_map_state;
        env->rewards[agent_idx] = 1.0;

        env->agent_logs[agent_idx].score = 1.0;
        env->agent_logs[agent_idx].episode_return += 1.0;

	    env->scores[agent_idx] = 0;
        add_log(env, &env->agent_logs[agent_idx]);
        env->agent_logs[agent_idx] = (Log){0};
    }
    // drop shelf at goal
    else if (agent_state == HOLDING_REQUESTED_SHELF && current_position_state == GOAL) {
        env->agent_states[agent_idx]=HOLDING_EMPTY_SHELF;
        env->rewards[agent_idx] = 0.5;
        env->agent_logs[agent_idx].episode_return += 0.5;
        env->agent_logs[agent_idx].score = 1.0;
        int shelf_count = 0;
        while (shelf_count < 1) {
            shelf_count += request_new_shelf(env);
        }
    }
}

void process_cycle_movements(CRware* env, MovementGraph* graph) {
    for (int cycle = 0; cycle < graph->num_cycles; cycle++) {
        int cycle_size = 0;
        for (int i = 0; i < env->num_agents; i++) {
            if (graph->cycle_ids[i] == cycle) {
                cycle_size++;
            }
        }
        if (cycle_size == 2) continue;

        bool can_move_cycle = true;
        // Verify all agents in cycle can move
        for (int i = 0; i < env->num_agents; i++) {
            if (graph->cycle_ids[i] != cycle) continue;
            int new_pos = get_new_position(env, i);
            if (new_pos == -1) {
                can_move_cycle = false;
                break;
            }
        }
        
        // Move all agents in cycle if possible
        if (!can_move_cycle) continue;
        for (int i = 0; i < env->num_agents; i++) {
            if (graph->cycle_ids[i] != cycle) continue;
            if (env->actions[i] != FORWARD) continue;            
            move_agent(env, i);
        }
    }
}

void process_tree_movements(CRware* env, MovementGraph* graph) {
    int max_weight = 0;
    for (int i = 0; i < env->num_agents; i++) {
        if (graph->cycle_ids[i] == -1 && graph->weights[i] > max_weight) {
            max_weight = graph->weights[i];
        }
    }
    // Process from highest weight to lowest
    for (int weight = max_weight; weight > 0; weight--) {
        for (int i = 0; i < env->num_agents; i++) {
            if (graph->cycle_ids[i] != -1 || graph->weights[i] != weight) continue;
            if (env->actions[i] != FORWARD) continue;

            int new_pos = get_new_position(env, i);
            if (new_pos == -1) continue;

            int target_agent = find_agent_at_position(env, new_pos);
            if (target_agent != -1) continue;
            move_agent(env, i);
        }
    }
}

void c_step(CRware* env) {
    memset(env->rewards, 0, env->num_agents * sizeof(float));
    MovementGraph* graph = env->movement_graph;
    for (int i = 0; i < env->num_agents; i++) {
        env->old_agent_locations[i] = env->agent_locations[i];
        env->agent_logs[i].episode_length += 1;
        int action = env->actions[i];
        
	    // Handle direction changes and non-movement actions
        if (action != NOOP && action != TOGGLE_LOAD) {
            env->agent_directions[i] = get_direction(env, action, i);
        }
        if (action == TOGGLE_LOAD) {
            pickup_shelf(env, i);
        }
        if (action == FORWARD) {
            update_movement_graph(env, i);
        }
    }
    int is_movement=0;
    for(int i=0; i<env->num_agents; i++) {
        if (env->actions[i] == FORWARD) is_movement++;
    }
    if (is_movement>=1) {
        // Process movements in cycles first
        process_cycle_movements(env, graph);
        // process tree movements
        process_tree_movements(env, graph);
    }

    compute_observations(env);
}

const Color STONE_GRAY = (Color){80, 80, 80, 255};
const Color PUFF_RED = (Color){187, 0, 0, 255};
const Color PUFF_CYAN = (Color){0, 187, 187, 255};
const Color PUFF_WHITE = (Color){241, 241, 241, 241};
const Color PUFF_GREY = (Color){128, 128, 128, 255};
const Color PUFF_BACKGROUND = (Color){6, 24, 24, 255};
const Color PUFF_BACKGROUND2 = (Color){18, 72, 72, 255};

struct Client {
    float width;
    float height;
    int tick;
    Texture2D puffers;
};

Client* make_client(CRware* env) {
    Client* client = (Client*)calloc(1, sizeof(Client));
    client->width = env->width;
    client->height = env->height;
    client->tick = 0;
    InitWindow(env->width, env->height, "PufferLib Ray RWare");
    SetTargetFPS(60);
    client->puffers = LoadTexture("resources/shared/puffers_128.png");
    return client;
}

void c_render(CRware* env) {
    if (env->client == NULL) {
        env->client = make_client(env);
    }
    Client* client = env->client;

    if (IsKeyDown(KEY_ESCAPE)) {
        exit(0);
    }
    BeginDrawing();
    ClearBackground(PUFF_BACKGROUND);    
    
    int map_size = map_sizes[env->map_choice - 1];
    int cols = map_cols[env->map_choice - 1];
    for (int i = 0; i < map_size; i++) {
        int state = env->warehouse_states[i];
        Color color;
        if (state == SHELF) {
            color = PUFF_CYAN;
        } else if (state == REQUESTED_SHELF) {
            color = RED;
        } else if (state == GOAL) {
            color = STONE_GRAY;
        } else {
            color = PUFF_BACKGROUND;
        }

        int x_pos = i % cols * env->grid_square_size;
        int y_pos = i / cols * env->grid_square_size;
        DrawRectangle(
            x_pos,  // x position
            y_pos,  // y position
            env->grid_square_size, 
            env->grid_square_size, 
            color
        );
        
        DrawRectangleLines(
            x_pos,
            y_pos,
            env->grid_square_size,
            env->grid_square_size,
            PUFF_GREY
        );

        DrawRectangleLines(
            x_pos + 1,
            y_pos + 1,
            env->grid_square_size - 2,
            env->grid_square_size - 2,
            state != EMPTY ? WHITE : BLANK
        );
    }
    // draw agent
    for (int j = 0; j < env->num_agents; j++) {
        int cols = map_cols[env->map_choice - 1];

        int old_agent_location = env->old_agent_locations[j];
        int old_x_pos = (old_agent_location % cols) * env->grid_square_size;
        int old_y_pos = (old_agent_location / cols) * env->grid_square_size;

        int new_agent_location = env->agent_locations[j];
        int new_x_pos = (new_agent_location % cols) * env->grid_square_size;
        int new_y_pos = (new_agent_location / cols) * env->grid_square_size;

        float interp_old = (1.0f - 1.0f/12.0f*(float)client->tick);
        float interp_new = 1.0f/12.0f*(float)client->tick;

        int x_pos = interp_old*(float)old_x_pos + interp_new*(float)new_x_pos;
        int y_pos = interp_old*(float)old_y_pos + interp_new*(float)new_y_pos;
 
        int starting_sprite_x = 0;
        int rotation = 90*env->agent_directions[j];
        if (rotation == 180) {
            starting_sprite_x = 128;
            rotation = 0;
        }
        DrawTexturePro(
            client->puffers,
            (Rectangle){starting_sprite_x, 0, 128, 128},
            (Rectangle){
                x_pos + env->grid_square_size/2,
                y_pos + env->grid_square_size/2,
                env->grid_square_size,
                env->grid_square_size
            },
            (Vector2){env->grid_square_size/2, env->grid_square_size/2},
            rotation,
            //env->agent_states[j] != UNLOADED ? RED : WHITE
            WHITE
        );
        // put a number on top of the agent
        DrawText(
            TextFormat("%d", j),
            x_pos + env->grid_square_size/2,
            y_pos + env->grid_square_size/2,
            20,
            WHITE
        );
    }

    client->tick = (client->tick + 1) % 12;

    EndDrawing();
}
void close_client(Client* client) {
    CloseWindow();
    free(client);
}



================================================
FILE: pufferlib/ocean/rware/rware.py
================================================
'''High-perf Pong

Inspired from https://gist.github.com/Yttrmin/18ecc3d2d68b407b4be1
& https://jair.org/index.php/jair/article/view/10819/25823
& https://www.youtube.com/watch?v=PSQt5KGv7Vk
'''

import numpy as np
import gymnasium

import pufferlib
from pufferlib.ocean.rware import binding

PLAYER_OBS_N = 27

class Rware(pufferlib.PufferEnv):
    def __init__(self, num_envs=1, render_mode=None, report_interval=1,
            width=1280, height=640,
            num_agents=4,
            map_choice=1,
            num_requested_shelves=4,
            grid_square_size=64,
            human_agent_idx=0,
            reward_type=1,
            buf = None, seed=0):

        # env
        self.num_agents = num_envs*num_agents
        self.render_mode = render_mode
        self.report_interval = report_interval
        
        self.num_obs = 27
        self.single_observation_space = gymnasium.spaces.Box(low=0, high=1,
            shape=(self.num_obs,), dtype=np.float32)
        self.single_action_space = gymnasium.spaces.Discrete(5)

        super().__init__(buf=buf)
        c_envs = []
        for i in range(num_envs):
            env_id = binding.env_init(
                self.observations[i*num_agents:(i+1)*num_agents],
                self.actions[i*num_agents:(i+1)*num_agents],
                self.rewards[i*num_agents:(i+1)*num_agents],
                self.terminals[i*num_agents:(i+1)*num_agents],
                self.truncations[i*num_agents:(i+1)*num_agents],
                i + seed * num_envs,
                width=width,
                height=height,
                map_choice=map_choice,
                num_agents=num_agents,
                num_requested_shelves=num_requested_shelves,
                grid_square_size=grid_square_size,
                human_agent_idx=human_agent_idx
            )
            c_envs.append(env_id)

        self.c_envs = binding.vectorize(*c_envs)

    def reset(self, seed=0):
        binding.vec_reset(self.c_envs, seed)
        self.tick = 0
        return self.observations, []

    def step(self, actions):
        self.actions[:] = actions
        binding.vec_step(self.c_envs)
        self.tick += 1

        info = []
        if self.tick % self.report_interval == 0:
            log = binding.vec_log(self.c_envs)
            if log:
                info.append(log)

        return (self.observations, self.rewards,
            self.terminals, self.truncations, info)

    def render(self):
        binding.vec_render(self.c_envs, 0)
        
    def close(self):
        binding.vec_close(self.c_envs)

def test_performance(timeout=10, atn_cache=1024):
    num_envs=1000;
    env = MyRware(num_envs=num_envs)
    env.reset()
    tick = 0

    actions = np.random.randint(0, env.single_action_space.n, (atn_cache, 5*num_envs))

    import time
    start = time.time()
    while time.time() - start < timeout:
        atn = actions[tick % atn_cache]
        env.step(atn)
        tick += 1

    sps = num_envs * tick / (time.time() - start)
    print(f'SPS: {sps:,}')
if __name__ == '__main__':
    test_performance()



================================================
FILE: pufferlib/ocean/shared_pool/binding.c
================================================
#include "shared_pool.h"

#define Env CCpr
#include "../env_binding.h"

static int my_init(Env* env, PyObject* args, PyObject* kwargs) {
    env->width = unpack(kwargs, "width");
    env->height = unpack(kwargs, "height");
    env->num_agents = unpack(kwargs, "num_agents");
    env->vision = unpack(kwargs, "vision");
    env->reward_food = unpack(kwargs, "reward_food");
    env->interactive_food_reward = unpack(kwargs, "interactive_food_reward");
    env->reward_move = unpack(kwargs, "reward_move");
    env->food_base_spawn_rate = unpack(kwargs, "food_base_spawn_rate");
    init_ccpr(env);
    return 0;
}

static int my_log(PyObject* dict, Log* log) {
    assign_to_dict(dict, "perf", log->perf);
    assign_to_dict(dict, "score", log->score);
    assign_to_dict(dict, "episode_return", log->episode_return);
    assign_to_dict(dict, "moves", log->moves);
    assign_to_dict(dict, "food_nb", log->food_nb);
    assign_to_dict(dict, "alive_steps", log->alive_steps);
    return 0;
}



================================================
FILE: pufferlib/ocean/shared_pool/grid.h
================================================
#ifndef GRID_H
#define GRID_H

#define GRID_HEIGHT 32
#define GRID_WIDTH 32

static const unsigned char grid_32_32_3v[GRID_HEIGHT][GRID_WIDTH] = {
    {0x03, 0x03, 0x03, 0x03, 0x03, 0x03, 0x03, 0x03, 0x03, 0x03, 0x03, 0x03, 0x03, 0x03, 0x03, 0x03, 0x03, 0x03, 0x03, 0x03, 0x03, 0x03, 0x03, 0x03, 0x03, 0x03, 0x03, 0x03, 0x03, 0x03, 0x03, 0x03},
    {0x03, 0x03, 0x03, 0x03, 0x03, 0x03, 0x03, 0x03, 0x03, 0x03, 0x03, 0x03, 0x03, 0x03, 0x03, 0x03, 0x03, 0x03, 0x03, 0x03, 0x03, 0x03, 0x03, 0x03, 0x03, 0x03, 0x03, 0x03, 0x03, 0x03, 0x03, 0x03},
    {0x03, 0x03, 0x03, 0x03, 0x03, 0x03, 0x03, 0x03, 0x03, 0x03, 0x03, 0x03, 0x03, 0x03, 0x03, 0x03, 0x03, 0x03, 0x03, 0x03, 0x03, 0x03, 0x03, 0x03, 0x03, 0x03, 0x03, 0x03, 0x03, 0x03, 0x03, 0x03},
    {0x03, 0x03, 0x03, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x03, 0x03, 0x03},
    {0x03, 0x03, 0x03, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x03, 0x03, 0x03},
    {0x03, 0x03, 0x03, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x03, 0x03, 0x03},
    {0x03, 0x03, 0x03, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x03, 0x03, 0x03},
    {0x03, 0x03, 0x03, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x03, 0x03, 0x03},
    {0x03, 0x03, 0x03, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x03, 0x03, 0x03},
    {0x03, 0x03, 0x03, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x03, 0x03, 0x03},
    {0x03, 0x03, 0x03, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x03, 0x03, 0x03},
    {0x03, 0x03, 0x03, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x03, 0x03, 0x03},
    {0x03, 0x03, 0x03, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x03, 0x03, 0x03},
    {0x03, 0x03, 0x03, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x03, 0x03, 0x03},
    {0x03, 0x03, 0x03, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x03, 0x03, 0x03},
    {0x03, 0x03, 0x03, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x03, 0x03, 0x03},
    {0x03, 0x03, 0x03, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x03, 0x03, 0x03},
    {0x03, 0x03, 0x03, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x03, 0x03, 0x03},
    {0x03, 0x03, 0x03, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x03, 0x03, 0x03},
    {0x03, 0x03, 0x03, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x03, 0x03, 0x03},
    {0x03, 0x03, 0x03, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x03, 0x03, 0x03},
    {0x03, 0x03, 0x03, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x03, 0x03, 0x03},
    {0x03, 0x03, 0x03, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x03, 0x03, 0x03},
    {0x03, 0x03, 0x03, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x03, 0x03, 0x03},
    {0x03, 0x03, 0x03, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x03, 0x03, 0x03},
    {0x03, 0x03, 0x03, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x03, 0x03, 0x03},
    {0x03, 0x03, 0x03, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x03, 0x03, 0x03},
    {0x03, 0x03, 0x03, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x03, 0x03, 0x03},
    {0x03, 0x03, 0x03, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x03, 0x03, 0x03},
    {0x03, 0x03, 0x03, 0x03, 0x03, 0x03, 0x03, 0x03, 0x03, 0x03, 0x03, 0x03, 0x03, 0x03, 0x03, 0x03, 0x03, 0x03, 0x03, 0x03, 0x03, 0x03, 0x03, 0x03, 0x03, 0x03, 0x03, 0x03, 0x03, 0x03, 0x03, 0x03},
    {0x03, 0x03, 0x03, 0x03, 0x03, 0x03, 0x03, 0x03, 0x03, 0x03, 0x03, 0x03, 0x03, 0x03, 0x03, 0x03, 0x03, 0x03, 0x03, 0x03, 0x03, 0x03, 0x03, 0x03, 0x03, 0x03, 0x03, 0x03, 0x03, 0x03, 0x03, 0x03},
    {0x03, 0x03, 0x03, 0x03, 0x03, 0x03, 0x03, 0x03, 0x03, 0x03, 0x03, 0x03, 0x03, 0x03, 0x03, 0x03, 0x03, 0x03, 0x03, 0x03, 0x03, 0x03, 0x03, 0x03, 0x03, 0x03, 0x03, 0x03, 0x03, 0x03, 0x03, 0x03}
};

#endif // GRID_H



================================================
FILE: pufferlib/ocean/shared_pool/shared_pool.c
================================================
#include <raylib.h>
#include <unistd.h>
#include "cpr.h"
#include "puffernet.h"
#include "shared_pool.h"

int main() {
  CCpr env = {
      .num_agents = 4,
      .width = 32,
      .height = 32,
      .vision = 3,
      .reward_food = 1.0f,
      .interactive_food_reward = 5.0f,
      .food_base_spawn_rate = 2e-3,
  };
  allocate_ccpr(&env);
  c_reset(&env);
  c_render(&env);

  Weights* weights = load_weights("resources/cpr/cpr_weights.bin", 139270);
  int logit_sizes[] = {5};
  LinearLSTM* net = make_linearlstm(weights, env.num_agents, 49, logit_sizes, 1);
 
  while (!WindowShouldClose()) {
    // User can take control of the first puffer
    if (IsKeyDown(KEY_LEFT_SHIFT)) {
      if (IsKeyDown(KEY_UP) || IsKeyDown(KEY_W))
        env.actions[0] = 0;
      if (IsKeyDown(KEY_DOWN) || IsKeyDown(KEY_S))
        env.actions[0] = 1;
      if (IsKeyDown(KEY_LEFT) || IsKeyDown(KEY_A))
        env.actions[0] = 2;
      if (IsKeyDown(KEY_RIGHT) || IsKeyDown(KEY_D))
        env.actions[0] = 3;

      printf("Getting user input %d\n", env.actions[0]);
      sleep(2);
    } else {
        for (int i = 0; i < env.num_agents*49; i++) {
            net->obs[i] = env.observations[i];
        }
        forward_linearlstm(net, net->obs, env.actions);
    }

    c_step(&env);
    c_render(&env);
  }
  //close_renderer(renderer);
  free_CCpr(&env);

  return 0;
}



================================================
FILE: pufferlib/ocean/shared_pool/shared_pool.h
================================================
#include <assert.h>
#include <stdbool.h>
#include <stdint.h>
#include <stdio.h>
#include <stdlib.h>
#include <string.h>
#include <time.h>
#include <math.h>

#include "raylib.h"

#include "grid.h"

#define EMPTY 0
#define NORMAL_FOOD 1
#define INTERACTIVE_FOOD 2
// Anything above Wall should be obstacles
#define WALL 3
#define AGENTS 4

#define LOG_BUFFER_SIZE 8192

#define SET_BIT(arr, i) (arr[(i) / 8] |= (1 << ((i) % 8)))
#define CLEAR_BIT(arr, i) (arr[(i) / 8] &= ~(1 << ((i) % 8)))
#define CHECK_BIT(arr, i) (arr[(i) / 8] & (1 << ((i) % 8)))
#define min(a, b) ((a) < (b) ? (a) : (b))

#define REWARD_20_HP -0
#define REWARD_80_HP 0
#define REWARD_DEATH -1.0f


#define LOG_SCORE_REWARD_SMALL 0.1f
#define LOG_SCORE_REWARD_MEDIUM 0.2f
#define LOG_SCORE_REWARD_MOVE - 0.0
#define LOG_SCORE_REWARD_DEATH -1

#define HP_REWARD_FOOD_MEDIUM 50
#define HP_REWARD_FOOD_SMALL 20
#define HP_LOSS_PER_STEP 1
#define MAX_HP 100 

typedef struct Log Log;
struct Log {
  float perf;
  float score;
  float episode_return;
  float moves;
  float food_nb;
  float alive_steps;
  float n;
};

typedef struct Agent Agent;
struct Agent {
  int r;
  int c;
  int id;
  float hp;
  int direction;
};

typedef struct FoodList FoodList;
struct FoodList {
  int *indexes; // Grid flattened index positions
  int size;
};

FoodList *allocate_foodlist(int size) {
  FoodList *foods = (FoodList *)calloc(1, sizeof(FoodList));
  foods->indexes = (int *)calloc(size, sizeof(int));
  foods->size = 0;
  return foods;
}

void free_foodlist(FoodList *foods) {
  free(foods->indexes);
  free(foods);
}

typedef struct Renderer Renderer;
typedef struct CCpr CCpr;
struct CCpr {
  Renderer* client;
  int width;
  int height;
  int num_agents;

  int vision;
  int vision_window;
  int obs_size;

  int tick;

  float reward_food;
  float reward_move;
  float interactive_food_reward;

  unsigned char *grid;
  unsigned char *observations;
  int *actions;
  float *rewards;
  unsigned char *terminals;
  unsigned char *truncations;
  unsigned char *masks;

  Agent *agents;

  Log log;
  Log* agent_logs;

  uint8_t *interactive_food_agent_count;

  FoodList *foods;
  float food_base_spawn_rate;
};

void add_log(CCpr *env, Log *log) {
  env->log.perf = fmaxf(0, 1.0 - 0.01*log->alive_steps);
  env->log.episode_return += log->episode_return;
  env->log.score += log->score;
  env->log.moves += log->moves / log->alive_steps;
  env->log.alive_steps += log->alive_steps;
  env->log.n += 1;
}

void init_ccpr(CCpr *env) {
  env->grid =
      (unsigned char *)calloc(env->width * env->height, sizeof(unsigned char));
  env->agents = (Agent *)calloc(env->num_agents, sizeof(Agent));
  env->vision_window = 2 * env->vision + 1;
  env->obs_size = env->vision_window * env->vision_window;// + 1;
  env->interactive_food_agent_count =
      (uint8_t *)calloc((env->width * env->height + 7) / 8, sizeof(uint8_t));
  env->foods = allocate_foodlist(env->width * env->height);
  env->agent_logs = (Log *)calloc(env->num_agents, sizeof(Log));
  env->masks = (unsigned char *)calloc(env->num_agents, sizeof(unsigned char));
}

void allocate_ccpr(CCpr *env) {
  // Called by C stuff
  int obs_size = (2 * env->vision + 1) * (2 * env->vision + 1); //+ 1;
  env->observations = (unsigned char *)calloc(env->num_agents * obs_size,
                                              sizeof(unsigned char));
  env->actions = (int *)calloc(env->num_agents, sizeof(unsigned int));
  env->rewards = (float *)calloc(env->num_agents, sizeof(float));
  env->terminals =
      (unsigned char *)calloc(env->num_agents, sizeof(unsigned char));
  env->truncations = (unsigned char*)calloc(env->num_agents, sizeof(unsigned char));
  init_ccpr(env);
}

void c_close(CCpr *env) {
  free(env->grid);
  free(env->agents);
  free(env->interactive_food_agent_count);
  free_foodlist(env->foods);
  free(env->masks);
  free(env->agent_logs);
}

void free_CCpr(CCpr *env) {
  free(env->observations);
  free(env->actions);
  free(env->rewards);
  free(env->terminals);
  free(env->truncations);
  c_close(env);
}

int grid_index(CCpr *env, int r, int c) { return r * env->width + c; }
int get_agent_tile_from_id(int agent_id) { return AGENTS + agent_id; }

int get_agent_id_from_tile(int tile) { return tile - AGENTS; }

void add_food(CCpr *env, int grid_idx, int food_type) {
  // Add food to the grid and the food_list at grid_idx
  assert(env->grid[grid_idx] == EMPTY);
  env->grid[grid_idx] = food_type;
  FoodList *foods = env->foods;
  foods->indexes[foods->size++] = grid_idx;
}

void reward_agent(CCpr *env, int agent_id, float reward) {
  // We don't reward if agent is full life
  // Agent *agent = &env->agents[agent_id];
  // if (agent->hp >= MAX_HP) {
  //   return;
  // }
  env->rewards[agent_id] += reward;
  env->agent_logs[agent_id].episode_return += reward;
}

void spawn_food(CCpr *env, int food_type) {
  // Randomly spawns such food in the grid
  int idx, tile;
  do {
    int r = rand() % (env->height - 1);
    int c = rand() % (env->width - 1);
    idx = r * env->width + c;
    tile = env->grid[idx];
  } while (tile != EMPTY);
  add_food(env, idx, food_type);
}

void remove_food(CCpr *env, int grid_idx) {
  // Removes food from the grid and food_list
  env->grid[grid_idx] = EMPTY;
  FoodList *foods = env->foods;
  for (int i = 0; i < foods->size; i++) {
    if (foods->indexes[i] == grid_idx) {
      foods->indexes[i] = foods->indexes[foods->size - 1];
      foods->size--;
      return;
    }
  }
}

void init_foods(CCpr *env) {
  // On reset spawns x number of each food randomly.
  int available_tiles = (env->width * env->height) -
                        (2 * env->vision * env->width +
                         2 * env->vision * (env->height - 2 * env->vision));
  int normalizer = (env->width * env->height) / 576;
  int normal = available_tiles / (20 * normalizer);
  int interactive = available_tiles / (50 * normalizer);
  for (int i = 0; i < normal; i++) {
    spawn_food(env, NORMAL_FOOD);
  }
  for (int i = 0; i < interactive; i++) {
    spawn_food(env, INTERACTIVE_FOOD);
  }
}

void spawn_foods(CCpr *env) {
  // After each step, check existing foods and spawns new food in the
  // neighborhood Iterates over food_list for efficiency instead of the entire
  // grid.
  FoodList *foods = env->foods;
  int original_size = foods->size;
  for (int i = 0; i < original_size; i++) {
    int idx = foods->indexes[i];
    int offset = idx - env->width - 1; // Food spawn in 1 radius
    int r = offset / env->width;
    int c = offset % env->width;
    for (int ri = 0; ri < 3; ri++) {
      for (int ci = 0; ci < 3; ci++) {
        int grid_idx = grid_index(env, (r + ri), (c + ci));
        if (env->grid[grid_idx] != EMPTY) {
          continue;
        }
        switch (env->grid[idx]) {
        // %Chance spawning new food
        case NORMAL_FOOD:
          if ((rand() / (double)RAND_MAX) < env->food_base_spawn_rate) {
            add_food(env, grid_idx, env->grid[idx]);
          }
          break;
        case INTERACTIVE_FOOD:
          if ((rand() / (double)RAND_MAX) <
              (env->food_base_spawn_rate / 10.0)) {
            add_food(env, grid_idx, env->grid[idx]);
          }
          break;
        }
      }
    }
  }

  // // Each turn there is random probability for a food to spawn at a random
  // // location To cope with resource depletion
  // int normalizer = (env->width * env->height) / 576;
  // if ((rand() / (double)RAND_MAX) <
  //     min((env->food_base_spawn_rate * 2 * normalizer), 1e-2)) {
  //   spawn_food(env, NORMAL_FOOD);
  // }
  // if ((rand() / (double)RAND_MAX) <
  //     min((env->food_base_spawn_rate / 5.0 * normalizer), 5e-3)) {
  //   spawn_food(env, INTERACTIVE_FOOD);
  // }
}

void compute_observations(CCpr *env) {
  // For full obs
  // memcpy(env->observations, env->grid,
  //        env->width * env->height * sizeof(unsigned char));
  // return;

  // For partial obs
  for (int i = 0; i < env->num_agents; i++) {
    Agent *agent = &env->agents[i];
    // env->observations[env->vision_window*env->vision_window + i*env->obs_size] = agent->hp;
    if (agent->hp == 0) {
      continue;
    }
    int obs_offset = i * env->obs_size;
    int r_offset = agent->r - env->vision;
    int c_offset = agent->c - env->vision;
    for (int r = 0; r < 2 * env->vision + 1; r++) {
      for (int c = 0; c < 2 * env->vision + 1; c++) {
        int grid_idx = (r_offset + r) * env->width + c_offset + c;
        int obs_idx = obs_offset + r * env->vision_window + c;
        env->observations[obs_idx] = env->grid[grid_idx];
      }
    }

    
  }
}

void add_hp(CCpr *env, int agent_id, float hp) {
  Agent *agent = &env->agents[agent_id];
  agent->hp += hp;
  if (agent->hp > MAX_HP) {
    agent->hp = MAX_HP;
  } else if (agent->hp <= 0) {
    agent->hp = 0;
    env->agent_logs[agent->id].score += LOG_SCORE_REWARD_DEATH;
    reward_agent(env, agent_id, REWARD_DEATH);
    env->terminals[agent->id] = 1;
    add_log(env, &env->agent_logs[agent_id]);
  }
}

void remove_hp(CCpr *env, int agent_id, float hp) {
    add_hp(env, agent_id, -hp);
}

void save_grid_to_file(CCpr *env, const char *filename) {
    FILE *file = fopen(filename, "w");
    if (!file) {
        perror("Failed to open file");
        return;
    }
    fprintf(file, "#ifndef GRID_H\n#define GRID_H\n\n");
    fprintf(file, "#define GRID_HEIGHT %d\n", env->height);
    fprintf(file, "#define GRID_WIDTH %d\n\n", env->width);
    fprintf(file, "static const unsigned char grid[GRID_HEIGHT][GRID_WIDTH] = {\n");

    for (int r = 0; r < env->height; r++) {
        fprintf(file, "    {");
        for (int c = 0; c < env->width; c++) {
            unsigned char val = env->grid[r * env->width + c];
            fprintf(file, "0x%02X%s", val, (c == env->width - 1) ? "" : ", ");
        }
        fprintf(file, "}%s\n", (r == env->height - 1) ? "" : ",");
    }
    fprintf(file, "};\n\n#endif // GRID_H\n");
    fclose(file);
}

void make_grid_from_scratch(CCpr *env){
  memset(env->grid, EMPTY, (env->height * env->width) * sizeof(env->grid[0]));
  // top walling
  for (int r = 0; r < env->vision; r++) {
    memset(env->grid + (r * env->width), WALL,
           env->width * sizeof(env->grid[0]));
  }
  // left side walling
  for (int r = 0; r < env->height; r++) {
    memset(env->grid + (r * env->width), WALL,
           env->vision * sizeof(env->grid[0]));
  }
  // bottom walling
  for (int r = env->height - env->vision; r < env->height; r++) {
    memset(env->grid + (r * env->width), WALL,
           env->width * sizeof(env->grid[0]));
  }

  // right side walling
  for (int r = 0; r < env->height; r++) {
    memset(env->grid + (r * env->width) + (env->width - env->vision), WALL,
           env->vision * sizeof(env->grid[0]));
  }
  save_grid_to_file(env, "grid.h");
}

void spawn_agent(CCpr *env, int i){
  Agent *agent = &env->agents[i];
  agent->id = i;
  agent->hp = 80;
  int adr = 0;

  bool allocated = false;
  while (!allocated) {
    adr = rand() % (env->height * env->width);
    if (env->grid[adr] == EMPTY) {
      int r = adr / env->width;
      int c = adr % env->width;
      agent->r = r;
      agent->c = c;
      allocated = true;
    }
  }
  assert(env->grid[adr] == EMPTY);
  env->grid[adr] = get_agent_tile_from_id(agent->id);
  env->agent_logs[i] = (Log){0};
}
void c_reset(CCpr *env) {
  env->tick = 0;
  memset(env->agent_logs, 0, env->num_agents * sizeof(Log));
  env->log = (Log){0};
  env->foods->size = 0;
  memset(env->foods->indexes, 0, env->width * env->height * sizeof(int));
  // make_grid_from_scratch(env);
  memcpy(env->grid, grid_32_32_3v, env->width * env->height * sizeof(unsigned char));

  for (int i = 0; i < env->num_agents; i++) {
    spawn_agent(env, i);
  }

  init_foods(env);
  memset(env->observations, 0, env->num_agents * env->obs_size * sizeof(unsigned char));
  //memset(env->truncations, 0, env->num_agents * sizeof(unsigned char));
  memset(env->terminals, 0, env->num_agents * sizeof(unsigned char));
  memset(env->masks, 1, env->num_agents * sizeof(unsigned char));
  compute_observations(env);
}

void reward_agents_near(CCpr *env, int food_index) {
  int food_r = food_index / env->width;
  int food_c = food_index % env->width;

  // TODO: could iterate over neighbors of food index and check if is agent
  // (remove iteration cost)
  for (int i = 0; i < env->num_agents; i++) {
    int ac = env->agents[i].c;
    int ar = env->agents[i].r;

    if ((ac == food_c && (ar == food_r - 1 || ar == food_r + 1)) ||
        (ar == food_r && (ac == food_c - 1 || ac == food_c + 1))) {
      reward_agent(env, i, env->interactive_food_reward);
      env->agent_logs[i].score += LOG_SCORE_REWARD_MEDIUM;
      add_hp(env, i, HP_REWARD_FOOD_MEDIUM);
    }
  }
  remove_food(env, food_index);
}

void step_agent(CCpr *env, int i) {

  Agent *agent = &env->agents[i];

  int action = env->actions[i];

  int dr = 0;
  int dc = 0;

  switch (action) {
  case 0:
    dr = -1;
    agent->direction = 3;
    break; // UP
  case 1:
    dr = 1;
    agent->direction = 1;
    break; // DOWN
  case 2:
    dc = -1;
    agent->direction = 2;
    break; // LEFT
  case 3:
    dc = 1;
    agent->direction = 0;
    break; // RIGHT
  case 4:
    return; // No moves
  }
  env->agent_logs[i].moves += 1;

  // Get next row and column

  int next_r = agent->r + dr;
  int next_c = agent->c + dc;

  int prev_grid_idx = grid_index(env, agent->r, agent->c);
  int next_grid_idx = env->width * next_r + next_c;
  int tile = env->grid[next_grid_idx];

  // Anything above should be obstacle
  // In this case the agent position does not change
  // We still have some checks to perform
  if (tile >= INTERACTIVE_FOOD) {
    env->agent_logs[i].score += LOG_SCORE_REWARD_MOVE;
    reward_agent(env, i, env->reward_move);
    next_r = agent->r;
    next_c = agent->c;
    next_grid_idx = env->width * next_r + next_c;
    tile = env->grid[next_grid_idx];
  }
  switch (tile) {
  case NORMAL_FOOD:
    reward_agent(env, i, env->reward_food);
    env->agent_logs[i].score += LOG_SCORE_REWARD_SMALL;
    add_hp(env, i, HP_REWARD_FOOD_SMALL);
    remove_food(env, next_grid_idx);
    break;
  case EMPTY:
    env->agent_logs[i].score += LOG_SCORE_REWARD_MOVE;
    reward_agent(env, i, env->reward_move);
    break;
  }

  // Interactive food logic
  int neighboors[4] = {
      grid_index(env, next_r - 1, next_c), // Up
      grid_index(env, next_r + 1, next_c), // Down
      grid_index(env, next_r, next_c + 1), // Right
      grid_index(env, next_r, next_c - 1)  // Left
  };

  for (int j = 0; j < 4; j++) {
    int grid_idx = neighboors[j];
    // If neighbooring grid tile is interactive food
    if (env->grid[grid_idx] == INTERACTIVE_FOOD) {
      // If was already marked as "ready to collect"
      if (CHECK_BIT(env->interactive_food_agent_count, grid_idx)) {
        reward_agents_near(env, grid_idx);
      } else {
        // First agent detected
        SET_BIT(env->interactive_food_agent_count, grid_idx);
      }
    }
  }

  // update the grid tiles values
  int agent_tile = get_agent_tile_from_id(agent->id);
  env->grid[prev_grid_idx] = EMPTY;
  env->grid[next_grid_idx] = agent_tile;
  agent->r = next_r;
  agent->c = next_c;

  return;
}

void clear_agent(CCpr *env, int agent_id) {
  Agent *agent = &env->agents[agent_id];
  if (agent->r < 0 || agent->c < 0) {
    return;
  }
  int grid_idx = grid_index(env, agent->r, agent->c);
  env->grid[grid_idx] = EMPTY;
  agent->r = -1;
  agent->c = -1;
}

void c_step(CCpr *env) {
  env->tick++;

  memset(env->rewards, 0, env->num_agents * sizeof(float));
  memset(env->interactive_food_agent_count, 0,
         (env->width * env->height + 7) / 8);

  for (int i = 0; i < env->num_agents; i++) {
    if (env->agents[i].hp == 0) {
      env->masks[i] = 0;
      clear_agent(env, i);
      continue;
    }
    step_agent(env, i);
    remove_hp(env, i, HP_LOSS_PER_STEP);
  }

  spawn_foods(env);

  //We loop again here because in the future an entity might have attacked an agent in the process
  int alive_agents = 0;
  for (int i = 0; i < env->num_agents; i++) {
    if (env->agents[i].hp > 0) {
      env->agent_logs[i].alive_steps += 1;
      alive_agents += 1;
      if (env->agents[i].hp < 20) {
        reward_agent(env, i, REWARD_20_HP);
        env->agent_logs[i].score += REWARD_20_HP;
      } else if (env->agents[i].hp > 80) {
        reward_agent(env, i, REWARD_80_HP);
        env->agent_logs[i].score += REWARD_80_HP;
      }
    } 
    // else {
      // int grid_idx = grid_index(env, env->agents[i].r, env->agents[i].c);
      // env->grid[grid_idx] = EMPTY;
      // spawn_agent(env, i);
    // }
  }
  /*
  if (alive_agents == 0) {
    env->agent_logs[i].moves = 0;
  }else{
    env->agent_logs[i].moves /= alive_agents;
  }
  env->agent_logs[i].food_nb = env->foods->size;
  env->agent_logs[i].alive_steps = env->tick;
  */
  env->log.food_nb = env->foods->size;
  compute_observations(env);
  if (alive_agents == 0 || env->tick > 1000) {
    c_reset(env);
    if (alive_agents == 0) {
      memset(env->terminals, 1, env->num_agents * sizeof(unsigned char)); 
    }
  }
}

// Raylib client
Color COLORS[] = {
    (Color){255, 0, 0, 255},     (Color){170, 170, 170, 255},
    (Color){255, 255, 0, 255},   (Color){0, 255, 0, 255},
    (Color){0, 255, 255, 255},   (Color){0, 128, 255, 255},
    (Color){128, 128, 128, 255}, (Color){255, 0, 0, 255},
    (Color){255, 255, 255, 255}, (Color){255, 85, 85, 255},
    (Color){170, 170, 170, 255}, (Color){0, 255, 255, 255},
    (Color){0, 0, 255, 255},     (Color){6, 24, 24, 255},
};

Rectangle UV_COORDS[7] = {
    (Rectangle){0, 0, 0, 0},       (Rectangle){512, 0, 128, 128},
    (Rectangle){0, 0, 0, 0},       (Rectangle){0, 0, 128, 128},
    (Rectangle){128, 0, 128, 128}, (Rectangle){256, 0, 128, 128},
    (Rectangle){384, 0, 128, 128},
};

struct Renderer {
  int cell_size;
  int width;
  int height;
  Texture2D puffer;
};

Renderer *init_renderer(int cell_size, int width, int height) {
  Renderer *renderer = (Renderer *)calloc(1, sizeof(Renderer));
  renderer->cell_size = cell_size;
  renderer->width = width;
  renderer->height = height;

  InitWindow(width * cell_size, height * cell_size, "CPR");
  SetTargetFPS(10);

  renderer->puffer = LoadTexture("resources/shared/puffers_128.png");
  return renderer;
}

void close_renderer(Renderer *renderer) {
  CloseWindow();
  free(renderer);
}

void c_render(CCpr *env) {
  if (env->client == NULL) {
      env->client = init_renderer(32, env->width, env->height);
  };
  Renderer *renderer = env->client;

  if (IsKeyDown(KEY_ESCAPE)) {
    exit(0);
  }

  BeginDrawing();
  ClearBackground((Color){6, 24, 24, 255});

  int ts = renderer->cell_size;
  for (int r = 0; r < env->height; r++) {
    for (int c = 0; c < env->width; c++) {
      int adr = grid_index(env, r, c);
      int tile = env->grid[adr];
      if (tile == EMPTY) {
        continue;
      } else if (tile == WALL) {
        DrawRectangle(c * ts, r * ts, ts, ts, (Color){227, 227, 227, 255});
      } else if (tile == NORMAL_FOOD || tile == INTERACTIVE_FOOD) {
        DrawRectangle(c * ts, r * ts, ts, ts, COLORS[tile]);
      } else {

        int agent_id = get_agent_id_from_tile(tile);
        int col_id = agent_id % (sizeof(COLORS) / sizeof(COLORS[0]));
        Color color = COLORS[col_id];
        int starting_sprite_x = 0;
        float rotation = env->agents[agent_id].direction * 90.0f;
        if (rotation == 180) {
          starting_sprite_x = 128;
          rotation = 0;
        }
        Rectangle source_rect = (Rectangle){starting_sprite_x, 0, 128, 128};
        Rectangle dest_rect = (Rectangle){c * ts + ts/2, r * ts + ts/2, ts, ts};        
        DrawTexturePro(renderer->puffer, source_rect, dest_rect,
                       (Vector2){ts/2, ts/2}, rotation, color);
      }
    }
  }
  EndDrawing();
}



================================================
FILE: pufferlib/ocean/shared_pool/shared_pool.py
================================================
import gymnasium 
import numpy as np 

import pufferlib 
from pufferlib.ocean.shared_pool import binding

class PyCPR(pufferlib.PufferEnv):
    def __init__(self, 
                num_envs=1,
                widths=[32],
                heights=[32], 
                num_agents=[8],  
                vision=3, 
                reward_food=1.0, 
                interactive_food_reward=5.0,
                reward_move=-0.01,
                food_base_spawn_rate=2e-3,
                report_interval=1,
                render_mode=None, 
                buf=None,
                seed=0,
            ):
        widths = num_envs*widths
        heights = num_envs*heights 
        num_agents = num_envs*num_agents 

        self.single_observation_space = gymnasium.spaces.Box(low=0, high=255, shape=((2*vision+1)*(2*vision+1),), dtype=np.uint8)
        self.single_action_space = gymnasium.spaces.Discrete(5)
        self.render_mode = render_mode
        self.num_agents = sum(num_agents)

        self.tick = 0
        self.report_interval = report_interval

        super().__init__(buf)
        c_envs = []
        for i in range(num_envs):
            n = num_agents[i]
            env_id = binding.env_init(
                self.observations[i*n:(i+1)*n],
                self.actions[i*n:(i+1)*n],
                self.rewards[i*n:(i+1)*n],
                self.terminals[i*n:(i+1)*n],
                self.truncations[i*n:(i+1)*n],
                i + seed * num_envs,
                width=widths[i],
                height=heights[i],
                num_agents=num_agents[i],
                vision=vision,
                reward_food=reward_food,
                interactive_food_reward=interactive_food_reward,
                reward_move=reward_move,
                food_base_spawn_rate=food_base_spawn_rate,
            )
            c_envs.append(env_id)

        self.c_envs = binding.vectorize(*c_envs)

    def reset(self, seed=None):
        self.tick = 0
        binding.vec_reset(self.c_envs, seed)
        return self.observations, []
    
    def step(self, actions):
        self.actions[:] = actions 
        binding.vec_step(self.c_envs)
        self.tick += 1

        info = []
        if self.tick % self.report_interval == 0:
            log = binding.vec_log(self.c_envs)
            if log:
                info.append(log)

        return (self.observations, self.rewards, self.terminals, self.truncations, info)

    def render(self):
        binding.vec_render(self.c_envs, 0)

    def close(self):
        binding.vec_close(self.c_envs)

if __name__ == "__main__":
    env = PyCPR()
    env.reset()
    tick = 0
    timeout=30

    tot_agents = env.num_agents
    actions = np.random.randint(0,5,(1024,tot_agents))

    import time 
    start = time.time()
    # while time.time() - start < timeout:
    while tick < 500:
        atns = actions[tick % 1024]
        env.step(atns)
        if -1 in env.rewards:
            breakpoint()
        # env.render()
        tick += 1

    print(f'SPS: {int(tot_agents * tick / (time.time() - start)):_}')

    env.close()







================================================
FILE: pufferlib/ocean/slimevolley/binding.c
================================================
#include "slimevolley.h"

#define Env SlimeVolley
#include "../env_binding.h"

static int my_init(Env* env, PyObject* args, PyObject* kwargs) {    
    env->num_agents = unpack(kwargs, "num_agents");
    init(env);
    return 0;
}

static int my_log(PyObject* dict, Log* log) {
    assign_to_dict(dict, "perf", log->perf);
    assign_to_dict(dict, "score", log->score);
    assign_to_dict(dict, "episode_return", log->episode_return);
    assign_to_dict(dict, "episode_length", log->episode_length);
    return 0;
}



================================================
FILE: pufferlib/ocean/slimevolley/eval.py
================================================
import gymnasium
import numpy as np

from pufferlib.ocean.slimevolley import binding
import pufferlib
from pufferlib.ocean.torch import Policy
import torch

class SlimeVolley(pufferlib.PufferEnv):
    def __init__(self, num_envs=1, render_mode=None, log_interval=128, buf=None, seed=0,
                 num_agents=1):
        assert num_agents in {1, 2}, "num_agents must be 1 or 2"
        num_obs = 12
        self.single_observation_space = gymnasium.spaces.Box(low=0, high=1,
            shape=(num_obs,), dtype=np.float32)
        self.single_action_space = gymnasium.spaces.MultiDiscrete([2, 2, 2])

        self.render_mode = render_mode
        self.num_agents = num_envs * num_agents
        self.log_interval = log_interval

        super().__init__(buf)
        c_envs = []
        for i in range(num_envs):
            c_env = binding.env_init(
                self.observations[i*num_agents:(i+1)*num_agents],
                self.actions[i*num_agents:(i+1)*num_agents],
                self.rewards[i*num_agents:(i+1)*num_agents],
                self.terminals[i*num_agents:(i+1)*num_agents],
                self.truncations[i*num_agents:(i+1)*num_agents],
                seed,
                num_agents=num_agents
                )
            c_envs.append(c_env)

        self.c_envs = binding.vectorize(*c_envs)

    def reset(self, seed=0):
        binding.vec_reset(self.c_envs, seed)
        self.tick = 0
        return self.observations, []

    def step(self, actions):
        self.tick += 1
        self.actions[:] = actions
        binding.vec_step(self.c_envs)

        info = []
        if self.tick % self.log_interval == 0:
            log = binding.vec_log(self.c_envs)
            if log:
                info.append(log)

        return (self.observations, self.rewards,
            self.terminals, self.truncations, info)

    def render(self):
        binding.vec_render(self.c_envs, 0)

    def close(self):
        binding.vec_close(self.c_envs)
        

if __name__ == "__main__":
    env = SlimeVolley(num_envs=1, num_agents=1)
    observations, _ = env.reset()
    env.render()
    policy = Policy(env)
    policy.load_state_dict(torch.load("checkpoint.pt", map_location="cpu"))
    with torch.no_grad():
        while True:
            actions = policy(torch.from_numpy(observations))
            actions = [float(torch.argmax(a)) for a in actions[0]]
            o, r, t, _, i = env.step([actions])
            env.render()
            if t[0]:
                break


================================================
FILE: pufferlib/ocean/slimevolley/slimevolley.c
================================================
/* Pure C demo file for SlimeVolley. Build it with:
 * bash scripts/build_ocean.sh target local (debug)
 * bash scripts/build_ocean.sh target fast
 * We suggest building and debugging your env in pure C first. You
 * get faster builds and better error messages
 */
#include "slimevolley.h"
#include <stdio.h>


void abranti_simple_policy(float* obs, float* action) {
    float x_agent = obs[0];
    float x_ball = obs[4];
    float vx_ball = obs[6];
    float backward = (-23.757145f * x_agent + 23.206863f * x_ball + 0.7943352f * vx_ball) + 1.4617119f;
    float forward = -64.6463748f * backward + 22.4668393f;
    action[0] = forward;
    action[1] = backward;
    action[2] = 1.0f; // always jump
}

void random_policy(float* obs, float* action) {
    action[0] = rand() * 2 - 1;
    action[1] = rand() * 2 - 1;
    action[2] = rand() * 2 - 1;
}

int main() {
    int num_obs = 12;
    int num_actions = 3;
    SlimeVolley env = {.num_agents = 1};
    init(&env);
    env.observations = (float*)calloc(env.num_agents*num_obs, sizeof(float));
    env.actions = (float*)calloc(num_actions*env.num_agents, sizeof(float));
    env.rewards = (float*)calloc(env.num_agents, sizeof(float));
    env.terminals = (unsigned char*)calloc(env.num_agents, sizeof(unsigned char));
    // Always call reset and render first
    c_reset(&env);
    c_render(&env);

    fprintf(stderr, "num agents: %d\n", env.num_agents);

    while (!WindowShouldClose()) {
        for (int i=0; i<env.num_agents; i++) {
            if (i == 0) {
                random_policy(&env.observations[12*i], &env.actions[3*i]);
                
            } else {
                abranti_simple_policy(&env.observations[12*i], &env.actions[3*i]);
            }
        }
        c_step(&env);
        c_render(&env);
        if (env.terminals[0] || env.terminals[1]) {
            fprintf(stderr, "Episode ended. Rewards: %f, %f\n", env.rewards[0], env.rewards[1]);
            break;
        }
    }

    // Try to clean up after yourself
    free(env.observations);
    free(env.actions);
    free(env.rewards);
    free(env.terminals);
    c_close(&env);
}



================================================
FILE: pufferlib/ocean/slimevolley/slimevolley.h
================================================
#include "raylib.h"
#include <stdlib.h>
#include <string.h>
#include <math.h>
#include <assert.h>

// CONFIG
#define REF_W 48
#define REF_H REF_W
#define REF_U 1.5 // ground height
#define REF_WALL_WIDTH 1.0
#define REF_WALL_HEIGHT 3.5
#define PLAYER_SPEED_X (10*1.75)
#define PLAYER_SPEED_Y (10*1.35)
#define MAX_BALL_SPEED (15*1.5)
#define TIMESTEP (1.0/30.0)
#define NUDGE 0.1
#define FRICTION 1.0 // 1 means no FRICTION, less means FRICTION. (should be called elasticity imo)
#define INIT_DELAY_FRAMES 30
#define GRAVITY (-9.8*2*1.5)
#define MAXLIVES 5 // game ends when one agent loses this many games
#define WINDOW_WIDTH 1200
#define WINDOW_HEIGHT 500
#define FACTOR (WINDOW_WIDTH / REF_W)
#define PIXEL_MODE false
#define PIXEL_SCALE 4
#define PIXEL_WIDTH (84*2)
#define PIXEL_HEIGHT 84
#define MAX_TICKS 3000

// Day colors
const Color BALL_COLOR = {255, 200, 20, 255};
const Color AGENT_LEFT_COLOR = {240, 75, 0, 255};
const Color AGENT_RIGHT_COLOR = {0, 150, 255, 255};
const Color PIXEL_AGENT_LEFT_COLOR = {240, 75, 0, 255};
const Color PIXEL_AGENT_RIGHT_COLOR = {0, 150, 255, 255};
const Color BACKGROUND_COLOR = {255, 255, 255, 255};
const Color FENCE_COLOR = {240, 210, 130, 255};
const Color COIN_COLOR = {240, 210, 130, 255};
const Color GROUND_COLOR = {128, 227, 153, 255};

// UTILS
typedef struct {
    float x;
    float y;
    float r;
    float vx;
    float vy;
} SphericalObject;


// convert from space to pixel coordinates
float to_x_pixel(float x){
    return (x + REF_W/2) * FACTOR;
}

float to_p(float x) {
    return x * FACTOR;
}

float to_y_pixel(float y){
    return WINDOW_HEIGHT - y * FACTOR;
}

// OBJECTS
typedef struct {
    float x;
    float y;
    float r;
    float vx;
    float vy;
    float prev_x;
    float prev_y;
    Color c;
} Ball;

void ball_display(Ball* ball){
    DrawCircleV((Vector2){to_x_pixel(ball->x), to_y_pixel(ball->y)}, to_p(ball->r), ball->c);
}

void ball_move(Ball* ball){
    ball->prev_x = ball->x;
    ball->prev_y = ball->y;
    ball->x += ball->vx * TIMESTEP;
    ball->y += ball->vy * TIMESTEP;
}

void ball_accelerate(Ball* ball, float ax, float ay){
    ball->vx += ax * TIMESTEP;
    ball->vy += ay * TIMESTEP;
}

int ball_check_edges(Ball* ball){
    if (ball->x <= (ball->r-REF_W/2)){
        ball->vx *= -FRICTION;
        ball->x = ball->r-REF_W/2+NUDGE*TIMESTEP;
    }
    if (ball->x >= (REF_W/2-ball->r)){
        ball->vx *= -FRICTION;
        ball->x = REF_W/2-ball->r-NUDGE*TIMESTEP;
    }
    if (ball->y <= (ball->r+REF_U)){
        ball->vy *= -FRICTION;
        ball->y = ball->r+REF_U+NUDGE*TIMESTEP;
        if (ball->x <= 0){
            return -1;
        }
        else{
            return 1;
        }
    }
    if (ball->y >= (REF_H-ball->r)){
        ball->vy *= -FRICTION;
        ball->y = REF_H-ball->r-NUDGE*TIMESTEP;
    }
    // fence:
    if ((ball->x <= (REF_WALL_WIDTH/2+ball->r)) && (ball->prev_x > (REF_WALL_WIDTH/2+ball->r)) && (ball->y <= REF_WALL_HEIGHT)){
        ball->vx *= -FRICTION;
        ball->x = REF_WALL_WIDTH/2+ball->r+NUDGE*TIMESTEP;
    }
    if ((ball->x >= (-REF_WALL_WIDTH/2-ball->r)) && (ball->prev_x < (-REF_WALL_WIDTH/2-ball->r)) && (ball->y <= REF_WALL_HEIGHT)){
        ball->vx *= -FRICTION;
        ball->x = -REF_WALL_WIDTH/2-ball->r-NUDGE*TIMESTEP;
    }
    return 0;
}

float ball_get_dist_squared(Ball* ball, SphericalObject* p){
    float dx = ball->x - p->x;
    float dy = ball->y - p->y;
    return dx*dx + dy*dy;
}

bool ball_is_colliding(Ball* ball, SphericalObject* p){
    float r = ball->r+p->r;
    return r*r > ball_get_dist_squared(ball, p);
}

void ball_bounce(Ball* ball, SphericalObject* p){
    float dx = ball->x - p->x;
    float dy = ball->y - p->y;
    float dist = sqrt(dx*dx + dy*dy);
    dx /= dist; // normalize. unit vector pointing from ball to p.
    dy /= dist;
    float nx = dx; // reuse calculation
    float ny = dy;

    dx *= NUDGE; // separate overlapping objects
    dy *= NUDGE;
    while(ball_is_colliding(ball, p)){
        ball->x += dx;
        ball->y += dy;
    }
    float ux = ball->vx - p->vx; // relative velocity of ball in relation to p
    float uy = ball->vy - p->vy;
    float un = ux*nx + uy*ny;
    float unx = nx*(un*2.); // added factor of 2 for conservation of momentum (elastic collision)
    float uny = ny*(un*2.); // added factor of 2 for conservation of momentum (elastic collision)
    ux -= unx;
    uy -= uny;
    ball->vx = ux + p->vx;
    ball->vy = uy + p->vy;
}

void ball_limit_speed(Ball* ball, float minSpeed, float maxSpeed){
    float mag2 = ball->vx*ball->vx+ball->vy*ball->vy;
    if (mag2 > (maxSpeed*maxSpeed)){
        float mag = sqrt(mag2);
        ball->vx /= mag;
        ball->vy /= mag;
        ball->vx *= maxSpeed;
        ball->vy *= maxSpeed;
    }
}

// Relative State
typedef struct {
    //agent
    float x;
    float y;
    float vx;
    float vy;
    //ball
    float bx;
    float by;
    float bvx;
    float bvy;
    //opponent
    float ox;
    float oy;
    float ovx;
    float ovy;
} RelativeState;

// WALL
typedef struct {
    float x;
    float y;
    float w;
    float h;
    Color c;
} Wall;

void wall_display(Wall* wall){
    Rectangle rec = {to_x_pixel(wall->x - wall->w/2), to_y_pixel(wall->y + wall->h/2),
         to_p(wall->w), to_p(wall->h)};
    DrawRectangleRec(rec, wall->c);
}

// AGENT
typedef struct {
    float x;
    float y;
    float r;
    float vx;
    float vy;
    int dir; // -1 means left, 1 means right player for symmetry
    Color c;
    float desired_vx;
    float desired_vy;
    float* observations;
    RelativeState *state;
    int lives;
} Agent;

void agent_display(Agent *agent, float bx, float by) {
    // fprintf(stderr, "agent_display: x=%f, y=%f, r=%f, lives=%d, dir=%d\n", agent->x, agent->y, agent->r, agent->lives, agent->dir);
    float x = agent->x;
    float y = agent->y;
    float r = agent->r;

    // Draw the agent's body as a half circle
    // Raylib: DrawCircleSector(center, radius, startAngle, endAngle, segments, color)
    DrawCircleSector(
        (Vector2){to_x_pixel(x), to_y_pixel(y)},
        to_p(r),
        180, 360,
        32, // segments
        agent->c
    );

    float angle = agent->dir == -1 ? PI * 60.0f / 180.0f : PI * 120.0f / 180.0f;

    // track ball with eyes
    float c = cosf(angle);
    float s = sinf(angle);
    float eye_base_x = x + 0.6f * r * c;
    float eye_base_y = y + 0.6f * r * s;
    float ballX = bx - eye_base_x;
    float ballY = by - eye_base_y;

    // If agent is sad (no lives), look down and away
    if (agent->lives == 0) {
        ballX = -agent->dir;
        ballY = -3;
    }

    float dist = sqrtf(ballX * ballX + ballY * ballY);
    float eyeX = 0, eyeY = 0;
    if (dist > 0) {
        float eyeX = ballX / dist;
        float eyeY = ballY / dist;
    }

    // Draw white of the eye
    DrawCircle(
        to_x_pixel(eye_base_x),
        to_y_pixel(eye_base_y),
        to_p(r) * 0.3f,
        WHITE
    );

    // Draw pupil
    DrawCircle(
        to_x_pixel(eye_base_x + eyeX * 0.15f * r),
        to_y_pixel(eye_base_y + eyeY * 0.15f * r),
        to_p(r) * 0.1f,
        BLACK
    );

    // Draw coins (lives) left
    for (int i = 1; i < agent->lives; i++) {
        DrawCircle(
            to_x_pixel(agent->dir * (REF_W / 2 + 0.5f - i * 2.0f)),
            WINDOW_HEIGHT - to_y_pixel(1.5f),
            to_p(0.5f),
            COIN_COLOR
        );
    }
}

void agent_set_action(Agent* agent, float* action){
    bool forward = false;
    bool backward = false;
    bool jump = false;
    if (action[0] > 0){
        forward = true;
    }
    if (action[1] > 0){
        backward = true;
    }
    if (action[2] > 0){
        jump = true;
    }
    agent->desired_vx = 0;
    agent->desired_vy = 0;
    if (forward && !backward){
        agent->desired_vx = -PLAYER_SPEED_X;
    }
    if (backward && !forward){
        agent->desired_vx = PLAYER_SPEED_X;
    }
    if (jump){
        agent->desired_vy = PLAYER_SPEED_Y;
    }
}

void agent_move(Agent* agent){
    agent->x += agent->vx * TIMESTEP;
    agent->y += agent->vy * TIMESTEP;
}

void agent_update(Agent* agent){
    agent->vy += GRAVITY * TIMESTEP;
    if (agent->y <= REF_U + NUDGE*TIMESTEP){ // if grounded
        agent->vy = agent->desired_vy;
    }
    agent->vx = agent->desired_vx*agent->dir;
    agent_move(agent);
    if (agent->y <= REF_U){
        agent->y = REF_U;
        agent->vy = 0;
    }
    // stay in their own half:
    if (agent->x*agent->dir <= (REF_WALL_WIDTH/2+agent->r)){
        agent->vx = 0;
        agent->x = agent->dir*(REF_WALL_WIDTH/2+agent->r);
    }
    if (agent->x*agent->dir >= (REF_W/2-agent->r)){
        agent->vx = 0;
        agent->x = agent->dir*(REF_W/2-agent->r);
    }
}

void agent_update_state(Agent* agent, Ball* ball, Agent* opponent){
    int obs_idx = 0;
    float* observations = agent->observations;
    
    // self
    observations[obs_idx++] = agent->x*agent->dir / 10.0f;
    observations[obs_idx++] = agent->y / 10.0f;  
    observations[obs_idx++] = agent->vx*agent->dir / 10.0f;
    observations[obs_idx++] = agent->vy / 10.0f;
    // ball
    observations[obs_idx++] = ball->x*agent->dir / 10.0f;
    observations[obs_idx++] = ball->y / 10.0f;
    observations[obs_idx++] = ball->vx*agent->dir / 10.0f;
    observations[obs_idx++] = ball->vy / 10.0f;
    // opponent
    observations[obs_idx++] = opponent->x*(-agent->dir) / 10.0f; // negate direction for opponent
    observations[obs_idx++] = opponent->y / 10.0f;
    observations[obs_idx++] = opponent->vx*(-agent->dir) / 10.0f ; // negate direction for opponent
    observations[obs_idx++] = opponent->vy / 10.0f;
}

// ENV

// Required struct. Only use floats!
typedef struct {
    float perf; // Recommended 0-1 normalized single real number perf metric
    float score; // Recommended unnormalized single real number perf metric
    float episode_return; // Recommended metric: sum of agent rewards over episode
    float episode_length; // Recommended metric: number of steps of agent episode
    // Any extra fields you add here may be exported to Python in binding.c
    float n; // Required as the last field 
} Log;

typedef struct {
    Log log; // Required field. Env binding code uses this to aggregate logs
    Agent* agents;
    Wall* ground;
    Wall* fence;
    Ball* fence_stub;
    Ball* ball;
    int delay_frames; // frames to wait before starting
    float* observations; // Required. You can use any obs type, but make sure it matches in Python!
    float* actions; // Required. int* for discrete/multidiscrete, float* for box
    float* rewards; // Required
    unsigned char* terminals; // Required. We don't yet have truncations as standard yet
    int num_agents; // Number of agents being trained. Either 1 or 2. If 1, the first agent is trained and the second is a bot.
    float* bot_observations; // Optional, for bot control
    float* bot_actions; // Optional, for bot control
    int tick;
} SlimeVolley;


/* Recommended to have an init function of some kind if you allocate 
* extra memory. This should be freed by c_close. Don't forget to call
* this in binding.c!
*/
void init(SlimeVolley* env) {
    env->ground = malloc(sizeof(Wall));
    *env->ground = (Wall){ .x = 0, .y = REF_U / 2.0, .w = REF_W, .h = REF_U, .c = GROUND_COLOR };
    env->fence = malloc(sizeof(Wall));
    *env->fence = (Wall){ .x = 0, .y = ( REF_U + REF_WALL_HEIGHT)/2.0, .w = REF_WALL_WIDTH, .h = REF_WALL_HEIGHT - 1.5, .c = FENCE_COLOR };
    env->fence_stub = malloc(sizeof(Ball));
    *env->fence_stub = (Ball){ .x = 0, .y = REF_WALL_HEIGHT, .vx=0, .vy=0, .r=REF_WALL_WIDTH/2.0, .c=FENCE_COLOR};
    env->agents = calloc(2, sizeof(Agent));
    env->ball = malloc(sizeof(Ball));
    if (env->num_agents == 1) {
        env->bot_observations = calloc(12, sizeof(float));
        env->bot_actions = calloc(3, sizeof(float));
    }
}

// Required function
void c_reset(SlimeVolley* env) {
    env->tick = 0;
    env->delay_frames = INIT_DELAY_FRAMES;
    float ball_vx = ((float) rand() / RAND_MAX)*40.0f - 20.0f;
    float ball_vy = ((float) rand() / RAND_MAX)*15.0f + 10.0f;
    *env->ball = (Ball){
        .x = 0,
        .y = REF_W/4,
        .vx = ball_vx,
        .vy = ball_vy,
        .r = 0.5,
        .c = BALL_COLOR
    };
    for (int i=0; i < 2; i++) {
        float* observations;
        if (i == 0) {
            observations = env->observations;
        }
        else {
            if (env->num_agents == 1) {
                observations = env->bot_observations;
            }
            else {
                observations = &env->observations[12]; // second agent in two-agent mode
            }
        }
        env->agents[i] = (Agent){
            .dir = i == 0 ? -1 : 1,
            .x = i == 0 ? -REF_W/4 : REF_W/4,
            .y = REF_U,
            .c = i == 0 ? AGENT_LEFT_COLOR : AGENT_RIGHT_COLOR,
            .r = 1.5,
            .lives = MAXLIVES,
            .observations = observations
        };
    }
    agent_update_state(&env->agents[0], env->ball, &env->agents[1]);
    agent_update_state(&env->agents[1], env->ball, &env->agents[0]);
}

float clip(float val, float min, float max) {
    if (val < min) {
        return min;
    } else if (val > max) {
        return max;
    }
    return val;
}

void new_match(SlimeVolley* env) {
    float ball_vx = ((float) rand() / RAND_MAX)*40.0f - 20.0f;
    float ball_vy = ((float) rand() / RAND_MAX)*15.0f + 10.0f;
    *env->ball = (Ball){
        .x = 0,
        .y = REF_W/4,
        .vx = ball_vx,
        .vy = ball_vy,
        .r = 0.5,
        .c = BALL_COLOR
    };
    env->delay_frames = INIT_DELAY_FRAMES;
}

void abranti_simple_bot(float* obs, float* action) {
    // the bot policy. just 7 params but hard to beat.
    float x_agent = obs[0];
    float x_ball = obs[4];
    float vx_ball = obs[6];
    float backward = (-23.757145f * x_agent + 23.206863f * x_ball + 0.7943352f * vx_ball) + 1.4617119f;
    float forward = -64.6463748f * backward + 22.4668393f;
    action[0] = forward;
    action[1] = backward;
    action[2] = 1.0f; // always jump
}

// Required function
void c_step(SlimeVolley* env) {
    env->rewards[0] = 0;
    env->terminals[0] = 0;
    if (env->num_agents == 2){
        env->rewards[1] = 0;
        env->terminals[1] = 0;
    }
    
    Agent* left = &env->agents[0];
    Agent* right = &env->agents[1];
    Ball* ball = env->ball;

    env->tick++;
    agent_set_action(left, &env->actions[0]);
    if (env->num_agents == 1){
        abranti_simple_bot(right->observations, env->bot_actions);
        agent_set_action(right, env->bot_actions);
    }
    else {
        agent_set_action(right, &env->actions[3]);
    }

    // Update
    agent_update(left);
    agent_update(right);

    if (env->delay_frames == 0) {
        ball_accelerate(ball, 0, GRAVITY);
        ball_limit_speed(ball, 0, MAX_BALL_SPEED);
        ball_move(ball);
    }
    else {
        env->delay_frames--;
    }

    if (ball_is_colliding(ball, (SphericalObject*)left)){
        ball_bounce(ball, (SphericalObject*)left);
    }
    if (ball_is_colliding(ball, (SphericalObject*)right)){
        ball_bounce(ball, (SphericalObject*)right);
    }
    if (ball_is_colliding(ball, (SphericalObject*)env->fence_stub)){
        ball_bounce(ball, (SphericalObject*)env->fence_stub);
    }

    int right_reward = -ball_check_edges(ball);

    if (right_reward != 0){
        new_match(env);
        if (right_reward == -1){
            right->lives--;
            env->rewards[0] = 1.0f;
            if (env->num_agents == 2){
                env->rewards[1] = -1.0f;
            }
        }
        else{
            left->lives--;
            env->rewards[0] = -1.0f;
            if (env->num_agents == 2){
                env->rewards[1] = 1.0f;
            }
        }
    }
    agent_update_state(left, ball, right);
    agent_update_state(right, ball, left);

    if (env->tick > MAX_TICKS || left->lives <= 0 || right->lives <= 0){
        env->terminals[0] = 1;
        if (env->num_agents == 2){
            env->terminals[1] = 1;
        }
        env->log.perf = (left->lives - right->lives + 5.0f)  / 10.0f; // normalize to 0-1
        env->log.score = (float)(left->lives - right->lives);
        env->log.episode_return = (5.0f - right->lives);
        env->log.episode_length = (float)env->tick;
        env->log.n++;
        c_reset(env);
    }
    
}

// Required function. Should handle creating the client on first call
void c_render(SlimeVolley* env) {
    if (!IsWindowReady()) {
        InitWindow(WINDOW_WIDTH, WINDOW_HEIGHT, "PufferLib SlimeVolley");
        SetTargetFPS(60);
    }

    // Standard across our envs so exiting is always the same
    if (IsKeyDown(KEY_ESCAPE)) {
        exit(0);
    }
    BeginDrawing();
    ClearBackground(BACKGROUND_COLOR);
    wall_display(env->ground);
    wall_display(env->fence);
    ball_display(env->fence_stub);
    ball_display(env->ball);

    for (int i=0; i<2; i++) {
        agent_display(&env->agents[i], env->ball->x, env->ball->y);
    }

    EndDrawing();
}

// Required function. Should clean up anything you allocated
// Do not free env->observations, actions, rewards, terminals
void c_close(SlimeVolley* env) {
    free(env->agents);
    free(env->ground);
    free(env->fence);
    free(env->fence_stub);
    free(env->ball);
    if (env->num_agents == 1) {
        free(env->bot_observations);
        free(env->bot_actions);
    }
    if (IsWindowReady()) {
        CloseWindow();
    }
}



================================================
FILE: pufferlib/ocean/slimevolley/slimevolley.py
================================================
'''A simple sample environment. Use this as a template for your own envs.'''

import gymnasium
import numpy as np

from pufferlib.ocean.slimevolley import binding
import pufferlib

class SlimeVolley(pufferlib.PufferEnv):
    def __init__(self, num_envs=1, render_mode=None, log_interval=128, buf=None, seed=0,
                 num_agents=1):
        assert num_agents in {1, 2}, "num_agents must be 1 or 2"
        num_obs = 12
        self.single_observation_space = gymnasium.spaces.Box(low=0, high=1,
            shape=(num_obs,), dtype=np.float32)
        self.single_action_space = gymnasium.spaces.MultiDiscrete([2, 2, 2])

        self.render_mode = render_mode
        self.num_agents = num_envs * num_agents
        self.log_interval = log_interval

        super().__init__(buf)
        c_envs = []
        for i in range(num_envs):
            c_env = binding.env_init(
                self.observations[i*num_agents:(i+1)*num_agents],
                self.actions[i*num_agents:(i+1)*num_agents],
                self.rewards[i*num_agents:(i+1)*num_agents],
                self.terminals[i*num_agents:(i+1)*num_agents],
                self.truncations[i*num_agents:(i+1)*num_agents],
                seed,
                num_agents=num_agents
                )
            c_envs.append(c_env)

        self.c_envs = binding.vectorize(*c_envs)

    def reset(self, seed=0):
        binding.vec_reset(self.c_envs, seed)
        self.tick = 0
        return self.observations, []

    def step(self, actions):
        self.tick += 1
        self.actions[:] = actions
        binding.vec_step(self.c_envs)

        info = []
        if self.tick % self.log_interval == 0:
            log = binding.vec_log(self.c_envs)
            if log:
                info.append(log)

        return (self.observations, self.rewards,
            self.terminals, self.truncations, info)

    def render(self):
        binding.vec_render(self.c_envs, 0)

    def close(self):
        binding.vec_close(self.c_envs)

if __name__ == '__main__':
    N = 8

    env = SlimeVolley(num_envs=N, num_agents=2)
    env.reset()
    steps = 0

    CACHE = 1024
    actions = np.random.randint(env.single_action_space.nvec, size=(CACHE, N*2, 3))

    i = 0
    import time
    start = time.time()
    while time.time() - start < 10:
        env.step(actions[i % CACHE])
        steps += env.num_agents
        i += 1

    print('SlimeVolley SPS:', int(steps / (time.time() - start)))


================================================
FILE: pufferlib/ocean/snake/README.md
================================================
# PufferLib Multi-Snake

This is a simple multi-agent snake environment runnable with any number of snakes, board size, food, etc. I originally implemented this to demonstrate how simple it is to implement ultra high performance environments that run at millions of steps per second. The exact same approaches you see here are used in all of my more complex simulators.

# Cython version

The Cython version is the original. It runs over 10M steps/second/core on a high-end CPU. This is the version that we currently have bound to training. You can use it with the PufferLib demo script (--env snake) or import it from pufferlib/environments/ocean. There are a number of default board sizes and settings. If you would like to contribute games to PufferLib, you can use this project as a template. There is a bit of bloat in the .py file because we have to trick PufferLib's vectorization into thinking this is a vecenv. In the future, there will be a more standard advanced API.

Key concepts:
- Memory views: Cython provides a way to access numpy arrays as C arrays or structs. This gives you C-speed numpy indexing and prevents you from having to copy data around. When running with multiprocessing, the observation buffers are stored in shared memory, so you are literally simulating into the experience buffer.
- No memory management: All data is allocated by Numpy and passed to C. This is fast and also prevents any chance of leaks
- No python callbacks: Compile and optimize with annotations enabled (see setup.py) to ensure that the Cython code never calls back to Python. You should be able to get >>1M agent steps/second for almost any sim

# C version

The C version is a direct port of the Cython version, plus a few minor tweaks. It includes a pure C raylib client and a pure C MLP forward pass for running local inference. I made this so that we could run a cool demo in the browser 100% client side. I may port additional simulators in the future, and you are welcome to contribute C code to PufferLib, but this is not required. You can make things plenty fast in Cython. To build this locally, all you need is the raylib source. If you want to build for web, follow RayLib's emscripten setup.




================================================
FILE: pufferlib/ocean/snake/__init__.py
================================================
[Empty file]


================================================
FILE: pufferlib/ocean/snake/binding.c
================================================
#include "snake.h"

#define Env CSnake
#include "../env_binding.h"

static int my_init(Env* env, PyObject* args, PyObject* kwargs) {   
    env->width = unpack(kwargs, "width");
    env->height = unpack(kwargs, "height");
    env->num_snakes = unpack(kwargs, "num_snakes");
    env->vision = unpack(kwargs, "vision");
    env->leave_corpse_on_death = unpack(kwargs, "leave_corpse_on_death");
    env->food = unpack(kwargs, "num_food");
    env->reward_food = unpack(kwargs, "reward_food");
    env->reward_corpse = unpack(kwargs, "reward_corpse");
    env->reward_death = unpack(kwargs, "reward_death");
    env->max_snake_length = unpack(kwargs, "max_snake_length");
    env->cell_size = unpack(kwargs, "cell_size");    
    init_csnake(env);
    return 0;
}

static int my_log(PyObject* dict, Log* log) {
    assign_to_dict(dict, "perf", log->perf);
    assign_to_dict(dict, "score", log->score);
    assign_to_dict(dict, "episode_return", log->episode_return);
    assign_to_dict(dict, "episode_length", log->episode_length);
    assign_to_dict(dict, "n", log->n);
    return 0;
}



================================================
FILE: pufferlib/ocean/snake/snake.c
================================================
#include <time.h>
#include "snake.h"
#include "puffernet.h"

int demo() {
    CSnake env = {
        .num_snakes = 256,
        .width = 640,
        .height = 360,
        .max_snake_length = 200,
        .food = 4096,
        .vision = 5,
        .leave_corpse_on_death = true,
        .reward_food = 1.0f,
        .reward_corpse = 0.5f,
        .reward_death = -1.0f,
    };
    allocate_csnake(&env);
    c_reset(&env);

    Weights* weights = load_weights("resources/snake/snake_weights.bin", 256773);
    int logit_sizes[] = {4};
    LinearLSTM* net = make_linearlstm(weights, env.num_snakes, 968, logit_sizes, 1);
    env.client = make_client(2, env.width, env.height);

    while (!WindowShouldClose()) {
        // User can take control of the first snake
        if (IsKeyDown(KEY_LEFT_SHIFT)) {
            if (IsKeyDown(KEY_UP)    || IsKeyDown(KEY_W)) env.actions[0] = 0;
            if (IsKeyDown(KEY_DOWN)  || IsKeyDown(KEY_S)) env.actions[0] = 1;
            if (IsKeyDown(KEY_LEFT)  || IsKeyDown(KEY_A)) env.actions[0] = 2;
            if (IsKeyDown(KEY_RIGHT) || IsKeyDown(KEY_D)) env.actions[0] = 3;
        } else {
            memset(net->obs, 0, env.num_snakes*968*sizeof(float));
            for (int i = 0; i < env.num_snakes*121; i++) {
                int obs = env.observations[i];
                net->obs[i*8 + obs] = 1.0f;
            }
            forward_linearlstm(net, net->obs, env.actions);
        }
        c_step(&env);
        c_render(&env);
    }
    free_linearlstm(net);
    free(weights);
    close_client(env.client);
    free_csnake(&env);
    return 0;
}

void test_performance(float test_time) {
    CSnake env = {
        .num_snakes = 1024,
        .width = 1280,
        .height = 720,
        .max_snake_length = 200,
        .food = 16384,
        .vision = 5,
        .leave_corpse_on_death = true,
        .reward_food = 1.0f,
        .reward_corpse = 0.5f,
        .reward_death = -1.0f,
    };
    allocate_csnake(&env);
    c_reset(&env);

    int start = time(NULL);
    int i = 0;
    while (time(NULL) - start < test_time) {
        for (int j = 0; j < env.num_snakes; j++) {
            env.actions[j] = rand()%4;
        }
        c_step(&env);
        i++;
    }
    int end = time(NULL);
    free_csnake(&env);
    printf("SPS: %f\n", (float)env.num_snakes*i / (end - start));
}

int main() {
    demo();
    // test_performance(30);
    return 0;
}



================================================
FILE: pufferlib/ocean/snake/snake.h
================================================
#include <stdio.h>
#include <stdlib.h>
#include <stdbool.h>
#include <math.h>
#include "raylib.h"

#define EMPTY 0
#define FOOD 1
#define CORPSE 2
#define WALL 3

typedef struct Log Log;
struct Log {
    float perf;
    float score;
    float episode_return;
    float episode_length;
    float n;
};

typedef struct Client Client;
typedef struct CSnake CSnake;
struct CSnake {
    char* observations;
    int* actions;
    float* rewards;
    unsigned char* terminals;
    Log log;
    Log* snake_logs;
    char* grid;
    int* snake;
    int* snake_lengths;
    int* snake_ptr;
    int* snake_lifetimes;
    int* snake_colors;
    int num_snakes;
    int width;
    int height;
    int max_snake_length;
    int food;
    int vision;
    int window;
    int obs_size;
    unsigned char leave_corpse_on_death;
    float reward_food;
    float reward_corpse;
    float reward_death;
    int tick;
    int cell_size;
    Client* client;
};

/**
 * Add a snake's log to the main log when the snake's episode ends (dies or hits a wall).
 * This should only be called during termination/truncation conditions for a specific snake.
 * Accumulates the snake's stats into the main log and resets the snake's individual log.
 */
void add_log(CSnake* env, int snake_id) {
    env->log.perf += env->snake_logs[snake_id].perf;
    env->log.score += env->snake_logs[snake_id].score;
    env->log.episode_return += env->snake_logs[snake_id].episode_return;
    env->log.episode_length += env->snake_logs[snake_id].episode_length;
    env->log.n += 1;
}

void init_csnake(CSnake* env) {
    env->grid = (char*)calloc(env->width*env->height, sizeof(char));
    env->snake = (int*)calloc(env->num_snakes*2*env->max_snake_length, sizeof(int));
    env->snake_lengths = (int*)calloc(env->num_snakes, sizeof(int));
    env->snake_ptr = (int*)calloc(env->num_snakes, sizeof(int));
    env->snake_lifetimes = (int*)calloc(env->num_snakes, sizeof(int));
    env->snake_colors = (int*)calloc(env->num_snakes, sizeof(int));
    env->snake_logs = (Log*)calloc(env->num_snakes, sizeof(Log));
    env->tick = 0;
    env->client = NULL;
    env->snake_colors[0] = 7;
    for (int i = 1; i<env->num_snakes; i++)
        env->snake_colors[i] = i%4 + 4; // Randomize snake colors
}

void c_close(CSnake* env) {
    free(env->grid);
    free(env->snake);
    free(env->snake_lengths);
    free(env->snake_ptr);
    free(env->snake_lifetimes);
    free(env->snake_colors);
    free(env->snake_logs);
}
void allocate_csnake(CSnake* env) {
    int obs_size = (2*env->vision + 1) * (2*env->vision + 1);
    env->observations = (char*)calloc(env->num_snakes*obs_size, sizeof(char));
    env->actions = (int*)calloc(env->num_snakes, sizeof(int));
    env->rewards = (float*)calloc(env->num_snakes, sizeof(float));    
    init_csnake(env);
}

void free_csnake(CSnake* env) {
    c_close(env);
    free(env->observations);
    free(env->actions);
    free(env->rewards);
}

void compute_observations(CSnake* env) {
    for (int i = 0; i < env->num_snakes; i++) {
        int head_ptr = i*2*env->max_snake_length + 2*env->snake_ptr[i];
        int r_offset = env->snake[head_ptr] - env->vision;
        int c_offset = env->snake[head_ptr+1] - env->vision;
        for (int r = 0; r < 2 * env->vision + 1; r++) {
            for (int c = 0; c < 2 * env->vision + 1; c++) {
                env->observations[i*env->obs_size + r*env->window + c] = env->grid[
                    (r_offset + r)*env->width + c_offset + c];
            }
        }
    }
}

void delete_snake(CSnake* env, int snake_id) {
    while (env->snake_lengths[snake_id] > 0) {
        int head_ptr = env->snake_ptr[snake_id];
        int head_offset = 2*env->max_snake_length*snake_id + 2*head_ptr;
        int head_r = env->snake[head_offset];
        int head_c = env->snake[head_offset + 1];
        if (env->leave_corpse_on_death && env->snake_lengths[snake_id] % 2 == 0)
            env->grid[head_r*env->width + head_c] = CORPSE;
        else
            env->grid[head_r*env->width + head_c] = EMPTY;

        env->snake[head_offset] = -1;
        env->snake[head_offset + 1] = -1;
        env->snake_lengths[snake_id]--;
        if (head_ptr == 0)
            env->snake_ptr[snake_id] = env->max_snake_length - 1;
        else
            env->snake_ptr[snake_id]--;
    }
}

void spawn_snake(CSnake* env, int snake_id) {
    int head_r, head_c, tile, grid_idx;
    delete_snake(env, snake_id);
    do {
        head_r = rand() % (env->height - 1);
        head_c = rand() % (env->width - 1);
        grid_idx = head_r*env->width + head_c;
        tile = env->grid[grid_idx];
    } while (tile != EMPTY && tile != CORPSE);
    int snake_offset = 2*env->max_snake_length*snake_id;
    env->snake[snake_offset] = head_r;
    env->snake[snake_offset + 1] = head_c;
    env->snake_lengths[snake_id] = 1;
    env->snake_ptr[snake_id] = 0;
    env->snake_lifetimes[snake_id] = 0;
    env->grid[grid_idx] = env->snake_colors[snake_id];
    env->snake_logs[snake_id] = (Log){0};
}

void spawn_food(CSnake* env) {
    int idx, tile;
    do {
        int r = rand() % (env->height - 1);
        int c = rand() % (env->width - 1);
        idx = r*env->width + c;
        tile = env->grid[idx];
    } while (tile != EMPTY && tile != CORPSE);
    env->grid[idx] = FOOD;
}

void c_reset(CSnake* env) {
    env->window = 2*env->vision+1;
    env->obs_size = env->window*env->window;
    env->tick = 0;
    env->log = (Log){0};
    
    for (int i = 0; i < env->num_snakes; i++)
        env->snake_logs[i] = (Log){0};

    for (int r = 0; r < env->vision; r++) {
        for (int c = 0; c < env->width; c++)
            env->grid[r*env->width + c] = WALL;
    }
    for (int r = env->height - env->vision; r < env->height; r++) {
        for (int c = 0; c < env->width; c++)
            env->grid[r*env->width + c] = WALL;
    }
    for (int r = 0; r < env->height; r++) {
        for (int c = 0; c < env->vision; c++)
            env->grid[r*env->width + c] = WALL;
        for (int c = env->width - env->vision; c < env->width; c++)
            env->grid[r*env->width + c] = WALL;
    }
    for (int i = 0; i < env->num_snakes; i++)
        spawn_snake(env, i);
    for (int i = 0; i < env->food; i++)
        spawn_food(env);

    compute_observations(env);
}

void step_snake(CSnake* env, int i) {
    env->snake_logs[i].episode_length += 1;
    int atn = env->actions[i];
    int dr = 0;
    int dc = 0;
    switch (atn) {
        case 0: dr = -1; break; // up
        case 1: dr = 1; break;  // down
        case 2: dc = -1; break; // left
        case 3: dc = 1; break;  // right
    }

    int head_ptr = env->snake_ptr[i];
    int snake_offset = 2*env->max_snake_length*i;
    int head_offset = snake_offset + 2*head_ptr;
    int next_r = dr + env->snake[head_offset];
    int next_c = dc + env->snake[head_offset + 1];

    // Disallow moving into own neck
    int prev_head_offset = head_offset - 2;
    if (prev_head_offset < 0)
        prev_head_offset += 2*env->max_snake_length;
    int prev_r = env->snake[prev_head_offset];
    int prev_c = env->snake[prev_head_offset + 1];
    if (prev_r == next_r && prev_c == next_c) {
        next_r = env->snake[head_offset] - dr;
        next_c = env->snake[head_offset + 1] - dc;
    }

    int tile = env->grid[next_r*env->width + next_c];
    if (tile >= WALL) {
        env->rewards[i] = env->reward_death;
        env->snake_logs[i].episode_return += env->reward_death;
        env->snake_logs[i].score = env->snake_lengths[i];
        env->snake_logs[i].perf = env->snake_logs[i].score / env->snake_logs[i].episode_length;
        add_log(env, i);
        spawn_snake(env, i);
        return;
    }

    head_ptr++; // Circular buffer
    if (head_ptr >= env->max_snake_length)
        head_ptr = 0;
    head_offset = snake_offset + 2*head_ptr;
    env->snake[head_offset] = next_r;
    env->snake[head_offset + 1] = next_c;
    env->snake_ptr[i] = head_ptr;
    env->snake_lifetimes[i]++;

    bool grow;
    if (tile == FOOD) {
        env->rewards[i] = env->reward_food;
        env->snake_logs[i].episode_return += env->reward_food;
        spawn_food(env);
        grow = true;
    } else if (tile == CORPSE) {
        env->rewards[i] = env->reward_corpse;
        env->snake_logs[i].episode_return += env->reward_corpse;
        grow = true;
    } else {
        env->rewards[i] = 0.0;
        grow = false;
    }

    int snake_length = env->snake_lengths[i];
    if (grow && snake_length < env->max_snake_length - 1) {
        env->snake_lengths[i]++;
    } else {
        int tail_ptr = head_ptr - snake_length;
        if (tail_ptr < 0) // Circular buffer
            tail_ptr = env->max_snake_length + tail_ptr;
        int tail_r = env->snake[snake_offset + 2*tail_ptr];
        int tail_c = env->snake[snake_offset + 2*tail_ptr + 1];
        int tail_offset = 2*env->max_snake_length*i + 2*tail_ptr;
        env->snake[tail_offset] = -1;
        env->snake[tail_offset + 1] = -1;
        env->grid[tail_r*env->width + tail_c] = EMPTY;
    }
    env->grid[next_r*env->width + next_c] = env->snake_colors[i];
}

void c_step(CSnake* env){
    env->tick++;
    for (int i = 0; i < env->num_snakes; i++)
        step_snake(env, i);

    compute_observations(env);
}

// Raylib client
Color COLORS[] = {
    (Color){6, 24, 24, 255},
    (Color){0, 0, 255, 255},
    (Color){0, 128, 255, 255},
    (Color){128, 128, 128, 255},
    (Color){255, 0, 0, 255},
    (Color){255, 255, 255, 255},
    (Color){255, 85, 85, 255},
    (Color){170, 170, 170, 255},
    (Color){0, 255, 255, 255},
    (Color){255, 255, 0, 255},
};

typedef struct Client Client;
struct Client {
    int cell_size;
    int width;
    int height;
};

Client* make_client(int cell_size, int width, int height) {
    Client* client= (Client*)malloc(sizeof(Client));
    client->cell_size = cell_size;
    client->width = width;
    client->height = height;
    InitWindow(width*cell_size, height*cell_size, "PufferLib Snake");
    SetTargetFPS(10);
    return client;
}

void close_client(Client* client) {
    CloseWindow();
    free(client);
}

void c_render(CSnake* env) {
    if (IsKeyDown(KEY_ESCAPE)) {
        exit(0);
    }
    
    if (env->client == NULL) {
        env->client = make_client(env->cell_size, env->width, env->height);
    }
    
    Client* client = env->client;
    
    BeginDrawing();
    ClearBackground(COLORS[0]);
    int sz = client->cell_size;
    for (int y = 0; y < env->height; y++) {
        for (int x = 0; x < env->width; x++){
            int tile = env->grid[y*env->width + x];
            if (tile != EMPTY)
                DrawRectangle(x*sz, y*sz, sz, sz, COLORS[tile]);
        }
    }
    EndDrawing();
}



================================================
FILE: pufferlib/ocean/snake/snake.py
================================================
'''High-perf many-agent snake. Inspired by snake env from https://github.com/dnbt777'''

import numpy as np
import gymnasium

import pufferlib
from pufferlib import APIUsageError
from pufferlib.ocean.snake import binding

class Snake(pufferlib.PufferEnv):
    def __init__(self, num_envs=16, width=640, height=360,
            num_snakes=256, num_food=4096,
            vision=5, leave_corpse_on_death=True,
            reward_food=0.1, reward_corpse=0.1, reward_death=-1.0,
            report_interval=128, max_snake_length=1024,
            render_mode='human', buf=None, seed=0):
        
        if num_envs is not None:
            num_snakes = num_envs * [num_snakes]
            width = num_envs * [width]
            height = num_envs * [height]
            num_food = num_envs * [num_food]
            leave_corpse_on_death = num_envs * [leave_corpse_on_death]

        if not (len(num_snakes) == len(width) == len(height) == len(num_food)):
            raise APIUsageError('num_snakes, width, height, num_food must be lists of equal length')

        for w, h in zip(width, height):
            if w < 2*vision+2 or h < 2*vision+2:
                raise APIUsageError('width and height must be at least 2*vision+2')

        max_area = max([w*h for h, w in zip(height, width)])
        self.max_snake_length = min(max_snake_length, max_area)
        self.report_interval = report_interval

        self.single_observation_space = gymnasium.spaces.Box(
            low=0, high=2, shape=(2*vision+1, 2*vision+1), dtype=np.int8)
        self.single_action_space = gymnasium.spaces.Discrete(4)
        self.num_agents = sum(num_snakes)
        self.render_mode = render_mode
        self.tick = 0

        self.cell_size = int(np.ceil(1280 / max(max(width), max(height))))

        super().__init__(buf)
        c_envs = []
        offset = 0
        for i in range(num_envs):
            ns = num_snakes[i]
            obs_slice = self.observations[offset:offset+ns]
            act_slice = self.actions[offset:offset+ns]
            rew_slice = self.rewards[offset:offset+ns]
            term_slice = self.terminals[offset:offset+ns]
            trunc_slice = self.truncations[offset:offset+ns]
            # Seed each env uniquely: i + seed * num_envs
            env_seed = i + seed * num_envs
            env_id = binding.env_init(
                obs_slice, 
                act_slice, 
                rew_slice, 
                term_slice, 
                trunc_slice,
                env_seed,
                width=width[i], 
                height=height[i],
                num_snakes=ns, 
                num_food=num_food[i],
                vision=vision, 
                leave_corpse_on_death=leave_corpse_on_death[i],
                reward_food=reward_food, 
                reward_corpse=reward_corpse,
                reward_death=reward_death, 
                max_snake_length=self.max_snake_length,
                cell_size=self.cell_size
            )
            c_envs.append(env_id)
            offset += ns
        self.c_envs = binding.vectorize(*c_envs)
 
    def reset(self, seed=None):
        self.tick = 0
        if seed is None:
            binding.vec_reset(self.c_envs, 0)
        else:
            binding.vec_reset(self.c_envs, seed)
        return self.observations, []

    def step(self, actions):
        self.actions[:] = actions
        self.tick += 1
        binding.vec_step(self.c_envs)

        info = []
        if self.tick % self.report_interval == 0:
            info.append(binding.vec_log(self.c_envs))

        return (self.observations, self.rewards,
            self.terminals, self.truncations, info)

    def render(self):
        binding.vec_render(self.c_envs, self.cell_size)

    def close(self):
        binding.vec_close(self.c_envs)

def test_performance(timeout=10, atn_cache=1024):
    env = Snake()
    env.reset()
    tick = 0

    total_snakes = env.num_agents
    actions = np.random.randint(0, 4, (atn_cache, total_snakes))

    import time
    start = time.time()
    while time.time() - start < timeout:
        atns = actions[tick % atn_cache]
        env.step(atns)
        tick += 1

    print(f'SPS: %f', total_snakes * tick / (time.time() - start))

if __name__ == '__main__':
    test_performance()



================================================
FILE: pufferlib/ocean/squared/binding.c
================================================
#include "squared.h"

#define Env Squared
#include "../env_binding.h"

static int my_init(Env* env, PyObject* args, PyObject* kwargs) {
    env->size = unpack(kwargs, "size");
    return 0;
}

static int my_log(PyObject* dict, Log* log) {
    assign_to_dict(dict, "perf", log->perf);
    assign_to_dict(dict, "score", log->score);
    assign_to_dict(dict, "episode_return", log->episode_return);
    assign_to_dict(dict, "episode_length", log->episode_length);
    return 0;
}



================================================
FILE: pufferlib/ocean/squared/squared.c
================================================
/* Pure C demo file for Squared. Build it with:
 * bash scripts/build_ocean.sh target local (debug)
 * bash scripts/build_ocean.sh target fast
 * We suggest building and debugging your env in pure C first. You
 * get faster builds and better error messages. To keep this example
 * simple, it does not include C neural nets. See Target for that.
 */

#include "squared.h"

int main() {
    Squared env = {.size = 11};
    env.observations = (unsigned char*)calloc(env.size*env.size, sizeof(unsigned char));
    env.actions = (int*)calloc(1, sizeof(int));
    env.rewards = (float*)calloc(1, sizeof(float));
    env.terminals = (unsigned char*)calloc(1, sizeof(unsigned char));

    c_reset(&env);
    c_render(&env);
    while (!WindowShouldClose()) {
        if (IsKeyDown(KEY_LEFT_SHIFT)) {
            env.actions[0] = 0;
            if (IsKeyDown(KEY_UP)    || IsKeyDown(KEY_W)) env.actions[0] = UP;
            if (IsKeyDown(KEY_DOWN)  || IsKeyDown(KEY_S)) env.actions[0] = DOWN;
            if (IsKeyDown(KEY_LEFT)  || IsKeyDown(KEY_A)) env.actions[0] = LEFT;
            if (IsKeyDown(KEY_RIGHT) || IsKeyDown(KEY_D)) env.actions[0] = RIGHT;
        } else {
            env.actions[0] = rand() % 5;
        }
        c_step(&env);
        c_render(&env);
    }
    free(env.observations);
    free(env.actions);
    free(env.rewards);
    free(env.terminals);
    c_close(&env);
}




================================================
FILE: pufferlib/ocean/squared/squared.h
================================================
/* Squared: a sample single-agent grid env.
 * Use this as a tutorial and template for your first env.
 * See the Target env for a slightly more complex example.
 * Star PufferLib on GitHub to support. It really, really helps!
 */

#include <stdlib.h>
#include <string.h>
#include "raylib.h"

const unsigned char NOOP = 0;
const unsigned char DOWN = 1;
const unsigned char UP = 2;
const unsigned char LEFT = 3;
const unsigned char RIGHT = 4;

const unsigned char EMPTY = 0;
const unsigned char AGENT = 1;
const unsigned char TARGET = 2;

// Required struct. Only use floats!
typedef struct {
    float perf; // Recommended 0-1 normalized single real number perf metric
    float score; // Recommended unnormalized single real number perf metric
    float episode_return; // Recommended metric: sum of agent rewards over episode
    float episode_length; // Recommended metric: number of steps of agent episode
    // Any extra fields you add here may be exported to Python in binding.c
    float n; // Required as the last field 
} Log;

// Required that you have some struct for your env
// Recommended that you name it the same as the env file
typedef struct {
    Log log; // Required field. Env binding code uses this to aggregate logs
    unsigned char* observations; // Required. You can use any obs type, but make sure it matches in Python!
    int* actions; // Required. int* for discrete/multidiscrete, float* for box
    float* rewards; // Required
    unsigned char* terminals; // Required. We don't yet have truncations as standard yet
    int size;
    int tick;
    int r;
    int c;
} Squared;

void add_log(Squared* env) {
    env->log.perf += (env->rewards[0] > 0) ? 1 : 0;
    env->log.score += env->rewards[0];
    env->log.episode_length += env->tick;
    env->log.episode_return += env->rewards[0];
    env->log.n++;
}

// Required function
void c_reset(Squared* env) {
    int tiles = env->size*env->size;
    memset(env->observations, 0, tiles*sizeof(unsigned char));
    env->observations[tiles/2] = AGENT;
    env->r = env->size/2;
    env->c = env->size/2;
    env->tick = 0;
    int target_idx;
    do {
        target_idx = rand() % tiles;
    } while (target_idx == tiles/2);
    env->observations[target_idx] = TARGET;
}

// Required function
void c_step(Squared* env) {
    env->tick += 1;

    int action = env->actions[0];
    env->terminals[0] = 0;
    env->rewards[0] = 0;

    env->observations[env->r*env->size + env->c] = EMPTY;

    if (action == DOWN) {
        env->r += 1;
    } else if (action == RIGHT) {
        env->c += 1;
    } else if (action == UP) {
        env->r -= 1;
    } else if (action == LEFT) {
        env->c -= 1;
    }

    if (env->tick > 3*env->size 
            || env->r < 0
            || env->c < 0
            || env->r >= env->size
            || env->c >= env->size) {
        env->terminals[0] = 1;
        env->rewards[0] = -1.0;
        add_log(env);
        c_reset(env);
        return;
    }

    int pos = env->r*env->size + env->c;
    if (env->observations[pos] == TARGET) {
        env->terminals[0] = 1;
        env->rewards[0] = 1.0;
        add_log(env);
        c_reset(env);
        return;
    }

    env->observations[pos] = AGENT;
}

// Required function. Should handle creating the client on first call
void c_render(Squared* env) {
    if (!IsWindowReady()) {
        InitWindow(64*env->size, 64*env->size, "PufferLib Squared");
        SetTargetFPS(5);
    }

    // Standard across our envs so exiting is always the same
    if (IsKeyDown(KEY_ESCAPE)) {
        exit(0);
    }

    BeginDrawing();
    ClearBackground((Color){6, 24, 24, 255});

    int px = 64;
    for (int i = 0; i < env->size; i++) {
        for (int j = 0; j < env->size; j++) {
            int tex = env->observations[i*env->size + j];
            if (tex == EMPTY) {
                continue;
            }
            Color color = (tex == AGENT) ? (Color){0, 187, 187, 255} : (Color){187, 0, 0, 255};
            DrawRectangle(j*px, i*px, px, px, color);
        }
    }

    EndDrawing();
}

// Required function. Should clean up anything you allocated
// Do not free env->observations, actions, rewards, terminals
void c_close(Squared* env) {
    if (IsWindowReady()) {
        CloseWindow();
    }
}



================================================
FILE: pufferlib/ocean/squared/squared.py
================================================
'''A simple sample environment. Use this as a template for your own envs.'''

import gymnasium
import numpy as np

import pufferlib
from pufferlib.ocean.squared import binding

class Squared(pufferlib.PufferEnv):
    def __init__(self, num_envs=1, render_mode=None, log_interval=128, size=11, buf=None, seed=0):
        self.single_observation_space = gymnasium.spaces.Box(low=0, high=1,
            shape=(size*size,), dtype=np.uint8)
        self.single_action_space = gymnasium.spaces.Discrete(5)
        self.render_mode = render_mode
        self.num_agents = num_envs
        self.log_interval = log_interval

        super().__init__(buf)
        self.c_envs = binding.vec_init(self.observations, self.actions, self.rewards,
            self.terminals, self.truncations, num_envs, seed, size=size)
 
    def reset(self, seed=0):
        binding.vec_reset(self.c_envs, seed)
        self.tick = 0
        return self.observations, []

    def step(self, actions):
        self.tick += 1

        self.actions[:] = actions
        binding.vec_step(self.c_envs)

        info = []
        if self.tick % self.log_interval == 0:
            info.append(binding.vec_log(self.c_envs))

        return (self.observations, self.rewards,
            self.terminals, self.truncations, info)

    def render(self):
        binding.vec_render(self.c_envs, 0)

    def close(self):
        binding.vec_close(self.c_envs)

if __name__ == '__main__':
    N = 4096

    env = Squared(num_envs=N)
    env.reset()
    steps = 0

    CACHE = 1024
    actions = np.random.randint(0, 5, (CACHE, N))

    i = 0
    import time
    start = time.time()
    while time.time() - start < 10:
        env.step(actions[i % CACHE])
        steps += N
        i += 1

    print('Squared SPS:', int(steps / (time.time() - start)))



================================================
FILE: pufferlib/ocean/tactical/__init__.py
================================================
[Empty file]


================================================
FILE: pufferlib/ocean/tactical/binding.c
================================================
#include "tactical.h"
#define Env Tactical
#include "../env_binding.h"

// no init args needed
static int my_init(Env* env, PyObject* args, PyObject* kwargs) {
    return 0;
}

// no logging implemented atm
static int my_log(PyObject* dict, Log* log) {
    return 0;
}



================================================
FILE: pufferlib/ocean/tactical/maps.h
================================================
// - = EMPTY (not walkable, not necessarily rendered)
// . = GROUND (walkable)
// | = HOLE (not walkable, blocks view)
// # = WALL (not walkable)

unsigned int map1_height = 12;
unsigned int map1_width = 12;
char map1[] =
    "............"
    ".#|..#.#|..#"
    "............"
    "..||....||.."
    "|##..#|##..#"
    "............"
    "............"
    ".#|..#.#|..#"
    "............"
    "..||....||.."
    "|##..#|##..#"
    "............";

unsigned int map2_height = 28;
unsigned int map2_width = 29;
char map2[] = 
    "-------------#.--------------"
    "-----------.#..--------------"
    "----------......-------------"
    "---------.|.#|...------------"
    "--------........#.-----------"
    "-------.............---------"
    "------...............--------"
    "------..............#.-------"
    "-----..................------"
    "---.....................-----"
    "--.|.....................----"
    "-..|......................---"
    ".................|#|.......--"
    "-#..............||||.......--"
    "--..............#|#|........."
    "---...............#.........-"
    "----.......................--"
    "-----.....................---"
    "------...................----"
    "-------.................-----"
    "--------...............------"
    "---------.............-------"
    "----------...........--------"
    "-----------.........---------"
    "------------.......----------"
    "-------------.....-----------"
    "--------------|.#------------"
    "---------------.-------------";

unsigned int map3_height = 28;
unsigned int map3_width = 29;
char map3[] = 
    "-------------#.--------------"
    "-----------.#..--------------"
    "----------.......------------"
    "---------.|.#|..#------------"
    "--------........#.-----------"
    "-------.............---------"
    "------...........#...--------"
    "------....||........#.-------"
    "-----..................------"
    "---.............##......-----"
    "--.|......#..............----"
    "-..|.....##...............---"
    "..................#|#......--"
    "-###............|..........--"
    "--..............#|#.........-"
    "---...............#.........-"
    "----......#|...............--"
    "-----.....................---"
    "------...........#|#.....----"
    "-------............#....-----"
    "--------...............------"
    "---------...##|#.......------"
    "----------...........--------"
    "-----------.........---------"
    "------------.......----------"
    "-------------.....-----------"
    "--------------..#------------"
    "---------------.-------------";

char* get_map(int map_id) {
    switch (map_id) {
        case 1: return map1; break;
        case 2: return map2; break;
        case 3: return map3; break;
        default: printf("Invalid map id <%i>\n", map_id); exit(1);
    }
}

unsigned int get_map_height(int map_id) {
    switch (map_id) {
        case 1: return map1_height; break;
        case 2: return map2_height; break;
        case 3: return map3_height; break;
        default: printf("Invalid map id <%i>\n", map_id); exit(1);
    }
}

unsigned int get_map_width(int map_id) {
    switch (map_id) {
        case 1: return map1_width; break;
        case 2: return map2_width; break;
        case 3: return map3_width; break;
        default: printf("Invalid map id <%i>\n", map_id); exit(1);
    }
}



================================================
FILE: pufferlib/ocean/tactical/tactical.c
================================================
#include "tactical.h"


int main() {
    Tactical* env = init_tactical();
    // allocate(&env);
    
    env->client = init_client(env);

    c_reset(env);
    while (!WindowShouldClose()) {
        if (IsKeyPressed(KEY_Q) || IsKeyPressed(KEY_BACKSPACE)) break;
        c_step(env);
        c_render(env);
    }

    close_client(env->client);
    free_tactical(env);
    
    // free_linearlstm(net);
    // free(weights);
    // free_allocated(&env);
    // close_client(client);
}




================================================
FILE: pufferlib/ocean/tactical/tactical.py
================================================
import numpy as np
import gymnasium
import os
#from raylib import rl
#import heapq

import pufferlib
from pufferlib.ocean.tactical import binding
# from pufferlib.environments.ocean import render

EMPTY = 0
GROUND = 1
HOLE = 2
WALL = 3

MAP_DICT = {
    '_': EMPTY,
    '.': GROUND,
    '|': HOLE,
    '#': WALL,
}


class Tactical:
    def __init__(self, num_envs=200, render_mode='human', seed=0):
        self.num_envs = num_envs
        self.render_mode = render_mode

        # env spec (TODO)
        self.observation_space = gymnasium.spaces.Box(
            low=0, high=2, shape=(10,), dtype=np.uint8)
        self.action_space = gymnasium.spaces.Discrete(4)
        self.single_observation_space = self.observation_space
        self.single_action_space = self.action_space
        self.num_agents = self.num_envs
        self.render_mode = render_mode
        self.emulated = None
        self.done = False
        self.buf = pufferlib.namespace(
            observations = np.zeros(
                (num_envs, 10), dtype=np.uint8),
            rewards = np.zeros(num_envs, dtype=np.float32),
            terminals = np.zeros(num_envs, dtype=bool),
            truncations = np.zeros(num_envs, dtype=bool),
            masks = np.ones(num_envs, dtype=bool),
        )
        self.actions = np.zeros(num_envs, dtype=np.uint32)
        
        self.c_envs = binding.vec_init(
            self.buf.observations,
            self.actions,
            self.buf.rewards,
            self.buf.terminals,
            self.buf.truncations,
            num_envs,
            seed,
            num_obs=self.buf.observations.shape[1]
        )

        # render
        # if render_mode == 'human':
        #     self.client = RaylibClient()

        # map_path = 'pufferlib/environments/ocean/tactical/map.txt'
        # map_path = 'pufferlib/environments/ocean/tactical/map_test.txt'
        # print(map_path)
        # self.load_map(map_path)
    
    def load_map(self, filename):
        with open(filename, 'r') as f:
            self.map_str = [line.strip() for line in f.read().strip().split('\n') if line[0] != ';']
        self.map_width = len(self.map_str[0])
        self.map_height = len(self.map_str)
        self.map = np.zeros((self.map_height, self.map_width), dtype=np.uint8)
        for i, row in enumerate(self.map_str):
            for j, cell in enumerate(row):
                self.map[i, j] = MAP_DICT[cell]

    def reset(self, seed=None):
        self.c_envs = []
        for i in range(self.num_envs):
            self.c_envs.append(binding.vec_init(
                self.buf.observations[i],
                self.actions[i:i+1],
                self.buf.rewards[i:i+1]))
            binding.vec_reset(self.c_envs[i])

        return self.buf.observations, {}

    def step(self, actions):
        self.actions[:] = actions
        for c_env in self.c_envs:
            binding.vec_step(c_env)
        
        info = {}

        return (self.buf.observations, self.buf.rewards,
            self.buf.terminals, self.buf.truncations, info)

    def render(self):
        return binding.vec_render(self.c_envs, 0)
        # if self.render_mode == 'human':
        #     return self.client.render(self.map)

    def close(self):
        for c_env in self.c_envs:
            binding.vec_close(c_env)

'''
def a_star_search(map, start, goal):
    frontier = []
    heapq.heappush(frontier, (0, start))

    came_from = {}
    cost_so_far = {}
    came_from[start] = None
    cost_so_far[start] = 0
    
    while len(frontier) > 0:
        current = heapq.heappop(frontier)[1]
        
        if current == goal:
            break
        
        for dx, dy in [(0, 1), (0, -1), (1, 0), (-1, 0)]:
            next = (current[0] + dx, current[1] + dy)
            if next[0] < 0 or next[1] < 0 or next[0] >= map.shape[0] or next[1] >= map.shape[1] or map[next] != GROUND:
                continue
            new_cost = cost_so_far[current] + 1
            if next not in cost_so_far or new_cost < cost_so_far[next]:
                cost_so_far[next] = new_cost
                priority = new_cost + abs(next[0] - goal[0]) + abs(next[1] - goal[1])
                heapq.heappush(frontier, (priority, next))
                came_from[next] = current
    
    # return came_from, cost_so_far
    # reconstruct path
    path = []
    if goal not in came_from:  # no path was found
        return []
    assert current == goal
    while current != start:
        path.append(current)
        current = came_from[current]
    # path.append(start)
    path.reverse()
    return path


class RaylibClient:
    def __init__(self):
        self.screenw = 1200
        self.screenh = 900
        rl.InitWindow(self.screenw, self.screenh, "Puffer Tactical".encode())
        rl.SetTargetFPS(60)

        self.row = 12
        self.col = 12

        self.anim = False
        self.anim_type = None
        self.anim_path = None
        self.anim_path_progress = None

        self.spell_mode = False

        self.cra_bottom = rl.LoadTexture('pufferlib/environments/ocean/tactical/sacri_bottom.png'.encode())
        self.cra_top = rl.LoadTexture('pufferlib/environments/ocean/tactical/sacri_top.png'.encode())
        self.cra_left = rl.LoadTexture('pufferlib/environments/ocean/tactical/sacri_left.png'.encode())
        self.cra_right = rl.LoadTexture('pufferlib/environments/ocean/tactical/sacri_right.png'.encode())
        self.cra_tex = self.cra_bottom

    def render(self, map):
        # TODO : rather than compute isometric coordinates
        # could be easier to do all cartesian and use a coordinate conversion (linear algebra, some cos/sin)
        # to go back and forth between the two coordinate systems?
        # see https://en.wikipedia.org/wiki/Isometric_projection
        if rl.IsKeyDown(rl.KEY_ESCAPE):
            exit(0)

        if rl.IsKeyDown(rl.KEY_E) and not self.anim:
            self.spell_mode = True
        if rl.IsKeyDown(rl.KEY_R) and not self.anim:
            self.spell_mode = False

        nrows, ncols = map.shape

        # figure out dimensions so the map scales to fit on the screen

        # map width = 14, map height = 16
        # find map width (longest bottomleft-topright diagonal)

        mapw = -1
        for i in range(nrows):
            horizontal_line = [map[i-k,k] for k in range(min(i + 1, ncols))]
            if set(horizontal_line) == {EMPTY}: continue
            i0, i1 = 0, len(horizontal_line) - 1
            while horizontal_line[i0] == EMPTY: i0 += 1
            while horizontal_line[i1] == EMPTY: i1 -= 1
            mapw = max(mapw, i1 - i0 + 1)
        maph = -1
        for i in range(ncols):
            vertical_line = [map[k,i+k] for k in range(min(ncols - i, nrows))]
            if set(vertical_line) == {EMPTY}: continue
            i0, i1 = 0, len(vertical_line) - 1
            while vertical_line[i0] == EMPTY: i0 += 1
            while vertical_line[i1] == EMPTY: i1 -= 1
            maph = max(maph, i1 - i0 + 1)


        padding_top = 100
        padding_bottom = 100
        cw_max = (self.screenw) / mapw
        ch_max = (self.screenh - padding_top - padding_bottom) / maph
        # we want ch = cw / 2 -> pick the best ratio
        if ch_max > cw_max / 2:
            cw = cw_max
            ch = cw / 2
        else:
            ch = ch_max
            cw = ch * 2

        # figure out correct offset to center the game
        xmin = 1e9
        ymin = 1e9
        for i, row in enumerate(map):
            for j, cell in enumerate(row):
                # todo not the most efficient + avoid code repetition
                if cell != EMPTY:
                    xa = 0.5 * (j + 1) * cw - 0.5 * (i + 1) * cw
                    ya = 0.5 * (j + 1) * ch + 0.5 * (i + 1) * ch
                    xmin = min(xmin, xa - cw / 2)
                    ymin = min(ymin, ya)

        # import sys; sys.exit(0)

        offset_x = -xmin + (self.screenw-cw*mapw)/2  # center
        offset_y = -ymin + padding_top
        # cw = 80
        # ch = cw / 2

        rl.BeginDrawing()
        rl.ClearBackground(render.PUFF_BACKGROUND)

        # get mouse pos
        mx, my = rl.GetMouseX(), rl.GetMouseY()
        rl.DrawText(f"Mouse: {mx}, {my}".encode(), 15, 10, 20, render.PUFF_TEXT)
        # get corresponding cell (if any)
        # to get the formula: we know that cell (row, col) = (i, j) starts at coordinates
        #   x = offset_x + 0.5 * (j + 1) * cw - 0.5 * (i + 1) * cw
        #   y = offset_y + 0.5 * (j + 1) * ch + 0.5 * (i + 1) * ch
        # Solve this to write i and j as a function of x and y and we get the formulas below
        ci = int((offset_x - mx) / cw + (my - offset_y) / ch - 1)
        cj = int((mx - offset_x) / cw + (my - offset_y) / ch - 1)
        cell = None if ci < 0 or cj < 0 or ci >= nrows or cj >= ncols else (ci, cj)
        rl.DrawText(f"Cell: {cell}".encode(), 15, 35, 20, render.PUFF_TEXT)


        # movement
        movement = np.zeros_like(map)

        if not self.anim and not self.spell_mode:
            if cell is not None:
                # draw movement path
                path = a_star_search(map, start=(self.row, self.col), goal=(ci, cj))
                if path:
                    path_rows, path_cols = zip(*path)
                    movement[path_rows, path_cols] = 1

                    if rl.IsMouseButtonPressed(rl.MOUSE_BUTTON_LEFT):
                        if cell is not None and map[cell] == GROUND:
                            # self.row = ci
                            # self.col = cj
                            self.anim = True
                            self.anim_type = 'move'
                            self.anim_path = [(self.row, self.col)] + path
                            self.anim_path_progress = 0

        # line of sight
        los = np.ones_like(map)

        for i in range(nrows):
            for j in range(ncols):
                cell = map[i, j]
                if cell != GROUND:
                    los[i, j] = 0
                elif (i, j) == (self.row, self.col):
                    los[i, j] = 0
                else:
                    # use bresenham-based supercover line algorithm
                    # http://eugen.dedu.free.fr/projects/bresenham/
                    # note: bresenham alone doesnt find all cells covered by the lines
                    # implementation from https://www.redblobgames.com/grids/line-drawing/#supercover (covers all quadrants) <- here it is explained very well, the algo is pretty simple
                    # now we could precompute this on the map for every pair of points
                    # the question is: if we add one obstacle, how does it change lines of sight? mb its fast enough to just simulate in real time? 
                    # ONE OTHER APPROACH: for every pair of points, assume one point is the observer and the other is a wall (so, ignoring the geometry of the map). then, what lines of sight do we have? then we just need to do a logical and for all lines of sight. not sure its even faster though, it doesnt seem to be. 
                    # an optimization: instead of doing lines of sight for all pair of points, we could check between observer and all border cells of the map? then, we set all cells to line of sight true and as soon as we hit an obstacle, we'll set all subsequent cells to line of sight false. this should hit all the cells?
                    # bressenham: check all points between character and (i, j), if any is an obstacle then cancel the line of sight
                    x0 = self.col
                    y0 = self.row
                    x1 = j
                    y1 = i
                    ###
                    dx = x1 - x0
                    dy = y1 - y0
                    nx = abs(dx)
                    ny = abs(dy)
                    sign_x = 1 if dx > 0 else -1 
                    sign_y = 1 if dy > 0 else -1
                    px = x0
                    py = y0
                    ix = 0
                    iy = 0
                    while ix < nx or iy < ny:
                        if map[py, px] == WALL:
                            los[i, j] = 0
                            break
                        decision = (1 + 2 * ix) * ny - (1 + 2 * iy) * nx
                        if decision == 0:
                            # next step is diagonal
                            px += sign_x
                            py += sign_y
                            ix += 1
                            iy += 1
                        elif decision < 0:
                            # next step is horizontal
                            px += sign_x
                            ix += 1
                        else:
                            # next step is vertical
                            py += sign_y
                            iy += 1



    # bool IsMouseButtonPressed(int button);   


        # naive (O(n^3)) for each pair of cell A, B
        # we draw the line from the center of cell A to the center of cell B
        # then we use bressenham's algo https://en.wikipedia.org/wiki/Bresenham%27s_line_algorithm
        # to find all the cells that the line goes through
        # if any of these is an obstacle, then there is no line of sight between A and B. Otherwise there is.

        # maybe better: for each obstacle, directly find all the cells this obstacle hides and mask them
        



        # draw cells from top-left to bottom-right
        #  isometric cell               link to bottom   link to top
        #    (ground)                                         4    
        #       a                             a           5   a   6
        #   b   e   c  (b<->c = cw)       b   0   c       b   7   c
        #       d                         1   d   2           d    
        #     (a<->d = ch)                    3                    
        # cell dimensions (as per drawing above)
        for i, row in enumerate(map):
            for j, cell in enumerate(row):
                # compute isometrics coordinates (points a,b,c,d) -- TODO of course all this should be precomputed
                xa = offset_x + 0.5 * (j + 1) * cw - 0.5 * (i + 1) * cw
                xb, xc, xd = xa - cw / 2, xa + cw / 2, xa
                ya = offset_y + 0.5 * (j + 1) * ch + 0.5 * (i + 1) * ch
                yb, yc, yd = ya + ch / 2, ya + ch / 2, ya + ch
                xe, ye = xa, yb
                # draw cell
                if cell == WALL:
                    dy = ch * 0.4
                    x4, x5, x6, x7 = xa, xb, xc, xd
                    y4, y5, y6, y7 = ya - dy, yb - dy, yc - dy, yd - dy
                    rl.DrawTriangleStrip([(x4, y4), (x5, y5), (x6, y6), (x7, y7)], 4, [163, 197, 69, 255])  # top square
                    rl.DrawTriangleStrip([(xc, yc), (x6, y6), (xd, yd), (x7, y7), (xb, yb), (x5, y5)], 6, [40, 20, 5, 255])  # connection with ground
                elif cell == HOLE:
                    pass  # leave empty, as a hole should be
                elif cell == GROUND:
                    if movement[(i, j)]:
                        col = [0, 180, 0, 255]
                    elif self.spell_mode:
                        #elif abs(i - self.row) + abs(j - self.col) <= 10 and abs(i - self.row) + abs(j - self.col) > 0:
                        if los[(i, j)]:
                            if (i, j) == (ci, cj):
                                col = [255, 165, 0, 255]                            
                            else:
                                col = [68, 109, 153, 255]
                        else:
                            col = [112, 123, 111, 255]
                    else:
                        col = [189, 205, 125, 255] if (i + j) % 2 == 0 else [180, 195, 118, 255]
                    rl.DrawTriangleStrip([(xa, ya), (xb, yb), (xc, yc), (xd, yd)], 4, col)

                    # draw white border around cell
                    rl.DrawLineStrip([(xa, ya), (xb, yb), (xd, yd), (xc, yc), (xa, ya)], 5, (255, 255, 255, 255))
                # Draw dirt below the cell
                if cell == GROUND or cell == WALL:
                    # here we only draw what will be seen ; maybe it's faster to draw everything and not do any checks
                    dy = ch * 0.7
                    x0, x1, x2, x3 = xa, xb, xc, xd
                    y0, y1, y2, y3 = ya + dy, yb + dy, yc + dy, yd + dy
                    if i == len(map) - 1 or map[i+1,j] in [HOLE, EMPTY]:
                        rl.DrawTriangleStrip([(xb, yb), (x1, y1), (xd, yd), (x3, y3)], 4, [68, 48, 10, 255])  # left side (b-1-3-d boundary)
                    if j == len(row) - 1 or map[i,j+1] in [HOLE, EMPTY]:
                        rl.DrawTriangleStrip([(xd, yd), (x3, y3), (xc, yc), (x2, y2)], 4, [95, 77, 21, 255])  # right side (d-3-2-c boundary)



        # draw character

        xe = offset_x + 0.5 * (self.col + 1) * cw - 0.5 * (self.row + 1) * cw
        ye = offset_y + 0.5 * (self.col + 1) * ch + 0.5 * (self.row + 1) * ch + ch / 2

        xe_m = offset_x + 0.5 * (cj + 1) * cw - 0.5 * (ci + 1) * cw
        ye_m = offset_y + 0.5 * (cj + 1) * ch + 0.5 * (ci + 1) * ch + ch / 2

        # 465*1129
        cra_tex_w = 465
        cra_tex_h = 1129
        cra_tex_desired_h = 1.6 * ch
        scale = cra_tex_desired_h / cra_tex_h
        cra_tex_desired_w = cra_tex_w * scale
        cra_x = xe - cra_tex_desired_w / 2
        cra_y = ye - cra_tex_desired_h + 0.1 * ch

        if self.anim and self.anim_type == "move":
            # cur is updated when we arrive at the center of a new cell
            cur = self.anim_path[int(self.anim_path_progress)]
            self.row, self.col = cur
            transition_progress = self.anim_path_progress - int(self.anim_path_progress)
            if cur == self.anim_path[-1]:
                self.anim = False
            else:
                next = self.anim_path[int(self.anim_path_progress)+1]
                # use correct facing of the texture
                if next[0] == cur[0] + 1:
                    self.cra_tex = self.cra_bottom
                    self.movx, self.movy = -1, 1
                elif next[0] == cur[0] - 1:
                    self.cra_tex = self.cra_top
                    self.movx, self.movy = 1, -1
                elif next[1] == cur[1] + 1:
                    self.cra_tex = self.cra_right
                    self.movx, self.movy = 1, 1
                elif next[1] == cur[1] - 1:
                    self.cra_tex = self.cra_left
                    self.movx, self.movy = -1, -1
            # add a delta to the x,y texture position for continuous movement
            delta_x = (transition_progress) * cw * 0.5 * self.movx
            delta_y = (transition_progress) * ch * 0.5 * self.movy
            self.anim_path_progress += 0.1
            cur = self.anim_path[int(self.anim_path_progress)]
            self.row, self.col = cur
        else:
            delta_x = delta_y = 0

        coef = 0.35
        thickness = 2
        if self.anim and self.anim_type == 'move':
            col = [189, 205, 125, 255] if (self.anim_path[0][0] + self.anim_path[0][1]) % 2 == 0 else [180, 195, 118, 255]
        else:
            col = [189, 205, 125, 255] if (self.row + self.col) % 2 == 0 else [180, 195, 118, 255]
        rl.DrawEllipse(int(xe + delta_x), int(ye + delta_y), cw * coef, ch * coef, [255, 0, 0, 255])
        rl.DrawEllipse(int(xe + delta_x), int(ye + delta_y), cw * coef - thickness, ch * coef - thickness, col)

        rl.DrawTextureEx(self.cra_tex, (cra_x + delta_x, cra_y + delta_y), 0, scale, [255, 255, 255, 255])

        # void DrawSplineLinear(Vector2 *points, int pointCount, float thick, Color color);                  // Draw spline: Linear, minimum 2 points
        # rl.DrawSplineLinear([(xe, ye), (mx, my)], 10, 5, [255, 0, 0, 255])
        # rl.DrawSplineBezierQuadratic([(xe, ye-cra_tex_desired_h/2), ((xe+mx)/2,(ye+my)/2-200), (mx, my)], 3, 5, [255, 0, 0, 255])

        if rl.IsMouseButtonPressed(rl.MOUSE_BUTTON_LEFT) and self.spell_mode and los[ci,cj]:
            self.anim = True
            self.anim_type = "spell"
            self.spell_mode = False

            self.anim_path = [(xe, ye-cra_tex_desired_h/2), ((xe+mx)/2,(ye+my)/2-200), (xe_m, ye_m)]
            self.anim_path_progress = 0.01

        if self.anim and self.anim_type == "spell":
            self.anim_path_progress += 0.025
            pt = rl.GetSplinePointBezierQuad(*self.anim_path, min(self.anim_path_progress, 1.0))

            if self.anim_path_progress <= 1.0:
                rl.DrawCircle(int(pt.x), int(pt.y), 10, [255, 0, 0, 255])
            else:
                rl.DrawCircle(int(pt.x), int(pt.y), 10 + (self.anim_path_progress - 1.0) * 100, [255, 0, 0, int(255 - (self.anim_path_progress - 1.0) * 1200)])

            if self.anim_path_progress >= 1.2:
                self.anim = False

        rl.EndDrawing()
        return render.cdata_to_numpy()
'''


if __name__ == '__main__':
    PROFILE = False
    env = Tactical(num_envs=1, render_mode='human')
    env.reset()
    import time
    t0 = time.time()
    steps = 0
    while not PROFILE or time.time() - t0 < 10:
        env.step([0] * env.num_envs)
        if not PROFILE:
            if env.render() == 1:  # exit code
                break
        steps += 1
    print('SPS:', 1 * steps / (time.time() - t0))
    



================================================
FILE: pufferlib/ocean/tcg/build_local.sh
================================================
clang -Wall -Wuninitialized -Wmisleading-indentation -fsanitize=address,undefined,bounds,pointer-overflow,leak -ferror-limit=3 -g -o tcg tcg.c -I./raylib-5.0_linux_amd64/include/ -L./raylib-5.0_linux_amd64/lib/ -lraylib -lGL -lm -lpthread -ldl -lrt -lX11 -DPLATFORM_DESKTOP





================================================
FILE: pufferlib/ocean/tcg/build_web.sh
================================================
emcc -o build/game.html tcg.c -Os -Wall ./raylib/src/libraylib.a -I./raylib/src -L. -L./raylib/src/libraylib.a -sASSERTIONS=2 -gsource-map -s USE_GLFW=3 -sUSE_WEBGL2=1 -s ASYNCIFY -sFILESYSTEM -s FORCE_FILESYSTEM=1 --shell-file ./raylib/src/minshell.html -DPLATFORM_WEB -DGRAPHICS_API_OPENGL_ES3



================================================
FILE: pufferlib/ocean/tcg/tcg.c
================================================
#include "tcg.h"

int main() {
    TCG env = {0}; // MUST ZERO
    allocate_tcg(&env);
    reset(&env);

    init_client(&env);

    int atn = -1;
    while (!WindowShouldClose()) {
        if (atn != -1) {
            step(&env, atn);
            atn = -1;
        }

        if (IsKeyPressed(KEY_ONE)) atn = 0;
        if (IsKeyPressed(KEY_TWO)) atn = 1;
        if (IsKeyPressed(KEY_THREE)) atn = 2;
        if (IsKeyPressed(KEY_FOUR)) atn = 3;
        if (IsKeyPressed(KEY_FIVE)) atn = 4;
        if (IsKeyPressed(KEY_SIX)) atn = 5;
        if (IsKeyPressed(KEY_SEVEN)) atn = 6;
        if (IsKeyPressed(KEY_EIGHT)) atn = 7;
        if (IsKeyPressed(KEY_NINE)) atn = 8;
        if (IsKeyPressed(KEY_ZERO)) atn = 9;
        if (IsKeyPressed(KEY_ENTER)) atn = 10;

        if (env.turn == 1) {
            atn = rand() % 11;
        }
 
        render(&env);
    }
    free_tcg(&env);
    return 0;
}



================================================
FILE: pufferlib/ocean/tcg/tcg.h
================================================
#include <stdlib.h>
#include <stdbool.h>
#include <stdio.h>
#include <assert.h>
#include "raylib.h"

#define HAND_SIZE 10
#define BOARD_SIZE 10
#define DECK_SIZE 60
#define STACK_SIZE 100

#define ACTION_ENTER 10
#define ACTION_NOOP 11

#define TO_USER true;
#define TO_STACK false;

typedef struct TCG TCG;
typedef bool (*call)(TCG*, unsigned char);
bool phase_untap(TCG* env, unsigned char atn);
bool phase_draw(TCG* env, unsigned char atn);
bool phase_play(TCG* env, unsigned char atn);
bool phase_attack(TCG* env, unsigned char atn);
bool phase_block(TCG* env, unsigned char atn);
void reset(TCG* env);

typedef struct Stack Stack;
struct Stack {
    call data[STACK_SIZE];
    int idx;
};

void push(Stack* stack, call fn) {
    assert(stack->idx < STACK_SIZE);
    stack->data[stack->idx] = fn;
    stack->idx += 1;
}

call pop(Stack* stack) {
    assert(stack->idx > 0);
    stack->idx -= 1;
    return stack->data[stack->idx];
}

call peek(Stack* stack) {
    assert(stack->idx > 0);
    return stack->data[stack->idx - 1];
}

typedef struct Card Card;
struct Card {
    int cost;
    int attack;
    int health;
    bool is_land;
    bool remove;
    bool tapped;
    bool attacking;
    int defending;
};

typedef struct CardArray CardArray;
struct CardArray {
    Card* cards;
    int length;
    int max;
};

CardArray* allocate_card_array(int max) {
    CardArray* hand = (CardArray*)calloc(1, sizeof(CardArray));
    hand->cards = (Card*)calloc(max, sizeof(Card));
    hand->max = max;
    return hand;
}

void free_card_array(CardArray* ary) {
    free(ary->cards);
    free(ary);
}

void condense_card_array(CardArray* ary) {
    int idx = 0;
    for (int i = 0; i < ary->length; i++) {
        if (!ary->cards[i].remove) {
            ary->cards[idx] = ary->cards[i];
            idx += 1;
        }
    }
    ary->length = idx;
}

struct TCG {
    CardArray* my_hand;
    CardArray* my_board;
    CardArray* my_deck;
    int my_health;
    int my_mana;
    bool my_land_played;

    CardArray* op_hand;
    CardArray* op_board;
    CardArray* op_deck;
    int op_health;
    int op_mana;
    bool op_land_played;

    Stack* stack;
    //bool attackers[BOARD_SIZE];
    //bool defenders[BOARD_SIZE][BOARD_SIZE];
    int block_idx;
    int turn;
};

void allocate_tcg(TCG* env) {
    env->stack = calloc(1, sizeof(Stack));
    env->my_hand = allocate_card_array(HAND_SIZE);
    env->op_hand = allocate_card_array(HAND_SIZE);
    env->my_board = allocate_card_array(BOARD_SIZE);
    env->op_board = allocate_card_array(BOARD_SIZE);
    env->my_deck = allocate_card_array(DECK_SIZE);
    env->op_deck = allocate_card_array(DECK_SIZE);
}

void free_tcg(TCG* env) {
    free_card_array(env->my_hand);
    free_card_array(env->op_hand);
    free_card_array(env->my_board);
    free_card_array(env->op_board);
    free_card_array(env->my_deck);
    free_card_array(env->op_deck);
}

void randomize_deck(CardArray* deck) {
    for (int i = 0; i < deck->length; i++) {
        deck->cards[i].defending = -1;
        if (rand() % 3 == 0) {
            deck->cards[i].is_land = true;
        } else {
            int cost = rand() % 6;
            deck->cards[i].cost = cost;
            deck->cards[i].attack = cost + 1;
            deck->cards[i].health = cost + 1;
        }
    }
}

void draw_card(TCG* env, CardArray* deck, CardArray* hand) {
    if (deck->length == 0) {
        reset(env);
        return;
    }
    if (hand->length == hand->max) {
        return;
    }
    Card card = deck->cards[deck->length - 1];
    hand->cards[hand->length] = card;
    deck->length -= 1;
    hand->length += 1;
}

bool can_attack(CardArray* board) {
    for (int i = 0; i < board->length; i++) {
        if (!board->cards[i].is_land) {
            return true;
        }
    }
    return false;
}

int tappable_mana(TCG* env) {
    CardArray* board = (env->turn == 0) ? env->my_board : env->op_board;
    int tappable = 0;
    for (int i = 0; i < board->length; i++) {
        Card card = board->cards[i];
        if (card.is_land && !card.tapped) {
            tappable += 1;
        }
    }
    return tappable;
}

bool can_play(TCG* env) {
    CardArray* hand = (env->turn == 0) ? env->my_hand : env->op_hand;
    int* mana = (env->turn == 0) ? &env->my_mana : &env->op_mana;
    bool* land_played = (env->turn == 0) ? &env->my_land_played : &env->op_land_played;

    int min_cost = 99;
    for (int i = 0; i < hand->length; i++) {
        if (hand->cards[i].is_land && !*land_played) {
            return true;
        } else if (hand->cards[i].cost < min_cost) {
            min_cost = hand->cards[i].cost;
        }
    }

    int tappable = tappable_mana(env);
    return *mana + tappable >= min_cost;
}

bool phase_untap(TCG* env, unsigned char atn) {
    printf("PHASE_UNTAP\n");
    bool* land_played = (env->turn == 0) ? &env->my_land_played : &env->op_land_played;
    *land_played = false;

    env->turn = 1 - env->turn;
    CardArray* board = (env->turn == 0) ? env->my_board : env->op_board;

    int* mana = (env->turn == 0) ? &env->my_mana : &env->op_mana;
    *mana = 0;

    for (int i = 0; i < board->length; i++) {
        Card card = board->cards[i];
        if (card.is_land && card.tapped) {
            board->cards[i].tapped = false;
        }
    }
    
    push(env->stack, phase_draw);
    return TO_STACK;
}

bool phase_draw(TCG* env, unsigned char atn) {
    printf("PHASE_DRAW\n");
    CardArray* deck = (env->turn == 0) ? env->my_deck : env->op_deck;
    CardArray* hand = (env->turn == 0) ? env->my_hand : env->op_hand;
    draw_card(env, deck, hand);
    push(env->stack, phase_play);
    return TO_STACK;
}

bool phase_play(TCG* env, unsigned char atn) {
    printf("PHASE_PLAY\n");
    CardArray* hand = (env->turn == 0) ? env->my_hand : env->op_hand;
    CardArray* board = (env->turn == 0) ? env->my_board : env->op_board;
    int* mana = (env->turn == 0) ? &env->my_mana : &env->op_mana;
    bool* land_played = (env->turn == 0) ? &env->my_land_played : &env->op_land_played;

    if (board->length == BOARD_SIZE) {
        printf("\t Board full\n");
        push(env->stack, phase_attack);
        return TO_STACK;
    }

    if (!can_play(env)) {
        printf("\t No valid moves\n");
        push(env->stack, phase_attack);
        return TO_STACK;
    }

    if (atn == ACTION_NOOP) {
        push(env->stack, phase_play);
        return TO_USER;
    } else if (atn == ACTION_ENTER) {
        push(env->stack, phase_attack);
        return TO_STACK;
    } else if (atn >= hand->length) {
        printf("\t Invalid action: %i\n. Hand length: %i\n", atn, hand->length);
        push(env->stack, phase_play);
        return TO_USER;
    }

    Card card = hand->cards[atn];
    if (card.is_land) {
        if (*land_played) {
            printf("\t Already played land this turn\n");
            push(env->stack, phase_play);
            return TO_USER;
        }
        board->cards[board->length] = card;
        board->length += 1;
        *land_played = true;
        hand->cards[atn].remove = true;
        condense_card_array(hand);
        printf("\t Land played\n");
        push(env->stack, phase_play);
        return TO_USER;
    }

    if (card.cost > *mana + tappable_mana(env)) {
        printf("\t Not enough mana\n");
        push(env->stack, phase_play);
        return TO_USER;
    }

    // Auto tap lands?
    for (int i = 0; i < board->length; i++) {
        if (card.cost <= *mana) {
            break;
        }
        Card card = board->cards[i];
        if (card.is_land && !card.tapped) {
            *mana += 1;
            board->cards[i].tapped = true;
        }
    }

    assert(*mana >= card.cost);
    *mana -= card.cost;
    board->cards[board->length] = card;
    board->length += 1;
    hand->cards[atn].remove = true;
    condense_card_array(hand);
    printf("\t Card played\n");
    push(env->stack, phase_play);
    return TO_USER;
}

bool phase_attack(TCG* env, unsigned char atn) {
    printf("PHASE_ATTACK\n");
    CardArray* board = (env->turn == 0) ? env->my_board : env->op_board;

    if (!can_attack(board)) {
        printf("\t No valid attacks. Phase end\n");
        push(env->stack, phase_untap);
        return TO_STACK;
    }

    if (atn == ACTION_NOOP) {
        push(env->stack, phase_attack);
        return TO_USER;
    } else if (atn == ACTION_ENTER) {
        printf("\t Attacks confirmed. Phase end\n");
        env->turn = 1 - env->turn;
        push(env->stack, phase_block);
        return TO_STACK;
    } else if (atn >= board->length) {
        printf("\t Invalid action %i\n", atn);
        push(env->stack, phase_attack);
        return TO_USER;
    } else if (board->cards[atn].is_land) {
        printf("\t Cannot attack with land\n");
        push(env->stack, phase_attack);
        return TO_USER;
    } else {
        printf("\t Setting attacker %i\n", atn);
        board->cards[atn].attacking = !board->cards[atn].attacking;
        push(env->stack, phase_attack);
        return TO_USER;
    }
}

bool phase_block(TCG* env, unsigned char atn) {
    printf("PHASE_BLOCK\n");
    CardArray* defender_board = (env->turn == 0) ? env->my_board : env->op_board;
    CardArray* board = (env->turn == 0) ? env->op_board : env->my_board;
    int* health = (env->turn == 0) ? &env->op_health : &env->my_health;

    while (env->block_idx < board->length && !board->cards[env->block_idx].attacking) {
        printf("\t Skipping block for %i (not attacking)\n", env->block_idx);
        env->block_idx++;
    }
    
    bool can_block = false;
    for (int i = 0; i < defender_board->length; i++) {
        Card* card = &defender_board->cards[i];
        if (card->is_land) {
            continue;
        }
        if (card->defending == -1 || card->defending == env->block_idx) {
            can_block = true;
            printf("\t Can block with %i\n", i);
            break;
        }
    }
    if (!can_block) {
        env->block_idx = board->length;
    }
 
    if (env->block_idx == board->length) {
        printf("\t Attacker board length: %i\n", board->length);
        for (int atk = 0; atk < board->length; atk++) {
            printf("\t Resolving %i\n", atk);
            Card* attacker = &board->cards[atk];
            if (!attacker->attacking) {
                printf("\t Not attacking\n");
                continue;
            }
            int attacker_attack = attacker->attack;
            int attacker_health = attacker->health;
            for (int def = 0; def < defender_board->length; def++) {
                Card* defender = &defender_board->cards[def];
                if (defender->defending != atk) {
                    continue;
                }
                if (attacker_attack >= defender->health) {
                    attacker_attack -= defender->health;
                    attacker_health -= defender->attack;
                    defender->health = 0;
                    defender->remove = true;
                } else {
                    attacker_health -= defender->attack;
                    attacker_attack = 0;
                }
                if (attacker_health <= 0) {
                    attacker->remove = true;
                    break;
                }
            }
            printf("\t Reducing health by %i\n", attacker_attack);
            *health -= attacker_attack;
        }

        if (*health <= 0) {
            printf("\t Game over\n");
            reset(env);
        }

        condense_card_array(env->my_board);
        condense_card_array(env->op_board);

        CardArray* defender_deck = (env->turn == 0) ? env->my_deck : env->op_deck;
        CardArray* defender_hand = (env->turn == 0) ? env->my_hand : env->op_hand;
        draw_card(env, defender_deck, defender_hand);

        for (int i = 0; i < board->length; i++) {
            board->cards[i].attacking = false;
        }
        for (int i = 0; i < defender_board->length; i++) {
            defender_board->cards[i].defending = -1;
        }
        printf("\t Set block idx to 0\n");
        env->block_idx = 0;
        env->turn = 1 - env->turn;
        push(env->stack, phase_untap);
        return TO_STACK;
    }

    if (atn == ACTION_NOOP) {
        push(env->stack, phase_block);
        return TO_USER;
    } else if (atn == ACTION_ENTER) {
        printf("\t Manual block confirm %i\n", env->block_idx);
        env->block_idx++;
        push(env->stack, phase_block);
        return TO_STACK;
    } else if (atn >= defender_board->length) {
        printf("\t Invalid block action %i\n", atn);
        push(env->stack, phase_block);
        return TO_USER;
    } else if (defender_board->cards[atn].is_land) {
        printf("\t Cannot block with land\n");
        push(env->stack, phase_block);
        return TO_USER;
    }

    for (int i = 0; i < env->block_idx; i++) {
        if (defender_board->cards[atn].defending == i) {
            printf("\t Already blocked\n");
            push(env->stack, phase_block);
            return TO_USER;
        }
    }
    printf("\t Blocking index %i with %i\n", env->block_idx, atn);
    Card* card = &defender_board->cards[atn];
    if (card->defending == env->block_idx) {
        card->defending = -1;
    } else {
        card->defending = env->block_idx;
    }
    push(env->stack, phase_block);
    return TO_USER;
}

void step(TCG* env, unsigned char atn) {
    printf("Turn: %i, Action: %i\n", env->turn, atn);
    while (true) {
        call fn = pop(env->stack);
        bool return_to_user = fn(env, atn);
        if (return_to_user) {
            return;
        }
        atn = ACTION_NOOP;
    }
}

void reset(TCG* env) {
    env->my_deck->length = DECK_SIZE;
    env->op_deck->length = DECK_SIZE;
    env->my_hand->length = 0;
    env->op_hand->length = 0;
    env->my_board->length = 0;
    env->op_board->length = 0;
    env->my_health = 20;
    env->op_health = 20;
    randomize_deck(env->my_deck);
    randomize_deck(env->op_deck);
    env->turn = rand() % 2;
    for (int i = 0; i < 5; i++) {
        draw_card(env, env->my_deck, env->my_hand);
        draw_card(env, env->op_deck, env->op_hand);
    }
    push(env->stack, phase_draw);
    step(env, ACTION_NOOP);
}

void init_client(TCG* env) {
    InitWindow(1080, 720, "Puffer the Schooling TCG");
    SetTargetFPS(60);
}

void close_client(TCG* env) {
    CloseWindow();
}

int card_x(int col, int n) {
    int cards_width = 72*n;
    int offset = 72*col;
    return GetScreenWidth()/2 - cards_width/2 + offset;
}

int card_y(int row) {
    return 64 + (128 + 20)*row;
}

void render_card(Card* card, int x, int y, Color color) {
    DrawRectangle(x, y, 64, 128, color);
    if (card->is_land) {
        DrawText("Land", x + 16, y + 40, 16, WHITE);
    } else {
        DrawText(TextFormat("%i", card->cost), x + 32, y+16, 20, WHITE);
        DrawText(TextFormat("%i", card->attack), x + 32, y + 40, 20, WHITE);
        DrawText(TextFormat("%i", card->health), x + 32, y + 64, 20, WHITE);
    }
}

void render_label(int x, int y, int idx) {
    DrawText(TextFormat("%i", (idx+1)%10), x+32, y+96, 20, YELLOW);
}

void render(TCG* env) {
    BeginDrawing();
    ClearBackground((Color){6, 24, 24, 255});
   
    for (int i = 0; i < env->my_hand->length; i++) {
        Card card = env->my_hand->cards[i];
        int x = card_x(i, env->my_hand->length);
        int y = card_y(3);
        render_card(&card, x, y, RED);
        if (env->turn == 0) {
            render_label(x, y, i);
        }
    }

    for (int i = 0; i < env->my_board->length; i++) {
        Card card = env->my_board->cards[i];
        int x = card_x(i, env->my_board->length);
        int y = card_y(2);
        if (card.attacking) {
            y -= 16;
        }
        Color color = (card.tapped) ? (Color){128, 0, 0, 255}: RED;
        render_card(&card, x, y, color);
        if (env->turn == 0) {
            render_label(x, y, i);
        }
    }

    for (int i = 0; i < env->op_board->length; i++) {
        Card card = env->op_board->cards[i];
        int x = card_x(i, env->op_board->length);
        int y = card_y(1);
        if (card.attacking) {
            y += 16;
        }
        Color color = (card.tapped) ? (Color){0, 0, 128, 255}: BLUE;
        render_card(&card, x, y, color);
    }

    for (int i = 0; i < env->my_board->length; i++) {
        Card card = env->my_board->cards[i];
        if (card.defending == -1) {
            continue;
        }
        DrawLineEx(
            (Vector2){32+card_x(i, env->my_board->length), 64+card_y(2)},
            (Vector2){32+card_x(card.defending, env->op_board->length), 64+card_y(1)},
            3.0f, WHITE
        );
    }

    for (int i = 0; i < env->op_hand->length; i++) {
        Card card = env->op_hand->cards[i];
        int x = card_x(i, env->op_hand->length);
        int y = card_y(0);
        render_card(&card, x, y, BLUE);
    }

    int x = GetScreenWidth() - 128;
    int y = 32;

    call fn = peek(env->stack);
    if (fn == phase_draw) {
        DrawText("Draw", x, y, 20, WHITE);
    } else if (fn == phase_play) {
        DrawText("Play", x, y, 20, WHITE);
    } else if (fn == phase_attack) {
        DrawText("Attack", x, y, 20, WHITE);
    } else if (fn == phase_block) {
        DrawText("Block", x, y, 20, WHITE);
    } 

    DrawText(TextFormat("Health: %i", env->my_health), 32, 32, 20, WHITE);
    DrawText(TextFormat("Health: %i", env->op_health), 32, GetScreenHeight() - 64, 20, WHITE);

    EndDrawing();
}



================================================
FILE: pufferlib/ocean/template/binding.c
================================================
#include "template.h"

#define Env Template 
#include "../env_binding.h"

static int my_init(Env* env, PyObject* args, PyObject* kwargs) {
    env->size = unpack(kwargs, "size");
    return 0;
}

static int my_log(PyObject* dict, Log* log) {
    assign_to_dict(dict, "score", log->score);
    return 0;
}



================================================
FILE: pufferlib/ocean/template/template.c
================================================
#include "template.h"

int main() {
    Template env = {.size = 5};
    env.observations = (unsigned char*)calloc(1, sizeof(unsigned char));
    env.actions = (int*)calloc(1, sizeof(int));
    env.rewards = (float*)calloc(1, sizeof(float));
    env.terminals = (unsigned char*)calloc(1, sizeof(unsigned char));

    c_reset(&env);
    c_render(&env);
    while (!WindowShouldClose()) {
        if (IsKeyDown(KEY_LEFT_SHIFT)) {
            if (IsKeyDown(KEY_A) || IsKeyDown(KEY_LEFT)) {
                env.actions[0] = 0;
            } else if (IsKeyDown(KEY_D) || IsKeyDown(KEY_RIGHT)) {
                env.actions[0] = 1;
            } else {
                env.actions[0] = -1;
            }
        } else {
            env.actions[0] = rand() % 2;
        }
        c_step(&env);
        c_render(&env);
    }
    free(env.observations);
    free(env.actions);
    free(env.rewards);
    free(env.terminals);
    c_close(&env);
}




================================================
FILE: pufferlib/ocean/template/template.h
================================================
#include <stdlib.h>
#include <string.h>
#include "raylib.h"

const Color PUFF_RED = (Color){187, 0, 0, 255};
const Color PUFF_CYAN = (Color){0, 187, 187, 255};
const Color PUFF_WHITE = (Color){241, 241, 241, 241};
const Color PUFF_BACKGROUND = (Color){6, 24, 24, 255};

// Only use floats!
typedef struct {
    float score;
    float n; // Required as the last field 
} Log;

typedef struct {
    Log log;                     // Required field
    unsigned char* observations; // Required field. Ensure type matches in .py and .c
    int* actions;                // Required field. Ensure type matches in .py and .c
    float* rewards;              // Required field
    unsigned char* terminals;    // Required field
    int size;
    int x;
    int goal;
} Template;

void c_reset(Template* env) {
    env->x = 0;
    env->goal = (rand()%2 == 0) ? env->size : -env->size;
}

void c_step(Template* env) {
    env->rewards[0] = 0;
    env->terminals[0] = 0;
    if (env->actions[0] == 0) {
        env->x -= 1;
    } else if (env->actions[0] == 1) {
        env->x += 1;
    }
    if (env->x == env->goal) {
        c_reset(env);
        env->rewards[0] = 1;
        env->terminals[0] = 1;
        env->log.score += 1;
        env->log.n += 1;
    } else if (env->x == -env->goal) {
        c_reset(env);
        env->rewards[0] = -1;
        env->terminals[0] = 1;
        env->log.score -= 1;
        env->log.n += 1;
    }
    env->observations[0] = (env->goal > 0) ? 1 : -1;
}

void c_render(Template* env) {
    if (!IsWindowReady()) {
        InitWindow(1080, 720, "PufferLib Template");
        SetTargetFPS(5);
    }

    if (IsKeyDown(KEY_ESCAPE)) {
        exit(0);
    }

    DrawText("Go to the red square!", 20, 20, 20, PUFF_WHITE);
    DrawRectangle(540 - 32 + 64*env->goal, 360 - 32, 64, 64, PUFF_RED);
    DrawRectangle(540 - 32 + 64*env->x, 360 - 32, 64, 64, PUFF_CYAN);

    BeginDrawing();
    ClearBackground(PUFF_BACKGROUND);
    EndDrawing();
}

void c_close(Template* env) {
    if (IsWindowReady()) {
        CloseWindow();
    }
}



================================================
FILE: pufferlib/ocean/template/template.py
================================================
'''A minimal template for your own envs.'''

import gymnasium
import numpy as np

import pufferlib
from pufferlib.ocean.template import binding

class Template(pufferlib.PufferEnv):
    def __init__(self, num_envs=1, render_mode=None, log_interval=128, size=5, buf=None, seed=0):
        self.single_observation_space = gymnasium.spaces.Box(low=0, high=1,
            shape=(1,), dtype=np.uint8)
        self.single_action_space = gymnasium.spaces.Discrete(2)
        self.render_mode = render_mode
        self.num_agents = num_envs

        super().__init__(buf)
        self.c_envs = binding.vec_init(self.observations, self.actions, self.rewards,
            self.terminals, self.truncations, num_envs, seed, size=size)
        self.size = size
 
    def reset(self, seed=0):
        binding.vec_reset(self.c_envs, seed)
        return self.observations, []

    def step(self, actions):
        self.actions[:] = actions
        binding.vec_step(self.c_envs)
        info = [binding.vec_log(self.c_envs)]
        return (self.observations, self.rewards,
            self.terminals, self.truncations, info)

    def render(self):
        binding.vec_render(self.c_envs, 0)

    def close(self):
        binding.vec_close(self.c_envs)

if __name__ == '__main__':
    N = 4096
    env = Template(num_envs=N)
    env.reset()
    steps = 0

    CACHE = 1024
    actions = np.random.randint(0, 5, (CACHE, N))

    import time
    start = time.time()
    while time.time() - start < 10:
        env.step(actions[steps % CACHE])
        steps += 1

    print('Squared SPS:', int(env.num_agents*steps / (time.time() - start)))



================================================
FILE: pufferlib/ocean/terraform/binding.c
================================================
#include "terraform.h"

#define Env Terraform
#include "../env_binding.h"

static int my_init(Env* env, PyObject* args, PyObject* kwargs) {
    env->size = unpack(kwargs, "size");
    env->num_agents = unpack(kwargs, "num_agents");
    env->reward_scale = unpack(kwargs, "reward_scale");
    env->reset_frequency = unpack(kwargs, "reset_frequency");
    init(env);
    return 0;
}

static int my_log(PyObject* dict, Log* log) {
    assign_to_dict(dict, "perf", log->perf);
    assign_to_dict(dict, "score", log->score);
    assign_to_dict(dict, "episode_return", log->episode_return);
    assign_to_dict(dict, "episode_length", log->episode_length);
    assign_to_dict(dict, "quadrant_progress", log->quadrant_progress);
    return 0;
}



================================================
FILE: pufferlib/ocean/terraform/simplex.h
================================================
// Original work (noise library) Copyright (c) 2008 Casey Duncan
// Modified work (vec_noise library) Copyright (c) 2017 Zev Benjamin
// Single-file C port (this file) Copyright (c) 2024 Joseph Suarez
// Distributed under the MIT license. This is a simple copy-paste job.
// I did this because the original code mixed Python bindings into the
// C source, so there wasn't a clean way to use it as a C standalone.

#include <math.h>
const float GRAD3[][3] = {
    {1,1,0},{-1,1,0},{1,-1,0},{-1,-1,0},
    {1,0,1},{-1,0,1},{1,0,-1},{-1,0,-1},
    {0,1,1},{0,-1,1},{0,1,-1},{0,-1,-1},
    {1,0,-1},{-1,0,-1},{0,-1,1},{0,1,1}};

const float GRAD4[][4] = {
    {0,1,1,1}, {0,1,1,-1}, {0,1,-1,1}, {0,1,-1,-1},
    {0,-1,1,1}, {0,-1,1,-1}, {0,-1,-1,1}, {0,-1,-1,-1},
    {1,0,1,1}, {1,0,1,-1}, {1,0,-1,1}, {1,0,-1,-1},
    {-1,0,1,1}, {-1,0,1,-1}, {-1,0,-1,1}, {-1,0,-1,-1},
    {1,1,0,1}, {1,1,0,-1}, {1,-1,0,1}, {1,-1,0,-1},
    {-1,1,0,1}, {-1,1,0,-1}, {-1,-1,0,1}, {-1,-1,0,-1},
    {1,1,1,0}, {1,1,-1,0}, {1,-1,1,0}, {1,-1,-1,0},
    {-1,1,1,0}, {-1,1,-1,0}, {-1,-1,1,0}, {-1,-1,-1,0}};

// At the possible cost of unaligned access, we use char instead of
// int here to try to ensure that this table fits in L1 cache
const unsigned char PERM[] = {
  151, 160, 137, 91, 90, 15, 131, 13, 201, 95, 96, 53, 194, 233, 7, 225, 140,
  36, 103, 30, 69, 142, 8, 99, 37, 240, 21, 10, 23, 190, 6, 148, 247, 120,
  234, 75, 0, 26, 197, 62, 94, 252, 219, 203, 117, 35, 11, 32, 57, 177, 33,
  88, 237, 149, 56, 87, 174, 20, 125, 136, 171, 168, 68, 175, 74, 165, 71,
  134, 139, 48, 27, 166, 77, 146, 158, 231, 83, 111, 229, 122, 60, 211, 133,
  230, 220, 105, 92, 41, 55, 46, 245, 40, 244, 102, 143, 54, 65, 25, 63, 161,
  1, 216, 80, 73, 209, 76, 132, 187, 208, 89, 18, 169, 200, 196, 135, 130,
  116, 188, 159, 86, 164, 100, 109, 198, 173, 186, 3, 64, 52, 217, 226, 250,
  124, 123, 5, 202, 38, 147, 118, 126, 255, 82, 85, 212, 207, 206, 59, 227,
  47, 16, 58, 17, 182, 189, 28, 42, 223, 183, 170, 213, 119, 248, 152, 2, 44,
  154, 163, 70, 221, 153, 101, 155, 167, 43, 172, 9, 129, 22, 39, 253, 19, 98,
  108, 110, 79, 113, 224, 232, 178, 185, 112, 104, 218, 246, 97, 228, 251, 34,
  242, 193, 238, 210, 144, 12, 191, 179, 162, 241, 81, 51, 145, 235, 249, 14,
  239, 107, 49, 192, 214, 31, 181, 199, 106, 157, 184, 84, 204, 176, 115, 121,
  50, 45, 127, 4, 150, 254, 138, 236, 205, 93, 222, 114, 67, 29, 24, 72, 243,
  141, 128, 195, 78, 66, 215, 61, 156, 180, 151, 160, 137, 91, 90, 15, 131,
  13, 201, 95, 96, 53, 194, 233, 7, 225, 140, 36, 103, 30, 69, 142, 8, 99, 37,
  240, 21, 10, 23, 190, 6, 148, 247, 120, 234, 75, 0, 26, 197, 62, 94, 252,
  219, 203, 117, 35, 11, 32, 57, 177, 33, 88, 237, 149, 56, 87, 174, 20, 125,
  136, 171, 168, 68, 175, 74, 165, 71, 134, 139, 48, 27, 166, 77, 146, 158,
  231, 83, 111, 229, 122, 60, 211, 133, 230, 220, 105, 92, 41, 55, 46, 245,
  40, 244, 102, 143, 54, 65, 25, 63, 161, 1, 216, 80, 73, 209, 76, 132, 187,
  208, 89, 18, 169, 200, 196, 135, 130, 116, 188, 159, 86, 164, 100, 109, 198,
  173, 186, 3, 64, 52, 217, 226, 250, 124, 123, 5, 202, 38, 147, 118, 126,
  255, 82, 85, 212, 207, 206, 59, 227, 47, 16, 58, 17, 182, 189, 28, 42, 223,
  183, 170, 213, 119, 248, 152, 2, 44, 154, 163, 70, 221, 153, 101, 155, 167,
  43, 172, 9, 129, 22, 39, 253, 19, 98, 108, 110, 79, 113, 224, 232, 178, 185,
  112, 104, 218, 246, 97, 228, 251, 34, 242, 193, 238, 210, 144, 12, 191, 179,
  162, 241, 81, 51, 145, 235, 249, 14, 239, 107, 49, 192, 214, 31, 181, 199,
  106, 157, 184, 84, 204, 176, 115, 121, 50, 45, 127, 4, 150, 254, 138, 236,
  205, 93, 222, 114, 67, 29, 24, 72, 243, 141, 128, 195, 78, 66, 215, 61, 156,
  180};

const unsigned char SIMPLEX[][4] = {
    {0,1,2,3},{0,1,3,2},{0,0,0,0},{0,2,3,1},{0,0,0,0},{0,0,0,0},{0,0,0,0},
    {1,2,3,0},{0,2,1,3},{0,0,0,0},{0,3,1,2},{0,3,2,1},{0,0,0,0},{0,0,0,0},
    {0,0,0,0},{1,3,2,0},{0,0,0,0},{0,0,0,0},{0,0,0,0},{0,0,0,0},{0,0,0,0},
    {0,0,0,0},{0,0,0,0},{0,0,0,0},{1,2,0,3},{0,0,0,0},{1,3,0,2},{0,0,0,0},
    {0,0,0,0},{0,0,0,0},{2,3,0,1},{2,3,1,0},{1,0,2,3},{1,0,3,2},{0,0,0,0},
    {0,0,0,0},{0,0,0,0},{2,0,3,1},{0,0,0,0},{2,1,3,0},{0,0,0,0},{0,0,0,0},
    {0,0,0,0},{0,0,0,0},{0,0,0,0},{0,0,0,0},{0,0,0,0},{0,0,0,0},{2,0,1,3},
    {0,0,0,0},{0,0,0,0},{0,0,0,0},{3,0,1,2},{3,0,2,1},{0,0,0,0},{3,1,2,0},
    {2,1,0,3},{0,0,0,0},{0,0,0,0},{0,0,0,0},{3,1,0,2},{0,0,0,0},{3,2,0,1},
    {3,2,1,0}};

#define fastfloor(n) (int)(n) - (((n) < 0.0f) & ((n) != (int)(n)))

// Fast sine/cosine functions from
// http://devmaster.net/forums/topic/4648-fast-and-accurate-sinecosine/page__st__80
// Note the input to these functions is not radians
// instead x = [0, 2] for r = [0, 2*PI]

inline float fast_sin(float x)
{
    // Convert the input value to a range of -1 to 1
    // x = x * (1.0f / PI);

    // Wrap around
    volatile float z = (x + 25165824.0f);
    x = x - (z - 25165824.0f);

    #if LOW_SINE_PRECISION
        return 4.0f * (x - x * fabsf(x));
    #else
    {
        float y = x - x * fabsf(x);
        const float Q = 3.1f;
        const float P = 3.6f;
        return y * (Q + P * fabsf(y));
    }
    #endif
}

inline float fast_cos(float x)
{
    return fast_sin(x + 0.5f);
}

// 2D simplex skew factors
#define F2 0.3660254037844386f  // 0.5 * (sqrt(3.0) - 1.0)
#define G2 0.21132486540518713f // (3.0 - sqrt(3.0)) / 6.0

float
noise2(float x, float y)
{
    int i1, j1, II, JJ, c;
    float s = (x + y) * F2;
    float i = floorf(x + s);
    float j = floorf(y + s);
    float t = (i + j) * G2;

    float xx[3], yy[3], f[3];
    float noise[3] = {0.0f, 0.0f, 0.0f};
    int g[3];

    xx[0] = x - (i - t);
    yy[0] = y - (j - t);

    i1 = xx[0] > yy[0];
    j1 = xx[0] <= yy[0];

    xx[2] = xx[0] + G2 * 2.0f - 1.0f;
    yy[2] = yy[0] + G2 * 2.0f - 1.0f;
    xx[1] = xx[0] - i1 + G2;
    yy[1] = yy[0] - j1 + G2;

    II = (int) i & 255;
    JJ = (int) j & 255;
    g[0] = PERM[II + PERM[JJ]] % 12;
    g[1] = PERM[II + i1 + PERM[JJ + j1]] % 12;
    g[2] = PERM[II + 1 + PERM[JJ + 1]] % 12;

    for (c = 0; c <= 2; c++)
        f[c] = 0.5f - xx[c]*xx[c] - yy[c]*yy[c];

    for (c = 0; c <= 2; c++)
        if (f[c] > 0)
            noise[c] = f[c]*f[c]*f[c]*f[c] * (GRAD3[g[c]][0]*xx[c] + GRAD3[g[c]][1]*yy[c]);

    return (noise[0] + noise[1] + noise[2]) * 70.0f;
}



================================================
FILE: pufferlib/ocean/terraform/terraform.c
================================================
#include "terraform.h"
#include "puffernet.h"

void allocate(Terraform* env) {
    env->observations = (float*)calloc(env->num_agents*442, sizeof(float));
    env->actions = (int*)calloc(3*env->num_agents, sizeof(int));
    env->rewards = (float*)calloc(env->num_agents, sizeof(float));
    env->terminals = (unsigned char*)calloc(env->num_agents, sizeof(unsigned char));
    init(env);
}

void free_allocated(Terraform* env) {
    free(env->observations);
    free(env->actions);
    free(env->rewards);
    free(env->terminals);
    free_initialized(env);
}

typedef struct TerraformNet TerraformNet;
struct TerraformNet {
    int num_agents;
    float* local_obs2d;
    float* global_obs2d;
    float* obs_1d;
    Conv2D* local_conv1;
    ReLU* relu1;
    Conv2D* local_conv2;
    ReLU* relu2;
    Conv2D* global_conv1;
    ReLU* relu3;
    Conv2D* global_conv2;
    ReLU* relu4;
    Linear* flat;
    CatDim1* cat1;
    CatDim1* cat2;
    Linear* proj;
    ReLU* relu5;
    LSTM* lstm;
    Linear* actor;
    Linear* value_fn;
    Multidiscrete* multidiscrete;
};
TerraformNet* init_terranet(Weights* weights, int num_agents, int vision_size, int quadrant_size) {
    TerraformNet* net = calloc(1, sizeof(TerraformNet));
    int hidden_size = 512;
    int cnn_channels = 32;
    int local_conv1_output_size = 3;
    int local_conv2_output_size = 1;
    int global_conv1_output_size = 4;
    int global_conv2_output_size = 2;
    int local_cnn_flat_size = cnn_channels * (local_conv2_output_size * local_conv2_output_size);
    int global_cnn_flat_size = cnn_channels * (global_conv2_output_size * global_conv2_output_size);

    net->num_agents = num_agents;
    net->local_obs2d = calloc(num_agents * vision_size * vision_size * 2, sizeof(float)); // 2 channels - height map & deltas
    net->global_obs2d = calloc(num_agents * quadrant_size * quadrant_size * 2, sizeof(float)); // 2 channels - global volume map and agent location
    net->obs_1d = calloc(num_agents * 5, sizeof(float)); // 2 additional features

    net->local_conv1 = make_conv2d(weights, num_agents, vision_size, vision_size, 2, cnn_channels, 5, 3);
    net->relu1 = make_relu(num_agents, cnn_channels * local_conv1_output_size * local_conv1_output_size);
    net->local_conv2 = make_conv2d(weights, num_agents, local_conv1_output_size, local_conv1_output_size, cnn_channels, cnn_channels, 3, 1);
    net->relu2 = make_relu(num_agents, cnn_channels * local_conv2_output_size * local_conv2_output_size);
    net->global_conv1 = make_conv2d(weights, num_agents, quadrant_size, quadrant_size, 2, cnn_channels, 3, 1);
    net->relu3 = make_relu(num_agents, cnn_channels * global_conv1_output_size * global_conv1_output_size);
    net->global_conv2 = make_conv2d(weights, num_agents, global_conv1_output_size, global_conv1_output_size, cnn_channels, cnn_channels, 3, 1);
    net->relu4 = make_relu(num_agents, cnn_channels * global_conv2_output_size * global_conv2_output_size);
    net->flat = make_linear(weights, num_agents, 5, hidden_size);
    net->cat1 = make_cat_dim1(num_agents, local_cnn_flat_size, global_cnn_flat_size);
    net->cat2 = make_cat_dim1(num_agents, local_cnn_flat_size + global_cnn_flat_size, hidden_size);
    net->proj = make_linear(weights, num_agents, local_cnn_flat_size + global_cnn_flat_size + hidden_size, hidden_size);
    net->relu5 = make_relu(num_agents, hidden_size);
    net->actor = make_linear(weights, num_agents, hidden_size, 13); // +1 for pass move
    net->value_fn = make_linear(weights, num_agents, hidden_size, 1);
    net->lstm = make_lstm(weights, num_agents, hidden_size, 512);
    int logit_sizes[3] = {5, 5, 3};
    net->multidiscrete = make_multidiscrete(num_agents, logit_sizes, 3);
    return net;
}

void free_terranet(TerraformNet* net) {
    free(net->local_obs2d);
    free(net->global_obs2d);
    free(net->obs_1d);
    free(net->local_conv1);
    free(net->relu1);
    free(net->local_conv2);
    free(net->relu2);
    free(net->global_conv1);
    free(net->relu3);
    free(net->global_conv2);
    free(net->relu4);
    free(net->flat);
    free(net->cat1);
    free(net->cat2);
    free(net->proj);
    free(net->relu5);
    free(net->lstm);
    free(net->actor);
    free(net->value_fn);
    free(net);
}

void forward(TerraformNet* net, float* observations, int* actions, int vision_size, int quadrant_size) {
    int local_vision_size = vision_size * vision_size;  
    int global_quadrant_size = quadrant_size * quadrant_size;
    // Clear previous observations
    memset(net->local_obs2d, 0, net->num_agents * vision_size * vision_size * 2 * sizeof(float));
    memset(net->global_obs2d, 0, net->num_agents * quadrant_size * quadrant_size * 2 * sizeof(float));
    memset(net->obs_1d, 0, net->num_agents * 5 * sizeof(float));
    
    // Reshape observations into 2D boards and additional features
    float (*local_obs2d)[2][vision_size][vision_size] = (float (*)[2][vision_size][vision_size])net->local_obs2d;
    float (*global_obs2d)[2][quadrant_size][quadrant_size] = (float (*)[2][quadrant_size][quadrant_size])net->global_obs2d;
    float (*obs_1d)[5] = (float (*)[5])net->obs_1d;
    
    for (int b = 0; b < net->num_agents; b++) {
        int b_offset = b * (local_vision_size * 2 + global_quadrant_size * 2 + 5);  // offset for each batch
        
        // Process local vision board
        int obs_2d_idx = 0;
        for(int z = 0; z < 2; z++) {
            for (int i = 0; i < vision_size; i++) {
                for (int j = 0; j < vision_size; j++) {
                    local_obs2d[b][z][i][j] = observations[b_offset + obs_2d_idx];
                    obs_2d_idx++;
                }
            }
        }

        // Process additional features
        obs_1d[b][0] = observations[b_offset + obs_2d_idx];
        obs_1d[b][1] = observations[b_offset + obs_2d_idx + 1];
        obs_1d[b][2] = observations[b_offset + obs_2d_idx + 2];
        obs_1d[b][3] = observations[b_offset + obs_2d_idx + 3];
        obs_1d[b][4] = observations[b_offset + obs_2d_idx + 4];
        obs_2d_idx += 5;
        
        // Process global quadrant board
        for(int z = 0; z < 2; z++) {
            for (int i = 0; i < quadrant_size; i++) {
                for (int j = 0; j < quadrant_size; j++) {
                    global_obs2d[b][z][i][j] = observations[b_offset + obs_2d_idx];
                    obs_2d_idx++;
                }
            }
        }
    }

    // Forward pass through the network
    // local convs
    conv2d(net->local_conv1, net->local_obs2d);
    relu(net->relu1, net->local_conv1->output);
    conv2d(net->local_conv2, net->relu1->output);
    relu(net->relu2, net->local_conv2->output);
    // global convs
    conv2d(net->global_conv1, net->global_obs2d);
    relu(net->relu3, net->global_conv1->output);
    conv2d(net->global_conv2, net->relu3->output);
    relu(net->relu4, net->global_conv2->output);

    linear(net->flat, net->obs_1d);

    cat_dim1(net->cat1, net->relu2->output, net->relu4->output);
    cat_dim1(net->cat2, net->cat1->output, net->flat->output);
    linear(net->proj, net->cat2->output);
    relu(net->relu5, net->proj->output);
    
    lstm(net->lstm, net->relu5->output);
    linear(net->actor, net->lstm->state_h);
    linear(net->value_fn, net->lstm->state_h);

    // Get action by taking argmax of actor output
    softmax_multidiscrete(net->multidiscrete, net->actor->output, actions);

}

void demo() {
    Weights* weights = load_weights("resources/terraform/puffer_terraform_weights.bin", 2476814);
    TerraformNet* net = init_terranet(weights, 1, 11, 6);
    srand(time(NULL));
    Terraform env = {.size = 64, .num_agents = 1, .reset_frequency = 8192, .reward_scale = 0.04f};
    allocate(&env);

    c_reset(&env);
    c_render(&env);
    while (!WindowShouldClose()) {
        forward(net, env.observations, env.actions, 11, 6);
        int policy_actions[3] = {env.actions[0], env.actions[1], env.actions[2]};
        
        if(IsKeyDown(KEY_LEFT_SHIFT)) {
            // When shift is held, stop the dozer
            env.actions[0] = 2;  // Stop vertical movement
            env.actions[1] = 2;  // Stop horizontal movement
            env.actions[2] = 0;  // no scoop or drop
            // Override with keyboard controls if keys are pressed
            if (IsKeyPressed(KEY_UP) || IsKeyPressed(KEY_W)) env.actions[0] = 4;
            if (IsKeyPressed(KEY_DOWN) || IsKeyPressed(KEY_S)) env.actions[0] = 0;
            if (IsKeyDown(KEY_LEFT) || IsKeyDown(KEY_A)) env.actions[1] = 0;
            if (IsKeyDown(KEY_RIGHT) || IsKeyDown(KEY_D)) env.actions[1] = 4;
            if (IsKeyPressed(KEY_SPACE)) env.actions[2] = 1;
            if (IsKeyPressed(KEY_ENTER)) env.actions[2] = 2;
        }
        c_step(&env);
        c_render(&env);
    }
    free_allocated(&env);
    close_client(env.client);
    free_terranet(net);
    free(weights);
}

void test_performance(int timeout) {
    srand(time(NULL));
    Terraform env = {
        .size = 64,
        .num_agents = 8,
        .reset_frequency = 512,
        .reward_scale = 0.01f,
    };
    allocate(&env);
    c_reset(&env);

    int start = time(NULL);
    int num_steps = 0;
    while (time(NULL) - start < timeout) {
        for (int i = 0; i < env.num_agents; i++) {
            env.actions[3*i] = rand() % 5;
            env.actions[3*i + 1] = rand() % 5;
            env.actions[3*i + 2] = rand() % 3;
        }

        c_step(&env);
        num_steps++;
    }

    int end = time(NULL);
    float sps = num_steps * env.num_agents / (end - start);
    printf("Test Environment SPS: %f\n", sps);
    free_allocated(&env);
}

int main() {
    // test_performance(10);
    demo();
}




================================================
FILE: pufferlib/ocean/terraform/terraform.h
================================================
#include <stdlib.h>
#include <stdio.h>
#include <string.h>
#include <unistd.h>
#include <time.h>
#include <math.h>
#include <limits.h>
#include <float.h>
#include <time.h>
#include "raylib.h"
#include "simplex.h"
#include "raymath.h"
#include "rlgl.h"

#if defined(PLATFORM_DESKTOP)
    #define GLSL_VERSION 330
#else
    #define GLSL_VERSION 100
#endif

const unsigned char NOOP = 0;
const unsigned char DOWN = 1;
const unsigned char UP = 2;
const unsigned char LEFT = 3;
const unsigned char RIGHT = 4;

const unsigned char EMPTY = 0;
const unsigned char AGENT = 1;
const unsigned char TARGET = 2;

#define MAX_DIRT_HEIGHT 32.0f
#define BUCKET_MAX_HEIGHT 1.0f
#define DOZER_MAX_V 2.0f
#define DOZER_CAPACITY 100.0f
#define BUCKET_OFFSET 2.0f
#define BUCKET_WIDTH 2.5f
#define BUCKET_LENGTH 0.8f
#define BUCKET_HEIGHT 1.0f
#define SCOOP_SIZE 1
#define VISION 5
#define OBSERVATION_SIZE (2*VISION + 1)
#define TOTAL_OBS (OBSERVATION_SIZE*OBSERVATION_SIZE + 4)
#define DOZER_STEP_HEIGHT 5.0f 
struct timespec ts;

typedef struct Log Log;
struct Log {
    float perf;
    float score;
    float episode_return;
    float episode_length;
    float n;
    float quadrant_progress;
};

typedef struct Dozer {
    float x;
    float y;
    float z;
    float v;
    float heading;
    float bucket_height;
    float bucket_tilt;
    float load;
    int target_quadrant;
    int* load_indices;
    float quadrant_progress;
    float highest_quadrant_progress;
    float target_quadrant_delta;
} Dozer;
 
typedef struct Client Client;
typedef struct Terraform {
    Log log;
    Log* agent_logs;
    Client* client;
    Dozer* dozers;
    float* observations;
    int* actions;
    float* rewards;
    float* returns;
    unsigned char* terminals;
    int size;
    int tick;
    float* orig_map;
    float* map;
    float* target_map;
    int num_agents;
    int reset_frequency;
    float reward_scale;
    float initial_total_delta;  
    float current_total_delta; 
    float delta_progress;
    int* stuck_count;
    int* grid_indices;
    int num_quadrants;
    float* quadrant_deltas;
    float* current_quadrant_deltas;
    float quadrants_solved;
    int* complete_quadrants;
    int* in_progress_quadrants;
    float* volume_deltas;
    float* quadrant_volume_deltas;
    float* quadrant_centroids;
} Terraform;

float randf(float min, float max) {
    return min + (max - min)*(float)rand()/(float)RAND_MAX;
}

void perlin_noise(float* map, int width, int height,
        float base_frequency, int octaves, int offset_x, int offset_y, float glob_scale) {
    float frequencies[octaves];
    for (int i = 0; i < octaves; i++) {
        frequencies[i] = base_frequency*pow(2, i);
    }

    float min_value = FLT_MAX;
    float max_value = FLT_MIN;
    for (int r = 0; r < height; r++) {
        for (int c = 0; c < width; c++) {
            int adr = r*width + c;
            for (int oct = 0; oct < octaves; oct++) {
                float freq = frequencies[oct];
                map[adr] += (1.0/pow(2, oct))*noise2(freq*c + offset_x, freq*r + offset_y);
            }
            float val = map[adr];
            if (val < min_value) {
                min_value = val;
            }
            if (val > max_value) {
                max_value = val;
            }
        }
    }

    float scale = 1.0/(max_value - min_value);
    for (int r = 0; r < height; r++) {
        for (int c = 0; c < width; c++) {
            int adr = r*width + c;
            map[adr] = glob_scale * scale * (map[adr] - min_value);
            if (map[adr] < 34.0f) {
                map[adr] = 0.0f;
            } else {
                map[adr] -= 34.0f;
            }
        }
    }
}

int map_idx(Terraform* env, float x, float y) {
    return env->size*(int)y + (int)x;
}

void calculate_total_delta(Terraform* env) {
    env->initial_total_delta = 0.0f;
    env->current_total_delta = 0.0f;
    // Calculate total volume in original and target maps
    float original_volume = 0.0f;
    float target_volume = 0.0f;
    for (int i = 0; i < env->size * env->size; i++) {
        original_volume += env->orig_map[i];
        target_volume += env->target_map[i];
    }
    
    float scale_factor = target_volume / original_volume;
    for (int i = 0; i < env->size * env->size; i++) {
        if(env->orig_map[i] * scale_factor > MAX_DIRT_HEIGHT) {
            env->orig_map[i] = MAX_DIRT_HEIGHT;
        } else {
            env->orig_map[i] *= scale_factor;
        }
    }

    for (int i = 0; i < env->size * env->size; i++) {
        float delta = fabsf(env->orig_map[i] - env->target_map[i]);
        env->initial_total_delta += delta;
        env->quadrant_deltas[env->grid_indices[i]] += delta;
        env->quadrant_volume_deltas[env->grid_indices[i]] += (env->orig_map[i] - env->target_map[i]);
        env->volume_deltas[env->grid_indices[i]] += (env->orig_map[i] - env->target_map[i]);
    }
    memcpy(env->current_quadrant_deltas, env->quadrant_deltas, env->num_quadrants*sizeof(float));
    env->current_total_delta = env->initial_total_delta;
    env->delta_progress = 0.0f;
}

void assign_grid_indices(Terraform* env) {
    int num_quads_x = (env->size + 10) / 11;
    int num_quads_y = (env->size + 10) / 11;
    for (int i = 0; i < env->size*env->size; i++) {
        int y = i / env->size;
        int x = i % env->size;
        int quad_x = x / 11;
        int quad_y = y / 11;
        int grid_index = quad_y * num_quads_x + quad_x;
        env->grid_indices[i] = grid_index;
    }
    env->num_quadrants = num_quads_x * num_quads_y;
}

void assign_quadrant_centroids(Terraform* env) {
    int num_quads_x = (env->size + 10) / 11;
    int num_quads_y = (env->size + 10) / 11;
    for (int i = 0; i < env->num_quadrants; i++) {
        float centroid_x = (i % num_quads_x) * 11 + 5;
        float centroid_y = (i / num_quads_x) * 11 + 5;
        env->quadrant_centroids[i*2] = centroid_x;
        env->quadrant_centroids[i*2 + 1] = centroid_y;
    }
}

void init(Terraform* env) {
    env->orig_map = calloc(env->size*env->size, sizeof(float));
    env->map = calloc(env->size*env->size, sizeof(float));
    env->target_map = calloc(env->size*env->size, sizeof(float));
    env->grid_indices = calloc(env->size*env->size, sizeof(int));
    assign_grid_indices(env);
    env->quadrant_centroids = calloc(env->num_quadrants*2, sizeof(float));
    assign_quadrant_centroids(env);
    env->quadrant_deltas = calloc(env->num_quadrants, sizeof(float));
    env->complete_quadrants = calloc(env->num_quadrants, sizeof(int));
    env->in_progress_quadrants = calloc(env->num_quadrants*(env->num_agents+1), sizeof(int));
    env->current_quadrant_deltas = calloc(env->num_quadrants, sizeof(float));
    env->quadrant_volume_deltas = calloc(env->num_quadrants, sizeof(float));
    env->volume_deltas = calloc(env->num_quadrants, sizeof(float));
    env->agent_logs = calloc(env->num_agents, sizeof(Log));
    // for (int i = 0; i < env->size*env->size; i++) {
    //     env->target_map[i] = 1;  // Initialize all to empty
    // }

    // Calculate grid dimensions for quadrants
    const int QUADRANT_SIZE = 11;
    const int MIN_SPACING = 3;
    const int TOTAL_SPACE = QUADRANT_SIZE + MIN_SPACING;

    // Calculate how many quadrants we can fit in each dimension
    int num_quadrants_x = (env->size - MIN_SPACING) / TOTAL_SPACE;
    int num_quadrants_y = (env->size - MIN_SPACING) / TOTAL_SPACE;

    // Place quadrants in a grid pattern
    for (int grid_y = 0; grid_y < num_quadrants_y; grid_y++) {
        for (int grid_x = 0; grid_x < num_quadrants_x; grid_x++) {
            // Calculate starting position for this quadrant
            int start_x = MIN_SPACING + grid_x * TOTAL_SPACE;
            int start_y = MIN_SPACING + grid_y * TOTAL_SPACE;
            
            // Place the 11x11 quadrant
            for (int y = 0; y < QUADRANT_SIZE; y++) {
                for (int x = 0; x < QUADRANT_SIZE; x++) {
                    int pos_x = start_x + x;
                    int pos_y = start_y + y;
                    if (pos_x < env->size && pos_y < env->size) {
                        env->target_map[pos_y * env->size + pos_x] = 1;  // Mark as target
                    }
                }
            }
        }
    } 
    env->dozers = calloc(env->num_agents, sizeof(Dozer));
    for (int i = 0; i < env->num_agents; i++) {
        env->dozers[i].load_indices = calloc((2*SCOOP_SIZE + 1)*(2*SCOOP_SIZE + 1), sizeof(int));
        for (int j = 0; j < (2*SCOOP_SIZE + 1)*(2*SCOOP_SIZE + 1); j++) {
            env->dozers[i].load_indices[j] = -1;
        }
        env->dozers[i].quadrant_progress = 0.0f;
        env->dozers[i].highest_quadrant_progress = 0.0f;
    }
    clock_gettime(CLOCK_REALTIME, &ts);
    unsigned int base_seed = (unsigned int)(ts.tv_nsec ^ ts.tv_sec ^ getpid());
    unsigned int seed1 = base_seed;
    unsigned int seed2 = base_seed + 99991;
    srand(seed1);
    int offset_x1 = rand() % 10000;
    int offset_y1 = rand() % 10000;
    srand(seed2);
    int offset_x2 = rand() % 10000;
    int offset_y2 = rand() % 10000;
    perlin_noise(env->orig_map, env->size, env->size, 1.0/(env->size / 4.0), 8, offset_x1, offset_y1, MAX_DIRT_HEIGHT+20);
    // perlin_noise(env->target_map, env->size, env->size, 1.0/(env->size / 4.0), 8, offset_x2, offset_y2, MAX_DIRT_HEIGHT+55);
    env->returns = calloc(env->num_agents, sizeof(float));
    calculate_total_delta(env);
    env->stuck_count = calloc(env->num_agents, sizeof(int));
    env->tick = rand() % 512;
    env->quadrants_solved = 0.0f;
}

void free_initialized(Terraform* env) {
    free(env->orig_map);
    free(env->map);
    for (int i = 0; i < env->num_agents; i++) {
        free(env->dozers[i].load_indices);
    }
    free(env->dozers);
    free(env->returns);
    free(env->target_map);
    free(env->stuck_count);
    free(env->quadrant_deltas);
    free(env->complete_quadrants);
    free(env->grid_indices);
    free(env->current_quadrant_deltas);
    free(env->in_progress_quadrants);
    free(env->agent_logs);
    free(env->quadrant_volume_deltas);
    free(env->volume_deltas);
    free(env->quadrant_centroids);
}

void add_log(Terraform* env, Log* agent_log) {
    env->log.perf += agent_log->perf;
    env->log.score += agent_log->score;
    env->log.episode_length += agent_log->episode_length;
    env->log.episode_return += agent_log->episode_return;
    env->log.n++;
    env->log.quadrant_progress += agent_log->quadrant_progress;
}

void compute_all_observations(Terraform* env) {
    int dialate = 1;
    int max_obs = 319;
    float (*observations)[max_obs] = (float(*)[max_obs])env->observations; 
    int channel_diff_offset = (2*VISION+1)*(2*VISION+1);
    for (int i = 0; i < env->num_agents; i++) {
        int obs_idx = 0;
        float* obs = &observations[i][obs_idx];
        int x_offset = env->dozers[i].x - dialate*VISION;
        int y_offset = env->dozers[i].y - dialate*VISION;
        for (int y = 0; y < 2 * dialate * VISION + 1; y += dialate) {  // ROW loop (Y-axis)
            for (int x = 0; x < 2 * dialate * VISION + 1; x += dialate) {  // COLUMN loop (X-axis)
                int map_x = x_offset + x;
                int map_y = y_offset + y;

                if (map_x < 0 || map_x >= env->size || map_y < 0 || map_y >= env->size) {
                    obs[obs_idx] = 0;
                    obs[obs_idx + channel_diff_offset] = 0;
                    obs_idx++;
                    continue;
                }
                int map_idx = map_y * env->size + map_x; 
                obs[obs_idx] = ((float)env->map[map_idx]) / MAX_DIRT_HEIGHT;
                float diff = ((float)(env->target_map[map_idx] - env->map[map_idx])) / (MAX_DIRT_HEIGHT * 2.0f);
                obs[obs_idx + channel_diff_offset] = diff;
                obs_idx++;
            }
        }
        obs_idx += channel_diff_offset;
        
        Dozer* dozer = &env->dozers[i];
        obs[obs_idx++] = dozer->x / env->size;
        obs[obs_idx++] = dozer->y / env->size;
        obs[obs_idx++] = (dozer->v) / (DOZER_MAX_V);
        // This is -5?
        obs[obs_idx++] = (dozer->heading) / (2*PI);
        obs[obs_idx++] = dozer->load / DOZER_CAPACITY;
        // float goal_x = env->quadrant_centroids[dozer->target_quadrant*2];
        // float goal_y = env->quadrant_centroids[dozer->target_quadrant*2+1];
        // float rel_x = goal_x - dozer->x;
        // float rel_y = goal_y - dozer->y;
        // float max_dist = sqrtf(2) * env->size;
        // obs[obs_idx++] = rel_x / max_dist;
        // obs[obs_idx++] = rel_y / max_dist;

        // Current and target quadrant - 249
        // obs[obs_idx++] = (float)dozer->target_quadrant / env->num_quadrants;
        // obs[obs_idx++] = (float)env->grid_indices[map_idx(env, dozer->x, dozer->y)] / env->num_quadrants;
        // relative directions to target quadrant center - 251
        for (int q = 0; q < env->num_quadrants; q++) {
            obs[obs_idx++] = env->quadrant_volume_deltas[q] / 121.0f;
        }
        float location_conv[env->num_quadrants];
        memset(location_conv, 0, env->num_quadrants*sizeof(float));
        location_conv[env->grid_indices[map_idx(env, dozer->x, dozer->y)]] = 1.0f;
        memcpy(obs + obs_idx, location_conv, env->num_quadrants*sizeof(float));
        obs_idx += env->num_quadrants;
    }
}

void c_reset(Terraform* env) {
    memcpy(env->map, env->orig_map, env->size*env->size*sizeof(float));
    memset(env->observations, 0, env->num_agents*319*sizeof(float));
    memset(env->returns, 0, env->num_agents*sizeof(float));
    env->tick = 0;
    env->current_total_delta = env->initial_total_delta;
    env->delta_progress = 0.0f;
    env->quadrants_solved = 0.0f;
    memset(env->stuck_count, 0, env->num_agents*sizeof(int));
    memcpy(env->quadrant_volume_deltas, env->volume_deltas, env->num_quadrants*sizeof(float));
    memset(env->complete_quadrants, 0, env->num_quadrants*sizeof(int));

    int num_quadrants_to_precomplete = rand() % 5 + 25; // e.g. 30 to 34
    
    // Create array of available quadrants
    int available[env->num_quadrants];
    int num_available = 0;
    for (int i = 0; i < env->num_quadrants; i++) {
        if (!env->complete_quadrants[i]) {
            available[num_available++] = i;
        }
    }

    // Complete exactly num_quadrants_to_precomplete quadrants
    // for (int i = 0; i < num_quadrants_to_precomplete && num_available > 0; i++) {
    //     // Pick random quadrant from remaining available ones
    //     int idx = rand() % num_available;
    //     int quad = available[idx];
        
    //     // Complete the quadrant
    //     for (int j = 0; j < env->size*env->size; j++) {
    //         if(env->grid_indices[j] == quad) {
    //             env->map[j] = env->target_map[j];
    //         }
    //     }

    //     env->complete_quadrants[quad] = 1;
    //     env->current_quadrant_deltas[quad] = 0.0f;
    //     env->quadrant_volume_deltas[quad] = 0.0f;
    //     env->quadrants_solved++;

    //     // Remove used quadrant by swapping with last available one
    //     available[idx] = available[--num_available];
    // }

    // // adjust remaining volume
    // float remaining_target_sum = 0.0f;
    // float remaining_map_sum = 0.0f;
    // for(int i = 0; i < env->size*env->size; i++) {
    //     int quad = env->grid_indices[i];
    //     if (!env->complete_quadrants[quad]) {
    //         remaining_target_sum += env->target_map[i];
    //         remaining_map_sum += env->map[i];
    //     }
    // }
    // // 2. Compute scale factor
    // float scale = (remaining_map_sum > 0.0f) ? (remaining_target_sum / remaining_map_sum) : 1.0f;
    // for (int i = 0; i < env->size * env->size; i++) {
    //     int quad = env->grid_indices[i];
    //     if (!env->complete_quadrants[quad]) {
    //         env->map[i] *= scale;
    //         if (env->map[i] > MAX_DIRT_HEIGHT) env->map[i] = MAX_DIRT_HEIGHT;
    //     }
    // }
    int available_quadrants[env->num_quadrants - (int)env->quadrants_solved];
    int available_quadrants_count = 0;
    for (int i = 0; i < env->num_quadrants; i++) {
        if (!env->complete_quadrants[i]) {
            available_quadrants[available_quadrants_count++] = i;
        }
    }
    for (int i = 0; i < env->num_agents; i++) {
        Dozer temp = {0};
        env->agent_logs[i] = (Log){0};
        temp.load_indices = env->dozers[i].load_indices;
        env->dozers[i] = temp;
        do {
            env->dozers[i].x = rand() % env->size;
            env->dozers[i].y = rand() % env->size;
        } while (env->map[map_idx(env, env->dozers[i].x, env->dozers[i].y)] != 0.0f);
        for (int j = 0; j < (2*SCOOP_SIZE + 1)*(2*SCOOP_SIZE + 1); j++) {
            env->dozers[i].load_indices[j] = -1;
        }
    }
    compute_all_observations(env);
}

void illegal_action(Terraform* env, int agent_idx) {
    env->rewards[agent_idx] += -0.05f;
    env->returns[agent_idx] += -0.05f;
    env->agent_logs[agent_idx].episode_return += -0.05f;
}

float scoop_dirt(Terraform* env, float x, float y, int bucket_atn, int agent_idx, Dozer* dozer){
    int scoop_idx = map_idx(env, x, y);
    float map_height = env->map[scoop_idx];
    float target_height = env->target_map[scoop_idx];
    float delta_pre = fabsf(map_height - target_height);
    float load_pre = dozer->load;

    if (bucket_atn == 0) {
        return 0.0f;
    } else if (bucket_atn == 1) { // Load
        // Can't load while backing up
        if (dozer->v < 0) {
            illegal_action(env, agent_idx);
            return 0.0f;
        }

        if (dozer->load >= DOZER_CAPACITY) {
            illegal_action(env, agent_idx);
            return 0.0f;
        }
        // if it aint broken dont fix it penalty
        // if (env->complete_quadrants[env->grid_indices[scoop_idx]]) {
        //     env->rewards[agent_idx] += (-1.0f / (SCOOP_SIZE*2 + 1));
        //     env->returns[agent_idx] += (-1.0f / (SCOOP_SIZE*2 + 1));
        //     env->agent_logs[agent_idx].episode_return += (-1.0f / (SCOOP_SIZE*2 + 1));
        //     env->complete_quadrants[env->grid_indices[scoop_idx]] = 0;
        //     env->quadrants_solved -= 1.0f;
        // }

        // Load up to 1 unit of dirt
        float load_amount = 1.0f;
        if (map_height <= 1.0f) {
            load_amount = map_height;
        }                    

        // Don't overload the bucket
        if (dozer->load + load_amount > DOZER_CAPACITY) {
            load_amount = DOZER_CAPACITY - dozer->load;
        }

        dozer->load += load_amount;
        env->map[scoop_idx] -= load_amount;
        map_height -= load_amount;
        env->quadrant_volume_deltas[env->grid_indices[scoop_idx]] -= load_amount;
    } else if (bucket_atn == 2) { // Unload
        // Can't unload while moving forward
        if (dozer->v > 0) {
            illegal_action(env, agent_idx);
            return 0.0f;
        }

        if (dozer->load == 0) {
            illegal_action(env, agent_idx);
            return 0.0f;
        }
        // if it aint broken dont fix it penalty
        // if (env->complete_quadrants[env->grid_indices[scoop_idx]]) {
        //     env->rewards[agent_idx] += (-1.0f / (SCOOP_SIZE*2 + 1));
        //     env->returns[agent_idx] += (-1.0f / (SCOOP_SIZE*2 + 1));
        //     env->agent_logs[agent_idx].episode_return += (-1.0f / (SCOOP_SIZE*2 + 1));
        //     env->complete_quadrants[env->grid_indices[scoop_idx]] = 0;
        //     env->quadrants_solved -= 1.0f;
        // }

        float unload_amount = 1.0f;
        if (dozer->load < unload_amount) {
            unload_amount = dozer->load;
        }

        if (map_height + unload_amount > MAX_DIRT_HEIGHT) {
            unload_amount = MAX_DIRT_HEIGHT - map_height;
        }

        dozer->load -= unload_amount;
        env->map[scoop_idx] += unload_amount;
        map_height += unload_amount;
        env->quadrant_volume_deltas[env->grid_indices[scoop_idx]] += unload_amount;
    }

    // Reward for terraforming towards target map
    float delta_post = fabsf(map_height - target_height);
    float load_post = dozer->load;
    env->current_total_delta += (delta_post - delta_pre);
    float normalize_value = (2*SCOOP_SIZE + 1)*(2*SCOOP_SIZE + 1) + 1;
    float reward = (delta_pre + env->reward_scale*load_pre) - (delta_post + env->reward_scale*load_post);
    reward /= normalize_value;
    return reward;
}

void c_step(Terraform* env) {
    env->tick += 1;
    if ((env->reset_frequency && env->tick % env->reset_frequency == 0) || env->current_total_delta < 0.01f) {
        if(env->current_total_delta < 0.01f) {
            for (int i = 0; i < env->num_agents; i++) {
                env->rewards[i] = 1.0f;
                env->returns[i] = 1.0f;
                env->agent_logs[i].episode_return += 1.0f;
            }
        }
        for (int i = 0; i < env->num_agents; i++) {
            env->agent_logs[i].episode_length = env->tick;
            env->agent_logs[i].score = env->delta_progress;
            env->agent_logs[i].perf = env->delta_progress;
            add_log(env, &env->agent_logs[i]);
        }
        c_reset(env);
        return;
    }

    memset(env->terminals, 0, env->num_agents*sizeof(unsigned char));
    memset(env->rewards, 0, env->num_agents*sizeof(float));
    int (*actions)[3] = (int(*)[3])env->actions; 
    for (int i = 0; i < env->num_agents; i++) {
        env->agent_logs[i].episode_length = env->tick;
        Dozer* dozer = &env->dozers[i];
        int* atn = actions[i];
        float accel = ((float)atn[0] - 2.0f) / 2.0f; // Discrete(5) -> [-1, 1]
        float steer = ((float)atn[1] - 2.0f) / 10.0f; // Discrete(5) -> [-0.2, 0.2]
        int bucket_atn = atn[2];

        float cx = dozer->x + BUCKET_OFFSET*cosf(dozer->heading);
        float cy = dozer->y + BUCKET_OFFSET*sinf(dozer->heading);
        float total_change = 0.0f;
        int load_idx = 0;
        for (int x = cx - SCOOP_SIZE; x < cx + SCOOP_SIZE; x++) {
            for (int y = cy - SCOOP_SIZE; y < cy + SCOOP_SIZE; y++) {
                if (x < 0 || x >= env->size || y < 0 || y >= env->size) {
                    env->dozers[i].load_indices[load_idx] = -1;
                    load_idx++;
                    continue;
                }
                env->dozers[i].load_indices[load_idx] = map_idx(env, x, y);
                load_idx++;
                total_change += scoop_dirt(env, x, y, bucket_atn, i, dozer);
                
            }
        }
        env->rewards[i] += total_change;
        env->returns[i] += total_change;
        env->agent_logs[i].episode_return += total_change;

        dozer->heading += steer;
        if (dozer->heading > 2*PI) {
            dozer->heading -= 2*PI;
        }
        if (dozer->heading < 0) {
            dozer->heading += 2*PI;
        }

        dozer->v += accel;
        if (dozer->v > DOZER_MAX_V) {
            dozer->v = DOZER_MAX_V;
        }
        if (dozer->v < -DOZER_MAX_V) {
            dozer->v = -DOZER_MAX_V;
        }
        int idx = map_idx(env, dozer->x, dozer->y);
        float dozer_height = env->map[idx];

        // Raytrace collision
        for (int d=0; d<dozer->v; d++) {
            float x = dozer->x + d*cosf(dozer->heading);
            float y = dozer->y + d*sinf(dozer->heading);
            if (x < 0 || x >= env->size-1 || y < 0 || y >= env->size-1) {
                continue;
            }

            int dst_idx = map_idx(env, x, y);
            float dst_height = env->map[dst_idx];
            if (fabsf(dozer_height - dst_height) > DOZER_STEP_HEIGHT) {
                dozer->v = 0;
            }
        }

        // Box collision around final destination
        float dst_x = dozer->x + dozer->v*cosf(dozer->heading);
        float dst_y = dozer->y + dozer->v*sinf(dozer->heading);
        for (int x=(int)(dst_x-1.0f); x<=(int)(dst_x+1.0f); x++) {
            for (int y=(int)(dst_y-1.0f); y<=(int)(dst_y+1.0f); y++) {
                if (x < 0 || x >= env->size-1 || y < 0 || y >= env->size-1) {
                    continue;
                }
                int dst_idx = map_idx(env, x, y);
                float dst_height = env->map[dst_idx];
                if (fabsf(dozer_height - dst_height) > DOZER_STEP_HEIGHT) {
                    dozer->v = 0;
                    env->stuck_count[i]++;
                }
            }
        }

        dozer->x += dozer->v*cosf(dozer->heading);
        dozer->y += dozer->v*sinf(dozer->heading);
        if (dozer->x < 0) {
            dozer->x = 0;
        }
        if (dozer->x >= env->size) {
            dozer->x = env->size - 1;
        }
        if (dozer->y < 0) {
            dozer->y = 0;
        }
        if (dozer->y >= env->size) {
            dozer->y = env->size - 1;
        }

        // Teleportitis
        if (env->tick % 512 == 0) {
             do {
                 env->dozers[i].x = rand() % env->size;
                 env->dozers[i].y = rand() % env->size;
                 env->stuck_count[i] = 0;
             } while (env->map[map_idx(env, env->dozers[i].x, env->dozers[i].y)] != 0.0f);
        }
 
    }
    int marked_to_skip[env->num_agents];
    memset(marked_to_skip, 0, env->num_agents*sizeof(int));
    for(int i = 0; i < env->num_agents; i++) {
        if(marked_to_skip[i]) {
            continue;
        }
        // compute delta progress
        if (env->initial_total_delta > 0) {
            env->delta_progress = 1.0f - (env->current_total_delta / env->initial_total_delta);
            env->delta_progress = fmaxf(0.0f, fminf(1.0f, env->delta_progress));
        }
    }
   
    //printf("observations\n");
    compute_all_observations(env);
    //int action = env->actions[0];
}

void c_close(Terraform* env) {
    free_initialized(env);
}

Mesh* create_heightmap_mesh(float* heightMap, Vector3 size) {
    int mapX = size.x;
    int mapZ = size.z;

    // NOTE: One vertex per pixel
    Mesh* mesh = (Mesh*)calloc(1, sizeof(Mesh));
    mesh->triangleCount = (mapX - 1)*(mapZ - 1)*2;    // One quad every four pixels

    mesh->vertexCount = mesh->triangleCount*3;

    mesh->vertices = (float *)RL_MALLOC(mesh->vertexCount*3*sizeof(float));
    mesh->normals = (float *)RL_MALLOC(mesh->vertexCount*3*sizeof(float));
    mesh->texcoords = (float *)RL_MALLOC(mesh->vertexCount*2*sizeof(float));
    mesh->colors = NULL;
    UploadMesh(mesh, false);
    return mesh;
}

void update_heightmap_mesh(Mesh* mesh, float* heightMap, Vector3 size) {
    int mapX = size.x;
    int mapZ = size.z;

    int vCounter = 0;       // Used to count vertices float by float
    int tcCounter = 0;      // Used to count texcoords float by float
    int nCounter = 0;       // Used to count normals float by float

    //Vector3 scaleFactor = { size.x/(mapX - 1), 1.0f, size.z/(mapZ - 1) };
    Vector3 scaleFactor = { 1.0f, 1.0f, 1.0f};

    Vector3 vA = { 0 };
    Vector3 vB = { 0 };
    Vector3 vC = { 0 };
    Vector3 vN = { 0 };

    for (int z = 0; z < mapZ-1; z++)
    {
        for (int x = 0; x < mapX-1; x++)
        {
            // Fill vertices array with data
            //----------------------------------------------------------

            // one triangle - 3 vertex
            mesh->vertices[vCounter] = (float)x*scaleFactor.x;
            mesh->vertices[vCounter + 1] = heightMap[x + z*mapX]*scaleFactor.y;
            mesh->vertices[vCounter + 2] = (float)z*scaleFactor.z;

            mesh->vertices[vCounter + 3] = (float)x*scaleFactor.x;
            mesh->vertices[vCounter + 4] = heightMap[x + (z + 1)*mapX]*scaleFactor.y;
            mesh->vertices[vCounter + 5] = (float)(z + 1)*scaleFactor.z;

            mesh->vertices[vCounter + 6] = (float)(x + 1)*scaleFactor.x;
            mesh->vertices[vCounter + 7] = heightMap[(x + 1) + z*mapX]*scaleFactor.y;
            mesh->vertices[vCounter + 8] = (float)z*scaleFactor.z;

            // Another triangle - 3 vertex
            mesh->vertices[vCounter + 9] = mesh->vertices[vCounter + 6];
            mesh->vertices[vCounter + 10] = mesh->vertices[vCounter + 7];
            mesh->vertices[vCounter + 11] = mesh->vertices[vCounter + 8];

            mesh->vertices[vCounter + 12] = mesh->vertices[vCounter + 3];
            mesh->vertices[vCounter + 13] = mesh->vertices[vCounter + 4];
            mesh->vertices[vCounter + 14] = mesh->vertices[vCounter + 5];

            mesh->vertices[vCounter + 15] = (float)(x + 1)*scaleFactor.x;
            mesh->vertices[vCounter + 16] = heightMap[(x + 1) + (z + 1)*mapX]*scaleFactor.y;
            mesh->vertices[vCounter + 17] = (float)(z + 1)*scaleFactor.z;
            vCounter += 18;     // 6 vertex, 18 floats

            // Fill texcoords array with data
            //--------------------------------------------------------------
            mesh->texcoords[tcCounter] = (float)x/(mapX - 1);
            mesh->texcoords[tcCounter + 1] = (float)z/(mapZ - 1);

            mesh->texcoords[tcCounter + 2] = (float)x/(mapX - 1);
            mesh->texcoords[tcCounter + 3] = (float)(z + 1)/(mapZ - 1);

            mesh->texcoords[tcCounter + 4] = (float)(x + 1)/(mapX - 1);
            mesh->texcoords[tcCounter + 5] = (float)z/(mapZ - 1);

            mesh->texcoords[tcCounter + 6] = mesh->texcoords[tcCounter + 4];
            mesh->texcoords[tcCounter + 7] = mesh->texcoords[tcCounter + 5];

            mesh->texcoords[tcCounter + 8] = mesh->texcoords[tcCounter + 2];
            mesh->texcoords[tcCounter + 9] = mesh->texcoords[tcCounter + 3];

            mesh->texcoords[tcCounter + 10] = (float)(x + 1)/(mapX - 1);
            mesh->texcoords[tcCounter + 11] = (float)(z + 1)/(mapZ - 1);
            tcCounter += 12;    // 6 texcoords, 12 floats

            // Fill normals array with data
            //--------------------------------------------------------------
            for (int i = 0; i < 18; i += 9)
            {
                vA.x = mesh->vertices[nCounter + i];
                vA.y = mesh->vertices[nCounter + i + 1];
                vA.z = mesh->vertices[nCounter + i + 2];

                vB.x = mesh->vertices[nCounter + i + 3];
                vB.y = mesh->vertices[nCounter + i + 4];
                vB.z = mesh->vertices[nCounter + i + 5];

                vC.x = mesh->vertices[nCounter + i + 6];
                vC.y = mesh->vertices[nCounter + i + 7];
                vC.z = mesh->vertices[nCounter + i + 8];

                vN = Vector3Normalize(Vector3CrossProduct(Vector3Subtract(vB, vA), Vector3Subtract(vC, vA)));

                mesh->normals[nCounter + i] = vN.x;
                mesh->normals[nCounter + i + 1] = vN.y;
                mesh->normals[nCounter + i + 2] = vN.z;

                mesh->normals[nCounter + i + 3] = vN.x;
                mesh->normals[nCounter + i + 4] = vN.y;
                mesh->normals[nCounter + i + 5] = vN.z;

                mesh->normals[nCounter + i + 6] = vN.x;
                mesh->normals[nCounter + i + 7] = vN.y;
                mesh->normals[nCounter + i + 8] = vN.z;
            }

            nCounter += 18;     // 6 vertex, 18 floats
        }
    }

    // Upload vertex data to GPU (static mesh)
    UpdateMeshBuffer(*mesh, 0, mesh->vertices, mesh->vertexCount * 3 * sizeof(float), 0); // Update vertices
    UpdateMeshBuffer(*mesh, 2, mesh->normals, mesh->vertexCount * 3 * sizeof(float), 0); // Update normals
}

const Color PUFF_RED = (Color){187, 0, 0, 255};
const Color PUFF_CYAN = (Color){0, 187, 187, 255};
const Color PUFF_WHITE = (Color){241, 241, 241, 241};
const Color PUFF_BACKGROUND = (Color){6, 24, 24, 255};
const Color PUFF_BACKGROUND2 = (Color){18, 72, 72, 255};

typedef struct Client Client;
struct Client {
    Texture2D ball;
    Camera3D camera;
    Mesh* mesh;
    Model model;
    Mesh* target_mesh;
    Model target_model;
    Texture2D texture;
    Model dozer;
    Shader shader;
    Shader target_shader;
    Texture2D shader_terrain;
    int shader_terrain_loc;
    unsigned char *shader_terrain_data;
};

Client* make_client(Terraform* env) {
    Client* client = (Client*)calloc(1, sizeof(Client));
    InitWindow(1080, 720, "PufferLib Terraform");
    SetConfigFlags(FLAG_MSAA_4X_HINT);
    SetTargetFPS(60);
    Camera3D camera = { 0 };
                                                       //
    camera.up = (Vector3){ 0.0f, 1.0f, 0.0f };          // Camera up vector (rotation towards target)
    camera.fovy = 45.0f;                                // Camera field-of-view Y
    camera.projection = CAMERA_PERSPECTIVE;             // Camera projection type
    camera.position = (Vector3){ 3*env->size/4, env->size, 3*env->size/4};
    camera.target = (Vector3){ env->size/2, 0, env->size/2-1};
    client->camera = camera;

    client->shader = LoadShader(
        TextFormat("resources/terraform/shader_%i.vs", GLSL_VERSION),
        TextFormat("resources/terraform/shader_%i.fs", GLSL_VERSION)
    );
    client->target_shader = LoadShader(
        TextFormat("resources/terraform/shader_%i.vs", GLSL_VERSION),
        TextFormat("resources/terraform/target_shader_%i.fs", GLSL_VERSION)
    );

    Image img = GenImageColor(env->size, env->size, WHITE);
    ImageFormat(&img, PIXELFORMAT_UNCOMPRESSED_R8G8B8A8);
    client->shader_terrain = LoadTextureFromImage(img);
    UnloadImage(img);

    client->shader_terrain_loc = GetShaderLocation(client->target_shader, "terrain");
    SetShaderValueTexture(client->target_shader, client->shader_terrain_loc, client->shader_terrain);

    client->shader_terrain_data = calloc(4*env->size*env->size, sizeof(unsigned char));

    int shader_width_loc = GetShaderLocation(client->target_shader, "width");
    SetShaderValue(client->target_shader, shader_width_loc, &env->size, SHADER_UNIFORM_INT);

    int shader_height_loc = GetShaderLocation(client->target_shader, "height");
    SetShaderValue(client->target_shader, shader_height_loc, &env->size, SHADER_UNIFORM_INT);
 
    //Image checked = GenImageChecked(env->size, env->size, 2, 2, PUFF_RED, PUFF_CYAN);
    img = LoadImage("resources/terraform/perlin.jpg");
    client->texture = LoadTextureFromImage(img);
    client->dozer = LoadModel("resources/terraform/dozer.glb");
    UnloadImage(img);
    client->mesh = NULL;
    return client;
}

void close_client(Client* client) {
    CloseWindow();
    free(client->mesh);
    free(client->shader_terrain_data);
    free(client->target_mesh);
    free(client);
    
}

void handle_camera_controls(Client* client) {
    static Vector2 prev_mouse_pos = {0};
    static bool is_dragging = false;
    float camera_move_speed = 0.5f;

    // Handle mouse drag for camera movement
    if (IsMouseButtonPressed(MOUSE_BUTTON_LEFT)) {
        prev_mouse_pos = GetMousePosition();
        is_dragging = true;
    }

    if (IsMouseButtonReleased(MOUSE_BUTTON_LEFT)) {
        is_dragging = false;
    }

    if (is_dragging) {
        Vector2 current_mouse_pos = GetMousePosition();
        Vector2 delta = {
            -(current_mouse_pos.x - prev_mouse_pos.x) * camera_move_speed,
            (current_mouse_pos.y - prev_mouse_pos.y) * camera_move_speed
        };

        // Apply 45-degree rotation to the movement
        // For a -45 degree rotation (clockwise)
        float cos45 = -0.7071f;  // cos(-45°)
        float sin45 = 0.7071f; // sin(-45°)
        Vector2 rotated_delta = {
            delta.x * cos45 - delta.y * sin45,
            delta.x * sin45 + delta.y * cos45
        };

        // Update camera position (only X and Y)
        client->camera.position.z += rotated_delta.x;
        client->camera.position.x += rotated_delta.y;

        // Update camera target (only X and Y)
        client->camera.target.z += rotated_delta.x;
        client->camera.target.x += rotated_delta.y;

        prev_mouse_pos = current_mouse_pos;
    }

    // Handle mouse wheel for zoom
    float wheel = GetMouseWheelMove();
    if (wheel != 0) {
        float zoom_factor = 1.0f - (wheel * 0.1f);
        // Calculate the current direction vector from target to position
        Vector3 direction = {
            client->camera.position.x - client->camera.target.x,
            client->camera.position.y - client->camera.target.y,
            client->camera.position.z - client->camera.target.z
        };

        // Scale the direction vector by the zoom factor
        direction.x *= zoom_factor;
        direction.y *= zoom_factor;
        direction.z *= zoom_factor;

        // Update the camera position based on the scaled direction
        client->camera.position.x = client->camera.target.x + direction.x;
        client->camera.position.y = client->camera.target.y + direction.y;
        client->camera.position.z = client->camera.target.z + direction.z;
    }
}

void c_render(Terraform* env) {
    if (env->client == NULL) {
        env->client = make_client(env);
        env->client->mesh = create_heightmap_mesh(env->map, (Vector3){env->size, 1, env->size});
        update_heightmap_mesh(env->client->mesh, env->map, (Vector3){env->size, 1, env->size});
        env->client->model = LoadModelFromMesh(*env->client->mesh);

        env->client->target_mesh = create_heightmap_mesh(env->target_map, (Vector3){env->size, 1, env->size});
        update_heightmap_mesh(env->client->target_mesh, env->target_map, (Vector3){env->size, 1, env->size});
        env->client->target_model = LoadModelFromMesh(*env->client->target_mesh);
    }
    if (IsKeyDown(KEY_ESCAPE)) {
        exit(0);
    }
    Client* client = env->client;

    handle_camera_controls(client);
    //Camera3D* camera = &client->camera;
    //camera->position = (Vector3){ x+30, z+100.0f, y+30 };
    //camera->target = (Vector3){ x, 0, y-1};
    rlSetBlendFactorsSeparate(RL_SRC_ALPHA, RL_ONE_MINUS_SRC_ALPHA, RL_ONE, RL_ONE, RL_FUNC_ADD, RL_MAX);

    if (env->tick % 10 == 0) {
        update_heightmap_mesh(client->mesh, env->map, (Vector3){env->size, 1, env->size});
        update_heightmap_mesh(client->target_mesh, env->target_map, (Vector3){env->size, 1, env->size});
        for (int i = 0; i < env->size*env->size; i++) {
            client->shader_terrain_data[4*i] = env->map[i];
            client->shader_terrain_data[4*i+3] = 255;
        }
        UpdateTexture(client->shader_terrain, client->shader_terrain_data);
        SetShaderValueTexture(client->target_shader, env->client->shader_terrain_loc, env->client->shader_terrain);

    }
    //client->model.materials[0].maps[MATERIAL_MAP_DIFFUSE].texture = client->texture;
    client->model.materials[0].shader = client->shader;

    //client->target_model.materials[0].maps[MATERIAL_MAP_DIFFUSE].texture = client->texture;
    client->target_model.materials[0].shader = client->target_shader;

    //update_heightmap_mesh(client->mesh, env->map, (Vector3){env->size, 1, env->size});
    //client->model = LoadModelFromMesh(*client->mesh);

    BeginDrawing();
    ClearBackground((Color){143, 86, 29, 255});
    BeginMode3D(client->camera);
    /*
    for(int i = 0; i < env->size*env->size; i++) {
        float height = env->map[i];
        int x = i%env->size;
        int z = i/env->size;
        DrawCube((Vector3){x, height, z}, 1.0f, 1.0f, 1.0f, DARKGREEN);
        DrawCubeWires((Vector3){x, height, z}, 1.0f, 1.0f, 1.0f, MAROON);
    }
    */

    BeginShaderMode(client->shader);
    DrawModel(client->model, (Vector3){0, 0, 0}, 1.0f, (Color){156, 50, 20, 255});
    EndShaderMode();
    rlDisableDepthTest();  // Add this line

    BeginBlendMode(RL_BLEND_CUSTOM_SEPARATE);  // Add this line
    BeginShaderMode(client->target_shader);
    DrawModel(client->target_model, (Vector3){0, 0, 0}, 1.0f, (Color){156, 50, 20, 255});
    EndShaderMode();
    EndBlendMode();
    rlEnableDepthTest();   // Add this line
    // for(int i = 0; i < env->size; i += 11){
    //     // draw grid lines every 11 units
    //     DrawLine3D((Vector3){i, 0, 0}, (Vector3){i, 0, env->size-1}, RED);
    //     DrawLine3D((Vector3){0, 0, i}, (Vector3){env->size-1, 0, i}, RED);
    // }
    for (int i = 0; i < env->num_agents; i++) {
        Dozer* dozer = &env->dozers[i];
        int x = (int)dozer->x;
        int z = (int)dozer->y;  
        int size = (int)env->size;
        
        // Get height from map using correct indexing
        float y = env->map[z * size + x] + 0.5f;
        float yy = y;
        rlPushMatrix();
        rlTranslatef(dozer->x, y, dozer->y);
        rlRotatef(-90.f - dozer->heading*RAD2DEG, 0, 1, 0);
        // if(i ==0 ){
        //     DrawCube((Vector3){0,50,0}, 10.0f, 10.0f, 10.0f, RED);
        // }
        DrawModel(client->dozer, (Vector3){0, 0, 0}, 0.25f, WHITE);
        rlPopMatrix();
        // DrawCube((Vector3){dozer->x, y, dozer->y}, 1.0f, 1.0f, 1.0f, PUFF_WHITE);
        if(IsKeyDown(KEY_LEFT_CONTROL) && i == 0) {
            int dialate = 1;
            int x_offset = env->dozers[i].x - dialate*VISION;
            int y_offset = env->dozers[i].y - dialate*VISION;
            for (int x = 0; x < 2*dialate*VISION + 1; x+=dialate) {
                for (int y = 0; y < 2*dialate*VISION + 1; y+=dialate) {
                    if(x_offset + x < 0 || x_offset + x >= env->size || y_offset + y < 0 || y_offset + y >= env->size) {
                        continue;
                    }
                    float obs_x = x_offset + x;
                    float obs_y = y_offset + y;
                    Color clr = PUFF_WHITE;
                    int idx = y*(2*VISION+1) + x;
                    int obs_idx = 319*i + 121 + idx;
                    if(env->observations[obs_idx] == 1.0f) {
                        clr = GREEN;
                    } else if(env->observations[obs_idx] == 0.66f) {
                        clr = PUFF_RED;
                    } else if(env->observations[obs_idx] == 0.33f) {
                        clr = YELLOW;
                    }
                    for(int j = 0; j < (2*SCOOP_SIZE + 1)*(2*SCOOP_SIZE + 1); j++) {
                        if(env->dozers[i].load_indices[j] == map_idx(env, x_offset + x, y_offset + y)){
                            clr = BLUE;
                            break;
                        }
                    }
                    DrawCube((Vector3){x_offset + x, yy, y_offset + y}, 0.5f, 0.5f, 0.5f, clr);
                }
            }
            int step = 1;
            for (int k = 0; k < env->size; k += step) {
                for (int l = 0; l < env->size; l += step) {
                int idx = k * env->size + l;
                    Color color = RED;
                    if (env->grid_indices[idx] == env->dozers[i].target_quadrant) {
                        color = GREEN;
                        DrawSphere((Vector3){l, 0, k}, 0.1f, color);

                    }
                }
            }
            for(int j = 0; j < env->num_quadrants; j++){
                Color color = PURPLE;
                if(env->quadrant_volume_deltas[j] > 0.0f) {
                    color = RED;
                } else if(env->quadrant_volume_deltas[j] < 0.0f) {
                    color = GREEN;
                }
                DrawLine3D((Vector3){env->quadrant_centroids[j*2], 0, env->quadrant_centroids[j*2+1]}, (Vector3){env->dozers[i].x, 0, env->dozers[i].y}, color);
            }
        }
        
        
    }
    EndMode3D();
    //DrawText(TextFormat("Dozer x: %f", x), 10, 150, 20, PUFF_WHITE);
    DrawText(TextFormat("score: %f", env->delta_progress), 10, 170, 20, PUFF_WHITE);
    DrawText(TextFormat("load: %f", env->dozers[0].load), 10, 190, 20, PUFF_WHITE);
    DrawText(TextFormat("Timestep: %d", env->tick), 10, 210, 20, PUFF_WHITE);
    DrawText(TextFormat("Current Quadrant: %d", env->grid_indices[map_idx(env, env->dozers[0].x, env->dozers[0].y)]), 10, 230, 20, PUFF_WHITE);
    DrawFPS(10, 10);
    EndDrawing();
}



================================================
FILE: pufferlib/ocean/terraform/terraform.py
================================================
'''A simple sample environment. Use this as a template for your own envs.'''

import gymnasium
import numpy as np
import random
import pufferlib
from pufferlib.ocean.terraform import binding
import time
OBS_SIZE = 11

class Terraform(pufferlib.PufferEnv):
    def __init__(self, num_envs=1, num_agents=8, map_size=64,
            render_mode=None, log_interval=32, buf=None, seed=0, reset_frequency=8192,
                 reward_scale=0.01):
        self.single_observation_space = gymnasium.spaces.Box(low=0, high=1,
            shape=(2*OBS_SIZE*OBS_SIZE + 5 + 36*2,), dtype=np.float32)
        self.single_action_space = gymnasium.spaces.MultiDiscrete([5, 5, 3], dtype=np.int32)
        self.render_mode = render_mode
        self.num_agents = num_envs*num_agents
        self.log_interval = log_interval
        self.reset_frequency = reset_frequency
        self.reward_scale = reward_scale
        super().__init__(buf)
        c_envs = []
        for i in range(num_envs):
            c_env = binding.env_init(
                self.observations[i*num_agents:(i+1)*num_agents],
                self.actions[i*num_agents:(i+1)*num_agents],
                self.rewards[i*num_agents:(i+1)*num_agents],
                self.terminals[i*num_agents:(i+1)*num_agents],
                self.truncations[i*num_agents:(i+1)*num_agents],
                seed,
                size=map_size,
                num_agents=num_agents,
                reset_frequency=reset_frequency,
                reward_scale=reward_scale,
            )
            c_envs.append(c_env)

        self.c_envs = binding.vectorize(*c_envs)
 
    def reset(self, seed=None):
        binding.vec_reset(self.c_envs, seed)
        self.tick = 0
        return self.observations, []

    def step(self, actions):
        self.tick += 1
        self.actions[:] = actions
        binding.vec_step(self.c_envs)

        episode_returns = self.rewards[self.terminals]

        info = []
        if self.tick % self.log_interval == 0:
            info.append(binding.vec_log(self.c_envs))

        return (self.observations, self.rewards,
            self.terminals, self.truncations, info)

    def render(self):
        binding.vec_render(self.c_envs, 0)

    def close(self):
        binding.vec_close(self.c_envs)

if __name__ == '__main__':
    TIME = 10
    env = Terraform(num_envs=512, num_agents=1, render_mode='human', map_size=64, seed=0)
    actions = np.random.randint(0, 5, (512, 3))  # Changed from the stack approach


    import time
    steps = 0
    start = time.time()
    while time.time() - start < TIME:
        env.step(actions)
        steps += 2048

    print('SPS:', env.num_agents * steps / (time.time() - start))








================================================
FILE: pufferlib/ocean/tetris/binding.c
================================================
#include "tetris.h"

#define Env Tetris
#include "../env_binding.h"

static int my_init(Env* env, PyObject* args, PyObject* kwargs) {
    env->n_rows = unpack(kwargs, "n_rows");
    env->n_cols = unpack(kwargs, "n_cols");
    env->deck_size = unpack(kwargs, "deck_size");
    init(env);
    return 0;
}

static int my_log(PyObject* dict, Log* log) {
    assign_to_dict(dict, "score", log->score);
    assign_to_dict(dict, "perf", log->perf);
    assign_to_dict(dict, "ep_length", log->ep_length);
    assign_to_dict(dict, "ep_return", log->ep_return);
    assign_to_dict(dict, "avg_combo", log->avg_combo);
    assign_to_dict(dict, "lines_deleted", log->lines_deleted);

    assign_to_dict(dict, "atn_frac_soft_drop", log->atn_frac_soft_drop);
    assign_to_dict(dict, "atn_frac_hard_drop", log->atn_frac_hard_drop);
    assign_to_dict(dict, "atn_frac_rotate", log->atn_frac_rotate);
    return 0;
}


================================================
FILE: pufferlib/ocean/tetris/tetris.c
================================================
#include <time.h>
#include "tetris.h"
#include "puffernet.h"
#define min(a, b) (((a) < (b)) ? (a) : (b))
#define max(a, b) (((a) > (b)) ? (a) : (b))

void demo() {
    Tetris env = {
        .n_rows = 20,
        .n_cols = 10,
        .deck_size=3,
    };
    allocate(&env);
    env.client = make_client(&env);
    c_reset(&env);

    Weights* weights = load_weights("resources/tetris/tetris_weights.bin", 163208);
    int logit_sizes[1] = {7};
    LinearLSTM* net = make_linearlstm(weights, 1, 234, logit_sizes, 1);

    while (!WindowShouldClose()) {
        if (IsKeyDown(KEY_LEFT_SHIFT)) {
            if (IsKeyDown(KEY_LEFT)  || IsKeyDown(KEY_A)){
                env.actions[0] = 1;
            }
            if (IsKeyDown(KEY_RIGHT)  || IsKeyDown(KEY_D)){
                env.actions[0] = 2;
            }
            if (IsKeyPressed(KEY_UP)  || IsKeyDown(KEY_W)) {
                env.actions[0] = 3;
            }
            if (IsKeyDown(KEY_DOWN)  || IsKeyDown(KEY_S)) {
                env.actions[0] = 4;
            }
            if (IsKeyPressed(KEY_SPACE)) {
                env.actions[0] = 5;
            }
            if (IsKeyPressed(KEY_C)) {
                env.actions[0] = 6;
            }
        } else {
            forward_linearlstm(net, env.observations, env.actions);
        }

        c_step(&env);
        env.actions[0] = 0;
        c_render(&env);
    }
    free_linearlstm(net);
    free_allocated(&env);
    close_client(env.client);
}

int main() {
    demo();
}



================================================
FILE: pufferlib/ocean/tetris/tetris.h
================================================
#include "raylib.h"
#include "tetrominoes.h"
#include <assert.h>
#include <limits.h>
#include <math.h>
#include <stdlib.h>
#include <string.h>
#include <unistd.h>
#define min(a, b) (((a) < (b)) ? (a) : (b))
#define max(a, b) (((a) > (b)) ? (a) : (b))

#define HALF_LINEWIDTH 1
#define SQUARE_SIZE 32

#define ACTION_NO_OP 0
#define ACTION_LEFT 1
#define ACTION_RIGHT 2
#define ACTION_ROTATE 3
#define ACTION_SOFT_DROP 4
#define ACTION_HARD_DROP 5
#define ACTION_HOLD 6

#define TICKS_FALL 4 // how many ticks before the tetromino naturally falls down of one square
#define MAX_TICKS 10000
#define PERSONAL_BEST 12565
#define SCORE_SOFT_DROP 1
#define REWARD_SOFT_DROP 0.01f
#define SCORE_HARD_DROP 2
#define REWARD_HARD_DROP 0.02f
#define REWARD_INVALID_ACTION 0.0f

const int SCORE_COMBO[5] = {0, 100, 300, 500, 1000};
const float REWARD_COMBO[5] = {0, 0.1, 0.3, 0.5, 1.0};

typedef struct Log {
	float perf;
	float score;
	float ep_length;
	float ep_return;
	float lines_deleted;
	float avg_combo;
	float atn_frac_soft_drop;
	float atn_frac_hard_drop;
	float atn_frac_rotate;
	float n;
} Log;

typedef struct Client {
	int total_cols;
	int total_rows;
	int ui_rows;
	int deck_rows;
	int preview_target_rotation;
	int preview_target_col;
} Client;

typedef struct Tetris {
	Client *client;
	Log log;
	float *observations;
	int *actions;
	float *rewards;
	unsigned char *terminals;

	int n_rows;
	int n_cols;
	int deck_size;
	int *grid;
	int tick;
	int tick_fall;
	int score;
	int can_swap;

	int *tetromino_deck;
	int hold_tetromino;
	int cur_position_in_deck;
	int cur_tetromino;
	int cur_tetromino_row;
	int cur_tetromino_col;
	int cur_tetromino_rot;

	float ep_return;
	int lines_deleted;
	int count_combos;
	int atn_count_hard_drop;
	int atn_count_soft_drop;
	int atn_count_rotate;
} Tetris;

void init(Tetris *env) {
	env->grid = (int *)calloc(env->n_rows * env->n_cols, sizeof(int));
	env->tetromino_deck = calloc(env->deck_size, sizeof(int));
}

void allocate(Tetris *env) {
	init(env);
	env->observations = (float *)calloc(
	    env->n_cols * env->n_rows + 6 + NUM_TETROMINOES * env->deck_size + NUM_TETROMINOES, sizeof(float));
	env->actions = (int *)calloc(1, sizeof(int));
	env->rewards = (float *)calloc(1, sizeof(float));
	env->terminals = (unsigned char *)calloc(1, sizeof(unsigned char));
}

void c_close(Tetris *env) {
	free(env->grid);
	free(env->tetromino_deck);
}

void free_allocated(Tetris *env) {
	free(env->actions);
	free(env->observations);
	free(env->terminals);
	free(env->rewards);
	c_close(env);
}

void add_log(Tetris *env) {
	env->log.score += env->score;
	env->log.perf += env->score / ((float)PERSONAL_BEST);
	env->log.ep_length += env->tick;
	env->log.ep_return += env->ep_return;
	env->log.lines_deleted += env->lines_deleted;
	env->log.avg_combo += env->count_combos > 0 ? ((float)env->lines_deleted) / ((float)env->count_combos) : 1.0f;
	env->log.atn_frac_hard_drop += env->atn_count_hard_drop / ((float)env->tick);
	env->log.atn_frac_soft_drop += env->atn_count_soft_drop / ((float)env->tick);
	env->log.atn_frac_rotate += env->atn_count_rotate / ((float)env->tick);
	env->log.n += 1;
}

void compute_observations(Tetris *env) {
	memset(env->observations, 0.0,
	       (env->n_cols * env->n_rows + 6 + NUM_TETROMINOES * env->deck_size + NUM_TETROMINOES) * sizeof(float));

	// content of the grid: 1st channel is the grid, 2nd channel is the
	for (int i = 0; i < env->n_cols * env->n_rows; i++) {
		env->observations[i] = env->grid[i] > 0;
	}

	for (int r = 0; r < SIZE; r++) {
		for (int c = 0; c < SIZE; c++) {
			if (TETROMINOES[env->cur_tetromino][env->cur_tetromino_rot][r][c] == 1) {
				env->observations[(env->cur_tetromino_row + r) * env->n_cols + c + env->cur_tetromino_col] = 2;
			}
		}
	}
	int offset = env->n_cols * env->n_rows;
	env->observations[offset] = env->tick / ((float)MAX_TICKS);
	env->observations[offset + 1] = env->tick_fall / ((float)TICKS_FALL);
	env->observations[offset + 2] = env->cur_tetromino_row / ((float)env->n_rows);
	env->observations[offset + 3] = env->cur_tetromino_col / ((float)env->n_cols);
	env->observations[offset + 4] = env->cur_tetromino_rot;
	env->observations[offset + 5] = env->can_swap;

	// deck, one hot endoded
	int tetromino_id;
	for (int j = 0; j < env->deck_size; j++) {
		tetromino_id = env->tetromino_deck[(env->cur_position_in_deck + j) % env->deck_size];
		env->observations[offset + 4 + j * NUM_TETROMINOES + tetromino_id] = 1;
	}

	// hold, one hot endoded
	if (env->hold_tetromino > -1) {
		env->observations[offset + 4 + env->deck_size * NUM_TETROMINOES + env->hold_tetromino] = 1;
	}
}

void restore_grid(Tetris *env) { memset(env->grid, 0, env->n_rows * env->n_cols * sizeof(int)); }

void initialize_deck(Tetris *env) {
	for (int i = 0; i < env->deck_size; i++) {
		env->tetromino_deck[i] = rand() % NUM_TETROMINOES;
	}
	env->cur_position_in_deck = 0;
	env->cur_tetromino = env->tetromino_deck[env->cur_position_in_deck];
}

void spawn_new_tetromino(Tetris *env) {
	env->tetromino_deck[env->cur_position_in_deck] = rand() % NUM_TETROMINOES;
	env->cur_position_in_deck = (env->cur_position_in_deck + 1) % env->deck_size;
	env->cur_tetromino = env->tetromino_deck[env->cur_position_in_deck];
	env->cur_tetromino_rot = 0;
	env->cur_tetromino_col = env->n_cols / 2;
	env->cur_tetromino_row = 0;
	env->tick_fall = 0;
}

bool can_spawn_new_tetromino(Tetris *env) {
	int next_tetromino = env->tetromino_deck[(env->cur_position_in_deck + 1) % env->deck_size];
	for (int c = 0; c < TETROMINOES_FILLS_COL[next_tetromino][0]; c++) {
		for (int r = 0; r < TETROMINOES_FILLS_ROW[next_tetromino][0]; r++) {
			if ((env->grid[r * env->n_cols + c + env->n_cols / 2] > 0) && (TETROMINOES[next_tetromino][0][r][c] == 1)) {
				return false;
			}
		}
	}
	return true;
}

bool can_soft_drop(Tetris *env) {
	if (env->cur_tetromino_row == (env->n_rows - TETROMINOES_FILLS_ROW[env->cur_tetromino][env->cur_tetromino_rot])) {
		return false;
	}
	for (int c = 0; c < TETROMINOES_FILLS_COL[env->cur_tetromino][env->cur_tetromino_rot]; c++) {
		for (int r = 0; r < TETROMINOES_FILLS_ROW[env->cur_tetromino][env->cur_tetromino_rot]; r++) {
			if ((env->grid[(r + env->cur_tetromino_row + 1) * env->n_cols + c + env->cur_tetromino_col] > 0) &&
			    (TETROMINOES[env->cur_tetromino][env->cur_tetromino_rot][r][c] == 1)) {
				return false;
			}
		}
	}
	return true;
}

bool can_go_left(Tetris *env) {
	if (env->cur_tetromino_col == 0) {
		return false;
	}
	for (int c = 0; c < TETROMINOES_FILLS_COL[env->cur_tetromino][env->cur_tetromino_rot]; c++) {
		for (int r = 0; r < TETROMINOES_FILLS_ROW[env->cur_tetromino][env->cur_tetromino_rot]; r++) {
			if ((env->grid[(r + env->cur_tetromino_row) * env->n_cols + c + env->cur_tetromino_col - 1] > 0) &&
			    (TETROMINOES[env->cur_tetromino][env->cur_tetromino_rot][r][c] == 1)) {
				return false;
			}
		}
	}
	return true;
}

bool can_go_right(Tetris *env) {
	if (env->cur_tetromino_col == (env->n_cols - TETROMINOES_FILLS_COL[env->cur_tetromino][env->cur_tetromino_rot])) {
		return false;
	}
	for (int c = 0; c < TETROMINOES_FILLS_COL[env->cur_tetromino][env->cur_tetromino_rot]; c++) {
		for (int r = 0; r < TETROMINOES_FILLS_ROW[env->cur_tetromino][env->cur_tetromino_rot]; r++) {
			if ((env->grid[(r + env->cur_tetromino_row) * env->n_cols + c + env->cur_tetromino_col + 1] > 0) &&
			    (TETROMINOES[env->cur_tetromino][env->cur_tetromino_rot][r][c] == 1)) {
				return false;
			}
		}
	}
	return true;
}

bool can_hold(Tetris *env) {
	if (env->can_swap == 0) {
		return false;
	}
	if (env->hold_tetromino == -1) {
		return true;
	}
	for (int c = 0; c < TETROMINOES_FILLS_COL[env->hold_tetromino][env->cur_tetromino_rot]; c++) {
		for (int r = 0; r < TETROMINOES_FILLS_ROW[env->hold_tetromino][env->cur_tetromino_rot]; r++) {
			if ((env->grid[(r + env->cur_tetromino_row) * env->n_cols + c + env->cur_tetromino_col + 1] > 0) &&
			    (TETROMINOES[env->hold_tetromino][env->cur_tetromino_rot][r][c] == 1)) {
				return false;
			}
		}
	}
	return true;
}

bool can_rotate(Tetris *env) {
	int next_rot = (env->cur_tetromino_rot + 1) % NUM_ROTATIONS;
	if (env->cur_tetromino_col > (env->n_cols - TETROMINOES_FILLS_COL[env->cur_tetromino][next_rot])) {
		return false;
	}
	if (env->cur_tetromino_row > (env->n_rows - TETROMINOES_FILLS_ROW[env->cur_tetromino][next_rot])) {
		return false;
	}
	for (int c = 0; c < TETROMINOES_FILLS_COL[env->cur_tetromino][next_rot]; c++) {
		for (int r = 0; r < TETROMINOES_FILLS_ROW[env->cur_tetromino][next_rot]; r++) {
			if ((env->grid[(r + env->cur_tetromino_row) * env->n_cols + c + env->cur_tetromino_col] > 0) &&
			    (TETROMINOES[env->cur_tetromino][next_rot][r][c] == 1)) {
				return false;
			}
		}
	}
	return true;
}

bool is_full_row(Tetris *env, int row) {
	for (int c = 0; c < env->n_cols; c++) {
		if (env->grid[row * env->n_cols + c] == 0) {
			return false;
		}
	}
	return true;
}

void clear_row(Tetris *env, int row) {
	for (int r = row; r > 0; r--) {
		for (int c = 0; c < env->n_cols; c++) {
			env->grid[r * env->n_cols + c] = env->grid[(r - 1) * env->n_cols + c];
		}
	}
	for (int c = 0; c < env->n_cols; c++) {
		env->grid[c] = 0;
	}
}

void c_reset(Tetris *env) {
	env->score = 0;
	env->hold_tetromino = -1;
	env->tick = 0;
	env->tick_fall = 0;
	env->can_swap = 1;

	env->ep_return = 0.0;
	env->count_combos = 0;
	env->lines_deleted = 0;
	env->atn_count_hard_drop = 0;
	env->atn_count_soft_drop = 0;
	env->atn_count_rotate = 0;

	restore_grid(env);
	initialize_deck(env);
	spawn_new_tetromino(env);
	compute_observations(env);
}

void place_tetromino(Tetris *env) {
	int row_to_check = env->cur_tetromino_row + TETROMINOES_FILLS_ROW[env->cur_tetromino][env->cur_tetromino_rot] - 1;
	int lines_deleted = 0;
	env->can_swap = 1;

	for (int c = 0; c < TETROMINOES_FILLS_COL[env->cur_tetromino][env->cur_tetromino_rot];
	     c++) { // Fill the main grid with the tetromino
		for (int r = 0; r < TETROMINOES_FILLS_ROW[env->cur_tetromino][env->cur_tetromino_rot]; r++) {
			if (TETROMINOES[env->cur_tetromino][env->cur_tetromino_rot][r][c] == 1) {
				env->grid[(r + env->cur_tetromino_row) * env->n_cols + c + env->cur_tetromino_col] =
				    env->cur_tetromino + 1;
			}
		}
	}
	for (int r = 0; r < TETROMINOES_FILLS_ROW[env->cur_tetromino][env->cur_tetromino_rot];
	     r++) { // Proceed to delete the complete rows
		if (is_full_row(env, row_to_check)) {
			clear_row(env, row_to_check);
			lines_deleted += 1;
		} else {
			row_to_check -= 1;
		}
	}
	if (lines_deleted > 0) {
		env->count_combos += 1;
		env->lines_deleted += lines_deleted;
		env->score += SCORE_COMBO[lines_deleted];
		env->rewards[0] += REWARD_COMBO[lines_deleted];
		env->ep_return += REWARD_COMBO[lines_deleted];
	}
	if ((!can_spawn_new_tetromino(env)) || (env->tick >= MAX_TICKS)) {
		env->terminals[0] = 1;
		add_log(env);
		c_reset(env);
	} else {
		spawn_new_tetromino(env);
	}
}

void c_step(Tetris *env) {
	env->terminals[0] = 0;
	env->rewards[0] = 0.0;
	env->tick += 1;
	env->tick_fall += 1;
	int action = env->actions[0];

	if (action == ACTION_LEFT) {
		if (can_go_left(env)) {
			env->cur_tetromino_col -= 1;
		} else {
			env->rewards[0] += REWARD_INVALID_ACTION;
			env->ep_return += REWARD_INVALID_ACTION;
			// action = ACTION_HARD_DROP;
		}
	}
	if (action == ACTION_RIGHT) {
		if (can_go_right(env)) {
			env->cur_tetromino_col += 1;
		} else {
			env->rewards[0] += REWARD_INVALID_ACTION;
			env->ep_return += REWARD_INVALID_ACTION;
			// action = ACTION_HARD_DROP;
		}
	}
	if (action == ACTION_ROTATE) {
		env->atn_count_rotate += 1;
		if (can_rotate(env)) {
			env->cur_tetromino_rot = (env->cur_tetromino_rot + 1) % NUM_ROTATIONS;
		} else {
			env->rewards[0] += REWARD_INVALID_ACTION;
			env->ep_return += REWARD_INVALID_ACTION;
			// action = ACTION_HARD_DROP;
		}
	}
	if (action == ACTION_SOFT_DROP) {
		env->atn_count_soft_drop += 1;
		if (can_soft_drop(env)) {
			env->cur_tetromino_row += 1;
			env->score += SCORE_SOFT_DROP;
			env->rewards[0] += REWARD_SOFT_DROP;
			env->ep_return += REWARD_SOFT_DROP;
		} else {
			env->rewards[0] += REWARD_INVALID_ACTION;
			env->ep_return += REWARD_INVALID_ACTION;
			// action = ACTION_HARD_DROP;
		}
	}
	if (action == ACTION_HOLD) {
		if (can_hold(env)) {
			int t1 = env->cur_tetromino;
			int t2 = env->hold_tetromino;
			if (t2 == -1) {
				spawn_new_tetromino(env);
				env->hold_tetromino = t1;
				env->can_swap = 0;
			} else {
				env->cur_tetromino = t2;
				env->tetromino_deck[env->cur_position_in_deck] = t2;
				env->hold_tetromino = t1;
				env->can_swap = 0;
				env->cur_tetromino_rot = 0;
				env->cur_tetromino_col = env->n_cols / 2;
				env->cur_tetromino_row = 0;
				env->tick_fall = 0;
			}
		} else {
			env->rewards[0] += REWARD_INVALID_ACTION;
			env->ep_return += REWARD_INVALID_ACTION;
			// action = ACTION_HARD_DROP;
		}
	}
	if (action == ACTION_HARD_DROP) {
		env->atn_count_hard_drop += 1;
		while (can_soft_drop(env)) {
			env->cur_tetromino_row += 1;
			env->score += SCORE_HARD_DROP;
			env->rewards[0] += REWARD_HARD_DROP;
			env->ep_return += REWARD_HARD_DROP;
		}
		place_tetromino(env);
	}
	if (env->tick_fall == TICKS_FALL) {
		env->tick_fall = 0;
		if (!can_soft_drop(env)) {
			place_tetromino(env);
		} else {
			env->cur_tetromino_row += 1;
		}
	}
	compute_observations(env);
}

Client *make_client(Tetris *env) {
	Client *client = (Client *)calloc(1, sizeof(Client));
	client->ui_rows = 1;
	client->deck_rows = SIZE;
	client->total_rows = 1 + client->ui_rows + 1 + client->deck_rows + 1 + env->n_rows + 1;
	client->total_cols = max(1 + env->n_cols + 1, 1 + 3 * (env->deck_size - 1));
	client->preview_target_col = env->n_cols / 2;
	client->preview_target_rotation = 0;
	InitWindow(SQUARE_SIZE * client->total_cols, SQUARE_SIZE * client->total_rows, "PufferLib Tetris");
	SetTargetFPS(10);
	return client;
}

void close_client(Client *client) {
	CloseWindow();
	free(client);
}

Color BORDER_COLOR = (Color){100, 100, 100, 255};
Color DASH_COLOR = (Color){80, 80, 80, 255};
Color DASH_COLOR_BRIGHT = (Color){150, 150, 150, 255};
Color DASH_COLOR_DARK = (Color){50, 50, 50, 255};

void c_render(Tetris *env) {
	if (env->client == NULL) {
		env->client = make_client(env);
	}
	Client *client = env->client;

	if (IsKeyDown(KEY_ESCAPE)) {
		exit(0);
	}
	if (IsKeyPressed(KEY_TAB)) {
		ToggleFullscreen();
	}

	BeginDrawing();
	ClearBackground(BLACK);
	int x, y;
	Color color;

	// outer grid
	for (int r = 0; r < client->total_rows; r++) {
		for (int c = 0; c < client->total_cols; c++) {
			x = c * SQUARE_SIZE;
			y = r * SQUARE_SIZE;
			if ((c == 0) || (c == client->total_cols - 1) ||
			    ((r >= 1 + client->ui_rows + 1) && (r < 1 + client->ui_rows + 1 + client->deck_rows)) ||
			    ((r >= 1 + client->ui_rows + 1 + client->deck_rows + 1) && (c >= env->n_rows)) || (r == 0) ||
			    (r == 1 + client->ui_rows) || (r == 1 + client->ui_rows + 1 + client->deck_rows) ||
			    (r == client->total_rows - 1)) {
				DrawRectangle(x + HALF_LINEWIDTH, y + HALF_LINEWIDTH, SQUARE_SIZE - 2 * HALF_LINEWIDTH,
				              SQUARE_SIZE - 2 * HALF_LINEWIDTH, BORDER_COLOR);
				DrawRectangle(x - HALF_LINEWIDTH, y - HALF_LINEWIDTH, SQUARE_SIZE, 2 * HALF_LINEWIDTH, DASH_COLOR_DARK);
				DrawRectangle(x - HALF_LINEWIDTH, y + SQUARE_SIZE - HALF_LINEWIDTH, SQUARE_SIZE, 2 * HALF_LINEWIDTH,
				              DASH_COLOR_DARK);
				DrawRectangle(x - HALF_LINEWIDTH, y - HALF_LINEWIDTH, 2 * HALF_LINEWIDTH, SQUARE_SIZE, DASH_COLOR_DARK);
				DrawRectangle(x + SQUARE_SIZE - HALF_LINEWIDTH, y - HALF_LINEWIDTH, 2 * HALF_LINEWIDTH, SQUARE_SIZE,
				              DASH_COLOR_DARK);
			}
		}
	}
	// main grid
	for (int r = 0; r < env->n_rows; r++) {
		for (int c = 0; c < env->n_cols; c++) {
			x = (c + 1) * SQUARE_SIZE;
			y = (1 + client->ui_rows + 1 + client->deck_rows + 1 + r) * SQUARE_SIZE;
			color =
			    (env->grid[r * env->n_cols + c] == 0) ? BLACK : TETROMINOES_COLORS[env->grid[r * env->n_cols + c] - 1];
			DrawRectangle(x + HALF_LINEWIDTH, y + HALF_LINEWIDTH, SQUARE_SIZE - 2 * HALF_LINEWIDTH,
			              SQUARE_SIZE - 2 * HALF_LINEWIDTH, color);
			DrawRectangle(x - HALF_LINEWIDTH, y - HALF_LINEWIDTH, SQUARE_SIZE, 2 * HALF_LINEWIDTH, DASH_COLOR);
			DrawRectangle(x - HALF_LINEWIDTH, y + SQUARE_SIZE - HALF_LINEWIDTH, SQUARE_SIZE, 2 * HALF_LINEWIDTH,
			              DASH_COLOR);
			DrawRectangle(x - HALF_LINEWIDTH, y - HALF_LINEWIDTH, 2 * HALF_LINEWIDTH, SQUARE_SIZE, DASH_COLOR);
			DrawRectangle(x + SQUARE_SIZE - HALF_LINEWIDTH, y - HALF_LINEWIDTH, 2 * HALF_LINEWIDTH, SQUARE_SIZE,
			              DASH_COLOR);
		}
	}

	// current tetromino
	for (int r = 0; r < SIZE; r++) {
		for (int c = 0; c < SIZE; c++) {
			x = (c + env->cur_tetromino_col + 1) * SQUARE_SIZE;
			y = (1 + client->ui_rows + 1 + client->deck_rows + 1 + r + env->cur_tetromino_row) * SQUARE_SIZE;

			if (TETROMINOES[env->cur_tetromino][env->cur_tetromino_rot][r][c] == 1) {
				color = TETROMINOES_COLORS[env->cur_tetromino];
				DrawRectangle(x + HALF_LINEWIDTH, y + HALF_LINEWIDTH, SQUARE_SIZE - 2 * HALF_LINEWIDTH,
				              SQUARE_SIZE - 2 * HALF_LINEWIDTH, color);
				DrawRectangle(x - HALF_LINEWIDTH, y - HALF_LINEWIDTH, SQUARE_SIZE, 2 * HALF_LINEWIDTH, DASH_COLOR);
				DrawRectangle(x - HALF_LINEWIDTH, y + SQUARE_SIZE - HALF_LINEWIDTH, SQUARE_SIZE, 2 * HALF_LINEWIDTH,
				              DASH_COLOR);
				DrawRectangle(x - HALF_LINEWIDTH, y - HALF_LINEWIDTH, 2 * HALF_LINEWIDTH, SQUARE_SIZE, DASH_COLOR);
				DrawRectangle(x + SQUARE_SIZE - HALF_LINEWIDTH, y - HALF_LINEWIDTH, 2 * HALF_LINEWIDTH, SQUARE_SIZE,
				              DASH_COLOR);
			}
		}
	}

	// Deck grid
	int tetromino_id;
	for (int i = 0; i < env->deck_size - 1; i++) {
		tetromino_id = env->tetromino_deck[(env->cur_position_in_deck + 1 + i) % env->deck_size];
		for (int r = 0; r < SIZE; r++) {
			for (int c = 0; c < 2; c++) {
				x = (c + 1 + 3 * i) * SQUARE_SIZE;
				y = (1 + client->ui_rows + 1 + r) * SQUARE_SIZE;
				int r_offset = (SIZE - TETROMINOES_FILLS_ROW[tetromino_id][0]);
				if (r < r_offset) {
					color = BLACK;
				} else {
					color =
					    (TETROMINOES[tetromino_id][0][r - r_offset][c] == 0) ? BLACK : TETROMINOES_COLORS[tetromino_id];
				}
				DrawRectangle(x + HALF_LINEWIDTH, y + HALF_LINEWIDTH, SQUARE_SIZE - 2 * HALF_LINEWIDTH,
				              SQUARE_SIZE - 2 * HALF_LINEWIDTH, color);
				DrawRectangle(x - HALF_LINEWIDTH, y - HALF_LINEWIDTH, SQUARE_SIZE, 2 * HALF_LINEWIDTH,
				              DASH_COLOR_BRIGHT);
				DrawRectangle(x - HALF_LINEWIDTH, y + SQUARE_SIZE - HALF_LINEWIDTH, SQUARE_SIZE, 2 * HALF_LINEWIDTH,
				              DASH_COLOR_BRIGHT);
				DrawRectangle(x - HALF_LINEWIDTH, y - HALF_LINEWIDTH, 2 * HALF_LINEWIDTH, SQUARE_SIZE,
				              DASH_COLOR_BRIGHT);
				DrawRectangle(x + SQUARE_SIZE - HALF_LINEWIDTH, y - HALF_LINEWIDTH, 2 * HALF_LINEWIDTH, SQUARE_SIZE,
				              DASH_COLOR_BRIGHT);
			}
		}
	}

	// hold tetromino
	for (int r = 0; r < SIZE; r++) {
		for (int c = 0; c < 2; c++) {
			x = (client->total_cols - 3 + c) * SQUARE_SIZE;
			y = (1 + client->ui_rows + 1 + r) * SQUARE_SIZE;
			if (env->hold_tetromino > -1) {
				int r_offset = (SIZE - TETROMINOES_FILLS_ROW[env->hold_tetromino][0]);
				if (r < r_offset) {
					color = BLACK;
				} else {
					color = (env->hold_tetromino > -1) && (TETROMINOES[env->hold_tetromino][0][r - r_offset][c] == 0)
					            ? BLACK
					            : TETROMINOES_COLORS[env->hold_tetromino];
				}
			} else {
				color = BLACK;
			}
			DrawRectangle(x + HALF_LINEWIDTH, y + HALF_LINEWIDTH, SQUARE_SIZE - 2 * HALF_LINEWIDTH,
			              SQUARE_SIZE - 2 * HALF_LINEWIDTH, color);
			DrawRectangle(x - HALF_LINEWIDTH, y - HALF_LINEWIDTH, SQUARE_SIZE, 2 * HALF_LINEWIDTH, DASH_COLOR_BRIGHT);
			DrawRectangle(x - HALF_LINEWIDTH, y + SQUARE_SIZE - HALF_LINEWIDTH, SQUARE_SIZE, 2 * HALF_LINEWIDTH,
			              DASH_COLOR_BRIGHT);
			DrawRectangle(x - HALF_LINEWIDTH, y - HALF_LINEWIDTH, 2 * HALF_LINEWIDTH, SQUARE_SIZE, DASH_COLOR_BRIGHT);
			DrawRectangle(x + SQUARE_SIZE - HALF_LINEWIDTH, y - HALF_LINEWIDTH, 2 * HALF_LINEWIDTH, SQUARE_SIZE,
			              DASH_COLOR_BRIGHT);
		}
	}
	// Draw UI
	DrawText(TextFormat("Score: %i", env->score), SQUARE_SIZE + 4, SQUARE_SIZE + 4, 30, (Color){255, 160, 160, 255});
	EndDrawing();
}



================================================
FILE: pufferlib/ocean/tetris/tetris.py
================================================
import gymnasium
import numpy as np
import pufferlib
from pufferlib.ocean.tetris import binding

class Tetris(pufferlib.PufferEnv):
    def __init__(
        self, 
        num_envs=1, 
        n_cols=10, 
        n_rows=20,
        deck_size=3,
        render_mode=None, 
        log_interval=32,
        buf=None, 
        seed=0
    ):
        self.single_observation_space = gymnasium.spaces.Box(low=0, high=1,
            shape=(n_cols*n_rows + 6 + 7 * (deck_size + 1),), dtype=np.float32)
        self.single_action_space = gymnasium.spaces.Discrete(7)
        self.render_mode = render_mode
        self.log_interval = log_interval
        self.num_agents = num_envs

        super().__init__(buf)
        self.deck_size = deck_size
        self.n_cols = n_cols
        self.n_rows = n_rows
        self.c_envs = binding.vec_init(
            self.observations,
            self.actions,
            self.rewards,
            self.terminals,
            self.truncations,
            num_envs,
            seed,
            n_cols=n_cols,
            n_rows=n_rows,
            deck_size=deck_size,
        )
 
    def reset(self, seed=0):
        binding.vec_reset(self.c_envs, seed)
        self.tick = 0
        return self.observations, []

    def step(self, actions):
        self.actions[:] = actions

        self.tick += 1
        binding.vec_step(self.c_envs)

        info = []
        if self.tick % self.log_interval == 0:
            info.append(binding.vec_log(self.c_envs))

        return (self.observations, self.rewards,
            self.terminals, self.truncations, info)

    def render(self):
        binding.vec_render(self.c_envs, 0)

    def close(self):
        binding.vec_close(self.c_envs)

if __name__ == '__main__':
    TIME = 10
    num_envs = 4096
    env = Tetris(num_envs=num_envs)
    actions = [
        [env.single_action_space.sample() for _ in range(num_envs) ]for _ in range(1000)
    ]
    obs, _ = env.reset(seed = np.random.randint(0,1000))

    import time
    start = time.time()
    tick = 0
    
    while time.time() - start < TIME:
        action = actions[tick%1000]
        env.render()
        print(np.array(obs[0][0:200]).reshape(20,10), obs[0][200:206], obs[0][206:(206+7*4)])
        obs, _, _, _, _ = env.step(action)
        tick += 1
    print('SPS:', (tick*num_envs) / (time.time() - start))
    env.close()




================================================
FILE: pufferlib/ocean/tetris/tetrominoes.h
================================================
#include "raylib.h"

#define NUM_TETROMINOES 7
#define NUM_ROTATIONS 4
#define SIZE 4

const Color TETROMINOES_COLORS[NUM_TETROMINOES] = {
    (Color){255, 255, 0, 255}, // Yellow
    (Color){0, 255, 255, 255}, // Cyan
    (Color){0, 255, 0, 255},   // Green
    (Color){255, 0, 0, 255},   // Red
    (Color){128, 0, 128, 255}, // Purple
    (Color){255, 165, 0, 255}, // Orange
    (Color){0, 0, 255, 255},   // Blue
};

const int TETROMINOES[NUM_TETROMINOES][NUM_ROTATIONS][SIZE][SIZE] = {
    {
        {
            {1, 1, 0, 0},
            {1, 1, 0, 0},
            {0, 0, 0, 0},
            {0, 0, 0, 0},
        },
        {
            {1, 1, 0, 0},
            {1, 1, 0, 0},
            {0, 0, 0, 0},
            {0, 0, 0, 0},
        },
        {
            {1, 1, 0, 0},
            {1, 1, 0, 0},
            {0, 0, 0, 0},
            {0, 0, 0, 0},
        },
        {
            {1, 1, 0, 0},
            {1, 1, 0, 0},
            {0, 0, 0, 0},
            {0, 0, 0, 0},
        },
    },
    {
        {
            {1, 0, 0, 0},
            {1, 0, 0, 0},
            {1, 0, 0, 0},
            {1, 0, 0, 0},
        },
        {
            {1, 1, 1, 1},
            {0, 0, 0, 0},
            {0, 0, 0, 0},
            {0, 0, 0, 0},
        },
        {
            {1, 0, 0, 0},
            {1, 0, 0, 0},
            {1, 0, 0, 0},
            {1, 0, 0, 0},
        },
        {
            {1, 1, 1, 1},
            {0, 0, 0, 0},
            {0, 0, 0, 0},
            {0, 0, 0, 0},
        },
    },
    {
        {
            {1, 0, 0, 0},
            {1, 1, 0, 0},
            {0, 1, 0, 0},
            {0, 0, 0, 0},
        },
        {
            {0, 1, 1, 0},
            {1, 1, 0, 0},
            {0, 0, 0, 0},
            {0, 0, 0, 0},
        },
        {
            {1, 0, 0, 0},
            {1, 1, 0, 0},
            {0, 1, 0, 0},
            {0, 0, 0, 0},
        },
        {
            {0, 1, 1, 0},
            {1, 1, 0, 0},
            {0, 0, 0, 0},
            {0, 0, 0, 0},
        },
    },
    {
        {
            {0, 1, 0, 0},
            {1, 1, 0, 0},
            {1, 0, 0, 0},
            {0, 0, 0, 0},
        },
        {
            {1, 1, 0, 0},
            {0, 1, 1, 0},
            {0, 0, 0, 0},
            {0, 0, 0, 0},
        },
        {
            {0, 1, 0, 0},
            {1, 1, 0, 0},
            {1, 0, 0, 0},
            {0, 0, 0, 0},
        },
        {
            {1, 1, 0, 0},
            {0, 1, 1, 0},
            {0, 0, 0, 0},
            {0, 0, 0, 0},
        },
    },
    {
        {
            {0, 1, 0, 0},
            {1, 1, 0, 0},
            {0, 1, 0, 0},
            {0, 0, 0, 0},
        },
        {
            {0, 1, 0, 0},
            {1, 1, 1, 0},
            {0, 0, 0, 0},
            {0, 0, 0, 0},
        },
        {
            {1, 0, 0, 0},
            {1, 1, 0, 0},
            {1, 0, 0, 0},
            {0, 0, 0, 0},
        },
        {
            {1, 1, 1, 0},
            {0, 1, 0, 0},
            {0, 0, 0, 0},
            {0, 0, 0, 0},
        },
    },
    {
        {
            {1, 0, 0, 0},
            {1, 0, 0, 0},
            {1, 1, 0, 0},
            {0, 0, 0, 0},
        },
        {
            {1, 1, 1, 0},
            {1, 0, 0, 0},
            {0, 0, 0, 0},
            {0, 0, 0, 0},
        },
        {
            {1, 1, 0, 0},
            {0, 1, 0, 0},
            {0, 1, 0, 0},
            {0, 0, 0, 0},
        },
        {
            {0, 0, 1, 0},
            {1, 1, 1, 0},
            {0, 0, 0, 0},
            {0, 0, 0, 0},
        },
    },
    {
        {
            {0, 1, 0, 0},
            {0, 1, 0, 0},
            {1, 1, 0, 0},
            {0, 0, 0, 0},
        },
        {
            {1, 0, 0, 0},
            {1, 1, 1, 0},
            {0, 0, 0, 0},
            {0, 0, 0, 0},
        },
        {
            {1, 1, 0, 0},
            {1, 0, 0, 0},
            {1, 0, 0, 0},
            {0, 0, 0, 0},
        },
        {
            {1, 1, 1, 0},
            {0, 0, 1, 0},
            {0, 0, 0, 0},
            {0, 0, 0, 0},
        },
    }
};


const int TETROMINOES_FILLS_COL[NUM_TETROMINOES][NUM_ROTATIONS] = {
    {
        2,2,2,2
    },
    {
        1,4,1,4
    },
    {
        2,3,2,3
    },
    {
        2,3,2,3
    },
    {
        2,3,2,3
    },
    {
        2,3,2,3
    },
    {
        2,3,2,3
    }
};


const int TETROMINOES_FILLS_ROW[NUM_TETROMINOES][NUM_ROTATIONS] = {
    {
        2,2,2,2
    },
    {
        4,1,4,1
    },
    {
        3,2,3,2
    },
    {
        3,2,3,2
    },
    {
        3,2,3,2
    },
    {
        3,2,3,2
    },
    {
        3,2,3,2
    }
};



================================================
FILE: pufferlib/ocean/tmaze/README.md
================================================
The T-maze is designed to test the memory capacity of RL algorithms under partial observability.

* The maze consists of a corridor of length N, ending in a T-junction with two final states (left and right). 
* Start condition: At the first tile of the corridor, the observation contains a special marker (3 or 4). This marker determines which final state (left or right) will yield reward +1 and which yields -1.
* Termination: The episode terminates immediately when the agent reaches a final state.
* Observations: local observation: obs = [current, front, left, right] with values: 0=wall, 1=open, 2&3 being the two possible states of the starting tile.
* Actions: The agent can either go forward, left or right. It can not go back or turn inside the corridor
* Rewards: Rewards are 0 everywhere except on the final states. If the starting state was a 2, the reward of 1 is on the left and -1 on the right. If the starting state was a 3, the reward of 1 is on the right and -1 on the left. 

Examples of observations:

* Middle of corridor: [1,1,0,0]
* At T-junction: [1,0,1,1]
* Starting state: [3,1,0,0]
* At a final/terminal state: [1,0,0,0]



================================================
FILE: pufferlib/ocean/tmaze/binding.c
================================================
#include "tmaze.h"

#define Env TMaze
#include "../env_binding.h"

static int my_init(Env* env, PyObject* args, PyObject* kwargs) {
    env->size = unpack(kwargs, "size");
    return 0;
}

static int my_log(PyObject* dict, Log* log) {
    assign_to_dict(dict, "perf", log->perf);
    assign_to_dict(dict, "score", log->score);
    assign_to_dict(dict, "episode_return", log->episode_return);
    assign_to_dict(dict, "episode_length", log->episode_length);
    return 0;
}



================================================
FILE: pufferlib/ocean/tmaze/tmaze.c
================================================
#include "tmaze.h"

int main() {
    TMaze env = {.size = 8};
    allocate_TMaze(&env);

    c_reset(&env);
    c_render(&env);
    while (!WindowShouldClose()) {
        if (IsKeyDown(KEY_LEFT_SHIFT)) {
            env.actions[0] = FORWARD;
            if (IsKeyDown(KEY_LEFT)  || IsKeyDown(KEY_A)) env.actions[0] = LEFT;
            if (IsKeyDown(KEY_RIGHT) || IsKeyDown(KEY_D)) env.actions[0] = RIGHT;

        } else {
            env.actions[0] = rand() % 3;
        }
        c_step(&env);
        c_render(&env);
    }
    free_allocated(&env);
}




================================================
FILE: pufferlib/ocean/tmaze/tmaze.h
================================================
#include <stdlib.h>
#include <string.h>
#include <stdio.h>
#include "raylib.h"

const unsigned char FORWARD = 0;
const unsigned char RIGHT = 1;
const unsigned char LEFT = 2;

const unsigned char EMPTY = 1;
const unsigned char WALL = 0; 

#define MAX(a,b) ((a) > (b) ? (a) : (b))
#define MIN(a,b) ((a) < (b) ? (a) : (b))

typedef struct {
    float perf; // Recommended 0-1 normalized single real number perf metric
    float score; // Recommended unnormalized single real number perf metric
    float episode_return; // Recommended metric: sum of agent rewards over episode
    float episode_length; // Recommended metric: number of steps of agent episode
    // Any extra fields you add here may be exported to Python in binding.c
    float n; // Required as the last field 
} Log;

// Required that you have some struct for your env
// Recommended that you name it the same as the env file
typedef struct {
    Log log; // Required field. Env binding code uses this to aggregate logs
    unsigned char* observations; // Required. You can use any obs type, but make sure it matches in Python!
    int* actions; // Required. int* for discrete/multidiscrete, float* for box
    float* rewards; // Required
    unsigned char* terminals; // Required. We don't yet have truncations as standard yet
    int size; // length of the corridor
    int tick;

    unsigned char state; // Internal current position in the maze
    unsigned char starting_state; // Starting state (2 or 3)

    Texture2D puffer; 

} TMaze;

TMaze* allocate_TMaze(TMaze *env) {
    env->observations = calloc(4, sizeof(unsigned char));
    env->actions = calloc(1, sizeof(int));
    env->rewards = calloc(1, sizeof(float));
    env->terminals = calloc(1, sizeof(unsigned char));
    return env;
}

void free_allocated(TMaze* env) {
    free(env->observations);
    free(env->actions);
    free(env->rewards);
    free(env->terminals);
    free(env);
}

void add_log(TMaze* env) {
    env->log.perf += (env->rewards[0]+1)/2; // Normalized to 0-1
    env->log.score += env->rewards[0];
    env->log.episode_length += env->tick;
    env->log.episode_return += env->rewards[0];
    env->log.n++;
}

// Required function
void c_reset(TMaze* env) {
    memset(env->observations, WALL, 4*sizeof(unsigned char));
    env->starting_state = rand() % 2 + 2; // 2 or 3 
    // [current, front, left, right]
    env->observations[0] = env->starting_state;
    env->observations[1] = EMPTY;
    env->tick = 0;
    env->state = 0;
}

void compute_observations(TMaze* env) {
    // if at the end of the maze
    if (env->state == env->size -1) {
        env->observations[0] = EMPTY;
        env->observations[1] = WALL;
        env->observations[2] = EMPTY;
        env->observations[3] = EMPTY;
        return;
    }
    // We don't have noops so the agent can not go back to the start
    env->observations[0] = EMPTY;
    env->observations[1] = EMPTY;
    env->observations[2] = WALL;
    env->observations[3] = WALL;
}

// Required function
void c_step(TMaze* env) {
    env->tick += 1;

    // Clear previous buffers
    env->terminals[0] = 0;
    env->rewards[0] = 0;

    int action = env->actions[0];

    if (env->state == env->size -1) {
        const int left_reward = (env->starting_state == 2) ? 1 : -1;
        const int right_reward = (env->starting_state == 3) ? 1 : -1;

        if (action == LEFT || action == RIGHT) {
            env->rewards[0] = (action == LEFT) ? left_reward : right_reward;
            env->terminals[0] = 1;
            add_log(env);
            c_reset(env);
        }

    } else {
        if (action == FORWARD) {
            env->state += 1;
            compute_observations(env);
        } 
    }
}

// Required function. Should handle creating the client on first call
void c_render(TMaze* env) {
    int px = MAX(8, 1024.0/env->size);

    if (!IsWindowReady()) {
        InitWindow(px*env->size, px*5, "PufferLib TMaze MDP");
        SetTargetFPS(4);
        env->puffer = LoadTexture("resources/shared/puffers_128.png");
    }

    // Standard across our envs so exiting is always the same
    if (IsKeyDown(KEY_ESCAPE)) {
        exit(0);
    }

    BeginDrawing();
    ClearBackground((Color){6, 24, 24, 255});

    int agent_pos = env->state;
    for (int i = 0; i < env->size; i++) {
        Color color = 
            (i == agent_pos) ? (Color){0, 255, 255, 255} :
            (i == 0 && env->starting_state == 2) ? (Color){255, 0, 0, 255} : 
            (i == 0 && env->starting_state == 3) ? (Color){0, 255, 0, 255} : 
            (Color){224, 224, 224, 255};

        if (i == agent_pos) {
            int starting_sprite_x = 0;
            float rotation = env->actions[0];
            if (rotation == -1) {
            starting_sprite_x = 128;
            rotation = 0;
            }
            Rectangle source_rect = (Rectangle){starting_sprite_x, 0, 128, 128};
            Rectangle dest_rect = (Rectangle){i*px, 2*px, px, px};        
            DrawTexturePro(env->puffer, source_rect, dest_rect,
                        (Vector2){0, 0}, 0, color);
        } else {
            DrawRectangle(i*px, 2*px, px, px, color);
        }

    }
    // Draw last terminal states
    DrawRectangle((env->size-1)*px, 1*px, px, px, (Color){255, 0, 0, 255});
    DrawRectangle((env->size-1)*px, 3*px, px, px, (Color){0, 255, 0, 255});

    char score_text[32];
    snprintf(score_text, sizeof(score_text), "Score: %f", env->rewards[0]);
    DrawText(score_text, env->size * px - 180, 10, 32, (Color){255, 255, 255, 255});
    

    EndDrawing();
}

// Required function. Should clean up anything you allocated
// Do not free env->observations, actions, rewards, terminals
void c_close(TMaze* env) {
    if (IsWindowReady()) {
        CloseWindow();
    }
}



================================================
FILE: pufferlib/ocean/tmaze/tmaze.py
================================================
'''A simple sample environment. Use this as a template for your own envs.'''

import gymnasium
import numpy as np

import pufferlib
from pufferlib.ocean.tmaze import binding

class TMaze(pufferlib.PufferEnv):
    def __init__(self, num_envs=1, render_mode=None, log_interval=128, size=11, buf=None, seed=0):
        self.single_observation_space = gymnasium.spaces.Box(low=0, high=1,
            shape=(4,), dtype=np.uint8)
        self.single_action_space = gymnasium.spaces.Discrete(3)
        self.render_mode = render_mode
        self.num_agents = num_envs
        self.log_interval = log_interval

        super().__init__(buf)
        self.c_envs = binding.vec_init(self.observations, self.actions, self.rewards,
            self.terminals, self.truncations, num_envs, seed, size=size)
 
    def reset(self, seed=0):
        binding.vec_reset(self.c_envs, seed)
        self.tick = 0
        return self.observations, []

    def step(self, actions):
        self.tick += 1

        self.actions[:] = actions
        binding.vec_step(self.c_envs)

        info = []
        if self.tick % self.log_interval == 0:
            info.append(binding.vec_log(self.c_envs))

        return (self.observations, self.rewards,
            self.terminals, self.truncations, info)

    def render(self):
        binding.vec_render(self.c_envs, 0)

    def close(self):
        binding.vec_close(self.c_envs)

if __name__ == '__main__':
    size = 10

    env = TMaze(size=size)
    env.reset()
    steps = 0

    CACHE = 1024
    actions = np.random.randint(0, 3, (CACHE,))

    i = 0
    import time
    start = time.time()
    while time.time() - start < 10:
        env.step(actions[i % CACHE])
        steps += 1
        i += 1

    print('Chain MDP SPS:', int(steps / (time.time() - start)))



================================================
FILE: pufferlib/ocean/tower_climb/binding.c
================================================
#include "tower_climb.h"

#define Env CTowerClimb
#define MY_SHARED
#include "../env_binding.h"

static PyObject* my_shared(PyObject* self, PyObject* args, PyObject* kwargs) {
    int num_maps = unpack(kwargs, "num_maps");
    Level* levels = calloc(num_maps, sizeof(Level));
    PuzzleState* puzzle_states = calloc(num_maps, sizeof(PuzzleState));

    for (int i = 0; i < num_maps; i++) {
        int goal_height = rand() % 4 + 5;
        int min_moves = 10;
        int max_moves = 15;
        init_level(&levels[i]);
        init_puzzle_state(&puzzle_states[i]);
        cy_init_random_level(&levels[i], goal_height, max_moves, min_moves, i);
        levelToPuzzleState(&levels[i], &puzzle_states[i]);
    }

    PyObject* levels_handle = PyLong_FromVoidPtr(levels);
    PyObject* puzzles_handle = PyLong_FromVoidPtr(puzzle_states);
    PyObject* state = PyDict_New();
    PyDict_SetItemString(state, "levels", levels_handle);
    PyDict_SetItemString(state, "puzzles", puzzles_handle);
    return PyLong_FromVoidPtr(state);
}

static int my_init(Env* env, PyObject* args, PyObject* kwargs) {
    env->num_maps = unpack(kwargs, "num_maps");
    env->reward_climb_row = unpack(kwargs, "reward_climb_row");
    env->reward_fall_row = unpack(kwargs, "reward_fall_row");
    env->reward_illegal_move = unpack(kwargs, "reward_illegal_move");
    env->reward_move_block = unpack(kwargs, "reward_move_block");
    init(env);

    PyObject* handle_obj = PyDict_GetItemString(kwargs, "state");
    if (handle_obj == NULL) {
        PyErr_SetString(PyExc_KeyError, "Key 'state' not found in kwargs");
        return 1;
    }

    // Check if handle_obj is a PyLong
    if (!PyLong_Check(handle_obj)) {
        PyErr_SetString(PyExc_TypeError, "state handle must be an integer");
        return 1;
    }

    // Convert PyLong to PyObject* (state dictionary)
    PyObject* state_dict = (PyObject*)PyLong_AsVoidPtr(handle_obj);
    if (state_dict == NULL) {
        PyErr_SetString(PyExc_ValueError, "Invalid state dictionary pointer");
        return 1;
    }

    // Verify it’s a dictionary
    if (!PyDict_Check(state_dict)) {
        PyErr_SetString(PyExc_TypeError, "State pointer does not point to a dictionary");
        return 1;
    }

    // Basic validation: check reference count
    if (state_dict->ob_refcnt <= 0) {
        PyErr_SetString(PyExc_RuntimeError, "State dictionary has invalid reference count");
        return 1;
    }

    PyObject* levels_obj = PyDict_GetItemString(state_dict, "levels");
    if (levels_obj == NULL) {
        PyErr_SetString(PyExc_KeyError, "Key 'levels' not found in state");
        return 1;
    }
    if (!PyLong_Check(levels_obj)) {
        PyErr_SetString(PyExc_TypeError, "levels must be an integer");
        return 1;
    }
    env->all_levels = (Level*)PyLong_AsVoidPtr(levels_obj);

    PyObject* puzzles_obj = PyDict_GetItemString(state_dict, "puzzles");
    if (!PyObject_TypeCheck(puzzles_obj, &PyLong_Type)) {
        PyErr_SetString(PyExc_TypeError, "puzzles handle must be an integer");
        return 1;
    }
    PuzzleState* puzzles = (PuzzleState*)PyLong_AsVoidPtr(puzzles_obj);
    if (!puzzles) {
        PyErr_SetString(PyExc_ValueError, "Invalid puzzles handle");
        return 1;
    }
    env->all_puzzles = puzzles;

    return 0;
}

static int my_log(PyObject* dict, Log* log) {
    assign_to_dict(dict, "perf", log->perf);
    assign_to_dict(dict, "score", log->score);
    assign_to_dict(dict, "episode_return", log->episode_return);
    assign_to_dict(dict, "episode_length", log->episode_length);
    return 0;
}



================================================
FILE: pufferlib/ocean/tower_climb/tower_climb.c
================================================
#include <time.h>
#include <unistd.h>
#include "tower_climb.h"
#include "puffernet.h"

typedef struct TowerClimbNet TowerClimbNet;
struct TowerClimbNet {
    int num_agents;
    float* obs_3d;
    float* obs_1d;
    Conv3D* conv1;
    ReLU* relu1;
    Conv3D* conv2;
    Linear* flat;
    CatDim1* cat;
    Linear* proj;
    LSTM* lstm;
    Linear* actor;
    Linear* value_fn;
    Multidiscrete* multidiscrete;
};

TowerClimbNet* init_tower_climb_net(Weights* weights, int num_agents) {
    TowerClimbNet* net = calloc(1, sizeof(TowerClimbNet));
    int hidden_size = 256;
    int cnn_channels = 16;
    // Calculate correct output sizes for Conv3D layers
    // First conv: (5,5,9) -> (4,4,8) with kernel=2, stride=1
    // Second conv: (4,4,8) -> (3,3,7) with kernel=2, stride=1
    int cnn_flat_size = cnn_channels * 1 * 1 * 5;  // Match PyTorch size

    net->num_agents = num_agents;
    net->obs_3d = calloc(5 * 5 * 9, sizeof(float));
    net->obs_1d = calloc(3, sizeof(float));
    net->conv1 = make_conv3d(weights, num_agents, 9, 5, 5, 1, cnn_channels, 3, 1);
    net->relu1 = make_relu(num_agents, cnn_channels * 3 * 3 * 7);
    net->conv2 = make_conv3d(weights, num_agents, 7, 3, 3, cnn_channels, cnn_channels, 3, 1);
    net->flat = make_linear(weights, num_agents, 3, 16);
    net->cat = make_cat_dim1(num_agents, cnn_flat_size, 16);
    net->proj = make_linear(weights, num_agents, cnn_flat_size + 16, hidden_size);
    net->actor = make_linear(weights, num_agents, hidden_size, 6);
    net->value_fn = make_linear(weights, num_agents, hidden_size, 1);
    net->lstm = make_lstm(weights, num_agents, hidden_size, hidden_size);
    int logit_sizes[1] = {6};
    net->multidiscrete = make_multidiscrete(num_agents, logit_sizes, 1);
    return net;
}

void forward(TowerClimbNet* net, unsigned char* observations, int* actions) {
    int vision_size = 5 * 5 * 9;
    int player_size = 3;
    // clear previous observations
    memset(net->obs_3d, 0, vision_size * sizeof(float));
    memset(net->obs_1d, 0, player_size * sizeof(float));
    // reshape board to 3d tensor
    float (*obs_3d)[1][5][5][9] = (float (*)[1][5][5][9])net->obs_3d;
    float (*obs_1d)[3] = (float (*)[3])net->obs_1d;
    // process vision board
    int obs_3d_idx = 0;
    for (int b = 0; b < 1; b++) {
        for (int d = 0; d < 5; d++) {
            for (int h = 0; h < 5; h++) {
                for (int w = 0; w < 9; w++) {
                    obs_3d[b][0][d][h][w] = observations[obs_3d_idx];
                    obs_3d_idx++;
                }
            }
        }
    }
    // process player board
    for (int i = 0; i < player_size; i++) {
        obs_1d[0][i] = observations[vision_size + i];
    }

    conv3d(net->conv1, net->obs_3d);
    relu(net->relu1, net->conv1->output);
    conv3d(net->conv2, net->relu1->output);
    linear(net->flat, net->obs_1d);
    cat_dim1(net->cat, net->conv2->output, net->flat->output);
    linear(net->proj, net->cat->output);
    lstm(net->lstm, net->proj->output);
    linear(net->actor, net->lstm->state_h);
    linear(net->value_fn, net->lstm->state_h);
    softmax_multidiscrete(net->multidiscrete, net->actor->output, actions);
}

void free_tower_climb_net(TowerClimbNet* net) {
    free(net->obs_3d);
    free(net->obs_1d);
    free(net->conv1);
    free(net->relu1);
    free(net->conv2);
    free(net->flat);
    free(net->cat);
    free(net->proj);
    free(net->actor);
    free(net->value_fn);
    free(net->lstm);
    free(net->multidiscrete);
    free(net);
}

void demo() {   
    Weights* weights = load_weights("resources/tower_climb/tower_climb_weights.bin", 560407);
    TowerClimbNet* net = init_tower_climb_net(weights, 1);

    int num_maps = 1;  // Generate 1 map only to start faster
    Level* levels = calloc(num_maps, sizeof(Level));
    PuzzleState* puzzle_states = calloc(num_maps, sizeof(PuzzleState));

    srand(time(NULL));
    
    for (int i = 0; i < num_maps; i++) {
        int goal_height = rand() % 4 + 5;
        int min_moves = 10;
        int max_moves = 15;
        init_level(&levels[i]);
        init_puzzle_state(&puzzle_states[i]);
        cy_init_random_level(&levels[i], goal_height, max_moves, min_moves, i);
        levelToPuzzleState(&levels[i], &puzzle_states[i]);
    }

    CTowerClimb* env = allocate();
    env->num_maps = num_maps;
    env->all_levels = levels;
    env->all_puzzles = puzzle_states;

    int random_level = 5 + (rand() % 4);
    init_random_level(env, random_level, 15, 10, rand());
    c_reset(env);
    c_render(env);
    Client* client = env->client;
    client->enable_animations = 1;
    int tick = 0;
    while (!WindowShouldClose()) {
        if (tick % 6 == 0 && !client->isMoving) {
            tick = 0;
            int human_action = env->actions[0];
            forward(net, env->observations, env->actions);
            if (IsKeyDown(KEY_LEFT_SHIFT)) {
                env->actions[0] = human_action;
            }
            c_step(env);
            if (IsKeyDown(KEY_LEFT_SHIFT)) {
                env->actions[0] = NOOP;
            }
        }
        tick++;
        if (IsKeyDown(KEY_LEFT_SHIFT)) {
            // Camera controls
            if (IsKeyPressed(KEY_UP)) { // || IsKeyPressed(KEY_W)) {
                env->actions[0] = UP;
            }
            if (IsKeyPressed(KEY_LEFT)) { //|| IsKeyPressed(KEY_A)) {
                env->actions[0] = LEFT;
            }
            if (IsKeyPressed(KEY_RIGHT)) { //|| IsKeyPressed(KEY_D)) {
                env->actions[0] = RIGHT;
            }
            if (IsKeyPressed(KEY_DOWN)) { //|| IsKeyPressed(KEY_S)){
                env->actions[0] = DOWN;
            }
            if (IsKeyPressed(KEY_SPACE)){
                env->actions[0] = GRAB;
            }
            if (IsKeyPressed(KEY_RIGHT_SHIFT)){
                env->actions[0] = DROP;
            }
        }
        c_render(env);
        
        // Handle delayed level reset after puffer animation finishes
        if (env->pending_reset) {
            bool shouldReset = false;
            
            if (env->celebrationStarted) {
                // Wait for full celebration sequence: 0.8s climbing + 0.4s beam + 0.7s banner = 1.9s total
                float celebrationDuration = GetTime() - env->celebrationStartTime;
                shouldReset = (celebrationDuration >= 1.9f);
            } else {
                // No celebration; reset when banner finishes
                shouldReset = (!client->showBanner || client->bannerType != 1);
            }
            
            if (shouldReset) {
                env->pending_reset = false;
                c_reset(env);
            }
        }
    }
    close_client(client);
    free_allocated(env);
    free_tower_climb_net(net);
    free(weights);
    free(levels[0].map);
    free(levels);
    free(puzzle_states[0].blocks);
    free(puzzle_states);
}

void performance_test() {
    long test_time = 10;
    CTowerClimb* env = allocate();
    int seed = 0;
    init_random_level(env, 8, 25, 15, seed);
    long start = time(NULL);
    int i = 0;
    while (time(NULL) - start < test_time) {
        env->actions[0] = rand() % 5;
        c_step(env);
        i++;
    }
    long end = time(NULL);
    printf("SPS: %ld\n", i / (end - start));
    free_allocated(env);
}

int main() {
    demo();
    // performance_test();
    return 0;
}





================================================
FILE: pufferlib/ocean/tower_climb/tower_climb.py
================================================
import numpy as np
import gymnasium

import pufferlib
from pufferlib.ocean.tower_climb import binding


class TowerClimb(pufferlib.PufferEnv):
    def __init__(self, num_envs=4096, render_mode=None, report_interval=1,
            num_maps=50, reward_climb_row = .25, reward_fall_row = 0, reward_illegal_move = -0.01,
            reward_move_block = 0.2, buf = None, seed=0):

        # env
        self.num_agents = num_envs
        self.render_mode = render_mode
        self.report_interval = report_interval
        
        self.num_obs = 228
        self.single_observation_space = gymnasium.spaces.Box(low=0, high=255,
            shape=(self.num_obs,), dtype=np.uint8)
        self.single_action_space = gymnasium.spaces.Discrete(6)

        super().__init__(buf=buf)   
        c_envs = []
        self.c_state = binding.shared(num_maps=num_maps)
        self.c_envs = binding.vec_init(self.observations, self.actions,
            self.rewards, self.terminals, self.truncations, num_envs, seed,
            num_maps=num_maps, reward_climb_row=reward_climb_row,
            reward_fall_row=reward_fall_row, reward_illegal_move=reward_illegal_move,
            reward_move_block=reward_move_block, state=self.c_state)

    def reset(self, seed=None):
        binding.vec_reset(self.c_envs, seed)
        self.tick = 0
        return self.observations, []

    def step(self, actions):
        self.actions[:] = actions
        binding.vec_step(self.c_envs)
        self.tick += 1
        info = []
        if self.tick % self.report_interval == 0:
            log = binding.vec_log(self.c_envs)
            if log:
                info.append(log)

        return (self.observations, self.rewards,
            self.terminals, self.truncations, info)

    def render(self):
        binding.vec_render(self.c_envs, 0)
        
    def close(self):
        #binding.vec_close(self.c_envs)
        pass

def test_performance(timeout=10, atn_cache=1024):
    num_envs=1000;
    env = TowerClimb(num_envs=num_envs)
    env.reset()
    tick = 0

    actions = np.random.randint(0, env.single_action_space.n, (atn_cache, num_envs))

    import time
    start = time.time()
    while time.time() - start < timeout:
        atn = actions[tick % atn_cache]
        env.step(atn)
        tick += 1

    sps = num_envs * tick / (time.time() - start)
    print(f'SPS: {sps:,}')



================================================
FILE: pufferlib/ocean/trash_pickup/README.md
================================================
# TrashPickup Environment

A lightweight multi-agent reinforcement learning (RL) environment designed for coordination and cooperation research. Agents pick up trash and deposit it in bins for rewards.

## Key Features
- **Multi-Agent Coordination:** Encourages teamwork, efficient planning, and resource allocation.
- **Configurable Setup:** Adjustable grid size, number of agents, trash, bins, and episode length.
- **Discrete Action Space:** Actions include `UP`, `DOWN`, `LEFT`, `RIGHT`.
- **Fast and Lightweight:** Optimized for rapid training and testing.

## Example Research Goals
- Investigate emergent behaviors like task allocation and coordination.
- Study efficient resource collection and bin-pushing strategies.

## Ideal For
- RL researchers exploring multi-agent cooperation.
- Students learning about multi-agent systems.
- Developers testing scalable RL algorithms.



================================================
FILE: pufferlib/ocean/trash_pickup/binding.c
================================================
#include "trash_pickup.h"

#define Env CTrashPickupEnv
#include "../env_binding.h"

static int my_init(Env* env, PyObject* args, PyObject* kwargs) {
    env->num_agents = unpack(kwargs, "num_agents");
    env->grid_size = unpack(kwargs, "grid_size");
    env->num_trash = unpack(kwargs, "num_trash");
    env->num_bins = unpack(kwargs, "num_bins");
    env->max_steps = unpack(kwargs, "max_steps");
    env->agent_sight_range = unpack(kwargs, "agent_sight_range");
    initialize_env(env);
    return 0;
}

static int my_log(PyObject* dict, Log* log) {
    assign_to_dict(dict, "perf", log->perf);
    assign_to_dict(dict, "score", log->score);
    assign_to_dict(dict, "episode_return", log->episode_return);
    assign_to_dict(dict, "episode_length", log->episode_length);
    assign_to_dict(dict, "trash_collected", log->trash_collected);
    return 0;
}



================================================
FILE: pufferlib/ocean/trash_pickup/cy_trash_pickup.pyx
================================================
cimport numpy as cnp
from libc.stdlib cimport calloc, free  # Use calloc for zero-initialized allocation
from libc.stdint cimport uint64_t

cdef extern from "trash_pickup.h":
    int LOG_BUFFER_SIZE

    ctypedef struct Log:
        float perf;
        float score;
        float episode_return;
        float episode_length;
        float trash_collected;

    ctypedef struct LogBuffer
    LogBuffer* allocate_logbuffer(int)
    void free_logbuffer(LogBuffer*)
    Log aggregate_and_clear(LogBuffer*)

    ctypedef struct CTrashPickupEnv:
        char* observations
        int* actions
        float* rewards
        unsigned char* dones
        LogBuffer* log_buffer

        int grid_size
        int num_agents
        int num_trash
        int num_bins
        int max_steps
        int agent_sight_range


    ctypedef struct Client

    void initialize_env(CTrashPickupEnv* env)
    void free_allocated(CTrashPickupEnv* env)

    Client* make_client(CTrashPickupEnv* env)
    void close_client(Client* client)
    void c_render(Client* client, CTrashPickupEnv* env) 
    void c_reset(CTrashPickupEnv* env)
    void c_step(CTrashPickupEnv* env)

cdef class CyTrashPickup:
    cdef:
        CTrashPickupEnv* envs
        Client* client
        LogBuffer* logs
        int num_envs

    def __init__(self, char[:, :] observations, int[:] actions,
            float[:] rewards, unsigned char[:] terminals, int num_envs, 
            int num_agents=3, int grid_size=10, int num_trash=15, 
            int num_bins=2, int max_steps=300, int agent_sight_range=5):
        self.num_envs = num_envs
        self.envs = <CTrashPickupEnv*>calloc(num_envs, sizeof(CTrashPickupEnv))
        if self.envs == NULL:
            raise MemoryError("Failed to allocate memory for CTrashPickupEnv")
        self.client = NULL

        self.logs = allocate_logbuffer(LOG_BUFFER_SIZE)

        cdef int inc = num_agents
        
        cdef int i
        for i in range(num_envs):
            self.envs[i] = CTrashPickupEnv(
                observations=&observations[inc*i, 0],
                actions=&actions[inc*i],
                rewards=&rewards[inc*i],
                dones=&terminals[inc*i],
                log_buffer=self.logs, 
                grid_size=grid_size, 
                num_agents=num_agents,
                num_trash=num_trash, 
                num_bins=num_bins, 
                max_steps=max_steps,
                agent_sight_range=agent_sight_range
            )
            initialize_env(&self.envs[i])

    def reset(self):
        cdef int i
        for i in range(self.num_envs):
            c_reset(&self.envs[i])

    def step(self):
        cdef int i
        for i in range(self.num_envs):
            c_step(&self.envs[i])

    def render(self):
        cdef CTrashPickupEnv* env = &self.envs[0]
        if self.client == NULL:
            self.client = make_client(env)

        c_render(self.client, env)

    def close(self):
        if self.client != NULL:
            close_client(self.client)
            self.client = NULL

        free(self.envs)

    def log(self):
        cdef Log log = aggregate_and_clear(self.logs)
        return log



================================================
FILE: pufferlib/ocean/trash_pickup/trash_pickup.c
================================================
#include <time.h>
#include "trash_pickup.h"
#include "puffernet.h"

// Demo function for visualizing the TrashPickupEnv
void demo(int grid_size, int num_agents, int num_trash, int num_bins, int max_steps) {
    CTrashPickupEnv env = {
        .grid_size = grid_size,
        .num_agents = num_agents,
        .num_trash = num_trash,
        .num_bins = num_bins,
        .max_steps = max_steps,
        .agent_sight_range = 5,
        .do_human_control = true
    };

    bool use_pretrained_model = true;

    Weights* weights;
    ConvLSTM* net;

    if (use_pretrained_model){
        weights = load_weights("resources/trash_pickup/trash_pickup_weights.bin", 150245);
        int vision = 2*env.agent_sight_range + 1;
        net = make_convlstm(weights, env.num_agents, vision, 5, 32, 128, 4);
    }

    allocate(&env);
    c_reset(&env);
    c_render(&env);

    int tick = 0;
    while (!WindowShouldClose()) {
        if (tick % 2 == 0) {
            // Random actions for all agents
            for (int i = 0; i < env.num_agents; i++) {
                if (use_pretrained_model)
                {
                    for (int e = 0; e < env.total_num_obs; e++) {
                        net->obs[e] = env.observations[e];
                    }
                    forward_convlstm(net, net->obs, env.actions);    
                }
                else{
                    env.actions[i] = rand() % 4; // 0 = UP, 1 = DOWN, 2 = LEFT, 3 = RIGHT
                }
                // printf("action: %d \n", env.actions[i]);
            }

            // Override human control actions
            if (IsKeyDown(KEY_LEFT_SHIFT)) {
                // Handle keyboard input only for selected agent
                if (IsKeyDown(KEY_UP) || IsKeyDown(KEY_W)) {
                    env.actions[0] = ACTION_UP;
                }
                if (IsKeyDown(KEY_LEFT) || IsKeyDown(KEY_A)) {
                    env.actions[0] = ACTION_LEFT;
                }
                if (IsKeyDown(KEY_RIGHT) || IsKeyDown(KEY_D)) {
                    env.actions[0] = ACTION_RIGHT;
                }
                if (IsKeyDown(KEY_DOWN) || IsKeyDown(KEY_S)) { env.actions[0] = ACTION_DOWN; }
            }

            // Step the environment and render the grid
            c_step(&env);
            
        }
        tick++;

        c_render(&env);
    }

    free_convlstm(net);
    free(weights);
    free_allocated(&env);
    //close_client(client);
}

// Performance test function for benchmarking
void performance_test() {
    long test_time = 10; // Test duration in seconds

    CTrashPickupEnv env = {
        .grid_size = 10,
        .num_agents = 4,
        .num_trash = 20,
        .num_bins = 1,
        .max_steps = 150,
        .agent_sight_range = 5
    };
    allocate(&env);
    c_reset(&env);

    long start = time(NULL);
    int i = 0;
    int inc = env.num_agents;
    while (time(NULL) - start < test_time) {
        for (int e = 0; e < env.num_agents; e++) {
            env.actions[e] = rand() % 4;
        }
        c_step(&env);
        i += inc;
    }
    long end = time(NULL);
    printf("SPS: %ld\n", i / (end - start));
    free_allocated(&env);
}


// Main entry point
int main() {
    demo(20, 8, 40, 2, 300); // Visual demo
    //performance_test(); // Uncomment for benchmarking
    return 0;
}



================================================
FILE: pufferlib/ocean/trash_pickup/trash_pickup.h
================================================
#include <stdlib.h>
#include <stdbool.h>
#include <stdint.h>
#include <string.h>
#include <stdio.h>
#include "raylib.h"

#define EMPTY 0
#define TRASH 1
#define TRASH_BIN 2
#define AGENT 3

#define ACTION_UP 0
#define ACTION_DOWN 1
#define ACTION_LEFT 2
#define ACTION_RIGHT 3

#define LOG_BUFFER_SIZE 1024

typedef struct Log {
    float perf;
    float score;
    float episode_return;
    float episode_length;
    float trash_collected;
    float n;
} Log;

typedef struct {
    int type; // Entity type: EMPTY, TRASH, TRASH_BIN, AGENT
    int pos_x;
    int pos_y;
    bool presence; // Whether or not Entity is present (not applicable to all types)
    bool carrying; // Whether agent is carrying trash (only applicable to Agent types)
} Entity;

typedef struct {
    Entity* entity;
    int index; // Index in the positions array (-1 if not applicable)
} GridCell;

typedef struct Client {
    int window_width;
    int window_height;
    int header_offset;
    int cell_size;
    Texture2D agent_texture;
} Client;

typedef struct {
    // Interface for PufferLib
    Client* client;
    char* observations;
    int* actions;
    float* rewards;
    unsigned char* terminals;
    Log log;

    int grid_size;
    int num_agents;
    int num_trash;
    int num_bins;
    int max_steps;
    int current_step;

    int total_num_obs;

    int agent_sight_range;

    float positive_reward;
    float negative_reward;
    float total_episode_reward;

    GridCell* grid; // 1D array for grid
    Entity* entities; // Indicies (0 - num_agents) for agents, (num_agents - num_bins) for bins, (num_bins - num_trash) for trash.

    bool do_human_control;
} CTrashPickupEnv;

void add_log(CTrashPickupEnv* env, Log* log) {
    env->log.perf += log->perf;
    env->log.score += log->score;
    env->log.episode_return += log->episode_return;
    env->log.episode_length += log->episode_length;
    env->log.trash_collected += log->trash_collected;
    env->log.n += 1;
}

int get_grid_index(CTrashPickupEnv* env, int x, int y) {
    return (y * env->grid_size) + x;
}

// returns the start index of each type of entity for iteration purposes
int get_entity_type_start_index(CTrashPickupEnv* env, int type)
{
    if (type == AGENT)
        return 0;
    else if (type == TRASH_BIN)
        return env->num_agents;
    else if (type == TRASH)
        return env->num_agents + env->num_bins;
    else
        return -1;
}

// Entity Attribute Based Obs-Space
/*
void compute_observations(CTrashPickupEnv* env) {
    float* obs = env->observations;
    float norm_factor = 1.0f / env->grid_size;

    int obs_index = 0;

    for (int agent_idx = 0; agent_idx < env->num_agents; agent_idx++){
        float current_norm_pos_x = (float) (env->entities[agent_idx].pos_x) * norm_factor;
        float current_norm_pos_y = (float) (env->entities[agent_idx].pos_y) * norm_factor;

        // Add the observing agent's own position and carrying status
        obs[obs_index++] = current_norm_pos_x;
        obs[obs_index++] = current_norm_pos_y;
        obs[obs_index++] = env->entities[agent_idx].carrying ? 1.0f : 0.0f;

        // Add other observations from other entities (other agents, bins, trash)
        for (int i = 0; i < env->num_agents + env->num_bins + env->num_trash; i++) {
            // skip if current this agent
            if (agent_idx == i)
                continue;

            obs[obs_index++] = ((float) (env->entities[i].pos_x) * norm_factor) - current_norm_pos_x;
            obs[obs_index++] = ((float) (env->entities[i].pos_y) * norm_factor) - current_norm_pos_y;

            if (env->entities[i].type == AGENT) {
                obs[obs_index++] = env->entities[i].carrying ? 1.0f : 0.0f;
            }
            else if (env->entities[i].type == TRASH_BIN) {
                obs[obs_index++] = env->entities[i].presence ? 1.0f : 0.0f;
            }
        }
    }
}
*/

// Local crop version
void compute_observations(CTrashPickupEnv* env) {
    int sight_range = env->agent_sight_range;
    char* obs = env->observations;

    int obs_dim = 2*env->agent_sight_range + 1;
    int channel_offset = obs_dim*obs_dim;
    memset(obs, 0, env->total_num_obs*sizeof(char));

    for (int agent_idx = 0; agent_idx < env->num_agents; agent_idx++) {
        // Add obs for whether the agent is carrying or not
        //obs[obs_index++] = env->entities[agent_idx].carrying;

        // Get the agent's position
        int agent_x = env->entities[agent_idx].pos_x;
        int agent_y = env->entities[agent_idx].pos_y;

        // Iterate over the sight range
        for (int dy = -sight_range; dy <= sight_range; dy++) {
            for (int dx = -sight_range; dx <= sight_range; dx++) {
                int cell_x = agent_x + dx;
                int cell_y = agent_y + dy;
                int obs_x = dx + env->agent_sight_range;
                int obs_y = dy + env->agent_sight_range;

                // Check if the cell is within bounds
                if (cell_x < 0 || cell_x >= env->grid_size || cell_y < 0 || cell_y >= env->grid_size) {
                    continue;
                }

                Entity* thisEntity = env->grid[get_grid_index(env, cell_x, cell_y)].entity;
                if (!thisEntity) {
                    continue;
                }

                int offset = agent_idx*5*channel_offset + obs_y*obs_dim + obs_x;
                int obs_idx = offset + thisEntity->type*channel_offset;
                obs[obs_idx] = 1;
                obs_idx = offset + 4*channel_offset;
                obs[obs_idx] = (float)thisEntity->carrying;
            }
        }
    }
}

// Helper functions
void place_random_entities(CTrashPickupEnv* env, int count, int item_type, int gridIndexStart) {
    int placed = 0;
    while (placed < count) 
    {
        int x = rand() % env->grid_size;
        int y = rand() % env->grid_size;

        GridCell* gridCell = &env->grid[get_grid_index(env, x, y)];

        if (gridCell->entity != NULL)
            continue;

        // Allocate and initialize a new Entity
        Entity* newEntity = &env->entities[gridIndexStart];
        newEntity->type = item_type;
        newEntity->pos_x = x;
        newEntity->pos_y = y;
        newEntity->presence = true;
        newEntity->carrying = false;

        gridCell->index = gridIndexStart;
        gridCell->entity = newEntity;

        gridIndexStart++;
        placed++;
    }
}

void add_reward(CTrashPickupEnv* env, int agent_idx, float reward){
    env->rewards[agent_idx] += reward;
    env->total_episode_reward += reward;
}

void move_agent(CTrashPickupEnv* env, int agent_idx, int action) {
    Entity* thisAgent = &env->entities[agent_idx];

    int move_dir_x = 0;
    int move_dir_y = 0;
    if (action == ACTION_UP) move_dir_y = -1;
    else if (action == ACTION_DOWN) move_dir_y = 1;
    else if (action == ACTION_LEFT) move_dir_x = -1;
    else if (action == ACTION_RIGHT) move_dir_x = 1;
    else printf("Undefined action: %d", action);
    
    int new_x = thisAgent->pos_x + move_dir_x;
    int new_y = thisAgent->pos_y + move_dir_y;

    if (new_x < 0 || new_x >= env->grid_size || new_y < 0 || new_y >= env->grid_size)
        return;

    GridCell* currentGridCell = &env->grid[get_grid_index(env, thisAgent->pos_x, thisAgent->pos_y)];
    GridCell* newGridCell = &env->grid[get_grid_index(env, new_x, new_y)];
    int cell_state_type = newGridCell->entity ? newGridCell->entity->type : EMPTY;

    if (cell_state_type == EMPTY) 
    {
        thisAgent->pos_x = new_x;
        thisAgent->pos_y = new_y;

        newGridCell->entity = currentGridCell->entity;
        newGridCell->index = agent_idx;

        currentGridCell->index = -1;
        currentGridCell->entity = NULL;
    } 
    else if (cell_state_type == TRASH && thisAgent->carrying == false) 
    {
        Entity* thisTrash = &env->entities[newGridCell->index];
        thisTrash->presence = false; // Mark as not present
        thisTrash->pos_x = -1;
        thisTrash->pos_y = -1;

        thisAgent->pos_x = new_x;
        thisAgent->pos_y = new_y;
        thisAgent->carrying = true;

        newGridCell->entity = currentGridCell->entity;
        newGridCell->index = currentGridCell->index;

        currentGridCell->entity = NULL;
        currentGridCell->index = -1;

        add_reward(env, agent_idx, env->positive_reward);
    } 
    else if (cell_state_type == TRASH_BIN) 
    {
        if (thisAgent->carrying)
        {
            // Deposit trash into bin
            thisAgent->carrying = false;
            add_reward(env, agent_idx, env->positive_reward);
        }
        else
        {
            int new_bin_x = new_x + move_dir_x;
            int new_bin_y = new_y + move_dir_y;

            if (new_bin_x < 0 || new_bin_x >= env->grid_size || new_bin_y < 0 || new_bin_y >= env->grid_size)
                return;

            GridCell* newGridCellForBin = &env->grid[get_grid_index(env, new_bin_x, new_bin_y)];
            if (newGridCellForBin->entity == NULL) {
                // Move the bin
                Entity* thisBin = newGridCell->entity;
                thisBin->pos_x = new_bin_x;
                thisBin->pos_y = new_bin_y;

                // Move the agent
                thisAgent->pos_x = new_x;
                thisAgent->pos_y = new_y;

                newGridCellForBin->entity = newGridCell->entity;
                newGridCellForBin->index = newGridCell->index;

                newGridCell->entity = currentGridCell->entity;
                newGridCell->index = currentGridCell->index;

                currentGridCell->entity = NULL;
                currentGridCell->index = -1;
            }
            // else don't move the agent
        }
    }
}

bool is_episode_over(CTrashPickupEnv* env) {
    for (int i = 0; i < env->num_agents; i++) 
    {
        if (env->entities[i].carrying) 
            return false;
    }

    int start_index = get_entity_type_start_index(env, TRASH);
    for (int i = start_index; i < start_index + env->num_trash; i++) 
    {
        if (env->entities[i].presence)
            return false;
    }

    return true;
}

void c_reset(CTrashPickupEnv* env) {
    env->current_step = 0;
    env->total_episode_reward = 0;

    for (int i = 0; i < env->grid_size * env->grid_size; i++) 
    {
        env->grid[i].entity = NULL;
        env->grid[i].index = -1;
    }

    // Place trash, bins, and agents randomly across the grid.
    place_random_entities(env, env->num_agents, AGENT, 0);
    place_random_entities(env, env->num_bins, TRASH_BIN, get_entity_type_start_index(env, TRASH_BIN));
    place_random_entities(env, env->num_trash, TRASH, get_entity_type_start_index(env, TRASH));

    compute_observations(env);
}

// Environment functions
void initialize_env(CTrashPickupEnv* env) {
    env->current_step = 0;

    env->positive_reward = 0.5f; // / env->num_trash;
    env->negative_reward = -0.0f; // / (env->max_steps * env->num_agents);

    env->grid = (GridCell*)calloc(env->grid_size * env->grid_size, sizeof(GridCell));
    env->entities = (Entity*)calloc(env->num_agents + env->num_bins + env->num_trash, sizeof(Entity));
    env->total_num_obs = env->num_agents * ((((env->agent_sight_range * 2 + 1) * (env->agent_sight_range * 2 + 1)) * 5));
}

void allocate(CTrashPickupEnv* env) {
    initialize_env(env);
    env->observations = (char*)calloc(env->total_num_obs, sizeof(char));
    env->actions = (int*)calloc(env->num_agents, sizeof(int));
    env->rewards = (float*)calloc(env->num_agents, sizeof(float));
    env->terminals = (unsigned char*)calloc(env->num_agents, sizeof(unsigned char));
}

void c_step(CTrashPickupEnv* env) {
    // Reset reward for each agent
    memset(env->rewards, 0, sizeof(float) * env->num_agents);
    memset(env->terminals, 0, sizeof(unsigned char) * env->num_agents);

    for (int i = 0; i < env->num_agents; i++) {
        move_agent(env, i, env->actions[i]);
        add_reward(env, i, env->negative_reward); // small negative reward to encourage efficiency
    }

    env->current_step++;
    if (env->current_step >= env->max_steps || is_episode_over(env)) 
    {
        memset(env->terminals, 1, sizeof(unsigned char) * env->num_agents);

        Log log = {0};

        log.episode_length = env->current_step;
        log.episode_return = env->total_episode_reward;

        int total_trash_not_collected = 0;
        for (int i = env->num_agents + 1; i < env->num_agents + env->num_trash; i++) 
        {
            total_trash_not_collected += env->entities[i].presence;
        }

        log.trash_collected = (float) (env->num_trash - total_trash_not_collected);
        log.score = log.trash_collected;
        log.perf = log.score / env->num_trash;
        add_log(env, &log);
        c_reset(env);
    }

    compute_observations(env);
}

void c_close(CTrashPickupEnv* env) {
    free(env->grid);
    free(env->entities);
}

void free_allocated(CTrashPickupEnv* env) {
    free(env->observations);
    free(env->actions);
    free(env->rewards);
    free(env->terminals);
    c_close(env);
}

const Color PUFF_RED = (Color){187, 0, 0, 255};
const Color PUFF_CYAN = (Color){0, 187, 187, 255};
const Color PUFF_WHITE = (Color){241, 241, 241, 241};
const Color PUFF_BACKGROUND = (Color){6, 24, 24, 255};
const Color PUFF_LINES = (Color){50, 50, 50, 255};

// Initialize a rendering client
Client* make_client(CTrashPickupEnv* env) {
    const int CELL_SIZE = 40;
    Client* client = (Client*)malloc(sizeof(Client));
    client->cell_size = CELL_SIZE;
    client->header_offset = 60;
    client->window_width = env->grid_size * CELL_SIZE;
    client->window_height = client->window_width + client->header_offset;

    InitWindow(client->window_width, client->window_height, "Trash Pickup Environment");
    SetTargetFPS(60);

    client->agent_texture = LoadTexture("resources/shared/puffers_128.png");

    return client;
}

// Render the TrashPickup environment
void c_render(CTrashPickupEnv* env) {
    if (env->client == NULL) {
        env->client = make_client(env);
    }
    Client* client = env->client;

    if (IsKeyDown(KEY_ESCAPE)) {
        exit(0);
    }

    BeginDrawing();
    ClearBackground(PUFF_BACKGROUND);

    // Draw header with current step and total episode reward
    int start_index = get_entity_type_start_index(env, TRASH);
    int total_trash_not_collected = 0;
    for (int i = start_index; i < start_index + env->num_trash; i++){
        total_trash_not_collected += env->entities[i].presence;
    }

    DrawText(
        TextFormat(
            "Step: %d\nTotal Episode Reward: %.2f\nTrash Collected: %d/%d",
            env->current_step,
            env->total_episode_reward,
            env->num_trash - total_trash_not_collected,
            env->num_trash
        ),
        5, 2, 10, PUFF_WHITE
    );

    // Draw the grid and its elements
    for (int x = 0; x < env->grid_size; x++) {
        for (int y = 0; y < env->grid_size; y++) {
            GridCell gridCell = env->grid[get_grid_index(env, x, y)];

            int cell_type;
            if (gridCell.entity)
            {
                cell_type = gridCell.entity->type;
            }
            else
            {
                cell_type = EMPTY;
            }

            int screen_x = x * client->cell_size;
            int screen_y = y * client->cell_size + client->header_offset;

            Rectangle cell_rect = {
                .x = screen_x,
                .y = screen_y,
                .width = client->cell_size,
                .height = client->cell_size
            };

            // Draw grid cell border
            DrawRectangleLines((int)cell_rect.x, (int)cell_rect.y, (int)cell_rect.width, (int)cell_rect.height, PUFF_LINES);

            // Draw grid cell content
            if (cell_type == EMPTY)
                continue;

            if (cell_type == TRASH) {
                DrawRectangle(
                    screen_x + client->cell_size / 4,
                    screen_y + client->cell_size / 4,
                    client->cell_size / 2,
                    client->cell_size / 2,
                    PUFF_CYAN
                );
            } else if (cell_type == TRASH_BIN) {
                DrawRectangle(
                    screen_x + client->cell_size / 8,
                    screen_y + client->cell_size / 8,
                    3 * client->cell_size / 4,
                    3 * client->cell_size / 4,
                    PUFF_RED
                );
            } else if (cell_type == AGENT) {
                Color color;
                if (env->do_human_control && gridCell.index == 0)
                {
                    // Make human controlled agent red
                    color = (Color){255, 128, 128, 255};
                }
                else
                {
                    // Non-human controlled agent
                    color = WHITE;
                }

                DrawTexturePro(
                    client->agent_texture, 
                    (Rectangle) {0, 0, 128, 128},
                    (Rectangle) {
                        screen_x + client->cell_size / 2, 
                        screen_y + client->cell_size / 2,
                        client->cell_size,
                        client->cell_size
                        },
                    (Vector2){client->cell_size / 2, client->cell_size / 2},
                    0,
                    color
                );

                Entity* thisAgent = &env->entities[gridCell.index];
                
                if (thisAgent->carrying)
                {
                    DrawRectangle(
                        screen_x + client->cell_size / 2,
                        screen_y + client->cell_size / 2,
                        client->cell_size / 4,
                        client->cell_size / 4,
                        PUFF_CYAN
                    );
                }
            }
        }
    }

    EndDrawing();
}

// Cleanup and free the rendering client
void close_client(Client* client) {
    UnloadTexture(client->agent_texture);
    CloseWindow();
    free(client);
}



================================================
FILE: pufferlib/ocean/trash_pickup/trash_pickup.py
================================================
import numpy as np
from gymnasium import spaces

import pufferlib
from pufferlib.ocean.trash_pickup import binding

class TrashPickupEnv(pufferlib.PufferEnv):
    def __init__(self, num_envs=1, render_mode=None, report_interval=1, buf=None, 
                 grid_size=10, num_agents=3, num_trash=15, num_bins=2, max_steps=300, agent_sight_range=5, seed=0):
        # Env Setup
        self.render_mode = render_mode
        self.report_interval = report_interval

        # Validate num_agents
        if not isinstance(num_agents, int) or num_agents <= 0:
            raise ValueError("num_agents must be an integer greater than 0.")
        self.num_agents = num_envs * num_agents
        self.num_agents_per_env = num_agents

        # Handle num_trash input
        if not isinstance(num_trash, int) or num_trash <= 0:
            raise ValueError("num_trash must be an int > 0")
        self.num_trash = num_trash

        # Handle num_bins input
        if not isinstance(num_bins, int) or num_bins <= 0:
            raise ValueError("num_bins must be an int > 0")
        self.num_bins = num_bins

        if not isinstance(max_steps, int) or max_steps < 10:
            raise ValueError("max_steps must be an int >= 10")
        self.max_steps = max_steps

        if not isinstance(agent_sight_range, int) or agent_sight_range < 2:
            raise ValueError("agent sight range must be an int >= 2")
        self.agent_sight_range = agent_sight_range

        # Calculate minimum required grid size
        min_grid_size = int((num_agents + self.num_trash + self.num_bins) ** 0.5) + 1
        if not isinstance(grid_size, int) or grid_size < min_grid_size:
            raise ValueError(
                f"grid_size must be an integer >= {min_grid_size}. "
                f"Received grid_size={grid_size}, with num_agents={num_agents}, num_trash={self.num_trash}, and num_bins={self.num_bins}."
            )
        self.grid_size = grid_size

        # Entity Attribute Based Obs-Space
        # num_obs_trash = num_trash * 3  # [presence, x pos, y pos] for each trash
        # num_obs_bin = num_bins * 2  # [x pos, y pos] for each bin
        # num_obs_agent = num_agents * 3  # [carrying trash, x pos, y pos] for each agent
        # self.num_obs = num_obs_trash + num_obs_bin + num_obs_agent;
        
        # 2D Local crop obs space
        self.num_obs = ((((agent_sight_range * 2 + 1) * (agent_sight_range * 2 + 1)) * 5));  # one-hot encoding for all cell types in local crop around agent (minus the cell the agent is currently in)

        self.single_observation_space = spaces.Box(low=0, high=1,
            shape=(self.num_obs,), dtype=np.int8)
        self.single_action_space = spaces.Discrete(4)

        super().__init__(buf=buf)
        c_envs = []
        for i in range(num_envs):
            env_id = binding.env_init(
                self.observations[i*num_agents:(i+1)*num_agents],
                self.actions[i*num_agents:(i+1)*num_agents],
                self.rewards[i*num_agents:(i+1)*num_agents],
                self.terminals[i*num_agents:(i+1)*num_agents],
                self.truncations[i*num_agents:(i+1)*num_agents],
                i + seed * num_envs,
                num_agents=num_agents,
                grid_size=grid_size,
                num_trash=num_trash,
                num_bins=num_bins,
                max_steps=max_steps,
                agent_sight_range=agent_sight_range,
            )
            c_envs.append(env_id)

        self.c_envs = binding.vectorize(*c_envs)

    def reset(self, seed=None):
        binding.vec_reset(self.c_envs, seed)
        self.tick = 0
        return self.observations, []

    def step(self, actions):
        self.actions[:] = actions
        binding.vec_step(self.c_envs)
        self.tick += 1

        info = []
        if self.tick % self.report_interval == 0:
            log = binding.vec_log(self.c_envs)
            if log:
                info.append(log)

        return (self.observations, self.rewards,
            self.terminals, self.truncations, info)

    def render(self):
        binding.vec_render(self.c_envs, 0)
        
    def close(self):
        binding.vec_close(self.c_envs)

def test_performance(timeout=10, atn_cache=1024):
    env = TrashPickupEnv(num_envs=1024, grid_size=10, num_agents=4,
        num_trash=20, num_bins=1, max_steps=150, agent_sight_range=5)
 
    env.reset()
    tick = 0

    actions = np.random.randint(0, 4, (atn_cache, env.num_agents))

    import time
    start = time.time()
    while time.time() - start < timeout:
        atn = actions[tick % atn_cache]
        env.step(atn)
        tick += 1

    print(f'SPS: %f', env.num_agents * tick / (time.time() - start))

if __name__ == '__main__':
    test_performance()



================================================
FILE: pufferlib/ocean/tripletriad/binding.c
================================================
#include "tripletriad.h"

#define Env CTripleTriad
#include "../env_binding.h"

static int my_init(Env* env, PyObject* args, PyObject* kwargs) {
    env->width = unpack(kwargs, "width");
    env->height = unpack(kwargs, "height");
    env->card_width = unpack(kwargs, "card_width");
    env->card_height = unpack(kwargs, "card_height");
    init_ctripletriad(env);
    return 0;
}

static int my_log(PyObject* dict, Log* log) {
    assign_to_dict(dict, "perf", log->perf);
    assign_to_dict(dict, "score", log->score);
    assign_to_dict(dict, "episode_return", log->episode_return);
    assign_to_dict(dict, "episode_length", log->episode_length);
    assign_to_dict(dict, "n", log->n);
    return 0;
}



================================================
FILE: pufferlib/ocean/tripletriad/tripletriad.c
================================================
#include "tripletriad.h"
#include "puffernet.h"
#include <time.h>

#define NOOP -1

void interactive() {
    Weights* weights = load_weights("resources/tripletriad/tripletriad_weights.bin", 148751);
    int logit_sizes[1] = {14};
    LinearLSTM* net = make_linearlstm(weights, 1, 114, logit_sizes, 1);

    CTripleTriad env = {
        .width = 990,
        .height = 690,
        .card_width = 576 / 3,
        .card_height = 672 / 3,
        .game_over = 0,
        .num_cards = 10,
    };
    allocate_ctripletriad(&env);
    c_reset(&env); 
    env.client = make_client(env.width, env.height);

    int tick = 0;
    int action;
    while (!WindowShouldClose()) {
        action = NOOP;

        // User can take control of the player
        if (IsKeyDown(KEY_LEFT_SHIFT)) {
            // Handle Card Selection ( 1-5 for selecting a card)
            if (IsKeyPressed(KEY_ONE)) action = SELECT_CARD_1;
            if (IsKeyPressed(KEY_TWO)) action = SELECT_CARD_2;
            if (IsKeyPressed(KEY_THREE)) action = SELECT_CARD_3;
            if (IsKeyPressed(KEY_FOUR)) action = SELECT_CARD_4;
            if (IsKeyPressed(KEY_FIVE)) action = SELECT_CARD_5;

            // Handle Card Placement ( 1-9 for placing a card)
            if (IsMouseButtonPressed(MOUSE_LEFT_BUTTON)) {
                Vector2 mousePos = GetMousePosition();
        
                // Calculate the offset for the board
                int boardOffsetX = 196 + 10; // 196 from the DrawRectangle call in render(), plus 10 for padding
                int boardOffsetY = 30; // From the DrawRectangle call in render()
                
                // Adjust mouse position relative to the board
                int relativeX = mousePos.x - boardOffsetX;
                int relativeY = mousePos.y - boardOffsetY;
                
                // Calculate cell indices
                int cellX = relativeX / env.card_width;
                int cellY = relativeY / env.card_height;
                
                // Calculate the cell index (1-9) based on the click position
                int cellIndex = cellY * 3 + cellX+1; 
                
                // Ensure the click is within the game board
                if (cellX >= 0 && cellX < 3 && cellY >= 0 && cellY < 3) {
                    action = cellIndex + 4;
                }
            }
        } else if (tick % 45 == 0) {
            forward_linearlstm(net, env.observations, env.actions);
            action = env.actions[0];
        }

        tick = (tick + 1) % 45;

        if (action != NOOP) {
            env.actions[0] = action;
            c_step(&env);
        }

        c_render(&env);
    }
    free_linearlstm(net);
    free(weights);
    close_client(env.client);
    free_allocated_ctripletriad(&env);
}

void performance_test() {
    long test_time = 10;
    CTripleTriad env = {
        .width = 990,
        .height = 690,
        .card_width = 576 / 3,
        .card_height = 672 / 3,
        .game_over = 0,
        .num_cards = 10,
    };
    allocate_ctripletriad(&env);
    c_reset(&env);

    long start = time(NULL);
    int i = 0;
    while (time(NULL) - start < test_time) {
        c_step(&env);
        i++;
    }
    long end = time(NULL);
    printf("SPS: %ld\n", i / (end - start));
    free_allocated_ctripletriad(&env);
}

int main() {
    // performance_test();
    interactive();
    return 0;
}



================================================
FILE: pufferlib/ocean/tripletriad/tripletriad.h
================================================
#include <stdlib.h>
#include <math.h>
#include "raylib.h"
#include <stdio.h>

#define SELECT_CARD_1 0
#define SELECT_CARD_2 1
#define SELECT_CARD_3 2
#define SELECT_CARD_4 3
#define SELECT_CARD_5 4
#define PLACE_CARD_1 5
#define PLACE_CARD_2 6
#define PLACE_CARD_3 7
#define PLACE_CARD_4 8
#define PLACE_CARD_5 9
#define PLACE_CARD_6 10
#define PLACE_CARD_7 11
#define PLACE_CARD_8 12
#define PLACE_CARD_9 13
#define TICK_RATE 1.0f/60.0f
#define MAX_EPISODE_LENGTH 30

const Color PUFF_RED = (Color){187, 0, 0, 255};
const Color PUFF_CYAN = (Color){0, 187, 187, 255};
const Color PUFF_WHITE = (Color){241, 241, 241, 241};
const Color PUFF_BACKGROUND = (Color){6, 24, 24, 255};

// how to start game compile - LD_LIBRARY_PATH=raylib-5.0_linux_amd64/lib ./tripletriadgame 

typedef struct Log Log;
struct Log {
    float perf;
    float score;
    float episode_return;
    float episode_length;
    float n;
};

typedef struct Client Client;
typedef struct CTripleTriad CTripleTriad;
struct CTripleTriad {
    float* observations;
    int* actions;
    float* rewards;
    unsigned char* terminals;
    Log log;
    int card_width;
    int card_height;
    float* board_x;
    float* board_y;
    int** board_states;
    int width;
    int height;
    int game_over;
    int num_cards;
    int*** cards_in_hand;
    int* card_selected;
    int** card_locations;
    int* action_masks;
    int*** board_card_values;
    int* score;
    int tick;
    float perf;
    float episode_return;
    float episode_length;
    Client* client;
};

void add_log(CTripleTriad* env) {
    env->log.perf += env->perf;
    env->log.score += env->score[0];
    env->log.episode_return += env->episode_return;
    env->log.episode_length += env->episode_length;
    env->log.n += 1;
}

void generate_board_positions(CTripleTriad* env) {
    for (int row = 0; row < 3; row++) {
        for (int col = 0; col < 3; col++) {
            int idx = row * 3 + col;
            env->board_x[idx] = col* env->card_width;
            env->board_y[idx] = row*env->card_height;
        }
    }
}

void generate_cards_in_hand(CTripleTriad* env) {
    for(int i=0; i< 2; i++) {
        for(int j=0; j< 5; j++) {
            for(int k=0; k< 4; k++) {
                env->cards_in_hand[i][j][k] = (rand() % 7) + 1;
            }
        }
    }
}

void generate_card_locations(CTripleTriad* env) {
    for(int i=0; i< 2; i++) {
        for(int j=0; j< 5; j++) {
            env->card_locations[i][j] = 0;
        }
    }
}

void generate_card_selections(CTripleTriad* env) {
    for(int i=0; i< 2; i++) {
        env->card_selected[i] = -1;
    }
}

void generate_board_states(CTripleTriad* env) {
    for(int i=0; i< 3; i++) {
        for(int j=0; j< 3; j++) {
            env->board_states[i][j] = 0;
        }
    }
}

void generate_board_card_values(CTripleTriad* env) {
    for(int i=0; i< 3; i++) {
        for(int j=0; j< 3; j++) {
            for(int k=0; k< 4; k++) {
                env->board_card_values[i][j][k] = 0;
            }
        }
    }
}

void generate_scores(CTripleTriad* env) {
    for(int i=0; i< 2; i++) {
        env->score[i] = 5;
    }
}

void init_ctripletriad(CTripleTriad* env) {
    // Allocate memory for board_x, board_y, and board_states
    env->board_x = (float*)calloc(9, sizeof(float));
    env->board_y = (float*)calloc(9, sizeof(float));
    env->cards_in_hand = (int***)calloc(2, sizeof(int**));
    env->card_selected = (int*)calloc(2, sizeof(int));
    env->card_locations = (int**)calloc(2, sizeof(int*));
    env->action_masks = (int*)calloc(15, sizeof(int));
    env->board_states = (int**)calloc(3, sizeof(int*));
    env->board_card_values = (int***)calloc(3, sizeof(int**));
    env->score = (int*)calloc(2, sizeof(int));
    for(int i=0; i< 2; i++) {
        env->cards_in_hand[i] = (int**)calloc(5, sizeof(int*));
        for(int j=0; j< 5; j++) {
            env->cards_in_hand[i][j] = (int*)calloc(4, sizeof(int));
        }
    }
    for(int i=0; i< 3; i++) {
        env->board_states[i] = (int*)calloc(3, sizeof(int));
    }
    for(int i=0; i< 2; i++) {
        env->card_locations[i] = (int*)calloc(5, sizeof(int));
    }
    for(int i=0; i< 3; i++) {
        env->board_card_values[i] = (int**)calloc(3, sizeof(int*));
        for(int j=0; j< 3; j++) {
            env->board_card_values[i][j] = (int*)calloc(4, sizeof(int));
        }
    }
    generate_board_positions(env);
    generate_cards_in_hand(env);
    generate_card_locations(env);
    generate_card_selections(env);
    generate_board_states(env);
    generate_board_card_values(env);
    generate_scores(env);
}

void allocate_ctripletriad(CTripleTriad* env) {
    env->actions = (int*)calloc(1, sizeof(int));
    env->observations = (float*)calloc(env->width*env->height, sizeof(float));
    env->terminals = (unsigned char*)calloc(1, sizeof(unsigned char));
    env->rewards = (float*)calloc(1, sizeof(float));
    init_ctripletriad(env);
}

void c_close(CTripleTriad* env) {
    free(env->board_x);
    free(env->board_y);
    for(int i=0; i< 2; i++) {
        for(int j=0; j< 5; j++) {
            free(env->cards_in_hand[i][j]);
        }
        free(env->cards_in_hand[i]);
        free(env->card_locations[i]);
    }
    free(env->cards_in_hand);
    free(env->card_locations);
    free(env->card_selected);
    free(env->action_masks);
    for(int i=0; i< 3; i++) {
        free(env->board_states[i]);
    }
    free(env->board_states);
    for(int i=0; i< 3; i++) {
        for(int j=0; j< 3; j++) {
            free(env->board_card_values[i][j]);
        }
        free(env->board_card_values[i]);
    }
    free(env->board_card_values);
    free(env->score);
}

void free_allocated_ctripletriad(CTripleTriad* env) {
    free(env->actions);
    free(env->observations);
    free(env->terminals);
    free(env->rewards);
    c_close(env);
}

void compute_observations(CTripleTriad* env) {
    int idx=0;
    for (int i = 0; i < 3; i++) {
        for (int j = 0; j < 3; j++) {
            env->observations[idx] = env->board_states[i][j];
            idx++;
        }
    }
    for (int i = 0; i < 15; i++) {
        env->observations[idx] = env->action_masks[i];
        idx++;
    }

    for (int i = 0; i < 2; i++) {
        env->observations[idx] = env->card_selected[i];
        idx++;
    }
    for (int i = 0; i < 2; i++) {
        env->observations[idx] = env->score[i];
        idx++;
    }
    for (int i=0;i<3;i++) {
        for (int j=0;j<3;j++) {
            for (int k=0;k<4;k++) {
                env->observations[idx] = env->board_card_values[i][j][k];
                idx++;
            }
        }
    }
    for (int i=0;i<2;i++){
        for (int j=0;j<5;j++) {
            for (int k=0;k<4;k++) {
                env->observations[idx] = env->cards_in_hand[i][j][k];
                idx++;
            }
        }
    }
    for (int i=0;i<2;i++) {
        for (int j=0;j<5;j++) {
            env->observations[idx] = env->card_locations[i][j];
            idx++;
        }
    }
}

void c_reset(CTripleTriad* env) {
    env->game_over = 0;
    for(int i=0; i< 2; i++) {
        for(int j=0; j< 5; j++) {
            for(int k=0; k< 4; k++) {
                env->cards_in_hand[i][j][k] = (rand() % 7) + 1;
            }
        }
    }
    for(int i=0; i< 2; i++) {
        for(int j=0; j< 5; j++) {
            env->card_locations[i][j] = 0;
        }
    }
    for(int i=0; i< 2; i++) {
        env->card_selected[i] = -1;
    }
    for(int i=0; i< 3; i++) {
        for(int j=0; j< 3; j++) {
            env->board_states[i][j] = 0;
        }
    }
    for (int i = 0; i < 15; i++) {
        env->action_masks[i] = 0;
    }
    for (int i=0; i< 3; i++) {
        for (int j=0; j< 3; j++) {
            for (int k=0; k< 4; k++) {
                env->board_card_values[i][j][k] = 0;
            }
        }
    }
    for(int i=0; i< 2; i++) {
        env->score[i] = 5;
    }
    env->terminals[0] = 0;
    compute_observations(env);
    env->tick = 0;
    env->episode_length = 0;
    env->episode_return = 0;
}

void select_card(CTripleTriad* env, int card_selected, int player) {
    int player_idx = (player == 1) ? 0 : 1;
    env->card_selected[player_idx] = card_selected-1;
}

void place_card(CTripleTriad* env, int card_placement, int player) {
    // Determine the player index (0 for player 1, 1 for player 2)
    int player_idx = (player == 1) ? 0 : 1;
    // Update the card's location on the board
    env->card_locations[player_idx][env->card_selected[player_idx]] = card_placement;
    // Update the board state to reflect the player who placed the card
    env->board_states[(card_placement-1)/3][(card_placement-1)%3] = player;
    // Copy the card values from the player's hand to the board
    for (int i = 0; i < 4; i++) {
        env->board_card_values[(card_placement-1)/3][(card_placement-1)%3][i] = env->cards_in_hand[player_idx][env->card_selected[player_idx]][i];
    }
}

void update_action_masks(CTripleTriad* env) {
    // First, reset all action masks to 0 (available)
    for (int i = 0; i < 15; i++) {
        env->action_masks[i] = 0;
    }

    // Update masks for card placement
    for (int i = 0; i < 2; i++) {
        for (int j = 0; j < 5; j++) {
            if (env->card_locations[i][j] != 0) {
                int action_idx = env->card_locations[i][j] + 4;
                if (action_idx >= 5 && action_idx < 14) {
                    env->action_masks[action_idx] = 1;  // Mark as unavailable
                }
            }
        }
    }
}

void check_win_condition(CTripleTriad* env, int player) {
    int count = 0;
    for (int i=0; i< 3; i++) {
        for (int j=0; j< 3; j++) {
            if (env->board_states[i][j] !=0) {
                count++;
            } 
        }
    }
    if (count == 9) {
        // add a draw condition and winner value is 0
        if (env->score[0] == env->score[1]) {
            env->terminals[0] = 1;
            env->rewards[0] = 0.0;
            env->game_over = 1;
        } else {
            int winner = env->score[0] > env->score[1] ? 1 : -1;
            env->terminals[0] = 1;
            env->rewards[0] = winner; // 1 for player win, -1 for opponent win
            env->episode_return += winner;
            env->game_over = 1;
        }
    }
    return;
}

int get_bot_card_placement(CTripleTriad* env) {
    int valid_placements[9];  // Maximum 9 possible placements
    int num_valid_placements = 0;

    // Find valid placements
    for (int i = 5; i < 14; i++) {
        if (env->action_masks[i] == 0) {
            valid_placements[num_valid_placements++] = i - 4;
            if (num_valid_placements == 9) break;  // Safety check
        }
    }
    
    // Randomly select a valid placement
    if (num_valid_placements > 0) {
        return valid_placements[rand() % num_valid_placements];
    }

    // If no valid placements, return 0 (this should not happen in a normal game)
    return 0;
}

int get_bot_card_selection(CTripleTriad* env) {
    int valid_selections[5];  // Maximum 5 possible selections
    int num_valid_selections = 0;

    // Find valid selections
    for (int i = 0; i < 5; i++) {
        if (env->card_locations[1][i] == 0) {  // Check if the card has not been placed
            valid_selections[num_valid_selections++] = i + 1;
        }
    }

    // Randomly select a valid card
    if (num_valid_selections > 0) {
        return valid_selections[rand() % num_valid_selections];
    }

    // If no valid selections, return 0 (this should not happen in a normal game)
    return 0;
}

bool check_legal_placement(CTripleTriad* env, int card_placement, int player) {
    int row = (card_placement - 1) / 3;
    int col = (card_placement - 1) % 3;
    if (env->board_states[row][col] != 0) {
        return 0;
    } else {
        return 1;
    }
}

void check_card_conversions(CTripleTriad* env, int card_placement, int player) {
    // Given that card locations last 4 values of the most inner array are organized, NSEW. 
    // Check the cards in those directions and what their values are 
    // If the card in the direction is greater than the card in the current location, then convert the game state of the lower value card to the player with the higher value card.
    int card_idx = card_placement - 1;
    int row = card_idx / 3;
    int col = card_idx % 3;
    int enemy_player = (player == 1) ? -1 : 1;
    int player_idx = (player == 1) ? 0 : 1;
    int enemy_player_idx = (player == 1) ? 1 : 0;
    int values[4] = {
        env->board_card_values[row][col][0],  // North
        env->board_card_values[row][col][1],  // South
        env->board_card_values[row][col][2],  // East
        env->board_card_values[row][col][3]   // West
    };

    int adjacent_indices[4][2] = {
        {row - 1, col},  // North
        {row + 1, col},  // South
        {row, col + 1},  // East
        {row, col - 1}   // West
    };

    int adjacent_value_indices[4] = {1, 0, 3, 2};  // South, North, West, East

    for (int i = 0; i < 4; i++) {
        int adj_row = adjacent_indices[i][0];
        int adj_col = adjacent_indices[i][1];

        // Check if the adjacent cell is within the board
        if (adj_row >= 0 && adj_row < 3 && adj_col >= 0 && adj_col < 3) {
            int adjacent_value = env->board_card_values[adj_row][adj_col][adjacent_value_indices[i]];
            if (adjacent_value < values[i] && adjacent_value != 0 && env->board_states[adj_row][adj_col] == enemy_player) {
                env->board_states[adj_row][adj_col] = player;
                env->score[player_idx]++;
                env->score[enemy_player_idx]--;
            }
        }
    }
}

void c_step(CTripleTriad* env) {
    env->episode_length += 1;
    env->rewards[0] = 0.0;
    int action = env->actions[0];

    if (env->episode_length >= MAX_EPISODE_LENGTH) {
        env->game_over = 1;
        env->episode_return -= 1.0;
        env->rewards[0] -= 1.0;
    }

    // reset the game if game over
    if (env->game_over == 1) {
        env->perf = (env->score[0] > env->score[1]) ? 1.0 : 0.0;
        add_log(env);
        c_reset(env);
        return;
    }
    // select a card if the card is in the range of 1-5 and the card is not placed
    if (action >= SELECT_CARD_1 && action <= SELECT_CARD_5 ) {
        // Prevent model from just swapping between selected cards to avoid playing
        env->episode_return -= 0.1;
        env->rewards[0] -= 0.1;

        int card_selected = action + 1;
        
        if(env->card_locations[0][card_selected-1] == 0) {
            select_card(env, card_selected, 1);
        }
    }
    // place a card if the card is in the range of 1-9 and the card is selected
    else if (action >= PLACE_CARD_1 && action <= PLACE_CARD_9  ) {
        int card_placement = action - 4;
        bool card_placed = false;
        if(env->card_selected[0] >= 0) {
            if(check_legal_placement(env, card_placement, 1)) {
                place_card(env,card_placement, 1);
                check_card_conversions(env, card_placement, 1);
                check_win_condition(env, 1);
                update_action_masks(env);
                env->card_selected[0] = -1;
                card_placed = true;
            } else {
                env->episode_return -= 0.1;
                env->rewards[0] -= 0.1;
            }
        } else {
            env->episode_return -= 0.1;
            env->rewards[0] -= 0.1;
        }

        // opponent turn 
        if (env->terminals[0] == 0 && card_placed == true ) {
            int bot_card_selected = get_bot_card_selection(env);
            if(bot_card_selected > 0) {
                select_card(env,bot_card_selected, -1);
                int bot_card_placement = get_bot_card_placement(env);
                place_card(env,bot_card_placement, -1);
                check_card_conversions(env, bot_card_placement, -1);
                check_win_condition(env, -1);
                update_action_masks(env);
                env->card_selected[1] = -1;
            }
            
        }
    }
    if (env->terminals[0] == 1) {
        env->game_over=1;
    }
    compute_observations(env);
}

typedef struct Client Client;
struct Client {
    float width;
    float height;
};

Client* make_client(int width, int height) {
    Client* client = (Client*)calloc(1, sizeof(Client));
    client->width = width;
    client->height = height;

    InitWindow(width, height, "PufferLib Ray TripleTriad");
    SetTargetFPS(60);

    return client;
}

void c_render(CTripleTriad* env) {
    if (IsKeyDown(KEY_ESCAPE)) {
        exit(0);
    }

    if (env->client == NULL) {
        env->client = make_client(env->width, env->height);
    }

    BeginDrawing();
    ClearBackground(PUFF_BACKGROUND);

    // create 3x3 board for triple triad
    for (int row = 0; row < 3; row++) {
        for (int col = 0; col < 3; col++) {
            int board_idx = row * 3 + col;
            Color piece_color=PURPLE;
            if (env->board_states[row][col] == 0.0) {
                piece_color = PUFF_BACKGROUND;
            } else if (env->board_states[row][col] == 1.0) {
                piece_color = PUFF_CYAN;
            } else if (env->board_states[row][col] == -1.0) {
                piece_color = PUFF_RED;
            }
            int x = env->board_x[board_idx];
            int y = env->board_y[board_idx];
            DrawRectangle(x+196+10 , y+10 , env->card_width, env->card_height, piece_color);
            DrawRectangleLines(x+196+10 , y+10 , env->card_width, env->card_height, PUFF_WHITE);
        }
    }
    for(int i=0; i< 2; i++) {
        for(int j=0; j< 5; j++) {
            // starting locations for cards in hand
            int card_x = (i == 0) ? 10 : (env->width - env->card_width - 10);
            int card_y = 10 + env->card_height/2*j;

            // locations if card is placed
            if (env->card_locations[i][j] != 0) {
                card_x = env->board_x[env->card_locations[i][j]-1] + 196 + 10;
                card_y = env->board_y[env->card_locations[i][j]-1] + 10;
            }
            // Draw card background
            // adjusts card color based on board state 
            Color card_color = (i != 0) ? PUFF_RED : PUFF_CYAN;
            // check if index is in bounds first    
            if (env->card_locations[i][j] != 0) {
                if (env->board_states[(env->card_locations[i][j]-1)/3][(env->card_locations[i][j]-1)%3] == -1) {
                    card_color = PUFF_RED;
                } else if (env->board_states[(env->card_locations[i][j]-1)/3][(env->card_locations[i][j]-1)%3] == 1) {
                    card_color = PUFF_CYAN;
                } else {
                    card_color = (i != 0) ? PUFF_CYAN : PUFF_RED;
                }
            }
            DrawRectangle(card_x, card_y, env->card_width, env->card_height, card_color);
            // change background if card is selected, highlight it
            Rectangle rect = (Rectangle){card_x, card_y, env->card_width, env->card_height};
            if (env->card_selected[i] == j) {
                DrawRectangleLinesEx(rect, 3, PUFF_RED);
            } else {
                DrawRectangleLinesEx(rect, 2, PUFF_WHITE);
            }
        
            for(int k=0; k< 4; k++) {
                int x_offset, y_offset;
                switch(k) {
                    case 0: // North (top)
                        x_offset = card_x + 25;
                        y_offset = card_y + 5;
                        break;
                    case 1: // South (bottom)
                        x_offset = card_x + 25;
                        y_offset = card_y + 45;
                        break;
                    case 2: // East (right)
                        x_offset = card_x + 45;
                        y_offset = card_y + 25;
                        break;
                    case 3: // West (left)
                        x_offset = card_x + 5;
                        y_offset = card_y + 25;
                        break;
                }

                Color text_color = PUFF_WHITE;
                DrawText(TextFormat("%d", env->cards_in_hand[i][j][k]), x_offset, y_offset, 20, text_color);
            }
            // add a little text on the top right that says Card 1, Card 2, Card 3, Card 4, Card 5
            DrawText(TextFormat("Card %d", j+1), card_x + env->card_width -50, card_y + 5, 10, PUFF_WHITE);
        }
        if (i == 0) {
            DrawText(TextFormat("%d", env->score[i]), env->card_width *0.4, env->height - 100, 100, PUFF_WHITE);
        } else {
            DrawText(TextFormat("%d", env->score[i]), env->width - env->card_width *.6, env->height - 100, 100, PUFF_WHITE);
        }
    }
    EndDrawing();

    //PlaySound(client->sound);
}

void close_client(Client* client) {
    CloseWindow();
    free(client);
}



================================================
FILE: pufferlib/ocean/tripletriad/tripletriad.py
================================================
import numpy as np
import gymnasium

import pufferlib
from pufferlib.ocean.tripletriad import binding

class TripleTriad(pufferlib.PufferEnv):
    def __init__(self, num_envs=1, render_mode=None, report_interval=1,
            width=990, height=690, card_width=192, card_height=224, buf=None, seed=0):
        self.single_observation_space = gymnasium.spaces.Box(low=0, high=1,
            shape=(114,), dtype=np.float32)
        self.single_action_space = gymnasium.spaces.Discrete(14)
        self.report_interval = report_interval
        self.render_mode = render_mode
        self.num_agents = num_envs

        super().__init__(buf=buf)
        self.c_envs = binding.vec_init(self.observations, self.actions,
            self.rewards, self.terminals, self.truncations, num_envs, seed, width=width, height=height,
            card_width=card_width, card_height=card_height)

    def reset(self, seed=None):
        self.tick = 0
        if seed is None:
            binding.vec_reset(self.c_envs, 0)
        else:
            binding.vec_reset(self.c_envs, seed)
        return self.observations, []

    def step(self, actions):
        self.actions[:] = actions
        binding.vec_step(self.c_envs)
        self.tick += 1

        info = []
        if self.tick % self.report_interval == 0:
            info.append(binding.vec_log(self.c_envs))

        return (self.observations, self.rewards,
            self.terminals, self.truncations, info)

    def render(self):
        binding.vec_render(self.c_envs, 0)

    def close(self):
        binding.vec_close(self.c_envs)

def test_performance(timeout=10, atn_cache=1024):
    env = TripleTriad(num_envs=1000)
    env.reset()
    tick = 0

    actions = np.random.randint(0, 2, (atn_cache, env.num_agents))

    import time
    start = time.time()
    while time.time() - start < timeout:
        atn = actions[tick % atn_cache]
        env.step(atn)
        tick += 1

    print(f'SPS: {env.num_agents * tick / (time.time() - start):,}')

if __name__ == '__main__':
    test_performance()



================================================
FILE: pufferlib/ocean/whisker_racer/binding.c
================================================
#include "whisker_racer.h"

#define Env WhiskerRacer
#include "../env_binding.h"

static int my_init(Env* env, PyObject* args, PyObject* kwargs) {
    env->frameskip = unpack(kwargs, "frameskip");
    env->width = unpack(kwargs, "width");
    env->height = unpack(kwargs, "height");
    env->llw_ang = unpack(kwargs, "llw_ang");
    env->flw_ang = unpack(kwargs, "flw_ang");
    env->frw_ang = unpack(kwargs, "frw_ang");
    env->rrw_ang = unpack(kwargs, "rrw_ang");
    env->max_whisker_length = unpack(kwargs, "max_whisker_length");
    env->turn_pi_frac = unpack(kwargs, "turn_pi_frac");
    env->maxv = unpack(kwargs, "maxv");
    env->render = unpack(kwargs, "render");
    env->continuous = unpack(kwargs, "continuous");
    env->reward_yellow = unpack(kwargs, "reward_yellow");
    env->reward_green = unpack(kwargs, "reward_green");
    env->gamma = unpack(kwargs, "gamma");
    env->track_width = unpack(kwargs, "track_width");
    env->num_radial_sectors = unpack(kwargs, "num_radial_sectors");
    env->num_points = unpack(kwargs, "num_points");
    env->bezier_resolution = unpack(kwargs, "bezier_resolution");
    env->turn_pi_frac = unpack(kwargs, "turn_pi_frac");
    env->w_ang = unpack(kwargs, "w_ang");
    env->corner_thresh = unpack(kwargs, "corner_thresh");
    env->ftmp1 = unpack(kwargs, "ftmp1");
    env->ftmp2 = unpack(kwargs, "ftmp2");
    env->ftmp3 = unpack(kwargs, "ftmp3");
    env->ftmp4 = unpack(kwargs, "ftmp4");
    env->mode7 = unpack(kwargs, "mode7");
    env->render_many = unpack(kwargs, "render_many");
    env->rng = unpack(kwargs, "rng");
    env->method = unpack(kwargs, "method");
    env->i = unpack(kwargs, "i");

    init(env);
    return 0;
}

static int my_log(PyObject* dict, Log* log) {
    assign_to_dict(dict, "perf", log->perf);
    assign_to_dict(dict, "score", log->score);
    assign_to_dict(dict, "episode_return", log->episode_return);
    assign_to_dict(dict, "episode_length", log->episode_length);
    return 0;
}



================================================
FILE: pufferlib/ocean/whisker_racer/whisker_racer.c
================================================
#include <time.h>
#include "whisker_racer.h"
#include "puffernet.h"

void demo() {
    printf("demo\n");
    Weights* weights = load_weights("resources/whisker_racer/puffer_whisker_racer_weights.bin", 133124);
    int logit_sizes[1] = {3};
    LinearLSTM* net = make_linearlstm(weights, 1, 3, logit_sizes, 1);

    WhiskerRacer env = {
        .frameskip = 1,
        .width = 1080,
        .height = 720,
        .max_whisker_length = 100,
        .turn_pi_frac = 40,
        .maxv = 5,
        .render = 0,
        .continuous = 0,
        .reward_yellow = 0.25,
        .reward_green = -0.001,
        .track_width = 75,
        .num_radial_sectors = 180,
        .num_points = 16,
        .bezier_resolution = 4,
        .w_ang = 0.777,
        .corner_thresh = 0.5,
        .mode7 = 1, // If mode7 = 1 then 640X480 recommended
        .render_many = 0,
        .rng = 3, // rng = 3 for puffer track
        .i = 1, // i = 1 for puffer track
        .method = 1, // method = 1 for puffer track
    };

    allocate(&env);

    env.client = make_client(&env);

    c_reset(&env);
    int frame = 0;
    SetTargetFPS(60);
    while (!WindowShouldClose()) {
        // User can take control of the paddle
        if (IsKeyDown(KEY_LEFT_SHIFT)) {
            if(env.continuous) {
                float move = GetMouseWheelMove();
                float clamped_wheel = fmaxf(-1.0f, fminf(1.0f, move));
                env.actions[0] = clamped_wheel;
            } else {
                env.actions[0] = 1.0;                                               // Straight
                if (IsKeyDown(KEY_LEFT)  || IsKeyDown(KEY_A)) env.actions[0] = 0.0; // Left
                if (IsKeyDown(KEY_RIGHT) || IsKeyDown(KEY_D)) env.actions[0] = 2.0; // Right
            }
        } else if (frame % 4 == 0) {
            // Apply frameskip outside the env for smoother rendering
            int* actions = (int*)env.actions;
            forward_linearlstm(net, env.observations, actions);
            env.actions[0] = actions[0];
        }

        frame = (frame + 1) % 4;
        c_step(&env);
        c_render(&env);
    }
    free_linearlstm(net);
    free(weights);
    free_allocated(&env);
    close_client(env.client);
}

int main() {
    demo();
}



================================================
FILE: pufferlib/ocean/whisker_racer/whisker_racer.h
================================================
#include <stdlib.h>
#include <math.h>
#include <assert.h>
#include <unistd.h>
#include <limits.h>
#include <string.h>
#include "raylib.h"
#include <time.h>

#define LEFT 0
#define NOOP 1
#define RIGHT 2

#define PI2 PI * 2

#define MAX_CONTROL_POINTS 32
#define NUM_RADIAL_SECTORS 16
#define MAX_BEZIER_RESOLUTION 16

typedef struct {
    Vector2 position;
} ControlPoint;

typedef struct {
    ControlPoint controls[MAX_CONTROL_POINTS];
    int num_points;
    Vector2 centerline[MAX_CONTROL_POINTS * MAX_BEZIER_RESOLUTION];
    Vector2 inner_edge[MAX_CONTROL_POINTS * MAX_BEZIER_RESOLUTION];
    Vector2 outer_edge[MAX_CONTROL_POINTS * MAX_BEZIER_RESOLUTION];
    int total_points;
    Vector2 curbs[MAX_CONTROL_POINTS][4];
    int curb_count;
} Track;

typedef struct Log {
    float perf;
    float score;
    float episode_return;
    float episode_length;
    float n;
} Log;

typedef struct Client {
    float width;   // 640
    float height;  // 480
    //float llw_ang; // left left whisker angle
    float flw_ang; // front left whisker angle
    float frw_ang; // front right whisker angle
    //float rrw_ang; // right right whisker angle
    float max_whisker_length;
    float turn_pi_frac; //  (pi / turn_pi_frac is the turn angle)
    float maxv;    // 5
    int render;
    int debug;
} Client;

typedef struct WhiskerRacer {
    Client* client;
    Log log;
    float* observations;
    float* actions;
    float* rewards;
    unsigned char* terminals;
    int i;

    int debug;
    unsigned int rng;
    int render_many;

    float corner_thresh;
    float ftmp1;
    float ftmp2;
    float ftmp3;
    float ftmp4;
    int method;

    float reward_yellow;
    float reward_green;
    float gamma;

    // Game
    int width;
    int height;
    float score;
    int tick;
    int max_score;
    int half_max_score;
    int frameskip;
    int render;
    int continuous;
    int current_sector;
    int sectors_completed[NUM_RADIAL_SECTORS];
    int total_sectors_crossed;
    int track_width;
    int num_radial_sectors;
    int num_points;
    int bezier_resolution;
    Track track;

    // Car
    float px;
    float py;
    float ang;
    float vx;
    float vy;
    float v;
    int near_point_idx;
    float maxv;
    float turn_pi_frac;

    // Whiskers
    int num_whiskers;
    Vector2 whisker_dirs[2];
    float w_ang;
    float llw_ang; // left left whisker angle
    float flw_ang; // front left whisker angle
    float frw_ang; // front right whisker angle
    float rrw_ang; // right right whisker angle
    float llw_length;
    float flw_length;
    float ffw_length;
    float frw_length;
    float rrw_length;
    float max_whisker_length;

    // Math
    float inv_width;
    float inv_height;
    float inv_maxv;
    float inv_pi2;
    float inv_bezier_res;

    Texture2D puffer;

    int texture_initialized;
    int mode7;

} WhiskerRacer;

void c_close(WhiskerRacer* env) {
    //unload_track();
}

void free_allocated(WhiskerRacer* env) {
    free(env->actions);
    free(env->observations);
    free(env->terminals);
    free(env->rewards);
    c_close(env);
}

void add_log(WhiskerRacer* env) {
    env->log.episode_length += env->tick;
    if (env->log.episode_length > 0.01f) {
    }
    env->log.episode_return += env->score;
    env->log.score += env->score;
    env->log.perf += env->score / (float)env->max_score;
    env->log.n += 1;
}

void compute_observations(WhiskerRacer* env) {
    env->observations[0] = env->flw_length;
    env->observations[1] = env->frw_length;
    env->observations[2] = env->score / 100.0f;
}

Client* make_client(WhiskerRacer* env) {
    Client* client = (Client*)calloc(1, sizeof(Client));
    client->width = env->width;
    client->height = env->height;
    //client->llw_ang = env->llw_ang;
    client->flw_ang = env->flw_ang;
    client->frw_ang = env->frw_ang;
    //client->rrw_ang = env->rrw_ang;
    client->max_whisker_length = env->max_whisker_length;
    client->turn_pi_frac = env->turn_pi_frac;
    client->maxv = env->maxv;

    InitWindow(env->width, env->height, "PufferLib Whisker Racer");
    if (env->render_many) SetTargetFPS(10 / env->frameskip);
    else SetTargetFPS(60 / env->frameskip);
    env->puffer = LoadTexture("resources/shared/puffers_128.png");

    return client;
}

void close_client(Client* client) {
    CloseWindow();
    free(client);
}

void get_random_start(WhiskerRacer* env) {
    int start_idx = rand() % env->track.total_points;
    env->near_point_idx = start_idx;

    env->px = env->track.centerline[start_idx].x;
    env->py = env->track.centerline[start_idx].y;

    int next_idx = (start_idx + 1) % env->track.total_points;
    float dx = env->track.centerline[next_idx].x - env->px;
    float dy = env->track.centerline[next_idx].y - env->py;
    env->ang = atan2f(dy, dx);

    //env->whisker_dirs[0] = (Vector2){cosf(env->ang + env->llw_ang), sinf(env->ang + env->llw_ang)};
    //env->whisker_dirs[1] = (Vector2){cosf(env->ang + env->flw_ang), sinf(env->ang + env->flw_ang)};
    //env->whisker_dirs[2] = (Vector2){cosf(env->ang), sinf(env->ang)};
    //env->whisker_dirs[3] = (Vector2){cosf(env->ang + env->frw_ang), sinf(env->ang + env->frw_ang)};
    //env->whisker_dirs[4] = (Vector2){cosf(env->ang + env->rrw_ang), sinf(env->ang + env->rrw_ang)};
    env->whisker_dirs[0] = (Vector2){cosf(env->ang + env->flw_ang), sinf(env->ang + env->flw_ang)};
    env->whisker_dirs[1] = (Vector2){cosf(env->ang + env->frw_ang), sinf(env->ang + env->frw_ang)};

    env->v = env->maxv;
    //env->llw_length = 0.25f;
    env->flw_length = 0.50f;
    //env->ffw_length = 1.00f;
    env->frw_length = 0.50f;
    //env->rrw_length = 0.25f;
}

void reset_radial_progress(WhiskerRacer* env) {
    float center_x = env->width * 0.5f;
    float center_y = env->height * 0.5f;

    float angle = atan2f(env->py - center_y, env->px - center_x);
    if (angle < 0) angle += PI2;

    env->current_sector = (int)(angle / (PI2 / 16.0f)) % 16;

    for (int i = 0; i < 16; i++) {
        env->sectors_completed[i] = 0;
    }
    env->total_sectors_crossed = 0;
}

void reset_round(WhiskerRacer* env) {
    get_random_start(env);
    reset_radial_progress(env);
    env->vx = 0.0f;
    env->vy = 0.0f;
    env->v = env->maxv;
}

void c_reset(WhiskerRacer* env) {
    compute_observations(env);
    env->score = 0;
    reset_round(env);
    env->tick = 0;
}

// Line segment intersection helper function
// Returns 1 if intersection found, 0 otherwise
// If intersection found, stores the parameter t in *t_out (0 <= t <= 1 along the whisker ray)
static inline int line_segment_intersect(Vector2 ray_start, Vector2 ray_dir, float ray_length,
                                       Vector2 seg_start, Vector2 seg_end, float* t_out) {
    Vector2 seg_dir = {seg_end.x - seg_start.x, seg_end.y - seg_start.y};
    Vector2 diff = {seg_start.x - ray_start.x, seg_start.y - ray_start.y};

    float cross_rd_sd = ray_dir.x * seg_dir.y - ray_dir.y * seg_dir.x;

    // Lines are parallel
    if (fabsf(cross_rd_sd) < 1e-3f) {
        return 0;
    }

    float cross_diff_sd = diff.x * seg_dir.y - diff.y * seg_dir.x;
    float cross_diff_rd = diff.x * ray_dir.y - diff.y * ray_dir.x;

    float t = cross_diff_sd / cross_rd_sd;
    float u = cross_diff_rd / cross_rd_sd;

    // Check if intersection is within both line segments
    if (t >= 0.0f && t <= ray_length && u >= 0.0f && u <= 1.0f) {
        *t_out = t;
        return 1;
    }

    return 0;
}

void update_nearest_point(WhiskerRacer* env) {
    float min_dist_sq = 100000;
    int closest_seg = env->near_point_idx;
    Vector2 car_pos = {env->px, env->py};

    int search_range = 3;
    for (int offset = 0; offset <= search_range; offset++) {
        int i = (env->near_point_idx + offset + env->track.total_points) % env->track.total_points;

        Vector2 center = env->track.centerline[i];
        float dx = car_pos.x - center.x;
        float dy = car_pos.y - center.y;
        float dist_sq = dx * dx + dy * dy;

        if (dist_sq < min_dist_sq) {
            min_dist_sq = dist_sq;
            closest_seg = i;
        }
    }

    env->near_point_idx = closest_seg;
}

void calc_whisker_lengths(WhiskerRacer* env) {
    float max_len = env->max_whisker_length;
    float inv_max_len = 1.0f / max_len;

    update_nearest_point(env);

    float* lengths[2] = {
        //&env->llw_length,
        &env->flw_length,
        //&env->ffw_length,
        &env->frw_length
        //&env->rrw_length
    };

    Vector2 car_pos = {env->px, env->py};

    for (int w = 0; w < 2; ++w) {
        Vector2 whisker_dir = env->whisker_dirs[w];
        float min_hit_distance = max_len;

        int window_size = 10;
        for (int offset = -window_size/2; offset <= window_size/2; offset++) {
            int i = (env->near_point_idx + offset + env->track.total_points) % env->track.total_points;
            int next_i = (i + 1) % env->track.total_points;

            float t;

            if (line_segment_intersect(car_pos, whisker_dir, max_len,
                                     env->track.inner_edge[i], env->track.inner_edge[next_i], &t)) {
                if (t < min_hit_distance) {
                    min_hit_distance = t;
                }
                if (t < 0.05) break;
            }

            if (line_segment_intersect(car_pos, whisker_dir, max_len,
                                     env->track.outer_edge[i], env->track.outer_edge[next_i], &t)) {
                if (t < min_hit_distance) {
                    min_hit_distance = t;
                }
                if (t < 0.05) break;
            }
        }

        *lengths[w] = fminf(1.0f, fmaxf(0.0f, min_hit_distance * inv_max_len));

        if (*lengths[w] < 0.05f) { // Car has crashed
            for (int j = 0; j < 2; j++) *lengths[j] = 0.0f;
            env->terminals[0] = 1;
            add_log(env);
            c_reset(env);
        }
    }

    if (*lengths[0] >= 0.99f && *lengths[1] >= 0.99f) { // Car probably left the track
        for (int j = 0; j < 2; j++) *lengths[j] = 0.0f;
        env->terminals[0] = 1;
        add_log(env);
        c_reset(env);
    }
}

void update_radial_progress(WhiskerRacer* env) {
    float center_x = env->width * 0.5f;
    float center_y = env->height * 0.5f;

    float angle = atan2f(env->py - center_y, env->px - center_x);

    if (angle < 0) angle += PI2;

    int sector = (int)(angle / (PI2 / 16.0f));
    sector = sector % env->num_radial_sectors;

    if (sector != env->current_sector) {
        int expected_next = (env->current_sector + 1) % 16;
        if (sector == expected_next) {
            if (!env->sectors_completed[sector]) {
                env->sectors_completed[sector] = 1;
                env->total_sectors_crossed++;
                env->rewards[0] += env->reward_yellow;
                env->score += env->reward_yellow;
            } else { // full lap
                env->rewards[0] += env->reward_yellow;
                env->score += env->reward_yellow;
            }
        }
        env->current_sector = sector;
    }
}

Vector2 EvaluateCubicBezier(Vector2 p0, Vector2 p1, Vector2 p2, Vector2 p3, float t) {
    float u = 1.0f - t;
    float tt = t * t;
    float uu = u * u;
    float uuu = uu * u;
    float ttt = tt * t;

    Vector2 result;
    result.x = uuu * p0.x + 3 * uu * t * p1.x + 3 * u * tt * p2.x + ttt * p3.x;
    result.y = uuu * p0.y + 3 * uu * t * p1.y + 3 * u * tt * p2.y + ttt * p3.y;
    return result;
}

Vector2 GetBezierDerivative(Vector2 p0, Vector2 p1, Vector2 p2, Vector2 p3, float t) {
    float u = 1.0f - t;
    float tt = t * t;
    float uu = u * u;

    Vector2 result;
    result.x = -3 * uu * p0.x + 3 * uu * p1.x - 6 * u * t * p1.x + 6 * u * t * p2.x - 3 * tt * p2.x + 3 * tt * p3.x;
    result.y = -3 * uu * p0.y + 3 * uu * p1.y - 6 * u * t * p1.y + 6 * u * t * p2.y - 3 * tt * p2.y + 3 * tt * p3.y;
    return result;
}

Vector2 NormalizeVector(Vector2 v) {
    float length = sqrtf(v.x * v.x + v.y * v.y);
    if (length < 0.00001f) return (Vector2){0, 0};
    return (Vector2){v.x / length, v.y / length};
}

Vector2 GetPerpendicular(Vector2 v) {
    return (Vector2){-v.y, v.x};
}

void GenerateRandomControlPoints(WhiskerRacer* env) {
    float center_x = env->width * 0.5f;
    float center_y = env->height * 0.5f;

    int n = env->num_points;

    if (env->method == -1) {
        env->method = rand() % 3;
    }

    if (env->method == 0) {
        // Randomly choose distinct, non-adjacent indices for tight and medium corners
        int opt1 = rand() % n;
        int opt2;
        do {
            opt2 = rand() % n;
        } while (opt2 == opt1 || abs(opt2 - opt1) == 1 || abs(opt2 - opt1) == n - 1);

        int opt3, opt4;
        do {
            opt3 = rand() % n;
        } while (opt3 == opt1 || opt3 == opt2);

        do {
            opt4 = rand() % n;
        } while (opt4 == opt1 || opt4 == opt2 || opt4 == opt3 || abs(opt4 - opt3) == 1 || abs(opt4 - opt3) == n - 1);

        // Generate control points
        for (int i = 0; i < n; i++) {
            float angle = (PI2 * i) / n;

            float dist_from_center;
            if (i == opt1) {
                dist_from_center = env->height * 0.2 + (rand() % 30);
            } else if (i == opt2 || i == opt3) {
                dist_from_center = env->height * 0.3 + (rand() % 40);
            } else {
                dist_from_center = env->height * 0.5 + (rand() % 30);
            }

            env->track.controls[i].position.x = center_x + dist_from_center * cosf(angle);
            env->track.controls[i].position.y = center_y + dist_from_center * 0.8f * sinf(angle);
        }
    } // end method 0
    else if (env->method == 1) {

        int corner_types[n];
        int assigned[n];

        // type 0 is close to center, 1 is medium, 2 is far from center
        for (int i = 0; i < n; i++) {
            corner_types[i] = 2;
            assigned[i] = 0;
        }

        int num_med = (n + 2) / 5;
        for (int placed = 0; placed < num_med; placed++) {
            int attempts = 0;
            int pos;
            do {
                pos = rand() % n;
                bool valid = (assigned[pos] == 0);
                if (valid) break;
                attempts++;
            } while (attempts < 50);

            if (attempts < 50) {
                corner_types[pos] = 1;
                assigned[pos] = 1;
            }
        }

        int num_close = (n + 2) / 8;
        for (int placed = 0; placed < num_close; placed++) {
            int attempts = 0;
            int pos;
            do {
                pos = rand() % n;
                int prev_prev = (pos - 2 + n) % n;
                int prev = (pos - 1 + n) % n;
                int next = (pos + 1) % n;

                bool valid = (corner_types[pos] == 2) &&
                            (corner_types[prev_prev] > 0) &&
                            (corner_types[prev] > 0) &&
                            (corner_types[next] > 0) &&
                            //(corner_types[next_next] > 0) &&
                            (assigned[pos] == 0);

                if (valid) break;
                attempts++;
            } while (attempts < 50);

            if (attempts < 50) {
                corner_types[pos] = 0;
                assigned[pos] = 1;
            }
        }

        for (int i = 0; i < n; i++ ) {
            int prev = (i - 1 + n) % n;
            int next = (i + 1 + n) % n;
            if (corner_types[prev] == 0 && corner_types[i] == 2 && corner_types[next] == 0) {
                corner_types[i] = 1;
            }
        }

        // Generate control points
        for (int i = 0; i < n; i++) {
            float angle = (PI2 * i) / n;

            float dist_from_center;
            if (corner_types[i] == 0) {
                dist_from_center = env->height * 0.35 + (rand() % 30);
            } else if (corner_types[i] == 1) {
                dist_from_center = env->height * 0.45 + (rand() % 40);
            } else {
                dist_from_center = env->height * 0.6 + (rand() % 30);
            }

            env->track.controls[i].position.x = center_x + dist_from_center * 1.2f * cosf(angle);
            env->track.controls[i].position.y = center_y + dist_from_center * 0.7f * sinf(angle);
        }

    } // end method 1
    else {
        float base_radius = env->height * 0.5f;
        float variation_strength = 0.5;
        float track_stretch_x = 1.0;
        float track_stretch_y = 0.6;

        float freq1 = 2.0f + (rand() % 5);
        float amp1 = (1.0f / freq1) * (0.9f + 0.2f * (rand() % 100) / 100.0f);
        float phase1 = PI2 * (rand() % 100) / 100.0f;

        float freq2 = 1.0f + (rand() % 2);
        float amp2 = 0.2f + 0.2f * (rand() % 100) / 100.0f;
        float phase2 = PI2 * (rand() % 100) / 100.0f;

        float freq3 = 10.0f + 0.5f * (rand() % 3);
        float amp3 = 0.3f + 0.1f * (rand() % 100) / 100.0f;
        float phase3 = PI2 * (rand() % 100) / 100.0f;

        for (int i = 0; i < n; i++) {
            float angle = (PI2 * i) / n;

            float radius_variation = amp1 * cosf(freq1 * angle + phase1) +
                                amp2 * cosf(freq2 * angle + phase2) +
                                amp3 * cosf(freq3 * angle + phase3);

            float radius = base_radius + (base_radius * variation_strength * radius_variation);

            env->track.controls[i].position.x = center_x + radius * track_stretch_x * cosf(angle);
            env->track.controls[i].position.y = center_y + radius * track_stretch_y * sinf(angle);
        }
    } // end method 2

    float tw2 = env->track_width * 0.5f;

    for (int i = 0; i < n; i++) {
        if (env->track.controls[i].position.x < tw2) env->track.controls[i].position.x = tw2;
        if (env->track.controls[i].position.x > env->width - tw2) env->track.controls[i].position.x = env->width - tw2;
        if (env->track.controls[i].position.y < tw2) env->track.controls[i].position.y = tw2;
        if (env->track.controls[i].position.y > env->height - tw2) env->track.controls[i].position.y = env->height - tw2;

        Vector2 prev = env->track.controls[(i - 1 + n) % n].position;
        Vector2 curr = env->track.controls[i].position;
        Vector2 next = env->track.controls[(i + 1) % n].position;


        float vx1 = prev.x - curr.x;
        float vy1 = prev.y - curr.y;
        float vx2 = next.x - curr.x;
        float vy2 = next.y - curr.y;

        float dot = vx1 * vx2 + vy1 * vy2;
        float mag1 = sqrtf(vx1 * vx1 + vy1 * vy1);
        float mag2 = sqrtf(vx2 * vx2 + vy2 * vy2);

        if (mag1 < 1e-3f || mag2 < 1e-3f) continue;

        float angle_cos = dot / (mag1 * mag2);

        if (angle_cos > env->corner_thresh) {
            float dx = curr.x - center_x;
            float dy = curr.y - center_y;
            float dist = sqrtf(dx * dx + dy * dy);

            float adjust_scale = 0.0f;
            if (dist > 200) {
                adjust_scale = 0.3f * angle_cos;
            }

            env->track.controls[i].position.x = env->track.controls[i].position.x - dx * adjust_scale;
            env->track.controls[i].position.y = env->track.controls[i].position.y - dy * adjust_scale;
        }
    }
}

void GenerateTrackCenterline(WhiskerRacer* env) {
    int point_index = 0;

    for (int i = 0; i < env->num_points; i++) {
        Vector2 p0 = env->track.controls[i].position;
        Vector2 p3 = env->track.controls[(i + 1) % env->num_points].position;

        Vector2 prev = env->track.controls[(i - 1 + env->num_points) % env->num_points].position;
        Vector2 next = env->track.controls[(i + 2) % env->num_points].position;

        Vector2 dir1 = NormalizeVector((Vector2){p3.x - prev.x, p3.y - prev.y});
        Vector2 dir2 = NormalizeVector((Vector2){next.x - p0.x, next.y - p0.y});

        float dist = sqrtf((p3.x - p0.x) * (p3.x - p0.x) + (p3.y - p0.y) * (p3.y - p0.y));

        float control_length;
        if (i == 1 || i == 3) {
            control_length = dist * 0.2f;
        } else if (i == 0 || i == 4) {
            control_length = dist * 0.3f;
        } else {
            control_length = dist * 0.4f;
        }

        Vector2 p1 = (Vector2){p0.x + dir1.x * control_length, p0.y + dir1.y * control_length};
        Vector2 p2 = (Vector2){p3.x - dir2.x * control_length, p3.y - dir2.y * control_length};

        for (int j = 0; j < env->bezier_resolution && point_index < MAX_CONTROL_POINTS * env->bezier_resolution - 1; j++) {
            float t = (float)j * env->inv_bezier_res;
            env->track.centerline[point_index] = EvaluateCubicBezier(p0, p1, p2, p3, t);
            point_index++;
        }
    }
    env->track.total_points = point_index;
}

void GenerateTrackEdges(WhiskerRacer* env) {
    for (int i = 0; i < env->track.total_points; i++) {
        Vector2 current = env->track.centerline[i];
        Vector2 next = env->track.centerline[(i + 1) % env->track.total_points];

        Vector2 tangent = NormalizeVector((Vector2){next.x - current.x, next.y - current.y});
        Vector2 normal = GetPerpendicular(tangent);

        // Create inner and outer edges
        float half_width = env->track_width * 0.5f;
        env->track.inner_edge[i] = (Vector2){current.x - normal.x * half_width, current.y - normal.y * half_width};
        env->track.outer_edge[i] = (Vector2){current.x + normal.x * half_width, current.y + normal.y * half_width};
    }
}

void GenerateCurbs(WhiskerRacer* env) {
    env->track.curb_count = 0;

    for (int i = 0; i < env->num_points; i++) {
        Vector2 prev = env->track.controls[(i - 1 + env->num_points) % env->num_points].position;
        Vector2 curr = env->track.controls[i].position;
        Vector2 next = env->track.controls[(i + 1) % env->num_points].position;

        float vx1 = prev.x - curr.x;
        float vy1 = prev.y - curr.y;
        float vx2 = next.x - curr.x;
        float vy2 = next.y - curr.y;

        Vector2 to_prev = {vx1, vy1};
        Vector2 to_next = {vx2, vy2};

        float cross = to_prev.x * to_next.y - to_prev.y * to_next.x;
        float dot = vx1 * vx2 + vy1 * vy2;

        float mag1 = sqrtf(vx1 * vx1 + vy1 * vy1);
        float mag2 = sqrtf(vx2 * vx2 + vy2 * vy2);

        if (mag1 < 1e-3f || mag2 < 1e-3f) continue;

        float angle_cos = dot / (mag1 * mag2);

        if (angle_cos > -0.8f) {
            int apex_idx = i * env->bezier_resolution;

            Vector2* edge_points = (cross > 0) ? env->track.inner_edge : env->track.outer_edge;

            for (int j = 0; j < 4; j++) {
                int idx = (apex_idx - 1 + j + env->track.total_points) % env->track.total_points; // -2? todo
                env->track.curbs[env->track.curb_count][j] = edge_points[idx];
            }
            env->track.curb_count++;
        }
    }
}

void GenerateRandomTrack(WhiskerRacer* env) {
    GenerateRandomControlPoints(env);
    GenerateTrackCenterline(env);
    GenerateTrackEdges(env);
    GenerateCurbs(env);
}

void TopDownTexture(WhiskerRacer* env, RenderTexture2D* mode7RenderTexture, Vector2* center_points) {
    if (env->texture_initialized == 0) {
        *mode7RenderTexture = LoadRenderTexture(env->width, env->height);

        BeginTextureMode(*mode7RenderTexture);
        ClearBackground(DARKGREEN);
        SetConfigFlags(FLAG_MSAA_4X_HINT);
        ClearBackground(DARKGREEN);
        DrawSplineBasis(center_points, env->track.total_points + 3, env->track_width, BLACK);
        //DrawSplineBasis(center_points, env->track.total_points + 3, 2, WHITE);

        for (int i = 0; i < env->track.curb_count; i++) {
            Vector2 curb_points[4];
            for (int j = 0; j < 4; j++) {
                curb_points[j] = env->track.curbs[i][j];
                curb_points[j].y = env->height - curb_points[j].y; // Flip Y coordinate
            }
            DrawSplineBasis(curb_points, 4, 5.0f, RED); // 5 pixel wide red curbs
        }

        EndTextureMode();
        env->texture_initialized = 1;
    }
}

void Mode7(WhiskerRacer* env, RenderTexture2D mode7RenderTexture) {
    BeginDrawing();
    ClearBackground(SKYBLUE);

    Texture2D scene = mode7RenderTexture.texture;
    float camX = env->px;
    float camY = env->py;
    float camAngle = env->ang;

    Image sceneImage = LoadImageFromTexture(scene);
    Color* pixels = LoadImageColors(sceneImage);

    int height = env->height;
    int width = env->width;
    float inv_width = env->inv_width;
    float inv_height = env->inv_height;

    int horizon = height / 2;

    float cos_ang = cosf(camAngle);
    float sin_ang = sinf(camAngle);

    float cos_ang2 = -2.0f * cos_ang;
    float sin_ang2 = 2.0f * sin_ang;

    float height9 = 9.0f * height;

    DrawRectangle(0, horizon, width, height-horizon, DARKGREEN);

    for (int screenY = horizon; screenY < height; screenY += 3) {
        float row = (float)(screenY - horizon);

        if (row < 0.0001) row = 0.0001;
        float z = height9 / row;

        float dx = -sin_ang * z;
        float dy =  cos_ang * z;

        float sx = camX + dy + dx;
        float sy = camY - dx + dy;

        dx = (sin_ang2 * z) * inv_width * 3.0f;
        dy = (cos_ang2 * z) * inv_width * 3.0f;

        for (int screenX = 0; screenX < width; screenX += 3) {
            int srcX = (int)sx;
            int srcY = (int)sy;

            if (srcX >= 0 && srcX < width &&
                srcY >= 0 && srcY < height) {
                Color color = pixels[srcY * width + srcX];
                DrawRectangle(screenX, screenY, 3, 3, color);
            }

            sx += dx;
            sy += dy;
        }
    }
    UnloadImageColors(pixels);
    UnloadImage(sceneImage);

    int minimap_width = width * 0.333f;
    int minimap_height = height * 0.333f;
    int minimap_x = width - minimap_width;
    int minimap_y = 0;

    DrawTexturePro(
        scene,
        (Rectangle){0, 0, width, -height},
        (Rectangle){minimap_x, minimap_y, minimap_width, minimap_height},
        (Vector2){0, 0},
        0,
        WHITE
    );

    float minimap_puffer_size = 8.0f;
    float minimap_px = minimap_x + (env->px * inv_width) * minimap_width;
    float minimap_py = minimap_y + ((height - env->py) * inv_height) * minimap_height;

    DrawTexturePro(
        env->puffer,
        (Rectangle){0, 0, 128, 128},
        (Rectangle){minimap_px, minimap_py, minimap_puffer_size, minimap_puffer_size},
        (Vector2){minimap_puffer_size / 2.0f, minimap_puffer_size / 2.0f},
        (-env->ang * 180.0f / PI) - 10,
        WHITE
    );

    EndDrawing();
}

void Draw(WhiskerRacer* env, Vector2* center_points) {
    BeginDrawing();
    SetConfigFlags(FLAG_MSAA_4X_HINT);
    ClearBackground(DARKGREEN);
    DrawSplineBasis(center_points, env->track.total_points + 3, env->track_width, BLACK);
    //DrawSplineBasis(center_points, env->track.total_points + 3, 2, WHITE);
    for (int i = 0; i < env->track.curb_count; i++) {
        Vector2 curb_points[4];
        for (int j = 0; j < 4; j++) {
            curb_points[j] = env->track.curbs[i][j];
            curb_points[j].y = env->height - curb_points[j].y; // Flip Y coordinate
        }
        DrawSplineBasis(curb_points, 4, 5.0f, RED); // 5 pixel wide red curbs
    }

    float puffer_width = 48.0f;
    float puffer_height = 48.0f;
    float puffer_x = env->px;
    float puffer_y = env->height - env->py;
    Vector2 origin = {puffer_width / 2.0f, puffer_height / 2.0f};

    DrawTexturePro(
        env->puffer,
        (Rectangle){0, 0, 128, 128},
        (Rectangle){puffer_x, puffer_y, puffer_width, puffer_height},
        origin,
        (-env->ang * 180.0f / PI) - 10,
        (Color){255, 255, 255, 255}
    );

    EndDrawing();
}

void c_render(WhiskerRacer* env) {

    static RenderTexture2D mode7RenderTexture;

    env->render = 1;
    if (env->client == NULL) {
        env->client = make_client(env);
    }

    if (IsKeyDown(KEY_ESCAPE)) {
        exit(0);
    }
    if (IsKeyPressed(KEY_TAB)) {
        ToggleFullscreen();
    }

    if (IsKeyDown(KEY_M)) {
        if (env->mode7 == 1) {
  	    env->mode7 = 0;
        }
        else {
   	    env->mode7 = 1;
        }
    }

    if (env->render_many)
    {
        env->method = rand() % 3;
        GenerateRandomTrack(env);
    }

    Vector2* center_points = malloc(sizeof(Vector2) * (env->track.total_points + 3));
    for (int i = 0; i < env->track.total_points; i++) {
        center_points[i] = env->track.centerline[i];
        center_points[i].y = env->height - center_points[i].y;
    }

    // Without enough overlap it draws a C rather than an O
    center_points[env->track.total_points] = center_points[0];
    center_points[env->track.total_points + 1] = center_points[1];
    center_points[env->track.total_points + 2] = center_points[2];

    if (env->mode7 == 1) {
        TopDownTexture(env, &mode7RenderTexture, center_points);
        Mode7(env, mode7RenderTexture);
    }
    else {
        Draw(env, center_points);
    }

    free(center_points);
}

void init(WhiskerRacer* env) {
    env->tick = 0;

    env->debug = 0;

    env->inv_width = 1.0f / env->width;
    env->inv_height = 1.0f / env->height;
    env->inv_maxv = 1.0f / env->maxv;
    env->inv_pi2 = 1.0f / PI2;
    env->inv_bezier_res = 1.0f / env->bezier_resolution;

    env->flw_ang = -env->w_ang;
    env->frw_ang = env->w_ang;

    env->texture_initialized = 0;

    srand(env->rng + env->i);

    GenerateRandomTrack(env);
}

void allocate(WhiskerRacer* env) {
    init(env);
    env->observations = (float*)calloc(3, sizeof(float));
    env->actions = (float*)calloc(1, sizeof(float));
    env->rewards = (float*)calloc(1, sizeof(float));
    env->terminals = (unsigned char*)calloc(1, sizeof(unsigned char));
}

void step_frame(WhiskerRacer* env, float action) {
    float act = 0.0;

    if (action == LEFT) {
        act = -1.0;
        env->ang += PI / env->turn_pi_frac;
    } else if (action == RIGHT) {
        act = 1.0;
        env->ang -= PI / env->turn_pi_frac;
    }
    if (env->ang > PI2) {
        env->ang -= PI2;
    }
    else if (env->ang < 0) {
        env->ang += PI2;
    }
    if (env->continuous){
        act = action;
    }
    //env->whisker_dirs[0] = (Vector2){cosf(env->ang + env->llw_ang), sinf(env->ang + env->llw_ang)}; // left-left
    //env->whisker_dirs[1] = (Vector2){cosf(env->ang + env->flw_ang), sinf(env->ang + env->flw_ang)}; // front-left
    //env->whisker_dirs[2] = (Vector2){cosf(env->ang), sinf(env->ang)};                               // front-forward
    //env->whisker_dirs[3] = (Vector2){cosf(env->ang + env->frw_ang), sinf(env->ang + env->frw_ang)}; // front-right
    //env->whisker_dirs[4] = (Vector2){cosf(env->ang + env->rrw_ang), sinf(env->ang + env->rrw_ang)}; // right-
    env->whisker_dirs[0] = (Vector2){cosf(env->ang + env->flw_ang), sinf(env->ang + env->flw_ang)};
    env->whisker_dirs[1] = (Vector2){cosf(env->ang + env->frw_ang), sinf(env->ang + env->frw_ang)};

    env->vx = env->v * cosf(env->ang);
    env->vy = env->v * sinf(env->ang);
    env->px = env->px + env->vx;
    env->py = env->py + env->vy;
    if (env->px < 0) env->px = 0;
    else if (env->px > env->width) env->px = env->width;
    if (env->py < 0) env->py = 0;
    else if (env->py > env->height) env->py = env->height;

    calc_whisker_lengths(env);

    update_radial_progress(env);
}

void c_step(WhiskerRacer* env) {
    env->terminals[0] = 0;
    env->rewards[0] = 0.0;

    float action = env->actions[0];
    for (int i = 0; i < env->frameskip; i++) {
        env->tick += 1;
        step_frame(env, action);
    }
    compute_observations(env);
}



================================================
FILE: pufferlib/ocean/whisker_racer/whisker_racer.py
================================================
import numpy as np
import gymnasium
import time

import pufferlib
from pufferlib.ocean.whisker_racer import binding

class WhiskerRacer(pufferlib.PufferEnv):
    def __init__(self, num_envs=1, render_mode=None,
                 frameskip=4, width=1080, height=720,
                 llw_ang=-3.14/4, flw_ang=-3.14/6,
                 frw_ang=3.14/6, rrw_ang=3.14/4,
                 max_whisker_length=100,
                 turn_pi_frac=20,
                 maxv=5, render=0,
                 continuous=False, log_interval=128,
                 reward_yellow=0.25, reward_green=0.0, gamma=0.9, track_width=50,
                 num_radial_sectors=16, num_points=4, bezier_resolution=16, w_ang=0.523,
                 corner_thresh=0.5, ftmp1=0.1, ftmp2=0.1, ftmp3=0.1, ftmp4=0.1,
                 mode7=0, render_many=0, seed=42,
                 buf=None, rng=42, i=1, method=0):
        self.single_observation_space = gymnasium.spaces.Box(low=0, high=1,
                                            shape=(3,), dtype=np.float32)
        self.render_mode = render_mode
        self.num_agents = num_envs
        self.continuous = continuous
        self.log_interval = log_interval
        self.tick = 0

        if continuous:
            self.single_action_space = gymnasium.spaces.Box(low=-1, high=1, shape=(1,), dtype=np.float32)
        else:
            self.single_action_space = gymnasium.spaces.Discrete(3)

        super().__init__(buf)

        if continuous:
            self.actions = self.actions.flatten()
        else:
            self.actions = self.actions.astype(np.float32)

        c_envs = []
        for i in range(num_envs):
            env_id = binding.env_init(
                self.observations[i:i+1],
                self.actions[i:i+1],
                self.rewards[i:i+1],
                self.terminals[i:i+1],
                self.truncations[i:i+1],
                seed, num_envs=num_envs, seed=seed, frameskip=frameskip, width=width, height=height,
                llw_ang=llw_ang, flw_ang=flw_ang, frw_ang=frw_ang, rrw_ang=rrw_ang, max_whisker_length=max_whisker_length,
                turn_pi_frac=turn_pi_frac, maxv=maxv, render=render, continuous=continuous,
                reward_yellow=reward_yellow, reward_green=reward_green, gamma=gamma, track_width=track_width,
                num_radial_sectors=num_radial_sectors, num_points=num_points, bezier_resolution=bezier_resolution, w_ang=w_ang,
                corner_thresh=corner_thresh, ftmp1=ftmp1,ftmp2=ftmp2,ftmp3=ftmp3,ftmp4=ftmp4,
                mode7=mode7, render_many=render_many, rng=rng+i, i=i, method=method
            )
            c_envs.append(env_id)
        self.c_envs = binding.vectorize(*c_envs)

    def reset(self, seed=0):
        binding.vec_reset(self.c_envs, seed)
        self.tick = 0
        return self.observations, []
    
    def step(self, actions):
        #start = time.time()
        if self.continuous:
            self.actions[:] = np.clip(actions.flatten(), -1.0, 1.0)
        else:
            self.actions[:] = actions

        self.tick += 1
        binding.vec_step(self.c_envs)

        info = []
        if self.tick % self.log_interval == 0:
            info.append(binding.vec_log(self.c_envs))
        #end = time.time()
        #print(f"python step took {end - start:.3e} seconds")
        return (self.observations, self.rewards,
            self.terminals, self.truncations, info)

    def render(self):
        binding.vec_render(self.c_envs, 0)

    def close(self):
        binding.vec_close(self.c_envs)

def test_performance(timeout=10, atn_cache=1024):
    print("test_performance in whisker_racer.py")
    env = WhiskerRacer(num_envs=1)
    env.reset()
    tick = 0

    actions = np.random.randint(0, 2, (atn_cache, env.num_agents))

    import time
    start = time.time()
    while time.time() - start < timeout:
        print("atn = actions[tick % atn_cache] in whisker_racer.py")
        atn = actions[tick % atn_cache]
        print("env.step in whisker_racer.py")
        env.step(atn)
        tick += 1

    print(f'SPS: %f', env.num_agents * tick / (time.time() - start))

if __name__ == '__main__':
    print("whisker_racer.py")
    test_performance()



================================================
FILE: pufferlib/resources/battle/bomber.glb
================================================
[Binary file]


================================================
FILE: pufferlib/resources/battle/drone.glb
================================================
Error reading file with 'utf-8': 'utf-8' codec can't decode byte 0xc8 in position 1424: invalid continuation byte


================================================
FILE: pufferlib/resources/battle/fighter.glb
================================================
[Binary file]


================================================
FILE: pufferlib/resources/battle/mothership.glb
================================================
[Binary file]


================================================
FILE: pufferlib/resources/battle/shader_330.fs
================================================
#version 330

// Input vertex attributes (from vertex shader)
in vec2 fragTexCoord;       // Fragment input: vertex attribute: texture coordinates
in vec4 fragColor;          // Fragment input: vertex attribute: color 
in vec3 fragPosition;       // Fragment input: vertex attribute: position

// Input uniform values
uniform sampler2D texture0; // Fragment input: texture

// Output fragment color
out vec4 finalColor;       // Fragment output: pixel color


//Ashima's simplex noise
vec3 mod289(vec3 x) {
  return x - floor(x * (1.0 / 289.0)) * 289.0;
}

vec2 mod289(vec2 x) {
  return x - floor(x * (1.0 / 289.0)) * 289.0;
}

vec3 permute(vec3 x) {
  return mod289(((x*34.0)+10.0)*x);
}

float snoise(vec2 v)
  {
  const vec4 C = vec4(0.211324865405187,  // (3.0-sqrt(3.0))/6.0
                      0.366025403784439,  // 0.5*(sqrt(3.0)-1.0)
                     -0.577350269189626,  // -1.0 + 2.0 * C.x
                      0.024390243902439); // 1.0 / 41.0
// First corner
  vec2 i  = floor(v + dot(v, C.yy) );
  vec2 x0 = v -   i + dot(i, C.xx);

// Other corners
  vec2 i1;
  //i1.x = step( x0.y, x0.x ); // x0.x > x0.y ? 1.0 : 0.0
  //i1.y = 1.0 - i1.x;
  i1 = (x0.x > x0.y) ? vec2(1.0, 0.0) : vec2(0.0, 1.0);
  // x0 = x0 - 0.0 + 0.0 * C.xx ;
  // x1 = x0 - i1 + 1.0 * C.xx ;
  // x2 = x0 - 1.0 + 2.0 * C.xx ;
  vec4 x12 = x0.xyxy + C.xxzz;
  x12.xy -= i1;

// Permutations
  i = mod289(i); // Avoid truncation effects in permutation
  vec3 p = permute( permute( i.y + vec3(0.0, i1.y, 1.0 ))
		+ i.x + vec3(0.0, i1.x, 1.0 ));

  vec3 m = max(0.5 - vec3(dot(x0,x0), dot(x12.xy,x12.xy), dot(x12.zw,x12.zw)), 0.0);
  m = m*m ;
  m = m*m ;

// Gradients: 41 points uniformly over a line, mapped onto a diamond.
// The ring size 17*17 = 289 is close to a multiple of 41 (41*7 = 287)

  vec3 x = 2.0 * fract(p * C.www) - 1.0;
  vec3 h = abs(x) - 0.5;
  vec3 ox = floor(x + 0.5);
  vec3 a0 = x - ox;

// Normalise gradients implicitly by scaling m
// Approximation of: m *= inversesqrt( a0*a0 + h*h );
  m *= 1.79284291400159 - 0.85373472095314 * ( a0*a0 + h*h );

// Compute final noise value at P
  vec3 g;
  g.x  = a0.x  * x0.x  + h.x  * x0.y;
  g.yz = a0.yz * x12.xz + h.yz * x12.yw;
  return 130.0 * dot(m, g);
}


float rand(vec2 co){
  return fract(sin(dot(co.xy ,vec2(12.9898,78.233))) * 43758.5453);
}

void main()
{
    // Color based on height (e.g., gradient from blue to red)
    float height = fragPosition.y/256.0;
    float delta = 0.0;
    for (int i = -4; i < 4; i++) {
        float scale = pow(2.0, float(i));
        delta += 0.02*snoise((1.0+fragPosition.y)*fragPosition.xz/scale);
    }
    //float scale = pow(2.0, 4.0);
    //float delta = round(2.0*snoise(fragPosition.xz/scale))/10.0;
    float val = 0.5 + height;


    float black = 0.25 - 0.25*fract(20.0*height);
    //height = round(height*5.0)/5.0;
    vec4 start_color = vec4(128.0/255.0, 48.0/255.0, 0.0, 1.0);
    vec4 mid_color = vec4(150.0/255.0, 75.0/255.0, 0.0/255.0, 1.0);
    vec4 end_color = vec4(48.0/255.0, 128.0/255.0, 0.0, 1.0);


    //if (height < 0.5) {
    //    finalColor.rgba = mix(start_color, mid_color, 2.0*height);
    //} else {
    //    finalColor.rgba = mix(mid_color, end_color, 2.0*(height-0.5));
    //}

    finalColor.rgba = mix(mid_color, end_color, height);

    finalColor.rgb -= black;

    if (height < 0.001) {
        finalColor.rgb = start_color.rgb + delta;
    }

    //if (height < 5.0) {
    //    float val = 0.5 + height/10.0;
    //    finalColor.rgba = vec4(val+delta, (val+delta)/2.0, 0.0, 1.0);
    //} else if (height < 15.0) {
    //    float val = 0.5 + (height - 5.0)/5.0;
    //    finalColor.rgba = vec4(val+delta, (val+delta)/3.0, 0.0, 1.0);
    //} else {
    //    float val = 0.5 + (height - 15.0)/(32.0 - 15.0)/2.0;
    //    finalColor.rgba = vec4(val+delta, (val+delta)/4.0, 0.0, 1.0);
    //}
}




================================================
FILE: pufferlib/resources/battle/shader_330.vs
================================================
#version 330

// Input vertex attributes
in vec3 vertexPosition;
in vec2 vertexTexCoord;
in vec3 vertexNormal;
in vec4 vertexColor;

// Input uniform values
uniform mat4 mvp;

// Output vertex attributes (to fragment shader)
out vec2 fragTexCoord;
out vec4 fragColor;
out vec3 fragPosition;

// NOTE: Add your custom variables here

void main()
{
    // Send vertex attributes to fragment shader
    fragTexCoord = vertexTexCoord;
    fragColor = vertexColor;
    fragPosition = vertexPosition;

    // Calculate final vertex position
    gl_Position = mvp*vec4(vertexPosition, 1.0);
}



================================================
FILE: pufferlib/resources/impulse_wars/create_texture.py
================================================
import numpy as np
from PIL import Image

PUFF_BG = (0, 0, 0)
PUFF_CYAN = (0, 121, 241)
PUFF_RED = (187, 0, 0)
PUFF_YELLOW = (160, 160, 0)
PUFF_GREEN = (0, 187, 0)

img = np.zeros((256, 256, 3), dtype=np.uint8)
img[:] = PUFF_BG

b = 6

img[:128, :b] = PUFF_CYAN
img[:128, 128-b:128] = PUFF_CYAN
img[:b, :128] = PUFF_CYAN
img[128-b:128, :128] = PUFF_CYAN

img[:128, 128:128+b] = PUFF_RED
img[:128, 256-b:256] = PUFF_RED
img[:b, 128:256] = PUFF_RED
img[128-b:128, 128:256] = PUFF_RED

img[128:256, :b] = PUFF_YELLOW
img[128:256, 128-b:128] = PUFF_YELLOW
img[128:128+b, :128] = PUFF_YELLOW
img[256-b:256, :128] = PUFF_YELLOW

img[128:256, 128:256] = (0, 40, 0)
img[128:256, 128:128+b] = PUFF_GREEN
img[128:256, 256-b:256] = PUFF_GREEN
img[128:128+b, 128:256] = PUFF_GREEN
img[256-b:256, 128:256] = PUFF_GREEN

Image.fromarray(img).save('wall_texture_map.png')




================================================
FILE: pufferlib/resources/impulse_wars/shaders/gls100/bloom.fs
================================================
/*
 * Copyright (c) 2025 Le Juez Victor
 *
 * This software is provided "as-is", without any express or implied warranty. In no event
 * will the authors be held liable for any damages arising from the use of this software.
 *
 * Permission is granted to anyone to use this software for any purpose, including commercial
 * applications, and to alter it and redistribute it freely, subject to the following restrictions:
 *
 *   1. The origin of this software must not be misrepresented; you must not claim that you
 *   wrote the original software. If you use this software in a product, an acknowledgment
 *   in the product documentation would be appreciated but is not required.
 *
 *   2. Altered source versions must be plainly marked as such, and must not be misrepresented
 *   as being the original software.
 *
 *   3. This notice may not be removed or altered from any source distribution.
 */

#version 100

precision mediump float;

#define BLOOM_DISABLED 0
#define BLOOM_ADDITIVE 1
#define BLOOM_SOFT_LIGHT 2

varying vec2 fragTexCoord;

uniform sampler2D uTexColor;
uniform sampler2D uTexBloomBlur;

uniform lowp int uBloomMode;
uniform float uBloomIntensity;

void main()
{
    // Sampling scene color texture
    vec3 result = texture2D(uTexColor, fragTexCoord).rgb;

    // Apply bloom
    vec3 bloom = texture2D(uTexBloomBlur, fragTexCoord).rgb;
    bloom *= uBloomIntensity;

    if (uBloomMode == BLOOM_SOFT_LIGHT) {
        bloom = clamp(bloom.rgb, vec3(0.0), vec3(1.0));
        result = max((result + bloom) - (result * bloom), vec3(0.0));
    } else if (uBloomMode == BLOOM_ADDITIVE) {
        result += bloom;
    }

    // Final color output
    gl_FragColor = vec4(result, 1.0);
}


================================================
FILE: pufferlib/resources/impulse_wars/shaders/gls100/blur.fs
================================================
/*
 * Copyright (c) 2025 Le Juez Victor
 *
 * This software is provided "as-is", without any express or implied warranty. In no event
 * will the authors be held liable for any damages arising from the use of this software.
 *
 * Permission is granted to anyone to use this software for any purpose, including commercial
 * applications, and to alter it and redistribute it freely, subject to the following restrictions:
 *
 *   1. The origin of this software must not be misrepresented; you must not claim that you
 *   wrote the original software. If you use this software in a product, an acknowledgment
 *   in the product documentation would be appreciated but is not required.
 *
 *   2. Altered source versions must be plainly marked as such, and must not be misrepresented
 *   as being the original software.
 *
 *   3. This notice may not be removed or altered from any source distribution.
 */

// NOTE: The coefficients for the two-pass Gaussian blur were generated using:
//       https://lisyarus.github.io/blog/posts/blur-coefficients-generator.html

#version 100

precision mediump float;

varying vec2 fragTexCoord;

uniform sampler2D uTexture;
uniform vec2 uTexelDir;

// hard‐coded offsets & weights
const float O0 = -4.455269417428358;
const float O1 = -2.4751038298192056;
const float O2 = -0.4950160492928827;
const float O3 =  1.485055021558738;
const float O4 =  3.465172537482815;
const float O5 =  5.0;

const float W0 = 0.14587920530480702;
const float W1 = 0.19230308352110734;
const float W2 = 0.21647621943673803;
const float W3 = 0.20809835496561988;
const float W4 = 0.17082879595769634;
const float W5 = 0.06641434081403137;

void main() {
    vec3 result = vec3(0.0);
    result += texture2D(uTexture, fragTexCoord + uTexelDir * O0).rgb * W0;
    result += texture2D(uTexture, fragTexCoord + uTexelDir * O1).rgb * W1;
    result += texture2D(uTexture, fragTexCoord + uTexelDir * O2).rgb * W2;
    result += texture2D(uTexture, fragTexCoord + uTexelDir * O3).rgb * W3;
    result += texture2D(uTexture, fragTexCoord + uTexelDir * O4).rgb * W4;
    result += texture2D(uTexture, fragTexCoord + uTexelDir * O5).rgb * W5;

    gl_FragColor = vec4(result, 1.0);
}



================================================
FILE: pufferlib/resources/impulse_wars/shaders/gls100/grid.fs
================================================
#version 100

precision highp float;

// Input vertex attributes (from vertex shader)
varying vec3 fragPosition;
varying vec4 fragColor;

// Input uniform values
uniform vec2 pos[4];
uniform vec4 color[4];

const float falloff = 6.0;
const float epsilon = 0.1;

void main()
{
    vec3 lightAccum = vec3(0.0);

    // Texel color fetching from texture sampler
    for (int i = 0; i < 4; i++) {
        vec2 playerPos = pos[i];
        vec4 playerColor = color[i];
        float dist = distance(playerPos, fragPosition.xz);
        if (dist == 0.0) {
            continue;
        }

        float intensity = falloff / (dist * dist);
        lightAccum.r += intensity * (playerColor.r / 255.0);
        lightAccum.g += intensity * (playerColor.g / 255.0);
        lightAccum.b += intensity * (playerColor.b / 255.0);
    }

    if (length(lightAccum) < epsilon) {
        discard;
    }

    gl_FragColor = vec4(lightAccum, 1.0);
}



================================================
FILE: pufferlib/resources/impulse_wars/shaders/gls100/shader.vs
================================================
#version 100

// Input vertex attributes
attribute vec3 vertexPosition;
attribute vec2 vertexTexCoord;
attribute vec3 vertexNormal;
attribute vec4 vertexColor;

// Input uniform values
uniform mat4 mvp;
uniform mat4 matModel;
uniform mat4 matNormal;

// Output vertex attributes (to fragment shader)
varying vec3 fragPosition;
varying vec2 fragTexCoord;
varying vec4 fragColor;
varying vec3 fragNormal;

// NOTE: Add here your custom variables

void main()
{
    // Send vertex attributes to fragment shader
    fragPosition = vec3(matModel*vec4(vertexPosition, 1.0));
    fragTexCoord = vertexTexCoord;
    fragColor = vertexColor;
    fragNormal = normalize(vec3(matNormal*vec4(vertexNormal, 1.0)));

    // Calculate final vertex position
    gl_Position = mvp*vec4(vertexPosition, 1.0);
}



================================================
FILE: pufferlib/resources/impulse_wars/shaders/gls330/bloom.fs
================================================
/*
 * Copyright (c) 2025 Le Juez Victor
 *
 * This software is provided "as-is", without any express or implied warranty. In no event
 * will the authors be held liable for any damages arising from the use of this software.
 *
 * Permission is granted to anyone to use this software for any purpose, including commercial
 * applications, and to alter it and redistribute it freely, subject to the following restrictions:
 *
 *   1. The origin of this software must not be misrepresented; you must not claim that you
 *   wrote the original software. If you use this software in a product, an acknowledgment
 *   in the product documentation would be appreciated but is not required.
 *
 *   2. Altered source versions must be plainly marked as such, and must not be misrepresented
 *   as being the original software.
 *
 *   3. This notice may not be removed or altered from any source distribution.
 */

#version 330 core

#define BLOOM_DISABLED 0
#define BLOOM_ADDITIVE 1
#define BLOOM_SOFT_LIGHT 2

noperspective in vec2 fragTexCoord;

uniform sampler2D uTexColor;
uniform sampler2D uTexBloomBlur;

uniform lowp int uBloomMode;
uniform float uBloomIntensity;

out vec4 fragColor;

void main()
{
    // Sampling scene color texture
    vec3 result = texture(uTexColor, fragTexCoord).rgb;

    // Apply bloom
    vec3 bloom = texture(uTexBloomBlur, fragTexCoord).rgb;
    bloom *= uBloomIntensity;

    if (uBloomMode == BLOOM_SOFT_LIGHT) {
        bloom = clamp(bloom.rgb, vec3(0.0), vec3(1.0));
        result = max((result + bloom) - (result * bloom), vec3(0.0));
    } else if (uBloomMode == BLOOM_ADDITIVE) {
        result += bloom;
    }

    // Final color output
    fragColor = vec4(result, 1.0);
}


================================================
FILE: pufferlib/resources/impulse_wars/shaders/gls330/blur.fs
================================================
/*
 * Copyright (c) 2025 Le Juez Victor
 *
 * This software is provided "as-is", without any express or implied warranty. In no event
 * will the authors be held liable for any damages arising from the use of this software.
 *
 * Permission is granted to anyone to use this software for any purpose, including commercial
 * applications, and to alter it and redistribute it freely, subject to the following restrictions:
 *
 *   1. The origin of this software must not be misrepresented; you must not claim that you
 *   wrote the original software. If you use this software in a product, an acknowledgment
 *   in the product documentation would be appreciated but is not required.
 *
 *   2. Altered source versions must be plainly marked as such, and must not be misrepresented
 *   as being the original software.
 *
 *   3. This notice may not be removed or altered from any source distribution.
 */

// NOTE: The coefficients for the two-pass Gaussian blur were generated using:
//       https://lisyarus.github.io/blog/posts/blur-coefficients-generator.html

#version 330 core

noperspective in vec2 fragTexCoord;

uniform sampler2D uTexture;
uniform vec2 uTexelDir;

out vec4 fragColor;

const int SAMPLE_COUNT = 6;

const float OFFSETS[6] = float[6](
    -4.455269417428358,
    -2.4751038298192056,
    -0.4950160492928827,
    1.485055021558738,
    3.465172537482815,
    5
);

const float WEIGHTS[6] = float[6](
    0.14587920530480702,
    0.19230308352110734,
    0.21647621943673803,
    0.20809835496561988,
    0.17082879595769634,
    0.06641434081403137
);

void main()
{
    vec3 result = vec3(0.0);

    for (int i = 0; i < SAMPLE_COUNT; ++i)
    {
        result += texture(uTexture, fragTexCoord + uTexelDir * OFFSETS[i]).rgb * WEIGHTS[i];
    }

    fragColor = vec4(result, 1.0);
}



================================================
FILE: pufferlib/resources/impulse_wars/shaders/gls330/grid.fs
================================================
#version 330

// Input vertex attributes (from vertex shader)
in vec3 fragPosition;
in vec4 fragColor;

// Input uniform values
uniform vec2 pos[4];
uniform vec4 color[4];

// Output fragment color
out vec4 finalColor;

const float falloff = 6.0;
const float epsilon = 0.1;

void main()
{
    vec3 lightAccum = vec3(0.0);

    // Texel color fetching from texture sampler
    for (int i = 0; i < 4; i++) {
        vec2 playerPos = pos[i];
        vec4 playerColor = color[i];
        float dist = distance(playerPos, fragPosition.xz);
        if (dist == 0.0) {
            continue;
        }

        float intensity = falloff / (dist * dist);
        lightAccum.r += intensity * (playerColor.r / 255.0);
        lightAccum.g += intensity * (playerColor.g / 255.0);
        lightAccum.b += intensity * (playerColor.b / 255.0);
    }

    if (length(lightAccum) < epsilon) {
        discard;
    }
    finalColor = vec4(lightAccum, 1.0);
}



================================================
FILE: pufferlib/resources/impulse_wars/shaders/gls330/shader.vs
================================================
#version 330

// Input vertex attributes
in vec3 vertexPosition;
in vec2 vertexTexCoord;
in vec3 vertexNormal;
in vec4 vertexColor;

// Input uniform values
uniform mat4 mvp;
uniform mat4 matModel;
uniform mat4 matNormal;

// Output vertex attributes (to fragment shader)
out vec3 fragPosition;
out vec2 fragTexCoord;
out vec4 fragColor;
out vec3 fragNormal;

// NOTE: Add here your custom variables

void main()
{
    // Send vertex attributes to fragment shader
    fragPosition = vec3(matModel*vec4(vertexPosition, 1.0));
    fragTexCoord = vertexTexCoord;
    fragColor = vertexColor;
    fragNormal = normalize(vec3(matNormal*vec4(vertexNormal, 1.0)));

    // Calculate final vertex position
    gl_Position = mvp*vec4(vertexPosition, 1.0);
}



================================================
FILE: pufferlib/resources/moba/bloom_shader_100.fs
================================================
#version 330

precision mediump float;

// Input vertex attributes (from vertex shader)
varying vec2 fragTexCoord;
varying vec4 fragColor;

// Input uniform values
uniform sampler2D texture0;
uniform vec4 colDiffuse;

// NOTE: Add here your custom variables

const vec2 size = vec2(800, 450);   // Framebuffer size
const float samples = 5.0;          // Pixels per axis; higher = bigger glow, worse performance
const float quality = 2.5;          // Defines size factor: Lower = smaller glow, better quality

void main()
{
    vec4 sum = vec4(0);
    vec2 sizeFactor = vec2(1)/size*quality;

    // Texel color fetching from texture sampler
    vec4 source = texture2D(texture0, fragTexCoord);

    const int range = 2;            // should be = (samples - 1)/2;

    for (int x = -range; x <= range; x++)
    {
        for (int y = -range; y <= range; y++)
        {
            sum += texture2D(texture0, fragTexCoord + vec2(x, y)*sizeFactor);
        }
    }

    // Calculate final fragment color
    gl_FragColor = ((sum/(samples*samples)) + source)*colDiffuse;
}



================================================
FILE: pufferlib/resources/moba/bloom_shader_330.fs
================================================
#version 330

//precision mediump float;

// Input vertex attributes (from vertex shader)
varying vec2 fragTexCoord;
varying vec4 fragColor;

// Input uniform values
uniform sampler2D texture0;
uniform vec4 colDiffuse;

// NOTE: Add here your custom variables

const vec2 size = vec2(800, 450);   // Framebuffer size
const float samples = 5.0;          // Pixels per axis; higher = bigger glow, worse performance
const float quality = 2.5;          // Defines size factor: Lower = smaller glow, better quality

void main()
{
    vec4 sum = vec4(0);
    vec2 sizeFactor = vec2(1)/size*quality;

    // Texel color fetching from texture sampler
    vec4 source = texture2D(texture0, fragTexCoord);

    const int range = 2;            // should be = (samples - 1)/2;

    for (int x = -range; x <= range; x++)
    {
        for (int y = -range; y <= range; y++)
        {
            sum += texture2D(texture0, fragTexCoord + vec2(x, y)*sizeFactor);
        }
    }

    // Calculate final fragment color
    gl_FragColor = ((sum/(samples*samples)) + source)*colDiffuse;
}



================================================
FILE: pufferlib/resources/moba/game_map.npy
================================================
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    


================================================
FILE: pufferlib/resources/moba/map_shader_100.fs
================================================
#version 100

// Vertex shader will need to pass these as varyings
precision mediump float;

varying vec2 fragTexCoord; 
varying vec4 fragColor;          

// Input uniform values
uniform sampler2D texture0; 
uniform sampler2D texture1; 
uniform vec4 colDiffuse;    

//uniform vec3 resolution;    
uniform vec4 mouse;         
uniform float time;         
uniform float camera_x;
uniform float camera_y;

const float SCALE_FACTOR = 16.0;
const float GLOW = 32.0;
const float DIST = GLOW / SCALE_FACTOR;

float rand(vec2 co){
  return fract(sin(dot(co.xy ,vec2(12.9898,78.233))) * 43758.5453);
}

float round(float x) {
    return floor(x + 0.5);
}

void main()
{
    vec3 resolution = vec3(2560.0, 1440.0, 1.0);
    float CONVERT_X = resolution.x / 32.0 / 128.0;
    float CONVERT_Y = resolution.y / 32.0 / 128.0;

    vec2 scaledTexCoord = vec2(CONVERT_X * fragTexCoord.x + camera_x, CONVERT_Y * fragTexCoord.y + camera_y);
    float dist = DIST / resolution.x; // Pass texture size as uniform from JS

    float borderColor = 0.8 + 0.2 * sin(time);

    vec4 texelColor = texture2D(texture1, scaledTexCoord) * colDiffuse * fragColor;
    vec4 texelRight = texture2D(texture1, vec2(scaledTexCoord.x + dist, scaledTexCoord.y)) * colDiffuse * fragColor;
    vec4 texelLeft = texture2D(texture1, vec2(scaledTexCoord.x - dist, scaledTexCoord.y)) * colDiffuse * fragColor;
    vec4 texelDown = texture2D(texture1, vec2(scaledTexCoord.x, scaledTexCoord.y + dist)) * colDiffuse * fragColor;
    vec4 texelUp = texture2D(texture1, vec2(scaledTexCoord.x, scaledTexCoord.y - dist)) * colDiffuse * fragColor;

    vec4 texelRightDown = texture2D(texture1, vec2(scaledTexCoord.x + dist, scaledTexCoord.y + dist)) * colDiffuse * fragColor;
    vec4 texelLeftDown = texture2D(texture1, vec2(scaledTexCoord.x - dist, scaledTexCoord.y + dist)) * colDiffuse * fragColor;
    vec4 texelRightUp = texture2D(texture1, vec2(scaledTexCoord.x + dist, scaledTexCoord.y - dist)) * colDiffuse * fragColor;
    vec4 texelLeftUp = texture2D(texture1, vec2(scaledTexCoord.x  - dist, scaledTexCoord.y - dist)) * colDiffuse * fragColor;

    vec2 tilePos = fract(scaledTexCoord * resolution.xy);

    bool isBorder = (texelColor.rgb == vec3(0.0, 0.0, 0.0)) && (
        (texelColor != texelRight) ||
        (texelColor != texelDown) ||
        (texelColor != texelLeft) ||
        (texelColor != texelUp) ||
        (texelColor != texelRightDown) ||
        (texelColor != texelLeftDown) ||
        (texelColor != texelRightUp) ||
        (texelColor != texelLeftUp));

    float lerp = 10.0 * (scaledTexCoord.x - scaledTexCoord.y);
    float lerp_red = clamp(lerp, 0.0, 1.0);
    float lerp_cyan = clamp(1.0 - lerp, 0.0, 1.0);

    float inp_x = round(4096.0 * scaledTexCoord.x) / 8.0;
    float inp_y = round(4096.0 * scaledTexCoord.y) / 8.0;

    vec2 inp = vec2(inp_x, inp_y);

    if (isBorder) {
        gl_FragColor = vec4(lerp_red * borderColor, lerp_cyan * borderColor, (lerp_cyan + 0.5) * borderColor, 1.0);
    } else if (texelColor.rgb == vec3(1.0, 1.0, 1.0)) {
        gl_FragColor = vec4(18.0 / 255.0 * lerp_red + 6.0 / 255.0, 18.0 / 255.0 * lerp_cyan + 6.0 / 255.0, 18.0 / 255.0 * lerp_cyan + 6.0 / 255.0, 1.0);
        float noise = sin(100.0 * inp.x - 100.0 * inp.y + cos(100.0 * inp.y));
        gl_FragColor.rgb += 0.005 + 0.005 * vec3(lerp_red * noise, lerp_cyan * noise, lerp_cyan * noise);
    } else if (texelColor.rgb == vec3(0.0, 0.0, 0.0)) {
        gl_FragColor = vec4(0.5 * lerp_red, 0.5 * lerp_cyan, 0.5 * lerp_cyan, 1.0);
    } else {
        gl_FragColor = texelColor;
    }
}



================================================
FILE: pufferlib/resources/moba/map_shader_330.fs
================================================
#version 330

// Input vertex attributes (from vertex shader)
in vec2 fragTexCoord;       // Fragment input: vertex attribute: texture coordinates
in vec4 fragColor;          // Fragment input: vertex attribute: color 

// Input uniform values
uniform sampler2D texture0; // Fragment input: texture
uniform sampler2D texture1; // Fragment input: texture
uniform vec4 colDiffuse;    // Fragment input: tint color normalized [0.0f..1.0f]

uniform vec3 resolution;    // Fragment input: .xy texture resolution in pixels, .z scale factor
uniform vec4 mouse;         // Fragment input: .xy mouse position on texture in pixels, .z mouse LMB down, .w mouse RMB down
uniform float time;         // Fragment input: elapsed time in seconds since program started
uniform float camera_x;
uniform float camera_y;

// Output fragment color
out vec4 outputColor;       // Fragment output: pixel color

// Fixed scale factor
const float SCALE_FACTOR = 16.0;

// Glow parameter (number of border pixels to alter)
const float GLOW = 4.0;

const float DIST = GLOW / SCALE_FACTOR;

float rand(vec2 co){
  return fract(sin(dot(co.xy ,vec2(12.9898,78.233))) * 43758.5453);
}

void main()
{
    //float CONVERT_X = 255.0 / textureSize(texture0, 0).x / SCALE_FACTOR;
    //float CONVERT_Y = 255.0 / textureSize(texture0, 0).y / SCALE_FACTOR;

    //float CONVERT_X = 41.0 / 255.0;
    //float CONVERT_Y = 23.0 / 255.0;

    //float convert_x = 41.0 / 128.0;
    //float convert_y = 23.0 / 128.0;

    float CONVERT_X = textureSize(texture0, 0).x / 32 / 128.0;
    float CONVERT_Y = textureSize(texture0, 0).y / 32 / 128.0;

    // Calculate the scaled texture coordinates
    //vec2 scaledTexCoord = vec2(CONVERT_X*fragTexCoord.x, CONVERT_Y*fragTexCoord.y);
    vec2 scaledTexCoord = vec2(CONVERT_X*fragTexCoord.x + camera_x, CONVERT_Y*fragTexCoord.y + camera_y);
    float dist = DIST / textureSize(texture1, 0).x;

    // Calculate the scaled texture coordinates
    //vec2 scaledTexCoord = vec2(fragTexCoord.x/SCALE_FACTOR + camera_x, fragTexCoord.y/SCALE_FACTOR + camera_y);
    //float dist = DIST / textureSize(texture1, 0).x;

	float borderColor = 0.8 + 0.2*sin(time);

    // Texel color fetching from texture sampler with scaled coordinates
    vec4 texelColor = texture(texture1, scaledTexCoord) * colDiffuse * fragColor;
    vec4 texelRight = texture(texture1, vec2(scaledTexCoord.x + dist, scaledTexCoord.y)) * colDiffuse * fragColor;
    vec4 texelLeft = texture(texture1, vec2(scaledTexCoord.x - dist, scaledTexCoord.y)) * colDiffuse * fragColor;
    vec4 texelDown = texture(texture1, vec2(scaledTexCoord.x, scaledTexCoord.y + dist)) * colDiffuse * fragColor;
    vec4 texelUp = texture(texture1, vec2(scaledTexCoord.x, scaledTexCoord.y - dist)) * colDiffuse * fragColor;

    vec4 texelRightDown = texture(texture1, vec2(scaledTexCoord.x + dist, scaledTexCoord.y + dist)) * colDiffuse * fragColor;
    vec4 texelLeftDown = texture(texture1, vec2(scaledTexCoord.x - dist, scaledTexCoord.y + dist)) * colDiffuse * fragColor;
    vec4 texelRightUp = texture(texture1, vec2(scaledTexCoord.x + dist, scaledTexCoord.y - dist)) * colDiffuse * fragColor;
    vec4 texelLeftUp = texture(texture1, vec2(scaledTexCoord.x  - dist, scaledTexCoord.y - dist)) * colDiffuse * fragColor;
    
    // Calculate the position within the scaled tile
    vec2 tilePos = fract(scaledTexCoord * resolution.xy);
    
    // Check if the pixel is on the border of the tile
	bool isBorder = 
    	(texelColor.rgb == vec3(0.0, 0.0, 0.0)) && (
    	(texelColor != texelRight) ||
    	(texelColor != texelDown) ||
    	(texelColor != texelLeft) ||
    	(texelColor != texelUp) ||
    	(texelColor != texelRightDown) ||
    	(texelColor != texelLeftDown) ||
    	(texelColor != texelRightUp) ||
    	(texelColor != texelLeftUp));

    float lerp = 10*(scaledTexCoord.x - scaledTexCoord.y);
    float lerp_red = clamp(lerp, 0, 1);
    float lerp_cyan = clamp(1.0-lerp, 0, 1);

    // Add some noise
    //float inp_x = round(512 * scaledTexCoord.x);
    //float inp_y = round(512 * scaledTexCoord.y);

    float inp_x = round(4096 * scaledTexCoord.x) / 8.0;
    float inp_y = round(4096 * scaledTexCoord.y) / 8.0;

    vec2 inp = vec2(inp_x, inp_y);
    //float noise = sin(inp.x * 0.01 + time * 0.1) * sin(inp.y * 0.01 + time * 0.1);
    //float noise = sin(0.1*(inp.x - inp.y)) + sin(0.1*(inp.x + inp.y));
    //float noise = rand(inp);

    if (isBorder) {
        // Change border pixels to (0, 128, 128, 255)
        outputColor = vec4(lerp_red*borderColor, lerp_cyan*borderColor, (lerp_cyan+0.5)*borderColor, 1.0);
    } else if (texelColor.rgb == vec3(1.0, 1.0, 1.0)) {
        // Change white pixels to (6, 24, 24, 255)
        outputColor = vec4(18.0/255.0*lerp_red + 6.0/255.0, 18.0/255.0*lerp_cyan + 6.0/255.0, 18.0/255.0*lerp_cyan + 6.0/255.0, 1.0);
        float noise = sin(100*inp.x - 100*inp.y + cos(100*inp.y));
        outputColor.rgb += 0.005 + 0.005*vec3(lerp_red*noise, lerp_cyan*noise, lerp_cyan*noise);
    } else if (texelColor.rgb == vec3(0.0, 0.0, 0.0)) {
        // Change black pixels to cyan (0, 255, 255, 255)
        outputColor = vec4(0.5*lerp_red, 0.5*lerp_cyan, 0.5*lerp_cyan, 1.0);
    } else {
        // Keep other colors unchanged
        outputColor = texelColor;
    }


    /*
    if (scaledTexCoord.x < scaledTexCoord.y) {
        if (isBorder) {
            // Change border pixels to (0, 128, 128, 255)
            outputColor = vec4(0.0, borderColor, borderColor, 1.0);
        } else if (texelColor.rgb == vec3(1.0, 1.0, 1.0)) {
            // Change white pixels to (6, 24, 24, 255)
            outputColor = vec4(6.0/255.0, 24.0/255.0, 24.0/255.0, 1.0);
        } else if (texelColor.rgb == vec3(0.0, 0.0, 0.0)) {
            // Change black pixels to cyan (0, 255, 255, 255)
            outputColor = vec4(0.0, 0.5, 0.5, 1.0);
        } else {
            // Keep other colors unchanged
            outputColor = texelColor;
        }
    } else {
        if (isBorder) {
            // Change border pixels to (128, 0, 0, 255)
            outputColor = vec4(borderColor, 0.0, 0.0, 1.0);
        } else if (texelColor.rgb == vec3(1.0, 1.0, 1.0)) {
            // Change white pixels to (24, 6, 6, 255)
            outputColor = vec4(24.0/255.0, 6.0/255.0, 6.0/255.0, 1.0);
        } else if (texelColor.rgb == vec3(0.0, 0.0, 0.0)) {
            // Change black pixels to red (255, 0, 0, 255)
            outputColor = vec4(0.5, 0.0, 0.0, 1.0);
        } else {
            // Keep other colors unchanged
            outputColor = texelColor;
        }
    }
    */
}




================================================
FILE: pufferlib/resources/nmmo3/ASSETS_LICENSE.md
================================================
Characters and assets subject to the license of the original artists. In particular, we use Mana Seed assets by Seliel the Shaper under a valid license purchased from itch.io. You may not repurpose these assets for other projects without purchasing your own license. To mitigate abuse, we release only collated spritesheets as exported by our postprocessor.



================================================
FILE: pufferlib/resources/nmmo3/map_shader_100.fs
================================================
precision mediump float;

// Input uniforms (unchanged from original)
uniform sampler2D terrain;
uniform sampler2D texture_tiles;
uniform vec4 colDiffuse;
uniform vec3 resolution;
uniform vec4 mouse;
uniform float time;
uniform float camera_x;
uniform float camera_y;
uniform float map_width;
uniform float map_height;

// Constants
const float TILE_SIZE = 64.0;
const float TILES_PER_ROW = 64.0;

void main()
{
    float ts = TILE_SIZE * resolution.z;
    // Get the screen pixel coordinates
    vec2 pixelPos = gl_FragCoord.xy;
    
    float x_offset = camera_x/64.0 + pixelPos.x/ts - resolution.x/ts/2.0;
    float y_offset = camera_y/64.0 - pixelPos.y/ts + resolution.y/ts/2.0;
    float x_floor = floor(x_offset);
    float y_floor = floor(y_offset);
    float x_frac = x_offset - x_floor;
    float y_frac = y_offset - y_floor;
    
    // Environment size calculation
    vec2 uv = vec2(
        x_floor/map_width,
        y_floor/map_height
    );
    
    vec2 tile_rg = texture2D(terrain, uv).rg;
    float tile_high_byte = floor(tile_rg.r * 255.0);
    float tile_low_byte = floor(tile_rg.g * 255.0);
    float tile = tile_high_byte * 64.0 + tile_low_byte;
    
    // Handle animated tiles
    if (tile >= 240.0 && tile < (240.0 + 4.0*4.0*4.0*4.0)) {
        tile += floor(3.9 * time);
    }
    
    tile_high_byte = floor(tile/64.0);
    tile_low_byte = floor(mod(tile, 64.0));
    
    vec2 tile_uv = vec2(
        tile_low_byte/64.0 + x_frac/64.0,
        tile_high_byte/64.0 + y_frac/64.0
    );
    
    gl_FragColor = texture2D(texture_tiles, tile_uv);
}



================================================
FILE: pufferlib/resources/nmmo3/map_shader_330.fs
================================================
#version 330

// Input vertex attributes (from vertex shader)
in vec2 fragTexCoord;
in vec4 fragColor;

// Input uniform values
uniform sampler2D terrain;
uniform sampler2D texture_tiles;    // Tile sprite sheet texture
uniform vec4 colDiffuse;
uniform vec3 resolution;
uniform vec4 mouse;
uniform float time;
uniform float camera_x;
uniform float camera_y;
uniform float map_width;
uniform float map_height;

// Output fragment color
out vec4 outputColor;

float TILE_SIZE = 64.0;

// Number of tiles per row in the sprite sheet
const int TILES_PER_ROW = 64; // Adjust this based on your sprite sheet layout

void main()
{
    float ts = TILE_SIZE * resolution.z;

    // Get the screen pixel coordinates
    vec2 pixelPos = gl_FragCoord.xy;

    float x_offset = camera_x/64.0 + pixelPos.x/ts - resolution.x/ts/2.0;
    float y_offset = camera_y/64.0 - pixelPos.y/ts + resolution.y/ts/2.0;

    float x_floor = floor(x_offset);
    float y_floor = floor(y_offset);

    float x_frac = x_offset - x_floor;
    float y_frac = y_offset - y_floor;

    // TODO: This is the env size
    vec2 uv = vec2(
        x_floor/map_width,
        y_floor/map_height
    );
    vec2 tile_rg = texture(terrain, uv).rg;

    int tile_high_byte = int(tile_rg.r*255.0);
    int tile_low_byte = int(tile_rg.g*255.0);

    int tile = tile_high_byte*64 + tile_low_byte;
    if (tile >= 240 && tile < 240+4*4*4*4) {
        tile += int(3.9*time);
    }

    tile_high_byte = int(tile/64.0);
    tile_low_byte = int(tile%64);
 
    vec2 tile_uv = vec2(
        tile_low_byte/64.0 + x_frac/64.0,
        tile_high_byte/64.0 + y_frac/64.0
    );

    outputColor = texture(texture_tiles, tile_uv);
}




================================================
FILE: pufferlib/resources/terraform/shader_100.fs
================================================
#version 100

// Precision qualifiers (required in GLSL 100)
precision highp float;

// Input from vertex shader (use 'varying' instead of 'in')
varying vec2 fragTexCoord;   // Texture coordinates
varying vec4 fragColor;      // Vertex color
varying vec3 fragPosition;   // Vertex position

// Uniforms (texture0 is not used in this shader but kept for completeness)
uniform sampler2D texture0;

// Ashima's simplex noise (unchanged, but ensure precision)
vec3 mod289(vec3 x) {
    return x - floor(x * (1.0 / 289.0)) * 289.0;
}

vec2 mod289(vec2 x) {
    return x - floor(x * (1.0 / 289.0)) * 289.0;
}

vec3 permute(vec3 x) {
    return mod289(((x * 34.0) + 10.0) * x);
}

float snoise(vec2 v) {
    const vec4 C = vec4(0.211324865405187,  // (3.0-sqrt(3.0))/6.0
                        0.366025403784439,  // 0.5*(sqrt(3.0)-1.0)
                       -0.577350269189626,  // -1.0 + 2.0 * C.x
                        0.024390243902439); // 1.0 / 41.0
    // First corner
    vec2 i = floor(v + dot(v, C.yy));
    vec2 x0 = v - i + dot(i, C.xx);

    // Other corners
    vec2 i1;
    i1 = (x0.x > x0.y) ? vec2(1.0, 0.0) : vec2(0.0, 1.0);
    vec4 x12 = x0.xyxy + C.xxzz;
    x12.xy -= i1;

    // Permutations
    i = mod289(i); // Avoid truncation effects in permutation
    vec3 p = permute(permute(i.y + vec3(0.0, i1.y, 1.0))
                     + i.x + vec3(0.0, i1.x, 1.0));

    vec3 m = max(0.5 - vec3(dot(x0, x0), dot(x12.xy, x12.xy), dot(x12.zw, x12.zw)), 0.0);
    m = m * m;
    m = m * m;

    // Gradients
    vec3 x = 2.0 * fract(p * C.www) - 1.0;
    vec3 h = abs(x) - 0.5;
    vec3 ox = floor(x + 0.5);
    vec3 a0 = x - ox;

    // Normalise gradients
    m *= 1.79284291400159 - 0.85373472095314 * (a0 * a0 + h * h);

    // Compute final noise value
    vec3 g;
    g.x = a0.x * x0.x + h.x * x0.y;
    g.yz = a0.yz * x12.xz + h.yz * x12.yw;
    return 130.0 * dot(m, g);
}

float rand(vec2 co) {
    return fract(sin(dot(co.xy, vec2(12.9898, 78.233))) * 43758.5453);
}

void main() {
    float height = fragPosition.y / 32.0;
    float delta = 0.0;
    for (float i = -4.0; i < 4.0; i += 1.0) { // Use float for loop counter
        float scale = pow(2.0, i);
        delta += 0.02 * snoise((1.0 + fragPosition.y) * fragPosition.xz / scale);
    }
    float val = 0.5 + height;

    float black = 0.25 - 0.25 * fract(20.0 * height);
    vec4 start_color = vec4(128.0 / 255.0, 48.0 / 255.0, 0.0, 1.0);
    vec4 mid_color = vec4(255.0 / 255.0, 184.0 / 255.0, 0.0 / 255.0, 1.0);
    vec4 end_color = vec4(220.0 / 255.0, 48.0 / 255.0, 0.0, 1.0);

    vec4 color = mix(mid_color, end_color, height);
    color.rgb -= black;

    if (height < 0.001) {
        color.rgb = start_color.rgb + delta;
    }

    gl_FragColor = color; // Output to gl_FragColor instead of finalColor
}



================================================
FILE: pufferlib/resources/terraform/shader_100.vs
================================================
#version 100

precision mediump float;

attribute vec3 vertexPosition;
attribute vec2 vertexTexCoord;
attribute vec3 vertexNormal;
attribute vec4 vertexColor;
uniform mat4 mvp;
varying vec2 fragTexCoord;
varying vec4 fragColor;
varying vec3 fragPosition;
void main()
{
    fragTexCoord = vertexTexCoord;
    fragColor = vertexColor;
    fragPosition = vertexPosition;
    gl_Position = mvp * vec4(vertexPosition, 1.0);
}



================================================
FILE: pufferlib/resources/terraform/shader_330.fs
================================================
#version 330

// Input vertex attributes (from vertex shader)
in vec2 fragTexCoord;       // Fragment input: vertex attribute: texture coordinates
in vec4 fragColor;          // Fragment input: vertex attribute: color 
in vec3 fragPosition;       // Fragment input: vertex attribute: position

// Input uniform values
uniform sampler2D texture0; // Fragment input: texture

// Output fragment color
out vec4 finalColor;       // Fragment output: pixel color


//Ashima's simplex noise
vec3 mod289(vec3 x) {
  return x - floor(x * (1.0 / 289.0)) * 289.0;
}

vec2 mod289(vec2 x) {
  return x - floor(x * (1.0 / 289.0)) * 289.0;
}

vec3 permute(vec3 x) {
  return mod289(((x*34.0)+10.0)*x);
}

float snoise(vec2 v)
  {
  const vec4 C = vec4(0.211324865405187,  // (3.0-sqrt(3.0))/6.0
                      0.366025403784439,  // 0.5*(sqrt(3.0)-1.0)
                     -0.577350269189626,  // -1.0 + 2.0 * C.x
                      0.024390243902439); // 1.0 / 41.0
// First corner
  vec2 i  = floor(v + dot(v, C.yy) );
  vec2 x0 = v -   i + dot(i, C.xx);

// Other corners
  vec2 i1;
  //i1.x = step( x0.y, x0.x ); // x0.x > x0.y ? 1.0 : 0.0
  //i1.y = 1.0 - i1.x;
  i1 = (x0.x > x0.y) ? vec2(1.0, 0.0) : vec2(0.0, 1.0);
  // x0 = x0 - 0.0 + 0.0 * C.xx ;
  // x1 = x0 - i1 + 1.0 * C.xx ;
  // x2 = x0 - 1.0 + 2.0 * C.xx ;
  vec4 x12 = x0.xyxy + C.xxzz;
  x12.xy -= i1;

// Permutations
  i = mod289(i); // Avoid truncation effects in permutation
  vec3 p = permute( permute( i.y + vec3(0.0, i1.y, 1.0 ))
		+ i.x + vec3(0.0, i1.x, 1.0 ));

  vec3 m = max(0.5 - vec3(dot(x0,x0), dot(x12.xy,x12.xy), dot(x12.zw,x12.zw)), 0.0);
  m = m*m ;
  m = m*m ;

// Gradients: 41 points uniformly over a line, mapped onto a diamond.
// The ring size 17*17 = 289 is close to a multiple of 41 (41*7 = 287)

  vec3 x = 2.0 * fract(p * C.www) - 1.0;
  vec3 h = abs(x) - 0.5;
  vec3 ox = floor(x + 0.5);
  vec3 a0 = x - ox;

// Normalise gradients implicitly by scaling m
// Approximation of: m *= inversesqrt( a0*a0 + h*h );
  m *= 1.79284291400159 - 0.85373472095314 * ( a0*a0 + h*h );

// Compute final noise value at P
  vec3 g;
  g.x  = a0.x  * x0.x  + h.x  * x0.y;
  g.yz = a0.yz * x12.xz + h.yz * x12.yw;
  return 130.0 * dot(m, g);
}


float rand(vec2 co){
  return fract(sin(dot(co.xy ,vec2(12.9898,78.233))) * 43758.5453);
}

void main()
{
    // Color based on height (e.g., gradient from blue to red)
    float height = fragPosition.y/32.0;
    float delta = 0.0;
    for (int i = -4; i < 4; i++) {
        float scale = pow(2.0, float(i));
        delta += 0.02*snoise((1.0+fragPosition.y)*fragPosition.xz/scale);
    }
    //float scale = pow(2.0, 4.0);
    //float delta = round(2.0*snoise(fragPosition.xz/scale))/10.0;
    float val = 0.5 + height;


    float black = 0.25 - 0.25*fract(20.0*height);
    //height = round(height*5.0)/5.0;
    vec4 start_color = vec4(128.0/255.0, 48.0/255.0, 0.0, 1.0);
    vec4 mid_color = vec4(255.0/255.0, 184.0/255.0, 0.0/255.0, 1.0);
    vec4 end_color = vec4(220.0/255.0, 48.0/255.0, 0.0, 1.0);


    //if (height < 0.5) {
    //    finalColor.rgba = mix(start_color, mid_color, 2.0*height);
    //} else {
    //    finalColor.rgba = mix(mid_color, end_color, 2.0*(height-0.5));
    //}

    finalColor.rgba = mix(mid_color, end_color, height);

    finalColor.rgb -= black;

    if (height < 0.001) {
        finalColor.rgb = start_color.rgb + delta;
    }

    //if (height < 5.0) {
    //    float val = 0.5 + height/10.0;
    //    finalColor.rgba = vec4(val+delta, (val+delta)/2.0, 0.0, 1.0);
    //} else if (height < 15.0) {
    //    float val = 0.5 + (height - 5.0)/5.0;
    //    finalColor.rgba = vec4(val+delta, (val+delta)/3.0, 0.0, 1.0);
    //} else {
    //    float val = 0.5 + (height - 15.0)/(32.0 - 15.0)/2.0;
    //    finalColor.rgba = vec4(val+delta, (val+delta)/4.0, 0.0, 1.0);
    //}
}




================================================
FILE: pufferlib/resources/terraform/shader_330.vs
================================================
#version 330

// Input vertex attributes
in vec3 vertexPosition;
in vec2 vertexTexCoord;
in vec3 vertexNormal;
in vec4 vertexColor;

// Input uniform values
uniform mat4 mvp;

// Output vertex attributes (to fragment shader)
out vec2 fragTexCoord;
out vec4 fragColor;
out vec3 fragPosition;

// NOTE: Add your custom variables here

void main()
{
    // Send vertex attributes to fragment shader
    fragTexCoord = vertexTexCoord;
    fragColor = vertexColor;
    fragPosition = vertexPosition;

    // Calculate final vertex position
    gl_Position = mvp*vec4(vertexPosition, 1.0);
}



================================================
FILE: pufferlib/resources/terraform/target_shader_100.fs
================================================
#version 100
precision mediump float;
varying vec2 fragTexCoord;
varying vec4 fragColor;
varying vec3 fragPosition;
uniform int width;
uniform int height;
uniform sampler2D texture0;
uniform sampler2D terrain;
void main()
{
    float x = fragPosition.x/float(width);
    float y = fragPosition.z/float(height);

    float terrain_height = texture2D(terrain, vec2(x, y)).r * 32.0;
    float target_height = fragPosition.y;
    float abs_delta = abs(target_height/32.0 - terrain_height/32.0);

    vec4 start_color = vec4(0.0, 0.0, 0.0, 1.0);
    vec4 end_color = vec4(0.0, 1.0, 1.0, 1.0);
    vec4 delta_color = mix(start_color, end_color, abs_delta);

    if (abs_delta > 0.0) {
        abs_delta += 0.25;
    }

    float grid = 16.0;
    float glow = max(exp(-grid * abs(sin(3.14159 * fragPosition.x / grid))), exp(-grid * abs(sin(3.14159 * fragPosition.z / grid))));

    float black = 0.25 - 0.25*fract(20.0*target_height/32.0);
    float r = delta_color.r - black;
    float g = max(glow, delta_color.g - black);
    float b = max(glow, delta_color.b - black);
    float a = max(glow, abs_delta);

    gl_FragColor = vec4(delta_color.r-black, delta_color.g-black, delta_color.b-black, abs_delta);
}



================================================
FILE: pufferlib/resources/terraform/target_shader_330.fs
================================================
#version 330

// Input vertex attributes (from vertex shader)
in vec2 fragTexCoord;       // Fragment input: vertex attribute: texture coordinates
in vec4 fragColor;          // Fragment input: vertex attribute: color 
in vec3 fragPosition;       // Fragment input: vertex attribute: position
uniform int width;               
uniform int height;              

// Input uniform values
uniform sampler2D texture0; // Fragment input: texture
uniform sampler2D terrain; // Fragment input: texture

// Output fragment color
out vec4 finalColor;       // Fragment output: pixel color

void main()
{
    // Color based on height (e.g., gradient from blue to red)

    float x = fragPosition.x/float(width);
    float y = fragPosition.z/float(height);

    float terrain_height = texture(terrain, vec2(x, y)).r * 32.0;
    float target_height = fragPosition.y;
    float abs_delta = abs(target_height/32.0 - terrain_height/32.0);

    vec4 start_color = vec4(0.0, 0.0, 0.0, 1.0);
    vec4 end_color = vec4(0.0, 1.0, 1.0, 1.0);
    vec4 delta_color = mix(start_color, end_color, abs_delta);

    if (abs_delta > 0.0) {
        abs_delta += 0.25;
    }

    float grid = 16.0;
    float glow = max(exp(-grid * abs(sin(3.14159 * fragPosition.x / grid))), exp(-grid * abs(sin(3.14159 * fragPosition.z / grid))));

    float black = 0.25 - 0.25*fract(20.0*target_height/32.0);
    float r = delta_color.r - black;
    float g = max(glow, delta_color.g - black);
    float b = max(glow, delta_color.b - black);
    float a = max(glow, abs_delta);
    // finalColor.rgba = vec4(r, g, b, a);


    finalColor.rgba = vec4(delta_color.r-black, delta_color.g-black, delta_color.b-black, abs_delta);
}




================================================
FILE: pufferlib/resources/terraform/target_shader_330.vs
================================================
#version 330

// Input vertex attributes
in vec3 vertexPosition;
in vec2 vertexTexCoord;
in vec3 vertexNormal;
in vec4 vertexColor;

// Input uniform values
uniform mat4 mvp;

// Output vertex attributes (to fragment shader)
out vec2 fragTexCoord;
out vec4 fragColor;
out vec3 fragPosition;

// NOTE: Add your custom variables here

void main()
{
    // Send vertex attributes to fragment shader
    fragTexCoord = vertexTexCoord;
    fragColor = vertexColor;
    fragPosition = vertexPosition;

    // Calculate final vertex position
    gl_Position = mvp*vec4(vertexPosition + vec3(0.0, 0.0, 0.0), 1.0);
}



================================================
FILE: pufferlib/resources/tower_climb/shaders/gls100/lighting.fs
================================================
#version 100

precision mediump float;

// Input vertex attributes (from vertex shader)
varying vec3 fragPosition;
varying vec2 fragTexCoord;
varying vec4 fragColor;
varying vec3 fragNormal;

// Input uniform values
uniform sampler2D texture0;
uniform vec4 colDiffuse;

// NOTE: Add here your custom variables

#define     MAX_LIGHTS              4
#define     LIGHT_DIRECTIONAL       0
#define     LIGHT_POINT             1

struct Light {
    int enabled;
    int type;
    vec3 position;
    vec3 target;
    vec4 color;
};

// Input lighting values
uniform Light lights[MAX_LIGHTS];
uniform vec4 ambient;
uniform vec3 viewPos;

void main()
{
    // Texel color fetching from texture sampler
    vec4 texelColor = texture2D(texture0, fragTexCoord);
    vec3 lightDot = vec3(0.0);
    vec3 normal = normalize(fragNormal);
    vec3 viewD = normalize(viewPos - fragPosition);
    vec3 specular = vec3(0.0);

    vec4 tint = colDiffuse * fragColor;

    // NOTE: Implement here your fragment shader code

    for (int i = 0; i < MAX_LIGHTS; i++)
    {
        if (lights[i].enabled == 1)
        {
            vec3 light = vec3(0.0);

            if (lights[i].type == LIGHT_DIRECTIONAL)
            {
                light = -normalize(lights[i].target - lights[i].position);
            }

            if (lights[i].type == LIGHT_POINT)
            {
                light = normalize(lights[i].position - fragPosition);
            }

            float NdotL = max(dot(normal, light), 0.0);
            lightDot += lights[i].color.rgb*NdotL;

            float specCo = 0.0;
            if (NdotL > 0.0) specCo = pow(max(0.0, dot(viewD, reflect(-(light), normal))), 16.0); // 16 refers to shine
            specular += specCo;
        }
    }

    vec4 finalColor = (texelColor*((tint + vec4(specular, 1.0))*vec4(lightDot, 1.0)));
    finalColor += texelColor*(ambient/10.0);

    // Gamma correction
    gl_FragColor = pow(finalColor, vec4(1.0/2.2));
}


================================================
FILE: pufferlib/resources/tower_climb/shaders/gls100/lighting.vs
================================================
#version 100

// Input vertex attributes
attribute vec3 vertexPosition;
attribute vec2 vertexTexCoord;
attribute vec3 vertexNormal;
attribute vec4 vertexColor;

// Input uniform values
uniform mat4 mvp;
uniform mat4 matModel;

// Output vertex attributes (to fragment shader)
varying vec3 fragPosition;
varying vec2 fragTexCoord;
varying vec4 fragColor;
varying vec3 fragNormal;

// NOTE: Add here your custom variables

// https://github.com/glslify/glsl-inverse
mat3 inverse(mat3 m)
{
  float a00 = m[0][0], a01 = m[0][1], a02 = m[0][2];
  float a10 = m[1][0], a11 = m[1][1], a12 = m[1][2];
  float a20 = m[2][0], a21 = m[2][1], a22 = m[2][2];

  float b01 = a22*a11 - a12*a21;
  float b11 = -a22*a10 + a12*a20;
  float b21 = a21*a10 - a11*a20;

  float det = a00*b01 + a01*b11 + a02*b21;

  return mat3(b01, (-a22*a01 + a02*a21), (a12*a01 - a02*a11),
              b11, (a22*a00 - a02*a20), (-a12*a00 + a02*a10),
              b21, (-a21*a00 + a01*a20), (a11*a00 - a01*a10))/det;
}

// https://github.com/glslify/glsl-transpose
mat3 transpose(mat3 m)
{
  return mat3(m[0][0], m[1][0], m[2][0],
              m[0][1], m[1][1], m[2][1],
              m[0][2], m[1][2], m[2][2]);
}

void main()
{
    // Send vertex attributes to fragment shader
    fragPosition = vec3(matModel*vec4(vertexPosition, 1.0));
    fragTexCoord = vertexTexCoord;
    fragColor = vertexColor;

    mat3 normalMatrix = transpose(inverse(mat3(matModel)));
    fragNormal = normalize(normalMatrix*vertexNormal);

    // Calculate final vertex position
    gl_Position = mvp*vec4(vertexPosition, 1.0);
}


================================================
FILE: pufferlib/resources/tower_climb/shaders/gls330/lighting.fs
================================================
#version 330

// Input vertex attributes (from vertex shader)
in vec3 fragPosition;
in vec2 fragTexCoord;
in vec4 fragColor;
in vec3 fragNormal;

// Input uniform values
uniform sampler2D texture0;
uniform vec4 colDiffuse;

// Output fragment color
out vec4 finalColor;

// NOTE: Add here your custom variables

#define     MAX_LIGHTS              4
#define     LIGHT_DIRECTIONAL       0
#define     LIGHT_POINT             1

struct Light {
    int enabled;
    int type;
    vec3 position;
    vec3 target;
    vec4 color;
};

// Input lighting values
uniform Light lights[MAX_LIGHTS];
uniform vec4 ambient;
uniform vec3 viewPos;

void main()
{
    // Texel color fetching from texture sampler
    vec4 texelColor = texture(texture0, fragTexCoord);
    vec3 lightDot = vec3(0.0);
    vec3 normal = normalize(fragNormal);
    vec3 viewD = normalize(viewPos - fragPosition);
    vec3 specular = vec3(0.0);

    vec4 tint = colDiffuse * fragColor;

    // NOTE: Implement here your fragment shader code

    for (int i = 0; i < MAX_LIGHTS; i++)
    {
        if (lights[i].enabled == 1)
        {
            vec3 light = vec3(0.0);

            if (lights[i].type == LIGHT_DIRECTIONAL)
            {
                light = -normalize(lights[i].target - lights[i].position);
            }

            if (lights[i].type == LIGHT_POINT)
            {
                light = normalize(lights[i].position - fragPosition);
            }

            float NdotL = max(dot(normal, light), 0.0);
            lightDot += lights[i].color.rgb*NdotL;

            float specCo = 0.0;
            if (NdotL > 0.0) specCo = pow(max(0.0, dot(viewD, reflect(-(light), normal))), 16.0); // 16 refers to shine
            specular += specCo;
        }
    }

    finalColor = (texelColor*((tint + vec4(specular, 1.0))*vec4(lightDot, 1.0)));
    finalColor += texelColor*(ambient/10.0)*tint;

    // Gamma correction
    finalColor = pow(finalColor, vec4(1.0/2.2));
}


================================================
FILE: pufferlib/resources/tower_climb/shaders/gls330/lighting.vs
================================================
#version 330

// Input vertex attributes
in vec3 vertexPosition;
in vec2 vertexTexCoord;
in vec3 vertexNormal;
in vec4 vertexColor;

// Input uniform values
uniform mat4 mvp;
uniform mat4 matModel;
uniform mat4 matNormal;

// Output vertex attributes (to fragment shader)
out vec3 fragPosition;
out vec2 fragTexCoord;
out vec4 fragColor;
out vec3 fragNormal;

// NOTE: Add here your custom variables

void main()
{
    // Send vertex attributes to fragment shader
    fragPosition = vec3(matModel*vec4(vertexPosition, 1.0));
    fragTexCoord = vertexTexCoord;
    fragColor = vertexColor;
    fragNormal = normalize(vec3(matNormal*vec4(vertexNormal, 1.0)));

    // Calculate final vertex position
    gl_Position = mvp*vec4(vertexPosition, 1.0);
}


================================================
SYMLINK: resources -> resources
================================================



================================================
FILE: scripts/build_ocean.sh
================================================
#!/bin/bash

# Usage: ./build_env.sh pong [local|fast|web]

ENV=$1
MODE=${2:-local}
PLATFORM="$(uname -s)"
SRC_DIR="pufferlib/ocean/$ENV"
WEB_OUTPUT_DIR="build_web/$ENV"
RAYLIB_NAME='raylib-5.5_macos'
BOX2D_NAME='box2d-macos-arm64'
if [ "$PLATFORM" = "Linux" ]; then
    RAYLIB_NAME='raylib-5.5_linux_amd64'
    BOX2D_NAME='box2d-linux-amd64'
fi
if [ "$MODE" = "web" ]; then
    RAYLIB_NAME='raylib-5.5_webassembly'
    BOX2D_NAME='box2d-web'
fi

LINK_ARCHIVES="./$RAYLIB_NAME/lib/libraylib.a"
if [ "$ENV" = "impulse_wars" ]; then
    LINK_ARCHIVES="$LINK_ARCHIVES ./$BOX2D_NAME/libbox2d.a"
fi

# Create build output directory
mkdir -p "$WEB_OUTPUT_DIR"

if [ "$MODE" = "web" ]; then
    echo "Building $ENV for web deployment..."
    emcc \
        -o "$WEB_OUTPUT_DIR/game.html" \
        "$SRC_DIR/$ENV.c" \
        -O3 \
        -Wall \
        $LINK_ARCHIVES \
        -I./$RAYLIB_NAME/include \
        -I./$BOX2D_NAME/include \
        -I./$BOX2D_NAME/src \
        -I./pufferlib/extensions \
        -I./pufferlib \
        -L. \
        -L./$RAYLIB_NAME/lib \
        -sASSERTIONS=2 \
        -gsource-map \
        -s USE_GLFW=3 \
        -s USE_WEBGL2=1 \
        -s ASYNCIFY \
        -sFILESYSTEM \
        -s FORCE_FILESYSTEM=1 \
        --shell-file ./scripts/minshell.html \
        -sINITIAL_MEMORY=512MB \
        -sALLOW_MEMORY_GROWTH \
        -sSTACK_SIZE=512KB \
        -DNDEBUG \
        -DPLATFORM_WEB \
        -DGRAPHICS_API_OPENGL_ES3 \
        --preload-file pufferlib/resources/$1@resources/$1 \
        --preload-file pufferlib/resources/shared@resources/shared 
    echo "Web build completed: $WEB_OUTPUT_DIR/game.html"
    echo "Preloaded files:"
    echo "  pufferlib/resources/$1@resources$1"
    echo "  pufferlib/resources/shared@resources/shared"
    exit 0
fi

FLAGS=(
    -Wall
    -I./$RAYLIB_NAME/include
    -I./$BOX2D_NAME/include
    -I./$BOX2D_NAME/src
    -I./pufferlib/extensions
    "$SRC_DIR/$ENV.c" -o "$ENV"
    $LINK_ARCHIVES
    -lm
    -lpthread
    -ferror-limit=3
    -DPLATFORM_DESKTOP
)


if [ "$PLATFORM" = "Darwin" ]; then
    FLAGS+=(
        -framework Cocoa
        -framework IOKit
        -framework CoreVideo
    )
fi

echo ${FLAGS[@]}

if [ "$MODE" = "local" ]; then
    echo "Building $ENV for local testing..."
    if [ "$PLATFORM" = "Linux" ]; then
        # These important debug flags don't work on macos
        FLAGS+=(
            -fsanitize=address,undefined,bounds,pointer-overflow,leak
            -fno-omit-frame-pointer
        )
    fi  
    clang -g -O0 ${FLAGS[@]}
elif [ "$MODE" = "fast" ]; then
    echo "Building optimized $ENV for local testing..."
    clang -pg -O2 -DNDEBUG ${FLAGS[@]}
    echo "Built to: $ENV"
else
    echo "Invalid mode specified: local|fast|web"
    exit 1
fi



================================================
FILE: scripts/build_simple.sh
================================================
#!/bin/bash

# Usage: ./build.sh your_file.c [debug|release]

SOURCE=$1
MODE=${2:-debug}
PLATFORM="$(uname -s)"

# Extract filename without extension
FILENAME=$(basename -- "$SOURCE")
FILENAME="${FILENAME%.*}"

FLAGS=(
    -Wall
    "$SOURCE" -o "$FILENAME"
    -lm
    -lpthread
)

if [ "$PLATFORM" = "Darwin" ]; then
    FLAGS+=(
        -framework Cocoa
        -framework IOKit
        -framework CoreVideo
    )
fi

echo "Compiling with: ${FLAGS[@]}"

if [ "$MODE" = "debug" ]; then
    echo "Building $SOURCE in debug mode..."
    if [ "$PLATFORM" = "Linux" ]; then
        # These important debug flags don't work on macos
        FLAGS+=(
            -fsanitize=address,undefined,bounds,pointer-overflow,leak -g
        )
    fi  
    clang -g -O0 ${FLAGS[@]}
    echo "Built to: $FILENAME (debug mode)"
elif [ "$MODE" = "release" ]; then
    echo "Building optimized $SOURCE..."
    clang -O2 ${FLAGS[@]}
    echo "Built to: $FILENAME (release mode)"
else
    echo "Invalid mode specified: debug|release"
    exit 1
fi


================================================
FILE: scripts/minshell.html
================================================
<!doctype html>
<html lang="EN-us">
  <head>
    <meta charset="utf-8">
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8">

    <title>raylib web game</title>

    <meta name="title" content="raylib web game">
    <meta name="description" content="New raylib web videogame, developed using raylib videogames library">
    <meta name="keywords" content="raylib, programming, examples, html5, C, C++, library, learn, games, videogames">
    <meta name="viewport" content="width=device-width">

    <!-- Open Graph metatags for sharing -->
    <meta property="og:type" content="website" />
    <meta property="og:title" content="raylib web game">
    <meta property="og:image:type" content="image/png">
    <meta property="og:image" content="https://www.raylib.com/common/raylib_logo.png">
    <meta property="og:image:alt" content="New raylib web videogame, developed using raylib videogames library" />
    <meta property="og:site_name" content="raylib - example">
    <meta property="og:url" content="https://www.raylib.com/games.html">
    <meta property="og:description" content="New raylib web videogame, developed using raylib videogames library">

    <!-- Twitter metatags for sharing -->
    <meta name="twitter:card" content="summary_large_image">
    <meta name="twitter:site" content="@raysan5">
    <meta name="twitter:title" content="raylib web game">
    <meta name="twitter:image" content="https://www.raylib.com/common/raylib_logo.png">
    <meta name="twitter:image:alt" content="New raylib web videogame, developed using raylib videogames library">
    <meta name="twitter:url" content="https://www.raylib.com/games.html">
    <meta name="twitter:description" content="New raylib web videogame, developed using raylib videogames library">

    <!-- Favicon -->
    <link rel="shortcut icon" href="https://www.raylib.com/favicon.ico">

    <style>
        body { 
          margin: 0px; 
          overflow: hidden; 
          background-color: black;
        }
        canvas.emscripten { border: 0px none; background-color: black;}
    </style>
    <script type='text/javascript' src="https://cdn.jsdelivr.net/gh/eligrey/FileSaver.js/dist/FileSaver.min.js"> </script>
    <script type='text/javascript'>
        function saveFileFromMEMFSToDisk(memoryFSname, localFSname)     // This can be called by C/C++ code
        {
            var isSafari = false; // Not supported, navigator.userAgent access is being restricted
            //var isSafari = /^((?!chrome|android).)*safari/i.test(navigator.userAgent);
            var data = FS.readFile(memoryFSname);
            var blob;

            if (isSafari) blob = new Blob([data.buffer], { type: "application/octet-stream" });
            else blob = new Blob([data.buffer], { type: "application/octet-binary" });

            // NOTE: SaveAsDialog is a browser setting. For example, in Google Chrome,
            // in Settings/Advanced/Downloads section you have a setting:
            // 'Ask where to save each file before downloading' - which you can set true/false.
            // If you enable this setting it would always ask you and bring the SaveAsDialog
            saveAs(blob, localFSname);
        }
    </script>
    </head>
    <body>
        <canvas class=emscripten id=canvas oncontextmenu=event.preventDefault() tabindex=-1></canvas>
        <p id="output" />
        <script>
            var Module = {
                print: (function() {
                    var element = document.getElementById('output');
                    if (element) element.value = ''; // clear browser cache
                    return function(text) {
                        if (arguments.length > 1) text = Array.prototype.slice.call(arguments).join(' ');
                        console.log(text);
                        if (element) {
                          element.value += text + "\n";
                          element.scrollTop = element.scrollHeight; // focus on bottom
                        }
                    };
                })(),
                canvas: (function() {
                    var canvas = document.getElementById('canvas');
                    return canvas;
                })()
            };
        </script>
        {{{ SCRIPT }}}
    </body>
</html>



================================================
FILE: scripts/sweep_atari.sh
================================================
#!/bin/bash

environments=(
    "pong"
    "breakout"
    "beam_rider"
    "enduro"
    "qbert"
    "space_invaders"
    "seaquest"
)

for env in "${environments[@]}"; do
    echo "Training: $env"
    python demo.py --mode sweep-carbs --vec multiprocessing --env "$env"
done



================================================
FILE: scripts/train_atari.sh
================================================
#!/bin/bash

environments=(
    "pong"
    "breakout"
    "beam_rider"
    "enduro"
    "qbert"
    "space_invaders"
    "seaquest"
)

for env in "${environments[@]}"; do
    echo "Training: $env"
    python demo.py --mode train --vec multiprocessing --track --env "$env"
done



================================================
FILE: scripts/train_ocean.sh
================================================
#!/bin/bash
 
environments=(
    "puffer_breakout"
    "puffer_connect4"
    "puffer_pong"
    "puffer_snake"
    "puffer_tripletriad"
    "puffer_rware"
    "puffer_go"
    "puffer_tactics"
    "puffer_moba"
    "puffer_whisker_racer"
)

for env in "${environments[@]}"; do
    echo "Training: $env"
    python demo.py --mode train --vec multiprocessing --track --env "$env"
done



================================================
FILE: scripts/train_procgen.sh
================================================
#!/bin/bash

environments=(
    "bigfish"
    "bossfight"
    "caveflyer"
    "chaser"
    "climber"
    "coinrun"
    "dodgeball"
    "fruitbot"
    "heist"
    "jumper"
    "leaper"
    "maze"
    "miner"
    "ninja"
    "plunder"
    "starpilot"
)

for env in "${environments[@]}"; do
    echo "Training on environment: $env"
    python demo.py --mode train --vec multiprocessing --track --env "$env"
done



================================================
FILE: scripts/train_sanity.sh
================================================
#!/bin/bash
 
environments=(
    "puffer_squared"
    "puffer_password"
    "puffer_stochastic"
    "puffer_memory"
    "puffer_multiagent"
    "puffer_spaces"
    "puffer_bandit"
)

for env in "${environments[@]}"; do
    echo "Training: $env"
    python demo.py --mode train --vec multiprocessing --track --env "$env"
done



================================================
FILE: tests/__init__.py
================================================
[Empty file]


================================================
FILE: tests/mem_test.py
================================================
from pdb import set_trace as T
import numpy as np
import time

import selectors
from multiprocessing import Process, Pipe, Array

def worker_process(envs_per_worker, shared_mem, bandwidth,
        delay_mean, delay_std, send_pipe, recv_pipe):
    data = np.random.randn(bandwidth)
    while True:
        request = recv_pipe.recv()
        for _ in range(envs_per_worker):
            start = time.process_time()
            idx = 0
            target_time = delay_mean + delay_std*np.random.randn()
            while time.process_time() - start < target_time:
                idx += 1
            shared_mem[:bandwidth] = data

        send_pipe.send('end')


def test_speed(envs_per_worker=1, bandwidth=1, delay_mean=0.01, delay_std=0.001,
        num_workers=4, batch_size=4, timeout=10):

    main_send_pipes, work_recv_pipes = zip(*[Pipe() for _ in range(num_workers)])
    work_send_pipes, main_recv_pipes = zip(*[Pipe() for _ in range(num_workers)])

    shared_mem = [Array('d', bandwidth) for _ in range(num_workers)]
    processes = [Process(
        target=worker_process,
        args=(envs_per_worker, shared_mem, bandwidth,
            delay_mean, delay_std, work_send_pipes[i], work_recv_pipes[i]))
        for i in range(num_workers)]

    for p in processes:
        p.start()
 
    send_idxs = {i for i in range(num_workers)}

    # Register all receive pipes with the selector
    sel = selectors.DefaultSelector()
    for pipe in main_recv_pipes:
        sel.register(pipe, selectors.EVENT_READ)

    steps_collected = 0
    start = time.time()
    while time.time() - start < timeout:
        for idx in send_idxs:
            main_send_pipes[idx].send('start')

        send_idxs = set()

        for key, _ in sel.select(timeout=None):
            pipe = key.fileobj
            idx = main_recv_pipes.index(pipe)

            if pipe.poll():
                assert pipe.recv() == 'end'
                send_idxs.add(idx)

            if len(send_idxs) == batch_size:
                break

    end = time.time()

    for p in processes:
        p.terminate()

    sps = steps_collected / (end - start)
    print(
        f'SPS: {sps:.2f}',
        f'envs_per_worker: {envs_per_worker}',
        f'delay_mean: {delay_mean}',
        f'delay_std: {delay_std}',
        f'num_workers: {num_workers}',
        f'batch_size: {batch_size}',
        f'sync: {sync}',
    )


if __name__ == '__main__':
    #timeout = 1
    #test_speed(timeout=1)
    test_speed(delay_mean=0, delay_std=0, num_workers=1, batch_size=1)
    test_speed(delay_mean=0, delay_std=0, num_workers=1, batch_size=1)
    test_speed(delay_mean=0, delay_std=0, num_workers=6, batch_size=6)
    test_speed(delay_mean=0, delay_std=0, num_workers=6, batch_size=6)
    test_speed(delay_mean=0, delay_std=0, num_workers=24, batch_size=6)
    test_speed(delay_mean=0, delay_std=0, num_workers=24, batch_size=24)
    test_speed(delay_mean=0, delay_std=0, num_workers=24, batch_size=6)




================================================
FILE: tests/test.py
================================================
from pdb import set_trace as T

import numpy as np

import pufferlib
import pufferlib.emulation
import pufferlib.utils
import pufferlib.vector
from pufferlib.environments import test

# Deprecation warnings from gymnasium
import gymnasium
import warnings
warnings.filterwarnings("ignore")

class RandomState:
    def __init__(self, seed):
        self.rng = np.random.RandomState(seed)

    def random(self):
        return self.rng.random()

    def probabilistic_round(self, n):
            frac, integer = np.modf(n)
            if self.random() < frac:
                return int(integer) + 1
            else:
                return int(integer)

    def sample(self, ary, n):
        n_rounded = self.probabilistic_round(n)
        return self.rng.choice(ary, n_rounded, replace=False).tolist()

    def choice(self, ary):
        return self.sample(ary, 1)[0]


# TODO: Fix this. Was in utils.py. Only used for tests
def make_zeros_like(data):
    if isinstance(data, dict):
        return {k: make_zeros_like(v) for k, v in data.items()}
    elif isinstance(data, (list, tuple)):
        return [make_zeros_like(v) for v in data]
    elif isinstance(data, np.ndarray):
        return np.zeros_like(data)
    elif isinstance(data, (int, float)):
        return 0
    else:
        raise ValueError(f'Unsupported type: {type(data)}')

def compare_arrays(array_1, array_2):
    assert isinstance(array_1, np.ndarray)
    assert isinstance(array_2, np.ndarray)
    assert array_1.shape == array_2.shape
    return np.allclose(array_1, array_2)

def compare_dicts(dict_1, dict_2, idx):
    assert isinstance(dict_1, (dict, OrderedDict))
    assert isinstance(dict_2, (dict, OrderedDict))

    if not all(k in dict_2 for k in dict_1):
        raise ValueError("Keys do not match between dictionaries.")

    for k, v in dict_1.items():
        if not compare_space_samples(v, dict_2[k], idx):
            return False

    return True

def compare_lists(list_1, list_2, idx):
    assert isinstance(list_1, (list, tuple))
    assert isinstance(list_2, (list, tuple))

    if len(list_1) != len(list_2):
        raise ValueError("Lengths do not match between lists/tuples.")

    for v1, v2 in zip(list_1, list_2):
        if not compare_space_samples(v1, v2, idx):
            return False
        
    return True
    
def compare_space_samples(sample_1, sample_2, sample_2_batch_idx=None):
    '''Compare two samples from the same space
    
    Optionally, sample_2 may be a batch of samples from the same space
    concatenated along the first dimension of the leaves. In this case,
    sample_2_batch_idx specifies which sample to compare.
    '''
    if isinstance(sample_1, (dict, OrderedDict)):
        return compare_dicts(sample_1, sample_2, sample_2_batch_idx)
    elif isinstance(sample_1, (list, tuple)):
        return compare_lists(sample_1, sample_2, sample_2_batch_idx)
    elif isinstance(sample_1, np.ndarray):
        assert isinstance(sample_2, np.ndarray)
        if sample_2_batch_idx is not None:
            sample_2 = sample_2[sample_2_batch_idx]
        return compare_arrays(sample_1, sample_2)
    elif isinstance(sample_1, (int, float)):
        if sample_2_batch_idx is not None:
            sample_2 = sample_2[sample_2_batch_idx]
        if isinstance(sample_2, np.ndarray):
            assert sample_2.size == 1, "Cannot compare scalar to non-scalar."
            sample_2 = sample_2[0]
        return sample_1 == sample_2
    else:
        raise ValueError(f"Unsupported type: {type(sample_1)}")



def test_gymnasium_emulation(env_cls, steps=100):
    raw_env = env_cls()
    puf_env = pufferlib.emulation.GymnasiumPufferEnv(env_creator=env_cls)

    raw_done = puf_done = True
    raw_truncated = puf_truncated = False

    for i in range(steps):
        assert puf_done == raw_done
        assert puf_truncated == raw_truncated

        if raw_done:
            puf_ob, _ = puf_env.reset()
            raw_ob, _ = raw_env.reset()

        # Reconstruct original obs format from puffer env and compare to raw
        if puf_env.is_obs_emulated:
            puf_ob = pufferlib.emulation.nativize(
                puf_ob, puf_env.env.observation_space, puf_env.obs_dtype)

        pufferlib.utils.compare_space_samples(raw_ob, puf_ob)

        action = raw_env.action_space.sample()
        raw_ob, raw_reward, raw_done, raw_truncated, _ = raw_env.step(action)

        # Convert raw actions to puffer format
        if puf_env.is_atn_emulated:
            action = pufferlib.emulation.emulate_copy(
                action, puf_env.action_space.dtype, puf_env.atn_dtype)

        puf_ob, puf_reward, puf_done, puf_truncated, _ = puf_env.step(action)
        assert puf_reward == raw_reward

def test_pettingzoo_emulation(env_cls, steps=100):
    raw_env = env_cls()
    puf_env = pufferlib.emulation.PettingZooPufferEnv(env_creator=env_cls)

    for i in range(steps):
        raw_done = len(raw_env.agents) == 0
        puf_done = len(puf_env.agents) == 0

        assert puf_done == raw_done

        if raw_done:
            puf_obs, _ = puf_env.reset()
            raw_obs, _ = raw_env.reset()

        for agent in puf_env.possible_agents:
            if agent not in raw_obs:
                assert np.sum(puf_obs[agent] != 0) == 0
                continue

            raw_ob = raw_obs[agent]
            puf_ob = puf_obs[agent]

            # Reconstruct original obs format from puffer env and compare to raw
            if puf_env.is_obs_emulated:
                puf_ob = pufferlib.emulation.nativize(
                    puf_ob, puf_env.env.single_observation_space, puf_env.obs_dtype)
            
            assert pufferlib.utils.compare_space_samples(raw_ob, puf_ob)

        raw_actions = {a: raw_env.action_space(a).sample()
            for a in raw_env.agents}
        puf_actions = raw_actions

        raw_obs, raw_rewards, raw_dones, raw_truncateds, _ = raw_env.step(raw_actions)

        # Convert raw actions to puffer format
        dummy_action = raw_actions[list(raw_actions.keys())[0]]
        if puf_env.is_atn_emulated:
            for agent in puf_env.possible_agents:
                if agent not in raw_actions:
                    puf_actions[agent] = dummy_action
                    continue

                puf_actions[agent] = pufferlib.emulation.emulate_copy(
                    raw_actions[agent], puf_env.single_action_space.dtype, puf_env.atn_dtype)

        puf_obs, puf_rewards, puf_dones, puf_truncateds, _ = puf_env.step(puf_actions)

        for agent in raw_rewards:
            assert puf_rewards[agent] == raw_rewards[agent]

        for agent in raw_dones:
            assert puf_dones[agent] == raw_dones[agent]

def test_puffer_vectorization(env_cls, puffer_cls, steps=100, num_envs=1, **kwargs):
    raw_envs = [puffer_cls(env_creator=env_cls) for _ in range(num_envs)]
    vec_envs = pufferlib.vector.make(puffer_cls,
        env_kwargs={'env_creator': env_cls}, num_envs=num_envs, **kwargs)

    num_agents = sum(env.num_agents for env in raw_envs)
    assert num_agents == vec_envs.num_agents

    raw_obs = [env.reset()[0] for i, env in enumerate(raw_envs)]
    vec_obs, _ = vec_envs.reset()

    for _ in range(steps):
        # PettingZoo dict observations
        if isinstance(raw_obs[0], dict):
            raw_obs = [v for d in raw_obs for v in d.values()]

        raw_obs = np.stack(raw_obs, axis=0)
        assert raw_obs.shape == vec_obs.shape
        assert np.all(raw_obs == vec_obs)

        actions = vec_envs.action_space.sample()
        raw_actions = np.split(actions, num_envs)

        # Copy reset behavior of VecEnv
        raw_obs, raw_rewards, raw_terminals, raw_truncations = [], [], [], []
        for idx, r_env in enumerate(raw_envs):
            if r_env.done:
                raw_obs.append(r_env.reset()[0])
                raw_rewards.extend([0] * r_env.num_agents)
                raw_terminals.extend([False] * r_env.num_agents)
                raw_truncations.extend([False] * r_env.num_agents)
            else:
                r_ob, r_rew, r_term, r_trunc, _ = r_env.step(raw_actions[idx])
                raw_obs.append(r_ob)
                raw_rewards.append(r_rew)
                raw_terminals.append(r_term)
                raw_truncations.append(r_trunc)
                
        vec_obs, vec_rewards, vec_terminals, vec_truncations, _ = vec_envs.step(actions)

        rew = raw_rewards
        if isinstance(raw_rewards[0], dict):
            raw_rewards = [v for d in raw_rewards for v in d.values()]
            raw_terminals = [v for d in raw_terminals for v in d.values()]
            raw_truncations = [v for d in raw_truncations for v in d.values()]

        raw_rewards = np.asarray(raw_rewards, dtype=np.float32)
        raw_terminals = np.asarray(raw_terminals)
        raw_truncations = np.asarray(raw_truncations)

        assert np.all(raw_rewards == vec_rewards)
        assert np.all(raw_terminals == vec_terminals)
        assert np.all(raw_truncations == vec_truncations)

    vec_envs.close()
    for raw_env in raw_envs:
        raw_env.close()

def test_emulation():
    for env_cls in test.MOCK_SINGLE_AGENT_ENVIRONMENTS:
        test_gymnasium_emulation(env_cls)

    print('Gymnasium emulation tests passed')

    for env_cls in test.MOCK_MULTI_AGENT_ENVIRONMENTS:
        test_pettingzoo_emulation(env_cls)

    print('PettingZoo emulation tests passed')

def test_vectorization():
    for vectorization in [
            pufferlib.vector.Serial,
            pufferlib.vector.Multiprocessing,
            pufferlib.vector.Ray]:
        for env_cls in test.MOCK_SINGLE_AGENT_ENVIRONMENTS:
            test_puffer_vectorization(
                env_cls,
                pufferlib.emulation.GymnasiumPufferEnv,
                steps=10,
                num_envs=4,
                num_workers=4,
                backend=vectorization,
            )

        print(f'Gymnasium {vectorization.__name__} vectorization tests passed')

        for env_cls in test.MOCK_MULTI_AGENT_ENVIRONMENTS:
            test_puffer_vectorization(
                env_cls,
                pufferlib.emulation.PettingZooPufferEnv,
                steps=10,
                num_envs=4,
                num_workers=4,
                backend=vectorization,
            )

        print(f'PettingZoo {vectorization.__name__} vectorization tests passed')

if __name__ == '__main__':
    test_emulation()
    test_vectorization()
    exit(0) # For Ray



================================================
FILE: tests/test_api.py
================================================
from pdb import set_trace as T

import pufferlib
import pufferlib.emulation
import pufferlib.vector
from pufferlib.exceptions import APIUsageError, InvalidAgentError
from pufferlib.environments import test

def print_if(e, print_errors):
    if print_errors:
        print(type(e).__name__ + ':', e)
        print('#################')
        print()

def test_gymnasium_api(print_errors=False):
    env = pufferlib.emulation.GymnasiumPufferEnv(
        env_creator=test.GymnasiumTestEnv)

    try:
        env.step({})
    except APIUsageError as e:
        print_if(e, print_errors)

    try:
        env.close()
    except APIUsageError as e:
        print_if(e, print_errors)

    env.observation_space.sample()
    env.action_space.sample()
    ob = env.reset()

    try:
        bad_action = env.observation_space.sample()
        env.step(bad_action)
    except APIUsageError as e:
        print_if(e, print_errors)

    action = env.action_space.sample()
    obs, rewards, terminals, truncateds, infos = env.step(action)

def test_pettingzoo_api_usage(print_errors=False):
    env = pufferlib.emulation.PettingZooPufferEnv(
        env_creator=test.PettingZooTestEnv)

    try:
        env.step({})
    except APIUsageError as e:
        print_if(e, print_errors)

    try:
        env.close()
    except APIUsageError as e:
        print_if(e, print_errors)

    try:
        env.observation_space('foo')
    except InvalidAgentError as e:
        print_if(e, print_errors)

    try:
        env.action_space('foo')
    except InvalidAgentError as e:
        print_if(e, print_errors)

    env.observation_space('agent_1')
    env.action_space('agent_1')
    obs = env.reset()

    try:
        bad_actions = {agent: env.observation_space(agent).sample() for agent in env.agents}
        env.step(bad_actions)
    except APIUsageError as e:
        print_if(e, print_errors)

    try:
        env.step({'foo': None})
    except InvalidAgentError as e:
        print_if(e, print_errors)


    actions = {agent: env.action_space(agent).sample() for agent in env.agents}
    obs, rewards, terminals, truncateds, infos = env.step(actions)

def test_vectorization_api(print_errors=False):
    gymnasium_creator = lambda: pufferlib.emulation.GymnasiumPufferEnv(
        env_creator=test.GymnasiumTestEnv)
    pettingzoo_creator = lambda: pufferlib.emulation.PettingZooPufferEnv(
        env_creator=test.PettingZooTestEnv)

    for backend in [
        pufferlib.vector.Serial,
        pufferlib.vector.Multiprocessing,
        pufferlib.vector.Ray]:
            
        for creator in [gymnasium_creator, pettingzoo_creator]:
            vec = pufferlib.vector.make(creator, num_envs=6,
                num_workers=3, backend=backend)

            # Sync API
            _, _ = vec.reset()
            actions = vec.action_space.sample()
            _, _, _, _, _ = vec.step(actions)
            vec.close()

            # Async API
            vec = pufferlib.vector.make(creator, num_envs=8,
                num_workers=4, batch_size=4, backend=backend)
            vec.async_reset()
            actions = vec.action_space.sample()
            _, _, _, _, _, _, _ = vec.recv()
            vec.send(actions)
            vec.close()

        try:
            vec = pufferlib.vector.make(test.GymnasiumTestEnv)
        except APIUsageError as e:
            print_if(e, print_errors)

        try:
            vec = pufferlib.vector.make(gymnasium_creator,
                num_envs=3, num_workers=2)
        except APIUsageError as e:
            print_if(e, print_errors)

        try:
            vec = pufferlib.vector.make(gymnasium_creator,
                num_envs=4, num_workers=2, batch_size=3)
        except APIUsageError as e:
            print_if(e, print_errors)


if __name__ == '__main__':
    test_gymnasium_api()
    test_pettingzoo_api_usage()
    test_vectorization_api()



================================================
FILE: tests/test_atari_reset.py
================================================
from pdb import set_trace as T
from pufferlib.environments import atari


def test_atari_reset():
    '''Common way to bug the wrappers can be detected
    by checking that the environment properly resets
    after hitting 0 lives'''
    env = atari.env_creator('BreakoutNoFrameskip-v4')(4)

    obs, info = env.reset()
    prev_lives = 5

    lives = []
    for i in range(1000):
        action = env.action_space.sample()
        obs, reward, terminal, truncated, info = env.step(action)

        if info['lives'] != prev_lives:
            lives.append(i)
            prev_lives = info['lives']

        if terminal or truncated:
            obs = env.reset()

    assert len(lives) > 10

if __name__ == '__main__':
    test_atari_reset()



================================================
FILE: tests/test_c_advantage.cu
================================================
#include <cuda_runtime.h>
#include <stdio.h>
#include <stdlib.h>
#include <string.h>
#include <math.h>
#include "c_advantage.cu"

// Kernel declaration
__global__ void advantage_kernel(
    float* reward_block, float* reward_mask, float* values_mean,
    float* values_std, float* buf, float* dones, float* rewards,
    float* advantages, int* bounds, int num_steps, float r_std, int horizon
);

#define NUM_STEPS 6
#define HORIZON 4

float test_values_mean[NUM_STEPS * HORIZON] = {
    1.0f, 1.0f, 1.0f, 1.0f,
    1.0f, 1.0f, 1.0f, 1.0f,
    1.0f, 1.0f, 1.0f, 1.0f,
    1.0f, 1.0f, 1.0f, 1.0f,
    1.0f, 1.0f, 1.0f, 1.0f,
    1.0f, 1.0f, 1.0f, 1.0f,
};

float g = sqrt(0.5f);

float test_values_std[NUM_STEPS * HORIZON] = {
    g, g, g, g,
    g, g, g, g,
    g, g, g, g,
    g, g, g, g,
    g, g, g, g,
};

float test_dones[NUM_STEPS] = {1.0f, 0.0f, 0.0f, 0.0f, 1.0f, 0.0f};
float test_rewards[NUM_STEPS] = {0.0f, 1.0f, 0.0f, 1.0f, 0.0f, 1.0f};

int main() {
    // Test parameters
    //const int num_steps = 4;
    //const int horizon = 3;
    const float r_std = 2.0f;
    
    // Calculate sizes
    const int block_size = NUM_STEPS * HORIZON * sizeof(float);
    const int steps_size = NUM_STEPS * sizeof(float);
    const int bounds_size = NUM_STEPS * sizeof(int);

    // Host buffers
    float* h_reward_block = (float*)malloc(block_size);
    float* h_reward_mask = (float*)malloc(block_size);
    float* h_values_mean = (float*)malloc(block_size);
    float* h_values_std = (float*)malloc(block_size);
    float* h_buf = (float*)malloc(block_size);
    float* h_dones = (float*)malloc(steps_size);
    float* h_rewards = (float*)malloc(steps_size);
    float* h_advantages = (float*)malloc(steps_size);
    int* h_bounds = (int*)malloc(bounds_size);

    // Device buffers
    float *d_reward_block, *d_reward_mask, *d_values_mean, *d_values_std;
    float *d_buf, *d_dones, *d_rewards, *d_advantages;
    int* d_bounds;

    // Allocate device memory
    cudaMalloc((void**)&d_reward_block, block_size);
    cudaMalloc((void**)&d_reward_mask, block_size);
    cudaMalloc((void**)&d_values_mean, block_size);
    cudaMalloc((void**)&d_values_std, block_size);
    cudaMalloc((void**)&d_buf, block_size);
    cudaMalloc((void**)&d_dones, steps_size);
    cudaMalloc((void**)&d_rewards, steps_size);
    cudaMalloc((void**)&d_advantages, steps_size);
    cudaMalloc((void**)&d_bounds, bounds_size);

    // Initialize test data
    // Copy input data to device
    cudaMemcpy(d_values_mean, test_values_mean, block_size, cudaMemcpyHostToDevice);
    cudaMemcpy(d_values_std, test_values_std, block_size, cudaMemcpyHostToDevice);
    cudaMemcpy(d_dones, test_dones, steps_size, cudaMemcpyHostToDevice);
    cudaMemcpy(d_rewards, test_rewards, steps_size, cudaMemcpyHostToDevice);

    // Launch configuration
    int threadsPerBlock = 256;
    int blocks = (NUM_STEPS + threadsPerBlock - 1) / threadsPerBlock;

    // Launch kernel
    advantage_kernel<<<blocks, threadsPerBlock>>>(
        d_reward_block, d_reward_mask, d_values_mean, d_values_std,
        d_buf, d_dones, d_rewards, d_advantages, d_bounds,
        NUM_STEPS, r_std, HORIZON
    );

    cudaGetLastError();
    cudaDeviceSynchronize();

    // Copy results back to host
    cudaMemcpy(h_reward_block, d_reward_block, block_size, cudaMemcpyDeviceToHost);
    cudaMemcpy(h_reward_mask, d_reward_mask, block_size, cudaMemcpyDeviceToHost);
    cudaMemcpy(h_advantages, d_advantages, steps_size, cudaMemcpyDeviceToHost);
    cudaMemcpy(h_bounds, d_bounds, bounds_size, cudaMemcpyDeviceToHost);

    // Print results
    printf("Advantages:\n");
    for (int i = 0; i < NUM_STEPS; i++) {
        printf("%.2f ", h_advantages[i]);
    }
    printf("\nBounds:\n");
    for (int i = 0; i < NUM_STEPS; i++) {
        printf("%d ", h_bounds[i]);
    }
    printf("\nReward Block:\n");
    for (int i = 0; i < NUM_STEPS; i++) {
        for (int j = 0; j < HORIZON; j++) {
            printf("%.2f ", h_reward_block[i * HORIZON + j]);
        }
        printf("\n");
    }

    // Cleanup
    cudaFree(d_reward_block); cudaFree(d_reward_mask);
    cudaFree(d_values_mean); cudaFree(d_values_std);
    cudaFree(d_buf); cudaFree(d_dones);
    cudaFree(d_rewards); cudaFree(d_advantages);
    cudaFree(d_bounds);

    free(h_reward_block); free(h_reward_mask);
    free(h_values_mean); free(h_values_std);
    free(h_buf); free(h_dones);
    free(h_rewards); free(h_advantages);
    free(h_bounds);

    return 0;
}



================================================
FILE: tests/test_carbs.py
================================================
import numpy as np

from carbs import CARBS
from carbs import CARBSParams
from carbs import LinearSpace
from carbs import LogSpace
from carbs import LogitSpace
from carbs import ObservationInParam
from carbs import ParamDictType
from carbs import Param

import wandb


class SyntheticExperiment:
    def __init__(self, n_params, noise=0.1):
        self.n_params = n_params
        self.noise = noise

        self.param_optima = np.random.randn(n_params)

    def optimize(self, params):
        dist = (params-self.param_optima)**2
        reward = 2**(-dist)
        noise = 1 + self.noise*np.random.randn()
        return noise * np.prod(reward)

class CARBSSearch:
    def __init__(self, experiment):
        self.experiment = experiment
        self.best_reward = None
        self.best_params = None

        param_spaces = [
            Param(name=str(i),
                    space=LinearSpace(min=-10, max=10, is_integer=False),
                    search_center=0.0)
            for i in range(self.experiment.n_params)
        ]
        carbs_params = CARBSParams(
            better_direction_sign=1,
            is_wandb_logging_enabled=False,
            resample_frequency=0,
        )
        self.carbs = CARBS(carbs_params, param_spaces)

    def sample(self):
        suggestion = self.carbs.suggest().suggestion
        params = np.array([suggestion[str(i)] for i in range(self.experiment.n_params)])
        reward = self.experiment.optimize(params)
        if self.best_reward is None or reward > self.best_reward:
            self.best_reward = reward
            self.best_params = params

        obs_out = self.carbs.observe(
            ObservationInParam(
                input=suggestion,
                output=reward,
                cost=1,#uptime,
            )
        )
        return params, reward

class GeneticAlgorithm:
    def __init__(self, experiment, mutation_rate=0.1):
        self.experiment = experiment
        self.mutation_rate = mutation_rate
        self.best_reward = None
        self.best_params = np.random.randn(self.experiment.n_params)

    def sample(self):
        mutation = self.mutation_rate*np.random.randn(self.experiment.n_params)
        params = self.best_params + mutation
        reward = self.experiment.optimize(params)
        if self.best_reward is None or reward > self.best_reward:
            self.best_reward = reward
            self.best_params = params

        return params, reward

class WandbSearch:
    def __init__(self, experiment, method='bayes', strategy=None):
        self.experiment = experiment
        self.strategy = strategy

        self.parameters = {f'param_{i}':
            {'distribution': 'normal', 'mu': 0, 'sigma': 1}
            for i in range(10)}

        name = strategy.__class__.__name__ if strategy is not None else method
        self.sweep_id = wandb.sweep(
            sweep=dict(
                method=method,
                name=f'sweep-{name}',
                metric=dict(
                    goal='maximize',
                    name='reward',
                ),
                parameters=self.parameters,
            ),
            project="sweeping",
        )
        self.idx = 0

    def run(self):
        def main():
            self.idx += 1
            wandb.init(name=f'experiment-{self.idx}')
            wandb.config.__dict__['_locked'] = {}
            if self.strategy is not None:
                params, reward = self.strategy.sample()
            else:
                params = np.array([float(wandb.config[k]) for k in self.parameters])
                reward = self.experiment.optimize(params)

            param_dict = dict(zip(self.parameters.keys(), params))
            wandb.config.update(param_dict, allow_val_change=True)
            wandb.log({'reward': reward})

        wandb.agent(self.sweep_id, main, count=100)


if __name__ == '__main__':
    experiment = SyntheticExperiment(10)

    strategy = CARBSSearch(experiment)
    wandb_search = WandbSearch(experiment, strategy=strategy)
    wandb_search.run()

    wandb_search = WandbSearch(experiment, method='random')
    wandb_search.run()

    wandb_search = WandbSearch(experiment, method='bayes')
    wandb_search.run()

    strategy = GeneticAlgorithm(experiment)
    wandb_search = WandbSearch(experiment, strategy=strategy)
    wandb_search.run()



================================================
FILE: tests/test_cleanrl_utils.py
================================================
from pdb import set_trace as T
import numpy as np

import torch
from torch.distributions import Categorical

import gym

import pufferlib
import pufferlib.models
import pufferlib.cleanrl
import pufferlib.environments.classic_control
import pufferlib.vectorization


def test_cleanrl_utils():
    envs = pufferlib.vectorization.Serial(
        env_creator=pufferlib.environments.classic_control.env_creator('cartpole'),
        num_envs=4, envs_per_worker=2
    )
 
    obs, info, _, _ = envs.reset()

    policy = pufferlib.models.Default(envs.driver_env)
    policy = pufferlib.models.LSTMWrapper(envs.driver_env, policy)
    policy = pufferlib.cleanrl.RecurrentPolicy(policy)

    obs = torch.tensor(obs).unsqueeze(1).float()
    actions = policy.get_action_and_value(obs)

def shape_check(a1, l1, e1, a2, l2, e2):
    assert a1.shape == a2.shape
    assert l1.shape == l2.shape
    assert e1.shape == e2.shape

def test_sample_logits():
    batch = 8

    d = gym.spaces.Discrete(5)
    d_logits = torch.randn(batch, 5)
    d_action = torch.tensor([d.sample() for _ in range(batch)])

    nvec = [3, 7, 4]
    md = gym.spaces.MultiDiscrete(nvec)
    md_logits = [torch.rand(batch, n) for n in nvec]
    md_action = torch.tensor(np.array([md.sample() for _ in range(batch)]))

    a1, l1, e1 = pufferlib.cleanrl.sample_logits(d_logits)
    a2, l2, e2 = correct_sample_logits(d_logits)
    shape_check(a1, l1, e1, a2, l2, e2)

    a1, l1, e1 = pufferlib.cleanrl.sample_logits(d_logits, action=d_action)
    a2, l2, e2 = correct_sample_logits(d_logits, d_action)
    shape_check(a1, l1, e1, a2, l2, e2)

    a1, l1, e1 = pufferlib.cleanrl.sample_logits(md_logits)
    a2, l2, e2 = pufferlib.cleanrl.sample_logits(md_logits, action=md_action)
    shape_check(a1, l1, e1, a2, l2, e2)

def correct_sample_logits(logits, action=None):
    '''A bad but known correct implementation'''
    if isinstance(logits, torch.Tensor):
        categorical = Categorical(logits=logits)
        if action is None:
            action = categorical.sample()
        else:
            action = action.view(-1)

        logprob = categorical.log_prob(action)
        entropy = categorical.entropy()
        return action, logprob, entropy

    multi_categorical = [Categorical(logits=l) for l in logits]

    if action is None:
        action = torch.stack([c.sample() for c in multi_categorical])
    else:
        action = action.view(-1, action.shape[-1]).T

    logprob = torch.stack([c.log_prob(a) for c, a in zip(multi_categorical, action)]).T.sum(1)
    entropy = torch.stack([c.entropy() for c in multi_categorical]).T.sum(1)
    return action, logprob, entropy

 
if __name__ == '__main__':
    test_cleanrl_utils()
    #test_sample_logits()



================================================
FILE: tests/test_env_binding.py
================================================
from pufferlib.ocean.breakout import breakout

kwargs = dict(
    frameskip=1,
    width=576,
    height=330,
    paddle_width=62,
    paddle_height=8,
    ball_width=32,
    ball_height=32,
    brick_width=32,
    brick_height=12,
    brick_rows=6,
    brick_cols=18,
    continuous=False,
)

def test_env_binding():
    reference = breakout.Breakout()

    # Correct usage
    c_env = breakout.binding.env_init(
        reference.observations,
        reference.actions,
        reference.rewards,
        reference.terminals,
        reference.truncations,
        0,
        **kwargs
    )
    c_envs = breakout.binding.vectorize(c_env)
    breakout.binding.vec_reset(c_envs, 0)
    breakout.binding.vec_step(c_envs)
    breakout.binding.vec_close(c_envs)

    # Correct vec usage
    c_envs = breakout.binding.vec_init(
        reference.observations,
        reference.actions,
        reference.rewards,
        reference.terminals,
        reference.truncations,
        reference.num_agents,
        0,
        **kwargs
    )

    # Correct vec usage
    c_envs = breakout.binding.vec_init(
        reference.observations,
        reference.actions,
        reference.rewards,
        reference.terminals,
        reference.truncations,
        reference.num_agents,
        0,
        **kwargs
    )
    breakout.binding.vec_reset(c_envs, 0)
    breakout.binding.vec_step(c_envs)
    breakout.binding.vec_close(c_envs)

    try:
        c_env = breakout.binding.env_init()
        raise Exception('init missing args. Should have thrown TypeError')
    except TypeError:
        pass

    try:
        c_env = breakout.binding.env_init(
            reference.observations,
            reference.actions,
            reference.rewards,
            reference.terminals,
            reference.truncations,
            reference.num_agents,
            0,
        )
        raise Exception('init missing kwarg. Should have thrown TypeError')
    except TypeError:
        pass

    try:
        c_envs = breakout.binding.vec_init()
        raise Exception('vec_init missing args. Should have thrown TypeError')
    except TypeError:
        pass

    try:
        c_envs = breakout.binding.vec_init(
            reference.observations,
            reference.actions,
            reference.rewards,
            reference.terminals,
            reference.truncations,
            reference.num_agents,
            0,
        )
        raise Exception('vec_init missing kwarg. Should have thrown TypeError')
    except TypeError:
        pass

    try:
        breakout.binding.vec_reset()
        raise Exception('vec_reset missing arg. Should have thrown TypeError')
    except TypeError:
        pass

    try:
        breakout.binding.vec_step()
        raise Exception('vec_step missing arg. Should have thrown TypeError')
    except TypeError:
        pass

if __name__ == '__main__':
    test_env_binding()



================================================
FILE: tests/test_extensions.py
================================================
from pdb import set_trace as T

import numpy as np
import timeit
import gym

from pufferlib.emulation import flatten_structure, flatten_space, flatten, unflatten, concatenate, split
import pufferlib.utils

def test_pack_unpack():
    for space in nested_spaces:
        sample = space.sample()
        flat_space = flatten_space(space)
        flat_sample = flatten(sample)
        pack_sample = concatenate(flat_sample)

        sz = [int(np.prod(subspace.shape)) for subspace in flat_space.values()]
        unpack_sample = split(pack_sample, flat_space, sz, batched=False)
        unflat_sample = unflatten(unpack_sample, space)
        assert pufferlib.utils.compare_space_samples(sample, unflat_sample), "Unflatten failed."
 
test_cases = [
    # Nested Dict with Box and Discrete spaces
    gym.spaces.Dict({
        "a": gym.spaces.Box(low=0, high=1, shape=(3,)),
        "b": gym.spaces.MultiDiscrete([3, 10]),
        "c": gym.spaces.Dict({
            "d": gym.spaces.Box(low=-10, high=10, shape=(100,)),
            "e": gym.spaces.Discrete(1000)
        })
    }),

    # Nested Tuple with Box spaces of different shapes
    gym.spaces.Tuple((
        gym.spaces.Box(low=0, high=1, shape=(1,)),
        gym.spaces.Box(low=-5, high=5, shape=(10,)),
        gym.spaces.Tuple((
            gym.spaces.Box(low=-100, high=100, shape=(1000,)),
            gym.spaces.Box(low=-1000, high=1000, shape=(10000,))
        ))
    )),

    # Nested Dict with Tuple, Box, and Discrete spaces
    gym.spaces.Dict({
        "f": gym.spaces.Tuple((gym.spaces.Discrete(2), gym.spaces.Discrete(3))),
        "g": gym.spaces.Box(low=-10, high=10, shape=(50,)),
        "h": gym.spaces.Tuple((
            gym.spaces.Box(low=0, high=1, shape=(500,)),
            gym.spaces.Dict({
                "i": gym.spaces.Discrete(5000),
                "j": gym.spaces.Box(low=-100, high=100, shape=(10000,))
            })
        ))
    }),

    # Flat spaces for control
    gym.spaces.Box(low=0, high=1, shape=(10,)),
    gym.spaces.Discrete(100)
]


def test_flatten_unflatten(iterations=10_000):
    flatten_times = []
    concatenate_times = []
    split_times = []
    unflatten_times = []
    for space in test_cases:
        data = space.sample()
        flat = flatten(data)
        structure = flatten_structure(data)
        flat_space = flatten_space(space)
        merged = concatenate(flat)
        sz = [int(np.prod(subspace.shape)) for subspace in flat_space.values()]
        unmerged = split(merged, flat_space, sz, batched=False)
        unflat = unflatten(unmerged, structure)
        assert pufferlib.utils.compare_space_samples(data, unflat), "Unflatten failed."

        flatten_times.append(timeit.timeit(
            lambda: flatten(data), number=iterations))
        concatenate_times.append(timeit.timeit(
            lambda: concatenate(flat), number=iterations))
        split_times.append(timeit.timeit(
            lambda: split(merged, flat_space, sz, batched=False), number=iterations))
        unflatten_times.append(timeit.timeit(
            lambda: unflatten(unmerged, structure), number=iterations))

    print(f'{np.mean(flatten_times)/iterations:.8f}: Flatten time')
    print(f'{np.mean(concatenate_times)/iterations:.8f}: Concatenate time')
    print(f'{np.mean(split_times)/iterations:.8f}: Split time')
    print(f'{np.mean(unflatten_times)/iterations:.8f}: Unflatten time')


if __name__ == '__main__':
    iterations = 10_000
    test_flatten_unflatten(iterations=iterations)



================================================
FILE: tests/test_flatten.py
================================================
import pufferlib.extensions as c
from pufferlib.emulation import flatten_structure
import timeit


samples = [
    [1, {'foo': (1, 2, 3)}],
    {'foo': 1, 'bar': {'baz': 2, 'qux': 3}},
    1,
    {'a': [1, 2, {'b': (3, 4)}]},
    {'x': {'y': {'z': [1, 2, 3]}}},
    (1, 2, [3, 4], {'a': 5}),
    {'nested': {'more': {'and_more': (1, 2, [3, {'deep': 4}])}}},
    [[1, 2], [3, 4], [5, 6]],
    {'a': 1, 'b': 2, 'c': {'d': 3, 'e': [4, 5]}},
    (1, {'a': 2, 'b': {'c': 3, 'd': [4, 5]}}),
    {'a': {'b': {'c': {'d': 1}}}},
    [1, 2, 3, [4, 5, {'a': 6}]],
    {'single': 1},
    (1,),
    {'a': {'b': [1, 2, (3, 4, {'e': 5})]}},
    [[1, 2], 3, {'a': (4, 5)}],
    (1, [2, {'a': 3}], {'b': 4}, [5, 6]),
    {'mixed': (1, [2, 3], {'a': 4, 'b': (5, [6, 7])})}
]


def compare_data(data, unflat):
    if isinstance(data, (list, tuple)) and isinstance(unflat, (list, tuple)):
        if len(data) != len(unflat):
            return False
        return all(compare_data(d, f) for d, f in zip(data, unflat))
    elif isinstance(data, dict) and isinstance(unflat, dict):
        if len(data) != len(unflat):
            return False
        return all(compare_data(data[key], unflat[key]) for key in sorted(data))
    else:
        return data == unflat

def test_flatten_unflatten():
    for sample in samples:
        structure = flatten_structure(sample)
        flat = c.flatten(sample)
        unflat = c.unflatten(flat, structure)
        if not compare_data(sample, unflat):
            print(f"Sample: {sample}")
            print(f"Flattened: {flat}")
            print(f"Unflattened: {unflat}")
            breakpoint()
        assert compare_data(sample, unflat)

def test_flatten_performance(n=100_000):
    print("\nFlatten Performance Testing:")
    total_calls_per_second = 0
    num_samples = len(samples)
    for sample in samples:
        wrapped = lambda: c.flatten(sample)
        time_per_call = timeit.timeit(wrapped, number=n) / n
        calls_per_second_in_k = int(1 / time_per_call / 1000)
        print(f"Sample {str(sample)[:10]}... - Average flatten calls per second: {calls_per_second_in_k} K")
        total_calls_per_second += calls_per_second_in_k
    avg_calls_per_second_in_k = total_calls_per_second // num_samples
    print(f"Average flatten calls per second across all samples: {avg_calls_per_second_in_k} K")

def test_unflatten_performance(n=100_000):
    print("\nUnflatten Performance Testing:")
    total_calls_per_second = 0
    num_samples = len(samples)
    for sample in samples:
        flat = c.flatten(sample)
        structure = flatten_structure(sample)
        wrapped = lambda: c.unflatten(flat, structure)
        time_per_call = timeit.timeit(wrapped, number=n) / n
        calls_per_second_in_k = int(1 / time_per_call / 1000)
        print(f"Sample {str(sample)[:10]}... - Average unflatten calls per second: {calls_per_second_in_k} K")
        total_calls_per_second += calls_per_second_in_k
    avg_calls_per_second_in_k = total_calls_per_second // num_samples
    print(f"Average unflatten calls per second across all samples: {avg_calls_per_second_in_k} K")


if __name__ == "__main__":
    test_flatten_unflatten()
    test_flatten_performance()
    test_unflatten_performance()



================================================
FILE: tests/test_import_performance.py
================================================
import time

def test_import_speed():
    start = time.time() 
    import pufferlib
    end = time.time()
    print(end - start, ' seconds to import pufferlib')
    assert end - start < 0.25

if __name__ == '__main__':
    test_import_speed()


================================================
FILE: tests/test_namespace.py
================================================
from pufferlib import namespace, dataclass

def test_namespace_as_function():
    ns = namespace(x=1, y=2, z=3)
    
    assert ns.x == 1
    assert ns.y == 2
    assert ns.z == 3
    assert list(ns.keys()) == ['x', 'y', 'z']
    assert list(ns.values()) == [1, 2, 3]
    assert list(ns.items()) == [('x', 1), ('y', 2), ('z', 3)]

@dataclass
class TestClass:
    a: int
    b = 1

def test_namespace_as_decorator():
    obj = TestClass(a=4, b=5)
    
    assert obj.a == 4
    assert obj.b == 5
    assert list(obj.keys()) == ['a', 'b']
    assert list(obj.values()) == [4, 5]
    assert list(obj.items()) == [('a', 4), ('b', 5)]

if __name__ == '__main__':
    test_namespace_as_function()
    test_namespace_as_decorator()



================================================
FILE: tests/test_nested.py
================================================
from pdb import set_trace as T
import numpy as np

import pufferlib.spaces
from pufferlib.emulation import flatten

def dtype_from_space(space):
    if isinstance(space, pufferlib.spaces.Tuple):
        dtype = []
        num_bytes = 0
        for i, elem in enumerate(space):
            dtype_ext, bytes_ext = dtype_from_space(elem)
            dtype.append((f'f{i}', dtype_ext))
            #dtype.append((dtype_ext,))
            num_bytes += bytes_ext
    elif isinstance(space, pufferlib.spaces.Dict):
        dtype = []
        num_bytes = 0
        for k, value in space.items():
            dtype_ext, bytes_ext = dtype_from_space(value)
            dtype.append((k, dtype_ext))
            num_bytes += bytes_ext
    else:
        dtype = (space.dtype, space.shape)
        num_bytes = space.dtype.itemsize * np.prod(space.shape)

    return dtype, num_bytes


def flat_dtype_from_space(space, name=None):
    dtype = []
    _flat_dtype_from_space(space, dtype, name)
    return dtype

def _flat_dtype_from_space(space, dtype, name=None):
    if isinstance(space, pufferlib.spaces.Tuple):
        for i, elem in enumerate(space):
            _flat_dtype_from_space(elem, dtype, name=f'f{i}')
            #_flat_dtype_from_space(elem, dtype, name=None)
    elif isinstance(space, pufferlib.spaces.Dict):
        for k, value in space.items():
            _flat_dtype_from_space(value, dtype, name=k)
    else:
        if name is not None:
            dtype.append((name, space.dtype, space.shape))
        else:
            dtype.append((space.dtype, space.shape))

    return dtype

def fill_with_sample(arr, sample):
    if isinstance(sample, dict):
        for k, v in sample.items():
            fill_with_sample(arr[k], v)
    elif isinstance(sample, tuple):
        for i, v in enumerate(sample):
            fill_with_sample(arr[f'f{i}'], v)
    else:
        arr[()] = sample


from gymnasium.spaces import Tuple, Dict, Box

test_space = Tuple([
    Dict({
        'a': Box(0, 1, shape=(2,)),
        'b': Box(0, 1, shape=(3,))
    }),
    Dict({
        'c': Box(0, 1, shape=(4,)),
    })
])

# Some notes:
# The flat version may be faster. It allows you to fill in a single
# numpy call, but you still have the precomputation step, although
# that one is with no copies. The main limit there is that you need
# unique dict keys everywhere to do it, since they are no longer
# unique when flat, even if they are in the structured version
# In either case, tuples need keys assigned for each element, which is
# the main limitation.

dtype, num_bytes = dtype_from_space(test_space)
dtype = np.dtype(dtype)
elem = np.zeros(1, dtype=dtype)
#flat_dtype = flat_dtype_from_space(test_space)
sample = test_space.sample()
fill_with_sample(elem, sample)
breakpoint()
#flat_sample = flatten(sample)
rec_array = np.rec.array(flat_sample, dtype=flat_dtype)
rec_array = rec_array.view(dtype)

'''
test_space = Dict({
    'a': Box(0, 1, shape=(3,)),
    'b': Dict({
        'c': Box(0, 1, shape=(4,)),
        'd': Box(0, 1, shape=(3,))
    }),
    'e': Box(0, 1, shape=(3,))
})
'''
 
breakpoint()
exit()

def mkdt(d):
    ll = []
    sz_bytes = 0
    for k,v in d.items():
        if isinstance(v,np.ndarray):
            ll.append((k,v.dtype))
            sz_bytes += v.nbytes
        else:
            l_ext, sz_ext = mkdt(v)
            ll.append((k,l_ext))
            sz_bytes += sz_ext
    return ll, sz_bytes

def mkdt_flat(d):
    dtype = []
    return _mkdt_flat(d, dtype)

def _mkdt_flat(d, dtype):
    for k,v in d.items():
        if isinstance(v,np.ndarray):
            dtype.append((k,v.dtype))
        else:
            _mkdt_flat(v, dtype)
    return dtype


arr1=np.arange(10).astype(np.float32)
arr2=np.arange(100.,110.).astype(np.uint8)
arr3=np.arange(200,210).astype(np.int32)
d={'a':arr1, 'b':{'b1':arr2, 'b2':{'c':arr3}}}
dt, sz_bytes = mkdt(d)

#A = np.zeros(sz_bytes, dtype=np.uint8)
flat = flatten(d)
flat_dtype = mkdt_flat(d)
rec_array = np.rec.array(flat, dtype=flat_dtype).view(dt)



================================================
FILE: tests/test_newbind.py
================================================
import time
import numpy as np

from pufferlib.ocean.squared import binding as squared_bind

N = 2048
TIME = 1.0

# Create NumPy arrays with varying dtypes and sizes
obs = np.zeros((N, 11, 11), dtype=np.uint8)
atn = np.zeros((N), dtype=np.int32)
rew = np.zeros((N), dtype=np.float32)
term = np.zeros((N), dtype=np.uint8)
trunc = np.zeros((N), dtype=np.uint8)

def make_envs():
    env_ptrs = []
    for i in range(N):
        ptr = squared_bind.env_init(obs[i].ravel(), atn[i:i+1], rew[i:i+1], term[i:i+1], trunc[i:i+1], size=11)
        env_ptrs.append(ptr)

    return env_ptrs

def time_loop():
    env_ptrs = make_envs()
    for ptr in env_ptrs:
        squared_bind.env_reset(ptr)

    start = time.time()
    atn[:] = np.random.randint(0, 5, (N))
    steps = 0
    while time.time() - start < TIME:
        steps += N
        for i in range(N):
            squared_bind.env_step(env_ptrs[i])

    print("Loop SPS:", steps / (time.time() - start))

def time_vec():
    env_ptrs = make_envs()
    vec_ptr = squared_bind.init_vec(obs, atn, rew, term, trunc, N, size=11)
    squared_bind.vec_reset(vec_ptr)
    start = time.time()
    atn[:] = np.random.randint(0, 5, (N))

    steps = 0
    while time.time() - start < TIME:
        squared_bind.vec_step(vec_ptr)
        steps += N

    print("Vec SPS:", steps / (time.time() - start))

    for ptr in env_ptrs:
        squared_bind.env_close(ptr)

def test_loop():
    env_ptrs = make_envs()
    for ptr in env_ptrs:
        squared_bind.env_reset(ptr)

    while True:
        atn[:] = np.random.randint(0, 5, (N))
        for i in range(N):
            squared_bind.env_step(env_ptrs[i])

        squared_bind.env_render(env_ptrs[0])

    for ptr in env_ptrs:
        squared_bind.env_close(ptr)

def test_vec():
    vec_ptr = squared_bind.init_vec(obs, atn, rew, term, trunc, N, size=11)
    squared_bind.vec_reset(vec_ptr)
    while True:
        atn[:] = np.random.randint(0, 5, (N))
        squared_bind.vec_step(vec_ptr)
        squared_bind.vec_render(vec_ptr, 0)

    squared_bind.vec_close(vec_ptr)

def test_env_binding():
    ptr = squared_bind.env_init(obs[0], atn[0:1], rew[0:1], term[0:1], trunc[0:1], size=11)
    squared_bind.env_reset(ptr)
    squared_bind.env_step(ptr)
    squared_bind.env_close(ptr)

def test_vectorize_binding():
    ptr = squared_bind.env_init(obs[0], atn[0:1], rew[0:1], term[0:1], trunc[0:1], size=11)
    vec_ptr = squared_bind.vectorize(ptr)
    squared_bind.vec_reset(vec_ptr)
    squared_bind.vec_step(vec_ptr)
    squared_bind.vec_close(vec_ptr)

def test_vec_binding():
    vec_ptr = squared_bind.init_vec(obs, atn, rew, term, trunc, N, size=11)
    squared_bind.vec_reset(vec_ptr)
    squared_bind.vec_step(vec_ptr)
    squared_bind.vec_close(vec_ptr)

def test_log():
    vec_ptr = squared_bind.init_vec(obs, atn, rew, term, trunc, N, size=11)
    squared_bind.vec_reset(vec_ptr)
    for i in range(1000):
        squared_bind.vec_step(vec_ptr)

    logs = squared_bind.vec_log(vec_ptr)
    print(logs)
    squared_bind.vec_close(vec_ptr)

def test_pong():
    from pufferlib.ocean.pong import binding as pong_bind
    N = 2048

    obs = np.zeros((N, 8), dtype=np.float32)
    atn = np.zeros((N), dtype=np.int32)
    rew = np.zeros((N), dtype=np.float32)
    term = np.zeros((N), dtype=np.uint8)
    trunc = np.zeros((N), dtype=np.uint8)

    ptr = pong_bind.init_vec(obs, atn, rew, term, trunc, N,
        width=500, height=640, paddle_width=20, paddle_height=70,
        ball_width=32, ball_height=32, paddle_speed=8,
        ball_initial_speed_x=10, ball_initial_speed_y=1,
        ball_speed_y_increment=3, ball_max_speed_y=13,
        max_score=21, frameskip=1, continuous=False
    )

    pong_bind.vec_reset(ptr)
    while True:
        pong_bind.vec_step(ptr)
        pong_bind.vec_render(ptr, 0)

    pong_bind.vec_close(ptr)

def test_pong_single():
    from pufferlib.ocean.pong import binding as pong_bind
    obs = np.zeros((8), dtype=np.float32)
    atn = np.zeros((1,), dtype=np.int32)
    rew = np.zeros((1,), dtype=np.float32)
    term = np.zeros((1,), dtype=np.uint8)
    trunc = np.zeros((1,), dtype=np.uint8)

    ptr = pong_bind.env_init(obs, atn, rew, term, trunc,
        width=500, height=640, paddle_width=20, paddle_height=70,
        ball_width=32, ball_height=32, paddle_speed=8,
        ball_initial_speed_x=10, ball_initial_speed_y=1,
        ball_speed_y_increment=3, ball_max_speed_y=13,
        max_score=21, frameskip=1, continuous=False
    )

    pong_bind.env_reset(ptr)
    while True:
        pong_bind.env_step(ptr)
        pong_bind.env_render(ptr)

    pong_bind.env_close(ptr)


if __name__ == '__main__':
    #test_loop()
    #test_vec()
    #time_loop()
    #time_vec()
    test_log()

    #test_pong_single()
    #test_env_binding()
    #test_vectorize_binding()
    #test_vec_binding()




================================================
FILE: tests/test_nmmo3_compile.py
================================================
from pdb import set_trace as T
import time
import torch
import numpy as np


@torch.compile(fullgraph=True, mode='reduce-overhead')
def fast_decode_map(codes, obs, factors, add, div):
    codes = codes.view(codes.shape[0], 1, -1)
    dec = add + (codes//div) % factors
    obs.scatter_(1, dec, 1)
    return obs

#@torch.compile(fullgraph=True, mode='reduce-overhead')
def decode_map(codes):
    codes = codes.unsqueeze(1).long()
    factors = [4, 4, 16, 5, 3, 5, 5, 6, 7, 4]
    n_channels = sum(factors)
    obs = torch.zeros(codes.shape[0], n_channels, 11, 15, device='cuda')

    add, div = 0, 1
    # TODO: check item/tier order
    for mod in factors:
        obs.scatter_(1, add+(codes//div)%mod, 1)
        add += mod
        div *= mod

    return obs


def test_perf(n=100, agents=1024):
    factors = np.array([4, 4, 16, 5, 3, 5, 5, 6, 7, 4])
    n_channels = sum(factors)
    add = np.array([0, *np.cumsum(factors).tolist()[:-1]])[None, :, None]
    div = np.array([1, *np.cumprod(factors).tolist()[:-1]])[None, :, None]

    factors = torch.tensor(factors)[None, :, None].cuda()
    add = torch.tensor(add).cuda()
    div = torch.tensor(div).cuda()

    codes = torch.randint(0, 4*4*16*5*3*5*5*6*7*4, (agents, 11, 15)).cuda()
    obs = torch.zeros(agents, n_channels, 11*15, device='cuda')
    obs_view = obs.view(agents, n_channels, 11, 15)

    # Warm up
    decode_map(codes)
    fast_decode_map(codes, obs, factors, add, div)
    torch.cuda.synchronize()

    start = time.time()
    for _ in range(n):
        fast_decode_map(codes, obs, factors, add, div)
        #obs2 = decode_map(codes)
        #print(torch.all(obs_view == obs2))


    torch.cuda.synchronize()
    end = time.time()
    sps = n / (end - start)
    print(f'SPS: {sps:.2f}')

if __name__ == '__main__':
    test_perf()




================================================
FILE: tests/test_performance.py
================================================
from pdb import set_trace as T
import time
from tqdm import tqdm
import importlib
import random
import sys

import pufferlib
import pufferlib.utils
import pufferlib.exceptions
import pufferlib.emulation
import pufferlib.environments

import numpy as np

import pufferlib
from pufferlib.environments import ocean
from pufferlib.vector import Multiprocessing, Serial, Ray, make, autotune

import time
import psutil
import gymnasium

DEFAULT_TIMEOUT = 10

import time
from functools import wraps

class TimedEnv:
    def __init__(self, env):
        self._env = env
        self.reset_times = []
        self.step_times = []

    def __getattr__(self, name):
        return getattr(self._env, name)

    def step(self, *args, **kwargs):
        start = time.time()
        result = self._env.step(*args, **kwargs)
        end = time.time()
        elapsed = end - start
        self.step_times.append(elapsed)
        return result

    def reset(self, *args, **kwargs):
        start = time.time()
        result = self._env.reset(*args, **kwargs)
        end = time.time()
        elapsed = end - start
        self.reset_times.append(elapsed)
        return result

def profile_emulation(env_creator, timeout=DEFAULT_TIMEOUT, seed=42):
    reset_times = []
    step_times = []
    agent_step_count = 0
    terminal = False
    truncated = False
    reset = True

    random.seed(seed)
    np.random.seed(seed)

    env = env_creator()
    env.env = TimedEnv(env.env)
    multiagent = callable(env.action_space)

    start = time.time()
    while time.time() - start < timeout:
        if reset:
            s = time.time()
            ob, info = env.reset(seed=seed)
            reset_times.append(time.time() - s)

        if multiagent:
            action = {agent: env.action_space(agent).sample() for agent in ob}
            agent_step_count += len(env.agents)    
        else:
            action = env.action_space.sample()
            agent_step_count += 1

        s = time.time()
        ob, reward, terminal, truncated, info = env.step(action)
        step_times.append(time.time() - s)

        reset = (multiagent and len(env.agents) == 0) or terminal or truncated

    env.close()

    puf_total_reset = sum(reset_times)
    puf_total_step = sum(step_times)
    puf_reset_mean = np.mean(reset_times)
    puf_step_mean = np.mean(step_times)
    puf_step_std = np.std(step_times)

    raw_total_reset = sum(env.env.reset_times)
    raw_total_step = sum(env.env.step_times)
    raw_reset_mean = np.mean(env.env.reset_times)
    raw_step_mean = np.mean(env.env.step_times)
    raw_step_std = np.std(env.env.step_times)

    env_sps = agent_step_count / (puf_total_step + puf_total_reset)
    env_percent_reset = 100 * puf_total_reset / (puf_total_reset + puf_total_step)
    env_percent_step_std = 100 * puf_step_std / puf_step_mean
    env_overhead = 100 * (puf_total_step - raw_total_step + puf_total_reset - raw_total_reset) / (puf_total_step + puf_total_reset)


    print(f'    SPS        : {env_sps:.1f}')
    print(f'    Overhead   : {env_overhead:.2g}%')
    print(f'    Reset      : {env_percent_reset:.3g}%')
    print(f'    Step STD   : {env_percent_step_std:.3g}%')

def profile_puffer(env_creator, timeout=DEFAULT_TIMEOUT, **kwargs):
    vecenv = make(env_creator, **kwargs)
    actions = [vecenv.action_space.sample() for _ in range(1000)]

    agent_steps = 0
    vecenv.reset()
    start = time.time()
    while time.time() - start < timeout:
        vecenv.send(actions[agent_steps%1000])
        o, r, d, t, i, env_id, mask = vecenv.recv()
        agent_steps += sum(mask)

    sps = agent_steps / (time.time() - start)
    vecenv.close()

    backend = kwargs.get('backend', Serial)
    if backend == Multiprocessing and 'batch_size' in kwargs:
        print(f'    Puffer     : {(sps):.1f} - Pool')
    else:
        print(f'    Puffer     : {(sps):.1f} - {backend.__name__}')
    return sps

def profile_gymnasium_vec(env_creator, num_envs, timeout=DEFAULT_TIMEOUT):
    vecenv = gymnasium.vector.AsyncVectorEnv([env_creator] * num_envs)
    actions = [vecenv.action_space.sample() for _ in range(1000)]

    steps = 0
    vecenv.reset()
    start = time.time()
    while time.time() - start < timeout:
        vecenv.step(actions[steps%1000])
        steps += 1

    sps = steps * vecenv.num_envs / (time.time() - start)
    vecenv.close()

    print(f'    Gymnasium  : {(sps):.1f}')
    return sps

def profile_sb3_vec(env_creator, num_envs, timeout=DEFAULT_TIMEOUT):
    with pufferlib.utils.Suppress():
        from stable_baselines3.common.vec_env import SubprocVecEnv
        vecenv = SubprocVecEnv([env_creator] * num_envs)
        actions = [[vecenv.action_space.sample() for _ in range(num_envs)]
            for _ in range(1000)]

        steps = 0
        vecenv.reset()
        start = time.time()
        while time.time() - start < timeout:
            vecenv.step(actions[steps%1000])
            steps += 1

        sps = steps * vecenv.num_envs / (time.time() - start)
        vecenv.close()

    print(f'    SB3        : {(sps):.1f}')
    return sps

def profile_all(name, env_creator, num_envs, num_workers=24,
        env_batch_size=None, zero_copy=True, timeout=DEFAULT_TIMEOUT):
    if env_batch_size is None:
        env_batch_size = num_envs

    print(name)
    profile_emulation(env_creator, timeout=timeout)
    profile_puffer(env_creator, num_envs=env_batch_size,
        backend=Multiprocessing, timeout=timeout,
        num_workers=min(num_workers, env_batch_size),
    )
    if env_batch_size is not None and env_batch_size != num_envs:
        profile_puffer(env_creator, num_envs=num_envs,
            backend=Multiprocessing, timeout=timeout, num_workers=num_workers,
            batch_size=env_batch_size, zero_copy=zero_copy
        )
    profile_gymnasium_vec(env_creator, num_envs=env_batch_size, timeout=timeout)
    profile_sb3_vec(env_creator, num_envs=env_batch_size, timeout=timeout)
    print()

if __name__ == '__main__':
    from pufferlib.environments import nocturne
    env_creator = nocturne.env_creator()
    profile_emulation(env_creator)
    #profile_puffer(env_creator, num_envs=8, backend=Multiprocessing)
    exit(0)

    from pufferlib.environments import vizdoom
    env_creator = vizdoom.env_creator()
    #profile_emulation(env_creator)
    profile_puffer(env_creator, num_envs=24,
        batch_size=8, backend=Multiprocessing, zero_copy=False)

    from pufferlib.environments import ocean
    env_creator = ocean.env_creator('grid')
    #profile_emulation(env_creator)

    import cProfile
    cProfile.run('profile_emulation(env_creator)', 'stats.profile')
    import pstats
    from pstats import SortKey
    p = pstats.Stats('stats.profile')
    p.sort_stats(SortKey.TIME).print_stats(10)

    exit(0)

    from pufferlib.environments import nmmo
    print('Neural MMO')
    env_creator = nmmo.env_creator()
    profile_emulation(env_creator)
    #profile_puffer(env_creator, num_envs=8, backend=Multiprocessing)
    profile_puffer(env_creator, num_envs=96,
        batch_size=48, backend=Multiprocessing, zero_copy=False)
    print()

    from pufferlib.environments import nethack
    profile_all('NetHack', nethack.env_creator(), num_envs=48)

    from pufferlib.environments import minihack
    profile_all('MiniHack', minihack.env_creator(), num_envs=48)

    from pufferlib.environments import pokemon_red
    profile_all('Pokemon Red', pokemon_red.env_creator(),
        num_envs=144, env_batch_size=48, zero_copy=False)

    from pufferlib.environments import procgen
    profile_all('ProcGen', procgen.env_creator('bigfish'),
        num_envs=144, env_batch_size=48, num_workers=24, zero_copy=False)

    from pufferlib.environments import classic_control
    profile_all('Classic Control', classic_control.env_creator(),
        num_envs=1152, env_batch_size=48)

    from pufferlib.environments import ocean
    profile_all('Ocean Squared', ocean.env_creator('squared'),
        num_envs=1152, env_batch_size=48)

    from pufferlib.environments import atari
    profile_all('Atari Breakout', atari.env_creator('BreakoutNoFrameskip-v4'),
        num_envs=144, env_batch_size=48, zero_copy=False)

    from pufferlib.environments import crafter
    profile_all('Crafter', crafter.env_creator(),
        num_envs=24, env_batch_size=8, zero_copy=False)

    from pufferlib.environments import minigrid
    profile_all('MiniGrid', minigrid.env_creator(),
        num_envs=192, env_batch_size=48, zero_copy=False)

    exit(0)

    '''
    # Small scale version for laptop
    from pufferlib.environments import nmmo
    print('Neural MMO')
    env_creator = nmmo.env_creator()
    profile_emulation(env_creator)
    profile_puffer(env_creator, num_envs=4, num_workers=4, backend=Multiprocessing)
    profile_puffer(env_creator, num_envs=12, num_workers=6,
        batch_size=4, backend=Multiprocessing)
    print()

    from pufferlib.environments import nethack
    profile_all('NetHack', nethack.env_creator(), num_envs=12, num_workers=6)

    from pufferlib.environments import minihack
    profile_all('MiniHack', minihack.env_creator(), num_envs=12, num_workers=6)

    from pufferlib.environments import pokemon_red
    profile_all('Pokemon Red', pokemon_red.env_creator(),
        num_envs=36, num_workers=6, env_batch_size=12, zero_copy=False)

    from pufferlib.environments import classic_control
    profile_all('Classic Control', classic_control.env_creator(),
        num_envs=36, num_workers=6, env_batch_size=12, zero_copy=False)

    from pufferlib.environments import ocean
    profile_all('Ocean Squared', ocean.env_creator('squared'),
        num_envs=36, num_workers=6, env_batch_size=12, zero_copy=False)

    from pufferlib.environments import atari
    profile_all('Atari Breakout', atari.env_creator('BreakoutNoFrameskip-v4'),
        num_envs=36, num_workers=6, env_batch_size=12, zero_copy=False)

    from pufferlib.environments import crafter
    profile_all('Crafter', crafter.env_creator(),
        num_envs=12, num_workers=6, env_batch_size=4, zero_copy=False)

    from pufferlib.environments import minigrid
    profile_all('MiniGrid', minigrid.env_creator(),
        num_envs=36, num_workers=6, env_batch_size=12, zero_copy=False)

    exit(0)
    '''

    #from functools import partial
    #counts = [1e5, 1e6, 1e7, 1e8]
    #delays = [0, 0.1, 0.25, 0.5, 1]
    #bandwidth = [1, 1e4, 1e5, 1e6]

    
    #synthetic_creators = {}
    #for count in counts:
    #    name = f'test_delay_{count}'

    #env_creators.test = partial(
    #    ocean.env_creator('performance_empiric'),
    #    count_n=270_000, bandwidth=150_000
    #)

    #timeout = 60
    #cores = psutil.cpu_count(logical=False)
    #for key, creator in env_creators.items():
    #    prof = profile_emulation(creator, timeout)
    #    profile_vec(creator, cores, timeout, prof.puf.sps)
    #    print()



================================================
FILE: tests/test_pokemon_red.py
================================================
from pufferlib.environments.pokemon_red import env_creator

env = env_creator()()
ob, info = env.reset()
for i in range(100):
    ob, reward, terminal, truncated, info = env.step(env.action_space.sample())
    print(f'Step: {i}, Info: {info}')

env.close()



================================================
FILE: tests/test_policy_pool.py
================================================
import unittest

import numpy as np
import torch

import pufferlib.policy_pool as pp

NUM_AGENTS = 4
NUM_ENVS = 2
POOL_AGENTS = NUM_AGENTS * NUM_ENVS  # batch size
OBS_DIM = 3
ACTION_DIM = 5

# TODO: add test for recurrent policy forward
BPTT_HORIZON = 16
LSTM_INPUT_DIM = POOL_AGENTS * BPTT_HORIZON
LSTM_HIDDEN_DIM = 32


class MockPolicy:
    def __call__(self, obs):
        batch_size = obs.shape[0]
        actions = torch.arange(batch_size * ACTION_DIM).view(batch_size, ACTION_DIM)
        logprobs = torch.arange(batch_size, dtype=torch.float32)
        values = torch.arange(batch_size, dtype=torch.float32) + 10  # add to make the values different
        return actions, logprobs, None, values

class MockPolicyStore:
    def __init__(self, num_policies):
        self._policies = {f'Policy{i+1}': MockPolicy() for i in range(num_policies)}
        self.path = 'mock_policy_store'

    def policy_names(self):
        return list(self._policies.keys())

    def get_policy(self, name):
        return self._policies[name]

class TestPolicyPool(unittest.TestCase):
    def setUp(self):
        self.mock_nonrecurrent_policy = MockPolicy()
        self.mock_nonrecurrent_policy.name = 'BasePolicy1'
        self.nonrecurrent_policy_pool = pp.PolicyPool(
            policy=self.mock_nonrecurrent_policy,
            total_agents=POOL_AGENTS,
            atn_shape=(ACTION_DIM,),
            device='cpu',
            policy_store=MockPolicyStore(3),
            kernel = [0, 1, 0, 2],
            skip_ranker=True,
        )

    def test_init_with_kernel(self):
        test_policy_pool = self.nonrecurrent_policy_pool
        kernel = [0, 1, 0, 2]
        policy_ids, sample_idxs, kernel = test_policy_pool._init_sample_idx_from_kernel(kernel)

        self.assertTrue(np.array_equal(policy_ids, np.array([0, 1, 2])))
        self.assertEqual(sample_idxs, {0: [0, 2, 4, 6], 1: [1, 5], 2: [3, 7]})  # map policy id to agent list
        self.assertEqual(kernel, [0, 1, 0, 2, 0, 1, 0, 2])  # tiled into POOL_AGENTS

    def test_update_policies(self):
        policy_pool = self.nonrecurrent_policy_pool

        # Test with no policies in the policy store
        # All policies should be the learner policy
        policy_store = MockPolicyStore(0)
        policy_pool.update_policies(policy_ids=np.array([0, 1, 2]), store=policy_store)
        for pol in policy_pool.current_policies.values():
            self.assertEqual(pol['name'], 'learner')
            self.assertEqual(pol['policy'], policy_pool.learner_policy)

        # Sample 2 policies when there is only one policy in the policy store
        # Both policies should be Policy1
        policy_store = MockPolicyStore(1)
        policy_pool.update_policies(policy_ids=np.array([0, 1, 2]), store=policy_store)
        for pol in policy_pool.current_policies.values():
            self.assertEqual(pol['name'], 'Policy1')
            self.assertEqual(pol['policy'], policy_store.get_policy('Policy1'))

        # Sample 3 policies when there are 10 policies in the policy store
        # All sampled policies should be different
        policy_store = MockPolicyStore(10)
        policy_pool.update_policies(policy_ids=np.array([0, 1, 2, 3]), store=policy_store)
        self.assertEqual(len(set(p['name'] for p in policy_pool.current_policies.values())), 3)

        # Use all_selector
        policy_store = MockPolicyStore(5)
        policy_pool.update_policies(policy_ids=np.array([0, 1, 2, 3, 4, 5]), store=policy_store,
                                    policy_selector=pp.AllPolicySelector(seed=0))
        self.assertEqual(len(set(p['name'] for p in policy_pool.current_policies.values())), 5)

    def test_nonrecurrent_forward(self):
        policy_pool = self.nonrecurrent_policy_pool

        obs = torch.arange(POOL_AGENTS * OBS_DIM).view(POOL_AGENTS, OBS_DIM)
        atn, lgprob, val, _ = policy_pool.forwards(obs)

        for policy_id in policy_pool.policy_ids:
            samp = policy_pool.sample_idxs[policy_id]
            policy = policy_pool.learner_policy if policy_id == 0 \
                else policy_pool.current_policies[policy_id]['policy']
            atn1, lgprob1, _, val1 = policy(obs[samp])

            self.assertTrue(torch.equal(atn[samp], atn1))
            self.assertTrue(torch.equal(lgprob[samp], lgprob1))
            self.assertTrue(torch.equal(val[samp], val1))

    def test_update_scores(self):
        policy_pool = self.nonrecurrent_policy_pool
        # With the kernel [0, 1, 0, 2], agents 1 and 3 are learner, and agents 2 and 4 are different

        infos = [{1: {'return': 1}, 2: {'return': 2}, 3: {'return': 3}, 4: {'return': 4}},
                 {1: {'return': 10}, 2: {'return': 20}, 4: {'return': 40}}]
        pol1_name = policy_pool._get_policy_name(2)
        pol2_name = policy_pool._get_policy_name(4)

        policy_infos = policy_pool.update_scores(infos, 'return')
        self.assertEqual(policy_infos['learner'], [{'return': 1}, {'return': 3}, {'return': 10}])
        self.assertEqual(policy_infos[pol1_name], [{'return': 2}, {'return': 20}])
        self.assertEqual(policy_infos[pol2_name], [{'return': 4}, {'return': 40}])

        # policy_pool.scores only keep the last game's results
        self.assertEqual(policy_pool.scores['learner'], 10)
        self.assertEqual(policy_pool.scores[pol1_name], 20)
        self.assertEqual(policy_pool.scores[pol2_name], 40)

if __name__ == '__main__':
    unittest.main()



================================================
FILE: tests/test_puffernet.py
================================================
import torch
import numpy as np


import pyximport
pyximport.install(
    setup_args={"include_dirs": [
        np.get_include(),
        'pufferlib/extensions',
    ]},
)

from pufferlib.extensions import puffernet

# TODO: Should probably add a safe mode that type checks input arrays
# It's user error, but it is a big foot gun

def make_dummy_data(*shape, seed=42):
    np.random.seed(seed)
    n = np.prod(shape)
    ary = np.random.rand(*shape).astype(np.float32) - 0.5
    return np.ascontiguousarray(ary)

def make_dummy_int_data(num_classes, *shape, seed=42):
    np.random.seed(seed)
    n = np.prod(shape)
    ary = np.random.randint(0, num_classes, shape).astype(np.int32)
    return np.ascontiguousarray(ary)

def assert_near(a, b):
    assert a.shape == b.shape
    assert np.all(np.abs(a - b) < 1e-4)

def test_puffernet_relu(batch_size=16, input_size=128):
    input_puffer = make_dummy_data(batch_size, input_size)

    input_torch = torch.from_numpy(input_puffer)
    output_torch = torch.relu(input_torch).detach()
    
    # PufferNet done second because it is in-place on the input
    puffernet.puf_relu(input_puffer, input_puffer, batch_size*input_size)

    assert_near(input_puffer, output_torch.numpy())

def test_puffernet_sigmoid(n=1024, epsilon=1e-4):
    input_np = make_dummy_data(n)

    input_torch = torch.from_numpy(input_np)
    output_torch = torch.sigmoid(input_torch).detach()

    for i in range(n):
        out_torch = output_torch[i]
        out_puffer = puffernet.puf_sigmoid(input_np[i])
        assert abs(out_puffer - out_torch) < epsilon

def test_puffernet_linear_layer(batch_size=16, input_size=128, hidden_size=128):
    input_np = make_dummy_data(batch_size, input_size, seed=42)
    weights_np = make_dummy_data(hidden_size, input_size, seed=43)
    bias_np = make_dummy_data(hidden_size, seed=44)
    output_puffer = np.zeros((batch_size, hidden_size), dtype=np.float32)
    puffernet.puf_linear_layer(input_np, weights_np, bias_np, output_puffer,
        batch_size, input_size, hidden_size)

    input_torch = torch.from_numpy(input_np)
    weights_torch = torch.from_numpy(weights_np)
    bias_torch = torch.from_numpy(bias_np)
    torch_linear = torch.nn.Linear(input_size, hidden_size)
    torch_linear.weight.data = weights_torch
    torch_linear.bias.data = bias_torch
    output_torch = torch_linear(input_torch).detach()

    assert_near(output_puffer, output_torch.numpy())

def test_puffernet_convolution_layer(batch_size=16, in_width=11, in_height=11,
        in_channels=19, out_channels=32, kernel_size=5, stride=3):
    input_np = make_dummy_data(batch_size, in_channels, in_height, in_width)
    weights_np = make_dummy_data(out_channels, in_channels, kernel_size, kernel_size)
    bias_np = make_dummy_data(out_channels)
    out_height = int((in_height - kernel_size)/stride + 1)
    out_width = int((in_width - kernel_size)/stride + 1)
    output_puffer = np.zeros((batch_size, out_channels, out_height, out_width), dtype=np.float32)
    puffernet.puf_convolution_layer(input_np, weights_np, bias_np, output_puffer,
        batch_size, in_width, in_height, in_channels, out_channels, kernel_size, stride)

    input_torch = torch.from_numpy(input_np)
    weights_torch = torch.from_numpy(weights_np)
    bias_torch = torch.from_numpy(bias_np)
    torch_conv = torch.nn.Conv2d(in_channels, out_channels, kernel_size, stride)
    torch_conv.weight.data = weights_torch
    torch_conv.bias.data = bias_torch
    output_torch = torch_conv(input_torch).detach()

    assert_near(output_puffer, output_torch.numpy())
    
def test_puffernet_convolution_3d_layer(batch_size=4096, in_width=9, in_height=5, in_depth=5,
        in_channels=1, out_channels=16, kernel_size=2, stride=1):
    input_np = make_dummy_data(batch_size, in_channels, in_depth, in_height, in_width)
    weights_np = make_dummy_data(out_channels, in_channels, kernel_size, kernel_size, kernel_size)
    bias_np = make_dummy_data(out_channels)
    
    out_depth = int((in_depth - kernel_size)/stride + 1)
    out_height = int((in_height - kernel_size)/stride + 1)
    out_width = int((in_width - kernel_size)/stride + 1)
    output_puffer = np.zeros((batch_size, out_channels, out_depth, out_height, out_width), dtype=np.float32)
    
    puffernet.puf_convolution_3d_layer(input_np, weights_np, bias_np, output_puffer,
        batch_size, in_width, in_height, in_depth, in_channels, out_channels, kernel_size, stride)
    
    input_torch = torch.from_numpy(input_np)
    weights_torch = torch.from_numpy(weights_np)
    bias_torch = torch.from_numpy(bias_np)
    
    torch_conv = torch.nn.Conv3d(in_channels, out_channels, kernel_size, stride)
    torch_conv.weight.data = weights_torch
    torch_conv.bias.data = bias_torch
    output_torch = torch_conv(input_torch).detach()
    assert_near(output_puffer, output_torch.numpy())
    
    
    

def test_puffernet_lstm(batch_size=16, input_size=128, hidden_size=128):
    input_np = make_dummy_data(batch_size, input_size, seed=42)
    state_h_np = make_dummy_data(batch_size, hidden_size, seed=43)
    state_c_np = make_dummy_data(batch_size, hidden_size, seed=44)
    weights_input_np = make_dummy_data(4*hidden_size, input_size, seed=45)
    weights_state_np = make_dummy_data(4*hidden_size, hidden_size, seed=46)
    bias_input_np = make_dummy_data(4*hidden_size, seed=47)
    bias_state_np = make_dummy_data(4*hidden_size, seed=48)
    buffer_np = make_dummy_data(4*batch_size*hidden_size, seed=49)

    input_torch = torch.from_numpy(input_np).view(1, batch_size, input_size)
    state_h_torch = torch.from_numpy(state_h_np).view(1, batch_size, hidden_size)
    state_c_torch = torch.from_numpy(state_c_np).view(1, batch_size, hidden_size)
    weights_input_torch = torch.from_numpy(weights_input_np)
    weights_state_torch = torch.from_numpy(weights_state_np)
    bias_input_torch = torch.from_numpy(bias_input_np)
    bias_state_torch = torch.from_numpy(bias_state_np)
    torch_lstm = torch.nn.LSTM(input_size, hidden_size, num_layers=1)
    torch_lstm.weight_ih_l0.data = weights_input_torch
    torch_lstm.weight_hh_l0.data = weights_state_torch
    torch_lstm.bias_ih_l0.data = bias_input_torch
    torch_lstm.bias_hh_l0.data = bias_state_torch
    output_torch, (state_h_torch, state_c_torch) = torch_lstm(input_torch, (state_h_torch, state_c_torch))
    state_h_torch = state_h_torch.detach()
    state_c_torch = state_c_torch.detach()

    # PufferNet done second because it is in-place on the state vars
    puffernet.puf_lstm(input_np, state_h_np, state_c_np, weights_input_np,
        weights_state_np, bias_input_np, bias_state_np, buffer_np,
        batch_size, input_size, hidden_size)

    assert_near(state_h_np, state_h_torch.numpy()[0])
    assert_near(state_c_np, state_c_torch.numpy()[0])

def test_puffernet_embedding(batch_size=16, num_embeddings=128, embedding_dim=32):
    input_np = make_dummy_int_data(num_embeddings, batch_size, seed=42)
    weights_np = make_dummy_data(num_embeddings, embedding_dim, seed=43)
    output_puffer = np.zeros((batch_size, embedding_dim), dtype=np.float32)
    puffernet.puf_embedding(input_np, weights_np, output_puffer,
        batch_size, num_embeddings, embedding_dim)

    input_torch = torch.from_numpy(input_np).long()
    weights_torch = torch.from_numpy(weights_np)
    output_torch = torch.nn.functional.embedding(input_torch, weights_torch).detach()

    input_torch = torch.from_numpy(input_np).long()
    weights_torch = torch.from_numpy(weights_np)
    torch_embedding = torch.nn.Embedding(num_embeddings, embedding_dim)
    torch_embedding.weight.data = weights_torch
    output_torch = torch_embedding(input_torch).detach()

    assert_near(output_puffer, output_torch.numpy())

def test_puffernet_layernorm(batch_size=16, input_size=128):
    input_np = make_dummy_data(batch_size, input_size, seed=42)
    weights_np = make_dummy_data(input_size, seed=43)
    bias_np = make_dummy_data(input_size, seed=44)
    output_puffer = np.zeros((batch_size, input_size), dtype=np.float32)
    puffernet.puf_layernorm(input_np, weights_np, bias_np, output_puffer,
        batch_size, input_size)

    input_torch = torch.from_numpy(input_np)
    weights_torch = torch.from_numpy(weights_np)
    bias_torch = torch.from_numpy(bias_np)
    torch_layernorm = torch.nn.LayerNorm(input_size)
    torch_layernorm.weight.data = weights_torch
    torch_layernorm.bias.data = bias_torch
    output_torch = torch_layernorm(input_torch).detach()

    assert_near(output_puffer, output_torch.numpy())

def test_puffernet_one_hot(batch_size=16, input_size=128, num_classes=10):
    input_np = make_dummy_int_data(num_classes, batch_size, input_size)
    output_puffer = np.zeros((batch_size, input_size, num_classes), dtype=np.int32)
    puffernet.puf_one_hot(input_np, output_puffer, batch_size, input_size, num_classes)

    input_torch = torch.from_numpy(input_np).long()
    output_torch = torch.nn.functional.one_hot(input_torch, num_classes).int().detach()

    assert_near(output_puffer, output_torch.numpy())

def test_puffernet_cat_dim1(batch_size=16, x_size=32, y_size=64):
    x_np = make_dummy_data(batch_size, x_size)
    y_np = make_dummy_data(batch_size, y_size)
    output_puffer = np.zeros((batch_size, x_size + y_size), dtype=np.float32)
    puffernet.puf_cat_dim1(x_np, y_np, output_puffer, batch_size, x_size, y_size)

    x_torch = torch.from_numpy(x_np)
    y_torch = torch.from_numpy(y_np)
    output_torch = torch.cat([x_torch, y_torch], dim=1).detach()

    assert_near(output_puffer, output_torch.numpy())

def test_puffernet_argmax_multidiscrete(batch_size=16, logit_sizes=[5,7,2]):
    logit_sizes = np.array(logit_sizes).astype(np.int32)
    num_actions = len(logit_sizes)
    input_np = make_dummy_data(batch_size, logit_sizes.sum())
    output_puffer = np.zeros((batch_size, num_actions), dtype=np.int32)
    puffernet.puf_argmax_multidiscrete(input_np, output_puffer, batch_size, logit_sizes, num_actions)

    input_torch = torch.from_numpy(input_np)
    action_slices = torch.split(input_torch, logit_sizes.tolist(), dim=1)
    output_torch = torch.stack([torch.argmax(s, dim=1) for s in action_slices], dim=1).detach()

    assert_near(output_puffer, output_torch.numpy())

def test_nmmo3(batch_size=1, input_size=512, hidden_size=512):
    input_torch = torch.arange(11*15*10 + 47 + 10) % 4
    input_torch = input_torch.view(1, -1)

    from pufferlib.ocean.torch import NMMO3, NMMO3LSTM
    from pufferlib.ocean import env_creator
    env = env_creator('puffer_nmmo3')()
    model = NMMO3(env, hidden_size=hidden_size)
    model = NMMO3LSTM(env, policy=model, input_size=input_size, hidden_size=hidden_size)
    state_dict = torch.load('nmmo3_642b.pt')
    state_dict = {k.replace('module.', ''): v for k, v in state_dict.items()}
    model.load_state_dict(state_dict)

    state = {
        'lstm_h': torch.zeros(batch_size, hidden_size),
        'lstm_c': torch.zeros(batch_size, hidden_size),
    }

    output = model.forward_eval(input_torch, state)
    pass

if __name__ == '__main__':
    test_nmmo3()
    exit()
    test_puffernet_relu()
    test_puffernet_sigmoid()
    test_puffernet_linear_layer()
    test_puffernet_convolution_layer()
    test_puffernet_convolution_3d_layer()
    test_puffernet_lstm()
    test_puffernet_embedding()
    test_puffernet_layernorm()
    test_puffernet_one_hot()
    test_puffernet_cat_dim1()
    test_puffernet_argmax_multidiscrete()



================================================
FILE: tests/test_pytorch.py
================================================
from typing import Any, Dict, List, Tuple

import gymnasium as gym
import numpy as np
import torch
import pytest

import pufferlib
import pufferlib.emulation
from pufferlib.pytorch import NativeDType, nativize_dtype, nativize_tensor


# TODO: align=True for dtype
@pytest.mark.parametrize(
    "observation_dtype,emulated_dtype,expected",
    [
        (
            np.dtype((np.uint8, (4,)), align=True),
            np.dtype(
                [
                    ("x", np.uint8, (4,)),
                ],
                align=True,
            ),
            {"x": (torch.uint8, (4,), 0, 4)},
        ),
        (
            np.dtype((np.uint8, (4, 5)), align=True),
            np.dtype(
                [
                    ("x", np.uint8, (4, 5)),
                ],
                align=True,
            ),
            {"x": (torch.uint8, (4, 5), 0, 20)},
        ),
        (
            np.dtype((np.uint8, (4,)), align=True),
            np.dtype([("x", np.uint32, (1,))], align=True),
            {"x": (torch.uint32, (1,), 0, 4)},
        ),
        (
            np.dtype((np.uint8, (12,)), align=True),
            np.dtype([("foo", np.int32, (1,)), ("bar", np.int32, (2,))], align=True),
            {"foo": (torch.int32, (1,), 0, 4), "bar": (torch.int32, (2,), 4, 8)},
        ),
        (
            np.dtype((np.uint8, (16,)), align=True),
            np.dtype(
                [
                    ("foo", np.int32, (1,)),
                    ("bar", [("a", np.int32, (2,)), ("b", np.int32, (1,))]),
                ],
                align=True,
            ),
            {
                "foo": (torch.int32, (1,), 0, 4),
                "bar": {
                    "a": (torch.int32, (2,), 4, 8),
                    "b": (torch.int32, (1,), 12, 4),
                },
            },
        ),
        (
            np.dtype((np.float32, (4,)), align=True),
            np.dtype(
                [
                    ("foo", np.float32, (1,)),
                    ("bar", [("a", np.float32, (2,)), ("b", np.float32, (1,))]),
                ],
                align=True,
            ),
            {
                "foo": (torch.float32, (1,), 0, 1),
                "bar": {
                    "a": (torch.float32, (2,), 1, 2),
                    "b": (torch.float32, (1,), 3, 1),
                },
            },
        ),
        (
            np.dtype((np.int32, (4,)), align=True),
            np.dtype(
                [
                    ("foo", np.int32, (1,)),
                    (
                        "bar",
                        [
                            ("a", [("y", np.int32, (1,)), ("z", np.int32, (1,))]),
                            ("b", np.int32, (1,)),
                        ],
                    ),
                ],
                align=True,
            ),
            {
                "foo": (torch.int32, (1,), 0, 1),
                "bar": {
                    "a": {
                        "y": (torch.int32, (1,), 1, 1),
                        "z": (torch.int32, (1,), 2, 1),
                    },
                    "b": (torch.int32, (1,), 3, 1),
                },
            },
        ),
        (
            np.dtype((np.uint8, (84,)), align=True),
            np.dtype(
                [
                    ("xx", np.float32, (1, 2)),
                    ("yy", [("aa", np.uint8, (7, 7)), ("bb", np.int32, (2, 3))],),
                ],
                align=True,
            ),
            {
                "xx": (torch.float32, (1, 2), 0, 8),
                "yy": {
                    "aa": (torch.uint8, (7, 7), 8, 49),
                    "bb": (torch.int32, (2, 3), 60, 24),
                },
            },
        ),
    ],
)
def test_nativize_dtype(
    observation_dtype: np.array, emulated_dtype: np.array, expected: NativeDType
):
    assert expected == nativize_dtype(
        pufferlib.namespace(
            observation_dtype=observation_dtype,
            emulated_observation_dtype=emulated_dtype,
        )
    )


@pytest.mark.parametrize(
    "space,sample_dtype",
    [
        (
            gym.spaces.Dict(
                {
                    "x": gym.spaces.Box(-1.0, 1.0, (1, 2), dtype=np.float32),
                    "y": gym.spaces.Dict(
                        {
                            "a": gym.spaces.Box(0, 255, (7, 7), dtype=np.uint8),
                            "b": gym.spaces.Box(-1024, 1024, (2, 3), dtype=np.int32),
                        }
                    ),
                }
            ),
            np.dtype(np.uint8),
        ),
        (
            gym.spaces.Dict(
                {
                    "xx": gym.spaces.Box(-1.0, 1.0, (1, 2), dtype=np.float32),
                    "yy": gym.spaces.Box(-1.0, 1.0, (4, 5), dtype=np.float32),
                }
            ),
            np.dtype(np.float32),
        ),
        (
            gym.spaces.Dict(
                {
                    "screen": gym.spaces.Box(0, 255, (18, 20), dtype=np.uint8),
                }
            ),
            np.dtype(np.uint8),
        ),
    ],
)
def test_nativize_tensor(space: gym.spaces.Space, sample_dtype: np.dtype):
    emulated_dtype = pufferlib.emulation.dtype_from_space(space)
    observation_space, observation_dtype = (
        pufferlib.emulation.emulate_observation_space(space)
    )
    native_dtype = nativize_dtype(
        pufferlib.namespace(
            observation_dtype=sample_dtype,
            emulated_observation_dtype=emulated_dtype,
        )
    )
    flat = np.zeros(observation_space.shape, dtype=observation_space.dtype).view(
        observation_dtype
    )
    structured = space.sample()
    pufferlib.emulation.emulate(flat, structured)

    def flatten(inp: Any | Dict[str, Any]) -> List[Any | Tuple[str, Any]]:
        result = []

        for k, v in inp.items():
            if isinstance(v, dict):
                result.extend(flatten(v))
            elif isinstance(v, np.ndarray):
                result.append((k, v))
            elif isinstance(v, torch.Tensor):
                result.append((k, v.numpy()))
            else:
                raise
        return result

    observation = torch.tensor(flat.view(observation_space.dtype)).unsqueeze(0)
    nativized_tensor = nativize_tensor(observation, native_dtype)
    assert all(
        nx == ny and np.all(vx == vy)
        for (nx, vx), (ny, vy) in zip(flatten(nativized_tensor), flatten(structured))
    )
    explain_out = torch._dynamo.explain(nativize_tensor)(observation, native_dtype)
    assert len(explain_out.break_reasons) == 0



================================================
FILE: tests/test_record_array.py
================================================
import gymnasium as gym
import numpy as np

# Create a custom Gym space using Dict, Tuple, and Box
space = gym.spaces.Dict({
    "position": gym.spaces.Box(low=-1.0, high=1.0, shape=(2,), dtype=np.float32),
    "velocity": gym.spaces.Box(low=-1.0, high=1.0, shape=(2,), dtype=np.float32),
    "description": gym.spaces.Tuple((
        #gym.spaces.Discrete(10),
        gym.spaces.Box(low=0, high=100, shape=(), dtype=np.int32),
        gym.spaces.Box(low=0, high=100, shape=(), dtype=np.int32)
    ))
})

space = gym.spaces.Dict({
    "position": gym.spaces.Box(low=-1.0, high=1.0, shape=(2,), dtype=np.float32),
})


# Define a function to create a dtype from the Gym space
def create_dtype_from_space(space):
    if isinstance(space, gym.spaces.Dict):
        dtype_fields = [(name, create_dtype_from_space(subspace)) for name, subspace in space.spaces.items()]
        return np.dtype(dtype_fields)
    elif isinstance(space, gym.spaces.Tuple):
        dtype_fields = [('field' + str(i), create_dtype_from_space(subspace)) for i, subspace in enumerate(space.spaces)]
        return np.dtype(dtype_fields)
    elif isinstance(space, gym.spaces.Box):
        return (space.dtype, space.shape)
    elif isinstance(space, gym.spaces.Discrete):
        return np.int64  # Assuming np.int64 for Discrete spaces

# Compute the dtype from the space
space_dtype = create_dtype_from_space(space)

sample = dict(space.sample())
breakpoint()
np.rec.array(sample, dtype=space_dtype)

# Function to sample from the space and convert to a structured numpy array
def sample_and_convert(space, dtype):
    sample = space.sample()
    flat_sample = {}
    def flatten(sample, name_prefix=""):
        for key, item in sample.items():
            full_key = name_prefix + key if name_prefix == "" else name_prefix + "_" + key
            if isinstance(item, dict):
                flatten(item, full_key)
            else:
                flat_sample[full_key] = item
    flatten(sample)
    return np.array(tuple(flat_sample.values()), dtype=dtype)

num_samples = 3
samples = [sample_and_convert(space, space_dtype) for _ in range(num_samples)]
print("Samples:", samples)

record_array = np.rec.array(samples)
print("Record Array:", record_array)

bytes_array = record_array.tobytes()
print("Bytes Array:", bytes_array)

record_array = np.rec.array(bytes_array, dtype=space_dtype)
print("Record Array from Bytes:", record_array)



================================================
FILE: tests/test_record_emulation.py
================================================
import pufferlib.emulation

from pufferlib.environments.ocean import env_creator

env = env_creator('spaces')()
env.reset()
env.step([1,0])
breakpoint()



================================================
FILE: tests/test_registry.sh
================================================
#!/bin/bash

# Loop through all folders in the `registry` directory
for folder in pufferlib/registry/*; do
  if [ -d "$folder" ]; then
    # Extract folder name
    folder_name=$(basename $folder)

    if [[ $folder_name == __* ]]; then
      continue
    fi
   
    # Install package with extras
    pip install -e .[$folder_name] > /dev/null 2>&1
    
    # Run tests
    python tests/test_registry.py $folder_name
  fi
done



================================================
FILE: tests/test_render.py
================================================
from pdb import set_trace as T

import argparse
import importlib
import time

import cv2


# Tested human: classic_control, atari, minigrid
# Tested rbg_array: atari, pokemon_red, crafter
# Tested ansii: minihack, nethack, squared
if __name__ == '__main__':
    parser = argparse.ArgumentParser()
    parser.add_argument('--env', type=str, default='atari')
    parser.add_argument('--render-mode', type=str, default='rgb_array')
    args = parser.parse_args()

    env_module = importlib.import_module(f'pufferlib.environments.{args.env}')

    if args.render_mode == 'human':
        env = env_module.env_creator()(render_mode='human')
    else:
        env = env_module.env_creator()()

    terminal = True
    while True:
        start = time.time()
        if terminal or truncated:
            ob, _ = env.reset()

        if args.render_mode == 'rgb_array':
            frame = env.render()
            frame = cv2.cvtColor(frame, cv2.COLOR_RGB2BGR)
            #if ob.shape[0] in (1, 3, 4):
            #    ob = ob.transpose(1, 2, 0)
            cv2.imshow('frame', frame)

            #cv2.imshow('ob', ob)
            cv2.waitKey(1)
        elif args.render_mode == 'ansi':
            chars = env.render()
            print("\033c", end="")
            print(chars)

        ob = ob.reshape(1, *ob.shape)
        action = env.action_space.sample()
        ob, reward, terminal, truncated, info = env.step(action)
        env.render()
        start = time.time()
        if time.time() - start < 1/60:
            time.sleep(1/60 - (time.time() - start))
           



================================================
FILE: tests/test_rich.py
================================================
import psutil
import GPUtil
import time
import sys

import rich
from rich.console import Console
from rich.layout import Layout
from rich.live import Live
from rich.table import Table
from rich.panel import Panel
from rich.progress import Progress, BarColumn, TextColumn, MofNCompleteColumn

#import pufferlib

ROUND_OPEN = rich.box.Box(
    "╭──╮\n"
    "│  │\n"
    "│  │\n"
    "│  │\n"
    "│  │\n"
    "│  │\n"
    "│  │\n"
    "╰──╯\n"
)

c1 = '[bright_cyan]'
c2 = '[white]'
c3 = '[cyan]'
b1 = '[bright_cyan]'
b2 = '[bright_white]'

def abbreviate(num):
    if num < 1e3:
        return f"{num:.0f}"
    elif num < 1e6:
        return f"{num/1e3:.1f}k"
    elif num < 1e9:
        return f"{num/1e6:.1f}m"
    elif num < 1e12:
        return f"{num/1e9:.1f}b"
    else:
        return f"{num/1e12:.1f}t"

def duration(seconds):
    h = seconds // 3600
    m = (seconds % 3600) // 60
    s = seconds % 60
    return f"{h}h {m}m {s}s" if h else f"{m}m {s}s" if m else f"{s}s"

def print_dashboard(performance_data, loss_data, user_data, min_interval=0.25, last_print=[0]):
    console = Console()

    util = Table(box=None, expand=True, show_header=False)
    cpu_percent = psutil.cpu_percent()
    dram_percent = psutil.virtual_memory().percent
    gpus = GPUtil.getGPUs()
    gpu_percent = gpus[0].load * 100 if gpus else 0
    vram_percent = gpus[0].memoryUtil * 100 if gpus else 0
    util.add_column(justify="left")
    util.add_column(justify="center")
    util.add_column(justify="center")
    util.add_column(justify="center")
    util.add_column(justify="right")
    util.add_row(
        f':blowfish: {c1}PufferLib {b2}1.0.0',
        f'{c1}CPU: {c3}{cpu_percent:.1f}%',
        f'{c1}GPU: {c3}{gpu_percent:.1f}%',
        f'{c1}DRAM: {c3}{dram_percent:.1f}%',
        f'{c1}VRAM: {c3}{vram_percent:.1f}%',
    )
        
    summary= Table(box=None, expand=True)
    summary.add_column(f"{c1}Summary", justify='left', vertical='top')
    summary.add_column(f"{c1}Value", justify='right', vertical='top')
    summary.add_row(f'{c2}Epoch', f'{b2}{performance.epoch}')
    summary.add_row(f'{c2}Uptime', f'{b2}{duration(performance.uptime)}')
    estimated_time = performance.total_steps / performance.sps
    summary.add_row(f'{c2}Estim', f'{b2}{duration(estimated_time)}')
    summary.add_row(f'{c2}Agent Steps', f'{b2}{abbreviate(performance.agent_steps)}')
    summary.add_row(f'{c2}Steps/sec', f'{b2}{abbreviate(performance.sps)}')
    summary.add_row(f'{c2}sec/Batch', f'{b2}{performance.epoch_time:.2f}')
   
    perf = Table(box=None, expand=True)
    perf.add_column(f"{c1}Performance", justify="left", ratio=1.0)
    perf.add_column(f"{c1}Time", justify="right", ratio=0.5)
    perf.add_row(f'{c2}Training', f'{b2}{performance.epoch_train_time:.2f}')
    perf.add_row(f'{c2}Evaluation', f'{b2}{performance.epoch_eval_time:.2f}')
    perf.add_row(f'{c2}Environment', f'{b2}{performance.epoch_env_time:.2f}')
    perf.add_row(f'{c2}Forward', f'{b2}{performance.epoch_forward_time:.2f}')
    perf.add_row(f'{c2}Misc', f'{b2}{performance.epoch_misc_time:.2f}')
    perf.add_row(f'{c2}Allocation', f'{b2}{performance.epoch_alloc_time:.2f}')
    perf.add_row(f'{c2}Backward', f'{b2}{performance.epoch_backward_time:.2f}')

    losses = Table(box=None, expand=True)
    losses.add_column(f'{c1}Losses', justify="left", ratio=1.0)
    losses.add_column(f'{c1}Value', justify="right", ratio=0.5)
    for metric, value in loss_data.items():
        losses.add_row(f'{c2}{metric}', f'{b2}{value}')

    monitor = Table(box=None, expand=True, pad_edge=False)
    monitor.add_row(summary, perf, losses)

    user = Table(box=None, expand=True, pad_edge=False)
    user1 = Table(box=None, expand=True)
    user2 = Table(box=None, expand=True)
    user.add_row(user1, user2)
    user1.add_column(f"{c1}User Stats", justify="left", ratio=1.0)
    user1.add_column(f"{c1}Value", justify="right",ratio=1.0)
    user2.add_column(f"{c1}User Stats", justify="left", ratio=1.0)
    user2.add_column(f"{c1}Value", justify="right",ratio=1.0)
    i = 0
    for metric, value in user_data.items():
        u = user1 if i % 2 == 0 else user2
        u.add_row(f'{c2}{metric}', f'{b2}{value}')
        i += 1

    table = Table(box=ROUND_OPEN, expand=True, show_header=False, width=80, border_style='bright_cyan')
    table.add_row(util)
    table.add_row(monitor)
    table.add_row(user)
    console.print(table)


class Dashboard:
    def __init__(self):
        self.console = Console()
        self.rich = rich

        layout = Layout()
        layout.split(
            Layout(name="utilization", size=5),
            Layout(name="monitoring"),
        )
 
        self.layout = layout
        '''
        layout.split(
            Layout(name="utilization", size=5),
            Layout(name="puffer", size=2),
            Layout(name="monitor", size=12),
            Layout(name="user")
        )
        layout["monitor"].split_row(
            Layout(name="performance"),
            Layout(name="losses")
        )
        layout["user"].split_row(
            Layout(name="user_stats")
        )
        '''

        utilization = Progress(
            BarColumn(bar_width=None, style="bar.back", complete_style="bar.complete"),
            TextColumn("[progress.description]{task.description}"),
            MofNCompleteColumn(),
            expand=True
        )
        self.cpu_task = utilization.add_task("[cyan]CPU", total=100)
        self.gpu_task = utilization.add_task("[red]GPU", total=100)
        self.dram_task = utilization.add_task("[blue]DRAM", total=100)
        self.vram_task = utilization.add_task("[magenta]VRAM", total=100)
        self.layout["utilization"].update(utilization)
        self.utilization = utilization

        #self.live = Live(self.layout, console=self.console)#, auto_refresh=4)
        #self.live.start()

    def _update_utilization(self):
        self.utilization.update(self.cpu_task, completed=psutil.cpu_percent())
        self.utilization.update(self.dram_task, completed=psutil.virtual_memory().percent)
        gpus = GPUtil.getGPUs()
        if gpus:
            self.utilization.update(self.gpu_task, completed=gpus[0].load * 100)
            self.utilization.update(self.vram_task, completed=gpus[0].memoryUtil * 100)
        else:
            self.utilization.update(self.gpu_task, completed=0)
            self.utilization.update(self.vram_task, completed=0)

        #self.layout['puffer'].update(f':blowfish: PufferLib {pufferlib.__version__}')
        #self.layout['puffer'].update(f':blowfish: PufferLib 1.0.0')

    def update(self, total_uptime, estimated_time, total_steps, steps_per_second, performance_data, loss_data, user_data):
        topline = self.update_topline(total_uptime, estimated_time, total_steps, steps_per_second)
        performance = self.update_performance(performance_data)
        losses = self.update_losses(loss_data)
        user = self.update_user_stats(user_data)

        megatable = Table(box=ROUND_OPEN, expand=True, show_header=False)
        megatable.add_row(topline)
        megatable.add_row('')
        perf = Table(box=None, expand=True)
        perf.add_column(performance, ratio=1.0)
        perf.add_column(losses, ratio=1.0)
        #megatable.add_row(performance)
        #megatable.add_row(losses)
        megatable.add_row(perf)
        megatable.add_row('')
        megatable.add_row(user)
        self.layout["monitoring"].update(megatable)
        self.console.clear()
        self.console.print(self.layout) 


    def update_topline(self, total_uptime, estimated_time, total_steps, steps_per_second):
        table = Table(box=None, expand=True)
        table.add_column(justify="left")
        table.add_column(justify="center")
        table.add_column(justify="right")
        table.add_row(
            f':blowfish: PufferLib 1.0.0',
            f'[bold magenta]Uptime: [cyan]{total_uptime}/{estimated_time}(est)',
            f'[bold magenta]Agent Steps: [cyan]{total_steps} ({steps_per_second}/s)'
        )
        return table

    def update_performance(self, data):
        table = Table(box=None, expand=True)
        #self.layout["performance"].update(table)
        table.add_column("[bold magenta]Performance", justify="right", ratio=1.0)
        table.add_column("Latency", justify="left", style="cyan", ratio=1.0)
        for metric, value in data.items():
            table.add_row(metric, str(value))

        return table
        self.console.clear()
        self.console.print(self.layout) 

    def update_losses(self, data):
        table = Table(box=None, expand=True)
        #self.layout["losses"].update(table)
        table.add_column("[bold magenta]Losses", justify="right", ratio=1.0)
        table.add_column("Value", justify="left", style="bright_cyan", ratio=1.0)
        for metric, value in data.items():
            table.add_row(metric, str(value))

        table.add_row("")

        return table
        self.console.clear()
        self.console.print(self.layout) 

    def update_user_stats(self, data):
        table = Table(box=None, expand=True)
        table.add_column("[bold magenta]User Stats", justify="right", style="bold yellow", ratio=1.0)
        table.add_column("Value", justify="left",ratio=1.0)
        #self.layout["user_stats"].update(table)
        for metric, value in data.items():
            table.add_row(metric, str(value))

        return table
        self.console.clear()
        self.console.print(self.layout) 


#dashboard = Dashboard()

# Update loop
try:
    while True:
        #dashboard._update_utilization()
        topline = (5000, 100000, 102332, 1038, 1.3)
        performance = {
            'training': 0.7,
            'evaluation': 0.6,
            'environment': 0.2,
            'forward': 0.3,
            'misc': 0.1,
            'allocation': 0.2,
            'backward': 0.3,
        }
        losses = {
            'policy': 0.4,
            'value': 0.2,
            'entropy': 0.1,
            'old_approx_kl': 0.1,
            'approx_kl': 0.2,
            'clip_fraction': 0.1,
            'explained_variance': 0.3,
        }
        user_stats = {
            'time_alive': 128,
            'exploration': 0.1,
            'experience': 1000,
        }
        #dashboard.update(*topline, performance, losses, user_stats)
        print_dashboard(*topline, performance, losses, user_stats)
        time.sleep(1)
except KeyboardInterrupt:
    dashboard.stop()




================================================
FILE: tests/test_sweep.py
================================================
import time
import random
import argparse
import configparser
import ast

import numpy as np
import torch

from rich_argparse import RichHelpFormatter
from rich.console import Console
from rich.traceback import install
install(show_locals=False) # Rich tracebacks

import pufferlib
import pufferlib.sweep

from bokeh.models import ColumnDataSource, LinearColorMapper
from bokeh.plotting import figure, show
from bokeh.palettes import Turbo256


def synthetic_basic_task(args):
    train_args = args['train']
    learning_rate = train_args['learning_rate']
    total_timesteps = train_args['total_timesteps']
    score = np.exp(-(np.log10(learning_rate) + 3)**2)
    cost = total_timesteps / 50_000_000
    return score, cost

def synthetic_linear_task(args):
    score, cost = synthetic_basic_task(args)
    return score*cost, cost

def synthetic_log_task(args):
    score, cost = synthetic_basic_task(args)
    noise_cost = cost + 0.20*np.random.randn()*cost
    noise_cost = min(noise_cost, 200)
    noise_cost = max(noise_cost, 1)
    return score*np.log10(noise_cost), cost

def synthetic_percentile_task(args):
    score, cost = synthetic_basic_task(args)
    noise_cost = cost - 0.20*abs(np.random.randn())*cost
    noise_cost = min(noise_cost, 200)
    noise_cost = max(noise_cost, 1)
    return score/(1 + np.exp(-noise_cost/10)), cost

def synthetic_cutoff_task(args):
    score, cost = synthetic_basic_task(args)
    return score*min(2, np.log10(cost)), cost

def test_sweep(args):
    method = args['sweep']['method']
    if method == 'Random':
        sweep = pufferlib.sweep.Random(args['sweep'])
    elif method == 'ParetoGenetic':
        sweep = pufferlib.sweep.ParetoGenetic(args['sweep'])
    elif method == 'Protein':
        sweep = pufferlib.sweep.Protein(
            args['sweep'],
            expansion_rate = 1.0,
        )
    else:
        raise ValueError(f'Invalid sweep method {method} (random/pareto_genetic/protein)')

    task = args['task']
    if task == 'linear':
        synthetic_task = synthetic_linear_task
    elif task == 'log':
        synthetic_task = synthetic_log_task
    elif task == 'percentile':
        synthetic_task = synthetic_percentile_task
    else:
        raise ValueError(f'Invalid task {task}')

    target_metric = args['sweep']['metric']
    scores, costs = [], []
    for i in range(args['max_runs']):
        seed = time.time_ns() & 0xFFFFFFFF
        random.seed(seed)
        np.random.seed(seed)
        torch.manual_seed(seed)
 
        try:
            _, info = sweep.suggest(args)
        except:
            break

        total_timesteps = args['train']['total_timesteps']
        for i in range(1, 6):
            args['train']['total_timesteps'] = i*total_timesteps/5
            score, cost = synthetic_task(args)
            sweep.observe(args, score, cost)
            print('Score:', score, 'Cost:', cost)

        scores.append(score)
        costs.append(cost)

    pareto, pareto_idx = pufferlib.sweep.pareto_points(sweep.success_observations)

    np.save(args['data_path']+'.npy', {'scores': scores, 'costs': costs})

    #pareto_scores = np.array(scores)[pareto_idx].tolist()
    #pareto_costs = np.array(costs)[pareto_idx].tolist()
    #np.save(args['data_path']+'_pareto.npy', {'scores': pareto_scores, 'costs': pareto_costs})

def visualize(args):
    data = np.load(args['vis_path'] + '.npy', allow_pickle=True).item()
    costs = data['costs']
    scores = data['scores']

    sorted_costs = np.sort(costs)
    aoc = np.max(scores) * np.cumsum(sorted_costs) / np.sum(costs)

    # Create a ColumnDataSource that includes the 'order' for each point
    source = ColumnDataSource(data=dict(
        x=costs,
        y=scores,
        order=list(range(len(scores)))  # index/order for each point
    ))

    curve = ColumnDataSource(data=dict(
        x=sorted_costs,
        y=aoc,
        order=list(range(len(scores)))  # index/order for each point
    ))

    # Define a color mapper across the range of point indices
    mapper = LinearColorMapper(
        palette=Turbo256,
        low=0,
        high=len(scores)
    )

    # Set up the figure
    p = figure(title='Synthetic Hyperparam Test', 
               x_axis_label='Cost', 
               y_axis_label='Score')

    # Use the 'order' field for color -> mapped by 'mapper'
    p.scatter(x='x', 
              y='y', 
              color={'field': 'order', 'transform': mapper}, 
              size=10, 
              source=source)

    p.line(x='x', 
           y='y', 
           color='purple',
           source=curve)

    show(p)


if __name__ == '__main__':
    parser = argparse.ArgumentParser(
        description=f':blowfish: PufferLib [bright_cyan]{pufferlib.__version__}[/]'
        ' demo options. Shows valid args for your env and policy',
        formatter_class=RichHelpFormatter, add_help=False)
    parser.add_argument('--task', type=str, default='linear', help='Task to optimize')
    parser.add_argument('--vis-path', type=str, default='',
        help='Set to visualize a saved sweep')
    parser.add_argument('--data-path', type=str, default='sweep',
        help='Used for testing hparam algorithms')
    parser.add_argument('--max-runs', type=int, default=100, help='Max number of sweep runs')
    parser.add_argument('--tag', type=str, default=None, help='Tag for experiment')
    parser.add_argument('--wandb', action='store_true', help='Track on WandB')
    parser.add_argument('--neptune', action='store_true', help='Track on Neptune')
    args = parser.parse_known_args()[0]

    p = configparser.ConfigParser()
    p.read('config/default.ini')
    for section in p.sections():
        for key in p[section]:
            argparse_key = f'--{section}.{key}'.replace('_', '-')
            parser.add_argument(argparse_key, default=p[section][key])

    # Late add help so you get a dynamic menu based on the env
    parser.add_argument('-h', '--help', default=argparse.SUPPRESS,
        action='help', help='Show this help message and exit')

    parsed = parser.parse_args().__dict__
    args = {'env': {}, 'policy': {}, 'rnn': {}}
    for key, value in parsed.items():
        next = args
        for subkey in key.split('.'):
            if subkey not in next:
                next[subkey] = {}
            prev = next
            next = next[subkey]
        try:
            prev[subkey] = ast.literal_eval(value)
        except:
            prev[subkey] = value

    if args['vis_path']:
        visualize(args)
        exit(0)

    test_sweep(args)



================================================
FILE: tests/test_utils.py
================================================
import sys
import gym

import pufferlib
import pufferlib.utils

def test_suppress():
    with pufferlib.utils.Suppress():
        gym.make('Breakout-v4')
        print('stdout (you should not see this)', file=sys.stdout)
        print('stderr (you should not see this)', file=sys.stderr)

if __name__ == '__main__':
    test_suppress()


================================================
FILE: tests/time_alloc.py
================================================
import numpy as np
import timeit

# Time np.zeros(2, 5) for 100000 iterations
time_zeros = timeit.timeit('np.zeros((2, 5))', setup='import numpy as np', number=100000)

# Pre-allocate the array
preallocated_array = np.zeros((2, 5))

# Time setting the pre-allocated array to zero for 100000 iterations
time_preallocated = timeit.timeit('preallocated_array[:] = 0', setup='import numpy as np; preallocated_array = np.zeros((2, 5))', number=100000)

print(f"Time for np.zeros(2, 5) over 100000 iterations: {time_zeros} seconds")
print(f"Time for preallocated *= 0 over 100000 iterations: {time_preallocated} seconds")




================================================
FILE: tests/pool/envpool_results.npy
================================================
[Binary file]


================================================
FILE: tests/pool/plot_packing.py
================================================
import plotly.graph_objects as go
import numpy as np

# Parameters
n_bars = 24
mu = 0.002
std = 0.002

background = '#061a1a'
forground = '#f1f1f1'

# Sampling from the normal distribution
bar_heights = mu + np.clip(np.random.normal(mu, std, n_bars), 0, np.inf)

# Creating the bar chart
fig = go.Figure(go.Bar(
    x=[i for i in range(n_bars)],
    y=bar_heights,
    marker_line_width=0,
    marker_color=forground,
))

# Updating the layout
fig.update_layout({
    'plot_bgcolor': background,
    'paper_bgcolor': background,
    'showlegend': False,
    'xaxis': {'visible': False},
    'yaxis': {'visible': False, 'range': [0, max(bar_heights)]},
    'margin': {'l': 0, 'r': 0, 't': 0, 'b': 0},
    'height': 400,
    'width': 800,
    'bargap': 0.0,
    'bargroupgap': 0.0,
})


fig.show()
fig.write_image('../docker/env_variance.png', scale=3)



================================================
FILE: tests/pool/test_basic_multprocessing.py
================================================
from pdb import set_trace as T
import numpy as np
import time

import selectors
from multiprocessing import Process, Pipe

def worker_process(envs_per_worker, delay_mean, delay_std, send_pipe, recv_pipe):
    while True:
        request = recv_pipe.recv()
        for _ in range(envs_per_worker):
            start = time.process_time()
            idx = 0
            target_time = delay_mean + delay_std*np.random.randn()
            while time.process_time() - start < target_time:
                idx += 1

        send_pipe.send('end')

def test_speed(envs_per_worker=1, delay_mean=0.01, delay_std=0.001, num_workers=4, batch_size=4, sync=True, timeout=10):
    main_send_pipes, work_recv_pipes = zip(*[Pipe() for _ in range(num_workers)])
    work_send_pipes, main_recv_pipes = zip(*[Pipe() for _ in range(num_workers)])

    processes = [Process(
        target=worker_process,
        args=(envs_per_worker, delay_mean, delay_std, work_send_pipes[i], work_recv_pipes[i]))
        for i in range(num_workers)]

    for p in processes:
        p.start()
 
    send_idxs = {i for i in range(num_workers)}

    # Register all receive pipes with the selector
    sel = selectors.DefaultSelector()
    for pipe in main_recv_pipes:
        sel.register(pipe, selectors.EVENT_READ)

    steps_collected = 0
    start = time.time()
    while time.time() - start < timeout:
        for idx in send_idxs:
            main_send_pipes[idx].send('start')

        send_idxs = set()

        if sync:
            for idx, pipe in enumerate(main_recv_pipes):
                assert pipe.recv() == 'end'
                send_idxs.add(idx)

            steps_collected += num_workers*envs_per_worker
        else:
            for key, _ in sel.select(timeout=None):
                pipe = key.fileobj
                idx = main_recv_pipes.index(pipe)

                if pipe.poll():
                    assert pipe.recv() == 'end'
                    send_idxs.add(idx)

                if len(send_idxs) == batch_size:
                    break

            steps_collected += batch_size*envs_per_worker

    end = time.time()

    for p in processes:
        p.terminate()

    sps = steps_collected / (end - start)
    print(
        f'SPS: {sps:.2f}',
        f'envs_per_worker: {envs_per_worker}',
        f'delay_mean: {delay_mean}',
        f'delay_std: {delay_std}',
        f'num_workers: {num_workers}',
        f'batch_size: {batch_size}',
        f'sync: {sync}',
    )


if __name__ == '__main__':
    #timeout = 1
    #test_speed(timeout=1)
    test_speed(delay_mean=0, delay_std=0, num_workers=1, batch_size=1, sync=False)
    test_speed(delay_mean=0, delay_std=0, num_workers=1, batch_size=1, sync=True)
    test_speed(delay_mean=0, delay_std=0, num_workers=6, batch_size=6, sync=False)
    test_speed(delay_mean=0, delay_std=0, num_workers=6, batch_size=6, sync=True)
    test_speed(delay_mean=0, delay_std=0, num_workers=24, batch_size=6, sync=False)
    test_speed(delay_mean=0, delay_std=0, num_workers=24, batch_size=24, sync=False)
    test_speed(delay_mean=0, delay_std=0, num_workers=24, batch_size=6, sync=True)




================================================
FILE: tests/pool/test_envpool.py
================================================
from pdb import set_trace as T
import numpy as np
import time

import gymnasium

import pufferlib
from pufferlib.vectorization import Serial, Multiprocessing, Ray


# This is about 1 second on a good CPU core. It is quite difficult to
# find good sources of a 1 second delay without using a timer that can swap
# on sleep
WORK_ITERATIONS = 150_000_000


class PerformanceEnv:
    def __init__(self, delay_mean, delay_std):
        np.random.seed(time.time_ns() % 2**32)

        self.observation_space = gymnasium.spaces.Box(
            low=-2**20, high=2**20,
            shape=(1,), dtype=np.float32
        )
        self.action_space = gymnasium.spaces.Discrete(2)
        self.observation = self.observation_space.sample()

        self.delay_mean = delay_mean
        self.delay_std = delay_std


    def reset(self, seed=None):
        return self.observation, {}

    def step(self, action):
        start = time.process_time()
        idx = 0
        target_time = self.delay_mean + self.delay_std*np.random.randn()
        while time.process_time() - start < target_time:
            idx += 1

        return self.observation, 0, False, False, {}

    def close(self):
        pass


def test_performance(vectorization, workers, envs_per_worker,
        delay_mean, delay_std, batch_size=None, timeout=1):
    def make_env():
        return pufferlib.emulation.GymnasiumPufferEnv(
            env_creator=PerformanceEnv, env_args=(delay_mean, delay_std))

    if batch_size is None:
        batch_size = workers * envs_per_worker

    actions = np.array([make_env().action_space.sample() for _ in range(batch_size)])

    if vectorization in (Serial, Multiprocessing, 'SyncMultiprocessing', 'SyncRay', Ray):
        synchronous = False
        if vectorization == 'SyncMultiprocessing':
            vectorization = Multiprocessing
            synchronous = True
        if vectorization == 'SyncRay':
            vectorization = Ray
            synchronous = True

        envs = vectorization(
            make_env,
            num_workers=workers,
            envs_per_worker=envs_per_worker,
            batch_size=batch_size,
            synchronous=synchronous,
        )
    else:
        envs = vectorization([make_env for _ in range(workers)])

    envs.reset()
    num_steps = 0
    start = time.time()
    while time.time() - start < timeout:
        obs = envs.step(actions)[0]
        num_steps += obs.shape[0]

    end = time.time()
    envs.close()

    return num_steps, end - start


def sweep_performance_tests():
    backends = (
        gymnasium.vector.SyncVectorEnv, Serial,
        gymnasium.vector.AsyncVectorEnv, 'SyncMultiprocessing',
        Multiprocessing, 
        'SyncRay', Ray,
    )
    results = {}
    delay_means = (1e-2, 1e-2, 1e-3, 1e-3, 1e-4, 1e-4)
    delay_stds = (1e-3, 1e-2, 1e-4, 1e-3, 1e-5, 1e-4)
    for mean, std in zip(delay_means, delay_stds):
        results[(mean, std)] = {}
        print('Environment delay: ', mean, std)
        for workers in (1, 6, 24, 96, 192):
            resul = {}
            results[(mean, std)][workers] = resul
            print('\t', workers)
            for vec in backends:
                res = {}
                if type(vec) != str:
                    name = vec.__name__
                else:
                    name = vec

                resul[name] = res
                print(2*'\t', name)

                for envs_per_worker in (1, 2, 4):
                    batch_sizes=[workers * envs_per_worker]
                    if vec in (Multiprocessing, Ray) and workers != 1:
                        batch_sizes.append(workers * envs_per_worker // 2)
                        batch_sizes.append(workers * envs_per_worker // 3)

                    for batch in batch_sizes:
                        steps, duration = test_performance(
                                vec, workers, envs_per_worker, mean, std, batch)

                        res[(envs_per_worker, batch)] = (steps, duration)

                        print('SPS, envs/worker, batch size: ',
                              steps / duration, envs_per_worker, batch)

    #np.save('envpool_results.npy', results, allow_pickle=True)

def plot_performance_tests():
    data = np.load('envpool_results.npy', allow_pickle=True).item()
    n_envs = len(data)

    inner_data = list(data.items())[0][1]
    n_cores, cores = len(inner_data), list(inner_data.keys())

    inner_inner_data = list(inner_data.items())[0][1]
    n_backends, backends = len(inner_inner_data), list(inner_inner_data.keys())

    from matplotlib import pyplot as plt
    import matplotlib.colors as mcolors

    # Create figure and axes
    fig, ax = plt.subplots(figsize=(15, 5))  # Adjust size as needed
    #plt.yscale('log')

    # Bar settings
    bar_width = 0.15
    group_width = n_backends * bar_width * n_cores
    index = np.arange(n_envs) * (group_width + bar_width * 2)  # Adding more space between environments

    # Grayscale colors for backends
    grayscale_colors = np.linspace(0.4, 1, n_backends)
    backend_colors = [str(g) for g in grayscale_colors]

    # Hue colors for cores
    hue_colors = 255*plt.cm.hsv(np.linspace(0, 0.6, n_cores))[:, :3]
    bars_data = []

    grayscale_colors = np.linspace(0.4, 1, n_cores)
    hue_colors = 255*plt.cm.hsv(np.linspace(0, 0.6, n_backends))[:, :3]

    import plotly.graph_objects as go
    import dash
    import dash_core_components as dcc
    import dash_html_components as html

    # Plotting the bars
    pos = 0

    x_labels = [f'{mean}±{std}' for mean, std in data.keys()]
    tick_vals = np.linspace(0, bar_width*n_envs*n_cores*(n_backends+1), n_envs)

    # Set up layout configuration
    layout = go.Layout(
        title=dict(
            text='Performance of Vectorization Backends on Various Workloads (24 core machine)',
            y=0.9
        ),
        width=2000,# 1000,
        height=500,
        yaxis=dict(title='Speedup over Expected Serial Performance'),
        plot_bgcolor='rgba(6, 26, 26, 1)',  # Dark cyan background
        paper_bgcolor='rgba(6, 26, 26, 1)',
        font=dict(color='rgba(241, 241, 241, 1)'),  # Light text
        barmode='group',
        xaxis = dict(
            title='Test Environment Delays (mean/std) and Process Counts',
            tickmode='array',
            tickvals = tick_vals,
            ticktext = x_labels,
        ),
        legend=dict(
            y=1.20,
            x=0.9,#0.80
        ),
    )

    fig = go.Figure(data=bars_data, layout=layout)
    x = 0
    for env_idx, (mean, std) in enumerate(data):
        env = data[(mean, std)]
        label = ('mean = %.1e, std = %.1e' % (mean, std))
        for workers_idx, workers in enumerate(env):
            runs = env[workers]
            for vec_idx, vec in enumerate(runs):
                results = runs[vec].values()
                best_sps = max(steps / duration for steps, duration in results)
                speedup = best_sps * mean

                color = hue_colors[vec_idx] * grayscale_colors[workers_idx]
                color = f'rgb{tuple(color[:3])}'  # Convert to RGB string
                fig.add_trace(go.Bar(
                    x=[x],
                    y=[speedup],  # Y value
                    marker_color=color,  # Color
                    text=label,
                    showlegend=False,
                ))
                x += bar_width
                label = ''
            x += bar_width
        x += 3*bar_width

    # Create figure with the collected bar data and layout
    for idx, vec in enumerate(backends):
        if vec == 'Serial':
            vec = 'Puffer Serial'
        elif vec == 'SyncMultiprocessing':
            vec = 'Puffer Multiproc.'
        elif vec == 'Multiprocessing':
            vec = 'Puffer Pool'

        color = f'rgb{tuple(hue_colors[idx])}'  # Convert to RGB string
        fig.add_trace(go.Bar(
            x=[None],  # No x value
            y=[None],  # No y value
            name=vec,  # Name for the legend entry
            marker_color=color,  # Transparent color
            showlegend=True,  # Show in legend
        ))

    for idx, core in enumerate(cores):
        color = f'rgb{tuple(3*[grayscale_colors[idx]])}'
        fig.add_trace(go.Bar(
            x=[None],  # No x value
            y=[None],  # No y value
            name=core,  # Name for the legend entry
            marker_color=color,  # Transparent color
            showlegend=True,  # Show in legend
        ))

    # Save the figure to a file
    fig.write_image('../docker/envpool_sps.png', scale=3)


if __name__ == '__main__':
    #sweep_performance_tests()
    plot_performance_tests()



================================================
FILE: tests/pool/test_multiprocessing.py
================================================
from pdb import set_trace as T
import numpy as np
import time

from pufferlib.vectorization import Multiprocessing
from pufferlib.environments import pokemon_red

def test_envpool(num_envs, envs_per_worker, envs_per_batch, steps=1000, env_pool=True):
    pool = Multiprocessing(pokemon_red.env_creator(), num_envs=num_envs,
        envs_per_worker=envs_per_worker, envs_per_batch=envs_per_batch,
        env_pool=True,
    )
    pool.async_reset()

    a = np.array([pool.single_action_space.sample() for _ in range(envs_per_batch)])
    start = time.time()
    for s in range(steps):
        o, r, d, t, i, mask, env_id = pool.recv()
        pool.send(a)
    end = time.time()
    print('Steps per second: ', envs_per_batch * steps / (end - start))
    pool.close()


if __name__ == '__main__':
    # 225 sps
    #test_envpool(num_envs=1, envs_per_worker=1, envs_per_batch=1, env_pool=False)

    # 600 sps
    #test_envpool(num_envs=6, envs_per_worker=1, envs_per_batch=6, env_pool=False)

    # 645 sps
    #test_envpool(num_envs=24, envs_per_worker=4, envs_per_batch=24, env_pool=False)

    # 755 sps 
    # test_envpool(num_envs=24, envs_per_worker=4, envs_per_batch=24)

    # 1050 sps
    # test_envpool(num_envs=48, envs_per_worker=4, envs_per_batch=24)

    # 1300 sps
    test_envpool(num_envs=48, envs_per_worker=4, envs_per_batch=12)



================================================
FILE: .github/workflows/install.yml
================================================
name: install

on:
  push:
  pull_request:

jobs:
  test:
    name: test ${{ matrix.py }} - ${{ matrix.os }} - ${{ matrix.env }}
    runs-on: ${{ matrix.os }}
    strategy:
      fail-fast: false
      matrix:
        os:
          - ubuntu-latest
          - macos-latest
        py:
          - "3.11"
          - "3.10"
          - "3.9"
        env:
          - pip
          - conda
    steps:
      - name: Checkout code
        uses: actions/checkout@v3

      - name: Setup Conda
        if: matrix.env == 'conda'
        uses: conda-incubator/setup-miniconda@v2
        with:
          python-version: ${{ matrix.py }}
          miniconda-version: "latest"
          activate-environment: test-env
          auto-update-conda: true

      - name: Setup Python for pip
        if: matrix.env == 'pip'
        uses: actions/setup-python@v4
        with:
          python-version: ${{ matrix.py }}

      - name: Upgrade pip
        run: python -m pip install -U pip

      - name: Install build dependencies
        run: pip install --upgrade "setuptools>=69.0.0" "packaging>=24.2" "numpy<2.0" wheel

      - name: Install PyTorch CPU  
        run: pip install torch --index-url https://download.pytorch.org/whl/cpu
      
      - name: Install pufferlib
        run: pip install -e . --no-build-isolation


